{
  "paper_id": "2509.14527v1",
  "title": "Claip-Emo: Parameter-Efficient Adaptation Of Language-Supervised Models For In-The-Wild Audiovisual Emotion Recognition",
  "published": "2025-09-18T01:45:44Z",
  "authors": [
    "Yin Chen",
    "Jia Li",
    "Jinpeng Hu",
    "Zhenzhen Hu",
    "Richang Hong"
  ],
  "keywords": [
    "Affective computing",
    "audiovisual emotion recognition",
    "transfer learning",
    "CLIP",
    "CLAP"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Audiovisual emotion recognition (AVER) in the wild is still hindered by pose variation, occlusion, and background noise. Prevailing methods primarily rely on large-scale domain-specific pre-training, which is costly and often mismatched to real-world affective data. To address this, we present CLAIP-Emo, a modular framework that reframes in-the-wild AVER as a parameter-efficient adaptation of language-supervised foundation models (CLIP/CLAP). Specifically, it (i) preserves language-supervised priors by freezing CLIP/CLAP backbones and performing emotion-oriented adaptation via LoRA (updating ≤4.0% of the total parameters), (ii) allocates temporal modeling asymmetrically, employing a lightweight Transformer for visual dynamics while applying mean pooling for audio prosody, and (iii) applies a simple fusion head for prediction. On DFEW and MAFW, CLAIP-Emo (ViT-L/14) achieves 80.14% and 61.18% weighted average recall with only 8M training parameters, setting a new state of the art. Our findings suggest that parameter-efficient adaptation of language-supervised foundation models provides a scalable alternative to domain-specific pre-training for real-world AVER. The code and models will be available at https://github.com/MSA-LMC/CLAIP-Emo.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recognizing human emotions in the wild from audiovisual cues (AVER) is a cornerstone of affective computing  [1] , yet it remains a formidable challenge due to uncontrolled environmental factors such as unpredictable lighting, pose variations, occlusions, and diverse acoustic noise  [2, 3] . The dominant paradigm to tackle this involves a two-stage process: large-scale self-supervised pre-training (e.g., MAE  [4] ) on domain-specific corpora like VoxCeleb2  [5] , which contains human faces and voices, followed by full fine-tuning on the target emotion dataset  [6, 7, 8] . While effective, this approach suffers from two fundamental limitations: (i) high computational costs, as both pre-training and full fine-tuning require substantial resources, slowing research iteration and limiting deployment; and (ii) potential semantic gap, as self-supervised objectives tend to capture holistic representations of the input modality, rather than focusing on the subtle, componential cues that constitute emotional expression.\n\nLanguage-supervised Foundation Models (LFMs), such as the vision-language model CLIP  [9]  and the audio-language model CLAP  [10] , offer a compelling alternative. Pre-trained on webscale data to align raw signals with natural language descriptions, these models acquire semantically rich and robust representations of the world. This linguistic grounding provides a more direct pathway to interpreting the specific, componential cues that constitute high-level concepts like affective states, thus offering a promising foundation for AVER without requiring costly domain-specific pre-training. Despite their potential, adapting these powerful LFMs for AVER poses two fundamental challenges. First, conventional full finetuning, while common in unimodal adaptation  [11, 12] , risks catastrophic forgetting  [13]  by aggressively updating entire parameters, thereby corrupting the semantic priors crucial for affective understanding and leading to overfitting on limited emotion data. Second, and more subtly, the inherent nature of CLIP and CLAP presents an asymmetry dilemma: CLIP's visual encoder, pre-trained on static images, excels at spatial representation but is blind to the temporal dynamics of emotional expressions. In contrast, CLAP's audio encoder, pre-trained on entire audio clips, innately captures holistic, clip-level vocal semantics. A naive, symmetric adaptation would either fail to build necessary temporal awareness for visual emotion cues or disrupt the powerful, pre-existing global prior for auditory sentiment.\n\nTo resolve these challenges, we introduce CLAIP-Emo (Fig.  1 , 2), a novel framework based on the core principle that adaptation must respect the inherent properties of the foundation models. This principle manifests in a two-fold strategy. First, to preserve the rich, pre-trained semantic priors and prevent catastrophic forgetting, we freeze the CLIP and CLAP backbones and employ lightweight LoRA  [14]  adapters. This allows for task-specific specialization by updating a mere fraction (≤4.0%) of the total parameters. Second, we introduce a deliberately asymmetric temporal aggregation module to align with each model's distinct architectural priors. For the vision stream, which lacks temporal context, we introduce a lightweight Transformer to model the dynamic evolution of expres- sions from frame-level features explicitly. In contrast, for the audio stream, we employ simple mean-pooling to leverage and preserve the holistic, clip-level representation intrinsic to the CLAP encoder. These tailored representations are then fused via a simple \"concat+linear\" head for final prediction. This principled design leads to a simple yet powerful architecture, eliminating the need for complex cross-modal alignment mechanisms.\n\nOur main contributions are threefold: 1) we propose a parameterefficient, prior-preserving adaptation framework that enables LFMs for AVER while bypassing the need for costly domain-specific pre-training; 2) we introduce CLAIP-Emo, a novel architecture that exploits the asymmetric strengths of CLIP and CLAP through LoRA-based tuning and a temporal aggregation module aligned with their visual-spatial and audio-holistic priors; 3) with only 8M tunable parameters, CLAIP-Emo sets new state-of-the-art performance on DFEW  [15]  (80.14% Weighted Average Recall, WAR) and MAFW  [16]  (61.18% WAR), establishing a simple, reproducible, and powerful baseline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "We propose CLAIP-Emo, a parameter-efficient and prior-preserving framework for adapting foundation models to audiovisual emotion recognition (AVER). Our end-to-end architecture (Fig.  2 ) first adapts frozen CLIP/CLAP encoders via LoRA. It then applies asymmetric temporal aggregation tailored to each modality's dynamics before fusing the representations with a simple fusion head for final classification. This design ensures both high performance and efficiency.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Problem Formulation",
      "text": "Given a video clip C = (V, A) consisting of T visual frames V = {v1, . . . , vT } and its corresponding audio waveform A, the AVER task is defined as predicting an emotion label ŷ from a predefined category set C = {c1, . . . , cK } by maximizing the conditional probability P (y|V, A), where K is the number of emotion classes and y ∈ C denotes the ground-truth label. Our framework Φ : (V, A) → ŷ is optimized end-to-end using cross-entropy loss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion-Oriented Parameter-Efficient Adaptation",
      "text": "A central challenge in adapting foundation models is to specialize them for a downstream task without suffering from catastrophic forgetting or corrupting their rich, language-aligned priors. To address this, we adopt a Parameter-Efficient Fine-Tuning (PEFT) strategy using Low-Rank Adapters (LoRA)  [14] . Instead of updating the entire model, we freeze the original weights of the CLIP vision (ViT) and CLAP audio (HTSAT) encoders, and inject lightweight, trainable LoRA modules into each attention and MLP layer. Formally, LoRA modifies a frozen weight matrix W0 ∈ R dout×d in by adding a low-rank update:\n\nwhere only the matrices A and B are trainable. We set r = 8 and α = 32, yielding a tunable fraction of 4.0% for ViT-B/16 and 2.5% for ViT-L/14 with CLAP-HTSAT, enabling efficient adaptation while preserving foundation-model priors.\n\nVisual branch. Let EV(•; θV, ϕV) denote the adapted CLIP encoder with frozen backbone θV and trainable LoRA parameters ϕV. Each sampled visual frame vt is independently encoded, and we use the [CLS] token as the frame representation, i.e., f V t = EV(vt; θV , ϕV\n\nSimilarly, we adapt the CLAP audio encoder EA(•; θA, ϕA). The waveform A is converted to a Mel spectrogram ma ∈ R Ta×Fa , where Ta is the number of time frames and Fa is the number of Mel frequency bins. It is then encoded as a sequence, giving F A = EA(ma; θA, ϕA) ∈ R Ta×d A .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Asymmetric Temporal Aggregation",
      "text": "We adopt an asymmetric temporal aggregation strategy aligned with the pretraining objectives and feature granularity of each encoder. The visual branch employs an image-based CLIP encoder, which produces frame-level features without modeling cross-frame context, whereas the audio branch leverages CLAP, trained with a clip-level contrastive objective to capture global acoustic information. Accordingly, we introduce branch-specific asymmetric temporal aggregation, which compresses F V and F A into compact clip-level representations while maintaining a balance between representational capacity and computational efficiency.\n\nModeling Visual Dynamics. Visual emotional expressions are inherently dynamic, defined by the temporal evolution of facial cues. To capture these inter-frame dependencies, we model the visual sequence F V with a lightweight temporal Transformer. We prepend a learnable [CLS] token and add positional embeddings PV ∈ R (T +1)×d V to form the input:\n\nAfter processing by a single Transformer layer, the output embedding of the [CLS] token, which aggregates sequence-wide information, is taken as the final clip-level visual feature:\n\nPreserving Audio Priors. In contrast, the CLAP audio encoder is optimized for clip-level summarization under a contrastive pretraining objective. We thus avoid adding heavy temporal heads on top of the audio sequence F A and instead apply simple mean pooling:\n\nThis operation is not only efficient but also preserves CLAP's language-supervised priors, yielding a robust and holistic representation of the clip's acoustic content.\n\nFinally, the asymmetric design yields two compact representations, z V clip and z A clip , which are subsequently fused for multimodal emotion understanding.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Lightweight Fusion And Classification",
      "text": "A key principle of our design is that effective modality-specific processing obviates the need for complex fusion. Building upon the tailored representations (z V clip , z A clip ) from the preceding stage, we therefore employ a minimalist fusion head. The features are simply concatenated and then projected by a linear classifier for final prediction:\n\nwhere Wc ∈ R K×(d V +d A ) and bc ∈ R K are the learnable parameters for the K emotion classes. Here, p denotes the predicted class probabilities, and the final predicted label is obtained as ŷ = arg max p.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "We evaluate CLAIP-Emo on the DFEW  [15]  and MAFW  [16]  datasets. Audio preprocessing follows the CLAP pipeline  [10] , while video preprocessing adopts S4D  [17] . We present two variants: CLAIP-Emo-B (ViT-B/16) and CLAIP-Emo-L (ViT-L/14), containing 4.0% and 2.5% trainable parameters, respectively. Both employ the CLAP audio encoder, with the B-variant used as the default for ablations. LoRA adapters are configured with r=8, α=32, and dropout 0.1. Training is performed for 100 epochs on two NVIDIA RTX A40 GPUs using Adam with a cosine learning-rate schedule (5-epoch warmup), batch size 16 per GPU, and an initial learning rate of 1 × 10 -5 . We report mean Unweighted Average Recall (UAR) and Weighted Average Recall (WAR) across the official five-fold splits.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Studies",
      "text": "Effectiveness of Model Components. We conducted ablation studies to assess the contributions of various components within CLAIP-Emo, with results summarized in Table WAR on MAFW. Upgrading the audio stream from mean pooling to a Transformer increases trainable parameters by about 62% with negligible gains, which supports our asymmetric design. For multimodal fusion, a minimalist \"Concat+Linear\" head yields the best trade-off. Additive fusion reduces accuracy and gated fusion brings no benefit on MAFW despite higher complexity. In summary, tuning 4% parameters of the backbone, modeling only visual dynamics, and employing a linear fusion head collectively provide CLAIP-Emo with an optimal balance between performance and efficiency. Modality Contribution. Table  2  quantifies the contribution of each modality. While the audio-only model provides a reasonable baseline (e.g., 38.15%/47.52% UAR/WAR on DFEW), the visual model serves as the primary contributor, achieving a significantly higher UAR/WAR of 62.60%/76.60%. This trend is consistent on MAFW, underscoring the primacy of facial cues for in-the-wild emotion recognition. Ultimately, fusing both modalities yields the best results on both datasets (65.96%/79.84% on DFEW and 37.34%/51.14% on MAFW), demonstrating that audio provides crucial complementary information and validating the synergistic power of the CLIP and CLAP backbones.\n\nVisual Prior Ablation. We investigate the impact of visual pre-training in Table  3  by replacing the CLIP-ViT backbone while keeping the CLAP audio branch fixed. As expected, adapting a",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Visualization",
      "text": "Figure  4  visualizes the learned feature spaces. The full fine-tuning baseline (a) suffers from severe cluster overlap, particularly for classes like Fear and Surprise, indicating feature confusion. In contrast, CLAIP-Emo (b) demonstrates a markedly improved feature space, yielding more compact and better-defined clusters overall. This enhanced feature discriminability, characterized by reduced intra-class variance and increased inter-class margins, helps explain our quantitative gains and validates the effectiveness of our prior-preserving adaptation strategy.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we reframed in-the-wild AVER as the parameterefficient and prior-preserving adaptation of language-supervised foundation models. We introduced CLAIP-Emo, a framework that strategically adapts frozen CLIP and CLAP backbones using LoRA (tuning ≤4% of parameters), processes temporal information with a tailored asymmetric architecture, and fuses features with a simple fusion head. This efficient design sets a new state of the art on DFEW (80.14% WAR) and MAFW (61.18% WAR) with only 8M tunable parameters, surpassing substantially larger, pre-trained models.\n\nOur results demonstrate that leveraging language-supervised priors is a scalable and effective alternative to costly domain-specific pre-training for real-world AVER.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Domain-specific pre-training and full fine-tuning vs. our",
      "page": 1
    },
    {
      "caption": "Figure 2: Overview of the CLAIP-Emo framework. Our model adapts frozen CLIP and CLAP backbones using lightweight LoRA adapters.",
      "page": 2
    },
    {
      "caption": "Figure 2: ) first adapts",
      "page": 2
    },
    {
      "caption": "Figure 3: , where CLAIP-Emo",
      "page": 4
    },
    {
      "caption": "Figure 3: CLAIP-Emo achieves state-of-the-art performance with sig-",
      "page": 4
    },
    {
      "caption": "Figure 4: t-SNE [21] visualization of features extracted by (a) full fine-",
      "page": 4
    },
    {
      "caption": "Figure 4: visualizes the learned feature spaces. The full fine-tuning",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Increasing the crucial complementary information and validating the synergistic",
      "data": [
        {
          "Modality": "A\nV\nA+V",
          "Backbone": "HTSAT\nViT-B/16\nViT-B/16 + HTSAT",
          "DFEW (fd1)\nUAR/WAR": "38.15 / 47.52\n62.60 / 76.60\n65.96 / 79.84",
          "MAFW (fd1)\nUAR/WAR": "20.92 / 30.88\n34.61 / 49.24\n37.34 / 51.14"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: Comparison with state-of-the-art methods on in-the-wild",
      "data": [
        {
          "Visual Backbone Pre-training": "Random Initialization\nImageNet-21k (Supervised)\nCLIP-ViT (Ours)",
          "DFEW (fd1)\nUAR/WAR": "39.86 / 48.91\n59.41 / 72.92\n65.96 / 79.84",
          "MAFW (fd1)\nUAR/WAR": "21.37 / 30.99\n31.75 / 45.97\n37.34 / 51.14"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: Comparison with state-of-the-art methods on in-the-wild",
      "data": [
        {
          "Method": "HuBERT [18]\nWavLM-Plus [19]\nMAE-DFER [6]\nDFER-CLIP [11]\nDK-CLIP [12]\nS2D [20]\nS4D [17]\nHiCMAE-B [6]\nVAEmo [7]\nAVF-MAE++ (B) [8]\nAVF-MAE++ (L) [8]\nAVF-MAE++ (H) [8]",
          "Mod.": "A\nA\nV\nV\nV\nV\nV\nAV\nAV\nAV\nAV\nAV",
          "TP.": "95\n95\n85\n-\n-\n9\n101\n81\n39\n169\n303\n521",
          "DFEW\nUAR/WAR": "36.95 / 43.24\n37.78 / 44.64\n63.41 / 74.43\n59.61 / 71.25\n64.95 / 75.41\n61.82 / 76.03\n66.80 / 76.68\n63.76 / 75.01\n64.02 / 75.78\n63.74 / 76.24\n65.14 / 75.42\n66.88 / 77.45",
          "MAFW\nUAR/WAR": "25.00 / 32.60\n26.33 / 34.07\n41.62 / 54.31\n39.89 / 52.59\n43.01 / 56.56\n41.86 / 57.37\n43.72 / 58.44\n42.65 / 56.17\n45.67 / 58.91\n43.10 / 57.50\n45.36 / 59.13\n46.05 / 60.24"
        },
        {
          "Method": "CLAIP-Emo (ViT-B/16)\nCLAIP-Emo (ViT-L/14)",
          "Mod.": "AV\nAV",
          "TP.": "5\n8",
          "DFEW\nUAR/WAR": "66.21 / 78.42\n69.52 / 80.14",
          "MAFW\nUAR/WAR": "45.58 / 60.44\n46.65 / 61.18"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Automatic analysis of facial expressions: The state of the art",
      "authors": [
        "Maja Pantic",
        "Leon Rothkrantz"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "3",
      "title": "Avec 2011-the first international audio/visual emotion challenge",
      "authors": [
        "Björn Schuller",
        "Michel Valstar",
        "Florian Eyben",
        "Gary Mckeown",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2011",
      "venue": "International conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "4",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015",
      "authors": [
        "Abhinav Dhall",
        "Roland Ov Ramana Murthy",
        "Jyoti Goecke",
        "Tom Joshi",
        "Gedeon"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "5",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "6",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "Son Joon",
        "Arsha Chung",
        "Andrew Nagrani",
        "Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition",
      "arxiv": "arXiv:1806.05622"
    },
    {
      "citation_id": "7",
      "title": "Hicmae: Hierarchical contrastive masked autoencoder for selfsupervised audio-visual emotion recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "Vaemo: Efficient representation learning for visual-audio emotion with knowledge injection",
      "authors": [
        "Hao Cheng",
        "Zhiwei Zhao",
        "Yichao He",
        "Zhenzhen Hu",
        "Jia Li",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2025",
      "venue": "Vaemo: Efficient representation learning for visual-audio emotion with knowledge injection",
      "arxiv": "arXiv:2505.02331"
    },
    {
      "citation_id": "9",
      "title": "Avf-mae++: Scaling affective video facial masked autoencoders via efficient audio-visual self-supervised learning",
      "authors": [
        "Xuecheng Wu",
        "Heli Sun",
        "Yifan Wang",
        "Jiayu Nie",
        "Jie Zhang",
        "Yabing Wang",
        "Junxiao Xue",
        "Liang He"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "10",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "11",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Yusong Wu",
        "Ke Chen",
        "Tianyu Zhang",
        "Yuchen Hui",
        "Taylor Berg-Kirkpatrick",
        "Shlomo Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "13",
      "title": "Domain knowledge enhanced vision-language pretrained model for dynamic facial expression recognition",
      "authors": [
        "Liupeng Li",
        "Yuhua Zheng",
        "Shupeng Liu",
        "Xiaoyin Xu",
        "Taihao Li"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Analyzing the forgetting problem in pretrain-finetuning of open-domain dialogue response models",
      "authors": [
        "Tianxing He",
        "Jun Liu",
        "Kyunghyun Cho",
        "Myle Ott",
        "Bing Liu",
        "James Glass",
        "Fuchun Peng"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"
    },
    {
      "citation_id": "15",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "ICLR"
    },
    {
      "citation_id": "16",
      "title": "Dfew: A largescale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "17",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM international conference on multimedia"
    },
    {
      "citation_id": "18",
      "title": "Static for dynamic: Towards a deeper understanding of dynamic facial expressions using static expression data",
      "authors": [
        "Yin Chen",
        "Jia Li",
        "Yu Zhang",
        "Zhenzhen Hu",
        "Shiguang Shan",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2024",
      "venue": "Static for dynamic: Towards a deeper understanding of dynamic facial expressions using static expression data",
      "arxiv": "arXiv:2409.06154"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "20",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Yin Chen",
        "Jia Li",
        "Shiguang Shan",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}