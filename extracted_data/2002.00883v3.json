{
  "paper_id": "2002.00883v3",
  "title": "Adversarial-Based Neural Networks For Affect Estimations In The Wild",
  "published": "2020-02-03T16:52:49Z",
  "authors": [
    "Decky Aspandi",
    "Adria Mallol-Ragolta",
    "Björn Schuller",
    "Xavier Binefa"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "There is a growing interest in affective computing research nowadays given its crucial role in bridging humans with computers. This progress has been recently accelerated due to the emergence of bigger data. One recent advance in this field is the use of adversarial learning to improve model learning through augmented samples. However, the use of latent features, which is feasible through adversarial learning, is not largely explored, yet. This technique may also improve the performance of affective models, as analogously demonstrated in related fields, such as computer vision. To expand this analysis, in this work, we explore the use of latent features through our proposed adversarial-based networks for valence and arousal recognition in the wild. Specifically, our models operate by aggregating several modalities to our discriminator, which is further conditioned to the extracted latent features by the generator. Our experiments on the recently released SEWA dataset suggest the progressive improvements of our results. Finally, we show our competitive results on the Affective Behavior Analysis in-the-Wild (ABAW) challenge dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective computing has recently attracted the attention of the research community, due to its applications in multiple and diverse areas, including education  [7]  or healthcare  [25] , among others. Furthermore, the growing availability of affect-related datasets, such as SEWA  [23]  and the recently introduced Aff-Wild2  [21] , enable the rapid development of deep learning-based techniques, which currently hold the state of the art  [22] ,  [23] ,  [18] .\n\nIn computer vision tasks, such as natural image generation  [32]  and image classification  [29] , adversarial learning techniques from the family of generative models have been extensively investigated  [32] ,  [29] ,  [5] . This learning technique enables rapid progress, not only to create additional data, but also to improve the performance of predictive models. Nevertheless, in the context of affective computingrelated applications, this technique is still young and confined to its usage for data augmentation purposes  [13] .\n\nTo expand the investigation of generative models in the field of affective computing, we investigate the use of latent features that are extracted in adversarial manners to improve the predictive capabilities of our model estimations. Specifically, we extract the visual latent features of the generator, which are then used to condition the discriminator on its estimations. Furthermore, we also aggregate the audio modality during training. We later show in our experiments on the SEWA  [23]  and Aff-Wild2  [21]  datasets the benefits of our proposed approach with our competitive results. Specifically, the contributions of this work are:\n\n1) We are the first to introduce the utilisation of latent features arranged in an adversarial way to improve affect-related model estimates. 2) We show the progressive improvements on our proposed works on the SEWA and Aff-Wild2 datasets and achieve competitive results on both datasets.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Early approaches on automatic affect estimations involved the use of classical machine learning techniques with some degree of success. Several techniques explored include linear regression  [30] , partial least square regression  [31] , and support vector machines  [28] . Furthermore, given the number of available modalities (e. g. video, audio, and bio-signals), several fusion techniques were also introduced to improve affect-related estimates. Different examples of these methods include early, late, model, and output-associative fusion  [40] . Diverse affective information has been progressed, starting from Action Units detection, emotion detection, to more recently continuous valence and arousal estimation  [22] ,  [23] .\n\nCurrent progress relates to the emergence of big data that creates the opportunity to introduce large scale datasets in many fields, including affective computing. Examples of these datasets are SEMAINE  [26] , AVEC  [34] , AFEW  [22] , RECOLA  [35] , SEWA  [23] , and the recently introduced Aff-Wild2 dataset  [21] ,  [20] ,  [39] ,  [16] . These datasets enable the development of powerful deep learning models that improve the accuracy of current state of the art  [22] ,  [19] ,  [18] . The investigations of deep learning-based techniques include the introduction of Convolutional Neural Networks (CNN)  [4] , incorporation of Recurrent Neural Network (RNN)  [23] ,  [33] , and recently the fusion with Tensor-based methods  [27] .\n\nAdversarial learning  [32]  as a generative approach has been intensively studied in other machine learning research, especially in computer vision  [5] . Given its potential, this method has also been explored in the field of affective computing, usually to augment the training data available for training  [13] . However, there is another aspect of generative models that is largely unexplored in this field, which is the use of latent features to improve the models estimations, as shown in previous works from other fields, such as computer vision  [38] , machine learning  [11] , and bio-signal analysis  [37] ,  [6] . This inspired us to investigate the use of adversarial learning to improve our proposed models' performance through extracted latent features.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Adversarial Latent-Based Networks",
      "text": "We build our model based on the Star-GAN network  [5] , with architectural modifications to allow the extraction of latent features and use of the audio features. Figure  1  shows the overview of our proposed network. Our model operates by aggregating two main modalities: facial and audio features. There are two main sub-networks involved in our overall networks as already outlined above: the Auto-Encoder-based Generator(AEG), and the Conditional Discriminator-based affect estimator (CD)  [24] . The main role of the AEG is to produce cleaned images from noisy images to fool the discriminator, while simultaneously extracting robust latent features. On the other hand, the CD tries to recognise the fake images created by the AEG, and, at the same time, estimates the actual valence and arousal values. We train the AEG and CD in an adversarial way as below:\n\nwhere x corresponds to the noisy image and x is the cleaned input image approximated by the AEG. We use similar noise introduction methods as in  [3] , which consist of four different types of artifacts: Gaussian blurring, Gaussian noise, image downsampling, and colour scaling.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Auto-Encoder-Based Generator",
      "text": "Given the noisy input image, x, the AEG will approximate the cleaned version of the input image, x. This is done by utilising coupled mirrored convolutions and deconvolutions with intermediate 2D bottleneck latent kernels; i. e. without skip connections. This scheme enforces the AEG to create latent robust features in order to effectively clean the input image. To improve the denoising and reconstruction process, we use the cycle loss  [5] ,  [15]  defined below :\n\n(2)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Conditional Discriminator-Based Affect Estimator",
      "text": "The CD employs both facial and audio features to identify the real/fake status of the current input and the corresponding valence and arousal values of θ. The facial features correspond to the cleaned image (denoised or reconstructed from the generator and the corresponding latent features of z. From the audio modality, we use the low-level descriptors (LLDs) of the EGEMAPS feature set  [8]  (cf. Section III-D). Both latent and audio features are combined through late fusion  [12] ,  [36] ,  [40]  alongside the main RGB input images. Specifically, the audio features are merged by feeding them into a 1D fully connected layer to enlarge its dimension and converting it to a single 2D kernel, which is then concatenated with the denoised image. The latent features are combined in middle pipelines of the CD by concatenating them with intermediate kernels.\n\nTo detect both real and fake status and estimate valence and arousal values, we add another classifier  [29]  on top of the main classifier, which consists of a 2x2 pixels layer  [14] . In the adversarial training, the CD will be optimised using real (r) and fake (f ) images to minimise the affect loss (L af c ) that judges the accuracy of the estimated valence and arousal values (cf. Equation  5 ). The corresponding loss of training the CD for both real (L r va ) and false examples (L f va ) can be seen below :\n\nwhere θ is the ground truth valence/arousal value, and the affect loss, L af c , corresponds to the amalgamations of multiple affect metrics: Mean Square Error(MSE) (Eq. 6), Correlation(COR) (Eq. 7), and Concordance Correlation Coefficients (CCC) (Eq. 8),  [22] ,  [21]  :\n\nwhere n i is the total number of instances of discrete valence/arousal class i, and N is the normalisation factor  [1]  for the total valence/arousal class. This normalisation factor is crucial given considerably unbalanced class instance on the Aff-Wild2 Challenge  [17] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Overall Objective",
      "text": "Finally, the overall objective functions to train both AEG and CD are expressed as follows:\n\nin which λ af c and λ rec are the regulariser parameters for affect estimations and reconstruction loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Audio Feature Extraction",
      "text": "One of the first challenges when combining audio and video signals is the difference in term of sampling rates between both modalities. To overcome this issue, we first generate audio frames from the original audio signal by selecting the portions of the audio signal corresponding to one frame of video. We then enlarge the audio frame with the samples corresponding to the previous and future video frame to ensure information overlap between consecutive audio frames. We finally extract the LLDs of the EGEMAPS  [8]  feature set using OPENSMILE  [9] , and concatenate the first two sets of LLDs for further analysis. These LLDs are extracted from windows of 0.060 seconds with a step size of 0.010 seconds. Selecting the first two sets of LLDs only, we ensure the same dimensionality of the audio features in spite of videos recorded at different sampling rates.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Model Training",
      "text": "In the challenge dataset, we only utilise the training subset to obtain our validation results. We exploit the full available data (training and validation) to train our final models submitted to the challenge organisers. Specifically, we used the crop-aligned samples provided by the organisers as facial features, in addition to the audio signals from the available videos.\n\nTo provide more actual comparisons of our results to the other state of the arts, we also include the experiments on the Sentiment Analysis in the Wild dataset (SEWA)  [23] . To obtain our results, we followed original person-independence protocols, and apply similar feature extraction techniques as those employed for this challenge. Furthermore, we also use the external tracker of  [2]  to refine the given bounding box.\n\nFor both datasets, we trained our model progressively to allow us to analyse the impact of each proposed step. We first train both of generator and discriminator together, and proceed by adding the extracted latent z features alongside the audio features. Our models were trained using an NVIDIA Titan X GPU and it took approximately two days to converge. The source code of our models is available at our github page  1",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we describe our results on the recently introduced Aff-Wild2 Challenge  [17]  and SEWA  [23]  datasets to confirm the advantages of each of our proposed approaches.\n\n• The Aff-Wild2 challenge dataset is being published as part of the first ABAW 2020 competitions  [17] . This competition consists of three main challenges : valencearousal, basic expression and eight action units. Aff-wild2 is considered to be the current, largest affect in the wild dataset with more than 558 videos and 458 total number of subjects. Specific on the valence and arousal challenge, there are 545 annotated videos with 2.786.201 frames which is split into three subsets : 346 videos of training, 68 videos of validation and 131 videos of test. • The SEWA dataset  [23]  is a recently published affect dataset which consists of video and audio recording involving 398 subjects from multiple cultures. This dataset is split into 538 sequences with various metadata (e.g. subject id, culture etc) are available alongside the actual affect ground truth of valence/arousal and liking/disliking. We use MSE, COR and CCC metrics to evaluate the quality of each affect estimations  [23] ,  [34] ,  [27] ,  [17] . That on the Aff-Wild2 dataset, we compared our results on the validation stage against the baseline provided by the organizers  [17] . While for the SEWA dataset, we report our results from original five cross validation settings  [23]  and compared them with the respective baseline  [23]  and recent state of the art of  [27]  Tables II and I provide our results on ABAW Challenge and SEWA, respectively. Method Disc corresponds to our results utilizing only plain Discriminator (CD) trained using standard 2 loss. Method AEG-CD means that our model uses adversarial training for both of AEG and CD. Lastly, the AEG-CD-ZS shows the results of our previous model trained with the inclusion of both latent features z from AEG and the mapped audio features.\n\nBased on these quantitative results, we can see that our models are able to produce quite competitive results, with high accuracy on the arousal dimension. Specifically, we observe quite high accuracy obtained by our discriminator   (Disc) which is able to outperform the current baseline results of SEWA, albeit it is still lower on the challenge dataset. This maybe attributed to limitations of using standard loss, which does not incorporate more objective metrics that adversarial learning offers  [5] ,  [29] ,  [10] . This is confirmed by the better accuracy obtained by using adversarial learning. Another potential reason can be attributed to the generated images that may reduce the available noises on the input images. An example of denoised images can be seen in Figure  2 . There, we can see that the denoised input image is quite cleaned and this may also be indicative of a successful generator training in creating the robust latent features. We finally can see that incorporating both of the latent and audio features improves the overall accuracy of our results, surpasses both of the baselines on the affect dataset, and the state of the art of the SEWA dataset, especially on the arousal dimension. This result highlights the benefit of incorporating such features.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we presented the first investigation using latent features extracted through adversarial learning in Affective Computing domain. Specifically, we performed progressive training on our generator to extract robust features given noisy inputs paired with a discriminator through adversarial learning. Then, we employed a conditional discriminator to aggregate several modality's inputs to achieve our affect estimations. We tested the performance of our models on two datasets: Aff-Wild2 and SEWA. In our experiments, we observed progressive improvements made by each of our approaches utlimately leading to competitive results on the ABAW challenge dataset. In the future, we seek to incorporate temporal modelling to further increase the accuracy of the proposed models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Acknowledgments",
      "text": "This work is partly supported by the Spanish Ministry of Economy and Competitiveness under project grant TIN2017-90124-P, the Ramon y Cajal programme, the Maria de",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Complete architecture of our proposed models which incorporate two main networks: ﬁrst is an Auto-Encoder-based Generator (AEG) which",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the",
      "page": 2
    },
    {
      "caption": "Figure 2: Example of denoised images. As we can see, the denoised image",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Heatmap-guided balanced deep convolution networks for family classification in the wild",
      "authors": [
        "D Aspandi",
        "O Martinez",
        "X Binefa"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "2",
      "title": "Fully end-toend composite recurrent convolution network for deformable facial tracking in the wild",
      "authors": [
        "D Aspandi",
        "O Martinez",
        "F Sukno",
        "X Binefa"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "3",
      "title": "Robust facial alignment with internal denoising auto-encoder",
      "authors": [
        "D Aspandi",
        "O Martinez",
        "F Sukno",
        "X Binefa"
      ],
      "year": "2019",
      "venue": "2019 16th Conference on Computer and Robot Vision (CRV)"
    },
    {
      "citation_id": "4",
      "title": "Ets system for av+ec 2015 challenge",
      "authors": [
        "P Cardinal",
        "N Dehak",
        "A Koerich",
        "J Alam",
        "P Boucher"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "5",
      "title": "Stargan: Unified generative adversarial networks for multi-domain image-toimage translation",
      "authors": [
        "Y Choi",
        "M Choi",
        "M Kim",
        "J.-W Ha",
        "S Kim",
        "J Choo"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "End-to-end facial and physiological model for affective computing and applications",
      "authors": [
        "J Comas",
        "D Aspandi",
        "X Binefa"
      ],
      "year": "2019",
      "venue": "End-to-end facial and physiological model for affective computing and applications"
    },
    {
      "citation_id": "7",
      "title": "An e-learning system based on affective computing",
      "authors": [
        "S Duo",
        "L Song"
      ],
      "year": "2010",
      "venue": "Physics Procedia"
    },
    {
      "citation_id": "8",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "openSMILE -The Munich Versatile and Fast Open-source Audio Feature Extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18 th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks",
      "authors": [
        "Q Gan",
        "Q Guo",
        "Z Zhang",
        "K Cho"
      ],
      "year": "2015",
      "venue": "First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks"
    },
    {
      "citation_id": "11",
      "title": "Expanding variational autoencoders for learning and exploiting latent representations in search distributions",
      "authors": [
        "U Garciarena",
        "R Santana",
        "A Mendiburu"
      ],
      "year": "2018",
      "venue": "Proceedings of the Genetic and Evolutionary Computation Conference"
    },
    {
      "citation_id": "12",
      "title": "Affect recognition from face and body: early fusion vs. late fusion",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2005",
      "venue": "2005 IEEE international conference on systems, man and cybernetics"
    },
    {
      "citation_id": "13",
      "title": "Adversarial training in affective computing and sentiment analysis: Recent advances and perspectives",
      "authors": [
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "14",
      "title": "Image-to-image translation with conditional adversarial networks",
      "authors": [
        "P Isola",
        "J.-Y Zhu",
        "T Zhou",
        "A Efros"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Learning to discover cross-domain relations with generative adversarial networks",
      "authors": [
        "T Kim",
        "M Cha",
        "H Kim",
        "J Lee",
        "J Kim"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "17",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first abaw 2020 competition"
    },
    {
      "citation_id": "18",
      "title": "Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "20",
      "title": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-task learning & generation framework: Valence-arousal, action units & primary expressions",
      "arxiv": "arXiv:1811.07771"
    },
    {
      "citation_id": "21",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "22",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "23",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "B Schuller",
        "K Star"
      ],
      "year": "2019",
      "venue": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "arxiv": "arXiv:1901.02839"
    },
    {
      "citation_id": "24",
      "title": "Semi-supervised learning with gans: Manifold invariance with improved inference",
      "authors": [
        "A Kumar",
        "P Sattigeri",
        "T Fletcher"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30"
    },
    {
      "citation_id": "25",
      "title": "Online affect detection and robot behavior adaptation for intervention of children with autism",
      "authors": [
        "C Liu",
        "K Conn",
        "N Sarkar",
        "W Stone"
      ],
      "year": "2008",
      "venue": "IEEE T Robot"
    },
    {
      "citation_id": "26",
      "title": "The semaine corpus of emotionally coloured character interactions",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "2010 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "27",
      "title": "Valence and arousal estimation in-the-wild with tensor methods",
      "authors": [
        "A Mitenkova",
        "J Kossaifi",
        "Y Panagakis",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "28",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valencearousal space",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Semi-supervised learning with generative adversarial networks",
      "authors": [
        "A Odena"
      ],
      "year": "2016",
      "venue": "Semi-supervised learning with generative adversarial networks",
      "arxiv": "arXiv:1606.01583"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition for avec 2016 challenge",
      "authors": [
        "F Povolny",
        "P Matejka",
        "M Hradis",
        "A Popková",
        "L Otrusina",
        "P Smrz",
        "I Wood",
        "C Robin",
        "L Lamel"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "31",
      "title": "Multimodal emotion recognition for avec 2016 challenge",
      "authors": [
        "F Povolny",
        "P Matejka",
        "M Hradis",
        "A Popková",
        "L Otrusina",
        "P Smrz",
        "I Wood",
        "C Robin",
        "L Lamel"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "32",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "A Radford",
        "L Metz",
        "S Chintala"
      ],
      "year": "2015",
      "venue": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "arxiv": "arXiv:1511.06434"
    },
    {
      "citation_id": "33",
      "title": "Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data",
      "authors": [
        "F Ringeval",
        "F Eyben",
        "E Kroupi",
        "A Yuce",
        "J.-P Thiran",
        "T Ebrahimi",
        "D Lalanne",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "Pattern Recognition in Human Computer Interaction"
    },
    {
      "citation_id": "34",
      "title": "Avec 2019 workshop and challenge: State-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "Avec 2019 workshop and challenge: State-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "arxiv": "arXiv:1907.11510"
    },
    {
      "citation_id": "35",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "36",
      "title": "Early versus late fusion in semantic video analysis",
      "authors": [
        "C Snoek",
        "M Worring",
        "A Smeulders"
      ],
      "year": "2005",
      "venue": "Proceedings of the 13th annual ACM international conference on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Multimodal emotion recognition using deep neural networks",
      "authors": [
        "H Tang",
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "38",
      "title": "Deep autoencoder for combined human pose estimation and body model upscaling",
      "authors": [
        "M Trumble",
        "A Gilbert",
        "A Hilton",
        "J Collomosse"
      ],
      "year": "2018",
      "venue": "The European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "39",
      "title": "Aff-wild: Valence and arousal in-the-wildchallenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "40",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    }
  ]
}