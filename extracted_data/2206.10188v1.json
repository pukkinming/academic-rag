{
  "paper_id": "2206.10188v1",
  "title": "Analysis Of Self-Supervised Learning And Dimensionality Reduction Methods In Clustering-Based Active Learning For Speech Emotion Recognition",
  "published": "2022-06-21T08:44:55Z",
  "authors": [
    "Einari Vaaras",
    "Manu Airaksinen",
    "Okko Räsänen"
  ],
  "keywords": [
    "active learning",
    "unsupervised learning",
    "contrastive learning",
    "manifold learning",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "When domain experts are needed to perform data annotation for complex machine-learning tasks, reducing annotation effort is crucial in order to cut down time and expenses. For cases when there are no annotations available, one approach is to utilize the structure of the feature space for clustering-based active learning (AL) methods. However, these methods are heavily dependent on how the samples are organized in the feature space and what distance metric is used. Unsupervised methods such as contrastive predictive coding (CPC) can potentially be used to learn organized feature spaces, but these methods typically create highdimensional features which might be challenging for estimating data density. In this paper, we combine CPC and multiple dimensionality reduction methods in search of functioning practices for clustering-based AL. Our experiments for simulating speech emotion recognition system deployment show that both the local and global topology of the feature space can be successfully used for AL, and that CPC can be used to improve clustering-based AL performance over traditional signal features. Additionally, we observe that compressing data dimensionality does not harm AL performance substantially, and that 2-D feature representations achieved similar AL performance as higher-dimensional representations when the number of annotations is not very low.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In many complex real-world machine-learning applications, annotating data can be expensive and time-consuming. This is often the case particularly in situations where domain experts are needed to carry out the annotation process  [1, 2] . For such cases, active learning (AL) algorithms can be used to reduce human annotation effort and to produce machine-learning models that perform well with limited labeled data  [3] . For example, data scarcity is an ever-present problem in deploying speech emotion recognition (SER) systems to new domains  [4] .\n\nThere are a number of different approaches for AL, of which by far the most popular methods are based on uncertainty or confidence scores of a classifier trained on already-labeled data (e.g.  [4] [5] [6] [7] [8] ). However, when the maximum number of labels that can be manually assigned, also known as the labeling budget, adds up to only a small subset of the data, the aforementioned AL approaches cannot be applied. This is because these methods often require a rather large number of annotated samples before they can outperform random sampling  [9] .\n\nIn the absence of any existing annotated data, a potential approach to AL is to utilize the distributional properties of the dataset with clustering-based AL methods (e.g.  [10] [11] [12] [13] ). These methods rely heavily on how the samples are organized in the feature space (i.e. the choice of features) and what distance metric is used, as the methods need to use these two to cluster the data points and to prioritize the order in which cluster samples are provided for human annotators. This puts particular emphasis on how the features behave in the given metric space with respect to the analysis task at hand. In addition, the dimensionality of the used features poses a potential challenge for the AL algorithms, as accurate data density estimation from finite data becomes less accurate in higher-dimensional spaces  [14] . Hence, it could be beneficial for AL if there was a systematic way of representing data samples in a low-dimensional feature space. Moreover, a lower dimensionality would heavily reduce the computational complexity of AL algorithms. Taken to the extreme, a 2-D feature space could be used to combine AL with data visualization and efficient human-based data exploration and reorganization, such as in the annotation platform described in  [15] .\n\nWhile the use of standard acoustic features such as i-vectors  [12]  or MFCCs  [16]  has been applied to previous AL clustering approaches, there are nowadays a number of highly promising unsupervised methods for learning feature spaces (e.g.  [17] [18] [19] ). These methods can learn linearly separable representations for many speech phenomena of interest (e.g.  [17] ), and could potentially be used to learn organized feature spaces for clusteringbased AL algorithms. However, the aforementioned methods typically result in high-dimensional feature spaces which can impose challenges for AL algorithms, as discussed above. Also, these methods have been used primarily for learning features that are relevant for supervised downstream tasks, but not for intermediate tasks such as clustering, where data grouping is equally important to data separability (see Fig.  1  for an example).\n\nIn the present study, we combine AL with unsupervised learning and dimensionality reduction methods in order to seek answers to the questions of whether self-supervised representation learning (SSRL) can improve AL performance over classical signal features, and how AL performance depends on input feature dimensionality. While doing so, the work also sheds light on the question of how the local and global topology of a feature space affects clustering-based AL systems. In order to answer these questions, we conduct a set of experiments involving a simulated annotation procedure and a clustering-based AL algorithm from  [16] . We use SER as our test case using four different SER corpora and with two distinct classification tasks. Our ultimate goal is to find functioning practices for developing machine-learning algorithms for applications where access to human expert labels is expensive. Hence, our primary focus is in cases with a low labeling budget, although we also include higher labeling budgets in our present experiments.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "Fig.  2  depicts a block diagram of the present experiments. First, a standard acoustic feature (log-mel) representation is obtained from an input signal. Then, an SSRL algorithm called contrastive predictive coding (CPC)  [17]  is applied to the data. CPC aims to produce linearly separable features that can be used to predict signal evolution over time  [20, 21] . CPC has already been successfully used with clustering-based approaches (e.g.  [21] [22] [23] ) and also produces features that separate suprasegmental properties such as speaker identities  [17] . However, to the best of our knowledge,  [24]  is the only study so far using CPC for AL.\n\nNext, an array of alternative dimensionality reduction methods is optionally applied to the log-mel and CPC features. In the present study, we explore the use of t-distributed stochastic neighbor embedding (t-SNE)  [25] , nonlinear bottleneck autoencoders (AEs), and principal component analysis (PCA) for dimensionality reduction. Among these, t-SNE preserves the local structure of the high-dimensional data while also revealing some global aspects, such as clusters at multiple scales  [25] . In contrast, PCA simply maps the data into principal axes of variation without differentially altering the local and global metric structure of the feature space. Bottleneck AEs, on the other hand, attempt to learn a low-dimensional feature embedding from which the original input features can be reconstructed.\n\nFinally, we run a simulated AL-based speech emotion annotation and a SER classifier deployment procedure using a support vector machine (SVM) classifier, similar to  [26] . For this, we use a clustering-based AL algorithm together with each possible combination of log-mel or CPC features and the aforementioned dimensionality reduction methods, including not using dimensionality reduction at all, and compare the resulting SER performance among the alternative strategies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Log-Mel",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cpc",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Predictive Coding",
      "text": "CPC  [17]  is an unsupervised method for extracting representations from data that encode underlying shared information between different parts of the input signal. This is achieved by predicting k ∈ {1, ..., K} future latent representations of the input signal where typically K > 1 (e.g. K = 12 for speech in  [17] ). A CPC model consists of two separate models, a nonlinear encoder, genc, and an autoregressive model, gar. First, genc maps the input observations, xt, into latent representations zt = genc(xt). Then, gar maps z ≤t into a context latent representation ct = gar(z ≤t ). Instead of directly modeling x t+k , CPC models a density ratio that aims to preserve the mutual information between x t+k and ct. For this, a log-bilinear model f k (x t+k , ct) = exp z T t+k W k ct is used, where W k are linear transformations. Both genc and gar are trained using the loss\n\nwhere X = {x1, ..., xN } is a set of N random samples containing one positive sample and N -1 negative samples. As shown in  [17] , minimizing the loss in Eq. 1 maximizes the mutual information between x t+k and ct.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Medoid-Based Active Learning",
      "text": "For AL, we use medoid-based active learning (MAL)  [16] , which is an AL method developed for scarce labeling budgets. The algorithm consists of three subsequent stages: 1) compute an affinity matrix containing the pairwise distances between each sample in a dataset, 2) perform k-medoids clustering using this affinity matrix, and 3) in a descending cluster size order, query for human annotations for the medoids.\n\nIn the first stage, the pairwise distances between each sample in a dataset are computed using a distance metric, d, and are stored to an affinity matrix, A. In the second stage, k-medoids clustering (see e.g.  [27] ) is performed for the data using A. First, one sample is randomly selected as a member of a set, S. Then, k -1 additional samples are added to S one at a time using the farthest-first traversal algorithm  [28] , each added sample being farthest from the current set S. Here, the distance from a sample, a, to the set S is defined as d(a, S) = min b∈S d(a, b). Next, the samples in S are used as initial medoids for a k-medoids clustering algorithm in order to assign each sample into one of the k clusters. Based on a previous study  [29] , k was set to be N 3 , where N is the number of samples in a corpus.\n\nIn the third stage, the clusters are first sorted in a descendingsize order, after which the cluster medoids are presented to human annotators for labeling. In the present experiments, the obtained labels were used in two different ways: i) using only the medoid labels as labeled data (referred to as \"medoid labels\"), or ii) propagating the medoid label to all elements in the cluster (referred to as \"cluster labels\").",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional Speech Corpora",
      "text": "We conducted our experiments using four SER corpora:\n\n1) The Berlin Emotional Speech Database (EMO-DB)  [30]  is perhaps the most widely used SER corpus. It contains 535 spoken utterances in German from 10 actors in seven emotions: anger, boredom, disgust, fear, joy, neutral, and sadness.\n\n2) eNTERFACE  [31]  contains 1,287 videos in English (42 test subjects, 14 nationalities), of which only the audio tracks were used in the present study. The corpus contains emotions in six categories: anger, disgust, fear, joy, sadness, and surprise.\n\n3) The Finnish Emotional Speech Corpus (FESC)  [32]  consists of 450 spoken passages from nine Finnish professional actors portraying five emotions: neutral, sadness, joy, anger, and tenderness. Based on long silences defined by an energy threshold  [26] , the passages were further split into 4,254 utterances.\n\n4) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [33]  is a multimodal database containing 7,356 recordings in English from 24 professional actors. Only the recordings including speech (1,440 utterances) were used in the present experiments, including eight emotions: neutral, calm, happy, sad, angry, fearful, surprise, and disgust.\n\nIn order to harmonize the emotional labels of each SER corpus, the labels were mapped into the quarters of the valencearousal plane following the mapping of  [34] , which has been popularly used in SER studies (e.g.  [35] [36] [37] ). The mapping also simplifies the SER classification task into two binary classification tasks: valence (positive/negative) and arousal (high/low).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cpc And Ae Model Training",
      "text": "A separate CPC model was trained for each SER corpus as a datadriven feature extractor. Log-mel frames (40 mel filters, 30-ms Hann window, 10-ms shifts) were used as input features for the encoder genc. To get constant-length inputs, 5-second segments were extracted from the acoustic signal by zero-padding utterances shorter than 5 s and randomly selecting 5-second segments of utterances longer than 5 s. The encoder genc consisted of three fully-connected ELU  [38]  layers of 256 units, each followed by a dropout of 20%. The autoregressive model gar was a one-layer GRU  [39]  with a 256-dimensional hidden state. Prediction was carried up to 12 steps (120 ms) ahead.\n\nEach corpus was randomly split into training and validation sets in a ratio of 80:20 utterances. The models were trained using the loss LCPC in Eq. 1, a batch size of 8 utterances, and Adam  [40]  optimizer. An initial learning rate of 10 -4 was used with a reduction factor of 0.7 based on the validation loss with a patience of 20 epochs. Early stopping with a patience of 100 based on validation loss was used to select the model with the lowest validation loss. For each sample in a minibatch, the rest of the minibatch samples acted as the negative samples.\n\nUsing a similar 80:20 split as above, an AE model was trained for dimensionality reduction individually for each corpus and for two input features (utterance-level log-mel and CPC features, see Sec. 3.3). The network consisted of six fullyconnected ELU layers of 512 units each, except for the third layer (32 units) and the last layer (600 and 256 units for logmel and CPC features, respectively). The 512-unit layers had a dropout of 10%. The AE models were trained using MSE loss, Adam optimizer, batch size of 1024, and a learning rate of 10 -4 . Early stopping with a patience of 300 based on validation loss was used, and the best encoder (the first three network layers) was selected from the AE model with the lowest validation loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Features",
      "text": "For log-mel features of each utterance, seven functionals (the first four moments, min, max, and range) were applied to the time dimension of log-mel frames (Sec. 3.2), and the first four moments were applied to the first and second order delta features to get a 600-dimensional utterance-level feature vector. For CPC  The features are ordered based on mean performance, with the best-performing feature being the rightmost.\n\nfeatures, the mean over the time dimension of the encoder outputs zt was computed to get 256-dimensional utterance features (ignoring potential zero padding). The context latent representations ct were also tested but resulted in slightly worse results than using zt, and hence are not separately reported. In addition to the mean, we also tested computing other functionals from the CPC features. However, since the mean is the only moment that preserves the metric properties of the original CPC feature space, the inclusion of these additional functionals turned out to systematically worsen the SER performance of the CPC features, and hence are also not reported. Fig.  3  illustrates the tested combinations of features and dimensionality reduction methods that were included in the experiments. For both the 600-dimensional log-mel and 256dimensional CPC features, 32-and 2-dimensional PCA and AE representations were computed. The AE representations were obtained using the encoder networks of Sec. 3.2. Additionally, 2-D t-SNE features were computed from the high-dimensional log-mel and CPC features as well as their 32-dimensional representations. The Scikit-learn  [41]  implementation of t-SNE was used with default values, with the exception of using PCA initialization for better overall stability. We also tested training CPC models directly into lower-dimensional feature spaces but with poor results. In order to save space, only the features that outperformed the random sampling baseline in AL-based classification are included in the present results. Computing 32-dimensional t-SNE features was not considered due to the poor scalability of the algorithm for higher than 2-dimensional projections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Al Simulation Setup For Ser",
      "text": "For each corpus and for each feature described in Sec. 3.3, SER experiments were carried out with the MAL algorithm using a simulated annotation procedure. In the simulation, samples selected for annotation by MAL were replaced by the groundtruth labels of the corpus as available based on earlier human annotations, and were then used to train a classifier for the valence and arousal classification tasks (see also  [26] ). The data was randomly split into training and test sets using 5-fold crossvalidation (CV), after which MAL was applied to the training set. Labeling budgets of 1%, 2%, 5%, 10%, 20%, 50%, and 100% of the total training set samples were used in the experiments. Although MAL was originally intended for scarce labeling budgets,  For the latter, the cases where only one dimensionality reduction method is being used are considered. Random sampling is shown as a baseline reference. Note that a SER classifier is always trained using 600-dim utterance-level log-mel features independently of the features and dimensionality used for AL.\n\nhigher labeling budgets are also included in the analyses. Each cross-fold experiment was repeated five times to account for the minor variability in the results due to t-SNE and MAL random initialization. We use Matthew's correlation coefficient (MCC)  [42]  as our primary evaluation argued by e.g.  [43] , MCC can be considered as one of the most informative quality measures for binary classifiers since it requires good results from all four confusion matrix categories to output a high score.\n\nAfter obtaining a set of labeled data from MAL, an SVM with an RBF kernel was trained with the labeled training data and tested on the test data. This was applied to both classification tasks and for both medoid and cluster labels. Since the primary focus of the present experiments was to find features that enhance the performance of clustering-based AL algorithms, the training and testing features for the SVMs were standardized for each experiment. For this, we selected the 600-dimensional utterancelevel log-mel feature statistics (z-score normalized at the corpus level), as they were found generally well-performing in valence and arousal classification tasks in  [26] . SVM hyperparameters (box constraint and kernel scale parameter) were optimized separately for each corpus and classification task based on a grid search using 5-fold CV on all labeled data for the given corpus. As a baseline reference, random sampling results with corresponding labeling budgets are also reported, with experiments conducted in a similar manner as with MAL.\n\nFor the sake of brevity, the results with MAL cluster labels are omitted since they provided an advantage over medoid labels only for 1% labeling budget. For the same reason, only the results from the best-performing distance metrics for MAL are reported, corresponding to the Euclidean distance for 2-D features, and the cosine distance for higher-dimensional features.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Fig.  4  presents the results of the experimental setup, averaged over the corpora and both classification tasks (valence and arousal). As the labeling budget increases, the differences between the tested features become smaller. Also, with a 50% labeling budget MAL does not provide a benefit over random sampling. For labeling budgets of 1%-20%, the 32-dimensional \"CPC PCA\" features had the highest average performance (approx. 0.38 MCC). For the four SER corpora, a labeling budget of 1% corresponds to approx. 15 samples on average. Fig.  5  (left) shows the performance comparison between CPC and log-mel features. For labeling budgets of 2%-10%, CPC features provided statistically significant improvements over log-mel features (paired t-test, p < 0.05, df = 999, t = 3.06-6.12 across the tests), showcasing better overall clusterability and/or more efficient feature space exploration for the CPC features in the SER task. Fig.  5  (right) shows the comparison between dimensionality reduction methods, demonstrating a similar performance for all three methods from a 5% labeling budget onward. With a 2% labeling budget, PCA and AE obtain a similar performance and outperform t-SNE, and with a 1% labeling budget, PCA outperforms the AE method which, in turn, outperforms t-SNE (paired t-test, p < 0.05, df = 399, t = 1.17-3.92 across the tests). Comparing different dimensionalities in Fig.  4 , we can observe that heavily reducing the dimensionality of the 600/256-dimensional features does not provide a major drop in average performance, with \"CPC PCA\" and \"CPC AE\" even outperforming the 256-dimensional CPC features. The 600/256-dimensional and 32-dimensional features obtained similar performance on average, and the 2-D t-SNE features also obtained similar performance for labeling budgets of 5% and larger. Overall, these results indicate that for clusteringbased AL approaches, both the global (600/256/32-dimensional features) and local (t-SNE features) properties of the data can be used to provide an advantage over random sampling. However, it seems that t-SNE works best if the labeling budget is not below 5%, possibly since t-SNE distorts the global distance metrics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this study, we combined CPC and various dimensionality reduction methods in search of functioning practices for clusteringbased AL. Our experiments revealed that SSRL can be utilized to improve clustering-based AL performance over traditional signal features. In addition, we found that both the local and global topologies of feature spaces can be successfully used for AL. Furthermore, we observed that compressing the dimensionality of high-dimensional features does not provide a major drop in AL performance. We also found that 2-D t-SNE features achieved similar AL performance as higher-dimensional features when the labeling budget is not very low. This finding could be utilized to reduce the computational complexity of AL algorithms, and to combine AL and data visualization to create interactive AL algorithms involving data exploration and visualization, in a similar manner as in the annotation platform of  [15] .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: for an example).",
      "page": 1
    },
    {
      "caption": "Figure 1: A toy example for 2-D data, clustered into two clusters",
      "page": 1
    },
    {
      "caption": "Figure 2: depicts a block diagram of the present experiments. First,",
      "page": 2
    },
    {
      "caption": "Figure 2: A block diagram of the present experimental setup.",
      "page": 2
    },
    {
      "caption": "Figure 3: A visualization of the feature, dimensionality reduc-",
      "page": 3
    },
    {
      "caption": "Figure 4: The MCC performance scores for different experi-",
      "page": 3
    },
    {
      "caption": "Figure 3: illustrates the tested combinations of features and",
      "page": 3
    },
    {
      "caption": "Figure 5: Comparison of CPC and log-mel features (left) and PCA, AE, and t-SNE dimensionality reduction methods (right) in MAL-",
      "page": 4
    },
    {
      "caption": "Figure 4: presents the results of the experimental setup, aver-",
      "page": 4
    },
    {
      "caption": "Figure 5: (left) shows the performance comparison between",
      "page": 4
    },
    {
      "caption": "Figure 5: (right) shows the compari-",
      "page": 4
    },
    {
      "caption": "Figure 4: , we can observe that heavily reducing the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2-dim\nInitial features\n32-dim\n2-dim": "PCA\nPCA\nt-SNE\nLog-mel \n(600-dim)\nAE\nAE\nt-SNE\nCPC \n(256-dim)\nt-SNE\nt-SNE\nOutperformed\nCPC \n        =\nrandom sampling\n(32-dim)\nRandom sampling-\n        =\nlevel performance\nCPC \n(2-dim)\n        =\nNot feasible"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Cheap and Fast -But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks",
      "authors": [
        "R Snow",
        "B O'connor",
        "D Jurafsky",
        "A Ng"
      ],
      "year": "2008",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "3",
      "title": "A Survey of Human-in-the-loop for Machine Learning",
      "authors": [
        "X Wu",
        "L Xiao",
        "Y Sun",
        "J Zhang",
        "T Ma",
        "L He"
      ],
      "year": "2021",
      "venue": "A Survey of Human-in-the-loop for Machine Learning",
      "arxiv": "arXiv:2108.00941"
    },
    {
      "citation_id": "4",
      "title": "From Theories to Queries: Active Learning in Practice",
      "authors": [
        "B Settles"
      ],
      "year": "2011",
      "venue": "Proc. PMLR"
    },
    {
      "citation_id": "5",
      "title": "Active Learning by Sparse Instance Tracking and Classifier Confidence in Acoustic Emotion Recognition",
      "authors": [
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2012",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "6",
      "title": "Active learning for dimensional speech emotion recognition",
      "authors": [
        "W Han",
        "H Li",
        "H Ruan",
        "L Ma",
        "J Sun",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "7",
      "title": "Active and Semi-Supervised Learning in ASR: Benefits on the Acoustic and Language Models",
      "authors": [
        "T Drugman",
        "J Pylkkönen",
        "R Kneser"
      ],
      "year": "2016",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "8",
      "title": "Learning for Sound Classification in Hybrid Learning Environments",
      "authors": [
        "W Han",
        "E Coutinho",
        "H Ruan",
        "H Li",
        "B Schuller",
        "X Yu",
        "X Zhu"
      ],
      "year": "2016",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "9",
      "title": "Adversarial Sampling for Active Learning",
      "authors": [
        "C Mayer",
        "R Timofte"
      ],
      "year": "2020",
      "venue": "Proc. WACV"
    },
    {
      "citation_id": "10",
      "title": "Inactive learning? difficulties employing active learning in practice",
      "authors": [
        "J Attenberg",
        "F Provost"
      ],
      "year": "2010",
      "venue": "ACM SIGKDD Explorations Newsletter"
    },
    {
      "citation_id": "11",
      "title": "Active Learning Using Preclustering",
      "authors": [
        "H Nguyen",
        "A Smeulders"
      ],
      "year": "2004",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "12",
      "title": "Active Learning with Clustering",
      "authors": [
        "Z Bodó",
        "Z Minier",
        "L Csató"
      ],
      "year": "2011",
      "venue": "Proc. PMLR"
    },
    {
      "citation_id": "13",
      "title": "Active Learning Methods for Low Resource End-To-End Speech Recognition",
      "authors": [
        "K Malhotra",
        "S Bansal",
        "S Ganapathy"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "14",
      "title": "Subspace Clustering with Active Learning",
      "authors": [
        "H Peng",
        "N Pavlidis"
      ],
      "year": "2019",
      "venue": "Proc. IEEE BigData"
    },
    {
      "citation_id": "15",
      "title": "Sparse Nonparametric Density Estimation in High Dimensions Using the Rodeo",
      "authors": [
        "H Liu",
        "J Lafferty",
        "L Wasserman"
      ],
      "year": "2007",
      "venue": "Proc. PMLR"
    },
    {
      "citation_id": "16",
      "title": "How to Annotate 100 Hours in 45 Minutes",
      "authors": [
        "P Fallgren",
        "Z Malisz",
        "J Edlund"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "17",
      "title": "Active learning for sound event classification by clustering unlabeled data",
      "authors": [
        "S Zhao",
        "T Heittola",
        "T Virtanen"
      ],
      "year": "2017",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "18",
      "title": "Representation Learning with Contrastive Predictive Coding",
      "authors": [
        "A Van Den Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation Learning with Contrastive Predictive Coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "19",
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "authors": [
        "J Shor",
        "A Jansen",
        "R Maor",
        "O Lang",
        "O Tuval",
        "F De Chaumont Quitry",
        "M Tagliasacchi",
        "I Shavitt",
        "D Emanuel",
        "Y Haviv"
      ],
      "year": "2020",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "20",
      "title": "General Framework for Self-supervised Learning in Speech, Vision and Language",
      "authors": [
        "A Baevski",
        "W Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "General Framework for Self-supervised Learning in Speech, Vision and Language",
      "arxiv": "arXiv:2202.03555"
    },
    {
      "citation_id": "21",
      "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding",
      "authors": [
        "O Henaff",
        "A Srinivas",
        "J Fauw",
        "A Razavi",
        "C Doersch",
        "S Eslami",
        "A Van Den Oord"
      ],
      "year": "2020",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "22",
      "title": "Analyzing Speaker Information in Self-Supervised Models to Improve Zero-Resource Speech Processing",
      "authors": [
        "B Van Niekerk",
        "L Nortje",
        "M Baas",
        "H Kamper"
      ],
      "venue": "Proc. INTERSPEECH, 2021"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised Neural-Based Graph Clustering for Variable-Length Speech Representation Discovery of Zero-Resource Languages",
      "authors": [
        "S Takahashi",
        "S Sakti",
        "S Nakamura"
      ],
      "year": "2021",
      "venue": "Proc. INTER-SPEECH"
    },
    {
      "citation_id": "24",
      "title": "Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021",
      "authors": [
        "T Maekaku",
        "X Chang",
        "Y Fujita",
        "L.-W Chen",
        "S Watanabe",
        "A Rudnicky"
      ],
      "venue": "Proc. INTERSPEECH, 2021"
    },
    {
      "citation_id": "25",
      "title": "Active Contrastive Learning of Audio-Visual Video Representations",
      "authors": [
        "S Ma",
        "Z Zeng",
        "D Mcduff",
        "Y Song"
      ],
      "year": "2021",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "26",
      "title": "Visualizing High-Dimensional Data Using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "JMLR"
    },
    {
      "citation_id": "27",
      "title": "Automatic Emotional Speech Analysis from Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit",
      "authors": [
        "E Vaaras"
      ],
      "year": "2021",
      "venue": "Automatic Emotional Speech Analysis from Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit"
    },
    {
      "citation_id": "28",
      "title": "A Simple and Fast Algorithm for K-Medoids Clustering",
      "authors": [
        "H.-S Park",
        "C.-H Jun"
      ],
      "year": "2009",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "29",
      "title": "Clustering to minimize the maximum intercluster distance",
      "authors": [
        "T Gonzalez"
      ],
      "year": "1985",
      "venue": "Theoretical Computer Science"
    },
    {
      "citation_id": "30",
      "title": "Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit",
      "authors": [
        "E Vaaras",
        "S Ahlqvist-Björkroth",
        "K Drossos",
        "O Räsänen"
      ],
      "venue": "Proc. INTERSPEECH, 2021"
    },
    {
      "citation_id": "31",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Speech Communication and Technology"
    },
    {
      "citation_id": "32",
      "title": "The eNTERFACE' 05 Audio-Visual Emotion Database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proc. ICDEW"
    },
    {
      "citation_id": "33",
      "title": "Emotions in Vowel Segments of Continuous Speech: Analysis of the Glottal Flow Using the Normalised Amplitude Quotient",
      "authors": [
        "M Airas",
        "P Alku"
      ],
      "year": "2006",
      "venue": "Phonetica"
    },
    {
      "citation_id": "34",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "35",
      "title": "Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wöllmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Using Multiple Databases for Training in Emotion Recognition: To Unite or to Vote?",
      "authors": [
        "B Schuller",
        "Z Zhang",
        "F Weninger",
        "G Rigoll"
      ],
      "year": "2011",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "37",
      "title": "Unsupervised Adversarial Domain Adaptation for Cross-Lingual Speech Emotion Recognition",
      "authors": [
        "S Latif",
        "J Qadir",
        "M Bilal"
      ],
      "year": "2019",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "38",
      "title": "Learning emotiondiscriminative and domain-invariant features for domain adaptation in speech emotion recognition",
      "authors": [
        "Q Mao",
        "G Xu",
        "W Xue",
        "J Gou",
        "Y Zhan"
      ],
      "year": "2017",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "39",
      "title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)",
      "authors": [
        "D.-A Clevert",
        "T Unterthiner",
        "S Hochreiter"
      ],
      "year": "2016",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "40",
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "41",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "42",
      "title": "Scikit-learn: Machine Learning in Python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort",
        "V Michel",
        "B Thirion",
        "O Grisel",
        "M Blondel",
        "P Prettenhofer",
        "R Weiss",
        "V Dubourg",
        "J Vanderplas",
        "A Passos",
        "D Cournapeau",
        "M Brucher",
        "M Perrot",
        "E Duchesnay"
      ],
      "year": "2011",
      "venue": "JMLR"
    },
    {
      "citation_id": "43",
      "title": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme",
      "authors": [
        "B Matthews"
      ],
      "year": "1975",
      "venue": "Biochimica et Biophysica Acta (BBA) -Protein Structure"
    },
    {
      "citation_id": "44",
      "title": "The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation",
      "authors": [
        "D Chicco",
        "G Jurman"
      ],
      "year": "2020",
      "venue": "BMC Genomics"
    }
  ]
}