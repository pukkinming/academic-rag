{
  "paper_id": "2404.04525v1",
  "title": "Iitk At Semeval-2024 Task 10: Who Is The Speaker? Improving Emotion Recognition And Flip Reasoning In Conversations Via Speaker Embeddings",
  "published": "2024-04-06T06:47:44Z",
  "authors": [
    "Shubham Patel",
    "Divyaksh Shukla",
    "Ashutosh Modi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents our approach for the SemEval-2024 Task 10: Emotion Discovery and Reasoning its Flip in Conversations. For the Emotion Recognition in Conversations (ERC) task, we utilize a masked-memory network along with speaker participation. We propose a transformer-based speaker-centric model for the Emotion Flip Reasoning (EFR) task. We also introduce Probable Trigger Zone, a region of the conversation that is more likely to contain the utterances causing the emotion to flip. For sub-task 3, the proposed approach achieves a 5.9 (F1 score) improvement over the task baseline. The ablation study results highlight the significance of various design choices in the proposed method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Conversations between participants carry information that evokes emotions. Emotions include personality, character, temper, and inspiration as the primary psychological parameters that drive them  (P S and G S, 2017) . Analyzing emotions through language helps uncover the interpersonal sentiments in a conversation at a finer level. This can help build better affective generative models  (Goswamy et al., 2020) , like chatbots that understand emotion and respond according to a person's behaviors and personality  (Kumar et al., 2021; Colombo et al., 2019) .\n\nThe  SemEval-2024 Task 10 (Kumar et al., 2024 ) aims at Emotion Recognition (ERC), sub-task 1, and Emotion Flip Reasoning (EFR), sub-tasks 2 and 3, in conversations for two languages, namely English and Hindi-English Code-Mixed. ERC refers to identifying the emotion of different utterances. EFR is about identifying those utterances in the dialogue that caused the emotion of a speaker to change.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "* Equal Contributions",
      "text": "We build upon the models presented in  Kumar et al. (2021)  for ERC and EFR. A speaker's personality is likely to influence the emotions developed in other participants  (Hazarika et al., 2018a) . This inspired us to include information regarding speaker participation to improve the analysis of the emotion of an utterance in conversations. Additionally, for Emotion Flip Reasoning, we propose the Probable Trigger Zone (PTZ), a region of the conversation more likely to consist of the utterance that caused an emotional change in the target participant. This helps us filter out significant nontrigger utterances, reducing the skew in the data. We utilize pre-trained models for computing text embeddings to obtain better representations of utterances.\n\nIn sub-task 1, we achieved a weighted F1 score of 45 and 9th rank. For sub-tasks 2 and 3, we secured 5th and 10th position with F1 scores of 56 and 60, respectively. The top scores for each sub-task were 78, 79, and 79, respectively. For sub-task 3, our model improves 5.9 F1 over the baseline model presented in  Kumar et al. (2021) . The proposed changes have assisted in improving the performance of the system. A limitation of our model is knowing speakers. It might not be possible in all circumstances that this information is available. Also, despite trying to reduce the skew in the data, our model's performance was still impacted. Our models and code can be found here. 1  2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Erc",
      "text": "The task of emotion prediction has been of active interest in recent years  (Witon et al., 2018; Kumar et al., 2020; Keswani et al., 2020; Singh et al., 2021b Singh et al., , 2023 Singh et al., , 2021a)) , including the development of models like ICON  (Hazarika et al., 2018a) , COG-MEN  (Joshi et al., 2022) , Instruct-ERC  (Lei et al., 2023)  and the models by  Kumar et al. (2021) . Also, there has been active research in affective text generation  (Goswamy et al., 2020) . Several datasets exist  (Bedi et al., 2023; Poria et al., 2019; Busso et al., 2008)  that use one or more additional emotions along with Ekman's scheme  (Ekman, 1992)  of emotion representation via six emotion classes, namely, fear, anger, joy, sad, disgust, and surprise.  Hazarika et al. (2018a)  and  Li et al. (2020)  highlight the importance of inter and intra-speaker interactions in a conversation.  Li et al. (2020)  achieves this by using three separate transformer-encoder blocks: (1) Conventional masking: masked multihead self-attention, (2) Intra-speaker masking: all utterances from other speakers are masked, and (3) Inter-speaker masking: all utterances from the current speaker are masked. While this captures relationships, it does not capture the speaker's personality or presence.  Hazarika et al. (2018a)  also considers speakers, but it was modeled on the IEMOCAP dataset  (Busso et al., 2008)  that contains only two participants.\n\nShapes of Emotion  (Bansal et al., 2022) , ICON  (Hazarika et al., 2018a)  and its derived model ERC_MMN  (Kumar et al., 2021)  proposed the concept of speaker-level outputs, which means that during conversational flow, there is a speaker-level GRU to encode the currently spoken utterance. They achieve this by storing vectors representing each speaker and updating them using the speakerlevel GRUs' hidden outputs, which are initialized to 0 during the start of a dialogue.\n\nCOGMEN  (Joshi et al., 2022)  introduces the concept of graphs to conversation flow for emotion recognition. They represent a graph in which each utterance is a node and is related to past or future utterances of the same or different speaker within a time window.  CORECT (Nguyen et al., 2023)  leverages on COGMEN and introduces speaker embeddings from MMGCN  (Wei et al., 2019)  to encode each speaker in the conversation for graphbased interaction and pairwise cross-modal feature interaction.  Kumar et al. (2021)  introduces the relatively new Emotion-Flip Reasoning (EFR) task, which aims to identify past utterances in a conversation that have triggered one's emotional state to flip at a certain time. The task of Emotion-Cause Pair Extraction (ECPE)  (Xia and Ding, 2019)  and Emotion Cause Extraction (ECE)  (Gui et al., 2016)  are similar to EFR, but they aim to extract the causes of emotions from a given text instead of conversations.  Kumar et al. (2021)  present a transformer-based model for EFR and also measure the performance of baseline models CMN  (Hazarika et al., 2018b) , ICON  (Hazarika et al., 2018a) , DGCN  (Ghosal et al., 2019) , AGHMN  (Jiao et al., 2019) , and Pointer Network  (Vinyals et al., 2017) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Efr",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Embeddings",
      "text": "The performance of models on tasks is influenced by the quality of text representation it uses  (Asudani et al., 2023) .  Nayak and Joshi (2022)  release HingBERT, a BERT model that has been fine-tuned on Hindi-English Code-Mixed corpus.  Muennighoff et al. (2023)  introduce the Massive Text Embedding Benchmark (MTEB), which evaluates the performance of text embeddings through different tasks across several datasets. One of the top performers, the voyage-embeddings 2  , utilize neural-net models to encode the text into text embeddings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task",
      "text": "SemEval-2024 Task 10: \"Emotion Discovery and its Reasoning it Flip in Conversations\"  (Kumar et al., 2024) , EDiReF, consisted of three sub-tasks:\n\n1. ERC in Hindi-English Code-Mixed.\n\n2. EFR in Hindi-English Code-Mixed.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Efr In English.",
      "text": "Emotion Recognition in Conversations (ERC) is classifying the utterances in a dialogue into one of the given emotion categories. An emotion flip is said to have occurred when a speaker's utterance differs from his/her previous utterance's emotion. Emotion Flip Reasoning (EFR) refers to identifying the utterances (triggers) that caused an emotional flip. These utterances could have been spoken either by the speaker himself or someone else. For the task of ERC, given the utterances in the conversation and corresponding speaker names, the emotion label for each utterance has to be predicted. For the task of EFR, the emotion labels of utterances have also been provided, and the triggers for a given emotion flip have to be predicted. Sub-Tasks 1 and 2 tailor the Hindi-English Code-Mixed dataset -MaSaC  (Bedi et al., 2023) . The training dataset consists of utterances in Roman script (e.g., \"yah plastic ke stickers tumne kahan se khariden?\"). Sub-Task 3 uses the MELD-FR dataset presented in  Kumar et al. (2021)  built upon the MELD dataset  (Poria et al., 2019) . Table  1  highlights the overall statistics regarding the training set for each sub-task. Figure  1a  and Figure  1b  show the distribution of the triggers as a function of the distance from the target utterance for the sub-task 2 and sub-task 3 training datasets, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Sub",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "System Overview",
      "text": "Inspired by the use of memory networks by  Hazarika et al. (2018a)  and  Kumar et al. (2021)  for emotion recognition, in 4.2 we present our model for the task of ERC, sub-task 1. Inspired by a transformer-based  (Vaswani et al., 2017)  approach for emotion flip reasoning presented by  Kumar et al. (2021) , in 4.3 we present our models for sub-tasks 2 and 3.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Utterance Embeddings",
      "text": "We utilize pre-trained models to compute representations of the utterances in the conversation. Sub-tasks 1 and 2 required the computation of utterance embeddings for code-mixed Hindi-English sentences. We utilized HingBERT to compute the utterance embedding as an average of all the token embeddings in an utterance. Sub-Task 3 consists of utterances in English. We referred to the Massive Text Embedding Benchmark to determine an efficient method to compute utterance embeddings. We experimented with the embeddings presented in  Li and Li (2023)  and voyage-embeddings, out of which the latter performed better. Hence, we used the voyage-lite-02-instruct model with query_type as a document.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Erc",
      "text": "We take inspiration from the Masked Memory Network architecture presented by  Kumar et al. (2021)    and speaker-specific GRUs proposed by  Kumar et al. (2021)  and  Hazarika et al. (2018a) . We used HingBERT to encode each utterance and then pass them through a dialog-level GRU and a speakerlevel GRU. The vectors from the global-level GRUs are passed through a memory network through multiple hops (a cycle of reading from memory and writing back to memory is called a hop. The output is taken from the final memory read operation.) Then, attention is computed between the memory and speaker-level outputs while masking future utterances and concatenating with speaker-level outputs to compute conversation-level outputs. Finally, the obtained features are passed through a trainable linear layer for predicting emotion class. Figure  2  shows the model architecture. Notations: u t denotes the embedded utterance at time t in a dialogue, while s (k) t denotes the k th speaker embedding for utterance u t . attention(q, k, v) is the attention operator applied on query q, key k and value v. r is the number of Speaker-level GRU sGRU : This gives a speakerlevel recurrent unit that takes inputs attention and speaker hidden state s (k) (taken from a dictionary of size k) and gives outputs so t . The hidden output replaces the dictionary entry for s (k) . At the start of a dialogue, the dictionary is empty, and the default hidden state for a new speaker is a zero vector. so t = sGRU (attention(do t , o (1:t-1) , do t ) + do t )\n\nMasked-Memory Attention: A memory vector, which represents the previous dialogues and utterances, is obtained by passing o (1:t) through a memory GRU (mGRU ). This then goes through masked attention with the so t while masking future utterances and a softmax activation α to give attention weights to each utterance in o (1:t-1) . This is then used to update the memory vectors via the mGRU and is concatenated with so t as an input to cGRU .\n\nConversation-level GRU cGRU : This layer represents the conversation flow of the dialogue and takes inputs as the concatenation of so t and masked attention output, to give conversation-level features. co t = cGRU (mem r + so t )\n\nFinally, the outputs of the cGRU are used to compute the emotion class.\n\nIn  Kumar et al. (2021) , the authors propose a transformer-based model for the task of EFR, whose architecture is as follows. Utterance embeddings for each utterance are computed using BERT  (Devlin et al., 2019) . The utterance embeddings of a conversation are passed through a transformer to compute a contextualized utterance embedding for each utterance. The emotion classes for each utterance are encoded as a one-hot vector and passed through a GRU to compute the emotion-history vector. For each utterance, its contextualized embedding, the contextualized embedding of the target utterance, and the emotion-history vector are passed through a linear layer to make a prediction.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Speaker-Aware Embeddings",
      "text": "As highlighted by  Li et al. (2020)  and  Hazarika et al. (2018a) , speaker interaction also drives the emotion of an utterance. Unlike their approach for modeling intra and inter-speaker interaction, we believe that the participation of certain speakers in the conversation drives the flip in the emotion of an utterance. Providing information regarding the speaker will help the model learn the nature of the specific speakers. To incorporate this, we utilize that the speakers in the test and the train set overlap.\n\nAn aspect of the conversation that has not been captured by the baseline model regarding the speakers of an utterance. In the baseline model, each utterance is treated as an independent text, and its embedding has been computed. This has failed to incorporate the information regarding who the speaker of a given utterance was. To incorporate this aspect, we concatenate the utterance embeddings with a one-hot vector denoting the speaker to create speaker-aware embeddings. This equips the model with the ability to capture the behavioral trends of specific speakers.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Probable Trigger Zone (Ptz)",
      "text": "We propose a hypothesis regarding the possible location of triggers. We divide the conversation into two parts. The first part consists of the utterances before the target speaker's previous utterance. The second part consists of utterances from the target speaker's previous utterance to his target utterance. We call the second part the Probable Trigger Zone (PTZ).\n\nWe hypothesize that no triggers lie in the first part of the conversation. Since the target speaker's emotion has flipped during the second part of the conversation, it is more likely that the causes for the emotion flip lie in the second part. Suppose the trigger causing the target emotion had been in the first part. In that case, it is more likely that it would have already caused the emotion of the previous utterance of the target speaker to flip. Then, the same emotion would have been carried to the target utterance, wrongly implying that no emotion flip occurred at the target utterance. To incorporate the hypothesis, we mask any predictions made by the model outside the Probable Trigger Zone. In section 5, we discuss how PTZ helps to reduce skew in the dataset.\n\nFor example, consider the conversation in Figure  3  utterance u 6 and his previous utterance u 4 . The probable trigger zone consists of utterances from u 4 to u 6 . Due to the \"surprise-causing\" statement u 5 in PTZ, Ross's emotion flips from Neutral to Surprise. If this \"surprise-causing\" statement had been present outside the PTZ, i.e., before the previous utterance u 4 , then the emotion of u 4 would likely have been Surprise.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion-Aware Embeddings",
      "text": "Using an Emotion-GRU, the baseline model computes an emotion-history vector from the emotion labels. It uses this emotion-history vector in the final linear layer to predict the utterance label. A possible shortcoming of the above is that the linear layer has access only to the emotion history rather than to the emotion labels of the individual utterances. Also, the transformer layer cannot access the emotion labels while computing the contextualized utterance embeddings. Providing those to the transformer will also allow the embeddings to be emotion-aware. We concatenate our speaker-aware utterance embeddings and one-hot emotion labels to incorporate the above.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Functioning",
      "text": "Figure  4  presents the model architecture used for the task of EFR. The target utterance is denoted by the subscript τ . Each utterance u t of a dialogue d is concatenated with its true emotion label e t and one-hot speaker embedding s t . This is then passed through the transformer to take into account the context. The Emotion-GRU computes the emotionhistory vector. For each utterance, its and the target utterance's contextualized representation and the emotion-history vector are passed through a linear layer to make the prediction.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setup Training Details",
      "text": "For sub-task 1, we chose a sequence length seq_len of 15, i.e., we break a long conversation into disjoint smaller conversations with utterances less than equal to seq_len. For sub-tasks 2 and 3, we use a window size w of 5. We consider only the last w utterances in a conversation to predict the trigger, i.e., U n-w+1 , U n-w+2 ...U n-1 , U n . Table  2  contains details of the hyperparameters we used to train the models. To limit the size of the vector denoting the speaker, keep retained information regarding the top k = 6 speakers. We chose the top 6 speakers since this covered nearly 80 -85% of utterances in the corpus. We used Adam optimizer (Kingma and Ba, 2017) for all the sub-tasks, with a weight decay of 1e-5. Training of models has been done using Kaggle 3 P100 GPUs.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Effect Of Hypothesis And Sequence Length",
      "text": "In Table  3  and Table  4 , we highlight the impact of the hypothesis and selection of sequence length on the datasets. On reducing the window size w to 5, a significant number of negative labels have been eliminated, while there has not been much Setting 1 and Setting 2 as defined in Section 5.2.\n\nimpact on the number of positive labels. Applying the hypothesis has helped mitigate the skew in the data, although there has been a slight impact on the number of positive labels. Setting 1 refers to considering only the utterances within the window size w = 5. Setting 2 refers to considering utterances that are both within the window and in the probable trigger zone. Setting 1 and Setting 2 as defined in Section 5.2.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "For Sub-Task 1, the dataset consisted of a nonuniform distribution of labels, with neutral being the most frequent. A model that predicts the emotion category of each utterance to be neutral achieves a weighted F1 of 24.36. We consider this as a simple neutral baseline for sub-task 1. For Sub-Task 2, we have kept the baseline as a rule-based model that predicts the previous utterance to be a trigger and the rest of all utterances non-triggers.\n\nThe data for the second sub-task is highly skewed as can be seen in Figure  1a . Due to this baseline performs exceptionally well, as can be observed in Table  5 . For Sub-Task 3, we use ERC True EFR-TX from  Kumar et al. (2021)  as the baseline. Table  5 : Baselines for various Sub-Tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Sub",
      "text": "Rule-Based: A rule-based model that predicts the previous utterance as a trigger and the rest as non-triggers.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Performance",
      "text": "We have highlighted the performance of our models on the test data in Table  6 . For sub-task 2, we get precision and recall scores of 0.73 and 0.83, respectively. For sub-task 3, we get precision and recall scores of 0.74 and 0.80, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Error Analysis",
      "text": "For sub-task 1 and sub-task 3, our model performed better than the baselines, but not for sub-task 2. For sub-task 1, the dataset consisted of a non-uniform distribution of labels in the training dataset. Due to this skew in the data, the model has shown different performances for different labels. The predictions for sub-task 1 have been highlighted in Figure  5 .\n\nFor EFR, the usage of a window size w = 5 utterances has helped to eliminate a large number of non-triggers. Due to this, the model's predictions have many true negatives, as can be seen in Table  7  and Table  8 . But despite this, there was still skew in the data, which impacted the model's performance in predicting the minority class of triggers. The data for sub-task 2 is highly skewed, as can be seen in Figure  1a . We suspect this is why our model has performed poorer than the baseline. Figure  6  is an example of an erroneous emotion labeling of the model on the test set of sub-task 1 (ERC). Here, the utterance marked in red has the true label as 'Fear,' but the model predicts 'Neutral.' This is due to the sharp change in conversation context at u 5 . Also, 'hahhaha' directly corresponds to laughing, but in this case, the speaker at u 6 utters 'hahhaha' as he is worried that the inspector is looking for 'Sharman'. The speaker, Indravardhan, who continuously interacted with the inspector, showed neutral emotion. This led to storing vectors corresponding to neutral for Indravardhan in the memory network, leading to misclassification of emotion at  u 6 .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "The application of the hypothesis has assisted in removing a few of the wrongly guessed triggers. This has improved the model's performance, as seen in Table  9 . We also experimented with another approach of making predictions only inside the PTZ instead of masking the outside labels. This was done by training in the model and making predictions only using the utterances within the probable target zone. In this case, the model's performance was poorer. We suspect this is because the context the model gets in the latter is more restricted than in the first case. Due to this, the model is not able to make predictions effectively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we discussed approaches to improve the masked memory network architecture for emotion recognition (ERC) and transformer-based architecture for emotion flip reasoning (EFR) by incorporating speaker information into the embeddings and making better use of the emotion labels. We also employed an approach of focusing the   model's prediction on more likely regions to identify triggers by defining the Probable Trigger Zone in conversations. This, along with considering a window of last-few utterances, assisted in reducing the bias in the data.\n\nLimitations: Our model assumes that the training and testing data consist of the same speakers. While this would be true for many benchmark datasets of emotion analysis in conversations, it might not be true in all real-world circumstances.\n\nAnother limitation of the proposed approach is the training time.\n\nFuture Work: In the future, we can include speaker information across dialogues for ERC to capture better speaker semantics by using learnable embeddings for each speaker updated by the hidden outputs of the speaker-level GRU. However, to apply the above, we need to know the number of speakers in the datasets, training, and testing. Additionally, the model becomes dependent on the number of speakers.\n\nA possible approach to help mitigate the assumption of having common speakers and knowing the number of speakers in the training and test time could be exploring further modeling inter and intraspeaker dependencies, as shown in  Li et al. (2020)  and  Hazarika et al. (2018a) . They propose models that capture speaker relationships but are not dependent on the number of speakers.\n\nMitigating the issues of skewed data can be further explored to enhance the system's performance. Also, addressing other aspects of conversations, such as whom the statement is being told to and treating names of speakers in utterances differently from simple pronouns, can be explored.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a and Figure 1b show",
      "page": 3
    },
    {
      "caption": "Figure 1: Distribution of the distance between the target",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the model architecture.",
      "page": 3
    },
    {
      "caption": "Figure 2: Masked Memory Network with Speaker-",
      "page": 4
    },
    {
      "caption": "Figure 3: Here, the target speaker is Ross with the target",
      "page": 5
    },
    {
      "caption": "Figure 3: Probable Trigger Zone.",
      "page": 5
    },
    {
      "caption": "Figure 4: presents the model architecture used for",
      "page": 5
    },
    {
      "caption": "Figure 4: Architecture of the model proposed for the",
      "page": 6
    },
    {
      "caption": "Figure 1: a. Due to this baseline",
      "page": 7
    },
    {
      "caption": "Figure 5: For EFR, the usage of a window size w = 5 ut-",
      "page": 7
    },
    {
      "caption": "Figure 1: a. We suspect this is why our model has",
      "page": 7
    },
    {
      "caption": "Figure 6: is an example of an erroneous emotion",
      "page": 7
    },
    {
      "caption": "Figure 5: Confusion Matrix for Sub-Task 1.",
      "page": 7
    },
    {
      "caption": "Figure 6: Example of an erroneous emotion labeling",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Hyperparameters for each of the sub-tasks.",
      "data": [
        {
          "Emotion\nGRU": "EFR-TX with Speaker Embedding\nand emotion labels\nTrigger Trigger Trigger\nTransformer Encoder"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: For sub-task 2, we",
      "data": [
        {
          "452": "150",
          "107": "172",
          "24": "4",
          "23": "3",
          "12": "6",
          "15": "7",
          "22": "6",
          "1": "1"
        },
        {
          "452": "67",
          "107": "13",
          "24": "37",
          "23": "6",
          "12": "8",
          "15": "7",
          "22": "2",
          "1": "2"
        },
        {
          "452": "66",
          "107": "22",
          "24": "8",
          "23": "43",
          "12": "7",
          "15": "5",
          "22": "4",
          "1": "0"
        },
        {
          "452": "74",
          "107": "11",
          "24": "11",
          "23": "4",
          "12": "10",
          "15": "4",
          "22": "8",
          "1": "0"
        },
        {
          "452": "39",
          "107": "16",
          "24": "4",
          "23": "5",
          "12": "2",
          "15": "14",
          "22": "0",
          "1": "2"
        },
        {
          "452": "28",
          "107": "6",
          "24": "1",
          "23": "0",
          "12": "2",
          "15": "2",
          "22": "18",
          "1": "0"
        },
        {
          "452": "6",
          "107": "2",
          "24": "5",
          "23": "0",
          "12": "0",
          "15": "2",
          "22": "1",
          "1": "1"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Impact of word embedding models on text analytics in deep learning environment: a review",
      "authors": [
        "Deepak Suresh Asudani",
        "Naresh Kumar Nagwani",
        "Pradeep Singh"
      ],
      "year": "2023",
      "venue": "Artif. Intell. Rev",
      "doi": "10.1007/s10462-023-10419-1"
    },
    {
      "citation_id": "2",
      "title": "Shapes of emotions: Multimodal emotion recognition in conversations via emotion shifts",
      "authors": [
        "Keshav Bansal",
        "Harsh Agarwal",
        "Abhinav Joshi",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the First Workshop on Performance and Interpretability Evaluations of Multimodal, Multipurpose, Massive-Scale Models"
    },
    {
      "citation_id": "3",
      "title": "Multi-modal sarcasm detection and humor classification in code-mixed conversations",
      "authors": [
        "Manjot Bedi",
        "Shivani Kumar",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2021.3083522"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resources &. Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "5",
      "title": "Affect-driven dialog generation",
      "authors": [
        "Pierre Colombo",
        "Wojciech Witon",
        "Ashutosh Modi",
        "James Kennedy",
        "Mubbasir Kapadia"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1374"
    },
    {
      "citation_id": "6",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "7",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "8",
      "title": "Di-alogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "9",
      "title": "Adapting a language model for controlled affective text generation",
      "authors": [
        "Tushar Goswamy",
        "Ishika Singh",
        "Ahsan Barkati",
        "Ashutosh Modi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.251"
    },
    {
      "citation_id": "10",
      "title": "Event-driven emotion cause extraction with corpus construction",
      "authors": [
        "Lin Gui",
        "Dongyin Wu",
        "Ruifeng Xu",
        "Qin Lu",
        "Yu Zhou"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D16-1170"
    },
    {
      "citation_id": "11",
      "title": "2018a. ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1280"
    },
    {
      "citation_id": "12",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N18-1193"
    },
    {
      "citation_id": "13",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2019",
      "venue": "Real-time emotion recognition via attention gated hierarchical memory network"
    },
    {
      "citation_id": "14",
      "title": "COGMEN: COntextualized GNN based multimodal emotion recognitioN",
      "authors": [
        "Abhinav Joshi",
        "Ashwani Bhat",
        "Ayush Jain",
        "Atin Singh",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2022.naacl-main.306"
    },
    {
      "citation_id": "15",
      "title": "IITK at SemEval-2020 task 8: Unimodal and bimodal sentiment analysis of Internet memes",
      "authors": [
        "Vishal Keswani",
        "Sakshi Singh",
        "Suryansh Agarwal",
        "Ashutosh Modi"
      ],
      "year": "2020",
      "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/2020.semeval-1.150"
    },
    {
      "citation_id": "16",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2017",
      "venue": "Adam: A method for stochastic optimization"
    },
    {
      "citation_id": "17",
      "title": "BAKSA at SemEval-2020 task 9: Bolstering CNN with self-attention for sentiment analysis of code mixed text",
      "authors": [
        "Ayush Kumar",
        "Harsh Agarwal",
        "Keshav Bansal",
        "Ashutosh Modi"
      ],
      "year": "2020",
      "venue": "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/2020.semeval-1.162"
    },
    {
      "citation_id": "18",
      "title": "Semeval 2024 -task 10: Emotion discovery and reasoning its flip in conversation (ediref)",
      "authors": [
        "Shivani Kumar",
        "Shad Md",
        "Erik Akhtar",
        "Tanmoy Cambria",
        "Chakraborty"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer",
      "authors": [
        "Shivani Kumar",
        "Anubhav Shrimal",
        "Md Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2021",
      "venue": "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer"
    },
    {
      "citation_id": "20",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "21",
      "title": "A hierarchical transformer with speaker modeling for emotion recognition in conversation",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Qingyi Si",
        "Weiping Wang"
      ],
      "year": "2020",
      "venue": "A hierarchical transformer with speaker modeling for emotion recognition in conversation"
    },
    {
      "citation_id": "22",
      "title": "Angle-optimized text embeddings",
      "authors": [
        "Xianming Li",
        "Jing Li"
      ],
      "year": "2023",
      "venue": "Angle-optimized text embeddings"
    },
    {
      "citation_id": "23",
      "title": "MTEB: Massive text embedding benchmark",
      "authors": [
        "Niklas Muennighoff",
        "Nouamane Tazi",
        "Loic Magne",
        "Nils Reimers"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.eacl-main.148"
    },
    {
      "citation_id": "24",
      "title": "L3Cube-HingCorpus and HingBERT: A code mixed Hindi-English dataset and BERT language models",
      "authors": [
        "Ravindra Nayak",
        "Raviraj Joshi"
      ],
      "year": "2022",
      "venue": "Proceedings of the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "25",
      "title": "Conversation understanding using relational temporal graph neural networks with auxiliary cross-modality interaction",
      "authors": [
        "Cam Van",
        "Thi Nguyen",
        "Tuan Mai",
        "Son The",
        "Dang Kieu",
        "Duc-Trong Le"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.937"
    },
    {
      "citation_id": "26",
      "title": "Emotion models: A review",
      "authors": [
        "Sreeja P S",
        "G S Mahalakshmi"
      ],
      "year": "2017",
      "venue": "International Journal of Control Theory and Applications"
    },
    {
      "citation_id": "27",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "28",
      "title": "2021a. An end-to-end network for emotion-cause pair extraction",
      "authors": [
        "Aaditya Singh",
        "Shreeshail Hingane",
        "Saim Wani",
        "Ashutosh Modi"
      ],
      "venue": "Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "29",
      "title": "2021b. Finegrained emotion prediction by modeling emotion definitions",
      "authors": [
        "G Singh",
        "D Brahma",
        "P Rai",
        "A Modi"
      ],
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII52823.2021.9597436"
    },
    {
      "citation_id": "30",
      "title": "Text-based fine-grained emotion prediction",
      "authors": [
        "Gargi Singh",
        "Dhanajit Brahma",
        "Piyush Rai",
        "Ashutosh Modi"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2023.3298405"
    },
    {
      "citation_id": "31",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "",
      "authors": [
        "Oriol Vinyals",
        "Meire Fortunato",
        "Navdeep Jaitly"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "33",
      "title": "Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia, MM '19",
      "doi": "10.1145/3343031.3351034"
    },
    {
      "citation_id": "34",
      "title": "Disney at IEST 2018: Predicting emotions using an ensemble",
      "authors": [
        "Wojciech Witon",
        "Pierre Colombo",
        "Ashutosh Modi",
        "Mubbasir Kapadia"
      ],
      "year": "2018",
      "venue": "Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
      "doi": "10.18653/v1/W18-6236"
    },
    {
      "citation_id": "35",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "Rui Xia",
        "Zixiang Ding"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1096"
    }
  ]
}