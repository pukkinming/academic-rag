{
  "paper_id": "2211.12103v1",
  "title": "Stiln: A Novel Spatial-Temporal Information Learning Network For Eeg-Based Emotion Recognition",
  "published": "2022-11-22T09:10:22Z",
  "authors": [
    "Yiheng Tang",
    "Yongxiong Wang",
    "Xiaoli Zhang",
    "Zhe Wang"
  ],
  "keywords": [
    "Emotion recognition",
    "Electroencephalogram (EEG)",
    "Spatial correlations",
    "Temporal contexts",
    "Attention mechanism"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The spatial correlations and the temporal contexts are indispensable in Electroencephalogram (EEG)-based emotion recognition. However, the learning of complex spatial correlations among several channels is a challenging problem. Besides, the temporal contexts learning is beneficial to emphasize the critical EEG frames because the subjects only reach the prospective emotion during part of stimuli. Hence, we propose a novel Spatial-Temporal Information Learning Network (STILN) to extract the discriminative features by capturing the spatial correlations and temporal contexts. Specifically, the generated 2D power topographic maps capture the dependencies among electrodes, and they are fed to the CNN-based spatial feature extraction network. Furthermore, Convolutional Block Attention Module (CBAM) recalibrates the weights of power topographic maps to emphasize the crucial brain regions and frequency bands. Meanwhile, Batch Normalizations (BNs) and Instance Normalizations (INs) are appropriately combined to relieve the individual differences. In the temporal contexts learning, we adopt the Bidirectional Long Short-Term Memory Network (Bi-LSTM) network to capture the dependencies among the EEG frames. To validate the effectiveness of the proposed method, subject-independent experiments are conducted on the public DEAP dataset. The proposed method has achieved the outstanding performance, and the accuracies of arousal and valence classification have reached 0.6831 and 0.6752 respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The research of brain and cognitive science aims to explore the linkage between human psychology and brain  [1]   [2] . EEG possesses the advantages of easy acquisition and accurate reflection of emotional state in physiological signals  [3] . The purpose of the EEG-based emotion recognition is to build harmonious humancomputer interaction and endow the system with the ability to distinguish and comprehend the emotions. Therefore, EEG-based emotion recognition becomes an important research direction in cognitive neuroscience and computer science.\n\nPsychologists establish discrete and dimensional models to quantify the emotional state. In the discrete emotion model, the emotional states can be divided into variety of discrete basic elements, such as happiness, fear, sadness, disgust, etc.  [4] . The Valence-arousal plane  [5]  proposed by J. Russel is an extensively used dimensional emotion model which maps emotions into a 2D space, as shown in Fig.  1(a) . Arousal represents emotional state of calm to excitement. Valence refers to positive or negative mental activity. Self-assessment Manikins (SAM) System  [6]  is an effective emotion annotation method and it could enhance the consistency of the ratings among different participants. The ratings of each dimension are distributed from 1 to 9. Based on the valence-arousal space and SAM System, the emotional state could be more precisely represented. channel attention weight learning module, and the internal spatial correlations of multi-channel EEG signals during continuous period time are extracted. Wang et al.  [17]  propose the spatial-temporal feature fusion network to extract classification features and integrate feature complementary relationships. The multi-layer perceptron (MLP) is applied to learn and extract temporal features, and spatial-temporal feature fusion by Bi-LSTM. Although the spatial-temporal fusion strategies  [15] -  [17]  adopt abundant information to improve the performance, it is inadequate learning for temporal contexts which is also discriminative to the emotional states.\n\nAs aforementioned, the following two issues can be identified: 1) The learning of complex spatial correlations among the electrodes have been inadequately studied, the essential electrodes and brain regions are necessary to be emphasized. 2) The EEG temporal contexts learning is effective way to capture the dependencies among the EEG frames and discriminate the emotions. Thus, we propose a spatial-temporal information learning network to extract discriminative features required for emotion recognition by capturing spatial correlations and temporal contexts robustly. The STILN model encompasses the following two elements:\n\n(1) Spatial Correlations Learning We use the frequency bands characteristics of windows in EEG and the position of electrodes to generate the 2D power topographic maps of PSD features  [17] . Different channels or different spaces of the feature maps contribute variously to the classification accuracy, and are not fully investigated. In the STILN model, CBAM recalibrates the weights of channels and space in the power topographic maps, the crucial brain regions and frequency bands are emphasized, and that is one of the differences in our works as compared with other models. BNs and INs are exploited to preserve the underlying features of EEG signal while mitigating the individual differences. The spatial correlations of the EEG are learned by the extraction network and the fusion network, and it greatly enhances the accuracy of emotion recognition.\n\n(2) Temporal Contexts Learning We adopt a more robustly temporal feature learning methods, the temporal contexts learning, rather than the spatial-temporal fusion strategy. Hence, Bi-LSTM is applied to bidirectional learning the temporal contexts of different EEG windows, and it captures the essential EEG frames. Then, the features of temporal and the downsampled spatial features are spliced to complete spatial-temporal information learning. In our methods, the EEG spatial correlations are effectively captured, while the temporal contexts are also learned.\n\nThe remainder of the paper is organized as follows. In section II, we describe the details of the DEAP database. In section III, we specify the details of the STILN for emotion recognition. In section IV, we conduct extensive experiments to validate the effectiveness of STILN. In section V, we discuss the performance of STILN, summarize the advantages and limitations of STILN, and compare the performance of STILN with related works. Finally, we conclude this work in section VI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset And Feature Extraction",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "The effectiveness of STILN is examine on publicly accessible datasets, specifically DEAP for emotion analysis using physiological signals. DEAP is the multimodal emotion classification dataset consisting of EEG and peripheral physiological signals. The multi-channel neurophysiological signals of 32 healthy subjects are collected and recorded according to the 10-20 international standard of 32 leads (Fig.  2 ). A selection of 40 1-minute-long music videos is applied to stimulate emotion. These videos are placed in 40 tracks. EEG and peripheral signals are recorded simultaneously when subjects watch a 1-minute-long music video. Trials recorded 63 seconds of EEG data from 32 electrodes at a sampling rate of 512 Hz (baseline 3 seconds, 60 seconds signal during stimulation). Subjects' emotional labels are subjectively assessed by a self-assessment model with valence and arousal dimensions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Eeg Preprocessing And Feature Extraction",
      "text": "The raw EEG is preprocessed to remove noise and improve the signal-to-noise ratio (SNR). The 32-channel raw signals are resampled at the sampling rate of 128 Hz, and band-pass-filtered in the range of 1-45 Hz to remove EMG artifact. The independent components analysis (ICA) is conducted on the EEG data to eliminate ocular artifacts. The DEAP dataset contains 32 √ó 40 (subjects √ó trials = 1280 samples, and the amount of data are far from enough data for deep learning network training. Therefore, a 6-second window (overlapping 3-second) is used to divide the data of each trail into several segments to increase sample sizes. In addition, arousal and valence are divided into two levels, defined as high/low arousal and high/low valence, respectively. Score higher than 5 is high grade, and score lower than 5 is low grade. Thereinto, we assume that when the arousal or valence score is 5, its emotional state cannot determine, and eliminate it. Finally, 17,252 samples for arousal classification and 17,347 samples for valence classification are obtained. Thus, the problem of emotion recognition is represented as two binary classification problems of arousal and valence.\n\nThe PSD features of EEG are extracted for spatial feature learning. For each 6S-long EEG segment, PSD features are extracted in five frequency bands: Delta-band (1-4 Hz), theta-band (4-8 Hz), alpha-band (8-12 Hz), beta-band (12-20 Hz), and gamma-band . The Cartesian coordinates of 32-channel are obtained by the EEGLAB toolbox  [15]  (As shown in Fig.  2 ), so that the power topography of the corresponding frequency bands can be obtained from the generated Cartesian coordinates. Electrode channels are composed into the 9√ó9 2D space matrices according to Cartesian coordinates, and the obtained sparse matrices is processed by biharmonic spline interpolation. To ensure learning enough edge features, the edge region of power topographic maps is zero-filled, and the 2D spatial matrices is mapped to 32√ó32 size. We take five different frequency bands as the channels of the power topographic maps. The generated power topographic maps of frequency bands are shown in Fig.  3 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Layout Mapping",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Novel Stiln For Eeg-Based Emotion Recognition",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overview Of Stiln",
      "text": "EEG among different subjects vary significantly. For sake of maximize the accuracy for emotion recognition, researchers usually set feature extractors for specific channels or perform recalibrated operations on channels  [19] -  [21] . Based on this, the proposed STILN model is the method for feature learning, and introduces spatial-temporal learning and attention mechanism into the network. Firstly, the EEG spectrum fragments are combined with the electrode position to form the power topographic maps. Then the training samples are sent to STILN for training, and the ADAM optimizer updates the corresponding network parameters. Finally, the trained model is utilized to classify the pre-processed test samples. The configuration of the STILN model is shown in Fig.  4 , and the parameters are shown in Table  1 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fc,Sigmoid",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Spatial Feature Extraction Network",
      "text": "According to the contribution of different electrodes and frequency bands to the classification, the corresponding weights are recalibrated by CBAM  [22]  to improve the classification accuracy. After the power topographic maps input the CNN-based spatial feature extraction network, CBAM continuously calculates the feature attention maps along the channel and spatial. Finally, the recalibrated power topographic maps are obtained by multiplying the elements of the attention maps and the input power maps. We assume that the power topographic maps of the input feature extraction network is ùêπ ‚àà ‚Ñù ùêª√óùëä√óùê∂ , H and W represent the size of the feature map, and C represents the number of feature map channels. Therefore, the recalibrating operation process of CBAM can be expressed as follows: In the above formula, ùúé(‚Ä¢) denotes the sigmoid function, MLP denotes the operation of multi-layer perceptron, ùê∫ ùê¥ùë£ùëî denotes average pooling operation, ùê∫ ùëÄùëéùë• denotes the operation of maximum pooling, ùê∫ ùê∂ùëÇùëÅùëâ denotes the convolution operation with convolution kernel of 7√ó7, concat denotes the splicing operation of tensors. One of the difficulties of emotion recognition is solving the inconsistency of feature distribution caused by individual differences in EEG. Instance normalizations (INs) are confirmed to be successful in transfer learning tasks  [23] . In EEG emotion recognition, INs independently normalize feature channels to alleviate the differences caused by various EEG between individuals. In parallel, Batch Normalizations (BNs) significantly improve the performance of classification tasks, and the combination of INs and BNs may achieve satisfactory results  [24] . Therefore, we design EEG feature extraction network combining INs and BNs. The specific network structure refers to Fig.  4  and Table  1 . The output of the feature extraction network is ùêπ ùëÜùêπùê∏ , ùêπ ùëÜùêπùê∏ ‚àà ‚Ñù ùêª√óùëä√óùê∂ .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Spatial Attention Module",
      "text": "The deep features of EEG have outstanding effects on emotion recognition. Inspired by the residual network model structure  [25] , a feature fusion block suitable for EEG is proposed, as shown in Fig.  4 . The input of feature maps and the output of convolution are addition performed at the element level, and the output of the feature fusion block is activated by the ReLU function. The formula operations for this layer are expressed as follows:\n\nwhere, ùêπ ùëÖùëíùë† represents the outputs of deep feature fusion block, ùêπ ùëÖùëíùë† ‚àà ‚Ñù ùêª√óùëä√óùê∂ . ùúé(‚Ä¢) is the ReLU activation function. CONV2D is convolution operation, where the convolution kernel size is 3√ó3 and the number of convolution kernels is 64. ‚®Å represents the addition operation of the corresponding channels.\n\nWe propose a strategy of twice recalibrating the feature map channel weights by the squeeze and excitation (SE) module  [26] . SE module recalibrates the weights of features in different channels through adaptive calibration of the interdependence of EEG. The adopted SE module includes two main operation operations: squeeze and excitation. The output of SE module is the proportional operation between the excitation scalar and the input network features mapping. Suppose ùë¢ ùëê ‚àà ‚Ñù ùêª√óùëä√óùê∂ is the feature mapping tensor of ùêπ ùëÖùëíùë† outputted by deep feature fusion layer, ùëà = [ùë¢ 1 , ùë¢ 2 , ‚Ä¶ , ùë¢ ùëê ]. The operation process of SE module is as follows:",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Se Module",
      "text": "Fig.  6  The SE Module structure. Note: The pool * denotes the global average pooling.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Temporal Contexts Learning Network",
      "text": "LSTM is a type of recurrent neural network (RNN), and remembers long sequences of input data  [27] . LSTM captures remote dependencies through gate control and storage units, but it only utilizes information from previous time steps. Bi-LSTM captures the temporal contexts of the sequence through bidirectional learning  [28] . Therefore, we utilize Bi-LSTM to learn temporal contexts.\n\nIn the spatial feature extraction network, we input the power topographic maps of 1-second EEG segment in trials (each sample is divided into 6 segments) into the network. And in temporal information learning, in order to learn the EEG features in temporal contexts, 6-separate spatial feature tensors are connected to obtain EEG time series containing spatial features. Assuming that ùêπ ùê∂ùê¥ùëá ‚àà ùëÖ ùêª√óùëä√óùê∂ represents the vector after the 6-seconds spatial feature map is connected, the operation process of Bi-LSTM is as follows:\n\nwhere, ùêø(‚Ä¢) represents the unit operation in Bi-LSTM, ùêø ùëôùëíùëìùë° (‚Ä¢) represents the operation in the left direction, ùêø ùëüùëñùëî‚Ñéùë° (‚Ä¢) represents the operation in the right direction, and ùêπ ùëöùë¢ùë°ùë¢ùëéùëô ‚àà ‚Ñù 2ùëë√óùëÅ represents the spatial features of EEG obtained by Bi-LSTM, where d is the number of hidden layers in Bi-LSTM. Finally, the down-sampled spatial features are linked with the temporal contexts to complete the co-learning of spatial-temporal information. The obtained spatial-temporal features are fed into the fully connected layer, and applied to arousal or valence prediction after the sigmoid function operation.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiments 4.1 Experimental Setups",
      "text": "The experiment is performed on the public DEAP dataset. The leave-one-out cross-validation (LOOCV) method is used to evaluate the classification performance of the STILN model. Specifically, the samples of one subject as the test dataset and the samples of the other 31 subjects as the training dataset, until samples of all 32 subjects are set to the test set once. The arousal and valence SAM scores 1-4 are divided into low grades and 6-9 are divided into high grades. The arousal samples are divided into 7144 low arousal samples and 10108 high arousal samples. The valence samples are divided into 6973 low valence samples and 10374 high valence samples. as shown in Table  2 .\n\nTable  2  The data labeling scheme and number of samples per emotion class Note: LA denotes low arousal; HA denotes high arousal; LV denotes low valence; HV denotes high valence. Configuration parameters are required for running the STILN model. ADAM is selected as the parameter optimizer. The learning rate is set to 0.0005, the batch size is set to 256. The cross-entropy loss selected as loss function, and the emotional labels of the samples are processed by one-hot coding. All networks in this work are implemented in Pytorch framework with NVIDIA GeForce RTX 2060 SUPER GPU. In addition, the accuracy (ùëÉ ùëéùëêùëê ) and F1 score (ùëÉ ùêπ1 ) are set for model evaluation, the distribution of test results is reflected by standard deviation. Note: Standard deviation in the brackets. The subject-independent experimental classification results of arousal and valence level are shown in Table  3 . The arousal and valence accuracy of the top ten subjects were 0.8132 and 0.7747, and the F1 scores are 0.7744 and 0.7776, respectively. The arousal and valence accuracy of all 32 subjects are 0.6831 and 0.6752, respectively, and the F1 scores are 0.6826 and 0.68, respectively. In addition, Fig.  7  shows the classification accuracy and F1 score histogram of 32 subjects. Experimental results indicate that the proposed EEG feature extraction network achieves outstanding performance in subject-independent arousal and valence classification. Since the parameters of the deep learning model have significant impact on its performance, we study the different model settings and select the parameters with the best performance. In this work, we hold that the number of hidden layers of Bi-LSTM and the learning rate of the optimizer have great effect on the performance. Therefore, the two parameters for different values are set to be tested. For the hidden layer of Bi-LSTM, five comparison parameters of 16,32,64,128,256 are set, and three parameters of 0.0001,0.0005 and 0.001 are set for the comparison of learning rate. In the parameter comparison experiment, except for the parameters to be compared, other network configurations are the same.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experimental Results",
      "text": "The effects of different numbers of hidden layers and different learning rates on the performance of accuracy are shown in Table  4  and 5 , respectively. According to the results of classification in Table  4 , when the number of hidden layers is 64, STILN achieves the highest average performance of accuracy (arousal is 0.6831, valence is 0.6752) and F1 score (arousal is 0.6826, valence is 0.68) in arousal and valence classification. According to the box plot of the accuracy and F1 score of each subject under different parameters in Fig.  8 , it can be analyzed that the performance index distribution of Bi-LSTM is more concentrate in the 64-layer hidden layer state, and the proposed emotion recognition network is also more robust. When the number of hidden layers is less, the features required for classification cannot be fully learned in the model, but the number of hidden layers is large, the model appears overfitting. According to Table  5  and Fig.  9 , the learning rate is set to 0.0005, the proposed model achieves the best performance, results indicate that too long or too short learning steps cannot effectively learn the necessary EEG features.  In this work, we compare STILN with classical deep neural networks, and test the improvement of performance for the proposed mothed compare to the common methods. Table  6  shows the performance of classification for different methods. Compared with CNN, LSTM, and DBN, the performance of accuracy for arousal in our proposed method is improved by 7.59 %, 9.92 %, and 8.49 % respectively, and the performance of accuracy for valence is improved by 6.09 %, 8.87 %, and 11.54 % respectively. Besides, we also reimplement the latest spatial and temporal EEG encoding networks, CNN-LSTM and DenseNet. Compared with these two networks, our method has 6.21 % and 4.94 % improvement in the accuracy of arousal, and 3.66 % and 2.95 % improvement in the accuracy of valence. Collectively, STILN achieves more stable performance on the DEAP standard dataset. The comparison results verify the effectiveness of the proposed STILN model in emotion recognition.   Table  7  shows the performance of arousal and valence for ablation experiment. The complete STILN (NET0) improves the accuracy of arousal by 0.74 % -3.49 % over other structures and F1 scores by 1.87 % -3.48 %. In the classification of valence, compared with other ablation experimental structures, the accuracy is increase by 0.56 % -2.99 % in NET0 and its F1 score increase by 1.52 % -4.44 %. Compared with other variant structures, the performance of complete STILN has been significantly improved. Ablation experiment verifies the important role of CBAM attention mechanism, INs, deep fusion structure, SE module, and Bi-LSTM in the proposed EEG emotion classification model.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Ablation Experiment",
      "text": "",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 4: The input of feature",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 5: Average accuracy and F1 score in EEG-based emotion recognition of the model with different learning",
      "data": [
        {
          "Mean\n Outliers": "Arousal Valence Arousal Valence Arousal Valence Arousal Valence Arousal Valence"
        },
        {
          "Mean\n Outliers": "16\n32\n64\n128\n256"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Average accuracy and F1 score in EEG-based emotion recognition of the model with different learning",
      "data": [
        {
          "Mean\n Outliers": "Arousal Valence Arousal Valence Arousal Valence Arousal Valence Arousal Valence"
        },
        {
          "Mean\n Outliers": "16\n32\n64\n128\n256"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Average accuracy and F1 score in EEG-based emotion recognition of the model with different learning",
      "data": [
        {
          "Mean\n Outliers": "Arousal\nValence\nArousal\nValence\nArousal\nValence"
        },
        {
          "Mean\n Outliers": "0.0001\n0.0005\n0.001"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Average accuracy and F1 score in EEG-based emotion recognition of the model with different learning",
      "data": [
        {
          "Mean\n Outliers": "Arousal\nValence\nArousal\nValence\nArousal\nValence"
        },
        {
          "Mean\n Outliers": "0.0001\n0.0005\n0.001"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "On Cognitive Dynamic Systems: Cognitive Neuroscience and Engineering Learning From Each Other",
      "authors": [
        "S Haykin",
        "J Fuster"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "2",
      "title": "Joint Action with Icub: A Successful Adaptation of a Paradigm of Cognitive Neuroscience in HRI",
      "authors": [
        "J P√©rez-Osorio",
        "D De Tommaso",
        "E Baykara",
        "A Wykowska"
      ],
      "year": "2018",
      "venue": "th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "3",
      "title": "EEG-Enabled Affective Applications",
      "authors": [
        "O Sourina",
        "Y Liu"
      ],
      "year": "2013",
      "venue": "Humaine Association Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "4",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "5",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "6",
      "title": "Measuring emotion: The self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "7",
      "title": "Cross-subjects Emotions Classification from EEG Signals using a Hierarchical LSTM based Classifier, 2019 E-Health and Bioengineering Conference (EHB)",
      "authors": [
        "B Badicu",
        "A Udrea"
      ],
      "year": "2019",
      "venue": "Cross-subjects Emotions Classification from EEG Signals using a Hierarchical LSTM based Classifier, 2019 E-Health and Bioengineering Conference (EHB)"
    },
    {
      "citation_id": "8",
      "title": "The Fusion of Electroencephalography and Facial Expression for Continuous Emotion Recognition",
      "authors": [
        "D Li",
        "Z Wang",
        "C Wang",
        "S Liu",
        "W Chi",
        "E Dong",
        "X Song",
        "Q Gao",
        "Y Song"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Learning CNN features from DE features for EEG-based emotion recognition",
      "authors": [
        "S Hwang",
        "K Hong",
        "G Son",
        "H Byun"
      ],
      "year": "2020",
      "venue": "Pattern Analysis & Applications"
    },
    {
      "citation_id": "10",
      "title": "ERNN: A Biologically Inspired Feedforward Neural Network to Discriminate Emotion from EEG Signal",
      "authors": [
        "R Khosrowabadi",
        "C Quek",
        "K Ang",
        "A Wahab"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "11",
      "title": "Real-time EEG-based happiness detection system",
      "authors": [
        "N Jatupaiboon",
        "S Pan-Ngum",
        "P Israsena"
      ],
      "year": "2013",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "12",
      "title": "Learning representa-tions from EEG with deep recurrent-convolutional neural networks",
      "authors": [
        "P Bashivan",
        "I Rish",
        "M Easin",
        "N Codella"
      ],
      "year": "2016",
      "venue": "4th International Conference on Learning Representations. (ICLR)"
    },
    {
      "citation_id": "13",
      "title": "Convolutional neural network approach for eeg-based emotion recognition using brain connectivity and its spatial information",
      "authors": [
        "S Moon",
        "S Jang",
        "J Lee"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Emotional processing in anterior cingulate and medial prefrontal cortex",
      "authors": [
        "A Etkin",
        "T Egner",
        "R Kalisch"
      ],
      "year": "2011",
      "venue": "Trends Cognitiveences"
    },
    {
      "citation_id": "15",
      "title": "A convolutional neural network feature fusion framework with ensemble learning for EEG-based emotion classification",
      "authors": [
        "K Guo",
        "H Mei",
        "X Xie",
        "X Xu"
      ],
      "year": "2019",
      "venue": "IEEE MTT-S International Microwave Symposium Digest"
    },
    {
      "citation_id": "16",
      "title": "3DCANN: A Spatio-Temporal Convolution Attention Neural Network for EEG Emotion Recognition",
      "authors": [
        "S Liu",
        "X Wang",
        "L Zhao",
        "B Li",
        "W Hu",
        "J Yu",
        "Y. -D Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "17",
      "title": "Spatial-Temporal Feature Fusion Neural Network for EEG-Based Emotion Recognition",
      "authors": [
        "Z Wang",
        "Y Wang",
        "J Zhang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "18",
      "title": "EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis",
      "authors": [
        "A Delorme",
        "S Makeig"
      ],
      "year": "2004",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "19",
      "title": "Learning Signal Representations for EEG Cross-Subject Channel Selection and Trial Classification",
      "authors": [
        "M Massi",
        "F Ieva"
      ],
      "year": "2021",
      "venue": "IEEE 31st International Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "E3GCAPS: Efficient EEG-based multi-capsule framework with dynamic attention for cross-subject cognitive state detection",
      "authors": [
        "Y Zhao",
        "G Dai",
        "X Fang",
        "Z Wu",
        "N Xia",
        "Y Jin",
        "H Zeng"
      ],
      "year": "2022",
      "venue": "E3GCAPS: Efficient EEG-based multi-capsule framework with dynamic attention for cross-subject cognitive state detection"
    },
    {
      "citation_id": "21",
      "title": "Cross-Subject EEG Emotion Recognition Using Domain Adaptive Few-Shot Learning Networks",
      "authors": [
        "R Ning",
        "C Philip Chen",
        "T Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "22",
      "title": "CBAM: Convolutional Block Attention Module",
      "authors": [
        "S Woo",
        "J Park",
        "J.-Y Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "Lecture Notes in Computer Science"
    },
    {
      "citation_id": "23",
      "title": "Instance normalization: The missing ingredient for fast stylization",
      "authors": [
        "D Ulyanov",
        "A Edaldi",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Instance normalization: The missing ingredient for fast stylization",
      "arxiv": "arXiv:1607.08022"
    },
    {
      "citation_id": "24",
      "title": "Improved Texture Networks: Maximizing Quality and Diversity in Feed-Forward Stylization and Texture Synthesis",
      "authors": [
        "D Ulyanov",
        "A Vedaldi",
        "V Lempitsky"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "Squeeze-and-Excitation Networks",
      "authors": [
        "J Hu",
        "L Shen",
        "S Albanie",
        "G Sun",
        "E Wu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Epileptic Seizure Detection Using EEG Signal Based LSTM Models",
      "authors": [
        "M Rabby",
        "R Eshun",
        "S Belkasim",
        "A Islam"
      ],
      "year": "2021",
      "venue": "IEEE Fourth International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)"
    },
    {
      "citation_id": "28",
      "title": "Using bidirectional lstm recurrent neural networks to learn high-level abstractions of sequential features for automated scoring of non-native spontaneous speech",
      "authors": [
        "Z Yu",
        "V Ramanarayanan",
        "D Suendermann-Oeft",
        "X Wang",
        "K Zechner",
        "L Chen",
        "J Tao",
        "A Ivanou",
        "Y Qian"
      ],
      "year": "2015",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition based on EEG using LSTM recurrent neural network",
      "authors": [
        "S Alhagry",
        "A Fahmy",
        "R El-Khoribi"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications (IJACSA)"
    },
    {
      "citation_id": "30",
      "title": "EEG-based emotion classification using deep belief networks",
      "authors": [
        "W. -L Zheng",
        "J.-Y Zhu",
        "Y Peng",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "31",
      "title": "A Deep Learning Method for Improving the Classification Accuracy of SSMVEP-Based BCI",
      "authors": [
        "Z Gao",
        "T Yuan",
        "X Zhou",
        "C Ma",
        "K Ma",
        "P Hui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems II: Express Briefs"
    },
    {
      "citation_id": "32",
      "title": "A Channel-Fused Dense Convolutional Network for EEG-Based Emotion Recognition",
      "authors": [
        "Z Gao",
        "X Wang",
        "Y Yang",
        "Y Li",
        "K Ma",
        "G Chen"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "33",
      "title": "A Hierarchical Bidirectional GRU Model with Attention for EEG-Based Emotion Classification",
      "authors": [
        "J Chen",
        "D Jiang",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "34",
      "title": "Cross-Subject emotion recognition from EEG using Convolutional Neural Networks",
      "authors": [
        "X Zhong",
        "Z Yin",
        "J Zhang"
      ],
      "year": "2020",
      "venue": "th Chinese Control Conference (CCC)"
    },
    {
      "citation_id": "35",
      "title": "Locally robust EEG feature selection for individual-independent emotion recognition",
      "authors": [
        "Z Yin",
        "L Liu",
        "J Chen",
        "B Zhao",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "36",
      "title": "Selecting transferrable neurophysiological features for inter-individual emotion recognition via a shared-subspace feature elimination approach",
      "authors": [
        "W Zhang",
        "Z Yin",
        "Z Sun",
        "Y Tian",
        "Y Wang"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "37",
      "title": "EEGFuseNet: Hybrid Unsupervised Deep Feature Characterization and Fusion for High-Dimensional EEG With an Application to Emotion Recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "38",
      "title": "Subject-independent Emotion recognition based on Entropy of EEG Signals, 2021 33rd Chinese Control and Decision Conference (CCDC)",
      "authors": [
        "H Yang",
        "P Rong",
        "G Sun"
      ],
      "year": "2021",
      "venue": "Subject-independent Emotion recognition based on Entropy of EEG Signals, 2021 33rd Chinese Control and Decision Conference (CCDC)"
    },
    {
      "citation_id": "39",
      "title": "Joint Temporal Convolutional Networks and Adversarial Discriminative Domain Adaptation for EEG-Based Cross-Subject Emotion Recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Transformers for EEG-Based Emotion Recognition: A Hierarchical Spatial Information Learning Model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    }
  ]
}