{
  "paper_id": "2107.13505v1",
  "title": "Deep Recurrent Semi-Supervised Eeg Representation Learning For Emotion Recognition",
  "published": "2021-07-28T17:21:30Z",
  "authors": [
    "Guangyi Zhang",
    "Ali Etemad"
  ],
  "keywords": [
    "Electroencephalography",
    "Semi-Supervised Learning",
    "Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based emotion recognition often requires sufficient labeled training samples to build an effective computational model. Labeling EEG data, on the other hand, is often expensive and time-consuming. To tackle this problem and reduce the need for output labels in the context of EEG-based emotion recognition, we propose a semi-supervised pipeline to jointly exploit both unlabeled and labeled data for learning EEG representations. Our semi-supervised framework consists of both unsupervised and supervised components. The unsupervised part maximizes the consistency between original and reconstructed input data using an autoencoder, while simultaneously the supervised part minimizes the cross-entropy between the input and output labels. We evaluate our framework using both a stacked autoencoder and an attention-based recurrent autoencoder. We test our framework on the large-scale SEED EEG dataset and compare our results with several other popular semi-supervised methods. Our semi-supervised framework with a deep attention-based recurrent autoencoder consistently outperforms the benchmark methods, even when small sub-sets (3%, 5% and 10%) of the output labels are available during training, achieving a new stateof-the-art semi-supervised performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human emotion is defined as a mental state related to the Central Nervous System (CNS)  [1] . It is caused by conscious or unconscious perception of emotion-related stimuli and often leads to changes in our physical and psychological processes  [2] . Emotions have important influence on rational mechanisms such as decision making, perception, and other cognitive processes  [3] . It is thus important to develop analytical or datadriven computational models for understanding, communicating, and responding to human emotions  [3] .\n\nMany non-invasive physiological signals have been employed for emotion recognition. Examples include muscle electrical activity with Electromyography  [4] , electrical signals from the heart through Electrocardiography  [5] , and measuring skin conductance via Galvanic Skin Response  [6] . Among such technologies, Electroencephalography (EEG) has the potential to demonstrate the highest fidelity due to the direct measurement of signals and information from the brain.\n\nEEG are non-stationary signals, often with high dimensionality. Moreover, EEG often contain various artefact and noise during the recordings, resulting in lower signal-tonoise ratio. Therefore, many supervised pipelines with deep learning techniques have been recently employed to learn the discriminative information from EEG time-series for various tasks such as hand movement classification  [7] , motor imagery classification  [8] , and driver vigilance estimation  [9] , achieving state-of-the-art results. These Fully Supervised Learning (FSL) pipelines highly rely on large amounts of labeled training samples to overcome challenges such as high dimensionality, mixing effect among recorded channels, and existence of noise and artifacts in the data. FSL method generally require enough labels for effective training. However, annotating and labeling EEG is often very hard and expensive, and requires the help of experts. Especially for the task of emotion recognition, EEG labeling may require both subject self-evaluation as well as objective evaluation (performed by experts)  [10] . Moreover, the performance of FSL methods often degrade when labeled samples are insufficient compared to when all the samples are labeled.\n\nIn this paper, to rely only on small amounts of labeled data, we propose a Semi-Supervised Learning (SSL) pipeline to recognize emotions using a deep recurrent autoencoder (AE). This AE is trained in an unsupervised fashion and its encoder component is followed by a simple two-layer classifier which is trained in a supervised manner simultaneously with the AE. Our approach can operate effectively by learning mainly from features extracted from unlabeled data while requiring only small subsets of the available labels for receiving the 978-1-6654-0019-0/21/$31.00 ©2021 IEEE arXiv:2107.13505v1 [cs.LG] 28 Jul 2021 necessary supervision. An overview of the training phase of our framework is depicted in Figure  1 . Training is performed jointly using the labeled and unlabeled data with the total loss incorporating both the supervised and unsupervised losses. We perform extensive experiments and show that our approach is able to provide better performance than other SSL benchmarks when small amounts of labeled data are used for training. In summary, our contributions are as follows.  (1)  We propose a semi-supervised approach for EEG-based emotion recognition to leverage large amounts of unlabeled and along with small amounts of labeled data.  (2)  We conduct extensive experiments by re-implementing high quality SSL pipelines, which are equipped with deep learning techniques.  (3)  We show the robustness of our SSL method by consistently achieving better results than the other methods when very few labeled samples are used (3%, 5% and 10%), achieving new state-of-the-art .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Emotion Recognition With Eeg",
      "text": "Classical Machine Learning. Feature extraction is a key step in EEG-based emotion recognition given the nonstationarity and non-linearity of EEG signals. As a result, extraction of discriminative EEG features from large-scale EEG data is essential prior to training classifiers. Most existing works extract features from five brain waves, notably delta, theta, alpha, beta, and gamma bands  [11] . The feature extraction methods include short-time Fourier transform, wavelet transform, power spectrum density, statistical, common spatial filter, and others  [11] . Recently, Differential Entropy (DE) has been used as a powerful feature extraction method suitable for many machine learning algorithms in emotion recognition  [10] .\n\nMachine learning classifiers play a fundamental role in feature-space learning for emotion recognition, and many pipelines have been built to map extracted features to emotion classes  [11] . Various algorithms have been used in conjunction with DE features. Canonical correlation analysis has been employed to model a linear relationship between DE features and emotional labels  [12] . Machine learning classifiers including Support Vector Machine (SVM), k-Nearest Neighbor, Logistic Regression, and Random Forest have been employed to explore non-linear relationships between DE feature vectors and affective labels  [10] ,  [13] .\n\nDeep Learning. In the context of EEG learning, deep learning techniques have been used to explore more powerful and task-relevant features than conventional machine learning algorithms. Deep Neural Network (DNN) has been used to improve the learning process using multiple hidden layers  [10] . A pipeline built with a Convolutional Neural Network (CNN) has been used to automatically extract features from raw EEG signals  [14] . Capsule network has been employed to learn spatio-temporal representations, achieving state-of-theart results for the task of vigilance estimation  [9] .\n\nRecurrent Neural Networks (RNN) have been successfully utilized for analysis of natural language sequences  [15] , image sequences  [16] , and bio-signals including EEG  [7]  to take advantage of their ability to learn short-term dependencies in sequential data. However, conventional RNNs face the vanishing/exploding gradient problems when sequences are long  [7] . An enhanced type of RNN capable of learning both long-and short-term dependencies, namely Long Short-Term Memory (LSTM) networks, have also been applied to EEG signals  [17] . Other than temporal information processing, deep RNNs have also been used to learn spatial dependencies of EEG electrodes on the scalp  [18] .\n\nVery recently, graph neural networks have been employed to discover topological information of EEG channels and further establish graph connections, achieving very promising results in an emotion recognition  [19] ,  [20] . To further explore topological structures, a Riemannian-based network was proposed to explore the spatial correlation information of EEG recordings that lie in the Riemannian manifold, demonstrating very strong performance in multiple EEG-based tasks including emotion classification  [8] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Semi-Supervised Learning",
      "text": "Unsupervised Pre-training. Unsupervised pre-training methods equipped with deep learning architectures have been successfully used in computer vision  [21] . In  [22] , it was argued that unsupervised pre-training acts as regularization which helps the model converge towards a local minimum during the main supervised training. Generally, unsupervised pre-training includes a Stacked AutoEncoder (SAE) which is used with available training data to update the model weights until minimum unsupervised training loss is reached. Next, the pre-trained encoder is followed by a classifier which is trained with labeled data in a supervised manner (also called fine-tuning)  [23] . Similarly, a pipeline consisting of a Deep belief Network (DBN) for unsupervised pre-training, followed by logistic regression for supervised fine-tuning has been used for EEG-based affective state recognition  [24] .\n\nPseudo Labeling. An alternative approach to the above is to develop pipelines that use labeled and unlabeled data simultaneously to train the network. In  [25] , a model is first trained on labeled data, and then used to generate pseudolabels for the unlabeled data. The model is then retrained on labeled data and pseudo-labeled data together. This approach outperformed several FSL methods with machine learning and deep learning backbones when small amounts of labeled data were available.\n\nΠ Model. The performance of the pseudo-labeling method mentioned above  [25]  has been reported unstable when used on several image datasets  [26]  when very few samples are labeled. The Π model was proposed to apply consistency regularization on both unlabeled and labeled data, leading the network to be more robust to noisy or perturbed inputs  [27] ,  [28] . To do so, the Π model first applies stochastic augmentation with additive noise on the input as well as dropout on the network. Next, it trains the network to obtain two different outputs for each input sample and minimizes the distance between two outputs. Thus, the network is encouraged to make similar prediction outputs given the same input with different random noise  [27] . This approach achieves better results than pseudo-labeling with very limited amount of labeled samples  [26] ,  [27] .\n\nTemporal Ensembling. The Π model trains slowly since the network trains twice for each input sample. Also, the network outputs are not stable due to the random noise applied on the inputs and the dropout in the network  [27] . To tackle these challenges, temporal ensembling was proposed to train the network only once for each input and aggregate the network output from previous epochs into an ensemble  [27] . Consequently the framework trains faster and the network output is more stable  [27] .\n\nMean Teacher. Temporal ensembling aggregates the previous model outputs for better model performance. However, it has considerable space requirements to save the previous models' outputs. Moreover, the ensemble outputs update slowly (only once per epoch), resulting in poor performance when the training data size is large  [29] . The mean teacher method was proposed to address these problems through using ensemble model weights instead of ensemble model outputs  [29] . Specifically, this framework consists of two networks with the same architecture, namely student and teacher. The student model plays the same role as in temporal ensembling, which is trained only once for each input  [29] . The teacher model is not trained, but instead uses the ensemble model weights from the student which updates them frequently (in each training step). Consequently, instead of minimizing the difference between the network output and the ensemble network output as in temporal ensembling, the mean teacher method minimizes the distance between the outputs of the student and the teacher networks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Our Approach",
      "text": "We aim to build a semi-supervised pipeline capable of using only small amounts of labeled data while mainly relying on unlabeled data for EEG representation learning. In the following sections, we first give a brief overview of feature space. Next, we introduce our proposed solution based on an Attention-based Recurrent AutoEncoder (Att. RAE).\n\nFeature Space. We perform the following pre-processing steps according to  [8] . First, EEG time-series were downsampled from 1 KHz to 200 Hz. Next, a bandpass filter was applied to minimize noise and artifacts outside the frequency range of [0.5 -70] Hz. Then, a notch filter was applied to reduce the power line effect around 50 Hz. Signal normalization was followed to ensure that the amplitude of recordings from different subjects fall into the same range of [-1, 1].\n\nFollowing pre-processing, we extract DE features from each consecutive 1-second windows with no overlap on each 8-second EEG segment. We assume the signals follow a Gaussian distribution, and therefore compute the DE feature using Eq. 1  [10] .\n\nWe extract DE features from five EEG bands for each of the 62 EEG channels, yielding a total of 310 features  [10] .\n\nDeep Recurrent AutoEncoder. We propose a recurrent autoencoder  [30]  consisting of an encoder which maps the input onto a latent space, followed by a decoder to reconstruct the original input using the latent representation. The encoder is made up of two layers of LSTM, followed by a soft attention mechanism  [7] ,  [8] , as illustrated in Figure  2 . The first LSTM layer employs 8 cells to process EEG features extracted from each window sequentially. Specifically, DE features (denoted as {x t } 8 t=1 ) from t th window are fed into the corresponding t th cell of the first LSTM layer, as shown in Eq. 2. The second LSTM layer inherits and further processes the hidden state outputs (h t enc1 ) from the previous LSTM layer and computes hidden state outputs in each cell (h t enc2 ), as shown in Eq. 3.\n\nTo obtain a well-performed output representation from the recurrent network, it is necessary to evaluate the importance of the output of each recurrent step  [7] . Therefore, we apply a soft attention mechanism following the last LSTM layer, enabling attention weights α t to be assigned to each LSTM cell's output h t enc2 as shown in Eq. 4 and 5, where W and b are the trainable weights and biases. Following, v is calculated using Eq. 6 as the output of the attention mechanism as the sum of the hidden outputs of all LSTM cells multiplied with the assigned trainable attention scores. More discriminative features can hence be obtained by optimizing the attention weights  [8] .\n\nIn the decoder, we employ two LSTM layers similar to the encoder. The first LSTM layer feeds the obtained latent representation v into each of the 8 LSTM cells, as shown in Eq. 7. Then, the last layer further processes the output hidden states h dec1 t and computes the output of each cell as shown in Eq. 8. We denote the reconstructed input from our recurrent autoencoder (Att. RAE) as xt .\n\nIn our LSTM settings, each layer contains 256 hidden units representing the size (dimension) of each cell's output vector.\n\nClassifier. As shown in Figure  2 , we also use a classifier which is fed the latent representation obtained by the autoencoder (after the encoder component) for the supervised learning part of our method. The classifier consists of two Fully Connected (FC) layers with 64 and 3 units, respectively, along with ReLU activations. A dropout rate of 0.5 is applied on the first FC layer.\n\nSemi-Supervised Algorithm. Going forward, we denote the entire training data as D, the labeled data as D l , and  the unlabeled data as D ul . Our loss function consists of two parts, namely unsupervised loss (L u ) and supervised loss (L s ). In the unsupervised component, we minimize the difference between the original and reconstructed input as obtained by the autoencoder for D ul and D l using MSE loss as in:\n\nwhere x, x are the input features of the labeled and unlabeled data and x. x are the corresponding reconstructed input features computed by the autoencoder for labeled and unlabeled data. B ul and B l are the batch sizes of D ul and D l .\n\nIn the supervised component, we employ cross-entropy loss L s for the labeled data only as in:\n\nwhere θ 1 , θ 2 are the parameters of the autoencoder and classifier respectively. Z l is the latent representation learned by the autoencoder with the input of the labeled data x. f θ2 is the classifier, and y are the labels for x.\n\nTo perform experiments when few samples are labeled, we randomly choose a very small portion of D as our D l D, and treat the remaining samples as our D ul . We then split D l and D ul equally into R batches, respectively, such that the ratio of B l /B ul remains the same in each batch. The losses are updated once per batch, as shown in Algorithm 1.\n\nDuring the joint training, we purpose to slowly increase the weight of the unsupervised loss term for accelerating model convergence  [27] . To do so, we apply the ramp-up function presented in Eq. 11 on the unsupervised loss where coefficient η(t) increases as the training epoch t increases  [27] .\n\nAlgorithm 1 Semi-Supervised Learning Require:\n\nx i , y i : i th data sample and label of D l 3: x i : i th data sample of D ul 4: R: total iterations for mini-batch training 5: f θ1 , h θ1 , g θ1 and f θ2 : encoder, attention, decoder and classifier 6: Z = f θ1 (x): latent representation learned by autoencoder Ensure:\n\nx ← g θ1 (Z ul ), x ← g θ1 (Z l )\n\n15:\n\nupdate L = ηL u + L s (Eq. 12) total loss 16:\n\nupdate θ 1 , θ 2 17:\n\nend for 18: end for Accordingly, the total loss is calculated as follows:",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments A. Dataset And Validation Scheme",
      "text": "We use the SEED dataset  [10]  to conduct emotion recognition experiments with three affective labels namely negative, neutral, and positive. The stimuli used to collect this dataset were 4-minute long emotional video clips. Each experiment run consists of 15 sessions. Each session contains a 5-second pre-stimuli notice followed by the session-specific visual stimuli. Each session ends with a self-assessment (45 seconds) and rest period (15 seconds). 15 subjects (8 female and 7 male) performed the experiments twice, yielding a total of 30 experiments. EEG were recorded from 62 electrodes on the scalp (with the international 10 -20 system) at a sampling frequency of 1 KHz  [10] . In each of the 30 experiments, we use the first 9 sessions for training and the remaining 6 sessions for testing, as pre-defined in  [10] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Implementation Details",
      "text": "We use the Adam optimizer and its default decay rates for model optimization. We apply gradient clipping with a range of [-1, 1] in order to avoid gradient explosion. The total training epochs are set to 30 and the learning rate is fixed at 0.001. All of the experiments are implemented using PyTorch  [31]  on a pair of NVIDIA Tesla P100 GPUs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Expansion To Other Autoencoders",
      "text": "In order to evaluate the generalizability of our method with other forms of autoencoders beyond the proposed recurrent attention-based model, we swap the recurrent autoencoder (plus attention) with an SAE. This variation contains an encoder consisting of 2 FC layers with 256 and 64 units with ReLU activation, followed by a decoder with the inverse architecture. The classifier component of the model that learns from the latent representations remains the same. The loss functions and hyperparameters also remain unchanged.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Comparison",
      "text": "We compare our framework with a number of other deep semi-supervised learning techniques  [22] ,  [23] ,  [25] -  [29] , which were explained in the Related Work section. As these methods have been generally evaluated in domains other than EEG, we re-implement all of them for comparison. The parameters of the models have been designed to achieve maximum performance. In the following sections, we describe the backbone networks used for these benchmarks along with the specifications for each approach.\n\nBackbone Networks. We employ two deep backbones (DNN and CNN) for the benchmark semi-supervised learning pipelines (except for unsupervised pre-training since it requires an autoencoder). The details of these backbones are as follows.\n\n(1) DNN: We use two stacked FC layers (256 and 64 hidden units) followed by the same classifier as the one used in our approach (in Figure  2 ) as the backbone DNN. (2) CNN: We employ two 1D convolutional layers as the backbone CNN. Specifically, the first convolutional layer has 1 input channel and 5 output channels, with kernel size of 3 and stride of 1. It is then followed by BatchNorm, LeakyReLU (ratio of 0.3), and MaxPooling layers. The second convolutional layer has 5 input channels and 10 output channels. The kernel size and stride size are kept the same as the first convolutional layer. This convolutional layer is also followed by BatchNorm and LeakyReLU layers to accelerate the training process. Following, a Flatten layer is applied before the classifier.\n\nUnsupervised Pre-training. In unsupervised pre-training, we pre-train an SAE as the network as in  [22]  (see the Related Work section). The encoder of the SAE consists of two stacked FC layers (256 and 64 hidden units) and the decoder has the inverse architecture as the encoder. For the DBN approach we employ the pipeline proposed in  [24] , which consists of 2 hidden layers (50 and 200 units) followed by a logistic regression layer.\n\nPseudo Labeling. Pseudo labeling is performed as in  [25]  and explained in the Related Work section. A cross-entropy loss is used for training.\n\nΠ Model. In the Π model, we apply a additive Gaussian noise (µ = 0, σ = 0.15) to the input. We do not apply additional dropout since the classifier already contains a dropout rate of 0.5. Cross-entropy and consistency losses are used for the supervised and unsupervised components respectively. The method is then carried out as in  [27]  (see the Related Work).\n\nTemporal Ensembling. Ensemble output (S) is zero initialized in the first epoch. In the remaining epochs, the output aggregation S is updated by S = δS +(1-δ)s, where δ = 0.6 is the momentum factor and s is the model output in the current epoch t. A bias correction operation is further applied on the ensemble output. Consequently, the ensemble output aggregates the model output in previous epochs and updates once per epoch. The remaining settings (e.g., model, input, loss function) are kept the same as in Π model  [27] .\n\nMean Teacher. The student and teacher models share the same architecture and initialized weights. The ensemble model weights are updated using exponential moving average as φ i = γφ i-1 + (1 -γ)φ i , where i, γ, φ, and φ denote the training step, smoothing coefficient, and the model weights for the student and teacher  [29] . We use γ = 0.95 in the experiments. The remaining settings are kept the same as in Π model and temporal ensembling  [27] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "Table  I  presents the performance of our method in comparison to a number of other SSL benchmarks when all the output labels are used D l = D. Here, the results for the pseudo labeling method  [25]  are not reported because in this part of our experiments, all the training data have their true labels. It can be observed that the unsupervised pre-training using the SAE performs poorly compared to other methods. When a DNN backbone is used, mean teacher obtains an accuracy of 0.9076 ± 0.0772, which is marginally better than the Π model and temporal ensembling. When CNN is used as backbone, the Π model achieves an accuracy of 0.8907 ± 0.0816 which slightly outperforms the others. For the Π model, temporal ensembling, and mean teacher methods, the performances are consistently better when DNN is used as a backbone. Our framework with the Att. RAE model achieves the highest accuracy of 0.9117 ± 0.0736.\n\nWe also perform experiments where only small subsets (3%, 5%, and 10%) of the ground-truth labels are used as Paper Method Accuracy Erhan et al.  [22]  Unsup. Pre-training SAE 0.7283 ± 0.1243 Xu and Plataniotis  [24]  Unsup. Pre-training DBN 0.7751 ± 0.0115 Laine and Aila  [27]  Π Model DNN 0.8962 ± 0.0775 Laine and Aila  [27]  Π Model CNN 0.8907 ± 0.0816 Laine and Aila  [27]  Temporal Ensembling DNN 0.9029 ± 0.0747 Laine and Aila  [27]  Temporal Ensembling CNN 0.8855 ± 0.0814 Tarvainen and Valpola  [29]",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D L",
      "text": "D, and compare the performance to other semisupervised learning methods. We ensure that the unlabeled data and labeled data are exactly the same for all the experiments. To show the robustness of our approach, we employ five different random seeds for choice of labeled samples (which are maintained identically throughout all the experiments). Moreover, the evaluation set is always untouched and kept the same for all the experiments. The performances are reported using the averaged results of the experiments repeated with the five different random seeds.\n\nOur experiments indicate that the unsupervised pre-training SAE benchmark obtains relatively low accuracies at 0.4679 ± 0.1146, 0.4712 ± 0.1246 and 0.5203 ± 0.1184 when 3%, 5% and 10% samples are labeled. This implies that the representation space obtained by the SAE during unsupervised pre-training lacks sufficient discriminative ability for emotion recognition when labeled data are scarce. Figure  3  shows the performance comparison when DNNs and CNNs are used as backbones for the remaining benchmarks (the unsupervised pre-training SAE is not shown to better highlight the differences among the other techniques). As shown in the figure, when only 3% of the samples are labeled and used with a DNN backbone, the Π model exhibits an accuracy of 0.6986 ± 0.1199, which is better than temporal ensembling (0.6734 ± 0.1104), mean teacher (0.6792 ± 0.0959), and pseudo labeling (0.6554 ± 0.1442). When 5% of samples are labeled, pseudo labeling and Π model achieve results of 0.7922 ± 0.1312 and 0.7999 ± 0.1115, which are better than temporal ensembling and mean teacher. When more samples are labeled (10%), pseudo labeling obtains an accuracy of 0.8479 ± 0.1123, outperforming others. When only 3% and 5% of the samples are labeled and a CNN is used as the model backbones, the Π model achieves accuracies of 0.7164±0.1011 and 0.7926±0.1064 with better performance than the other benchmarks. When more samples are labeled (10%), pseudo labeling obtains an accuracy of 0.8487 ± 0.0996, which is better than others benchmarks. Nonetheless, our framework with Att. RAE consistently achieves the best results (0.7317±0.1072, 0.8010±0.0938, and 0.8678 ± 0.0890) when 3%, 5%, and 10% of training samples are labeled, showing the robustness of our framework.\n\nInterestingly, when comparing the benchmark models together, we observe that a CNN backbone generally obtains better results for D l D ul (when 3%, 5%, and 10% of the data are labeled), which is in contrast to our finding when all the data were labeled. This may be due the fact that the non-linear features learned by CNNs from only a few labeled samples could be more representative of the entire feature space.\n\nFigure  4  shows the normalized confusion matrix obtained by our approach (Att. RAE), when trained with different amounts of labeled samples. Each row and column present the true labels and predicted labels for each emotion class, respectively. We observe that negative emotion class is generally more difficult to recognize compared to the other two emotion classes, especially when labeled data are extremely low (3% and 5%). Also, the samples of the negative emotion class are frequently misclassified as positive (0.63 and 0.41) when 3% and 5% amounts of training samples are labeled, respectively. In contrast, neutral emotion is much easier to classify compared to the others. Moreover, samples of neutral emotion are misclassified as positive emotion in a few cases, but never misclassified as negative emotion. Both negative and neutral class recognition rates consistently increase when more labeled training samples are available. Especially, the recognition rate of negative emotion increases rapidly. Meanwhile, the recognition rate of positive emotion interestingly stays relatively stable when the size of labeled samples changes.\n\nHere we aim to analyze the difference between our semisupervised framework vs. a fully supervised solution. To do so, we consider a simple multi-class SVM as a fully supervised method and our proposed framework with an SAE. We then visualize the decision boundaries for when different amounts of labels are using for training in each method. It should be noted that while our semi-supervised solution will leverage the unlabeled portion of the dataset (D ul ), the fully supervised SVM will not take advantage of D ul and only rely on D l . Figure  5  illustrates the decision boundaries obtained using each approach for when 3%, 5%, 10%, and 100% of a training set are labeled. In the figure, the labeled samples for 'negative', 'neutral', and 'positive' are depicted in blue, red, and green respectively, while the unlabeled data are shown in grey. The decision boundaries are specified by the corresponding background colors. In order obtain the decision boundaries in 2D for visualization purposes, we first apply principle component analysis to reduce the EEG features into two a dimensional feature space. Then, we feed the obtained latent features into the two models (our semi-supervised and the fully supervised SVM) to generate the decision boundaries as shown in the figure. Interestingly, we observe that as the amount of labeled samples varies from 3% to 100%, the decision boundaries of the fully supervised SVM vary substantially, indicating the naturally the performance is highly dependant on the distribution of labeled data (see Figures  5 (A ) through (D)). However, the decision boundaries of our semi-supervised framework are much more stable and exhibit considerably less reliance on the amount and distribution of labeled samples. Even when increasing the labels from 10% to 100% (Figure  5  (G) and (H)), the decision boundaries obtained by our model remain relatively consistent. This observation can be attributed to the fact that our method leverages the unlabeled data when small amounts of labeled samples are used, whereas the fully supervised method does not benefit from such information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose a novel semi-supervised approach for learning EEG representations with reduced dependence on labeled samples. Our framework, consisting of a deep attention-based recurrent autoencoder, leverages both large amounts of unlabeled data and few labeled samples for semi-supervised learning as it conducts both unsupervised and supervised learning simultaneously. The unsupervised component maximizes the consistency between original and reconstructed input features for the entire training data (both labeled and unlabeled). Meanwhile the supervised component minimizes the cross-entropy between the input and output labels for the labeled data only. We evaluate our framework on a popular EEG-based emotion recognition dataset and compare our approach with high quality deep semi-supervised benchmarks, demonstrating that it consistently outperforms other methods when few labeled samples are available (3%, 5% and 10%). The results show that our approach is capable of learning strong discriminative representations by jointly learning from small amounts of labeled samples and large amounts of unlabeled samples. Further analysis shows that that the decision boundaries obtained by our framework are more stable and less sensitive to limited labeled samples compared to a benchmark fully supervised method.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of the training phase of our semi-supervised EEG-based",
      "page": 1
    },
    {
      "caption": "Figure 1: Training is performed",
      "page": 2
    },
    {
      "caption": "Figure 2: The ﬁrst LSTM",
      "page": 3
    },
    {
      "caption": "Figure 2: , we also use a classiﬁer",
      "page": 3
    },
    {
      "caption": "Figure 2: The proposed semi-supervised model using an attention-based recurrent autoencoder.",
      "page": 4
    },
    {
      "caption": "Figure 2: ) as the backbone DNN. (2) CNN: We",
      "page": 5
    },
    {
      "caption": "Figure 3: The average test accuracies obtained by our solution in comparison",
      "page": 6
    },
    {
      "caption": "Figure 3: shows the",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion matrices for our method (Att. RAE) when different amounts",
      "page": 6
    },
    {
      "caption": "Figure 4: shows the normalized confusion matrix obtained",
      "page": 6
    },
    {
      "caption": "Figure 5: The decision boundaries of a benchmark fully supervised method (SVM) and our approach (SAE) is depicted when different amounts of training",
      "page": 7
    },
    {
      "caption": "Figure 5: illustrates the decision boundaries obtained using",
      "page": 7
    },
    {
      "caption": "Figure 5: (G) and (H)), the decision boundaries obtained by our model",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3% labeled\n0.75\nevitagenlartuen 0.24 0.13 0.63\n0.60\n0 0.79 0.21 0.45\n0.30\nevitisop 0 0.16 0.84 0.15\n0.00\nnegative neutral positive": "10% labeled\n1.0\nevitagenlartuen 0.83 0.074 0.093 0.8\n0.6\n0 1 0\n0.4\nevitisop 0.034 0.28 0.69 0.2\n0.0\nnegative neutral positive",
          "5% labeled\nevitagenlartuen 0.48 0.11 0.41 0.75\n0.60\n0 0.9 0.1 0.45\n0.30\nevitisop 0 0.22 0.78 0.15\n0.00\nnegative neutral positive": "100% labeled\n1.0\nevitagenlartuen 0.87 0 0.13 0.8\n0.6\n0 1 0\n0.4\nevitisop 0 0.16 0.84 0.2\n0.0\nnegative neutral positive"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.24": "0",
          "0.13": "0.79",
          "0.63": "0.21"
        },
        {
          "0.24": "0",
          "0.13": "0.16",
          "0.63": "0.84"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.48": "0",
          "0.11": "0.9",
          "0.41": "0.1"
        },
        {
          "0.48": "0",
          "0.11": "0.22",
          "0.41": "0.78"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.83": "0",
          "0.074": "1",
          "0.093": "0"
        },
        {
          "0.83": "0.034",
          "0.074": "0.28",
          "0.093": "0.69"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.87": "0",
          "0": "1",
          "0.13": "0"
        },
        {
          "0.87": "0",
          "0": "0.16",
          "0.13": "0.84"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The emotional brain",
      "authors": [
        "T Dalgleish"
      ],
      "year": "2004",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Affective Neuroscience: The Foundations of Human and Animal Emotions",
      "authors": [
        "J Panksepp"
      ],
      "year": "2004",
      "venue": "Affective Neuroscience: The Foundations of Human and Animal Emotions"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "R Picard",
        "Affective Computing"
      ],
      "year": "2000",
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "Physiological signals based human emotion recognition: a review",
      "authors": [
        "S Jerritta",
        "M Murugappan",
        "R Nagarajan",
        "K Wan"
      ],
      "year": "2011",
      "venue": "2011 IEEE 7th International Colloquium on Signal Processing and its Applications"
    },
    {
      "citation_id": "5",
      "title": "Classification of cognitive load and expertise for adaptive simulation using deep multitask learning",
      "authors": [
        "P Sarkar",
        "K Ross",
        "A Ruberto",
        "D Rodenburg",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "Classification of cognitive load and expertise for adaptive simulation using deep multitask learning",
      "arxiv": "arXiv:1908.00385"
    },
    {
      "citation_id": "6",
      "title": "Toward dynamically adaptive simulation: Multimodal classification of user expertise using wearable devices",
      "authors": [
        "K Ross",
        "P Sarkar",
        "D Rodenburg",
        "A Ruberto",
        "P Hungler",
        "A Szulewski",
        "D Howes",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "7",
      "title": "Classification of hand movements from eeg using a deep attention-based lstm network",
      "authors": [
        "G Zhang",
        "V Davoodnia",
        "A Sepas-Moghaddam",
        "Y Zhang",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "8",
      "title": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "arxiv": "arXiv:2008.08633"
    },
    {
      "citation_id": "9",
      "title": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "authors": [
        "G Zhang",
        "A Etemad"
      ],
      "year": "2019",
      "venue": "Capsule attention for multimodal eeg and eog spatiotemporal representation learning with application to driver vigilance estimation",
      "arxiv": "arXiv:1912.07812"
    },
    {
      "citation_id": "10",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "11",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "13",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2019",
      "venue": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "arxiv": "arXiv:1906.01704"
    },
    {
      "citation_id": "14",
      "title": "Using deep and convolutional neural networks for accurate emotion classification on deap dataset",
      "authors": [
        "S Tripathi",
        "S Acharya",
        "R Sharma",
        "S Mittal",
        "S Bhattacharya"
      ],
      "year": "2017",
      "venue": "Twenty-Ninth IAAI Conference"
    },
    {
      "citation_id": "15",
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "16",
      "title": "Long short-term memory with gate and state level fusion for light fieldbased face recognition",
      "authors": [
        "A Sepas-Moghaddam",
        "A Etemad",
        "F Pereira",
        "P Correia"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "17",
      "title": "Continuous vigilance estimation using lstm neural networks",
      "authors": [
        "N Zhang",
        "W.-L Zheng",
        "W Liu",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "18",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "19",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Pre-training strategies and datasets for facial representation learning",
      "authors": [
        "A Bulat",
        "S Cheng",
        "J Yang",
        "A Garbett",
        "E Sanchez",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "Pre-training strategies and datasets for facial representation learning",
      "arxiv": "arXiv:2103.16554"
    },
    {
      "citation_id": "22",
      "title": "Why does unsupervised pre-training help deep learning",
      "authors": [
        "D Erhan",
        "A Courville",
        "Y Bengio",
        "P Vincent"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings"
    },
    {
      "citation_id": "23",
      "title": "A survey on semi-supervised learning",
      "authors": [
        "J Van Engelen",
        "H Hoos"
      ],
      "year": "2020",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "24",
      "title": "Affective states classification using eeg and semi-supervised deep learning approaches",
      "authors": [
        "H Xu",
        "K Plataniotis"
      ],
      "year": "2016",
      "venue": "2016 IEEE 18th International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "25",
      "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
      "authors": [
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Workshop on challenges in representation learning"
    },
    {
      "citation_id": "26",
      "title": "Realistic evaluation of deep semi-supervised learning algorithms",
      "authors": [
        "A Oliver",
        "A Odena",
        "C Raffel",
        "E Cubuk",
        "I Goodfellow"
      ],
      "year": "2018",
      "venue": "Realistic evaluation of deep semi-supervised learning algorithms",
      "arxiv": "arXiv:1804.09170"
    },
    {
      "citation_id": "27",
      "title": "Temporal ensembling for semi-supervised learning",
      "authors": [
        "S Laine",
        "T Aila"
      ],
      "year": "2006",
      "venue": "Temporal ensembling for semi-supervised learning",
      "arxiv": "arXiv:1610.02242"
    },
    {
      "citation_id": "28",
      "title": "There are many consistent explanations of unlabeled data: Why you should average",
      "authors": [
        "B Athiwaratkun",
        "M Finzi",
        "P Izmailov",
        "A Wilson"
      ],
      "year": "2018",
      "venue": "There are many consistent explanations of unlabeled data: Why you should average",
      "arxiv": "arXiv:1806.05594"
    },
    {
      "citation_id": "29",
      "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "authors": [
        "A Tarvainen",
        "H Valpola"
      ],
      "year": "2006",
      "venue": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
      "arxiv": "arXiv:1703.01780"
    },
    {
      "citation_id": "30",
      "title": "Extraction and interpretation of deep autoencoderbased temporal features from wearables for forecasting personalized mood, health, and stress",
      "authors": [
        "B Li",
        "A Sano"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "31",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, high-performance deep learning library",
      "arxiv": "arXiv:1912.01703"
    }
  ]
}