{
  "paper_id": "2104.02804v1",
  "title": "Efficient Emotion Recognition Using Hyperdimensional Computing With Combinatorial Channel Encoding And Cellular Automata",
  "published": "2021-04-06T21:46:16Z",
  "authors": [
    "Alisha Menon",
    "Anirudh Natarajan",
    "Reva Agashe",
    "Daniel Sun",
    "Melvin Aristio",
    "Harrison Liew",
    "Yakun Sophia Shao",
    "Jan M. Rabaey"
  ],
  "keywords": [
    "Brain-inspired",
    "Hyperdimensional computing",
    "Emotion recognition",
    "Wearable",
    "Memory optimization",
    "Hardware efficient",
    "Multi-modal sensor fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, a hardware-optimized approach to emotion recognition based on the efficient brain-inspired hyperdimensional computing (HDC) paradigm is proposed. Emotion recognition provides valuable information for human-computer interactions, however the large number of input channels (>200) and modalities (>3) involved in emotion recognition are significantly expensive from a memory perspective. To address this, methods for memory reduction and optimization are proposed, including a novel approach that takes advantage of the combinatorial nature of the encoding process, and an elementary cellular automaton. HDC with early sensor fusion is implemented alongside the proposed techniques achieving two-class multi-modal classification accuracies of >76% for valence and >73% for arousal on the multi-modal AMIGOS and DEAP datasets, almost always better than state of the art. The required vector storage is seamlessly reduced by 98% and the frequency of vector requests by at least 1/5. The results demonstrate the potential of efficient hyperdimensional computing for low-power, multi-channeled emotion recognition tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing for informed human-computer interaction (HCI) is an area of growing research interest  [23] . Traditional interfaces such as keyboards and mouse are limited to conveying explicit information; the HCI experience can be enhanced through the inclusion and interpretation of additional implicit information  [24] . For example, context-dependent human behavioral patterns can be learned and used to inform feedback systems of user intention in a wide variety of applications such as driver warning systems, smart environments, automated tutoring systems, etc.\n\n[24],  [17] ,  [8] . Providing computers with emotional skills will allow them to intelligently react to subtle user context changes such as emotional state  [14] .\n\nThough a common approach is interpreting audiovisual signals such as facial expressions and voices, these may not be the primary source of expression. Emotion is not always easily observable, rather it re-quires inference through various behavioral observations and physiological indices which together can provide sufficient information  [4] . Many existing datasets collected for affective computing include various forms of physiological signals to create a comprehensive observation of emotional state  [5] ,  [11] . In the era of Internet-of-things (IoT), advances in wearable devices make the inclusion of various sensing modalities in intelligent HCI applications increasingly feasible  [3] .\n\nA representation of emotion used for affective computing is the arousal-valence plane  [15] . This model describes discrete emotional states on this space in terms of varying levels of arousal or emotional intensity, and of valence or polarity of emotion (positive or negative). For emotion recognition tasks, these are reduced to high and low arousal and valence values which can, in combination, be used to define the nature of the emotion.\n\nThe emotion recognition system must also be able to address the challenge of multimodal classification which results from the inclusion of diverse physiological sensors  [22] . For this work, the AMIGOS and DEAP datasets were selected specifically for the large number of sensor channels and modalities. The AMIGOS dataset contains electroencephalogram (EEG), galvanic skin response (GSR) and electrocardiogram (ECG) sensors  [5] . The DEAP dataset includes EEG, Electrooculography (EOG), Electromyography (EMG), GSR, blood volume pressure (BVP), temperature and respiration sensors  [11] .\n\nPrevious work on multi-modal fusion for the AMI-GOS dataset includes Fisher's linear discriminant with Gaussian Naive Bayes, which was shown to achieve F1 scores of 57% and 58.5% on valence and arousal  [5] ,  [3] . Wang et al. implemented recursive feature elimination (RFE) with a support vector machine (SVM) and obtained 68% and 66.3% accuracy on valence and arousal  [22] . Wang et al. also implemented Extreme Gradient Boosting (XGBoost) for accuracies of 80.1% and 68.4% on valence and arousal. Siddharth et al. used extreme learning machines (ELM) for accuracies of 83.9% and 82.7% on valence and arousal  [19] . Previous binary classification multimodal fusion approaches for the DEAP dataset include a restricted boltzmann machine (RBM) with an SVM classifier, with accuracies of 60.7% and 64.6% for valence and arousal, respectively  [18] . Wang et al. used a deep belief network (DBN) through multi-layer RBMs for valence and arousal accuracies of 51.2% and 68.4%  [21] . Yin et al. used a multiple-fusion-layer based ensemble classifier of stacked autoencoder (MESAE) for accuracies of 76.2% and 77.2% for valence and arousal  [23] .\n\nSince emotion recognition can provide valuable information for HCI, a hardware-efficient platform that allows for extended-use, on-board classification, would increase the feasibility of long-term wearable monitoring and thus increase the scope of potential feedback applications. While previous work shows strong results for the AMIGOS and DEAP datasets in terms of classification accuracy, the ease of hardware implementation for training and inference are not considered while designing the models; these methods have high computational complexity that reduces implementation feasibility on resource-limited wearable platforms. SVMs, for example, while demonstrating high accuracy, are challenging to implement efficiently on hardware, and demonstrate a trade-off between precise accuracy and meeting hardware constraints  [1] ,  [13] . In addition, multimodal fusion approaches require parallel encoding schemes prior to the fusion point which further increase the complexity creating a bottleneck for real-time wearable classification.\n\nTo address this, in this work Brain-inspired Hyperdimensional Computing (HDC) is used for emotion recognition. HDC is an area of active research that has been successfully demonstrated for classifying physiological signals such as the wearable EMG classification system implemented from Moin et al. that achieves 97.12% accuracy in recognizing 13 different hand gestures  [12] , the iEEG seizure detection system developed by Burrello et al.  [2] , and the EEG error-related potentials classification for brain-computer interfaces implemented by Rahimi et al.  [16] . It is based on the idea that human brains do not perform inference tasks using scalar arithmetic, but rather manipulate large patterns of neural activity. These patterns of information are encoded in binary hypervectors, with dimensions ranging in the thousands to ensure that any two random HVs are likely to be nearly orthogonal to each other  [7] . There are three operations that are performed on these hypervectors: bundling, binding, and permutation. Bundling is a component-wise add operation across input vectors, binding is a componentwise XOR operation, and permutation is a 1-bit cyclical shift. The simplicity of these operations suggests that HDC is very hardware efficient, as confirmed in previous work  [6] ,  [13] . Montagna et al. demonstrated that HDC computing achieved 2x faster execution and lower power at iso-accuracy on an ARM Cortex M4 compared to an optimized SVM  [13] .\n\nHDC has additional properties that demonstrate its potential for a wearable emotion recognition system. Previous work by Chang et al. developed a baseline, unoptimized architecture for emotion classification on the AMIGOS dataset, which was able to achieve valence and arousal accuracies of 83.2% and 70.1%, respectively, demonstrating higher performance than SVM, XGB and gaussian naive bayes for all amounts of training data  [3] . HDC encodes information in the same form no matter the type, number or complexity of the inputs. This is accomplished through basic vectors (items) that are random, and typically stored in an item memory (a codebook). Each channel is assigned a unique item memory vector, and feature values are typically encoded through a discretized mapping to additional unique hypervectors representing values within a set range. Each stream of information is encoded into this representation as shown in Figure  1 , which lends HDC well to sensor fusion.\n\nHDC inherently binds features extracted from various physiological data streams. This suggests early fusion with reduction of parallel encoding schemes will have little effect on its accuracy, breaking the complexity-accuracy tradeoff. HDC offers a reduction of computation and memory requirements in contrast to traditional machine learning models, demonstrated by Montagna et al.  [13] . It also offers the ability to use the same hardware for training & inference, rapid training, and robustness to noise/variations in input data making it a viable choice for wearable, hardwareconstrained sensor fusion applications.\n\nFigure  1  The sensor fusion datapath from electrodes to a fused hypervector for the three-modality emotion recognition system used in AMIGOS with GSR, ECG, and EEG sensor inputs. The sensor inputs are pre-processed into a set of features which are then mapped into the HD space to create a set of spatial encoder (SE) inputs. These vectors are encoded within each modality, and then finally fused together to create one vector representing information from all of the channels. The process is detailed in Section 2 Datta et al. synthesized an implementation of a generic HDC application-specific integrated circuit (ASIC) processor that provided a power breakdown between the various blocks involved  [6] . The item memory, which stores channel identification vectors, contributed the most, 42%, to the overall power of the processor. For the emotion recognition-specific application, the large number of channels (>200) and modalities (>3) requires advance storage of a correspondingly large number of unique vectors in the item memory. More channels translates into more memory. This would result in memory storage consuming ∼50% of the overall processor power similar to  [6] . Reduction of memory usage would allow HDC to meet stricter power/complexity constraints, improving its potential for implementation on wearable platforms.\n\nIn this work, use of pseudo-random vector generation through computation using a cellular automata is proposed and implemented for this purpose. A cellular automata consists of a grid of cells which evolve with complex, random behavior over time through a set of discrete computations using the current state and that of nearby cells  [9] . Cellular automata rule 90 assigns the next state in a method shown to be equivalent to an XOR of the two nearest cells  [9] . For a hypervector, each cell represents a single bit and rule 90 can be implemented through XOR of the cyclical left-shift and cyclical right-shift of the original vector. If HV n is the hypervector state at step n, and ρ is the cyclical shift operation (+1 for right, -1 for left), then HV n+1 can be generated by:\n\nThese operations are vectorizable and computationally minimal. The emotion recognition architecture uses a fixed sequential channel (item) access pattern, therefore, this technique, with which the item memory vectors are sequentially-evolving, can be used. Cellular automata grid sizes over 24 have been shown to generate new degrees of freedom for more than 10 3 steps before saturating  [10] . Hypervectors, with tens of thousands of cells in the grid, provide linearly longer randomization periods; this is sufficient for most applications including emotion recognition. Using a single random seed vector, full-dimension random item memory hypervectors can be generated during the encoding process instead of being precomputed and stored. With this approach, the memory is constant regardless of the number of channels.\n\nIn this work, an HDC architecture is designed specifically targeting sensor fusion using an early fusion approach. This reduces the parallel encoding paths previously used for HDC sensor fusion to a single one by taking advantage of HDC's inherent projection of features into large-capacity hypervector representations. Methods for memory reduction and optimization are proposed and implemented, including a novel approach that takes advantage of the combinatorial nature of the HDC encoding process, and the usage of an elementary cellular automata with rule 90 together to reduce vector storage and request frequency. Finally, hypervector dimension reduction is further explored as a method of comprehensive reduction. Results are reported for both the DEAP and AMIGOS datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Hdc Early Fusion Architecture",
      "text": "The HDC physiological architecture includes four main blocks: the map into the hyperdimensional space (HDS), the spatial encoder, the temporal encoder, and the associative memory as shown in Figure  2 . The first block maps incoming data into HDS using an item memory or a generator. HDC depends on the pseudo-orthogonality of random vectors to be able to distinguish between various classes; a random vector will be nearly orthogonal to another random vector in the hyperdimensional space. Random vectors are used for the channel item memory vectors so that the source channel of a feature value can be included as information in the encoding process. These are stored in an item memory (iM).\n\nIn order to encode feature values, in this implementation, additional feature projection vectors are randomly generated for each channel and stored as well. In traditional architectures, the feature projection vector {-1, 0, 1} is multiplied by the feature value and then binarized by reducing the positive values in the vector to 1s, and the zeros and negative values to 0s. This process can be simplified to multiplexers selecting between a pre-generated random negative or pos-itive binary feature projection vector depending on the feature value's sign to eliminate computationallyexpensive multipliers. This allows the feature projection vectors to maintain pseudo-orthogonality but have the same sparsity as the item memory vectors, making them interchangeable. As a result, the feature projection vectors can also be stored in the item memory instead of separately.\n\nIn the spatial encoder, the binding operation (XOR) is utilized to generate a spatially encoded hypervector for each channel. If iM i represents the item memory vector for channel i and FP i,j represents the feature projection vector selected for channel i for sample j, then the spatially encoded hypervector for sample j SE i,j is computed as:\n\nTo develop a complete hypervector, the bundling operation (vertical majority count across vectors) combines the many spatially encoded hypervectors within a sensor modality. If the sensor modality m has k channels and the bundling operation is represented as +, SE m,j is computed as:\n\nBecause emotion recognition involves various sensor modalities, it requires fusion. Previous sensor fusion implementations fused after the temporal encoder, but in this work, an early fusion approach is taken, which fuses the modalities directly after the spatial encoding process. Therefore, this architecture requires only a single temporal encoder as opposed to one per modality, as shown in Figure  3 . This reduces the parallel encoding paths while still allowing each modality to be weighted equally instead of by number of features. If there are m sensor modalities, the fused spatially encoded hypervector for sample j is:\n\nHDC also has the ability to encode temporal changes through the use of n-grams based on a sequence of N samples. This is invaluable for physiological signals that are time-varying as it allows for the capturing of time-dependent emotional fluctuations within the same class or between segments of the same class. The permutation operation (cyclical shift, represented as ρ) is used to keep track of previous samples. Hypervectors coming from the spatial encoder are permuted and then bound with the next hypervector N times in the temporal encoder. This results in an output that observes changes over time, TE j , that can be computed as:\n\nDuring the training process, many such encoded hypervectors are generated, bundled to represent a class and then stored into the final block, the associative memory. During inference, the encoded hypervector is compared against each trained hypervector using Hamming distance. For binary vectors, this involves an XOR and then popcount. The comparison with least distance is the inferred label.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Implementation",
      "text": "The HDC early fusion architecture is implemented on both the AMIGOS and DEAP datasets with a standard dimension of 10,000 for the full datapath in the baseline implementation. In the AMIGOS study, GSR, ECG and EEG signals were measured for 33 subjects as they watched 16 videos  [5] . Each video for each subject was classified to have either led to a positive or negative emotion (valence), and the strength of the emotion was classified as either strong or weak (arousal). From the 3 modalities, Wang et al. selected 214 time and frequency domain features relevant to accurate emotion classification  [22] . GSR has 32 features, ECG has 77 features, and EEG has 105 features. Similar features are used in this work. The data for all 33 subjects was appended and a moving average of 15 seconds over 30 seconds was applied. The signals were scaled to be between -1 and +1 to meet the HDC encoding process and downsampled by a factor of 8 for more rapid processing and usage of the HDC classification algorithm. Previous work uses the leave-one-subject-out approach to evaluate performance, this was also implemented for the early fusion architecture  [5] ,  [22] ,  [19] . The temporal encoder was tuned and an optimal n-gram of 3 feature windows was selected. For both datasets, transitionary ngrams (those with samples from both classes) were excluded from training and testing.\n\nThe DEAP study was collected in a similar format as the AMIGOS with 32 subjects watching 40 oneminute highlight excerpts of music videos selected to trigger distinct emotional reactions, however it con- tains more extensive sensor modalities: EEG, EMG, GSR, BVP, EOG, temperature and respiration  [11] . The arousal and valence scores were self-assessed by the participants on a scale between 1-9. A binary classification system is maintained for high and low valence and arousal by thresholding the scale at 5. Preprocessing and feature selection were done using the TEAP toolbox which selected time and frequency domain features for 5 of the modalities based on previous work in those areas  [20] . EMG has 10 features, EEG has 192 features, GSR has 7 features, BVP has 17 features, and respiration has 12 features per video. This results in 40 samples with a total of 238 features from 5 modalities per subject. The signals were then scaled to be between -1 and +1 for the HDC encoding scheme. Previous work for this dataset does training and inference independently by subject which was adopted in this work as well  [18] ,  [11] ,  [23] . Typically, 90% of the dataset is used for training per subject with the remaining 4 videos used for testing. For HDC, due to the inclusion of the temporal encoder, this would result in limited number of inferences leading to imprecise classification accuracies. As a result, the size of the training set was decreased to be 80% of the dataset with 20% used for testing. A temporal n-gram of 3 was selected for this dataset as well.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Memory Optimization",
      "text": "For both the AMIGOS and DEAP datasets, there are over 200 features that need to be spatially encoded. This requires advance storage of 214/238 iM vectors and 420/476 feature projection (FP) vectors -positive (PFP) and negative (NFP) -totalling to 642/714 vectors that need to be stored in the item memory. Use of a unique iM and FP vector set per channel is shown in first column of Figure  4 . Without significant reduction of the memory requirements, optimizations of other blocks will provide limited benefits to the overall efficiency.\n\nIn the spatial encoder, the iM vector and the FP vector are bound together to form a unique representation containing feature information that is specific to a feature channel. However, both the iM and FP vectors do not need to be unique to the feature channel in order to generate a unique combination of the two. The binding operation will inherently create a vector different, and pseudo-orthogonal to both of its inputs. Therefore, as long as one of these inputs is different for a specific feature channel, the spatially encoded feature channel vector (represented by the SE vectors in Figure  4 ) will be unique. Using this idea, a set of optimizations were developed and implemented on the DEAP and AMIGOS datasets:   5  'Combinatorial pairs' feature channel vector set generation demonstrated for 7 stored vectors. iM loops through vector bank after exhausting available sequential pairs for FP. Hybrid method follows by burst re-generating the vector bank with rule 90 so that new combinatorial pairs can be formed for more feature channels. Generation of 18 feature channel vector sets using a bank of only 7 vectors is shown.\n\n'iM vectors constant per modality': the iM is replicated across the various modalities, shown in the second column of Figure  4 . If, between each modalities, the FP vectors are different, then orthogonality and input feature channel uniqueness are maintained even if the iM is the same.\n\n'FP constant per feature channel': though the iM is now the same between each modalities, each feature channel within a modality still has a unique iM vector. Therefore, it is possible to re-use the same FP vectors for every feature channel within a modality, as shown in the third column of Figure  4 . This requires maintaining 2 unique FP vectors (PFP and NFP) for each modalities, and unique iM vectors within a modality.\n\n'Combinatorial pairs': taking this combinatorial binding strategy to its limit, the 2-input binding operation can be used to generate many unique vectors from a smaller set of vectors by following an algorithmic process. Each feature channel requires a distinct set containing an iM vector, and two FP (positive & negative) vectors: {iM, PFP, NFP}. If the vectors for feature channel 1 are {A, B, C}, then the bound pairs that could result from spatial encoding (iM ⊕ PFP or iM ⊕ NFP) are:\n\n• A ⊕ B • A ⊕ C B ⊕ C will not occur because they are both FP vectors. However, it is a unique pairing that could be re-used for another channel. For example, the set for feature channel 2 could be: {B, C, D}. The encoding process would use the following pairings:\n\nThis re-use strategy is the key to saving memory; it can be applied across all channels using a bank of the minimal required vectors, as shown in the first part of Figure  5 .\n\nEach vector can be paired with every other vector only once in order to maintain orthogonality and paired uniqueness across all feature channel. For a feature channel, one vector (the iM) must have two other available vectors (PFP and NFP) to pair with. With x defined as the floor function of x, the following equation can be used to calculate the total feature channels, TFC, possible given a bank of v vectors:\n\nThe formula can be derived by looping through each vector in the vector bank and sequentially grouping it with pairs of the other vectors. The generation of feature channel sets can be algorithmic, following the pattern shown in the tables in Figure  5 .\n\n'Rule 90 generation': implementation of the cellular automata with rule 90 will allow trading off vector storage with vector generation. If there are m modalities, the first 2 × m generated vectors would be used for the PFP and NFP vector for each modality. These would be maintained throughout training and inference resulting in 2×m+1 locally stored vectors including an initial seed vector. However, the rest of the iM vectors would be generated on the fly for each feature channel during the encoding process, requiring no additional vector storage. This is possible because of the fixed access pattern of the iM. The generation process requires use of rule 90 across the entire hypervector, and local storage of the most recently generated vector to use as the next seed. 1 vector is requested and then generated for each feature channel.\n\n'Hybrid': to reduce vector requests and hence the computation for rule90, the last two schemes: 'combinatorial paired binding' and 'rule 90 generation', can be combined. This hybrid strategy could include burst generation of a small set of vectors which could be locally stored. From this set, combinatorial pairs are assigned to feature channels and spatially encoded. This set can be gradually re-populated with new vectors as the old vectors are exhausted in the encoding process providing new possible pairs. This provides further tradeoff between vector storage and computation. The vector request rate (vector generation requests per feature channel) is minimized when the vector storage is large enough for the combinatorial paired binding scheme alone at which point no generation is required.\n\n'Dimensionality reduction': the final method of memory reduction is in the form of hypervector dimension reduction. This changes the size of the entire datapath, impacting both the logic complexity and the memory storage approximately linearly. However, smaller hypervectors also have reduced pseudoorthogonality -random lower-dimensional vectors are less likely to be nearly orthogonal in the hyperdimensional space than higher-dimensional vectors. The capacity for information is reduced, potentially impacting classification accuracy. This optimization is a tradeoff between algorithm accuracy performance and overall efficiency. The impact of changing dimensions on emotion recognition accuracy for the various memory optimizations is also explored.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "The HDC early fusion architecture was implemented on the AMIGOS and DEAP datasets for classification of high vs. low arousal and high vs. low valence. HDC early fusion achieved the highest average valence and arousal accuracy on AMIGOS, with the Rule 90 generation encoding method. A comparison against other AMIGOS binary classification multi-modal work using SVM, XGB, Gaussian Naive Bayes (GaussianNB) and ELM is shown in Table  1 . The early fusion encoding process provided a boost of 3.9% for valence and 10.4% for arousal from the late fusion HDC architecture previously implemented by Chang et al.  [3]  and demonstrates higher average accuracy than state of the art.\n\nOn the DEAP dataset, HDC early fusion achieved the highest average valence and arousal accuracy with the FP constant per feature channel encoding method. A comparison against other DEAP binary classification multi-modal work using GaussianNB, RBM with SVM, MESAE, and DBN is shown in Table  2 . HDC early fusion accuracy is very comparable with other high performance multi-modal approaches to the DEAP dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Table 1 Amigos Classification Accuracy Comparison Table",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Method",
      "text": "HV vs. LV (%) HA vs. LA (%) GaussianNB*  [5]   [3]  57 58.5 SVM  [22]  68.0 66.3 ELM  [19]  83.9 82.8 XGB  [22]  80.1 68.4 HDC late fusion  [3]  83.2 70.1 HDC early fusion 87.1 80.5 *F1 score. Accuracy value not available.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Table 2 Deap Classification Accuracy Comparison Table",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Method",
      "text": "HV vs. LV (%) HA vs. LA (%) GaussianNB  [11]  57.6 62.0 RBM with SVM  [18]  60.7 64.6 MESAE  [23]  76.2 77.2 DBN  [21]  51.2 68.4 HDC early fusion 76.7 74.2\n\nOne of the key benefits of HDC is the hardware efficiency, which is further improved for large-channeled emotion recognition tasks through the memory optimizations discussed earlier. The results for both valence and arousal accuracy as well as the resulting stored vector count for AMIGOS and DEAP across all memory-optimizing encoding methods are shown in Figure  6 .\n\nFor AMIGOS, with 214 channels and 3 modalities, the unoptimized method requires 3 unique vectors per feature channel {iM, PFP, NFP} -a total of 642 vectors. The 'iM vectors constant per modality' scheme is limited by the largest modality which is EEG with 105 feature channels. This results in a total of 105 + 214 × 2 = 533 vectors. The 'FP constant per feature channel' reduces the total vector set to 105 + 2 × 3 = 111. The 'combinatorial pairs' method uses Equation 6 and results in a required 31 vectors. Finally, the 'rule 90 generation' stores one FP pair {PFP, NFP} for each modality along with the seed vector, a total of 2 × 3 + 1 = 7. The memory optimizations result in an overall decrease in required vector storage by 98.91% from 642 vectors to 7 while the accuracy actually increased by 1.9% for arousal and 2.7% for valence.\n\nFor DEAP the overall memory storage is higher due to increased feature channels, 238, and modalities, 5. The unoptimized method requires 714 vectors. The 'iM vectors constant per modality' scheme is limited, again, by EEG with 195 channels totalling 668 vectors. The 'FP constant per feature channel' reduces this to 202. The 'combinatorial pairs' method requires 32 vectors. Finally, the 'rule 90 generation' totals to 11 vectors. The memory optimizations result in an overall decrease of 98.46% from 714 vectors to 11 while the accuracy actually increased by 0.6% for arousal and minimally decreased by 1.4% for valence.\n\nUsing the combinatorial pair method alone, the relationship between feature channel sets generated and number of stored vectors is shown in Figure  7 . Linear increases in number of stored vectors will cause result in a quadratically increasing number of available feature channel sets. This plot demonstrates that with 7 vectors, 9 feature channel sets are available, but with 50 vectors, 600 feature channel sets are available.\n\nThe combinatorial pair method can be used together with rule 90 in a hybrid scheme to provide options for tradeoff between memory and vector requests. In the solely rule 90 version, 7 vectors are stored for AMI-GOS and 11 vectors for DEAP; a total of 214 and 238 vector requests are made during the encoding process for AMIGOS and DEAP for a single sample -a vector request rate of 1. However, using the burst generation technique, a small subset of vectors could be generated in one shot using rule 90, stored, and then used for spatially encoding a quadratically larger number of feature channels to reduce the total number of vector requests made. The relationships between vector request rate (total vector requests / number of feature channels) and vector storage for AMIGOS and DEAP are shown in Figure  8 .\n\nThe only rule 90 method stores 7 and 11 vectors regardless which, if used with the hybrid scheme, could be used to generate 9 feature channels with every burst instead of only 7 for AMIGOS, or 25 feature channels instead of just 11 for DEAP. This results in a reduction in frequency of vector requests by 22.22% and 56.00% for AMIGOS and DEAP, respectively, even while using the same number of stored vectors.\n\nFinally, the impact of reducing dimension on the overall accuracy performances of the algorithm for emotion recognition tasks are shown for AMIGOS and DEAP in Figure  9 . For the AMIGOS dataset, a gradual decrease in accuracy is observed particularly from dimensions of 7000 by which point the average accuracy has dropped by 1%. Steeper decreases ∼0.6% and ∼4.4% are seen between dimensions 3000 and 2000 and between 2000 and 1000. Overall, between a dimension of 10, 000 and 1000, there is a decrease in average accuracy of ∼7%.\n\nFor the DEAP dataset, there is greater variation in accuracy across the dimensions and between methods, however an overall trend of decreasing accuracy can still be seen, particularly past dimensions of 5000 at which point the accuracy drops below 74.5% and continues to decrease rapidly including a ∼4.4% drop between 2000 and 1000. Overall, between 10, 000 and 1000 there is a decrease in average accuracy of ∼5.4%.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "The first change that was implemented was an overall architecture shift from late to early fusion. The results demonstrate an improvement in performance on the AMIGOS dataset despite moving the fusion point to combine the parallel data streams earlier in the encoding process. The boost in accuracy may come from the fact that different modalities may have different temporal behavior, which may lead to different optimal n-grams. For late fusion, an n-gram of 4 was used for all modalities without individual tuning. For early fusion, an optimal n-gram of 3 was selected for the fused modalities temporal behavior, improving the overall performance. The early fusion method requires tuning of only a single temporal encoder and still achieves higher accuracy even with reduction of the overall encoding complexity. This indicates the potential, benefits, and feasibility of early fusion encoding processes in HDC. Information is retained in the high-capacity vectors even with only a single encoding path after the spatial encoder.\n\nAdditionally, compared to other works, as shown in Table  1 , HDC early fusion performed better than GaussianNB, SVM, XGB, ELM and HDC late fusion on AMIGOS. It also performed better, as shown in Table 2 than GaussianNB, RBM with SVM and DBN and showed similar performance to MESAE on the DEAP dataset. Given its high performance, HDC early fusion appears well-suited for emotion recognition tasks.\n\nThe performance of various memory optimizations were explored and shown in Figure  6 . HDC depends on near-orthogonality between different data streams and feature values to ensure that samples from different classes that vary in these ways are encoded into sufficiently orthogonal class vectors. Each optimization reduces the total number of unique vectors that need to be stored in advance, however, there was no decrease in accuracy on AMIGOS between the most unoptimized and most optimized. There was actually an average increase of ∼2.3%; this accuracy change may be attributed to the random element of HDC vector initialization/generation which may result in either beneficial or detrimental random patterns. This is further demonstrated by the DEAP dataset for which the optimizations increased the arousal accuracy by 0.6%, yet decreased valence accuracy by 1.4%. With an overall memory reduction of >98%, the optimizations have a significant impact on the hardware requirements while displaying little to no performance degredation for both AMIGOS and DEAP, demonstrating that the techniques generalize across datasets.\n\nA hybrid, burst generation technique was proposed, in which a small vector set would be used maximally, as shown in Figure  7 , and then re-generated. By using this method, the total number of vectors that need to be generated during training or inference of a single sample can be decreased as shown in Figure  8 . Rule 90 alone requires generation of at least one vector per feature channel and doesn't take advantage of the combinatorial pairs available with its existing storage, hence implementation of this hybrid technique decreases the overall required vector generation. The benefit is higher for the DEAP dataset with more modalities due to the prior storage of a larger number of vectors for rule 90.\n\nThis technique also allows for scalability while still maintaining memory size; existing vectors pairs can be used to their highest capacity and then the vector bank can be re-generated using rule 90 for the further capacity required by additional channels or modalities. This could be done until the limits of the cellular automata are reached (>>10 3 ). The trade-off between the computation for vector generation and additional storage provides options. The optimal performance point based on power or memory constraints can be determined for specific applications/platforms.\n\nThe dimension reduction shown in Figure  9  demonstrates the trade-off between accuracy and comprehensive datapath size reduction. An optimal point could be selected that provides the accuracy needs of the system with minimum HDC dimensionality. With a tolerance of ∼2%, the dimension can be reduced by 70% to hypervectors of 3000 bits for AMIGOS and by 80% to hypervectors of 2000 bits for DEAP.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In conclusion, this work proposed a solution to the many-channeled (>200) memory-expensive emotion recognition task in the form of a brain-inspired early fusion hyperdimensional computing architecture alongside several optimization techniques that make emotion recognition feasible for hardware-constrained, low-power wearable applications. The various methods explored were able to achieve significant reduction >98% in required memory and >20% decrease in frequency of vector requests. Finally, the impact of hypervector dimension on emotion recognition accuracy demonstrated <2% performance degradation for datapath reductions of 70-80%.\n\nThough this work focuses on emotion recognition, all the proposed techniques maintain the properties required for successful hyperdimensional computing and therefore could generalize to other applications, and will be particularly useful for those with many, varied streams of input information.\n\nTo demonstrate the impact of the memory optimizations, the overall area of an ASIC realization of the emotion-classification engine was reduced by 87.3% for the cellulator automata rule 90 over the unoptimized version in a recent implementation study (unpublished). Next steps could include implementation of the proposed techniques for other applications with significantly larger numbers of channels and modalities to explore generalizability and scalability.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which lends HDC well to sensor fusion.",
      "page": 2
    },
    {
      "caption": "Figure 1: The sensor fusion datapath from electrodes to a fused hypervector for the three-modality emotion recognition system used",
      "page": 3
    },
    {
      "caption": "Figure 2: HDC early fusion detailed architecture for m modalities with the four main blocks: map into HDS, spatial encoder,",
      "page": 4
    },
    {
      "caption": "Figure 3: This reduces the parallel",
      "page": 5
    },
    {
      "caption": "Figure 3: HDC (a) early fusion and (b) late fusion",
      "page": 5
    },
    {
      "caption": "Figure 4: iM and FP vectors used to map into HDS to generate unique SE vectors per channel for (a) ‘unoptimized’ with distinct iM",
      "page": 6
    },
    {
      "caption": "Figure 4: Without signiﬁcant reduction",
      "page": 6
    },
    {
      "caption": "Figure 4: ) will be unique. Using this idea, a set of",
      "page": 6
    },
    {
      "caption": "Figure 5: ‘Combinatorial pairs’ feature channel vector set generation demonstrated for 7 stored vectors. iM loops through vector",
      "page": 7
    },
    {
      "caption": "Figure 4: If, between each modalities,",
      "page": 7
    },
    {
      "caption": "Figure 4: This requires",
      "page": 7
    },
    {
      "caption": "Figure 5: Each vector can be paired with every other vec-",
      "page": 7
    },
    {
      "caption": "Figure 5: ‘Rule 90 generation’: implementation of the cellu-",
      "page": 7
    },
    {
      "caption": "Figure 6: For AMIGOS, with 214 channels and 3 modali-",
      "page": 8
    },
    {
      "caption": "Figure 6: Arousal and valence accuracies and required vector storage for the various memory optimization as compared to",
      "page": 9
    },
    {
      "caption": "Figure 8: The only rule 90 method stores 7 and 11 vectors",
      "page": 9
    },
    {
      "caption": "Figure 7: Feature channel vector sets {iM, PFP, NFP}",
      "page": 10
    },
    {
      "caption": "Figure 9: For the AMIGOS dataset, a grad-",
      "page": 10
    },
    {
      "caption": "Figure 8: Vector request rate (number of vectors requested per",
      "page": 10
    },
    {
      "caption": "Figure 6: HDC depends",
      "page": 10
    },
    {
      "caption": "Figure 9: Average accuracy of valence and arousal vs. HDC",
      "page": 11
    },
    {
      "caption": "Figure 7: , and then re-generated. By us-",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The early fusion en-",
      "page": 8
    },
    {
      "caption": "Table 2: HDC early fusion accuracy is very comparable with",
      "page": 8
    },
    {
      "caption": "Table 1: AMIGOS classiﬁcation accuracy comparison table",
      "page": 8
    },
    {
      "caption": "Table 2: DEAP classiﬁcation accuracy comparison table",
      "page": 8
    },
    {
      "caption": "Table 1: , HDC early fusion performed better than",
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fpga implementations of svm classifiers: A review",
      "authors": [
        "S Afifi",
        "H Gholamhosseini",
        "R Sinha"
      ],
      "year": "2020",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "2",
      "title": "Laelaps: An energy-efficient seizure detection algorithm from long-term human ieeg recordings without false alarms",
      "authors": [
        "A Burrello",
        "L Cavigelli",
        "K Schindler",
        "L Benini",
        "A Rahimi"
      ],
      "year": "2019",
      "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
    },
    {
      "citation_id": "3",
      "title": "Hyperdimensional computing-based multimodality emotion recognition with physiological signals",
      "authors": [
        "E Chang",
        "A Rahimi",
        "L Benini",
        "Aya Wu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)"
    },
    {
      "citation_id": "4",
      "title": "Foundations of human computing: Facial expression and emotion",
      "authors": [
        "J Cohn"
      ],
      "year": "2007",
      "venue": "Artifical Intelligence for Human Computing"
    },
    {
      "citation_id": "5",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "Jam Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "A programmable hyper-dimensional processor architecture for human-centric iot",
      "authors": [
        "S Datta",
        "R Antonio",
        "A Ison",
        "J Rabaey"
      ],
      "year": "2019",
      "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems"
    },
    {
      "citation_id": "7",
      "title": "Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors",
      "authors": [
        "P Kanerva"
      ],
      "year": "2009",
      "venue": "Cognitive computation"
    },
    {
      "citation_id": "8",
      "title": "Automatic prediction of frustration",
      "authors": [
        "A Kapoor",
        "W Burleson",
        "R Picard"
      ],
      "year": "2007",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "9",
      "title": "No two brains are alike: Cloning a hyperdimensional associative memory using cellular automata computations",
      "authors": [
        "D Kleyko",
        "E Osipov"
      ],
      "year": "2017",
      "venue": "First International Early Research Career Enhancement School on Biologically Inspired Cognitive Architectures"
    },
    {
      "citation_id": "10",
      "title": "Cellular automata can reduce memory requirements of collective-state computing",
      "authors": [
        "D Kleyko",
        "E Frady",
        "F Sommer"
      ],
      "year": "2020",
      "venue": "Cellular automata can reduce memory requirements of collective-state computing"
    },
    {
      "citation_id": "11",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "12",
      "title": "A wearable biosensing system with in-sensor adaptive machine learning for hand gesture recognition",
      "authors": [
        "A Moin",
        "A Zhou",
        "A Rahimi",
        "A Menon",
        "S Benatti",
        "G Alexandrov",
        "S Tamakloe",
        "J Ting",
        "N Yamamoto",
        "Y Khan"
      ],
      "year": "2020",
      "venue": "Nature Electronics"
    },
    {
      "citation_id": "13",
      "title": "Pulp-hd: Accelerating brain-inspired high-dimensional computing on a parallel ultra-low power platform",
      "authors": [
        "F Montagna",
        "A Rahimi",
        "S Benatti",
        "D Rossi",
        "L Benini"
      ],
      "year": "2018",
      "venue": "2018 55th ACM/ESDA/IEEE Design Automation Conference"
    },
    {
      "citation_id": "14",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "15",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "16",
      "title": "Hyperdimensional computing for noninvasive brain-computer interfaces: Blind and one-shot classification of eeg error-related potentials",
      "authors": [
        "A Rahimi",
        "P Kanerva",
        "Millán Jdr",
        "J Rabaey"
      ],
      "year": "2017",
      "venue": "th EAI Int. Conf. on Bio-inspired Information and Communications Technologies"
    },
    {
      "citation_id": "17",
      "title": "Robust representation and recognition of facial emotions using extreme sparse learning",
      "authors": [
        "S Shojaeilangari",
        "W Yau",
        "K Nandakumar",
        "J Li",
        "E Teoh"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition through integrating eeg and peripheral signals",
      "authors": [
        "Y Shu",
        "S Wang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Toolbox for emotional feature extraction from physiological signals (teap)",
      "authors": [
        "M Soleymani",
        "F Villaro-Dixon",
        "T Pun",
        "G Chanel"
      ],
      "year": "2017",
      "venue": "Frontiers in ICT"
    },
    {
      "citation_id": "21",
      "title": "Modeling physiological data with deep belief networks",
      "authors": [
        "D Wang",
        "Y Shang"
      ],
      "year": "2013",
      "venue": "International journal of information and education technology (IJIET)"
    },
    {
      "citation_id": "22",
      "title": "Entropy-assisted emotion recognition of valence and arousal using xgboost classifier",
      "authors": [
        "S Wang",
        "H Li",
        "E Chang",
        "Aya Wu"
      ],
      "year": "2018",
      "venue": "IFIP International Conference on Artificial Intelligence Applications and Innovations"
    },
    {
      "citation_id": "23",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep",
      "authors": [
        "Z Yin",
        "M Zhao",
        "Y Wang",
        "J Yang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Recognition of emotions using multimodal physiological signals and an ensemble deep"
    }
  ]
}