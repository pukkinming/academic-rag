{
  "paper_id": "2203.13046v1",
  "title": "Facial Action Unit Recognition With Multi-Models Ensembling",
  "published": "2022-03-24T12:50:02Z",
  "authors": [
    "Wenqiang Jiang",
    "Yannan Wu",
    "Fengsheng Qiao",
    "Liyu Meng",
    "Yuanyuan Deng",
    "Chuanhe Liu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The Affective Behavior Analysis in-the-wild (ABAW) 2022 Competition gives Affective Computing a large promotion. In this paper, we present our method of AU challenge in this Competition. We use improved IResnet100 as backbone. Then we train AU dataset in Aff-Wild2 on three pertained models pretrained by our private au and expression dataset, and Glint360K respectively. Finally, we ensemble the results of our models. We achieved F1 score (macro) 0.731 on AU validation set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As an important part of Artificial Intelligence and Human Interaction, affective computing has arising more and more attention. Meanwhile, it has lots of applications in many fields, such as customer satisfaction survey, financial anti-fraud, and psychological analysis, etc.\n\nThe 3th ABAW Competition 2022 is a large-scale in the wild emotion database which is held by Dimitrios Kollias  [8] [9]  [12]   [11] , etc. It provides Aff-Wild2 which consists of three emotional database including categorical expression (such as happy, angry, sad) , valence arousal and 12 facial action units. Aff-Wild2 has 564 videos downloaded from YouTube. There are variety in ethnics, poses and ages, etc.  [20] [14]  [13] [10]  [7]  Different from seven basic categorical expressions and valence arousal, action units(AU) describe facial muscle movements developed by Paul Ekman in 1970s  [5] . Action units usually have concurrence. For example, AU 25 (lips part ) an AU 26 (jaw drop) often occur at the same time.\n\nIn this paper, we address AU task in ABAW 2022. In section 3, we present our methods of data balancing, model structure and loss function design. In section 4, we give the details about the datasets we used, experiment settings and our model ensembling strategy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Balancing",
      "text": "Facial action unit recognition is a multi-label visual task in deep learning. There usually exist label imbalance problem in multi-label task and data balancing is very difficult because of label concurrence. Several papers are proposed to solve this problem. Wu Tong designed loss functions  [19]  to solve this problem. Other scientists use data sampling to alleviate data unbalance.\n\nIn this paper, we make Aff-Wild2 AU dataset more balanced with ML-ROS method  [2] .Another method we try to alleviate data unbalance problem is batch sampling. When we train a batch from the training set. We wish all 12 Action Units (negative and positive ) are included in a batch. First, we organize the training data with 12 Action Units labels. Then when training is on, we sample each batch from the organized data. By doing this we can get a more balanced dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Structure",
      "text": "We use IResnet100  [4]  as the backbone. In order to better extract facial feature information, We choose pretrained weights on the Glint360K dataset  [1] .During the experiment, We found that increasing the texture feature information of the face is helpful for the classification of AU. Due to the increase of the network depth, the semantic features are more abundant but the texture features will be lost, so we added the FPN and SSH modules  [3] to increase the texture information and receptive field of the face.see Figure  1 .\n\nAt the same time, we flatten the features of each layer passing through FPN  [15]  and SSH  [18]  modules, and splicing the features of each layer to output 512 dimensions of features through a fully connected layer.And in order to make the network pay more attention to a certain part, we added the Coordinate Attention module  [6]  to the shallow and deep layers of the network.The experimental results show that the feature information obtained in this way contains more texture features, and the classification effect is better than the previous AU classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Loss Function",
      "text": "For AU dataset in Aff-Wild2, we have counted the numbers of each AU in the training set and verification set, and the distribution of data set is shown in Figure  2 .\n\nAs shown in Table  1 , the data distribution of training set and verification set is extremely imbalanced, this will make AU15,AU23 and AU24 difficult to train. Because Action Unit Detection is a multi-label problem, Data Augment cannot solve the data imbalance problem. Therefore, we try to solve this problem from loss function. (\n\nwhere L represents the sum of the 12 AU, N represents the number of AU.\n\n(2) is binary classification loss function, where W represents the loss weight of each AU, in our method, W = [1, 2, 1, 1, 1, 1, 1, 6, 6, 5, 1, 5], where x represents softmax output and in [0, 1] as  (1) , where y represents the target and takes either 0 or 1.\n\n(5) is multi label loss, where x represents softmax output and in [0, 1], where y represents the target and takes either 0 or 1.\n\ntotal loss = multi label loss(x, y) + ce loss(x, y)\n\nFinally, add bce loss and multi label loss together, as (6).",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Post Processing",
      "text": "Considering this au challenge needs video sequence predictions, we smooth the logits generated by the last layer of the network with a sliding window.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "Three datasets are used to train the backbone network as pretrained models respectively: Glint360K: which is the largest and cleanest face recognition dataset and contains 170M images of 360k IDs, baseline models trained on Glint360K can easily achieve excellent performance.\n\nPrivate commercial EXPR dataset: which contains 7K high definition images, each of which is human annotated into one of 7 facial expression categories(Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger and Other).\n\nPrivate commercial AU dataset: which contains 7K high definition images, each of which is human annotated into 15 face action unit categories(AU1, AU2, AU4, AU5, AU6, AU7, AU9, AU10, AU11, AU12, AU15, AU17, AU20, AU24 and AU26).\n\nAfter we get the pretrained models trained from the above three datasets, we train AU dataset in Aff-Wild2  [13]  to get our final results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training And Testing",
      "text": "In experiments, we use Iresnet100 to implement our framework. Our framework input size is 112x112. The SGD optimizer is used with a learning rate of 0.001, momentum of 0.9 and weight decay of 5e-4 and with a batch size of 256.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "We begin our oblation study by exploring the effectiveness of difference operation in our framework(Table  1 ). All experiments are based on Iresnet100(r100).Using glint360(glint) dataset pretrain can improve by 14.4%, using data balance(ba): oversampling AU2/AU15/AU23/AU24/AU26 and downsampling AU1/AU4/AU6/AU7/AU10/AU12/AU25 can improve by 1.5%, using multi label loss(mll) can improve by 2.1%, adding coordinate attention(ca) module can improve 4.4%, using label smooth(ls) can improve 4.8%, using bce loss + multi label loss(b+m) can improve 1.1%, using feature pyramid networks(fpn) can improve 1.9%, using bigger batch size 256 (bs256) can improve 0.3%, using Single Stage Headless(shh) can improve 0.3%, adding additional AU data can improve 0.6%.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ensemble Model",
      "text": "In previous competitions  [17] [16] we have used ensemble. In numerous experiments, we also adopted a model ensemble strategy, which ensemble the model with the highest F1 scale for each AU in the experiment, and obtained a higher F1 score on the validation set.see Figure  3 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "For AU task in ABAW Competition 2022, we train the backbone on three different datasets and get three pretrained models respectively. Using multi label loss function, ML-ROS and batch sampling, the problem of data imbalancing is alleviated. Then we train Aff-Wild2 AU dataset on the three pretrained models. Finally, best checkpoint for each AU and each model are chosen and ensembled.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: At the same time, we ﬂatten the features of each layer passing through FPN[15] and SSH[18]",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview system of proposed method",
      "page": 2
    },
    {
      "caption": "Figure 2: As shown in Table 1, the data distribution of training set and veriﬁcation set is extremely imbal-",
      "page": 2
    },
    {
      "caption": "Figure 2: Numbers of positive and negative AU in both training set and validation set.",
      "page": 3
    },
    {
      "caption": "Figure 3: Figure 3: F1 score results for each AU",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: All the effective methods are compared on the validation set.",
      "data": [
        {
          "r100": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "glint": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "bal": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "mll": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "ca": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "sa": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "ls": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "b+m": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "fpn": "(cid:88)\n(cid:88)\n(cid:88)\n(cid:88)",
          "bs256": "(cid:88)\n(cid:88)\n(cid:88)",
          "ssh": "(cid:88)\n(cid:88)",
          "data": "(cid:88)",
          "f1": "0.390\n0.534\n0.549\n0.570\n0.614\n0.662\n0.673\n0.690\n0.709\n0.712\n0.715\n0.721"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Partial FC: Training 10 Million Identities on a Single Machine",
      "authors": [
        "Xiang An"
      ],
      "venue": "Partial FC: Training 10 Million Identities on a Single Machine"
    },
    {
      "citation_id": "2",
      "title": "Addressing imbalance in multilabel classification: Measures and random resampling algorithms",
      "authors": [
        "Francisco Charte"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild",
      "authors": [
        "Jiankang Deng"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "4",
      "title": "Improved residual networks for image and video recognition",
      "authors": [
        "Ionut Cosmin"
      ],
      "year": "2020",
      "venue": "25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "5",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "6",
      "title": "Coordinate Attention for Efficient Mobile Network Design",
      "authors": [
        "Qibin Hou",
        "Daquan Zhou",
        "Jiashi Feng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "7",
      "title": "Analysing Affective Behavior in the First ABAW 2020 Competition",
      "authors": [
        "Kollias"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "8",
      "title": "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Multi-Task Learning Challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "9",
      "title": "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution Matching for Heterogeneous Multi-Task Learning: a Large-scale Face Study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "10",
      "title": "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face Behavior a la carte: Expressions, Affect and Action Units in a Single Network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "11",
      "title": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "12",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021"
    },
    {
      "citation_id": "13",
      "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "14",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Feature pyramid networks for object detection",
      "authors": [
        "Tsung-Yi Lin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "Group level audio-video emotion recognition using hybrid networks",
      "authors": [
        "Chuanhe Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "17",
      "title": "Multi-feature based emotion recognition for video clips",
      "authors": [
        "Chuanhe Liu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "18",
      "title": "Ssh: Single stage headless face detector",
      "authors": [
        "Mahyar Najibi"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "19",
      "title": "Distribution-balanced loss for multi-label classification in long-tailed datasets",
      "authors": [
        "Tong Wu"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference"
    }
  ]
}