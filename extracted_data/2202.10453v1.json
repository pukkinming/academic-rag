{
  "paper_id": "2202.10453v1",
  "title": "Predicting Emotion From Music Videos: Exploring The Relative Contribution Of Visual And Auditory Information To Affective Responses",
  "published": "2022-02-19T07:36:43Z",
  "authors": [
    "Phoebe Chua",
    "Dimos Makris",
    "Dorien Herremans",
    "Gemma Roig",
    "Kat Agres"
  ],
  "keywords": [
    "Multimodal Modelling",
    "Emotion Prediction",
    "Multimodal Emotion Prediction",
    "Dataset",
    "Long Short-Term Memory",
    "Affective Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although media content is increasingly produced, distributed, and consumed in multiple combinations of modalities, how individual modalities contribute to the perceived emotion of a media item remains poorly understood. In this paper we present MusicVideos (MuVi), a novel dataset for affective multimedia content analysis to study how the auditory and visual modalities contribute to the perceived emotion of media. The data were collected by presenting music videos to participants in three conditions: music, visual, and audiovisual. Participants annotated the music videos for valence and arousal over time, as well as the overall emotion conveyed. We present detailed descriptive statistics for key measures in the dataset and the results of feature importance analyses for each condition. Finally, we propose a novel transfer learning architecture to train Predictive models Augmented with Isolated modality Ratings (PAIR) and demonstrate the potential of isolated modality ratings for enhancing multimodal emotion recognition. Our results suggest that perceptions of arousal are influenced primarily by auditory information, while perceptions of valence are more subjective and can be influenced by both visual and auditory information. The dataset is made publicly available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "H OW do we perceive, integrate, and interpret affective information that is conveyed through multiple sensory modalities simultaneously, and how can we model and predict the effects of visual and auditory information on perceived emotion states? The strong connection between music and emotions  [1]  as well as video and emotions  [2]  is well-studied and intuitive -indeed, research has found that many people cite emotion regulation as the primary reason they listen to music  [3] . As multimedia content becomes easier to create and access, music can increasingly be used as a tool to shape the emotional narratives in video  [4] ; likewise, visual displays can shape our perception of a melody's affective content  [5] . The development of affective multimedia content analysis methods to predict and understand the influence of different media modalities on emotion is hence a key step in the development of affect-aware technologies and has utility in a wide range of applications including content delivery, indexing and search  [6] .\n\nThe defining feature of multimedia content is its use of multiple modalities to convey information: for example, we might easily imagine a video that plays text (captions), auditory (voices and music) and visual information simultaneously. Publishers have begun to exploit the flexibility of multimedia content by releasing their work on multiple platforms, each tailored to a different modality. For example, contemporary podcasts are frequently recorded with video then released on both audio and video streaming platforms. In turn, users can choose to consume multimedia content in various modalities or combinations of modalities based on their needs and preferences  [7] . Of the various forms of multimedia content available, music-related content is consumed in an especially wide range of modalities. It can be accessed through audio-only platforms or in audio-visual form (music videos), and is frequently used as background music for videos watched on mute, just to name a few scenarios. Although music is typically understood as an \"art of sound\"  [8] , video and audio music streaming currently account for similar proportions of the total music streaming volume, with the International Federation of the Phonographic Industry (IFPI) reporting in their latest Global Music Report  [9]  that video streaming made up 47% of music streaming consumption globally in 2019.\n\nWhile a great deal of work has been dedicated to music and video emotion recognition, to our knowledge, few studies have acknowledged that multimedia content can be flexibly consumed in different combinations of modalities, which may have differing effects on the emotions of the user. In this paper we present MuVi, a novel dataset for affective multimedia content analysis which contains music videos annotated with both dimensional and discrete measures of emotion in three conditions: audiovisual (original music videos), music (music-only) and visual (muted music videos). Each annotation is accompanied by annotator metadata relevant to the perception of emotion in music including gender, song familiarity and musical training. We provide a comprehensive descriptive analysis of the dataset's main features and examine whether modality and demographic factors influence the perceived emotion of stimuli. Then, we investigate the relative contribution of audio and visual modalities to prediction of arousal and valence in audiovisual stimuli, and discuss the most important auditory and visual features. Finally, we propose a novel model architecture that leverages knowledge transfer from isolated modality ratings for multimodal emotion recognition -a Predictive model Augmented with Isolated modality Ratings (PAIR).\n\nThe paper is organized as follows: Motivation for this study and related work on emotion recognition systems for media content are covered in Section 2. Data collection procedures and key features of the MuVi dataset, including dataset analysis and visualization, are covered in Sections 3 and 4. In Section 5, we present PAIR, our novel architecture for multimodal emotion recognition along with implementation details. Sections 6 and 7 detail the experimental setup and evaluation of our approach. Finally, in Section 8, we conclude and discuss some avenues for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Models Of Emotion",
      "text": "Emotion is generally defined as a collection of psychological states that include subjective experience, expressive behavior and peripheral physiological responses  [10] . While there are multiple ways to understand and measure emotions, the two most frequently used in affective computing research are the dimensional  [11]  and categorical  [12]  approaches. The dimensional approach represents emotions as points in a continuous space defined by a set of affective dimensions. Intuitively, it implies that emotion can be understood as a linear combination of the dimensions. A commonly employed dimensional model for Music Emotion Recognition (MER) is the circumplex model proposed by Russell  [11] , which consists of two dimensions: arousal (indicating emotional intensity) and valence (indicating pleasant versus unpleasant emotions). This valence-arousal (VA) model is easily applied across different domains and can be used to collect both static and dynamic emotion annotations -the difference being that static annotations generally reflect the overall emotion or mood conveyed by a media item, while dynamic annotations reflect the emotions conveyed by a media item as it unfolds over time. However, the VA model can lack the coherence of categorical approaches that define emotions with semantic terms  [13] .\n\nCategorical approaches represent emotions with a set of discrete labels such as \"joy\" or \"fear\", based on the underlying hypothesis that there are a finite number of distinct basic emotions  [12] . Basic emotion theories typically hypothesize that there are between 6 to 15 emotion categories, while recent work suggests that there are up to 27 distinct varieties of general emotional experiences  [13] . Zentner et al.  [1]  proposed the Geneva Emotional Music Scale (GEMS) as a domain-specific model of emotion to more accurately capture the range of aesthetic emotions that can be induced by music. Compared to mainstream emotion models, most of the emotions in GEMS are positive. The model also contains categories that are not always included in traditional approaches to the classification of emotions, such as \"transcendence\" and \"nostalgia,\" which help to better capture the range of emotion states induced by music. The full GEMS-45 scale contains 45 terms, but shorter variants of the scale (GEMS-25 and GEMS-9) were later defined through factor analysis. Categorical approaches may be fairly rich, due to the abstract and complex concepts expressed by natural language; they may also be more intuitive, especially when we want to describe the overall emotions conveyed by a song or video. However, they often do not lend themselves readily to dynamic emotion annotations.\n\nTo integrate dimensional and categorical approaches to understanding and measuring emotion, some have suggested that discrete emotions represent positions in an affective space defined by dimensions such as arousal and valence  [14] ,  [15] . Empirical tests of this relationship with emotionally evocative videos  [13]  have found that affective dimension judgements are able to explain the variance in categorical judgements and vice versa, and that categories seem to have more semantic value than affective dimensions. However, to our knowledge, similar studies to investigate the relative contributions of music and video content on emotion induction have not been conducted.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Perception Of Multimedia Content",
      "text": "An understanding of how we weigh and integrate affective information from different sensory modalities can inform the design of computational models for multimedia content analysis. At the same time, computational models can provide hints as to how humans perceive affective multimedia content  [16] . With regards to music multimedia content in particular, we are interested in the relative influence of audio and visual input on the perceived emotion of the content. Empirical studies on the influence of visual information on the evaluation music performance have consistently replicated significant effects  [17] ,  [18] , suggesting that visual information influences the way we perceive sound and music.\n\nMore broadly, studies have examined the ways in which music influences the perception of visual scenes, as well as the reverse relationship -the ways in which visual information influences the cognitive processing of music. Boltz  [19]  proposed that music influences visual perception by providing an interpretative framework for story comprehension. For example, mood-congruent music could be utilized by artistic directors to heighten emotional impact or clarify the meaning of ambiguous scenes in a visual story, while mood-incongruent music could be used to convey subtler meanings such as irony  [5] . Although comparatively little work has been done to investigate the way visual information influences the interpretation of music content, empirical findings suggest that music videos help maintain listener's interest when songs are relatively ambiguous  [20] . Both perspectives hint at how the construction of a meaningful narrative is an important component of the way we consume media. Hence, one way to think about multimedia might be as a form of expression that provides multiple information channels through which artists can convey meaning to their audience, and conversely by which audiences can make inferences about the artists' intent. Indeed, Sun and Lull  [21]  argue that one of the main appeals of music videos is that the visual scenes enrich a song's meaning and underlying message.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Datasets For Affective Multimedia Content Analysis",
      "text": "The development of computational models for emotion recognition and understanding has motivated the collection of novel datasets, usually based on audio, visual, or audiovisual stimuli that are accompanied by (dynamic) timeseries annotations of emotion or (static) emotion labels. While there are many dimensions along which to label affective computing datasets, here we focus on four key factors: emotion measurement, temporal nature, domain and features.\n\nEmotion measurement refers to the categorical and dimensional approaches described earlier, while temporal nature refers to whether the modeling target is a single, static label capturing the overall emotion of the content, or a vector that captures the dynamically changing emotion as it unfolds over time. Across datasets, emotion is most frequently measured using continuous valence and arousal scales, though some datasets include additional dimensions such as dominance  [28]  or use categorical measures  [24] .\n\nDomain refers to the modality in which stimuli are annotated; here, we focus on music, visual and audiovisual stimuli. Datasets using music-based stimuli have largely focused on music consumed in an audio-only format  [23] ,  [30] ,  [31] . One notable exception is the Database for Emotion Analysis using Physiological Signals (DEAP) which used music videos as the underlying stimuli  [28] , although the focus of the dataset is predicting emotion labels from a combination of databased features extracted from the underlying stimuli and physiological data. Datasets based on multimodal stimuli include those based on film repositories  [32]  [27], videos collected \"in the wild\"  [33] ,  [34]  and those provided by the Audiovisual Emotion Challenge (AVEC). We draw particular attention to the Ryerson Audio-Visual Database of Speech and Song (RAVDESS)  [29] , which consists of spoken and song utterances vocalized by professional actors to express emotions based on Ekman's theory of basic emotions  [12] . In contrast to the other datasets, the stimuli in RAVDESS were presented and annotated in three conditions: audio-only, video-only and audiovisual.\n\nFeatures that can be used for emotion recognition are also a key component of each dataset. Reviews have summarized extracted features relevant to affect detection in the audio modality such as intensity (loudness, energy), timbre (MFCC) and rhythm (tempo, regularity) features  [31]  and video modality such as colour, lighting key, motion intensity and shot length  [35] ,  [36] . Features that can capture complex latent dimensions in the data, such as the audio embeddings generated by the VGGish model  [37]  [38]  [39] , are also becoming increasingly popular. Features may be provided in the dataset or extracted from the source data if it is available. Apart from data-based features, however, the subjective nature of emotion perception suggests that it may be useful to build personalized models that account for characteristics of the user in addition to only the characteristics of the media content being consumed  [40]    [41] . Several of the datasets in Table  1  provide demographic and contextual information such as the annotator's age, gender, familiarity with the media and current mood.\n\nA comprehensive review of databases that utilize only one modality or other combinations of modalities is out of the scope of this paper, but we refer interested readers to recent survey studies (e.g.,  [35]  for multimodal emotion recognition,  [30]  for music and  [42]  for audiovisual). Table  1  instead highlights several of the prominent datasets relevant to music and audiovisual emotion recognition in order to show that affective computing datasets are typically focused on a single domain, restricting a researchers' ability to study the relative contribution of individual modalities to multimodal emotion recognition.\n\nThe similarities in the literature on music and video emotion recognition present a clear opportunity to bring together these two lines of research and better understand how individuals respond emotionally to media, and specifically music, presented in multimodal formats. To our knowledge, the dataset collected in this research, the MuVi dataset, is the first published dataset that contains multimodal stimuli annotated with both static and time-series measures of emotion in individual, isolated modalities (music, visual) as well as the original multimodal format (audiovisual). In other words, we have three sets of stimuli: the original music videos (audiovisual modality), the music clips (music modality) and muted video clips (visual modality). The presence of both static and dynamic annotations for each stimuli provides more multifaceted information about its affective content and enables us to study how discrete emotion labels might map onto continuous measures of valence and arousal  [15] . Finally, we provide the anonymized profile and demographic information of annotators.\n\nMuVi has several advantages over RAVDESS, the only other dataset we are aware of that contains isolated modality ratings. Firstly, while RAVDESS uses short utterances lasting several seconds as stimuli, MuVi uses longer 60s excerpts from music videos. As such, while the stimuli in RAVDESS are only rated with static emotion labels, the stimuli in MuVi are accompanied by both static overall emotion labels and dynamic time-series annotations. Additionally, while a media item is only shown once per participant in MuVi (ignoring modality), it is unclear whether a participant would be exposed to the same utterance in different modalities in RAVDESS -a concern since repeated presentation might influence the perceived emotion of an utterance  [29] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Collection",
      "text": "In this section, we describe the experimental protocol for the collection of MuVi dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Stimuli Selection",
      "text": "The MuVi dataset consists of music videos (MVs) annotated in three modalities: music-only, visual (i.e. muted video only), and audiovisual. We selected a corpus of 81 music videos from the LAKH MIDI Dataset  [43] , a collection of MIDI files matched to entries in the Million Song Dataset  [44] . Entries in the Million Song Dataset contain additional information about each music video including audio features and metadata, making it a valuable resource for users of MuVi who may be interested in obtaining additional features. As there were some MVs in which the song was preceded by a silent video intro, we manually scanned each MV to determine the timestamp at which music began playing, then obtained excerpts by taking the first minute of each MV beginning from that timestamp.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Procedure",
      "text": "We recruited 48 participants (31 males) aged between 19 and 35 (mean = 23.35y, std = 3.28y). All participants provided informed consent according to the procedures of the SUTD Institutional Review Board. The study was approved by the Institutional Review Board under SUTD-IRB 20-315. Prior to the experiment, participants filled out a survey that included demographic questions and music listening preferences. Once in the lab, participants were informed of the listening experimental protocol and the meaning of the arousal-valence scale used for annotations. For each quadrant of the arousal-valence space, participants could listen to a short (14s-17s) music clip that was typically rated as belonging to that quadrant. An experimenter was present to answer any questions.\n\nEach participant was then given two practice trials to familiarize themselves with the annotation interface before beginning the actual task. During the session, each participant annotated between 30 to 36 media items (not inclusive of the practice trials) in a randomly chosen media modality, and to ensure the reliability of ratings, the same media item would not be shown to a participant twice even if in a different modality. We compensated participants with a $10sgd voucher at the end of the session. The final dataset contains a total of 1,494 annotations (music = 350, visual = 349, audiovisual = 342). Each one-minute excerpt received between five to nine annotations in each modality, with the median number of annotations for each media item being six.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Annotation Process",
      "text": "For each media item, participants first completed the dynamic (continuous) annotation task. To date, continuous annotations of arousal and valence for video stimuli have typically been collected using a slider placed outside the video frame  [33] ,  [34] . However, this design might result in participants having to repeatedly shift their attention between the stimuli and their current position on the slider. Superimposing the arousal-valence scale over the video stimuli might mitigate this issue. To investigate which design participants would prefer, we tested two versions of the dynamic annotation interface in the video-only and music video conditions: with the 2D valence-arousal axes overlaid over the video (Figure  1 ) or located beside the video. Each version was randomly selected with 50% probability. Participants were asked to indicate which annotation interface they preferred at the end of the experiment.\n\nFor the dynamic annotation task, participants were asked to rate the emotion they thought the media item was trying to convey (rather than how they felt as they watched or listened to the media item) by moving their mouse over a 2D graph of the arousal-valence space as the item played. Figure  1  shows an example of our annotation interface, which displays the axes labels and an emoticon for each quadrant of the arousal-valence space to help participants remember where different perceived emotions lie on the axes. To collect the dynamic annotations, we used a script that sampled a participant's cursor location every 0.5 seconds. However, the actual sampling frequency slightly varied depending on the lab computer's browser, Internet connection speed and CPU load. For consistency, we resampled the annotations to maintain a sampling interval of as close to 2Hz as possible; the published annotations have a mean sampling interval of 2Hz and a standard deviation of 0.0177s.  Once the media item finished playing, participants indicated whether or not they had watched or listened to the media item before and completed the static (discrete) annotation task. The static annotation task consisted of selecting the terms that they felt described the media item's overall emotion. The terms were taken from GEMS-28  [45] , an extended version of the GEMS-25 scale, resulting in a total of 27 possible emotion labels. As shown in Table  2 , the labels can be grouped into nine different categories which in turn can be condensed into three \"superfactors\". Both these categories and superfactors were also displayed to participants. Participants had to select at least one label per media item with no upper limit on the number of labels they could choose; the median number of labels chosen was four.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Superfactor Category Emotion Labels",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sublimity",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "For each media item in our dataset, we extracted a set of audio and video features. To match the rate at which dynamic arousal-valence annotations were collected, time-varying acoustic features were extracted from the underlying music videos in non-overlapping 500ms windows.\n\nAudio feature extraction was performed with openS-MILE  [46] , a popular open-source library for audio feature extraction. Specifically, we used the \"emobase\" configuration file to extract a set of 988 low-level descriptors (LLDs) including MFCC, pitch, spectral, zero-crossing rate, loudness and intensity statistics, many of which have been shown to be effective for identifying emotion in music  [38] ,  [39] ,  [47] ,  [48] . Many other configurations are available in openSMILE but we provide the \"emobase\" set of acoustic features since it is well-documented and was designed for emotion recognition applications  [49] .\n\nSix types of visual features are provided: color, lighting key, facial expressions, scenes, objects, and actions. With the exception of action features, visual features were extracted from still frames sampled from the underlying music videos at 500ms intervals. Features related to color  [50]  and lighting  [51]  have been used in a number of video emotion recognition studies, based on the insight that these visual elements are often manipulated by filmmakers to convey a chosen mood or induce emotional reactions. These insights are supported by experiments showing that the hue and saturation of a color has significant influence on induced arousal and valence  [52] . Similarly, there is evidence that the facial expressions of individuals in a scene influence the emotion perceived by a viewer  [53] . We use OpenCV 1 to extract hue and saturation histograms as well as \"lighting key\" information. Facial expressions were extracted using a pre-trained VGG19  [54]  model.\n\nRecent techniques have begun to recognize the importance of contextual information such as scenes, objects and actions for image and video emotion recognition  [55]    [56] . Intuitively, certain contexts may tend to be associated with certain affective states: for example, a beach scene or images of pets might be associated with happy emotions/positive affect, while arguments might be associated with anger/negative affect. Scene classifications were extracted using a ResNet50 model  [57]  trained on the Places 365 dataset  [58]  while Yolov5 2 was used for object detection. To obtain the action features, we first employed the popular open source shot detection method SceneDetect 3 to decompose each music video into a sequence of scenes. Then, the I3D model described in  [59]  was used to compute the probability of each action class in each scene.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Arousal-Valence Annotations",
      "text": "The overall distribution of arousal-valence values across participants, sampled at 2Hz intervals, is displayed in Figure  2 . Across the entire dataset, the mean arousal value is 0.163 (music = 0.250, audiovisual = 0.192, visual= 0.044) while the mean valence is 0.073 (music = 0.177, audiovisual = 0.115, visual = -0.075).  Across the dataset, the most frequently selected emotion labels were \"energetic\", which was selected for 38.82% of the media stimuli and makes up 9.55% of the labels, and \"strong\", which was selected for 30.83% of the media stimuli and makes up 7.58% of the labels. Both \"strong\" and \"energetic\" belong to the Power category, which was also the most frequently selected category with 55.3% of discrete annotations containing at least one Power label. The least frequently selected labels were \"soothed\" and \"feeling of transcendence\". The distribution of GEMS emotion labels selected might be a reflection of the media stimuli used, which all belonged to the pop music genre and are hence generally more upbeat.\n\nWe also computed the pairwise Pearson's correlation of the emotion labels selected by each participant for each stimulus. A heatmap of the results is displayed in Figure  3 . The strongest positive correlations were between sad-tearful (r = 0.419, p <0.001) and bouncy-energetic (r = 0.378, p <0.001). One possible interpretation of the results is that these labels were frequently selected together. A second possibility is that the labels were selected by different people to indicate the same emotion for a particular media file, which might suggest partially redundant labels  [24] . Indeed, the pair sad-tearful belongs to the same GEMS category \"Sadness\", while bouncy-energetic belongs to the same superfactor \"Vitality\" (encompassing Power and Joyful Activation), as shown in Table  2 . The strongest negative correlations were observed between energetic-sad (r = -0.255, p <0.001) and joyful-sad (r = -0.206, p <0.001), which is logical as the terms are antonyms semantically.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Gems Labels",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of Annotation Interface, Media Modality And Annotator Profile On Perceived Emotion",
      "text": "We performed a series of analyses to examine whether the annotation interface, modality, and demographic and profile characteristics of participants influence the perceived emotion of media stimuli as measured by dynamic arousalvalence ratings as well as overall discrete emotion labels.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Arousal-Valence Annotations",
      "text": "A visual inspection of the full time-series arousal-valence annotations reveals interesting differences between the average emotion trajectories in modalities. Although the annotations appear to be similar across all modalities for some stimuli, on the whole, annotations for the audiovisual modality tend to resemble those for the music modality. First, for each stimulus, we average the arousal annotations to obtain mean arousal ratings for the music, visual and audiovisual modalities. Then, we compute the Pearson's correlation between each pair of modalities. For arousal, the highest correlations were obtained for the music-audiovisual modality pair (mean = 0.731, sd = 0.309), followed by visualaudiovisual (mean = 0.479, sd = 0.453) then music-visual (mean = 0.411, sd = 0.436). We observe the same pattern for valence, where the highest correlations were again for the music-audiovisual modality pair (mean = 0.433, sd = 0.423), followed by visual-audiovisual (mean = 0.372, sd = 0.495) then music-visual (mean = 0.084, sd = 0.577).\n\nGiven that the arousal and valence annotations are timeseries data, individual data points are not independent. As such, in order to perform the following set of statistical analyses, we pre-process all annotations by taking the median arousal and valence of each annotation sequence. A Kolmogorov-Smirnov test of goodness-of-fit indicates that median arousal (D = 0.273, p < 0.001) and valence (D = 0.229, p < 0.001) are not normally distributed. Hence, non-parametric statistical tests were used for the following analyses.\n\nOverlay type: Mann-Whitney U tests indicate that there is no significant difference between median arousal (U = 1.15×10 5 , p = 0.2798) and valence (U = 1.14×10 5 , p = 0.2456) for side-by-side versus overlaid annotation interfaces, which are relevant to the visual and audiovisual modalities. Hence, we combine the overlay types in our remaining analyses. The majority of participants (31 out of 49, 63.27%) indicated a preference for the annotation interface with the 2D valencearousal axes located beside the video.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Media Modality:",
      "text": "The Kruskal-Wallis one-way ANOVA was run to compare arousal-valence annotations for stimuli presented in the music, visual, and audiovisual modalities. The results were significant for both arousal (H = 79.45, p < 0.001) and valence (H = 100.38, p < 0.001), indicating that there is a significant difference between median emotion annotations for at least two of the modalities. Arousal is highest in the music modality (median = 0.2957), followed by the audiovisual modality (median = 0.2087), and lowest in the visual modality (median = 0.0). Similarly, valence is highest in the music modality (mean = 0.2087), followed by the audiovisual modality (mean = 0.1130), and lowest in the visual modality (mean = -0.0087). This suggests that modality in which a media item is consumed exerts an effect on its perceived arousal and valence.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Gems Emotion Labels",
      "text": "A contingency Chi-square test was used to assess whether the frequency distribution of emotion labels is significantly different for media stimuli presented in different modalities. Additionally, we investigated whether extra musical factors (such as gender, musical training, and familiarity with the media item) and characteristics of the annotation interface (whether it was overlaid on the media stimulus or placed on the side) would influence the frequency of each emotion label's selection. We found that neither modality nor overlay type significantly influenced the frequency with which emotion labels were selected. However, the test statistic was significant for gender (χ 2 = 46.36, df = 26, p = 0.008), formal musical training (χ 2 = 93.43, df = 26, p < 0.001), and familiarity (χ 2 = 263.26, df = 26, p < 0.001).\n\nWe manually inspected the difference between expected and observed frequencies of emotion labels for each of these three groups and found that for gender (Figure  4 ), labels under the GEMS category Vitality such as \"energetic\", \"animated\" and \"bouncy\" tended to show the greatest deviation from the expected frequencies according to the Chi-square test. Interestingly, this difference was not consistent across gender: female participants selected the label \"energetic\" less frequently than male participants, while selecting \"animated\" and \"bouncy\" more frequently.\n\nFor 'years of musical training' (Figure  5 ), the label \"nostalgic\" showed the greatest deviation from the expected frequencies according to the Chi-square test. Participants with three years or less of musical training selected the label \"nostalgic\" less frequently than expected, even though on average they were more likely to have seen or listened to the media stimuli previously.\n\nFor familiarity (Figure  6 ), the label \"tense\" shows the greatest deviation from the expected frequencies according to the Chi-square test. Participants who were unfamiliar with the media item selected labels from the GEMS category Tension such as \"tense\" and \"nervous\" much more frequently than expected, perhaps because they were less sure what to expect. They also selected the label \"nostalgic\" less frequently than expected, a result that is fairly intuitive since nostalgia is generally induced by familiarity. Finally, participants who were familiar with the media items selected labels from the GEMS category Sadness such as \"sad\" and \"blue\" more frequently than expected.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "A Predictive Model Augmented With Iso-Lated Modality Ratings (Pair)",
      "text": "In this section, we present a novel approach for multimodal time-series emotion recognition, termed a Predictive model Augmented with Isolated modality Ratings (PAIR). We first discuss models for unimodal emotion recognition relevant to the music and visual modalities, and culminate in a discussion of models for multimodal emotion recognition relevant to the audiovisual modality.\n\nWhile sophisticated modelling techniques are undoubtedly required to improve state of the art performance on emotion recognition tasks, our focus here is to investigate whether an approach that utilizes parameters pre-trained on isolated modality ratings can lead to better performance on a multimodal (audiovisual) emotion recognition task. As   such, all our models are based on long short-term memory (LSTM) networks  [60] . LSTMs are a popular variant of the recurrent neural network (RNN)  [61]  that can retain 'memory' of information seen previously over arbitrarily long intervals  [62] , and have been widely used to model time-series data for mood prediction and classification tasks (e.g.  [63] ,  [64] ,  [65] ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Unimodal Architectures",
      "text": "Unimodal systems can be viewed as building blocks for a multimodal emotion recognition framework  [35] . In this case, we model arousal-valence annotations from the two unimodal conditions (music, visual) in MuVi. We term these annotations isolated modality ratings, as they capture the emotion conveyed by the individual modalities that make up multimedia content in isolation. We trained separate models for arousal and valence in both isolated modalities. Isolated music modality ratings were predicted using audio features only while isolated visual modality ratings were predicted using visual features only, matching the information that human participants would have had access to during the experiment.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multimodal Architectures",
      "text": "In this section, we model arousal-valence annotations from the multimodal (audiovisual) condition in MuVi, exhaustively considering all possible combinations of features. First, we examine model performance when using only audio features, and when using only visual features. Next, we compare the perfomance of three different model architectures for audio and visual feature fusion (denoted as A1, A2 and PAIR). A1 and A2 utilize early (feature-level) fusion and late (decision-level) fusion respectively, two of the most commonly-employed feature fusion methods  [35] . The proposed PAIR architecture builds on the unimodal models described above. Figure  7  presents the following architectures which are described in detail below:\n\n•",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Early Fusion (A1):",
      "text": "In this architecture, we apply an early fusion method by concatenating the audio and visual features to form a single input feature vector. The feature vector is then fed to the LSTM blocks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "•",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Late Fusion (A2):",
      "text": "In this architecture, we apply a late fusion method by feeding the two input modalities (i.e., audio and visual features) to separate LSTM blocks. Then, the outputs of both LSTMs are concatenated and fed to a final, fully-connected layer for prediction.\n\n• PAIR: This architecture is identical to A2, except that the LSTM blocks are initialized with pre-trained weights from the corresponding unimodal models (trained on ratings from the isolated music and visual modalities). Then, the weights are fine-tuned on ratings from the multimodal audiovisual modality. The motivation for this architecture is twofold: firstly, from a computational perspective, the amount of data in MuVi is relatively small for training deep learning models. Additionally, there is evidence from neuroimaging studies that the integration of emotional information from auditory and visual channels can be attributed to the interaction between unimodal auditory and visual cortices plus higher order supramodal cortices, rather than from direct crosstalk between the auditory and visual cortices  [66] . As such, we hypothesized that we may obtain better performance on a multimodal affect recognition task by mimicking this neural architecture and initializing a multimodal model using weights pre-trained on relatively 'pure' isolated modality ratings.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Implementation Details",
      "text": "In all architectures (i.e., unimodal and multimodal) an LSTM block consists of two stacked LSTM layers each of size 256, with dropout of 0.2 to prevent overfitting. The output of the LSTM layers is fed to a fully connected layer with a \"tanh\" activation that outputs the final prediction. This value lies in the range of [-1, 1], matching that of the arousal and valence annotations. In the Late Fusion (A2) and PAIR (A3) architectures, we have an additional fully connected layer before the final predictive layer with 256 hidden units and \"relu\" activation.\n\nDuring the optimisation phase, the Adam  [67]  optimizer was used with a learning rate of 0.0001, while \"Mean Squared Error\" (MSE) was used as the loss function. Since the annotation and the feature extraction process are both dynamic, based on timesteps of 0.5s, our benchmark models predict arousal and valence at the next time step by taking the information of previous time-steps as input (i.e., sequence length of the LSTM). After experimenting, we found that the optimal sequence length for both arousal and valence across all modalities is 4 timesteps (i.e., 2 seconds). The model was implemented using the Tensorflow 2.x  [68]  deep learning framework, and is available online 4 .\n\n4. https://github.com/AMAAI-Lab/MuVi",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this section, we introduce our experimental design to evaluate our predictive models and examine the effectiveness of the novel transfer-learning that PAIR architecture utilizes. Moreover, we introduce the equivalent linear models as baselines, not only for comparison with the proposed PAIR architecture, but also for finding the most influential features.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Baseline: Linear Models",
      "text": "We implemented linear LASSO regression models as a simple baseline predictive model for valence and arousal. In addition to investigating the relative importance of audio and visual features for predicting emotion in audiovisual media, we are also interested in examining the individual features that are most predictive of arousal and valence across modalities.\n\nIn a review of music emotion recognition methods,  [31]  found that the use of too many features generally results in performance degradation. Feature selection methods such as manual selection or removal of highly correlated features can help reduce computational complexity and improve model performance by mitigating overfitting issues. Additionally, all possible combinations of modalities and corresponding feature sets are examined. We use the least absolute shrinkage and selection operator (LASSO) as our baseline linear predictive model, as it can also be used for feature selection  [69] . The LASSO minimizes the absolute sum of all coefficients (L1 regularization), and if subsets of features are highly correlated, the model tends to 'select' one feature from the pool while shrinking the remaining coefficients to zero. Crossvalidation was performed to determine optimal values for the complexity parameter alpha, which controls the strength of the regularization.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Predictive Model Evaluation Metrics",
      "text": "To account for variance in the media items and fully utilize the information provided by the dataset, the PAIR and linear models are evaluated using k-fold cross-validation. We partitioned the media items into five folds using the KFold  iterator in sklearn 5 with random state set to 42, iteratively holding out each fold as a validation set for the model trained on the remaining four fold.\n\nRegarding metrics, we modified the approach taken by  [23]  to design benchmarks for the MediaEval Emotion in Music task, and we use two evaluation metrics to benchmark the performance of baseline methods: root mean square error (RMSE) and Lin's Concordance Correlation Coefficient (CCC). Intuitively, the RMSE is an indicator of how well the predicted emotion matches the gold-standard or \"true\" emotion of a media item, while the CCC (interrater agreement, or the extent to which the human raters provide similar ratings across trials) was initially designed as a reproducibility index and quantifies the agreement between two vectors by measuring their difference from the 45 degree line through the origin. As such, it captures both the pattern of association and systematic differences between two sets of points. The CCC for two vectors (r 1 , r 2 ) is calculated as:\n\nRMSE and predicted CCC are computed for each media item in each validation set. The mean and standard devia-5. https://scikit-learn.org/stable/modules/cross validation.html# k-fold tions of each evaluation metric are then computed across all cross-validation folds.\n\nA gold-standard annotation sequence to be used as the modelling target was computed for each media item in each modality using the Evaluator Weighted Estimator (EWE) approach  [70] . As its name suggests, the EWE is a weighted average which gives more reliable annotations a higher weight. The weight of an annotation r j is calculated as the correlation between r j and the unweighted average of all annotations r. It is plausible that the dataset contains unreliable annotations with w j < 0; in the following analyses we deal with unreliable annotations by setting their weight to zero such that they are effectively not taken into account when calculating the gold-standard annotation sequence.\n\nr EW E = 1 j w j j w j r j (3)",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Human Inter-Rater Agreement",
      "text": "We also calculate inter-rater agreement, which provides a useful auxiliary metric for comparison with the predictive model results. Inter-rater agreement indicates the extent to which the perception of emotional content of a media item is  similar between human raters, and may serve as a measure of the quality of the human annotations of the dataset. We calculate the inter-rater agreement as CCC(r j , r EW E ). The CCC of each annotation r j with the corresponding EW E for the media stimuli whereby the EWE was calculated without r j so as to avoid overstating inter-rater agreement.\n\nAgreement was low to moderate. Across the entire dataset, the mean CCC for arousal was 0.34 ± 0.32 and the mean CCC for valence was 0.26 ± 0.32. The results in Table  5  are in line with previous empirical studies  [40]  which show that raters tend to demonstrate greater agreement in their perceptions of arousal in music as compared to valence, possibly because auditory cues relevant to arousal such as tempo and pitch are more salient and the perception of arousal is hence less subjective than that of valence. There was also greater agreement for arousal compared to valence in the audiovisual condition.   4  summarises the results of the LASSO models in different modality configurations. As expected, the best model performance was observed for arousal in the music modality. A t-test indicates that the simple linear model produced results that were not significantly different from human-level performance as determined by annotator's inter-rater agreement (t = 1.82, p > 0.05). Similarly, linear model performance was not significantly different from human-level performance when predicting arousal in the video modality with audio features only, and with both audio and visual features. However, none of the linear models were able to predict valence accurately enough to match human-level performance. These results agree with existing empirical work on music emotion recognition that typically achieves much higher accuracy for arousal compared to valence  [71] .\n\nA visual inspection of the linear model predictions in Figure  8  reveals that even though the predicted annotations are not significantly different from those of human raters, they are unrealistic looking and much less smooth, possibly because the model does not account for features at the preceding timesteps.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Pair (Lstm) Models",
      "text": "Table  6  shows the results of the proposed LSTM models for all modality-feature combinations examined using the linear models, and for the three additional architectures (A1, A2, PAIR) described in Section 5.2. Generally, the LSTM models improve over the linear model's results in all modality-feature combinations. As with the linear models, the best performance was observed when predicting arousal in the music modality, and predictions of arousal were more accurate than predictions of valence. The worst performance was observed in the visual modality in which only visual features were available (i.e., a unimodal architecture).\n\nNext, we compare the A1 (early fusion), A2 (late fusion) and PAIR architectures for multimodal emotion recognition. For both arousal and valence, the early and late fusion architectures improve over the linear model's results in terms of RMSE, but not for CCC. However, the PAIR architecture initialized with weights pre-trained on the unimodal LSTM networks produces a noticeable improvement in arousal prediction in terms of CCC. Paired t-tests show that the PAIR is significantly different (better) than the linear models for predicting arousal in terms of CCC (t(40) = 2.81, p < 0.01) and RMSE (t(40) = -13.0, p < .001), and for predicting valence in terms of CCC (t(40) = -11.7, p < 0.001), highlighting the effectiveness of our proposed transfer learning approach leveraging isolated modality ratings. Similarly, the PAIR model predictions in Figure  9  appear to be less erratic than the linear model predictions, and more closely follows the contours of the human participants' ratings.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Most Influential Features",
      "text": "LASSO performs feature selection by shrinking the coefficients of variables that are less significant to zero  [69] . We report the features with the largest absolute coefficient values for each model in Table  7 . Coefficient values were calculated by averaging across all cross-validation folds. When annotators were presented with audiovisual stimuli, the variance in arousal values seems to be explained largely by audio features while the variance in valence can be largely expained by visual features. This is supported by the observation that models trained on target labels from the audiovisual modality with audio features only outperformed those trained with visual features only when predicting arousal, but not when predicting valence.\n\nAcross the media modalities that included visual features, the LASSO regression models consistently estimated coefficients of relatively large magnitude for several action classes extracted using the I3D model  [59]  including \"stretching arm\" and \"sneezing\". We manually inspected the video scenes where these actions were classified as highly likely. The sign of the coefficient for \"stretching arm\" is negative, indicating that the action is associated with decreased arousal and valence. In the video scenes, \"stretching arm\" seemed to correspond generally to moving of the arms in the context of actions such as embraces or dancing. Additionally, most of the scenes were shot at least partially in slow motion. Previous work on affective gestures  [72]  has demonstrated that the spatial extension of movements  [73]  and openness of arm arrangements  [74]  express emotionally relevant information by indicating attributes of the communicator such as accessibility. The tempo conveyed through cinematic techniques such as camera and subject movement has also been found to be a highly expressive aspect of videos  [36] . It might be possible that it is not the action of stretching one's arm that conveys affective information, but the action classification model is recognizing latent dimensions such as open gestures and slowed motion that convey anticipation or affective states such as relaxedness that correspond to lowered arousal and valence.\n\nThe sign of the coefficient for \"sneezing\" is also negative. In the video scenes, \"sneezing\" seemed to correspond generally to shots centering on a single actor in which facial expressions were clearly visible. One possible reason why the feature could be associated with lower arousal and valence is because the actors depicted were usually trying to express seriousness or intensity. The reader may ask: if this were the case, why was the model assigning much higher weights to facial actions than facial expression features, which were also available? Action features were extracted from scenes that could stretch over longer durations, while facial expressions were extracted from still frames sampled from the underlying video at regular intervals. Consequently, predicted facial expressions could change significantly between consecutive sampled frames while predicted action features tended to be more stable. The stability of the action features might hence be a better match to the human annotations of arousal and valence which tend to change smoothly over time. This finding further justifies the use of time-series models that can incorporate information provided in previous time-steps for emotion recognition.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we explore the relative contribution of audio and visual information on perceived emotion in media items. To do so, we provide a new dataset of music videos labeled with both continuous as well as discrete measures of emotion in both isolated and combined modalities. We then used the dataset to train two types of emotion recognition models: interpretable linear predictive models for feature importance, and more sophisticated LSTM models. We trained three LSTM models for audiovisual emotion recognition: early fusion (A1), late fusion (A2), and a novel transfer learning model pre-trained on isolated modality ratings which we named PAIR. Notably, we find that transfer effects from isolated modalities can enhance performance of a multimodal model through transfer learning, a result that could not be shown from previous datasets due to the lack of isolated modality ratings.\n\nOur analyses show that affective multimedia content analysis is a complex problem for several reasons, including the subjectivity of perceived emotion, as evidenced by low to moderate agreement between human annotators using both continuous and discrete measures of emotion  [34]    [23] , and the association between factors such as gender and familiarity with perceived emotion  [75] . Specifically, we find that gender, familiarity, and musical training have a significant influence on the emotion labels selected by participants. In the current study, we randomly assigned stimuli to participants and focused on modeling the average emotion ratings reported. However, future work could examine methods to integrate profile and demographic information in addition to databased features to build personalized models, through model adaptation  [41]  or other techniques.\n\nOur work investigated several visual context features rarely used in affect detection including scene, object and action features. We find evidence that action features are an important source of affective information, and that scenebased features might be more appropriate for time-series emotion recognition than features extracted from windows of fixed duration. These results add to a growing body of work showing that people make use of contextual information when making affective judgements  [76] .\n\nFurther, although the visual modality is generally considered to be dominant  [77]  [18], we find that the auditory modality explains most of the variance in arousal for multimedia stimuli, while both the auditory and visual modalities contribute to explaining the variance in valence. Future work should investigate the impact of incongruent information across modalities, and explore which features cause the user's responses to be more oriented towards the visual or auditory modality when multimedia stimuli are presented, as this may may clarify when visual or auditory dominance occurs. A model that is able to fully capture the influence of emotion of each modality may even be used as a conditional generative model for music  [78] ,  [79] ,  [80]  or video. Additionally, although we have begun to examine feature importance, there is still room for future work on feature engineering. For example, extracting more extensive deep pretrained repesentations of latent audio and video dimensions might enable models to take into account more complex information.\n\nThe isolated modality ratings revealed several interesting patterns. Compared to listening to music alone, we find that the addition of a visual modality in music videos generally produces slightly lower arousal and valence and lower interrater agreement for arousal. In line with previous work  [30] , agreement for valence was lower than agreement for arousal in the music and audiovisual modalities. In the visual modality, however, agreement for valence was higher than agreement for arousal. In fact, the visual modality produced the lowest agreement in arousal but the highest agreement in valence. Taken together with the feature importance results above, the data suggests that visual information contributes to people's perceptions of valence more than arousal.\n\nOur results also have several practical implications. Convergent evidence across multiple analyses strongly suggests that in music videos, our perception of arousal is largely determined by the music while both the music and visual narrative contribute to our perception of valence. Additionally, an individual's impression of the emotion conveyed by a song could be very different depending on the modality in which it is consumed. Content platforms may find it useful to incorporate modality-related information into their search and recommendation algorithms. Similarly, music video producers might benefit from a greater awareness of each modality's contribution to the emotion conveyed by their content, and to the different modalities in which their content can be consumed. Of course, music videos are a subset of audiovisual content and, more broadly, multimodal content  [35] . While music videos usually involve the music being produced first with the visual narrative subsequently designed around the song, other scenarios are also possible during audiovisual content production: the video may be shot first, with music selection occurring subsequently, or the video and music production may proceed in parallel and inform one another. Investigating isolated modality ratings in other types of audiovisual and multimodal content is an important avenue for future work. We hope that our research and dataset will provide a useful steppdfing-stone towards advancing the field of affective computing through the study of isolated modalities.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) or located beside the video. Each",
      "page": 4
    },
    {
      "caption": "Figure 1: shows an example of our annotation interface, which",
      "page": 4
    },
    {
      "caption": "Figure 1: Dynamic annotation interface (overlayed mode). Participants",
      "page": 5
    },
    {
      "caption": "Figure 2: Across the entire dataset, the mean arousal value",
      "page": 6
    },
    {
      "caption": "Figure 2: Distribution of arousal-valence annotations for each modality,",
      "page": 6
    },
    {
      "caption": "Figure 3: The strongest positive correlations were between sad-tearful",
      "page": 6
    },
    {
      "caption": "Figure 3: Pearson’s correlations between categorical GEMS emotion labels.",
      "page": 6
    },
    {
      "caption": "Figure 5: ), the label",
      "page": 7
    },
    {
      "caption": "Figure 6: ), the label “tense” shows the",
      "page": 7
    },
    {
      "caption": "Figure 4: Distribution of GEMS emotion labels selected by female (left) and male (right) participants.",
      "page": 8
    },
    {
      "caption": "Figure 5: Left: Distribution of GEMS emotion labels selected by those with little (0 - 3 years) musical training; Right: more than 3 years of musical",
      "page": 8
    },
    {
      "caption": "Figure 6: Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant",
      "page": 8
    },
    {
      "caption": "Figure 7: presents the following",
      "page": 8
    },
    {
      "caption": "Figure 7: Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),",
      "page": 9
    },
    {
      "caption": "Figure 8: LASSO model predictions of arousal and valence ratings in the visual (left), audiovisual (center) and music (right) modalities for the media item",
      "page": 10
    },
    {
      "caption": "Figure 8: reveals that even though the predicted annotations",
      "page": 11
    },
    {
      "caption": "Figure 9: PAIR model predictions of arousal and valence ratings in the",
      "page": 11
    },
    {
      "caption": "Figure 9: appear to be less erratic",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "and Kat Agres, Member,\nIEEE"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "is increasingly produced, distributed, and consumed in multiple combinations of modalities, how\nAbstract—Although media content"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "individual modalities contribute to the perceived emotion of a media item remains poorly understood.\nIn this paper we present"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "MusicVideos (MuVi), a novel dataset\nfor affective multimedia content analysis to study how the auditory and visual modalities contribute"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "to the perceived emotion of media. The data were collected by presenting music videos to participants in three conditions: music, visual,"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "and audiovisual. Participants annotated the music videos for valence and arousal over time, as well as the overall emotion conveyed. We"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "present detailed descriptive statistics for key measures in the dataset and the results of\nfeature importance analyses for each condition."
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "Finally, we propose a novel\ntransfer learning architecture to train Predictive models Augmented with Isolated modality Ratings (PAIR) and"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "demonstrate the potential of\nisolated modality ratings for enhancing multimodal emotion recognition. Our results suggest\nthat perceptions"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "of arousal are inﬂuenced primarily by auditory information, while perceptions of valence are more subjective and can be inﬂuenced by"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "both visual and auditory information. The dataset\nis made publicly available."
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "Index Terms—Multimodal Modelling, Emotion Prediction, Multimodal Emotion Prediction, Dataset, Long Short-Term Memory, Affective"
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "Computing."
        },
        {
          "Phoebe Chua, Dimos Makris, Dorien Herremans, Senior Member,\nIEEE, Gemma Roig, Member,\nIEEE": "(cid:70)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "While a great deal of work has been dedicated to music": ""
        },
        {
          "While a great deal of work has been dedicated to music": "and video emotion recognition,\nto our knowledge,\nfew"
        },
        {
          "While a great deal of work has been dedicated to music": ""
        },
        {
          "While a great deal of work has been dedicated to music": "studies have acknowledged that multimedia content can"
        },
        {
          "While a great deal of work has been dedicated to music": "be ﬂexibly consumed in different combinations of modalities,"
        },
        {
          "While a great deal of work has been dedicated to music": ""
        },
        {
          "While a great deal of work has been dedicated to music": "which may have differing effects on the emotions of the user."
        },
        {
          "While a great deal of work has been dedicated to music": "In this paper we present MuVi, a novel dataset for affective"
        },
        {
          "While a great deal of work has been dedicated to music": ""
        },
        {
          "While a great deal of work has been dedicated to music": "multimedia content analysis which contains music videos"
        },
        {
          "While a great deal of work has been dedicated to music": ""
        },
        {
          "While a great deal of work has been dedicated to music": "annotated with both dimensional and discrete measures of"
        },
        {
          "While a great deal of work has been dedicated to music": ""
        },
        {
          "While a great deal of work has been dedicated to music": "emotion in three conditions: audiovisual\n(original music"
        },
        {
          "While a great deal of work has been dedicated to music": ""
        },
        {
          "While a great deal of work has been dedicated to music": "videos), music (music-only) and visual (muted music videos)."
        },
        {
          "While a great deal of work has been dedicated to music": "Each annotation is accompanied by annotator metadata"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        },
        {
          "(cid:70)": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "also contains\ncategories\nthat are not always\nincluded in"
        },
        {
          "2": "traditional approaches to the classiﬁcation of emotions, such"
        },
        {
          "2": "as “transcendence” and “nostalgia,” which help to better"
        },
        {
          "2": "capture the range of emotion states induced by music. The"
        },
        {
          "2": "full GEMS-45 scale contains 45 terms, but shorter variants of"
        },
        {
          "2": "the scale (GEMS-25 and GEMS-9) were later deﬁned through"
        },
        {
          "2": "factor analysis. Categorical approaches may be fairly rich,"
        },
        {
          "2": "due to the abstract and complex concepts expressed by"
        },
        {
          "2": "natural language; they may also be more intuitive, especially"
        },
        {
          "2": "when we want to describe the overall emotions conveyed by"
        },
        {
          "2": "a song or video. However, they often do not lend themselves"
        },
        {
          "2": "readily to dynamic emotion annotations."
        },
        {
          "2": "To integrate dimensional and categorical approaches to"
        },
        {
          "2": "understanding and measuring emotion, some have suggested"
        },
        {
          "2": "that discrete emotions represent positions in an affective"
        },
        {
          "2": "space deﬁned by dimensions such as arousal and valence [14],"
        },
        {
          "2": "[15]. Empirical\ntests of\nthis relationship with emotionally"
        },
        {
          "2": "evocative videos [13] have found that affective dimension"
        },
        {
          "2": "judgements are able to explain the variance in categorical"
        },
        {
          "2": "judgements and vice versa, and that categories seem to have"
        },
        {
          "2": "more semantic value than affective dimensions. However, to"
        },
        {
          "2": "our knowledge, similar studies to investigate the relative con-"
        },
        {
          "2": "tributions of music and video content on emotion induction"
        },
        {
          "2": "have not been conducted."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "2.2\nPerception of multimedia content"
        },
        {
          "2": ""
        },
        {
          "2": "An understanding of how we weigh and integrate affective"
        },
        {
          "2": "information from different sensory modalities can inform"
        },
        {
          "2": "the design of computational models for multimedia content"
        },
        {
          "2": "analysis. At the same time, computational models can pro-"
        },
        {
          "2": "vide hints as to how humans perceive affective multimedia"
        },
        {
          "2": "content [16]. With regards to music multimedia content in"
        },
        {
          "2": "particular, we are interested in the relative inﬂuence of audio"
        },
        {
          "2": "and visual\ninput on the perceived emotion of\nthe content."
        },
        {
          "2": "Empirical\nstudies on the inﬂuence of visual\ninformation"
        },
        {
          "2": "on the\nevaluation music performance have\nconsistently"
        },
        {
          "2": "replicated signiﬁcant effects [17], [18], suggesting that visual"
        },
        {
          "2": "information inﬂuences\nthe way we perceive\nsound and"
        },
        {
          "2": "music."
        },
        {
          "2": "More broadly, studies have examined the ways in which"
        },
        {
          "2": "music inﬂuences the perception of visual scenes, as well as"
        },
        {
          "2": "the reverse relationship – the ways in which visual informa-"
        },
        {
          "2": "tion inﬂuences the cognitive processing of music. Boltz [19]"
        },
        {
          "2": "proposed that music inﬂuences visual perception by provid-"
        },
        {
          "2": "ing an interpretative framework for story comprehension."
        },
        {
          "2": "For example, mood-congruent music could be utilized by"
        },
        {
          "2": "artistic directors\nto heighten emotional\nimpact or\nclarify"
        },
        {
          "2": "the meaning of ambiguous scenes in a visual story, while"
        },
        {
          "2": "mood-incongruent music could be used to convey subtler"
        },
        {
          "2": "meanings such as irony [5]. Although comparatively little"
        },
        {
          "2": "work has been done to investigate the way visual information"
        },
        {
          "2": "inﬂuences\nthe interpretation of music\ncontent, empirical"
        },
        {
          "2": "ﬁndings suggest that music videos help maintain listener’s"
        },
        {
          "2": "interest when songs are relatively ambiguous\n[20]. Both"
        },
        {
          "2": "perspectives hint at how the construction of a meaningful"
        },
        {
          "2": "narrative is an important component of the way we consume"
        },
        {
          "2": "media. Hence, one way to think about multimedia might be"
        },
        {
          "2": "as a form of expression that provides multiple information"
        },
        {
          "2": "channels through which artists can convey meaning to their"
        },
        {
          "2": "audience, and conversely by which audiences\ncan make"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: provide demographic and contextual information",
      "data": [
        {
          "3": "Table 1 provide demographic and contextual\ninformation"
        },
        {
          "3": "such as\nthe annotator’s age, gender,\nfamiliarity with the"
        },
        {
          "3": "media and current mood."
        },
        {
          "3": "A comprehensive review of databases that utilize only"
        },
        {
          "3": "one modality or other combinations of modalities is out of the"
        },
        {
          "3": "scope of this paper, but we refer interested readers to recent"
        },
        {
          "3": ""
        },
        {
          "3": "survey studies (e.g., [35] for multimodal emotion recognition,"
        },
        {
          "3": "[30]\nfor music and [42]\nfor audiovisual). Table 1 instead"
        },
        {
          "3": "highlights several of the prominent datasets relevant to music"
        },
        {
          "3": "and audiovisual emotion recognition in order to show that"
        },
        {
          "3": "affective computing datasets are typically focused on a single"
        },
        {
          "3": "domain, restricting a researchers’ ability to study the relative"
        },
        {
          "3": "contribution of individual modalities to multimodal emotion"
        },
        {
          "3": "recognition."
        },
        {
          "3": "The similarities\nin the literature on music and video"
        },
        {
          "3": "emotion recognition present a clear opportunity to bring to-"
        },
        {
          "3": "gether these two lines of research and better understand how"
        },
        {
          "3": "individuals respond emotionally to media, and speciﬁcally"
        },
        {
          "3": "music, presented in multimodal formats. To our knowledge,"
        },
        {
          "3": "the dataset collected in this research,\nthe MuVi dataset,\nis"
        },
        {
          "3": "the ﬁrst published dataset that contains multimodal stimuli"
        },
        {
          "3": "annotated with both static\nand time-series measures of"
        },
        {
          "3": "emotion in individual,\nisolated modalities (music, visual)"
        },
        {
          "3": "as well as\nthe original multimodal\nformat\n(audiovisual)."
        },
        {
          "3": "In other words, we have three sets of stimuli:\nthe original"
        },
        {
          "3": "music videos (audiovisual modality), the music clips (music"
        },
        {
          "3": "modality) and muted video clips\n(visual modality). The"
        },
        {
          "3": "presence of both static and dynamic annotations for each"
        },
        {
          "3": "stimuli provides more multifaceted information about\nits"
        },
        {
          "3": "affective\ncontent\nand enables us\nto study how discrete"
        },
        {
          "3": "emotion labels might map onto continuous measures of"
        },
        {
          "3": "valence and arousal [15]. Finally, we provide the anonymized"
        },
        {
          "3": "proﬁle and demographic information of annotators."
        },
        {
          "3": "MuVi has several advantages over RAVDESS,\nthe only"
        },
        {
          "3": "other dataset we are aware of that contains isolated modality"
        },
        {
          "3": "ratings. Firstly, while RAVDESS uses short utterances lasting"
        },
        {
          "3": "several seconds as stimuli, MuVi uses longer 60s excerpts"
        },
        {
          "3": "from music videos. As such, while the stimuli in RAVDESS"
        },
        {
          "3": "are only rated with static emotion labels,\nthe stimuli\nin"
        },
        {
          "3": "MuVi are accompanied by both static overall emotion labels"
        },
        {
          "3": "and dynamic time-series annotations. Additionally, while"
        },
        {
          "3": "a media item is only shown once per participant\nin MuVi"
        },
        {
          "3": "(ignoring modality), it is unclear whether a participant would"
        },
        {
          "3": "be exposed to the same utterance in different modalities in"
        },
        {
          "3": "RAVDESS - a concern since repeated presentation might"
        },
        {
          "3": "inﬂuence the perceived emotion of an utterance [29]."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "3\nDATASET COLLECTION"
        },
        {
          "3": ""
        },
        {
          "3": "In this section, we describe the experimental protocol for the"
        },
        {
          "3": "collection of MuVi dataset."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "3.1\nStimuli selection"
        },
        {
          "3": ""
        },
        {
          "3": "The MuVi dataset consists of music videos (MVs) annotated"
        },
        {
          "3": "in three modalities: music-only, visual (i.e. muted video only),"
        },
        {
          "3": "and audiovisual. We selected a corpus of 81 music videos"
        },
        {
          "3": "from the LAKH MIDI Dataset\n[43], a collection of MIDI"
        },
        {
          "3": "ﬁles matched to entries in the Million Song Dataset\n[44]."
        },
        {
          "3": "Entries\nin the Million Song Dataset\ncontain additional"
        },
        {
          "3": "information about each music video including audio features"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "contains a total of 1,494 annotations (music = 350, visual ="
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "349, audiovisual = 342). Each one-minute excerpt received"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "between ﬁve to nine annotations in each modality, with the"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "median number of annotations for each media item being"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "six."
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": ""
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": ""
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "3.2.1\nAnnotation process"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "For each media item, participants ﬁrst completed the dy-"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": ""
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "namic\n(continuous) annotation task. To date,\ncontinuous"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "annotations of arousal and valence for video stimuli have"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "typically been collected using a slider placed outside the"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "video frame [33],\n[34]. However,\nthis design might result"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "in participants having to repeatedly shift\ntheir attention"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "between the stimuli and their current position on the slider."
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "Superimposing the arousal-valence\nscale over\nthe video"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "stimuli might mitigate this issue. To investigate which design"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "participants would prefer, we tested two versions of\nthe"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "dynamic annotation interface in the video-only and music"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "video conditions: with the 2D valence-arousal axes overlaid"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "over the video (Figure 1) or located beside the video. Each"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "version was randomly selected with 50% probability. Partici-"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "pants were asked to indicate which annotation interface they"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "preferred at the end of the experiment."
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "For the dynamic annotation task, participants were asked"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "to rate the emotion they thought the media item was trying"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "to convey (rather\nthan how they felt as they watched or"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "listened to the media item) by moving their mouse over a"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "2D graph of\nthe arousal-valence space as the item played."
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "Figure 1 shows an example of our annotation interface, which"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "displays the axes labels and an emoticon for each quadrant"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "of the arousal-valence space to help participants remember"
        },
        {
          "Comparison of existing datasets for emotion recognition based on music, visual and audiovisual stimuli.": "where different perceived emotions lie on the axes."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "annotations to maintain a sampling interval of as close to": "2Hz as possible;\nthe published annotations have a mean",
          "recognition studies, based on the insight\nthat\nthese visual": "elements are often manipulated by ﬁlmmakers to convey a"
        },
        {
          "annotations to maintain a sampling interval of as close to": "sampling interval of 2Hz and a standard deviation of 0.0177s.",
          "recognition studies, based on the insight\nthat\nthese visual": "chosen mood or induce emotional reactions. These insights"
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "are supported by experiments showing that\nthe hue and"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Superfactor\nCategory\nEmotion labels",
          "recognition studies, based on the insight\nthat\nthese visual": "saturation of a color has signiﬁcant\ninﬂuence on induced"
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "arousal and valence [52]. Similarly,\nthere is evidence that"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Moved, Allured,",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Sublimity\nWonder",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Filled with wonder",
          "recognition studies, based on the insight\nthat\nthese visual": "the facial expressions of individuals in a scene inﬂuence the"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Fascinated, Overwhelmed,",
          "recognition studies, based on the insight\nthat\nthese visual": "emotion perceived by a viewer [53]. We use OpenCV 1\nto"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Transcendence",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Feeling of transcendence",
          "recognition studies, based on the insight\nthat\nthese visual": "extract hue and saturation histograms as well as “lighting"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Peacefulness\nSerene, Calm, Soothed",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "key” information. Facial expressions were extracted using a"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Tender, Affectionate,",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Tenderness",
          "recognition studies, based on the insight\nthat\nthese visual": "pre-trained VGG19 [54] model."
        },
        {
          "annotations to maintain a sampling interval of as close to": "Mellow",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "Recent\ntechniques have begun to recognize the impor-"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Nostalgic, Sentimental,",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Nostalgia",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Dreamy",
          "recognition studies, based on the insight\nthat\nthese visual": "tance of contextual information such as scenes, objects and ac-"
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "tions for image and video emotion recognition [55] [56]. Intu-"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Strong, Energetic,",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Vitality\nPower",
          "recognition studies, based on the insight\nthat\nthese visual": "itively, certain contexts may tend to be associated with certain"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Triumphant",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "Joyful Activation\nAnimated, Bouncy, Joyful",
          "recognition studies, based on the insight\nthat\nthese visual": "affective states: for example, a beach scene or images of pets"
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "might be associated with happy emotions/positive affect,"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Unease\nSadness\nSad, Tearful, Blue",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "while arguments might be associated with anger/negative"
        },
        {
          "annotations to maintain a sampling interval of as close to": "Tension\nTense, Agitated, Nervous",
          "recognition studies, based on the insight\nthat\nthese visual": ""
        },
        {
          "annotations to maintain a sampling interval of as close to": "",
          "recognition studies, based on the insight\nthat\nthese visual": "affect. Scene classiﬁcations were extracted using a ResNet50"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "3.3\nFeature extraction"
        },
        {
          "5": "For each media item in our dataset, we extracted a set of"
        },
        {
          "5": "audio and video features. To match the rate at which dynamic"
        },
        {
          "5": "arousal-valence annotations were collected,\ntime-varying"
        },
        {
          "5": "acoustic features were extracted from the underlying music"
        },
        {
          "5": "videos in non-overlapping 500ms windows."
        },
        {
          "5": "Audio feature extraction was performed with openS-"
        },
        {
          "5": "MILE [46], a popular open-source library for audio feature"
        },
        {
          "5": "extraction. Speciﬁcally, we used the “emobase” conﬁguration"
        },
        {
          "5": "ﬁle\nto extract\na\nset of\n988\nlow-level descriptors\n(LLDs)"
        },
        {
          "5": "including MFCC, pitch, spectral, zero-crossing rate, loudness"
        },
        {
          "5": "and intensity statistics, many of which have been shown to be"
        },
        {
          "5": "effective for identifying emotion in music [38], [39], [47], [48]."
        },
        {
          "5": "Many other conﬁgurations are available in openSMILE but"
        },
        {
          "5": ""
        },
        {
          "5": "we provide the “emobase” set of acoustic features since it is"
        },
        {
          "5": ""
        },
        {
          "5": "well-documented and was designed for emotion recognition"
        },
        {
          "5": ""
        },
        {
          "5": "applications [49]."
        },
        {
          "5": "Six types of visual features are provided: color, lighting"
        },
        {
          "5": "key,\nfacial expressions,\nscenes, objects, and actions. With"
        },
        {
          "5": "the exception of action features, visual\nfeatures were ex-"
        },
        {
          "5": "tracted from still frames sampled from the underlying music"
        },
        {
          "5": "videos at 500ms intervals. Features related to color [50] and"
        },
        {
          "5": "lighting [51] have been used in a number of video emotion"
        },
        {
          "5": "recognition studies, based on the insight\nthat\nthese visual"
        },
        {
          "5": "elements are often manipulated by ﬁlmmakers to convey a"
        },
        {
          "5": "chosen mood or induce emotional reactions. These insights"
        },
        {
          "5": "are supported by experiments showing that\nthe hue and"
        },
        {
          "5": "saturation of a color has signiﬁcant\ninﬂuence on induced"
        },
        {
          "5": "arousal and valence [52]. Similarly,\nthere is evidence that"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "the facial expressions of individuals in a scene inﬂuence the"
        },
        {
          "5": "emotion perceived by a viewer [53]. We use OpenCV 1\nto"
        },
        {
          "5": ""
        },
        {
          "5": "extract hue and saturation histograms as well as “lighting"
        },
        {
          "5": ""
        },
        {
          "5": "key” information. Facial expressions were extracted using a"
        },
        {
          "5": ""
        },
        {
          "5": "pre-trained VGG19 [54] model."
        },
        {
          "5": ""
        },
        {
          "5": "Recent\ntechniques have begun to recognize the impor-"
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "tance of contextual information such as scenes, objects and ac-"
        },
        {
          "5": "tions for image and video emotion recognition [55] [56]. Intu-"
        },
        {
          "5": ""
        },
        {
          "5": "itively, certain contexts may tend to be associated with certain"
        },
        {
          "5": ""
        },
        {
          "5": "affective states: for example, a beach scene or images of pets"
        },
        {
          "5": "might be associated with happy emotions/positive affect,"
        },
        {
          "5": ""
        },
        {
          "5": "while arguments might be associated with anger/negative"
        },
        {
          "5": ""
        },
        {
          "5": "affect. Scene classiﬁcations were extracted using a ResNet50"
        },
        {
          "5": ""
        },
        {
          "5": "model [57] trained on the Places 365 dataset [58] while Yolov5"
        },
        {
          "5": ""
        },
        {
          "5": "2 was used for object detection. To obtain the action features,"
        },
        {
          "5": ""
        },
        {
          "5": "we ﬁrst employed the popular open source shot detection"
        },
        {
          "5": "method SceneDetect 3 to decompose each music video into"
        },
        {
          "5": "a sequence of scenes. Then, the I3D model described in [59]"
        },
        {
          "5": ""
        },
        {
          "5": "was used to compute the probability of each action class in"
        },
        {
          "5": ""
        },
        {
          "5": "each scene."
        },
        {
          "5": ""
        },
        {
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: The strongest negative",
      "data": [
        {
          "6": "Songs %"
        },
        {
          "6": ""
        },
        {
          "6": "12.33%"
        },
        {
          "6": ""
        },
        {
          "6": "12.33%"
        },
        {
          "6": ""
        },
        {
          "6": "11.45%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "17.34%"
        },
        {
          "6": "11.86%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "7.32%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "7.05%"
        },
        {
          "6": "10.70%"
        },
        {
          "6": "5.96%"
        },
        {
          "6": ""
        },
        {
          "6": "10.23%"
        },
        {
          "6": "18.09%"
        },
        {
          "6": "7.79%"
        },
        {
          "6": ""
        },
        {
          "6": "21.54%"
        },
        {
          "6": "17.95%"
        },
        {
          "6": "16.67%"
        },
        {
          "6": ""
        },
        {
          "6": "30.83%"
        },
        {
          "6": "38.82%"
        },
        {
          "6": ""
        },
        {
          "6": "10.23%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "11.99%"
        },
        {
          "6": "24.32%"
        },
        {
          "6": "19.51%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "20.05%"
        },
        {
          "6": ""
        },
        {
          "6": "7.38%"
        },
        {
          "6": "10.57%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "22.36%"
        },
        {
          "6": "12.60%"
        },
        {
          "6": ""
        },
        {
          "6": "9.15%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: The strongest negative",
      "data": [
        {
          "6": "Songs %"
        },
        {
          "6": ""
        },
        {
          "6": "12.33%"
        },
        {
          "6": ""
        },
        {
          "6": "12.33%"
        },
        {
          "6": ""
        },
        {
          "6": "11.45%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "17.34%"
        },
        {
          "6": "11.86%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "7.32%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "7.05%"
        },
        {
          "6": "10.70%"
        },
        {
          "6": "5.96%"
        },
        {
          "6": ""
        },
        {
          "6": "10.23%"
        },
        {
          "6": "18.09%"
        },
        {
          "6": "7.79%"
        },
        {
          "6": ""
        },
        {
          "6": "21.54%"
        },
        {
          "6": "17.95%"
        },
        {
          "6": "16.67%"
        },
        {
          "6": ""
        },
        {
          "6": "30.83%"
        },
        {
          "6": "38.82%"
        },
        {
          "6": ""
        },
        {
          "6": "10.23%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "11.99%"
        },
        {
          "6": "24.32%"
        },
        {
          "6": "19.51%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "20.05%"
        },
        {
          "6": ""
        },
        {
          "6": "7.38%"
        },
        {
          "6": "10.57%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "22.36%"
        },
        {
          "6": "12.60%"
        },
        {
          "6": ""
        },
        {
          "6": "9.15%"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "4.2.2\nGEMS emotion labels"
        },
        {
          "7": ""
        },
        {
          "7": "A contingency Chi-square test was used to assess whether"
        },
        {
          "7": "the frequency distribution of emotion labels is signiﬁcantly"
        },
        {
          "7": "different for media stimuli presented in different modalities."
        },
        {
          "7": "Additionally, we investigated whether extra musical factors"
        },
        {
          "7": "(such as gender, musical training, and familiarity with the"
        },
        {
          "7": "media item) and characteristics of the annotation interface"
        },
        {
          "7": "(whether it was overlaid on the media stimulus or placed"
        },
        {
          "7": ""
        },
        {
          "7": "on the side) would inﬂuence the frequency of each emotion"
        },
        {
          "7": "label’s selection. We found that neither modality nor overlay"
        },
        {
          "7": "type\nsigniﬁcantly\ninﬂuenced the\nfrequency with which"
        },
        {
          "7": "emotion labels were selected. However,\nthe test\nstatistic"
        },
        {
          "7": "was signiﬁcant for gender (χ2 = 46.36, df = 26, p = 0.008),"
        },
        {
          "7": "formal musical training (χ2 = 93.43, df = 26, p < 0.001), and"
        },
        {
          "7": "familiarity (χ2 = 263.26, df = 26, p < 0.001)."
        },
        {
          "7": "We manually inspected the difference between expected"
        },
        {
          "7": "and observed frequencies of emotion labels for each of these"
        },
        {
          "7": "three groups and found that\nfor gender (Figure 4),\nlabels"
        },
        {
          "7": "under the GEMS category Vitality such as “energetic”, “ani-"
        },
        {
          "7": "mated” and “bouncy” tended to show the greatest deviation"
        },
        {
          "7": "from the expected frequencies according to the Chi-square"
        },
        {
          "7": "test. Interestingly, this difference was not consistent across"
        },
        {
          "7": "gender: female participants selected the label “energetic” less"
        },
        {
          "7": "frequently than male participants, while selecting “animated”"
        },
        {
          "7": "and “bouncy” more frequently."
        },
        {
          "7": "For\n‘years\nof musical\ntraining’\n(Figure\n5),\nthe\nlabel"
        },
        {
          "7": "“nostalgic” showed the greatest deviation from the expected"
        },
        {
          "7": "frequencies according to the Chi-square test. Participants"
        },
        {
          "7": "with three years or less of musical training selected the label"
        },
        {
          "7": "“nostalgic” less frequently than expected, even though on"
        },
        {
          "7": "average they were more likely to have seen or listened to the"
        },
        {
          "7": "media stimuli previously."
        },
        {
          "7": "For familiarity (Figure 6),\nthe label “tense” shows the"
        },
        {
          "7": "greatest deviation from the expected frequencies according"
        },
        {
          "7": "to the Chi-square test. Participants who were unfamiliar"
        },
        {
          "7": "with the media item selected labels from the GEMS category"
        },
        {
          "7": "Tension such as “tense” and “nervous” much more frequently"
        },
        {
          "7": "than expected, perhaps because they were less sure what to"
        },
        {
          "7": "expect. They also selected the label “nostalgic” less frequently"
        },
        {
          "7": "than expected, a result that is fairly intuitive since nostalgia"
        },
        {
          "7": "is generally induced by familiarity. Finally, participants who"
        },
        {
          "7": "were familiar with the media items selected labels from the"
        },
        {
          "7": "GEMS category Sadness\nsuch as “sad” and “blue” more"
        },
        {
          "7": "frequently than expected."
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "5\nA PREDICTIVE MODEL AUGMENTED WITH ISO-"
        },
        {
          "7": ""
        },
        {
          "7": "LATED MODALITY RATINGS (PAIR)"
        },
        {
          "7": ""
        },
        {
          "7": "In this section, we present a novel approach for multimodal"
        },
        {
          "7": "time-series emotion recognition, termed a Predictive model"
        },
        {
          "7": "Augmented with Isolated modality Ratings (PAIR). We ﬁrst"
        },
        {
          "7": "discuss models for unimodal emotion recognition relevant"
        },
        {
          "7": "to the music and visual modalities, and culminate\nin a"
        },
        {
          "7": "discussion of models\nfor multimodal emotion recognition"
        },
        {
          "7": "relevant to the audiovisual modality."
        },
        {
          "7": "While sophisticated modelling techniques are undoubt-"
        },
        {
          "7": "edly required to improve state of\nthe art performance on"
        },
        {
          "7": "emotion recognition tasks, our focus here is to investigate"
        },
        {
          "7": "whether an approach that utilizes parameters pre-trained"
        },
        {
          "7": "on isolated modality ratings can lead to better performance"
        },
        {
          "7": "on a multimodal (audiovisual) emotion recognition task. As"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "has viewed or listened to the media item previously."
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "such, all our models are based on long short-term memory"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "(LSTM) networks [60]. LSTMs are a popular variant of the"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "recurrent neural network (RNN) [61] that can retain ’memory’"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "of information seen previously over arbitrarily long intervals"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "[62], and have been widely used to model time-series data"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "for mood prediction and classiﬁcation tasks (e.g. [63], [64],"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "[65])."
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": ""
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "5.1\nUnimodal architectures"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": ""
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "Unimodal\nsystems can be viewed as building blocks\nfor"
        },
        {
          "Fig. 6. Left: Distribution of GEMS emotion labels selected by participant who were exposed to the media stimuli for the ﬁrst time; Right: Participant": "a multimodal emotion recognition framework [35]. In this"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The proposed PAIR architecture builds on the unimodal": "models described above. Figure 7 presents the following"
        },
        {
          "The proposed PAIR architecture builds on the unimodal": "architectures which are described in detail below:"
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": "•"
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": "•"
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": "•"
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        },
        {
          "The proposed PAIR architecture builds on the unimodal": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "and c) the proposed PAIR. PAIR’s LSTM blocks initializes the pre-trained weights (w) from the corresponding Music and Visual Unimodal networks."
        },
        {
          "Fig. 7.": "from a computational perspective, the amount of data",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "6\nEXPERIMENTAL SETUP"
        },
        {
          "Fig. 7.": "in MuVi is relatively small for training deep learning",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "In this section, we introduce our experimental design to"
        },
        {
          "Fig. 7.": "models. Additionally,\nthere is evidence from neu-",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "evaluate our predictive models and examine the effectiveness"
        },
        {
          "Fig. 7.": "roimaging studies that the integration of emotional",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "of the novel transfer-learning that PAIR architecture utilizes."
        },
        {
          "Fig. 7.": "information from auditory and visual channels can be",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "Moreover, we introduce the equivalent\nlinear models as"
        },
        {
          "Fig. 7.": "attributed to the interaction between unimodal audi-",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "baselines, not only for comparison with the proposed PAIR"
        },
        {
          "Fig. 7.": "tory and visual cortices plus higher order supramodal",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "architecture, but also for ﬁnding the most inﬂuential features."
        },
        {
          "Fig. 7.": "cortices,\nrather\nthan from direct crosstalk between",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "the auditory and visual cortices [66]. As such, we",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "hypothesized that we may obtain better performance",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "6.1\nBaseline: Linear Models"
        },
        {
          "Fig. 7.": "on a multimodal affect recognition task by mimicking",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "We implemented linear LASSO regression models as a simple"
        },
        {
          "Fig. 7.": "this neural architecture and initializing a multimodal",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "baseline predictive model for valence and arousal. In addition"
        },
        {
          "Fig. 7.": "model using weights pre-trained on relatively ’pure’",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "to investigating the relative importance of audio and visual"
        },
        {
          "Fig. 7.": "isolated modality ratings.",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "features for predicting emotion in audiovisual media, we are"
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "also interested in examining the individual features that are"
        },
        {
          "Fig. 7.": "5.3\nImplementation details",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "most predictive of arousal and valence across modalities."
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "In a review of music emotion recognition methods, [31]"
        },
        {
          "Fig. 7.": "In all architectures (i.e., unimodal and multimodal) an LSTM",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "found that the use of too many features generally results in"
        },
        {
          "Fig. 7.": "block consists of two stacked LSTM layers each of size 256,",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "performance degradation. Feature selection methods such as"
        },
        {
          "Fig. 7.": "with dropout of 0.2 to prevent overﬁtting. The output of the",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "manual selection or removal of highly correlated features can"
        },
        {
          "Fig. 7.": "LSTM layers is fed to a fully connected layer with a “tanh”",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "help reduce computational complexity and improve model"
        },
        {
          "Fig. 7.": "activation that outputs the ﬁnal prediction. This value lies",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "performance by mitigating overﬁtting issues. Additionally,"
        },
        {
          "Fig. 7.": "in the range of\n[−1, 1], matching that of\nthe arousal and",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "all possible combinations of modalities and corresponding"
        },
        {
          "Fig. 7.": "valence annotations. In the Late Fusion (A2) and PAIR (A3)",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "feature sets are examined. We use the least absolute shrinkage"
        },
        {
          "Fig. 7.": "architectures, we have an additional fully connected layer",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "and selection operator (LASSO) as our baseline linear predic-"
        },
        {
          "Fig. 7.": "before the ﬁnal predictive layer with 256 hidden units and",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "tive model, as it can also be used for feature selection [69]."
        },
        {
          "Fig. 7.": "“relu” activation.",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "The LASSO minimizes the absolute sum of all coefﬁcients"
        },
        {
          "Fig. 7.": "During the optimisation phase, the Adam [67] optimizer",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "(L1 regularization), and if\nsubsets of\nfeatures are highly"
        },
        {
          "Fig. 7.": "was used with a\nlearning rate of\n0.0001, while\n“Mean",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "correlated, the model tends to ‘select’ one feature from the"
        },
        {
          "Fig. 7.": "Squared Error” (MSE) was used as the loss function. Since",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "pool while shrinking the remaining coefﬁcients to zero. Cross-"
        },
        {
          "Fig. 7.": "the annotation and the feature extraction process are both",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "validation was performed to determine optimal values for"
        },
        {
          "Fig. 7.": "dynamic, based on timesteps of 0.5s, our benchmark models",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "the complexity parameter alpha, which controls the strength"
        },
        {
          "Fig. 7.": "predict arousal and valence at the next time step by taking",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "of the regularization."
        },
        {
          "Fig. 7.": "the information of previous time-steps as input (i.e., sequence",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "length of the LSTM). After experimenting, we found that the",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "optimal sequence length for both arousal and valence across",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "6.2\nPredictive Model Evaluation Metrics"
        },
        {
          "Fig. 7.": "all modalities is 4 timesteps (i.e., 2 seconds). The model was",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "To account for variance in the media items and fully utilize"
        },
        {
          "Fig. 7.": "implemented using the Tensorﬂow 2.x [68] deep learning",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "the\ninformation provided by the dataset,\nthe PAIR and"
        },
        {
          "Fig. 7.": "framework, and is available online 4.",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": ""
        },
        {
          "Fig. 7.": "",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "linear models are evaluated using k-fold cross-validation. We"
        },
        {
          "Fig. 7.": "4. https://github.com/AMAAI-Lab/MuVi",
          "Illustrated workﬂow for the proposed Multimodal architectures for the audiovisual modality featuring: a) Early Fusion (A1), b) Late Fusion (A2),": "partitioned the media items into ﬁve folds using the KFold"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "teal (valence) lines represent"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "predicted rating at that\ntimestep."
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "iterator in sklearn 5 with random state set to 42, iteratively"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "holding out each fold as a validation set for the model trained"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "on the remaining four fold."
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "Regarding metrics, we modiﬁed the\napproach taken"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "by [23]\nto design benchmarks for the MediaEval Emotion"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "in Music\ntask,\nand we use\ntwo\nevaluation metrics\nto"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "benchmark the performance of baseline methods: root mean"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "square error\n(RMSE) and Lin’s Concordance Correlation"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "Coefﬁcient (CCC). Intuitively, the RMSE is an indicator of"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "how well the predicted emotion matches the gold-standard"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "or “true” emotion of a media item, while the CCC (inter-"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "rater agreement, or the extent\nto which the human raters"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "provide similar ratings across trials) was initially designed as"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "a reproducibility index and quantiﬁes the agreement between"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "two vectors by measuring their difference from the 45 degree"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "line through the origin. As such, it captures both the pattern"
        },
        {
          "’Living On A Prayer’ by Bon Jovi. Each colored line in the background represents an individual participants’ rating, while the bold red (arousal) and": "of association and systematic differences between two sets"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 6: shows the results of the proposed LSTM models",
      "data": [
        {
          "compared to valence, possibly because auditory cues relevant": "to arousal such as tempo and pitch are more salient and"
        },
        {
          "compared to valence, possibly because auditory cues relevant": "the perception of arousal\nis hence less subjective than that"
        },
        {
          "compared to valence, possibly because auditory cues relevant": "of valence. There was also greater agreement\nfor arousal"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "compared to valence in the audiovisual condition."
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "7\nRESULTS AND DISCUSSION"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "7.1\nPredicting valence and arousal"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "7.1.1\nBaseline: LASSO models"
        },
        {
          "compared to valence, possibly because auditory cues relevant": "Starting with the baselines, Table 4 summarises the results"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "of the LASSO models in different modality conﬁgurations."
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "As expected, the best model performance was observed for"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "arousal in the music modality. A t-test indicates that the sim-"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "ple linear model produced results that were not signiﬁcantly"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "different from human-level performance as determined by"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "annotator’s\ninter-rater\nagreement\n(t = 1.82, p > 0.05)."
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "Similarly,\nlinear model performance was not signiﬁcantly"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "different\nfrom human-level performance when predicting"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "arousal in the video modality with audio features only, and"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "with both audio and visual features. However, none of the"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "linear models were able to predict valence accurately enough"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "to match human-level performance. These\nresults\nagree"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "with existing empirical work on music emotion recognition"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "that\ntypically achieves much higher accuracy for arousal"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "compared to valence [71]."
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "A visual\ninspection of\nthe linear model predictions in"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "Figure 8 reveals that even though the predicted annotations"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "are not signiﬁcantly different\nfrom those of human raters,"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "they are unrealistic looking and much less smooth, possibly"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "because\nthe model does not account\nfor\nfeatures at\nthe"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "preceding timesteps."
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "7.1.2\nPAIR (LSTM) models"
        },
        {
          "compared to valence, possibly because auditory cues relevant": "Table 6 shows the results of\nthe proposed LSTM models"
        },
        {
          "compared to valence, possibly because auditory cues relevant": ""
        },
        {
          "compared to valence, possibly because auditory cues relevant": "for all modality-feature combinations examined using the"
        },
        {
          "compared to valence, possibly because auditory cues relevant": "linear models, and for\nthe three additional architectures"
        },
        {
          "compared to valence, possibly because auditory cues relevant": "(A1, A2, PAIR) described in Section 5.2. Generally,\nthe"
        },
        {
          "compared to valence, possibly because auditory cues relevant": "LSTM models improve over the linear model’s results in"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": "CCC"
        },
        {
          "12": "0.1741 ± 0.2142"
        },
        {
          "12": "0.0675 ± 0.2294"
        },
        {
          "12": "0.0497 ± 0.1523"
        },
        {
          "12": "0.0449 ± 0.1859"
        },
        {
          "12": "0.0734 ± 0.2136"
        },
        {
          "12": "0.0708 ± 0.2191"
        },
        {
          "12": "0.0886 ± 0.2198"
        },
        {
          "12": ""
        },
        {
          "12": ""
        },
        {
          "12": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "information by indicating attributes of\nthe communicator"
        },
        {
          "PAIR architectures in the audiovisual modality.": "such as accessibility. The tempo conveyed through cinematic"
        },
        {
          "PAIR architectures in the audiovisual modality.": "techniques such as camera and subject movement has also"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "been found to be a highly expressive aspect of videos [36]."
        },
        {
          "PAIR architectures in the audiovisual modality.": "It might be possible that\nit\nis not\nthe action of stretching"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "one’s arm that conveys affective information, but the action"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "classiﬁcation model is recognizing latent dimensions such as"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "open gestures and slowed motion that convey anticipation"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "or affective states such as relaxedness that correspond to"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "lowered arousal and valence."
        },
        {
          "PAIR architectures in the audiovisual modality.": "The sign of the coefﬁcient for “sneezing” is also negative."
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "In the\nvideo\nscenes,\n“sneezing”\nseemed to\ncorrespond"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "generally to shots centering on a single actor in which facial"
        },
        {
          "PAIR architectures in the audiovisual modality.": "expressions were clearly visible. One possible reason why the"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "feature could be associated with lower arousal and valence"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "is because the actors depicted were usually trying to express"
        },
        {
          "PAIR architectures in the audiovisual modality.": "seriousness or intensity. The reader may ask: if this were the"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "case, why was the model assigning much higher weights to"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "facial actions than facial expression features, which were also"
        },
        {
          "PAIR architectures in the audiovisual modality.": "available? Action features were extracted from scenes that"
        },
        {
          "PAIR architectures in the audiovisual modality.": "could stretch over longer durations, while facial expressions"
        },
        {
          "PAIR architectures in the audiovisual modality.": "were extracted from still frames sampled from the underlying"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "video at\nregular\nintervals. Consequently, predicted facial"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "expressions could change signiﬁcantly between consecutive"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "sampled frames while predicted action features tended to"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "be more stable. The stability of\nthe action features might"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "hence be a better match to the human annotations of arousal"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "and valence which tend to change smoothly over time. This"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "ﬁnding further justiﬁes the use of\ntime-series models that"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "can incorporate information provided in previous time-steps"
        },
        {
          "PAIR architectures in the audiovisual modality.": ""
        },
        {
          "PAIR architectures in the audiovisual modality.": "for emotion recognition."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13": "that\nin music videos, our perception of arousal\nis largely"
        },
        {
          "13": "determined by the music while both the music and visual"
        },
        {
          "13": "narrative contribute to our perception of valence. Addition-"
        },
        {
          "13": "ally, an individual’s impression of the emotion conveyed by"
        },
        {
          "13": "a song could be very different depending on the modality"
        },
        {
          "13": "in which it\nis\nconsumed. Content platforms may ﬁnd it"
        },
        {
          "13": "useful to incorporate modality-related information into their"
        },
        {
          "13": "search and recommendation algorithms. Similarly, music"
        },
        {
          "13": "video producers might beneﬁt from a greater awareness of"
        },
        {
          "13": "each modality’s contribution to the emotion conveyed by"
        },
        {
          "13": "their content, and to the different modalities in which their"
        },
        {
          "13": "content can be consumed. Of course, music videos are a"
        },
        {
          "13": "subset of audiovisual content and, more broadly, multimodal"
        },
        {
          "13": "content [35]. While music videos usually involve the music"
        },
        {
          "13": "being produced ﬁrst with the visual narrative subsequently"
        },
        {
          "13": "designed around the song, other scenarios are also possible"
        },
        {
          "13": "during audiovisual content production:\nthe video may be"
        },
        {
          "13": "shot ﬁrst, with music selection occurring subsequently, or"
        },
        {
          "13": "the video and music production may proceed in parallel and"
        },
        {
          "13": "inform one another. Investigating isolated modality ratings"
        },
        {
          "13": "in other types of audiovisual and multimodal content is an"
        },
        {
          "13": "important avenue for future work. We hope that our research"
        },
        {
          "13": "and dataset will provide a useful steppdﬁng-stone towards"
        },
        {
          "13": "advancing the ﬁeld of affective computing through the study"
        },
        {
          "13": "of isolated modalities."
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "ACKNOWLEDGMENTS"
        },
        {
          "13": ""
        },
        {
          "13": "This work was supported by the RIE2020 Advanced Manu-"
        },
        {
          "13": "facturing and Engineering (AME) Programmatic Fund (No."
        },
        {
          "13": "A20G8b0102), Singapore, as well as the Singapore Ministry"
        },
        {
          "13": "of Education, Grant no. MOE2018-T2-2-161."
        },
        {
          "13": ""
        },
        {
          "13": ""
        },
        {
          "13": "REFERENCES"
        },
        {
          "13": ""
        },
        {
          "13": "[1]\nM. Zentner, D. Grandjean, and K. R. Scherer, “Emotions evoked by"
        },
        {
          "13": "the sound of music: characterization, classiﬁcation, and measure-"
        },
        {
          "13": ""
        },
        {
          "13": "ment.” Emotion, vol. 8, no. 4, p. 494, 2008."
        },
        {
          "13": ""
        },
        {
          "13": "[2]\nY. Baveye, J.-N. Bettinelli, E. Dellandr´ea, L. Chen, and C. Chamaret,"
        },
        {
          "13": "“A large video database for computational models of\ninduced"
        },
        {
          "13": "on Affective\nemotion,”\nin 2013 Humaine Association Conference"
        },
        {
          "13": "Computing and Intelligent Interaction.\nIEEE, 2013, pp. 13–18."
        },
        {
          "13": ""
        },
        {
          "13": "A. J. Lonsdale and A. C. North, “Why do we listen to music? a uses\n[3]"
        },
        {
          "13": ""
        },
        {
          "13": "and gratiﬁcations analysis,” British journal of psychology, vol. 102,"
        },
        {
          "13": "no. 1, pp. 108–134, 2011."
        },
        {
          "13": "[4]\nA. J. Cohen, “Music as a source of emotion in ﬁlm.” in Handbook"
        },
        {
          "13": ""
        },
        {
          "13": "of music and emotion: Theory, research, applications, P. N. Juslin and"
        },
        {
          "13": ""
        },
        {
          "13": "J. Sloboda, Eds.\nOxford University Press, 2010."
        },
        {
          "13": "[5]\nM. G. Boltz, B. Ebendorf, and B. Field, “Audiovisual interactions:"
        },
        {
          "13": "The impact of visual information on music perception and memory,”"
        },
        {
          "13": "Music Perception, vol. 27, no. 1, pp. 43–59, 2009."
        },
        {
          "13": ""
        },
        {
          "13": "[6]\nP\n. Lamere, “Social tagging and music information retrieval,” Journal"
        },
        {
          "13": ""
        },
        {
          "13": "of new music research, vol. 37, no. 2, pp. 101–114, 2008."
        },
        {
          "13": "[7]\nM.-Y. Chung and H.-S. Kim, “College students’ motivations for"
        },
        {
          "13": "using podcasts,” Journal of Media Literacy Education, vol. 7, no. 3, pp."
        },
        {
          "13": ""
        },
        {
          "13": "13–28, 2015."
        },
        {
          "13": ""
        },
        {
          "13": "[8]\nS. Davies, Musical meaning and expression.\nCornell University Press,"
        },
        {
          "13": "1994."
        },
        {
          "13": "[9]\n“Ifpi\nissues\nannual\nglobal\nmusic\nreport,”"
        },
        {
          "13": "Aug\n2020.\n[Online].\nAvailable:\nhttps://www.ifpi.org/"
        },
        {
          "13": ""
        },
        {
          "13": "ifpi-issues-annual-global-music-report/"
        },
        {
          "13": ""
        },
        {
          "13": "[10]\nJ.\nJ. Gross\nand L. Feldman Barrett,\n“Emotion generation and"
        },
        {
          "13": "emotion regulation: One or two depends on your point of view,”"
        },
        {
          "13": "Emotion review, vol. 3, no. 1, pp. 8–16, 2011."
        },
        {
          "13": "[11]\nJ. A. Russell, “A circumplex model of affect.” Journal of personality"
        },
        {
          "13": ""
        },
        {
          "13": "and social psychology, vol. 39, no. 6, p. 1161, 1980."
        },
        {
          "13": ""
        },
        {
          "13": "[12] P. Ekman, “An argument for basic emotions,” Cognition & emotion,"
        },
        {
          "13": "vol. 6, no. 3-4, pp. 169–200, 1992."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14": "S. Wang and Q. Ji, “Video affective content analysis: a survey of\n[36]"
        },
        {
          "14": "state-of-the-art methods,” IEEE Transactions on Affective Computing,"
        },
        {
          "14": "vol. 6, no. 4, pp. 410–430, 2015."
        },
        {
          "14": "[37]\nJ. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence,"
        },
        {
          "14": "R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and"
        },
        {
          "14": "human-labeled dataset for audio events,” in 2017 IEEE International"
        },
        {
          "14": "Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "14": "2017, pp. 776–780."
        },
        {
          "14": "[38] H. T. P. Thao, B. Balamurali, D. Herremans, and G. Roig, “At-"
        },
        {
          "14": "tendaffectnet: Self-attention based networks for predicting affective"
        },
        {
          "14": "responses from movies,” in 2020 25th International Conference on"
        },
        {
          "14": "Pattern Recognition (ICPR).\nIEEE, 2021, pp. 8719–8726."
        },
        {
          "14": "[39] H. T. P. Thao, B. T. Balamurali, G. Roig,\nand D. Herremans,"
        },
        {
          "14": "“Attendaffectnet–emotion\nprediction\nof movie\nviewers\nusing"
        },
        {
          "14": "multimodal fusion with self-attention,” Sensors, vol. 21, no. 24, p."
        },
        {
          "14": "8356, Dec 2021.\n[Online]. Available: http://dx.doi.org/10.3390/"
        },
        {
          "14": "s21248356"
        },
        {
          "14": ""
        },
        {
          "14": "[40] Y.-H. Yang, Y.-C. Lin, Y.-F. Su, and H. H. Chen, “A regression"
        },
        {
          "14": ""
        },
        {
          "14": "approach to music emotion recognition,” IEEE Transactions on audio,"
        },
        {
          "14": ""
        },
        {
          "14": "speech, and language processing, vol. 16, no. 2, pp. 448–457, 2008."
        },
        {
          "14": ""
        },
        {
          "14": "[41]\nJ.-C. Wang, Y.-H. Yang, H.-M. Wang, and S.-K. Jeng, “Personalized"
        },
        {
          "14": ""
        },
        {
          "14": "music emotion recognition via model adaptation,” in Proceedings"
        },
        {
          "14": ""
        },
        {
          "14": "of The 2012 Asia Paciﬁc Signal and Information Processing Association"
        },
        {
          "14": ""
        },
        {
          "14": "Annual Summit and Conference.\nIEEE, 2012, pp. 1–7."
        },
        {
          "14": ""
        },
        {
          "14": "[42] Y. Baveye, E. Dellandrea, C. Chamaret, and L. Chen, “Liris-accede:"
        },
        {
          "14": ""
        },
        {
          "14": "A video database for affective content analysis,” IEEE Transactions"
        },
        {
          "14": ""
        },
        {
          "14": "on Affective Computing, vol. 6, no. 1, pp. 43–55, 2015."
        },
        {
          "14": ""
        },
        {
          "14": "[43] C. Raffel, “Learning-based methods for comparing sequences, with"
        },
        {
          "14": ""
        },
        {
          "14": "applications\nto audio-to-midi alignment and matching,” Ph.D."
        },
        {
          "14": ""
        },
        {
          "14": "dissertation, Columbia University, 2016."
        },
        {
          "14": ""
        },
        {
          "14": "[44] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, “The"
        },
        {
          "14": ""
        },
        {
          "14": "million song dataset,” in Proceedings of the 12th International Society"
        },
        {
          "14": ""
        },
        {
          "14": "for Music Information Retrieval, 2011."
        },
        {
          "14": ""
        },
        {
          "14": "[45] A. Lykartsis, A. Pysiewicz, H. von Coler,\nand S. Lepa,\n“The"
        },
        {
          "14": ""
        },
        {
          "14": "emotionality of sonic events: testing the geneva emotional music"
        },
        {
          "14": ""
        },
        {
          "14": "scale (gems)\nfor popular and electroacoustic music,” in The 3rd"
        },
        {
          "14": ""
        },
        {
          "14": "International Conference on Music & Emotion, Jyv¨askyl¨a, Finland, June"
        },
        {
          "14": ""
        },
        {
          "14": "11-15, 2013.\nUniversity of Jyv¨askyl¨a, Department of Music, 2013."
        },
        {
          "14": ""
        },
        {
          "14": "[46]\nF. Eyben, M. W ¨ollmer, and B. Schuller, “Opensmile:\nthe munich"
        },
        {
          "14": ""
        },
        {
          "14": "versatile and fast open-source audio feature extractor,” in Proceed-"
        },
        {
          "14": ""
        },
        {
          "14": "ings of the 18th ACM international conference on Multimedia, 2010, pp."
        },
        {
          "14": ""
        },
        {
          "14": "1459–1462."
        },
        {
          "14": ""
        },
        {
          "14": "[47]\nJ. L. Zhang, X. L. Huang, L. F. Yang, Y. Xu, and S. T. Sun, “Feature"
        },
        {
          "14": ""
        },
        {
          "14": "selection and feature\nlearning in arousal dimension of music"
        },
        {
          "14": ""
        },
        {
          "14": "emotion by using shrinkage methods,” Multimedia systems, vol. 23,"
        },
        {
          "14": ""
        },
        {
          "14": "no. 2, pp. 251–264, 2017."
        },
        {
          "14": ""
        },
        {
          "14": "[48] K. W. Cheuk, Y.-J. Luo, B. Balamurali, G. Roig, and D. Herremans,"
        },
        {
          "14": ""
        },
        {
          "14": "“Regression-based music emotion prediction using triplet neural"
        },
        {
          "14": ""
        },
        {
          "14": "networks,” in 2020 International Joint Conference on Neural Networks"
        },
        {
          "14": ""
        },
        {
          "14": "(IJCNN).\nIEEE, 2020, pp. 1–7."
        },
        {
          "14": ""
        },
        {
          "14": "[49]\nF. Eyben, F. Weninger, F. Gross, and B. Schuller, “Recent develop-"
        },
        {
          "14": ""
        },
        {
          "14": "ments in opensmile, the munich open-source multimedia feature"
        },
        {
          "14": ""
        },
        {
          "14": "extractor,” in Proceedings of the 21st ACM international conference on"
        },
        {
          "14": "Multimedia, 2013, pp. 835–838."
        },
        {
          "14": ""
        },
        {
          "14": "[50] Z. Rasheed and M. Shah, “Movie genre classiﬁcation by exploiting"
        },
        {
          "14": ""
        },
        {
          "14": "audio-visual features of previews,” in Object recognition supported by"
        },
        {
          "14": ""
        },
        {
          "14": "user interaction for service robots, vol. 2.\nIEEE, 2002, pp. 1086–1089."
        },
        {
          "14": "[51] Z. Rasheed, Y. Sheikh, and M. Shah, “On the use of computable"
        },
        {
          "14": "features for ﬁlm classiﬁcation,” IEEE Transactions on Circuits and"
        },
        {
          "14": "Systems for Video Technology, vol. 15, no. 1, pp. 52–64, 2005."
        },
        {
          "14": "[52] L. Wilms and D. Oberfeld, “Color and emotion: effects of hue,"
        },
        {
          "14": "saturation, and brightness,” Psychological research, vol. 82, no. 5, pp."
        },
        {
          "14": "896–914, 2018."
        },
        {
          "14": "[53]\nJ. Li, S. Roy, J. Feng, and T. Sim, “Happiness level prediction with"
        },
        {
          "14": "the\nsequential\ninputs via multiple regressions,” in Proceedings of"
        },
        {
          "14": "18th ACM International Conference on Multimodal Interaction, 2016,"
        },
        {
          "14": "pp. 487–493."
        },
        {
          "14": "[54] K. Simonyan and A. Zisserman, “Very deep convolutional networks"
        },
        {
          "14": "for large-scale image recognition,” arXiv preprint arXiv:1409.1556,"
        },
        {
          "14": "2014."
        },
        {
          "14": "[55] C. Chen, Z. Wu, and Y.-G. Jiang, “Emotion in context: Deep seman-"
        },
        {
          "14": "tic feature fusion for video emotion recognition,” in Proceedings"
        },
        {
          "14": "of\nthe 24th ACM international conference on Multimedia, 2016, pp."
        },
        {
          "14": "127–131."
        },
        {
          "14": "[56]\nJ. Lee, S. Kim, S. Kim, J. Park, and K. Sohn, “Context-aware emotion"
        },
        {
          "14": "recognition networks,” in Proceedings of the IEEE/CVF International"
        },
        {
          "14": "Conference on Computer Vision, 2019, pp. 10 143–10 152."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "15": "in Information\nPhoebe Chua received her M.Sc."
        },
        {
          "15": "Systems and Analytics at\nthe National University"
        },
        {
          "15": "of Singapore, and is currently pursuing a PhD in"
        },
        {
          "15": "Information Systems and Analytics with a focus"
        },
        {
          "15": "on computational social science at the National"
        },
        {
          "15": "University of Singapore under\nthe mentorship"
        },
        {
          "15": "of Professor Desmond C. Ong. Her\nresearch"
        },
        {
          "15": "interests include understanding emotion in the"
        },
        {
          "15": "contexts of\ninterpersonal relationships and aes-"
        },
        {
          "15": "thetic experiences."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": "Dr. Dimos (Dimosthenis) Makris is an active"
        },
        {
          "15": "researcher in the ﬁeld of Music Information Re-"
        },
        {
          "15": "trieval. His PhD research includes A.I. applica-"
        },
        {
          "15": "tions for music generation using symbolic data,"
        },
        {
          "15": "dataset creation, and track separation/instrument"
        },
        {
          "15": "recognition tasks. After his PhD, he worked as a"
        },
        {
          "15": "postdoctoral researcher at\nthe Singapore Univer-"
        },
        {
          "15": "sity of Technology and Design under the super-"
        },
        {
          "15": "vision of Assistant Professor Dorien Herremans."
        },
        {
          "15": "He also has experience in the music industry"
        },
        {
          "15": "as a Technical Director of Mercury Orbit Music,"
        },
        {
          "15": "an A.I. music generation start-up company (2017-19, 2021-now), and"
        },
        {
          "15": "as a Recording Engineer/Producer\nfor over seven years. His current"
        },
        {
          "15": "research interests include Deep Learning architectures for conditional"
        },
        {
          "15": "music generation and exploring efﬁcient encoding representations. Finally,"
        },
        {
          "15": "he holds a diploma in Music Theory and is an active piano player."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": "is an Assis-\nDorien Herremans (M’12, SM’17)"
        },
        {
          "15": "tant Professor at Singapore University of Technol-"
        },
        {
          "15": "ogy and Design.\nIn 2015, she was awarded the"
        },
        {
          "15": "individual Marie-Curie Fellowship for Experienced"
        },
        {
          "15": "Researchers, and worked at the Centre for Digital"
        },
        {
          "15": "Music, Queen Mary University of London. Prof."
        },
        {
          "15": "Herremans received her PhD in Applied Eco-"
        },
        {
          "15": "nomics from the University of Antwerp. After grad-"
        },
        {
          "15": "uating as a business engineer\nin management"
        },
        {
          "15": "information systems at\nthe University of Antwerp"
        },
        {
          "15": "in 2005, she worked as a Drupal consultant and"
        },
        {
          "15": "was an IT lecturer at Les Roches University in Bluche, Switzerland."
        },
        {
          "15": "Prof. Herremans’\nresearch focuses on the intersection of machine"
        },
        {
          "15": "learning/optimization and digital music/audio. She is a Senior Member"
        },
        {
          "15": "of\nthe IEEE and co-organizer of\nthe First\nInternational Workshop on"
        },
        {
          "15": "Deep Learning and Music as part of IJCNN, as well as guest editor for"
        },
        {
          "15": "Springer’s Neural Computing and Applications and the Proceedings of"
        },
        {
          "15": "Machine Learning Research. She was featured on the Singapore 100"
        },
        {
          "15": "Women in Technology list\nin 2021."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": "the computer\nGemma Roig is a professor at"
        },
        {
          "15": "science department\nin the Goethe University"
        },
        {
          "15": "Frankfurt since January 2020. Before, she was an"
        },
        {
          "15": "assistant professor at\nthe Singapore University"
        },
        {
          "15": "of Technology and Design. She conducted her"
        },
        {
          "15": "postdoctoral work at MIT in the Center for Brains"
        },
        {
          "15": "Minds and Machines. She pursued her doctoral"
        },
        {
          "15": "degree in Computer Vision at ETH Zurich. Her"
        },
        {
          "15": "research aim is to build computational models"
        },
        {
          "15": "of human vision and cognition to understand its"
        },
        {
          "15": "underlying principles, and to use those models to"
        },
        {
          "15": "build applications of artiﬁcial\nintelligence."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "15": "in Information\nPhoebe Chua received her M.Sc."
        },
        {
          "15": "Systems and Analytics at\nthe National University"
        },
        {
          "15": "of Singapore, and is currently pursuing a PhD in"
        },
        {
          "15": "Information Systems and Analytics with a focus"
        },
        {
          "15": "on computational social science at the National"
        },
        {
          "15": "University of Singapore under\nthe mentorship"
        },
        {
          "15": "of Professor Desmond C. Ong. Her\nresearch"
        },
        {
          "15": "interests include understanding emotion in the"
        },
        {
          "15": "contexts of\ninterpersonal relationships and aes-"
        },
        {
          "15": "thetic experiences."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": "Dr. Dimos (Dimosthenis) Makris is an active"
        },
        {
          "15": "researcher in the ﬁeld of Music Information Re-"
        },
        {
          "15": "trieval. His PhD research includes A.I. applica-"
        },
        {
          "15": "tions for music generation using symbolic data,"
        },
        {
          "15": "dataset creation, and track separation/instrument"
        },
        {
          "15": "recognition tasks. After his PhD, he worked as a"
        },
        {
          "15": "postdoctoral researcher at\nthe Singapore Univer-"
        },
        {
          "15": "sity of Technology and Design under the super-"
        },
        {
          "15": "vision of Assistant Professor Dorien Herremans."
        },
        {
          "15": "He also has experience in the music industry"
        },
        {
          "15": "as a Technical Director of Mercury Orbit Music,"
        },
        {
          "15": "an A.I. music generation start-up company (2017-19, 2021-now), and"
        },
        {
          "15": "as a Recording Engineer/Producer\nfor over seven years. His current"
        },
        {
          "15": "research interests include Deep Learning architectures for conditional"
        },
        {
          "15": "music generation and exploring efﬁcient encoding representations. Finally,"
        },
        {
          "15": "he holds a diploma in Music Theory and is an active piano player."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": "is an Assis-\nDorien Herremans (M’12, SM’17)"
        },
        {
          "15": "tant Professor at Singapore University of Technol-"
        },
        {
          "15": "ogy and Design.\nIn 2015, she was awarded the"
        },
        {
          "15": "individual Marie-Curie Fellowship for Experienced"
        },
        {
          "15": "Researchers, and worked at the Centre for Digital"
        },
        {
          "15": "Music, Queen Mary University of London. Prof."
        },
        {
          "15": "Herremans received her PhD in Applied Eco-"
        },
        {
          "15": "nomics from the University of Antwerp. After grad-"
        },
        {
          "15": "uating as a business engineer\nin management"
        },
        {
          "15": "information systems at\nthe University of Antwerp"
        },
        {
          "15": "in 2005, she worked as a Drupal consultant and"
        },
        {
          "15": "was an IT lecturer at Les Roches University in Bluche, Switzerland."
        },
        {
          "15": "Prof. Herremans’\nresearch focuses on the intersection of machine"
        },
        {
          "15": "learning/optimization and digital music/audio. She is a Senior Member"
        },
        {
          "15": "of\nthe IEEE and co-organizer of\nthe First\nInternational Workshop on"
        },
        {
          "15": "Deep Learning and Music as part of IJCNN, as well as guest editor for"
        },
        {
          "15": "Springer’s Neural Computing and Applications and the Proceedings of"
        },
        {
          "15": "Machine Learning Research. She was featured on the Singapore 100"
        },
        {
          "15": "Women in Technology list\nin 2021."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": "the computer\nGemma Roig is a professor at"
        },
        {
          "15": "science department\nin the Goethe University"
        },
        {
          "15": "Frankfurt since January 2020. Before, she was an"
        },
        {
          "15": "assistant professor at\nthe Singapore University"
        },
        {
          "15": "of Technology and Design. She conducted her"
        },
        {
          "15": "postdoctoral work at MIT in the Center for Brains"
        },
        {
          "15": "Minds and Machines. She pursued her doctoral"
        },
        {
          "15": "degree in Computer Vision at ETH Zurich. Her"
        },
        {
          "15": "research aim is to build computational models"
        },
        {
          "15": "of human vision and cognition to understand its"
        },
        {
          "15": "underlying principles, and to use those models to"
        },
        {
          "15": "build applications of artiﬁcial\nintelligence."
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        },
        {
          "15": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "at\nthe Yong Siew Toh Conservatory of Music"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "(YSTCM) at\nthe National University of Singapore"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "(NUS), with a joint appointment at Yale-NUS Col-"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "lege. Previously, she founded the Music Cognition"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "group at the Institute of High Performance Com-"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "puting, AQ∗STAR,\nin Singapore. Kat\nreceived"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "her PhD in Psychology (with a graduate minor"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "in Cognitive Science) from Cornell University in"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "2013, and holds a bachelor’s degree in Cog-"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "nitive Psychology and Cello Performance from"
        },
        {
          "is an Assistant Professor\nKat R. Agres (M’17)": "Carnegie Mellon University. Her postdoctoral research was conducted at"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "her PhD in Psychology (with a graduate minor"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "in Cognitive Science) from Cornell University in"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "2013, and holds a bachelor’s degree in Cog-"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "nitive Psychology and Cello Performance from"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "Carnegie Mellon University. Her postdoctoral research was conducted at"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "Queen Mary University of London, in the areas of Music Cognition and"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "Computational Creativity. She has received funding from the National\nIn-"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "stitute of Health (NIH), the European Commission’s Future and Emerging"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "Technologies (FET) program, and the Agency for Science, Technology"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "and Research (Singapore), amongst others,\nto support her\nresearch."
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "Currently, Kat’s research explores a range of\ntopics including music"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "technology for healthcare and well-being, music perception and cognition,"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "computational modeling of learning and memory, and automatic music"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "generation. She has presented her work in over ﬁfteen countries across"
        },
        {
          "puting, AQ∗STAR,\nin Singapore. Kat\nreceived": "four continents, and remains an active cellist\nin Singapore."
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions evoked by the sound of music: characterization, classification, and measurement",
      "authors": [
        "M Zentner",
        "D Grandjean",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Emotion"
    },
    {
      "citation_id": "2",
      "title": "A large video database for computational models of induced emotion",
      "authors": [
        "Y Baveye",
        "J.-N Bettinelli",
        "E Dellandréa",
        "L Chen",
        "C Chamaret"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "3",
      "title": "Why do we listen to music? a uses and gratifications analysis",
      "authors": [
        "A Lonsdale",
        "A North"
      ],
      "year": "2011",
      "venue": "British journal of psychology"
    },
    {
      "citation_id": "4",
      "title": "Music as a source of emotion in film",
      "authors": [
        "A Cohen"
      ],
      "year": "2010",
      "venue": "Handbook of music and emotion: Theory, research"
    },
    {
      "citation_id": "5",
      "title": "Audiovisual interactions: The impact of visual information on music perception and memory",
      "authors": [
        "M Boltz",
        "B Ebendorf",
        "B Field"
      ],
      "year": "2009",
      "venue": "Music Perception"
    },
    {
      "citation_id": "6",
      "title": "Social tagging and music information retrieval",
      "authors": [
        "P Lamere"
      ],
      "year": "2008",
      "venue": "Journal of new music research"
    },
    {
      "citation_id": "7",
      "title": "College students' motivations for using podcasts",
      "authors": [
        "M.-Y Chung",
        "H.-S Kim"
      ],
      "year": "2015",
      "venue": "Journal of Media Literacy Education"
    },
    {
      "citation_id": "8",
      "title": "Musical meaning and expression",
      "authors": [
        "S Davies"
      ],
      "year": "1994",
      "venue": "Musical meaning and expression"
    },
    {
      "citation_id": "9",
      "title": "Ifpi issues annual global music report",
      "year": "2020",
      "venue": "Ifpi issues annual global music report"
    },
    {
      "citation_id": "10",
      "title": "Emotion generation and emotion regulation: One or two depends on your point of view",
      "authors": [
        "J Gross",
        "L Barrett"
      ],
      "year": "2011",
      "venue": "Emotion review"
    },
    {
      "citation_id": "11",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "12",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "13",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "14",
      "title": "Core affect and the psychological construction of emotion",
      "authors": [
        "J Russell"
      ],
      "year": "2003",
      "venue": "Psychological review"
    },
    {
      "citation_id": "15",
      "title": "Seeing stars of valence and arousal in blog posts",
      "authors": [
        "G Paltoglou",
        "M Thelwall"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Y Kim",
        "E Schmidt",
        "R Migneco",
        "B Morton",
        "P Richardson",
        "J Scott",
        "J Speck",
        "D Turnbull"
      ],
      "year": "2010",
      "venue": "Proc. ismir"
    },
    {
      "citation_id": "17",
      "title": "When the eye listens: A meta-analysis of how audio-visual presentation enhances the appreciation of music performance",
      "authors": [
        "F Platz",
        "R Kopiez"
      ],
      "year": "2012",
      "venue": "Music Perception: An Interdisciplinary Journal"
    },
    {
      "citation_id": "18",
      "title": "Sight over sound in the judgment of music performance",
      "authors": [
        "C.-J Tsay"
      ],
      "year": "2013",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "19",
      "title": "Musical soundtracks as a schematic influence on the cognitive processing of filmed events",
      "authors": [
        "M Boltz"
      ],
      "year": "2001",
      "venue": "Music Perception"
    },
    {
      "citation_id": "20",
      "title": "Music, music videos, and wear out",
      "authors": [
        "M Goldberg",
        "A Chattopadhyay",
        "G Gorn",
        "J Rosenblatt"
      ],
      "year": "1993",
      "venue": "Psychology & Marketing"
    },
    {
      "citation_id": "21",
      "title": "The adolescent audience for music videos and why they watch",
      "authors": [
        "S.-W Sun",
        "J Lull"
      ],
      "year": "1986",
      "venue": "Journal of Communication"
    },
    {
      "citation_id": "22",
      "title": "The amg1608 dataset for music emotion recognition",
      "authors": [
        "Y.-A Chen",
        "Y.-H Yang",
        "J.-C Wang",
        "H Chen"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "A Aljanaki",
        "Y.-H Yang",
        "M Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "24",
      "title": "Studying emotion induced by music through a crowdsourcing game",
      "authors": [
        "A Aljanaki",
        "F Wiering",
        "R Veltkamp"
      ],
      "year": "2016",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "25",
      "title": "Modeling musical emotion dynamics with conditional random fields",
      "authors": [
        "E Schmidt",
        "Y Kim"
      ],
      "year": "2011",
      "venue": "ISMIR"
    },
    {
      "citation_id": "26",
      "title": "Assessing the effectiveness of a large database of emotion-eliciting films: A new tool for emotion researchers",
      "authors": [
        "A Schaefer",
        "F Nils",
        "X Sanchez",
        "P Philippot"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "27",
      "title": "Deep learning vs. kernel methods: Performance for emotion prediction in videos",
      "authors": [
        "Y Baveye",
        "E Dellandréa",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "2015 international conference on affective computing and intelligent interaction (acii)"
    },
    {
      "citation_id": "28",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "29",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "30",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "31",
      "title": "Review of data features-based music emotion recognition methods",
      "authors": [
        "X Yang",
        "Y Dong",
        "J Li"
      ],
      "year": "2018",
      "venue": "Multimedia systems"
    },
    {
      "citation_id": "32",
      "title": "Affective video content analysis: A multidisciplinary insight",
      "authors": [
        "Y Baveye",
        "C Chamaret",
        "E Dellandréa",
        "L Chen"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "34",
      "title": "Modeling emotion in complex stories: the stanford emotional narratives dataset",
      "authors": [
        "D Ong",
        "Z Wu",
        "Z.-X Tan",
        "M Reddan",
        "I Kahhale",
        "A Mattek",
        "J Zaki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "36",
      "title": "Video affective content analysis: a survey of state-of-the-art methods",
      "authors": [
        "S Wang",
        "Q Ji"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Attendaffectnet: Self-attention based networks for predicting affective responses from movies",
      "authors": [
        "H Thao",
        "B Balamurali",
        "D Herremans",
        "G Roig"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "39",
      "title": "Attendaffectnet-emotion prediction of movie viewers using multimodal fusion with self-attention",
      "authors": [
        "H Thao",
        "B Balamurali",
        "G Roig",
        "D Herremans"
      ],
      "year": "2021",
      "venue": "Sensors",
      "doi": "10.3390/s21248356"
    },
    {
      "citation_id": "40",
      "title": "A regression approach to music emotion recognition",
      "authors": [
        "Y.-H Yang",
        "Y.-C Lin",
        "Y.-F Su",
        "H Chen"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "41",
      "title": "Personalized music emotion recognition via model adaptation",
      "authors": [
        "J.-C Wang",
        "Y.-H Yang",
        "H.-M Wang",
        "S.-K Jeng"
      ],
      "year": "2012",
      "venue": "Proceedings of The 2012 Asia Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "42",
      "title": "Liris-accede: A video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching",
      "authors": [
        "C Raffel"
      ],
      "year": "2016",
      "venue": "Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching"
    },
    {
      "citation_id": "44",
      "title": "The million song dataset",
      "authors": [
        "T Bertin-Mahieux",
        "D Ellis",
        "B Whitman",
        "P Lamere"
      ],
      "year": "2011",
      "venue": "Proceedings of the 12th International Society for Music Information Retrieval"
    },
    {
      "citation_id": "45",
      "title": "The emotionality of sonic events: testing the geneva emotional music scale (gems) for popular and electroacoustic music",
      "authors": [
        "A Lykartsis",
        "A Pysiewicz",
        "H Coler",
        "S Lepa"
      ],
      "year": "2013",
      "venue": "The 3rd International Conference on Music & Emotion"
    },
    {
      "citation_id": "46",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "47",
      "title": "Feature selection and feature learning in arousal dimension of music emotion by using shrinkage methods",
      "authors": [
        "J Zhang",
        "X Huang",
        "L Yang",
        "Y Xu",
        "S Sun"
      ],
      "year": "2017",
      "venue": "Multimedia systems"
    },
    {
      "citation_id": "48",
      "title": "Regression-based music emotion prediction using triplet neural networks",
      "authors": [
        "K Cheuk",
        "Y.-J Luo",
        "B Balamurali",
        "G Roig",
        "D Herremans"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "49",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "50",
      "title": "Movie genre classification by exploiting audio-visual features of previews",
      "authors": [
        "Z Rasheed",
        "M Shah"
      ],
      "year": "2002",
      "venue": "Object recognition supported by user interaction for service robots"
    },
    {
      "citation_id": "51",
      "title": "On the use of computable features for film classification",
      "authors": [
        "Z Rasheed",
        "Y Sheikh",
        "M Shah"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "52",
      "title": "Color and emotion: effects of hue, saturation, and brightness",
      "authors": [
        "L Wilms",
        "D Oberfeld"
      ],
      "year": "2018",
      "venue": "Psychological research"
    },
    {
      "citation_id": "53",
      "title": "Happiness level prediction with sequential inputs via multiple regressions",
      "authors": [
        "J Li",
        "S Roy",
        "J Feng",
        "T Sim"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "54",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "55",
      "title": "Emotion in context: Deep semantic feature fusion for video emotion recognition",
      "authors": [
        "C Chen",
        "Z Wu",
        "Y.-G Jiang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM international conference on Multimedia"
    },
    {
      "citation_id": "56",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "57",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "58",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "59",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "61",
      "title": "A learning algorithm for continually running fully recurrent neural networks",
      "authors": [
        "R Williams",
        "D Zipser"
      ],
      "year": "1989",
      "venue": "Neural computation"
    },
    {
      "citation_id": "62",
      "title": "Understanding lstm networks",
      "authors": [
        "C Olah"
      ],
      "year": "2015",
      "venue": "Understanding lstm networks"
    },
    {
      "citation_id": "63",
      "title": "On-line continuous-time music mood regression with deep recurrent neural networks",
      "authors": [
        "F Weninger",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "64",
      "title": "Automatically estimating emotion in music with deep long-short term memory recurrent neural networks",
      "authors": [
        "E Coutinho",
        "G Trigeorgis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "CEUR Workshop Proceedings"
    },
    {
      "citation_id": "65",
      "title": "Multimodal deep models for predicting affective responses evoked by movies",
      "authors": [
        "H Thao",
        "D Herremans",
        "G Roig"
      ],
      "year": "2019",
      "venue": "ICCV Workshops"
    },
    {
      "citation_id": "66",
      "title": "Audiovisual integration of emotional signals in voice and face: an event-related fmri study",
      "authors": [
        "B Kreifelts",
        "T Ethofer",
        "W Grodd",
        "M Erb",
        "D Wildgruber"
      ],
      "year": "2007",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "67",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "68",
      "title": "Tensorflow: A system for large-scale machine learning",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen",
        "Z Chen",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "G Irving",
        "M Isard"
      ],
      "year": "2016",
      "venue": "12th {USENIX} symposium on operating systems design and implementation"
    },
    {
      "citation_id": "69",
      "title": "Lasso: a feature selection technique in predictive modeling for machine learning",
      "authors": [
        "R Muthukrishnan",
        "R Rohini"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on advances in computer applications (ICACA)"
    },
    {
      "citation_id": "70",
      "title": "Primitivesbased evaluation and estimation of emotions in speech",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "E Mower",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Speech communication"
    },
    {
      "citation_id": "71",
      "title": "A closed-loop, music-based brain-computer interface for emotion mediation",
      "authors": [
        "S Ehrlich",
        "K Agres",
        "C Guan",
        "G Cheng"
      ],
      "year": "2019",
      "venue": "PloS one",
      "doi": "10.1371/journal.pone.0213516"
    },
    {
      "citation_id": "72",
      "title": "Toward a minimal representation of affective gestures",
      "authors": [
        "D Glowinski",
        "N Dael",
        "A Camurri",
        "G Volpe",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "73",
      "title": "Bodily expression of emotion",
      "authors": [
        "H Wallbott"
      ],
      "year": "1998",
      "venue": "European journal of social psychology"
    },
    {
      "citation_id": "74",
      "title": "Nonverbal communication",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Nonverbal communication"
    },
    {
      "citation_id": "75",
      "title": "Music emotion recognition: The role of individuality",
      "authors": [
        "Y.-H Yang",
        "Y.-F Su",
        "Y.-C Lin",
        "H Chen"
      ],
      "year": "2007",
      "venue": "Proceedings of the international workshop on Human-centered multimedia"
    },
    {
      "citation_id": "76",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "77",
      "title": "Visual dominance: an information-processing account of its origins and significance",
      "authors": [
        "M Posner",
        "M Nissen",
        "R Klein"
      ],
      "year": "1976",
      "venue": "Psychological review"
    },
    {
      "citation_id": "78",
      "title": "Morpheus: generating structured music with constrained patterns and tension",
      "authors": [
        "D Herremans",
        "E Chew"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "79",
      "title": "Music fadernets: Controllable music generation based on high-level features via low-level feature modelling",
      "authors": [
        "H Tan",
        "D Herremans"
      ],
      "year": "2020",
      "venue": "Proceedings of The International Society for Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "80",
      "title": "Generating lead sheets with affect: A novel conditional seq2seq framework",
      "authors": [
        "D Makris",
        "K Agres",
        "D Herremans"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    }
  ]
}