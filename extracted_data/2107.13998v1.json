{
  "paper_id": "2107.13998v1",
  "title": "\"Excavating Ai\" Re-Excavated: Debunking A Fallacious Account Of The Jaffe Dataset",
  "published": "2021-07-28T01:31:59Z",
  "authors": [
    "Michael J. Lyons"
  ],
  "keywords": [
    "facial expression",
    "machine learning",
    "training sets",
    "affective computing",
    "artificial intelligence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Twenty-five years ago, my colleagues Miyuki Kamachi and Jiro Gyoba and I designed and photographed JAFFE, a set of facial expression images intended for use in a study of face perception. In 2019, without seeking permission or informing us, Kate Crawford and Trevor Paglen exhibited JAFFE in two widely publicized art shows. In addition, they published a nonfactual account of the images in the essay \"Excavating AI: The Politics of Images in Machine Learning Training Sets.\" The present article recounts the creation of the JAFFE dataset and unravels each of Crawford and Paglen's fallacious statements. I also discuss JAFFE more broadly in connection with research on facial expression, affective computing, and human-computer interaction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "What Is Jaffe?",
      "text": "JAFFE-The Japanese Female Facial Expression dataset-is a set of images depicting facial expressions posed by Japanese women, accompanied by semantic ratings on nouns describing the expressions. JAFFE was created specifically for use in an experiment that compared a biologically-inspired representation of facial appearance  [30, 31, 40]  with facial expression perception by human observers  [27, 28, 29, 39] . JAFFE is described as a dataset because it includes observer ratings for each facial expression image. Without this data, it would be more accurate to describe JAFFE as an image set or stimulus set. We planned and assembled JAFFE in 1996 in collaboration with Miyuki Kamachi and Jiro Gyoba of the Psychology Department at Kyushu University, Japan. Miyuki recruited volunteers in the Psychology Department and asked them to pose three or four examples of six facial expressions (happiness, sadness, surprise, anger, disgust, fear) and a neutral or expressionless face.  5  These six expressions are the basic facial expressions (BFE) studied by Paul Ekman and many others and thought by some of these scientists to be recognizable, to an extent, in all cultures  [18] .  6 JAFFE volunteers posed the requested expressions while facing the camera through a transparent, semi-reflective plastic pane  [39]  and triggered the shutter remotely while viewing the reflection of their face-the JAFFE images are selfies. Unlike some other facial expression image sets (see, for example,  [17, 42] ), volunteers were not coached to pose specified facial configurations or to imitate prototypes, nor were they trained in the facial action coding system (FACS)  [19] ). The volunteers were given a Japanese adjectival noun for each of the six basic facial expressions and asked to photograph three or four poses for each of these while attending to their reflected expressions. JAFFE images did not undergo a selection procedure, as with, for example, Ekman's POFA (Pictures of Facial Affect) set, for which an iterative procedure selected the most prototypical images for each basic facial expression  [17] . JAFFE includes nearly all of the photographs posed by all of the volunteers.  7 Because of this, the facial configuration for each expression category varies considerably between poses (figure  2 ) and between posers (figure  3 ).\n\nWe created JAFFE to experimentally test a mathematical model of facial expression representation derived from a simplified description of the receptive fields of neurons in the visual cortex  [29, 30, 39, 40] . We aimed to compare the model with human perception. To reduce arbitrary assumptions, we compared image similarity derived from the model with similarity measured from human observations of the facial expression images. We did not use categorical facial expression labels directly in the comparison. Instead, we obtained an empirical six-dimensional description for each image by asking observers to view expression images and rate the perceived expression using a five-level Likert scale for each of the six adjectival nouns used to request poses from the volunteers. Images were viewed and rated by 60 Japanese female observers (volunteers recruited at Kyushu University), and the responses averaged. Figure  2  shows sample mean ratings for two JAFFE images. The semantic ratings afford a more detailed and nuanced description of the images than that obtained with the more commonly used forced categorization approach  [5] . Similarity calculated from the multidimensional ratings was correlated with similarity derived from our mathematical model. Model and empirical similarity measures were also analyzed using non-metric multidimensional scaling, revealing low dimensional structure resembling the affective circumplex  [47, 50] .\n\nThe approach outlined above does not require the JAFFE expressions to be prototypical. Strictly speaking, neither was it required that the volunteers pose authentic or felt expressions because we compared data on the observers' perceptions with the mathematical model, not the poser's intended expression. Category labels are not directly used in our analysis  [39] . This does leave open the question of how posed expression images relate to facial expressions encountered 'in the wild,' and we were fully aware that our research project did not address this point.\n\nA large number of scientific studies in the published literature make use of posed expressions  [5] : our study and JAFFE are not unusual in this respect. As it happens, Lisa Feldman Barrett, whom Crawford and Paglen often cite as an Ekman critic, distributes the IASLab Face Set, a dataset of posed facial expression images categorized and labelled with emotion nouns.  8  IASLab does not include multidimensional semantic ratings data: would it not serve better than JAFFE as the 'Anatomical Model' for \"Excavating AI\"?",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cultural Specificity Of Jaffe",
      "text": "As one might expect, JAFFE exhibits cultural specificity. At the outset of our project, when we were thinking through the design of JAFFE, Jiro Gyoba, an experienced perceptual psychologist, warned that we could not expect Japanese subjects to recognize images of fear poses reliably. Previous studies of facial expression recognition involving Japanese subjects  [48]  reported such findings. Most experiments have found that negative valence expressions are recognized less reliably in Japan than in Western cultures. This effect is most significant with fear expressions, where poser-observer agreement is measured to be just above chance.\n\nNevertheless, we included fear poses in the experiments. Our experimental observations confirmed Jiro's prediction (see Table  1  and figure  3 ): with fear poses, the agreement between the intended expression and the observer ratings is slightly above chance (17%). Fear poses are rated more strongly as surprise and disgust than fear itself (Table  1 ). These findings, however, did not change the conclusions of our study because we did not force categorization of the facial expressions. In parallel with the measurement on six expression words, we carried out independent measurements from a separate group of observers that did not include fear poses or ratings.  included fear poses and ratings, the visual-cortex-inspired model showed a significantly higher correlation with observer ratings than the control, a similarity measure derived from feature-point geometry.\n\nWe confirmed the cultural specificity of the JAFFE expressions a few years later in a collaborative cross-cultural study of facial expression judgments by Americans and Japanese  [13, 14] . The study found, for example, that the anger pose shown in the left panel of figure  4  was classified as anger by 82% of Japanese study participants but only 34% of American participants. Similarly, the disgust pose shown in the right panel of figure  4  was classified as disgust by 66% of Japanese participants but only 18% of American participants  [13] .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Jaffe As A Training Set",
      "text": "We first reported the results of our project at the May 1997 edition of ARVO 9  a vision science conference held in Florida  [29] , and at a domestic cognitive science meeting held in Okinawa in June 1997  [28] . We learned that the IEEE Face and Gesture conference would take place not far from ATR,  10  and prepared an article for submission. The article  [27]  was shortlisted for a best paper award and invited to be published in a special issue of the Elsevier journal Image and Vision Computing.  11 After hearing my talk, Zhengyou Zhang, a computer vision researcher who visited ATR in 1997, suggested we extend the visual-cortex-inspired model by training and testing a multi-layer neural network to classify the JAFFE facial expression images. By leveraging existing work, the project progressed quickly, and an article  [58]  was submitted to the same edition of the IEEE Face and Gesture conference. Zhang compared the classification rates obtained with the neural network directly with the poser-rater agreement levels (see table  1 ) to conclude that the neural network had exceeded human performance. However, this comparison was misleading because the neural network generalization rate measurement had not distinguished individual posers. Moreover, the human observers had rated the facial expression images without any initial instruction on poser intentions, whereas the neural network model was exposed to intended pose labels during training via supervised learning. In other words, the human observers were not 'trained' using the pose labels. In accord with my objections, the results of a subsequent study showed that we could equal or slightly exceed the performance of the multilayer neural network using a simple statistical model, while the cross-individual generalization rate was significantly lower  [37] .\n\nIn the interests of an Open Data policy  [44] , we decided to provide JAFFE images and ratings to others for use in non-commercial scientific research. To my surprise, JAFFE became a frequently used benchmark dataset in academic research on facial expression classification algorithms. By 2007, such studies had become so numerous that I publicly questioned the value of optimizing expression classification on benchmark datasets and cautioned that this was not the same research problem as emotion recognition  [34] .  12 Despite the impression given by \"Excavating AI,\" JAFFE is not used exclusively for machine learning research but also in many other fields, including visual psychology, cognitive science, and experimental neuroscience.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "'Archaeology' Of Jaffe: The Bones Of Contention",
      "text": "Crawford and Paglen adopted JAFFE as their 'anatomical model' of machine learning training sets for the essay \"Excavating AI: The Politics of Images in Machine Learning Training Sets\"  [12] . The relevant discussion is in the section titled \"Anatomy of a Training Set,\" that they intend to serve as an exposition of what they describe as their 'dataset archaeology methodology.'\n\nNearly every statement about JAFFE in this important theoretical section of \"Excavating AI\" is unequivocally false. In the following, excerpts from Crawford and Paglen's account of JAFFE are quoted verbatim, in italics. My commentary, in plain font, follows each excerpt. The excerpts are quoted from the web version  [12] . The text of the AI & Society article is nearly identical.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotional States",
      "text": "The dataset contains photographs of 10 Japanese female models making seven facial expressions that are meant to correlate with seven basic emotional states.\n\nWe asked volunteers to freely pose six basic facial expressions as well as an expressionless face. We did not instruct or guide the volunteers to imitate specific prototypical facial action configurations. The volunteers were not 'models.'",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Intended Purpose",
      "text": "The intended purpose of the dataset is to help machine-learning systems recognize and label these emotions for newly captured, unlabelled images.\n\nJAFFE was designed and photographed to compare a mathematical model of facial expression representation with data from human observers  [27, 28, 29, 39] . Our project did not train a machine learning system, nor did it 'label emotions for newly captured images.' A central concern of \"Excavating AI\" is the critique of image taxonomy; ironically, Crawford and Paglen's mistaken description of JAFFE's origins as a machine-learning training set is itself a taxonomic error.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Depicting Emotions",
      "text": "The implicit, top-level taxonomy here is something like \"facial expressions depicting the emotions of Japanese women.\"\n\nWe have never described JAFFE as anything but a set of images of posed facial expressions. The accompanying article  [27, 39]  describes in detail the procedure for photographing the JAFFE images. We made no explicit or implicit claim that these depict felt emotions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Organizing Buckets",
      "text": "If we go down a level from taxonomy, we arrive at the level of the class. In the case of JAFFE, those classes are happiness, sadness, surprise, disgust, fear, anger, and neutral. These categories become the organizing buckets into which all of the individual images are stored. They are the distinct concepts used to order the underlying images.\n\nThe title of each JAFFE image file is composed of a two-letter volunteer identifier, two-letter pose identifier, pose number, and image number. The distribution includes a README_FIRST file containing multidimensional ratings for each image based on sixty Japanese observers' judgments. README_FIRST contains the following explicit caveat: . . . it is important to realize that the expressions are never pure expressions of one emotion but always admixtures of different emotions. The image expression labels represent the predominant expression in that image-the expression that we asked the subject to pose.\n\nIntended pose may not agree with the observer ratings, as in the right panel of figure  3 . Crawford and Paglen's concept of categorical 'organizing buckets' ignores the nuanced description available from the subjective rating data and is a misleading characterization of JAFFE.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotions As Visual Concepts",
      "text": "There are several implicit assertions in the JAFFE set. First there's the taxonomy itself: that \"emotions\" is a valid set of visual concepts.\n\nJAFFE makes no such assertion, explicitly or implicitly. Our study modelled ratings by observers, not the emotions or mental states of those posing the expressions. Uninformed users of JAFFE might assume this about the image labels if they have not read and understood README_FIRST and the article that describes JAFFE  [27] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotions Applied To Photographs",
      "text": "Then there's a string of additional assumptions: that the concepts within \"emotions\" can be applied to photographs of people's faces specifically Japanese women) . . .\n\nWe made no such claims about the emotions of the volunteers who posed the expressions. Our study modelled perceived expression based on observer ratings, not felt emotion. Crawford and Paglen have misunderstood this point. Our measurement protocol (semantic rating on expression nouns) is not unusual but widely used-the misunderstanding is symptomatic of a lack of basic familiarity with the published literature on the psychology of facial expression.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "Six Emotions",
      "text": ". . . that there are six emotions plus a neutral state . . . Our work is concerned with the visual perception of facial expressions, not emotions of the posers. We have never stated that there are only six emotions or facial expressions, a nonsensical claim.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "True Emotional State",
      "text": "... there is a fixed relationship between a person's facial expression and her true emotional state ... This premise has nothing to do with JAFFE, which is instead characterized by subjective ratings data and accompanied by our explicit caveat about the meaning of the pose labels.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Consistent And Uniform",
      "text": ". . . and that this relationship between the face and the emotion is consistent, measurable, and uniform across the women in the photographs.\n\nThis premise cannot derive from actual observation of JAFFE images and data. Look again at the image samples and ratings in figures 1, 2, 3, and 4. Each volunteer posed several variations for each facial expression. No two images in the JAFFE set have identical facial expressions or rating values.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Neutral Facial Expression",
      "text": "At the level of the class, we find assumptions such as \"there is such a thing as a \"neutral\" facial expression While the concept of a neutral facial expression is not meaningless, the ratings data indicate that none of the images depicting expressionless (無表情) poses were perceived by the observers as devoid of expression.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Significant Six Emotional States",
      "text": "and \"the significant six emotional states are happy, sad, angry, disgusted, afraid, surprised.\"\n\nCrawford and Paglen have failed to understand a fundamental aspect of our research project: it is concerned with the visual appearance of facial expressions in images, not 'emotional states.' Regarding the significance of the facial expression categories chosen for the study, these are, of course, the Basic Facial Expressions (BFEs) used in many psychological studies of facial expression, not only by the Ekman camp. The use of these expression categories in research dates back at least to Charles Darwin's work  [15] . Our project did not assume that these are the only expressions, nor did we make any such claim. We were well aware of criticism of BFE theory. Indeed one of the aims of our work was to compare categorical and dimensional descriptions of facial expressions  [36, 39] . Furthermore, our project did not rely on the validity of the BFE paradigm.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Interior State",
      "text": "At the level of labelled image, there are other implicit assumptions such as \"this particular photograph depicts a woman with an 'angry' facial expression,\" rather than, for example, the fact that this is an image of a woman mimicking an angry expression. These, of course, are all \"performed\" expressions-not relating to any interior state, but acted out in a laboratory setting.\n\nOne wonders if Crawford and Paglen have looked at any of the documentation associated with JAFFE. We clearly describe the images as depicting posed facial expressions. Our article clearly describes the laboratory conditions under which we photographed JAFFE  [27] . There is no claim or implication that JAFFE images depict natural expressions, and there is no discussion of 'interior state.'",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Implicit Claims",
      "text": "Every one of the implicit claims made at each level is, at best, open to question, and some are deeply contested.\n\nAn implication is a compound statement having the form X implies Y. But Crawford and Paglen's statements concerning JAFFE take the form 'Y is implied,' without specifying a premise, X, and without providing the slightest explanation about how these unknown factors imply the alleged 'claims.' All of the allegations about JAFFE's 'implicit claims' are furnished as self-evident conclusions made without evidence, elaboration, or reference to statements in the relevant publications.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Relative Modesty",
      "text": "The JAFFE training set is relatively modest as far as contemporary training sets go. It was created before the advent of social media, before developers were able to scrape images from the internet at scale, Crawford and Paglen add to the fiction that we intended JAFFE as a training set. The fabulation continues: JAFFE is a scale-challenged 'modest' training set, stunted by the impossibility of scraping for images. Images scraped online would not have been suitable for the project. JAFFE was designed and explicitly photographed for our experiment. The relatively small size of the JAFFE set had nothing to do with a lack of big data: we chose the numbers of volunteers and poses to permit a statistically significant comparison of two measures of facial expression similarity  [39] . Had we wished to design an image dataset for facial expression machine learning, we would have recruited more volunteers.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "The Straw Men",
      "text": "Each of the alleged 'implicit claims' about JAFFE is a misrepresentation of our work. As a member of the team that created JAFFE, I can state unequivocally that these allegations do not correspond with our views or intentions.\n\nHow did Crawford and Paglen arrive at such a mistaken account of our work? It appears that the veracity of their allegations about JAFFE was not a priority. Instead, these serve a mainly rhetorical function. Examining the list of statements reveals a pattern:\n\n• Each statement is made without evidence (e.g. 'JAFFE is intended as a machine learning training set') • Each of the 'implied claims' is easy to refute without specialized knowledge (e.g. 'posed images depict interior states') • Some of the 'implied claims' are ridiculous (e.g. 'there are only six emotions')\n\nA straw man argument is a common fallacy of informal reasoning that distorts or misrepresents a position in order to make it easier to refute  [56] . This fallacy provides the key to understanding Crawford and Paglen's account of our work. As we have seen, \"Excavating AI\" does not critique factual attributes of JAFFE, but nonfactual 'implied claims'-misrepresentations posited without a shred of evidence. Each false allegation is simplistic and intuitively refutable. Some are ridiculous: what kind of deranged charlatan would propose that there are no more than 'six facial expressions and a neutral face' and that it is possible to 'mind read' from a single photograph of a posed expression? These ridiculous caricatures, fabulated by Crawford and Paglen in \"Excavating AI,\" appear entirely believable by readers who have limited knowledge of the research literature on the psychology of facial expression.\n\nThe straw man offers several incentives: it is easier to communicate simplistic, fallacious views to a non-specialist reader than faithful, nuanced ones. Imagined 'claims' can be constructed for ease of refutation. Distorted caricatures are more memorable than plain reality. An influential study has found that falsehood spreads more quickly on social networks than the truth  [55] . Another recent study found that populist politicians employ informal fallacies to attract attention and persuade  [6] . Does \"Excavating AI\" frame JAFFE using straw man arguments for rhetorical purposes? This is the most plausible explanation I have found for the consistently erroneous account of JAFFE given in the essay.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Cognitive Biases",
      "text": "\"Excavating AI\" has enjoyed a largely positive reception by humanities scholars who cite it as having identified problematic epistemology in machine learning training sets. We do not deny the importance of this issue-there is no doubt that there are machine learning datasets embodying questionable epistemology. Nonetheless, it is surprising that the incorrect description of JAFFE has slipped past the discernment of so many: how can the fallacies have escaped the critical faculties of these readers?\n\nFirst, consider the context in which \"Excavating AI\" has appeared: the decade preceding its publication has witnessed accelerated interest and investment in artificial intelligence and machine learning and the intensification of ideology that favours technology as the primary driver of social and economic innovation. Resistance has also grown: increasingly, the critical study of technology is a valued scholarly activity that is impacting public discourse and policy. Readers of \"Excavating AI\" who are skeptical of AI hype may be susceptible to confirmation bias  [10]  that leads them to overlook invalid parts of Crawford and Paglen's argument unwittingly.\n\nThe halo effect  [10]  could be another factor: Crawford and Paglen are frequent, high-profile speakers at public events, sometimes portrayed in the media as heroic social activists: we can expect a level of positive bias.\n\nNarrative bias  [7]  may also be a factor: \"Excavating AI\" offers readers a compelling narrative that can overrule judgments on the veracity of its elements. Readers may assume that the 'implied claims' linked to JAFFE are factual, even without being offered evidence, because this interpretation fits the essay's overall theme: false image taxonomy. Ironically, René Magritte's La Trahison des Images, a point of reference in Crawford and Paglen's thesis, is relevant here: a false ontology accompanies the JAFFE images-the straw man arguments offer the reader/viewer a deceptive misrepresentation.\n\nCrawford and Paglen provoked a media spectacle involving: the viral selfie app ImageNet Roulette; the highly visible art exhibitions Training Humans and Making Faces sponsored by global fashion brand Prada; mass media exposure that included coverage by the Netflix tech-lite variety show Connections; and torrents of activity on the social networks  [36, 54] . Public attention accrued to such a degree that the bandwagon effect  [10]  may significantly bias consumers' judgments, even if widespread popularity is no evidence for validity.\n\nTo recognize the misinformation, a reader would need to examine and understand the documentation accompanying JAFFE. It is unlikely that most readers will go to such lengths. Taken together with the biases I have outlined, this helps us understand how the fallacies concerning JAFFE pass inconspicuously.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Mind Reading Machines?",
      "text": "One of the most striking of the 'implied claims' in \"Excavating AI\" concerns the purported ability to read 'true emotional state' or 'interior state' from images of posed facial expressions. As I have shown, our work made no claim to read internal states. Where did Crawford and Paglen get such ideas? Perhaps their imaginations were stimulated by media reports of emotion reading software and the websites of affective computing startups? Here are some press headlines and excerpts from company web pages:\n\n• Apple Buys Emotient, a Company That Uses AI to Read Emotions  [8]  • . . . Startup That Can Read Your Face To Tell If You're Angry  [52]  • AI Can Read Your Emotions. Should it?  [26]  • Human Perception AI. Our software detects all things human: nuanced emotions, complex cognitive states. . .  13 My first encounter with this genre of publicity dates to about 2006, when the New York Times reported  [51]  on a project by Rana el Kaliouby and Rosalind Picard of the MIT Media Lab, who would found the startup Affectiva a few years later  [20] . The article described an \"Emotional-Social Intelligence Prosthesis\" that promised to augment an autistic user's ability to 'mind read' by providing facial expression recognition support. Some might naïvely misinterpret the mention of mind reading as an 'implied claim' hinting at telepathy or phrenology. It is nothing of the kind: the project and el Kaliouby's earlier Ph.D. thesis  [21]  drew on the Theory of Mind theory of cognitive sciencethe hypothesis that we use folk psychology to make inferences about the beliefs and intentions of other beings  [3, 9, 16, 46] . The New York Times report mentioned Simon Baron-Cohen, who proposed a model of autism based on the Theory of Mind theory  [2, 3]  and was one of el Kaliouby's thesis advisors. I was impressed by el Kaliouby's ambitious attempt to model facial expression perception more meaningfully than the image classification studies still commonplace. At the same time, I was uncomfortable with the anthropomorphic description of computer software. I expressed my reservations at conferences in Tokyo in 2007  [34]    14  and in Amsterdam in 2008  [35] .  15  My skepticism was twofold. First, as mentioned above, the term 'mind reading' has alternative quasi-magical connotations likely to mislead the popular imagination. The researchers could have avoided potential misunderstandings by describing their work as, for example, a computational model of the theory of mind, which would be accurate though less sensational. Second, there were practical difficulties involved in interpreting facial displays. My presentations in Tokyo and Amsterdam gave a simple demonstration 16 showing that the crucial role played by context would be difficult to take into account generally. Barring a solution of the hard AI problem, one did not expect 'mind reading' systems to function except in rather constrained situations. Indeed, as late as 2011, after several years of further development, Affectiva's technology was still having difficulty distinguishing some types of grimace from smiles  [20] .\n\nAffectiva evolved and is valued by investors and clients in the automotive, marketing, and advertising industries, but Shoshana Zuboff argues that, despite initial good intentions, Affectiva was drawn into \"surveillance capitalism's powerful force field.\" 17 I wonder if the outcome would have been more benign had el Kaliouby continued to pursue the computational modelling of the Theory of Mind theory scientifically rather than commercially?",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Challenging The Common View",
      "text": "In December 2000, I took part in a workshop on affective computing held as part of the Neural Information Processing Systems conference (aka NeurIPS). 18  The call for participation encouraged speakers . . . to talk about challenges and controversial topics both in their prepared talks and in the ensuing discussions. 19   My contribution offered two challenges and a proposal  [32] . 20  My first challenge to the workshop reported our experimental finding that head tilt influences facial expression perception. Our cross-cultural study of this phenomenon, in collaboration with Ruth Campbell at the University College London, was inspired by the variable expressions of masks used in the Japanese Noh theatre  [38] . Tilting the mask toward or away from a viewer changed the perceived expression. The effect differed for Japanese and British viewers. Our experiments demonstrated that a related effect obtains with human faces. This revealed a challenge: most research on facial expression recognition had assumed frontal, untilted views of the face. If slight changes in head pose could change the apparent expression, how had this influenced psychological studies of expression perception? Moreover, our results showed clearly that automatic facial expression recognition algorithms would need to account for head pose variations.\n\nMy second challenge concerned the cultural variability of facial expressions. Our study of Noh masks had revealed a cultural difference between Japanese and British experimental subjects. Furthermore, we had already observed cultural specificity with the JAFFE images and ratings data  [39] . I recommended that affective computing researchers should carefully re-examine the facial expression universality hypothesis. 16 See the Amsterdam presentation slides: https://zenodo.org/record/4026982. 17 Chapter 9, section III, \"Machine Emotion,\" of The Age of Surveillance Capitalism  [59] . 18 https://www.cs.cmu.edu/Groups/NIPS/NIPS2000/Workshops/ 19 https://www.kdnuggets.com/news/2000/n20/34i.html 20 NeurIPS'2000 Workshop presentation slides: https://zenodo.org/record/5016197.\n\nNext, I proposed an alternative paradigm for facial expression computing, suggesting that it would be worthwhile to pursue the development of systems permitting the use of voluntary facial gestures for intentional human-computer interaction. I had already been working on such facial gesture interfaces  [33] . To illustrate the concept, I showed workshop attendees a video of my mouthesizer prototype: a wearable facial gesture interface allowing musical performers to control audio effects in real-time with their mouths  [41] .\n\nMy presentation concluded with the announcement of a workshop we were organizing at the upcoming ACM CHI'2001 conference  21  in Seattle, entitled \"New Interfaces for Musical Expression,\" and encouraged interested listeners to get involved. The NIME workshop would explore prospects for interdisciplinary research involving human-computer interaction and musical expression  [45] .\n\nI wonder what impact, if any, the unconventional presentation had on the workshop attendees. Several years later, three of the four workshop organizers founded the startups Affectiva and Emotient to commercialize technology to recognize 'emotion' displayed involuntarily by the face. In the audience was the director of the US Department of Defence sponsored Facial Recognition Technology (FERET) program.  22  He had organized competitive tests of automatic facial recognition algorithms and told me he was now looking into possibilities in the affective computing field. I recall asking the other speakers during the coffee break if they were not concerned about the potential exploitation of their research for surveillance purposes.\n\nMy presentation had at least one tangible outcome: after the workshop, discussion continued with the fourth workshop organizer, cognitive scientist Gary Cottrell: it was the beginning of our collaborative study of culturally dependent aspects of facial expression perception  [13, 14] .\n\nThe NeurIPS'2000 Affective Computing workshop is a personally significant landmark that signalled a shift in my research activities: I had already begun to prioritize the voluntary, intentional, and expressive aspects of action and enaction. It also marked the gestation of a new research community, NIME,  23  that would grow and confer every year after the first CHI'2001 workshop. NIME offers a leading venue for scientific research and artistic practice at the intersection of musical expression and technology  [23] . In December 2000, NIME extended the possibility of a new locus of positive resistance against the infiltration of the surveillance paradigm into the human-machine relationship and an exploration of the potential of technology in the realm of artistic expression.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Algorithms Of Inequity",
      "text": "If Crawford and Paglen's essay is an excavation, then JAFFE is its Piltdown Man, a hoax created by would-be archaeologists  [25] . As we have seen, the JAFFE dataset was abused as fodder for rhetorical straw man arguments about machine learning training data. But what can we conclude about the essay in general? Is it not advisable to scrutinize other aspects of \"Excavating AI\"?\n\nConsider the overall focus on training set taxonomy. Imagine, for purposes of argument, that collaboration between socially aware engineers and humanities scholars has managed to resolve the data taxonomy issues raised in \"Excavating AI.\" Would this lead to the development of ethical facial recognition technology? Should the possibility of reducing bias ease concerns about the deployment of surveillance systems? Will dataset debiasing efforts by the surveillance capitalist behemoths amount to anything more than digital ethics bluewashing  [22] ?\n\nWhether for policing or profit, surveillance technology affords mediated social relations that are, by definition, unequal.  24  Automated surveillance is a species of machine-mediated interaction for which the framing of interaction as an exchange of information lacking detailed reciprocity leads directly to algorithms for the generation of inequity. Taxonomy fixes will never repair the root issue: surveillance presupposes a bias that privileges the watcher over the watched. By contrast, ethical human-machine systems afford transparency, volition, and reciprocity. Such systems do not engender or enhance power asymmetries and social inequity. I embraced these views in the 1990s, when a scientific interest in face perception briefly brought me into contact with the field of surveillance technology.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Lessons Learned?",
      "text": "I want to share some of the insights gained from debunking Crawford and Paglen's fallacious account of the JAFFE dataset. These lessons may seem selfevident but will be stated explicitly in the general spirit of this commentary.\n\nForemost, before criticizing something, it is necessary to understand it correctly. If there is a README file, read it. If there is associated documentation, study it. Failing to do this, one generates misinformation and misleads readers.\n\nIf anything is unclear in the documentation, and even if there is not, get in touch with the authors. Find out what they think about your analysis.\n\nIf a target of your critique raises objections, listen to what they have to say and understand their viewpoint. To do so is both good research practice and a matter of basic respect for other human beings.\n\nIf some part of your critique is mistaken, admit it. Sooner is better than later: avoid escalating a commitment to beliefs that have already been proven to be incorrect.\n\nBefore using facial images for an art exhibition, or publicity, clearly understand the terms of use and confirm the informed consent of the persons depicted  [36] . The pamphlet entitled \"Making AI Art Responsibly: A Field Guide\" by the Partnership on AI  [49]    25  offers good advice about reusing image data and other resources. Do not misjudge the informal presentation style: the guide is well researched and based on a solid understanding of its subject.\n\nFinally, to critique affective computing technology, choose a worthy target rather than a convenient straw man. The leading companies in the field began as research projects that spawned startups: paper trails of academic publications and patent applications are readily available. Less well documented approaches should be amenable to fieldwork and some reverse engineering. The recently published investigation of the Vibraimage system is an excellent example of thorough and productive critical research  [57] .",
      "page_start": 16,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Posed facial expression images from the JAFFE dataset.",
      "page": 3
    },
    {
      "caption": "Figure 2: Two happy expressions posed by YM.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows sample mean ratings for two JAFFE images.",
      "page": 4
    },
    {
      "caption": "Figure 3: Fear expressions posed by KA and KM.",
      "page": 5
    },
    {
      "caption": "Figure 4: Anger and disgust expressions posed by KR and UY.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RATED": "HA"
        },
        {
          "RATED": "100%"
        },
        {
          "RATED": ""
        },
        {
          "RATED": "3%"
        },
        {
          "RATED": "3%"
        },
        {
          "RATED": "3%"
        },
        {
          "RATED": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Special Issue: Ways of Machine Seeing",
      "authors": [
        "M Azar",
        "G Cox",
        "L Impett"
      ],
      "year": "2021",
      "venue": "Special Issue: Ways of Machine Seeing"
    },
    {
      "citation_id": "2",
      "title": "An Essay on Autism and Theory of Mind",
      "authors": [
        "S Baron-Cohen",
        "Mindblindness"
      ],
      "year": "1997",
      "venue": "An Essay on Autism and Theory of Mind"
    },
    {
      "citation_id": "3",
      "title": "Does the autistic child have a theory of mind?",
      "authors": [
        "S Baron-Cohen",
        "A Leslie",
        "U Frith"
      ],
      "year": "1985",
      "venue": "Cognition"
    },
    {
      "citation_id": "4",
      "title": "AI weighs in on debate about universal facial expressions",
      "authors": [
        "L Barrett"
      ],
      "year": "2021",
      "venue": "Nature"
    },
    {
      "citation_id": "5",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "6",
      "title": "Populism and informal fallacies: An analysis of right-wing populist rhetoric in election campaigns",
      "authors": [
        "S Blassnig",
        "F Büchel",
        "N Ernst",
        "S Engesser"
      ],
      "year": "2019",
      "venue": "Argumentation"
    },
    {
      "citation_id": "7",
      "title": "The differential impact of abstract vs. concrete information on decisions",
      "authors": [
        "E Borgida",
        "R Nisbett"
      ],
      "year": "1977",
      "venue": "Journal of Applied Social Psychology"
    },
    {
      "citation_id": "8",
      "title": "Apple buys Emotient, a company that uses AI to read emotions",
      "authors": [
        "S Byford"
      ],
      "year": "2016",
      "venue": "The Verge"
    },
    {
      "citation_id": "9",
      "title": "Toward a Unified Science of the Mindbrain",
      "authors": [
        "P Churchland",
        "Neurophilosophy"
      ],
      "year": "1986",
      "venue": "Toward a Unified Science of the Mindbrain"
    },
    {
      "citation_id": "10",
      "title": "A Dictionary of Psychology. Oxford quick reference",
      "authors": [
        "A Colman"
      ],
      "year": "2015",
      "venue": "A Dictionary of Psychology. Oxford quick reference"
    },
    {
      "citation_id": "11",
      "title": "Sixteen facial expressions occur in similar contexts worldwide",
      "authors": [
        "A Cowen",
        "D Keltner",
        "F Schroff",
        "B Jou",
        "H Adam",
        "G Prasad"
      ],
      "year": "2021",
      "venue": "Nature"
    },
    {
      "citation_id": "12",
      "title": "Excavating AI: The politics of images in machine learning training sets",
      "authors": [
        "K Crawford",
        "T Paglen"
      ],
      "year": "2019",
      "venue": "Excavating AI: The politics of images in machine learning training sets"
    },
    {
      "citation_id": "13",
      "title": "Evidence and a computational explanation of cultural differences in facial expression recognition",
      "authors": [
        "M Dailey",
        "C Joyce",
        "M Lyons",
        "M Kamachi",
        "H Ishi",
        "J Gyoba",
        "G Cottrell"
      ],
      "year": "2010",
      "venue": "Emotion"
    },
    {
      "citation_id": "14",
      "title": "Cultural differences in facial expression classification",
      "authors": [
        "M Dailey",
        "M Lyons",
        "M Kamachi",
        "H Ishi",
        "J Gyoba",
        "G Cottrell"
      ],
      "year": "2002",
      "venue": "Cognitive Neuroscience Society, 9th Annual Meeting"
    },
    {
      "citation_id": "15",
      "title": "The Expression of the Emotions in Man and Animals",
      "authors": [
        "C Darwin"
      ],
      "year": "1872",
      "venue": "John Murray"
    },
    {
      "citation_id": "16",
      "title": "The Intentional Stance",
      "authors": [
        "D Dennett"
      ],
      "year": "1987",
      "venue": "The Intentional Stance"
    },
    {
      "citation_id": "17",
      "title": "Pictures of Facial Affect",
      "authors": [
        "P Ekman"
      ],
      "year": "1976",
      "venue": "Pictures of Facial Affect"
    },
    {
      "citation_id": "18",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "19",
      "title": "Facial action coding system: A technique for the measurement of facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system: A technique for the measurement of facial movement"
    },
    {
      "citation_id": "20",
      "title": "Girl Decoded: My Quest to Make Technology Emotionally Intelligent-and Change the Way We Interact Forever. Penguin UK",
      "authors": [
        "R El Kaliouby"
      ],
      "year": "2020",
      "venue": "Girl Decoded: My Quest to Make Technology Emotionally Intelligent-and Change the Way We Interact Forever. Penguin UK"
    },
    {
      "citation_id": "21",
      "title": "Mind-reading Machines: Automated Inference of Complex Mental States",
      "authors": [
        "R El Kaliouby"
      ],
      "year": "2005",
      "venue": "Mind-reading Machines: Automated Inference of Complex Mental States"
    },
    {
      "citation_id": "22",
      "title": "Translating principles into practices of digital ethics: Five risks of being unethical",
      "authors": [
        "L Floridi"
      ],
      "year": "2019",
      "venue": "Philosophy & Technology"
    },
    {
      "citation_id": "23",
      "title": "Fifteen Years of New Interfaces for Musical Expression",
      "authors": [
        "A Jensenius",
        "M Lyons",
        "Nime Reader"
      ],
      "year": "2017",
      "venue": "Fifteen Years of New Interfaces for Musical Expression"
    },
    {
      "citation_id": "24",
      "title": "Stages #9: The Next Biennial Should Be Curated By A Machine. Liverpool Biennial in partnership with DATA browser series",
      "authors": [
        "J Krysa",
        "M Moscoso"
      ],
      "venue": "Stages #9: The Next Biennial Should Be Curated By A Machine. Liverpool Biennial in partnership with DATA browser series"
    },
    {
      "citation_id": "25",
      "title": "Bones of Contention: Controversies in the Search for Human Origins",
      "authors": [
        "R Lewin"
      ],
      "year": "1997",
      "venue": "Bones of Contention: Controversies in the Search for Human Origins"
    },
    {
      "citation_id": "26",
      "title": "AI can read your emotions. should it? The Guardian",
      "authors": [
        "T Lewis"
      ],
      "year": "2019",
      "venue": "AI can read your emotions. should it? The Guardian"
    },
    {
      "citation_id": "27",
      "title": "Coding facial expressions with Gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "28",
      "title": "Gabor wavelet representation of facial expression",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1997",
      "venue": "Gabor wavelet representation of facial expression"
    },
    {
      "citation_id": "29",
      "title": "V1 similarity measure recovers dimensions of facial expression perception",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "P Tran",
        "J Gyoba"
      ],
      "year": "1997",
      "venue": "Investigative Ophthalmology & Visual Science"
    },
    {
      "citation_id": "30",
      "title": "A model based on V1 cell responses predicts human perception of facial similarity",
      "authors": [
        "M Lyons",
        "K Morikawa"
      ],
      "year": "1996",
      "venue": "Investigative Ophthalmology and Visual Science"
    },
    {
      "citation_id": "31",
      "title": "Gabor-based coding and facial similarity perception",
      "authors": [
        "M Lyons",
        "K Morikawa"
      ],
      "year": "1997",
      "venue": "Perception"
    },
    {
      "citation_id": "32",
      "title": "Facial expression recognition: From perception to application",
      "authors": [
        "M Lyons"
      ],
      "year": "2000",
      "venue": "NIPS'2000 Workshop on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Facial gesture interfaces for expression and communication",
      "authors": [
        "M Lyons"
      ],
      "year": "2004",
      "venue": "IEEE International Conference on Systems, Man and Cybernetics"
    },
    {
      "citation_id": "34",
      "title": "Dimensional affect and expression in natural and mediated interaction",
      "authors": [
        "M Lyons"
      ],
      "year": "2007",
      "venue": "Dimensional affect and expression in natural and mediated interaction",
      "arxiv": "arXiv:1707.09599"
    },
    {
      "citation_id": "35",
      "title": "A mimetic strategy to engage voluntary physical activity in interactive entertainment",
      "authors": [
        "M Lyons"
      ],
      "year": "2008",
      "venue": "IEEE FG'08 Workshop on Facial and Bodily Expressions for Control and Adaptation of Games"
    },
    {
      "citation_id": "36",
      "title": "Excavating AI",
      "authors": [
        "M Lyons"
      ],
      "year": "2020",
      "venue": "The elephant in the gallery",
      "arxiv": "arXiv:2009.01215"
    },
    {
      "citation_id": "37",
      "title": "Automatic classification of single facial images",
      "authors": [
        "M Lyons",
        "J Budynek"
      ],
      "year": "1999",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "38",
      "title": "The Noh mask effect: Vertical viewpoint dependence of facial expression perception",
      "authors": [
        "M Lyons",
        "R Campbell",
        "A Plante",
        "M Coleman",
        "M Ka-Machi"
      ],
      "year": "2000",
      "venue": "Proceedings of the Royal Society of London. Series B: Biological Sciences"
    },
    {
      "citation_id": "39",
      "title": "Coding facial expressions with Gabor wavelets (IVC special issue",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Coding facial expressions with Gabor wavelets (IVC special issue",
      "arxiv": "arXiv:2009.05938"
    },
    {
      "citation_id": "40",
      "title": "A linked aggregate code for processing faces",
      "authors": [
        "M Lyons",
        "K Morikawa"
      ],
      "year": "2000",
      "venue": "Pragmatics & Cognition"
    },
    {
      "citation_id": "41",
      "title": "Facing the music: a facial action controlled musical interface",
      "authors": [
        "M Lyons",
        "N Tetsudani"
      ],
      "year": "2001",
      "venue": "CHI'01 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "42",
      "title": "Japanese and Caucasian facial expressions of emotion (JACFEE) and neutral faces (JACNeuF). Intercultural and Emotion Research Laboratory",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1988",
      "venue": "Japanese and Caucasian facial expressions of emotion (JACFEE) and neutral faces (JACNeuF). Intercultural and Emotion Research Laboratory"
    },
    {
      "citation_id": "43",
      "title": "The Analects of Confucius",
      "authors": [
        "A Muller",
        "Trans"
      ],
      "year": "2012",
      "venue": "The Analects of Confucius"
    },
    {
      "citation_id": "44",
      "title": "Open data in science",
      "year": "2008",
      "venue": "Nature Precedings"
    },
    {
      "citation_id": "45",
      "title": "New Interfaces for Musical Expression",
      "authors": [
        "I Poupyrev",
        "M Lyons",
        "S Fels",
        "T Blaine"
      ],
      "year": "2001",
      "venue": "CHI'01 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "46",
      "title": "Does the chimpanzee have a theory of mind?",
      "authors": [
        "D Premack",
        "G Woodruff"
      ],
      "year": "1978",
      "venue": "Behavioral and Brain Sciences"
    },
    {
      "citation_id": "47",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "48",
      "title": "Canadian, Greek, and Japanese freely produced emotion labels for facial expressions",
      "authors": [
        "J Russell",
        "N Suzuki",
        "N Ishida"
      ],
      "year": "1993",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "49",
      "title": "Making AI Art Responsibly: A Field Guide",
      "authors": [
        "E Saltz",
        "L Coleman",
        "C Leibowicz"
      ],
      "year": "2020",
      "venue": "Making AI Art Responsibly: A Field Guide"
    },
    {
      "citation_id": "50",
      "title": "The description of facial expressions in terms of two dimensions",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1952",
      "venue": "Journal of Experimental Psychology"
    },
    {
      "citation_id": "51",
      "title": "The social-cue reader",
      "authors": [
        "J Schuessler"
      ],
      "year": "2006",
      "venue": "The New York Times"
    },
    {
      "citation_id": "52",
      "title": "Apple buys startup that can read your face to tell if you're angry",
      "authors": [
        "B Solomon"
      ],
      "year": "2016",
      "venue": "Forbes"
    },
    {
      "citation_id": "53",
      "title": "Cross-cultural and culturalspecific production and perception of facial expressions of emotion in the wild",
      "authors": [
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "From spectacle to extraction. and all over again. unthinking photography",
      "authors": [
        "G Tedone"
      ],
      "year": "2020",
      "venue": "From spectacle to extraction. and all over again. unthinking photography"
    },
    {
      "citation_id": "55",
      "title": "The spread of true and false news online",
      "authors": [
        "S Vosoughi",
        "D Roy",
        "S Aral"
      ],
      "year": "2018",
      "venue": "Science"
    },
    {
      "citation_id": "56",
      "title": "Informal Logic: A Pragmatic Approach",
      "authors": [
        "D Walton"
      ],
      "year": "2008",
      "venue": "Informal Logic: A Pragmatic Approach"
    },
    {
      "citation_id": "57",
      "title": "Suspect AI: Vibraimage, emotion recognition technology and algorithmic opacity",
      "authors": [
        "J Wright"
      ],
      "year": "2021",
      "venue": "Science, Technology and Society"
    },
    {
      "citation_id": "58",
      "title": "Comparison between geometry-based and Gabor-wavelets-based facial expression recognition using multi-layer perceptron",
      "authors": [
        "Z Zhang",
        "M Lyons",
        "M Schuster"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic face and gesture recognition"
    },
    {
      "citation_id": "59",
      "title": "The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power",
      "authors": [
        "S Zuboff"
      ],
      "year": "2019",
      "venue": "The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power"
    }
  ]
}