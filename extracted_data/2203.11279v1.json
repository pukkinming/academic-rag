{
  "paper_id": "2203.11279v1",
  "title": "Eeg Based Emotion Recognition: A Tutorial And Review",
  "published": "2022-03-16T08:28:28Z",
  "authors": [
    "Xiang Li",
    "Yazhou Zhang",
    "Prayag Tiwari",
    "Dawei Song",
    "Bin Hu",
    "Meihong Yang",
    "Zhigang Zhao",
    "Neeraj Kumar",
    "Pekka Marttinen"
  ],
  "keywords": [
    "EEG",
    "emotion recognition",
    "affective computing",
    "psychophysiological computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition technology through analyzing the EEG signal is currently an essential concept in Artificial Intelligence and holds great potential in emotional health care, human-computer interaction, multimedia content recommendation, etc. Though there have been several works devoted to reviewing EEG-based emotion recognition, the content of these reviews needs to be updated. In addition, those works are either fragmented in content or only focus on specific techniques adopted in this area but neglect the holistic perspective of the entire technical routes. Hence, in this paper, we review from the perspective of researchers who try to take the first step on this topic. We review the recent representative works in the EEG-based emotion recognition research and provide a tutorial to guide the researchers to start from the beginning. The scientific basis of EEG-based emotion recognition in the psychological and physiological levels is introduced. Further, we categorize these reviewed works into different technical routes and illustrate the theoretical basis and the research motivation, which will help the readers better understand why those techniques are studied and employed. At last, existing challenges and future investigations are also discussed in this paper, which guides the researchers to decide potential future research directions. CCS Concepts: • Human-centered computing → Human computer interaction (HCI); Ubiquitous and mobile computing; • Computing methodologies → Artificial intelligence.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion (or affect) recognition (or detection) has increasingly drawn attention from researchers with a multidisciplinary background. It is the leading scientific problem in Affective Computing, which is a comparatively new research field proposed by Picart  [1] , namely how to empower computer systems to precisely process, recognize and comprehend emotional information expressed by a human for natural human-computer interactions (HCI)  [2] . It is an important concept both in Artificial Intelligence and Ambient Intelligence  [3] . Interdisciplinary knowledge is needed in Affective Computing, and the findings can further promote the development of the various disciplines, including Computer Science, Electronic Engineering, Human Factors Engineering, Psychology, Neuroscience, Medical Science, etc. As a complex psychological state, emotion is reflected in physical behaviors and physiological activities  [4] . In the past decade, much effort has been made to recognize emotions based on affective information gathered from various physical behaviors and physiological activities, such as voices from the microphone, signals from neurophysiological activity measuring devices, videos from cameras, and texts from the website, etc. The essence of emotion detection research is utilizing statistical machine learning techniques (e.g., classification, regression, or clustering) to identify users' different emotional states in real-time or offline. The problem is challenging as we have to dig and utilize the latent components embedded in the weak and noisy emotionrelated data sources, including natural language, facial expressions, speech, body gestures, bio-signals, text, eye gaze, etc., collected from multiple monitor platforms mentioned above. Currently, judging emotional states based on physiological activities (physiological clue) is a hot topic in Affective Computing. Some psycho or physiological researches have manifested there exists dependencies between the physiological process and the emotion cognition process even though there still exists debates on the order of appearance of these two processes  [5] . Hence, Computational Psychophysiology-based approaches are supposed to be effective complements for facial or speech information (non-physiological clue) based recognition methods, whose performance could be greatly influenced when users intentionally dissemble their true feelings by wearing 'social masks'. Considering the central nervous system (brain) regulates and controls the autonomic nervous system to participate in emotional processes, directly utilizing the brain activities information (e.g., EEG) to study the emotional cognition mechanism and recognize emotional state is especially worth studying.\n\nThe EEG based emotion recognition has wide application prospects. For example, developing emotion-aware driver assistance systems for cars is currently recognized as a potential way to enhance driving safety  [6] . In the field of neurology, identified emotions in response to specific stimuli and the corresponding neural activities can be analyzed for diagnosing some affective disorders, such as PTSD (post straumatic stress disorder)  [7]  and depression  [8, 9] . The psychological studies have found an attentional bias phenomenon in depressed individuals, in which increased attention to negative or dysphoric stimuli rather than positive contents  [10] . Besides, the detected emotion can be utilized to guide various emotion disorder therapy, e.g. robotassisted therapy  [11]  and music-assisted therapy  [12, 13] . In the field of Information Retrieval (IR), emotion recognition (or called sentiment analysis) has always been an active research field. The detected emotion states can be used for emotion-associated IR needs, e.g., for implicit tagging of the multimedia contents  [14] [15] [16] , or for enriching user profiles to improve the topical relevance of the recommended multimedia contents  [17] . Emotion recognition contributes to building the human-centered information retrieval (IR) system  [18, 19] . In the field of leisure and entertainment, e.g., the computer gaming, researchers sought to detect gamers' emotional states in order to adjust to game's level of difficulty, punishment, and encouragement  [20] . In Virtual Reality (VR) applications, e.g., VR in education, the influence of emotional states on memory has been verified, the positive mood has beneficial effects on spatial learning. Hence, the emotion should be recognized during learning in VR environment  [21, 22] .\n\nEssentially, EEG-based emotion recognition belongs to one kind of pattern recognition research. As we know, resolving a pattern recognition problem usually contains several main steps, namely as follows:\n\n• Firstly, the definition and quantification of the recognition target should be determined, by which the problem can be resolved as one computable problem. • Secondly, acquiring sufficient and valid research data is vitally important in preparing comprehensive space for searching model decision boundaries.\n\n• Thirdly, preprocessing the data and acquiring the representation (e.g., feature extraction or feature learning) are typically needed in building pattern recognition models. Target-related representative characteristics extracted from raw data can eliminate redundant information that may influence model construction. • Finally, the recognition models are designed, trained, and evaluated based on the processed data iteratively until a model with acceptable recognition accuracy can be determined.\n\nDomestic and overseas research on EEG-based emotion recognition studies also covers these topics. Such being the case, in this paper, we choose to outline the review covering these topics as mentioned above. Before writing this survey, there have been several works devoted to reviewing EEGbased emotion recognition. It's necessary to introduce those related survey papers that have been published in recent three years and expound on the motivation and the necessity to make a new survey paper for this research field. Firstly, the contents of several survey papers need to be updated. For example, the surveys of Lotte et al.  [23]  and Alarcao and Fonseca  [24]  were published before 2019, and the surveyed works were developed between 2006 and 2017. Dadebayev et al.  [25]  mainly introduces the application of consumer-grade EEG acquisition devices (e.g., the Emotive, OpenBCI, and NeuroSky) in emotion recognition. It mainly reviews works between 2014 and 2019. Although the surveys conducted by Suhaimi et al.  [26] , Arya et al.  [27] , and Rahman et al.  [28]  were published after 2021, most of the reviewed methodologies were developed before 2019. For example, Arya et al.  [27]  and Suhaimi et al.  [26]  choose to survey works between 2009 and 2018. Secondly, these surveys emphasize traditional feature engineering and Machine Learning-based approaches. The Deep Learning-based approaches have not been systematically introduced. We think these surveys should be improved by adding more content about Deep Learning. Thirdly, several surveys are not written specifically for the affective brain-computer interface (BCI) tasks. For example, although Craik et al.  [29]  focuses on introducing the Deep Learning-based methodologies applied to EEG modeling. The reviewed EEG classification tasks are not restricted to emotion recognition. Other tasks related to mental workload, motor imagery, event-related potential, seizure detection, and sleep stage scoring are also included. In addition, its content needs to be updated considering its publication year is 2019. Similarly, the survey of Lotte et al.  [23]  mainly discusses the classification algorithms for motor imagery-based BCI.\n\nIn our review, we focus on the EEG-based emotion recogntion tasks, and try to add more introduction about the up-to-date methodologies, especially various Deep Learning-based approaches that have not been systematically reviewed in these two papers. Further, we will discuss several up-to-date research problems, such as the 'domain shift' problem, the few-shot learning problem, etc. We also discuss potential routes in this research field, such as the large-scale pre-trained EEG model applied in emotion recognition. In addition, other surveys only focus on specific techniques adopted in this area but neglect the holistic technical perspective. Hence, in this paper, we review from the perspective of researchers who try to take the first step on this topic. In addition to reviewing the recent advances in the EEG-based emotion recognition research, we also provide a tutorial to guide the researchers to start from the very beginning. For example, we introduce the scientific basis of EEG-based emotion recognition in the psychological and physiological levels. Further, we categorize these reviewed works into different technical routes and illustrate the theoretical basis and the research motivation, which will help the readers better understand why those techniques are studied and employed. At last, existing challenges and future investigations are also discussed in this paper, which guides the researchers to decide potential future research directions. We also carefully discuss various EEG segmentation strategies when discussing the evaluation methods, which need to be treated carefully but have been overlooked in related surveys. We believe this review provides valuable research references and research problems for researchers who are new to this field. Overall, this review paper draws on the advantages of many survey articles, and is a high-quality complementation to these classical surveys. Acquiring users' behavioral or physiological data with quantified emotional state labels is vital for statistical analysis-based psychological research and intelligent computing-based emotion recognition systems. Hence, the emotional state characterized by target samples should be effectively recorded and quantitatively evaluated. All the quantitative methods of emotion can be divided into two main categories, as follows.\n\n2.1.1 Discrete type of emotion quantification model. According to Darwin's theory of evolution, human emotions are discrete, and they are preserved by natural selection  [30] . Ekman  [31]  and Plutchik  [32]  approved the point of view of Darwin and then proposed that the emotion was composed of six or eight basic states, e.g., anger, anticipation, fear, sadness, disgust, trust, surprise, joy, etc., which could be further expanded to fifteen or more types. On this basis, an improved model was put forward by scholars to improve the quantification of emotions further. For example, the Palette Theory indicated that the basic emotional states could be further taken as Primary Colors, and then other emotional states would be generated by mixing the Primary Colors. For instance, the two basic emotions of surprise and sadness can be compounded into disappointment. Subsequently, Plutchik developed an emotion wheel representation method  [33] . In addition, a tree type or hierarchical type of emotion quantification method was proposed by Parrott and Gerrod  [34] . In essence, these quantification models can all be incorporated into the discrete-type emotion quantization models.\n\nWhen building a recognition system on the discrete type of emotion quantification model, we typically regard this task as a classification modeling task, in which classification models are applied and studied, including support vector machine (SVM) algorithm, K-nearest neighbor (KNN) algorithm, decision tree algorithm, etc.\n\n2.1.2 Continuous dimensional type of emotion quantification model. The boundaries for distinguishing the emotional states are vague, and the changes and evolutions of states are continuous without breakpoint. Partitioning the emotional states into a dozen discrete types can only show the main aspects of emotion and fail in the accurate quantification of emotional state. In addition, discrete emotional labels are not consistent among various cultures and nationalities. For example, we can not find a corresponding translation in Polish for the emotion of 'disgust'. Hence, the continuous dimensional type of emotion quantification method is proposed. This quantification method uses several mutually orthogonal basic axes to display different dimensions of emotion, which solves the contradiction between the discrete quantification method and the rich emotional connotation. The Valence-Arousal bipolar emotional quadrant system was put forward by Russell  [35] , which has been widely accepted in Affective Computing. As shown in Figure  1(A) , the classical two dimensions of Valence and Arousal are used to depict the Valence level and Arousal level of emotion.\n\nFig.  1 . Valence-Arousal Bipolar Coordinate System Porposed by Russell (A) and the corresponding Self-Assessment Manikins scale (B)  [36] .\n\nThe values (or ratings) of the Valence axis from positive to negative refer to the measurement for individuals' happy and sad degrees. Likewise, a positive value in Arousal indicates an activated state (excitement), while the negative value indicates an unactivated state (calmness). In addition to the two standard base axes, adding more dimensions for a comprehensive measurement of emotions is feasible. The Dominance dimension represents the dominance control degree of the individual in the emotional process. When an external environment controls a user, the emotional state is at a lower dominance level (e.g., surprise, fear, etc.). Conversely, when a user can master the external environment, the emotional state is at a higher dominance level. It should be pointed out that the various discrete emotional states can be located to specific locations in the continuous dimensional state space with a one-to-one correspondence. For example, the sadness emotion is located in the low Arousal-low Dominance-low Valence coordinate space in the continuous emotional coordinate system, while the happiness emotion is located in a high Arousal-high Dominance-high Valence coordinate space. A broadly adopted method for evaluating continuous emotional states is the Self-Assessment Manikins (SAM) scale-based approach. SAM is designed by introducing the manikins into the questionnaire to visually evaluate the degree of Valence and arousal. The SAM questionnaire generally sets a discrete scale from 1 to 9, as shown in Figure  1(B) .\n\nWhen building a recognition system on the continuous type of emotion quantification model, researchers can either tackle this task through classification modeling or regression modeling. Briefly speaking, constructing a model directly based on samples with continuous emotional ratings is a regression modeling task, in which the built model (e.g., ridge regression, recurrent neural networks) should be able to precisely predict the unknown samples' emotional ratings. Whereas, when regarding this task as a classification problem, researchers typically need to divide the emotional ratings into several levels, the samples are assigned with specific class labels according to at which level the emotional ratings are located. After that, classification algorithms are trained on the labeled samples and applied to inferring the unknown samples' emotional classes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Eeg Activity'S Specificity And Neural Correlate In The Emotional Process",
      "text": "The critical function of the central nervous system (brain) is to regulate the whole-body physical or psycho activities to participate in the emotional process. Intracranial EEG or non-implanted EEG can record the physiological activities of the central nervous system. For the non-implanted EEG, the EEG signals are acquired by deploying multiple electrodes on different brain regions according to the standard 10-20 topology, as shown in Figure  2 . Fig.  2 . Differences between high and low Valence on le and right frontal EEG alpha power in music listening (Note that EEG power is inversely related to activity, thus lower power reflects more activity  [37] ) .\n\nThe correlation between neural sources and emotions is one of the key scientific questions of cognitive neuroscience research. There are two different views on neural sources and emotions. The locationist view postulates that some discrete emotions reflect the discrete anatomical structure in the brain (e.g., amygdala)  [38, 39] . The distributionist view argues that no single anatomical structure uniquely specializes for individual emotion categories  [40, 41] . Human emotion is a product of the cooperation of multiple cortex regions  [42] . Localizing neural sources from scalp EEG is problematic. We should not assume that the electrical activity comes from the adjacent cortex. Hence, cognitive neuroscience adopts intracranial electrophysiology to study these questions. Intracranial electrophysiology-based studies provide evidence for the distributionist view. For example, negative emotions are processed in multiple brain regions. Stimulation of subcortical nuclei, the temporal lobe and gyrus, temporal-parietal junction, inferior frontal gyrus, etc, can affect the perception of sadness, fear, and anger  [43, 44] . The positive emotions (e.g., joy and mirth) are also processed in multiple brain regions  [45, 46] . It is found that the forebrain takes part in the regulation process of emotion  [47] [48] [49] , and asynchronous activities would occur in different locations of the brain during the emotional process. A higher left prefrontal activity is the reflection of an 'approach model' of emotion process (e.g., positive emotions), whereas a higher right prefrontal activity is the reflection of the 'withdrawal model' of emotion process (e.g., negative emotions)  [50] . Researches on people with depression also found greater activation in the right forebrain than other brain regions.\n\nThe correlation between EEG components and various emotions is also one of the key scientific questions in cognitive neuroscience and is critical for building effective recognition models. Joyful music has been reported to be positively correlated with the power energy in the theta band (4∼7 Hz) near the midline of the prefrontal cortex  [51] . As shown in Figure  2 , the valence and arousal of musical stimuli have been reported to correlate with frontal alpha (8∼13 Hz) asymmetry  [37] . When playing happy music for subjects, the EEG activity in the left front area of the brain is more active than that in the right front area of the brain. However, the opposite result appears if playing sad music. Nevertheless, some works found no effects in the alpha band and instead found a relationship in the beta-2 band (18∼22 Hz)  [52] . The effect of high-frequency EEG, including the Beta-2 band (18∼22 Hz), Beta-3 band (22∼30 Hz), and Gamma band (30∼45 Hz), in emotion also have been verified. The experience of happiness results in the decreases in beta-2 power in the front central regions and beta-3 and gamma power over the entire cortex. Whereas, when feeling anger, the Beta-2, Beta-3, and Gamma bands get increased power in the front of both hemispheres  [53] . Subjects in the DEAP experiments produced higher gamma and frontal midline theta power while watching emotion provoking music videos  [54] . Balconi and Lucchiari  [55]  reported the Gamma band activity is a marker of the subject's evaluation of the Arousal dimension in emotional faces stimuli. Yang et al.  [56]  revealed that the network connections in the high Gamma band have significant differences among the positive, neutral, and negative emotional states. In addition, the specific phenomena related to emotion in other brain areas and other frequency components have also been reported  [57, 58] .\n\nCritical EEG bands and brain regions in recognizing emotions are also extensively studied in pattern recognition. Earlier traditional machine learningbased researches have reported that the features extracted from high-frequency bands (the Gamma and Beta bands) are more effective for an algorithm to recognize both Valence and Arousal dimensions  [59] . Currently, extensive Deep Learning-based recognition approaches also verify the Beta and Gamma band information are the most suitable bands for emotion recognition. For example, the differential entropy (DE) features extracted from these bands lead to a higher recognition performance compared with the other bands when adopting the deep belief networks  [60]  and the convolutional neural networks  [61] . Even so, these studies also approve that combining the information from all the bands can achieve the best performance. Feature Selection based approaches are suitable for analyzing the key EEG variables in emotion recognition. Li et al.  [62]  studies the key frequency bands and channels by analyzing features selected out by the L1-norm logistic regression model. They find the Hjorth parameter of mobility in the Beta band achieves the best performance. They also find that the electrodes on the bilateral temporal and left anterior regions help get a higher performance for cross-subject emotion recognition, especially when the information in the beta band was utilized. Naser and Saha  [63]  adopt a Minimum Redundancy Maximum Relevance (MRMR) feature selection method on the asymmetrical features. Besides the Beta and Gamma bands, they also find features selected from Alpha bands promote the Arousal and Valence classification. Also adopting the mRMR method, Zhuang et al.  [64]  find that the DE of the Gamma band have good classification performance, and important electrodes are distributed in the bilateral temporal, the prefrontal, and the occipital regions. In addition to directly compare the performance between different bands and regions in emotion recognition, analyzing the model weights associated with the variables is another common way. For example, Zheng and Lu  [60]  explore the critical EEG electrodes by analyzing the weights of the trained DBNs. The results show the lateral temporal and prefrontal brain areas activate more than other brain areas in beta and gamma frequency bands.\n\nTo sum up, it can be seen from these studies that the central nervous system is directly related to the emotional process. As an external manifestation of the brain's cognitive psychological activity, EEG can be taken as an effective measurement to study emotional psychology and provides the feasibility for studying emotion recognition technology. Nevertheless, the research findings mentioned above indicate that EEG activity's specificity and neural correlate in the emotional process are not yet fully explored and understood. It's better to build recognition models on EEG from all critical frequency bands and cortex regions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Classical Research Methodologies Of Eeg Based Emotion Recognition Studies",
      "text": "Classical research methodologies in this field follow the procedure shown in Figure  3 , namely applying pattern recognition models to the handcrafted emotion-related EEG features to distinguish different emotional states. Firstly, if you want to build and validate the models on self-collected EEG data, you should design a user experiment to collect emotion-induced EEG data. There are two main categories of methods for emotion induction. The first one aims to measure EEG during viewing emotionally provoking stimuli. Recommended stimuli include presentation of faces with emotional facial expressions, display of emotional pictures (e.g., IAPS pictures  [65] ), and emotionally provoking video or audio (e.g., film, music, music video, IADS sounds  [66] ). The classical benchmark datasets introduced in Section 5.1 were collected under these types of stimuli. Building Virtual Reality scenes to induce emotions is reported effective and has been extensively adopted in recent years. For example, Zhang et al.  [67]  proposed the Affective Virtual Reality System, which combines IAPS, IADS, and China Affective Video System (CAVS) to produce a virtual environment that would accommodate VR headset for emotion induction. Another method asks the test subject to imagine the target emotion (e.g., recalling the past experience). After each trial, self-reports of evoked emotional states are required. Then, classical methods need conducting data preprocessing for the acquired raw EEG data, based on which domain knowledge (signal processing, sequential pattern mining, emotional psychology, etc.) guided feature extraction is further conducted to distill as many emotion-related characteristics as possible. The refined characteristics are called data features in machine learning that is further organized to construct the training samples. Furthermore, the features closely related to emotional state and helpful for improving the emotion recognition performance can be screened out from all the candidate features set by the feature selection method. Finally, we attempt to select various statistical machine learning models or Deep Learning models to build on the chosen features and iteratively evaluate their performance by computing the evaluation metrics until achieving the goal that the model outputs approximate the ground truth. Many classical studies that follow this research methodologies have verified the feasibility of building an emotion recognition system on EEGs. In addition, based on the classical methodologies, great advances have been made in this field in recent years. This review will give a more detailed introduction of the classical and latest methodologies in the following sections.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Eeg Preprocessing And Feature Engineering",
      "text": "Its high time resolution characterizes EEG. The high-sampled EEG data contains much emotion-related information, which has excellent potential for building precise and real-time recognition systems. Generally, EEG with a higher sampling rate may cover more details, however meanwhile may introduce lots of noise and increase the computation cost for signal processing, feature engineering, and modeling training. Regarding the device features and the problems mentioned above, the EEGs are usually sampled with a frequency of 256Hz, 512Hz, or 1024Hz. Theoretically speaking, a sampling frequency of 128 Hz would give a Nyquist frequency of 64 Hz, which is adequate for extracting sufficient emotion-related features. As discussed in Section 2.2, the Beta-2 band (18∼22 Hz), Beta-3 band (22∼30 Hz), and Gamma band (30∼45 Hz) are reported closely correlated with emotion recognition.\n\nIt is challenging to represent the emotional EEG signals effectively due to the many noises embedded in EEG signals. Besides, it is difficult to capture the implicit correlations between EEG and a specific cognitive process. Hence, preprocessing and feature engineering usually need to be included in the workflow.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg Preprocessing",
      "text": "EEG signals are mixed with various noises of the human body and environment, thus bringing challenges to the anti-interference and robustness of the recognition algorithm. Therefore, the collected EEG signals are not directly used to build the recognition models and systems. The research paradigm should not overlook preprocessing the signals and extracting representative features. The preprocessing for the acquired EEG data is mainly to remove EOG artifacts with a frequency less than 4Hz that caused by eye blink, ECG artifacts with a frequency about 1.2Hz, EMG artifacts with a frequency more than 30Hz, power frequency artifacts in the environment with a frequency between 50 to 60Hz, and so on. These above artifacts can be removed with the independent component analysis (ICA), discrete wavelet, or band-pass filters (such as the Butterworth filter) to retain the rhythmic components associated with the emotional activity. The filtering process cannot remove all the artifacts from the EEG signals, and thus additional processing is needed. We can utilize the Artifact Subspace Reconstruction (ASR) method for enhanced artifact removal. The ASR method consists of a sliding window Principal Component Analysis (PCA), which statistically interpolates any high-variance signal components exceeding a threshold. Furthermore, the Common Average Reference (CAR) method is recommended to compute the average value over all electrodes and subtract it from each sample of each electrode  [68] .\n\nEven though the emotion-provoking experiments strictly follow the experimental protocol, some problems may arise with the recordings from some subjects or trials due to technical issues or personal issues, resulting in incomplete or high-noise data. This kind of data is recommended to be entirely abandoned without further processing and analysis. By the way, we could choose only to select out a subset of the whole EEG channels for studying if we can determine the specific emotion-related brain areas, e.g., the forehead. Channel selection will decrease the computation cost and may increase the recognition performance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Time Domain Features",
      "text": "The most direct method for extracting EEG features is to calculate statistics such as mean value, variance, skewness, kurtosis and peak-to-peak interval, etc., which can characterize the time-domain properties of EEG signal. Some studies also adopt the higher-order crossing (HOC) method, which calculates the number of zero-crossing points of EEG as a feature after being processed by different filters  [69, 70] . Event-related potentials (ERPs) reflect the underlying cognitive process, including emotion. Early ERP components have been verified to correlate with Valence  [71] , whereas the late ERP components have been verified to correlate with Arousal  [72] . Hence, characteristics of ERP components, e.g, P100, N100, N200, P200, P300, were also taken into studies  [73] . It should be noted, ERPs are obtained by averaging multiple EEG trials, which may not be feasible for online applications.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Frequency Domain Features",
      "text": "The frequency-domain attribute is a description of the signal from another perspective. Moreover, it has a better anti-interference to noise and can reflect the details of each component of the signal. Therefore, the frequency-domain features, e.g., the power spectral density (PSD), are widely extracted in physiological emotion calculation  [60, 74] . PSD reflects the signal power of a specific frequency band. It is usually obtained through fast Fourier transform (FFT), by which the raw EEG signal can be decomposed into several distinct frequency bands, e.g., the Delta (1-4 Hz), Theta (4-8 Hz), Alpha (8-13 Hz), Beta (13-30 Hz), and Gamma (>30 Hz). Then the average power of a specific frequency band is computed and adopted as the feature. The power spectra density (PSD) can also be estimated using Welch's method. For simplicity, we can directly take advantage of the MATLAB Signal Processing Toolbox 2 to get this feature.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Time-Frequency Domain Composition Features",
      "text": "However, the single domain of time or frequency can not fully depict the characteristics of the signal. For the non-steady signal, the frequency components often change with the cranial neural, cognitive process. It needs a transition from static frequency component analysis to dynamic time-frequency joint analysis, for example, adopting the short-time Fourier transform (STFT)  [75, 76]  and wavelet analysis (e.g., discrete wavelet transform)  [77, 78] . Wavelet analysis is usually considered as a type of 'Sparse Representation'. It is good at manifesting subtle local characteristics in both signal domains. Hence, it is broadly applied to process non-steady neural signals, especially for the EEG  [79] . STFT is only suitable for analyzing steady signals, and it is hard to get a high resolution simultaneously in frequency and time domain based on Heisenberg's Uncertainty Principle. Hence, we recommend adopting wavelet analysis to extract fine-grained EEG features.\n\nThe raw EEG signal is decomposed by the basis functions obtained through scaling and translating the mother wavelet. Specifically, the discrete wavelet transform (DWT) decomposes the signal into approximation and detail coefficients. Approximation coefficients describe the high-scale, low-frequency parts of the signal, while the detail coefficients describe the low-scale, high-frequency parts of the signal. The decomposition process is an iteration process, i.e., the obtained approximation coefficients of one scale can be further decomposed into more granular coefficients corresponding to the next scale. This process is iterative, yielding a series of approximation coefficients and detail coefficients belonging to different scales, as shown in Figure  4 . The effects of decomposition will be affected by what kind of mother wavelet is predetermined, e.g., the Haar wavelet, Daubechies wavelet, Bior Wavelet, etc.  [80] . For example, the Daubechies4-based DWT helps to extract the A5, D5, D4, D3, D2, and D1 signal components, the corresponding frequency range and decomposition level is listed in Table  1 . The frequency ranges of these components and their correlation with various human states are provided. Then, the wavelet engery feature and the wavelet entropy featuer can be computed in each frequency range according to the following formula 1 and 2, respectively, where is the decomposition scale, is the number of wavelet coefficients. DWT can be easily obtained through utilizing the MATLAB Wavelet Toolbox 3 .  In addition, several mode decomposition approaches, e.g., the empirical mode decomposition (EMD), the multiple empirical mode decomposition (MEMD) and the variational mode decomposition (VMD), are also quite suitable for the nonlinear unsteady EEG feature extraction, in which the multichannel EEG is decomposed into multiple intrinsic mode functions (IMFs), based on which the more representational features can be extracted  [81] . In addition to time-frequency-domain analysis, the researchers pay more and more attention to the chaotic characteristics in the neural system activities  [82] . For instance, the brain and heart are considered non-linear dynamical systems. As shown in Figure  5 , the neurophysiological signals present the properties of a non-linear dynamic system so that they can be studied and analyzed by the non-linear dynamic method. The fractal dimension is an index to describe the complexity and self-similarity of a chaotic non-linear system. EEG has multi-fractal dimensions  [83] .The brain states under different cognitive tasks correspond to specific fractal dimension  [84] . For example, Liu has found that the fractal dimension value corresponding to the high Arousal level was higher than that corresponding to the low Arousal level, so it could be used for emotion recognition  [85] . In addition, the features describing the non-linear dynamical system, such as correlation dimension, approximate entropy, Lyapunov index, K-C complexity, etc., have all appeared in some studies on emotion recognition  [86, 87] . One less studied property called Recurrence also reflects the characteristics of dynamical systems and can help to predict its evolution. Recurrence Plots accurately depict the distance correlation among trajectories in the non-linear system  [88] . Hence, several studies also focus on Recurrence Plots-based EEG representation and feature extraction  [89, 90] . However, the non-linear feature calculation cost is higher, which is disadvantageous to build a real-time recognition system. Moreover, its calculation is sensitive to parameter setting (such as embedded dimension, etc.). Simultaneously, the parameter setting is still needed to be studied and explored. In addition, differential entropy (DE) also is a kind of representative non-linear feature. It is the extension of Shannon entropy on continuous EEG variable , which are assumed to obey the Gaussian distribution ( , 2 ). Hence, it can be easily computed according to the following Formula 3.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Nonlinear Dynamical System Features",
      "text": "The effectiveness and robustness of DE has been extensively verified in related works, and have advantage over PSD  [60] . Some researches indicate that the computation of DE is equivalent to the logarithm power spectrum in a certain frequency range for a fixed length signal  [91] . More details about the non-linear dynamical features in EEG based emotion recognition can be found in the review paper of García-Martínez et al.  [92] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Asymmetry Features",
      "text": "In addition, specific phenomena found in brain cognition studies can also be taken as the feature to infer emotional states. Some cognitive studies based on brain imaging technology have seen the lateralization phenomenon of the brain in different locations in emotion processing  [93] . Clinical studies have also found that there is the lateralization phenomenon of brain activity in the brains of patients with mood disorders such as anxiety and depression  [94] . Asymmetry exists throughout the brain, and the hemispheres are not strictly symmetric in structure and function  [95] . Therefore, some studies have adopted the differences in the EEG characteristics among electrodes in symmetrical positions of the brain to indicate the emotional states. For example, the Theta, Alpha, Beta, and Gamma spectral power asymmetry (SPA) derived from fourteen electrode pairs in the left and right lobes can be extracted as features  [54] . The index asymmetry feature is generally obtained by calculating the difference  [85, 96]  or the ratio  [97]  of the indexes of two signal sources such as power spectrum, fractal dimension and so on. In addition, Petrantonakis and Hadjileontiadis  [98]  proposed the measure by estimating the mutual information shared between the brain hemispheres and further expanded to be applicable in the time-frequency domain.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Brain Network Features",
      "text": "High-level cognition function depends on subtle cooperation between local and global brain activities and is inseparable from a network of brain neurons and brain regions  [99] . There exist intrinsic correlations between EEG signals from different brain regions. Researchers believe that the functional connectivity graph and the derived structural characteristics could significantly enhance the distinctiveness of various emotions  [100] . Several studies have found the effectiveness of brain network indexes such as correlation, coherence, and synchronization in emotion recognition  [101] . Therefore, the study of the brain from the perspective of the brain network has received widespread attention  [102] . It provides a kind of 'Graph Theory' based research basis for studying the cognitive process of emotion. For example, Rotem-Kohavi et al.  [103]  analyzed several indicators, including connection density, clustering coefficient through the EEG-derived brain network. As far as we know, it takes the first step on utilizing graph theory to analyze infants' brain processing of emotional facial expressions.\n\nFig.  6 . Illustration of the brain network construction and the derived network features for classification  [104] .\n\nThe estimation and construction of brain networks are usually achieved by studying the time correlation or spectral coherence between multichannel brain signals, including cross-spectrum  [105] , Pearson correlation coefficients, mutual information  [106] , synchronization likelihood  [107] , phase-lag index (PLI), phase-locking value (PLV)  [100] , Granger causality  [108] , etc. For example, the PLV reflects the mean difference between the instantaneous phases of two channels of EEGs over time. The value of PLV ranges from 0 to 1, where 0 indicates the inexistence of phase coupling, while 1 reflects a strict phase coupling. PLV has been verified effective in evaluating the cooperation over different brain areas  [109] . Then, based on the constructed brain network, specific indicators related to network topology can be derived to build the recognition models  [104] , including the modularity, clustering coefficient, degree (in-degree, out-degree, average-degree), characteristic path length, closeness centrality, local or global-efficiency, and small-world property, etc.  [110]  (see Figure  6 ). The constructed brain network reflects the coupling correlation between two EEG channels. Hence it is not sensitive to amplitude variations. This property reduces the influence of inter-person difference and helps to build robust and accurate EEG-based recognition models. The representative EEG features utilized in related emotion recognition researches are listed in Table  2 . Even though so many candidates handcrafted EEG features can be extracted, it should be pointed out that those traditional handcrafted features are obtained based on a quantity of domain knowledge, thus improving the learning cost of researchers, especially for those only majoring in computer science. In addition, most features of the current neural signals are based on the traditional time-series signal analysis theory and method. The correlation between those signal features and the emotional states is unknown and still needs to be explored, and the effects are also limited. Furthermore, EEG variations triggered by physiologic or psychological factors can easily disturb those features, e.g., cardiac activity, eye movement, etc.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature Processing",
      "text": "3.8.1 Automatic Feature Selection Method. Automatic feature selection techniques can be categorized into 'filter method' based selection and the 'wrapper method' based selection  [111] . Either way, the obtained EEG features need to be ranked according to specific criteria, e.g., by evaluating the relationship between the features and the target emotions or assessing the feature importance derived from the model parameters. The top vital features can be reserved for further model design, while the others will be abandoned  [112, 113] .\n\nThe 'filter method' based selection does not depend on the built recognition models, and its computation cost is usually less than the 'wrapper method. ' Hence we recommend utilizing it in real-time and big-data scenarios. The most widely used 'filter methods' are the chi-squared ( 2 ) test-based approach, mutual information-based approach, ANOVA F-test-based approach, etc. The 2 test-based approach tests the independence of two variables by measuring the distribution difference between the feature variable and the emotion classes. Features with a higher 2 value have a close relationship with the target emotions that will be reserved. The mutual information is calculated to evaluate the interdependence between the specific feature and the emotions. The most representative mutual information-based approach is referred to as minimal-redundancy-maximal-relevance (MRMR)  [114] . For example, by combining the MRMR selection method with the kernel function classifier, the recognition performance can be improved  [115] . ANOVA F-test measures the difference over multiple distributions by calculating the ratio of the between-class variance to within-class variance, which reflects the degree of discrepancy. Features with higher F-ratio values can better differentiate the different emotions.\n\nThe 'wrapper method' needs to work with a specific machine learning model, among which the recursive feature elimination (RFE) method is one representative algorithm. It is originally designed by Guyon et al.  [116]  for gene selection. It works based on a sequential backward abandon scheme. The algorithm initially starts on the entire feature set. Then, some features with smaller feature weights are abandoned from the feature set. The process iterates several times until the desired objective is achieved. In addition, the highly efficient L1-norm penalty-based method is also recommended. It adds an L1-norm regularization term at the end of the original loss function to encourage weight sparsity. Regularization is one strategy adopted in machine learning in case that the feature dimensions are larger than the size of the samples. It guarantees to produce small-value model parameters to prevent over-fitting. We can abandon those features with 0 weight from the current feature set. According to some research findings, the L1-norm-based method has a big advantage over the L2-norm-based method when faced with lots of redundant features  [117] .\n\n3.8.2 Manually Operated Feature Selection. The definitions of feature importance differ with the change of the research objectives. For recognition tasks, the automatically selected features contribute to enhancing the recognition performance. However, the automatically selected features are not recommended for cognition studies to analyze cognition phenomenons. For example, Haufe et al.  [118]  pointed out that the research on the parameters of backward methods (multivariate classifiers) is not recommended in brain imaging data analysis. The derived findings may be inaccurate. Specifically, in machine learning, if several features are highly related to each other, only one feature may be reserved in model construction. Other features could be neglected by assigning relatively low weights without influencing the model performance. Simply considering the high-weight features may lose valuable information related to emotional cognition. Hence, in some studies, a manually operated feature selection method is recommended instead of the automation methods. For example, the 'searchlight' strategy can be adopted as an alternative method, by which you manually distill features from different angles, e.g., electrode groups, brain regions, rhythms, feature types, etc.  [62] . The features in critical brain regions or EEG rhythms have more impacts on the recognition performance.\n\n3.8.3 Feature Smoothing. We know that EEG is a mixture of various neuronal activities in the brain and various noises from the body or environment. The features extracted from EEG will vary within short periods, but the human emotions may be relatively stable, which means the extracted features are still a not precise reflection of the emotional patterns. In addition, though increasing the kinds of features extracted may improve the recognition accuracy, it will introduce more noise and computational cost in feature extraction, model training, and the inference task. Regarding this, feature smoothing methods are also recommended in the feature processing process to decrease the influence of the emotion-irrelevant patterns and improve the emotion recognition accuracy without increasing the feature dimension. For example, we can first divide EEG data into non-overlapping windows and extract features from each window, and further adopt the Savitzky-Golay smoothing method or the moving average method to smooth the features in time sequence  [119, 120] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Pattern Recognition Technical Routes Applied In The Field",
      "text": "Emotion recognition follows the nature of pattern recognition research: judging target samples' emotion categories based on existing data and some measurement criteria. We summarize existing pattern recognition approaches adopted in most related works using a flow chart, shown in Figure  7 , in which different technical routes are clearly divided. With the rapid development of Deep Learning (DL) in graph and image processing and natural language processing, the DL-based technical route has begun to attract the attention of researchers in this field, and existing works indeed have been proved effective  [121] . Hence, In this review, we pay more attention to Deep Learning-based studies. The following parts provide a summary of these technical routes and representative works.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Route: 0→1→2→4→11",
      "text": "The simplest method follows the route 0 → 1 → 2 → 4 → 11, namely setting a threshold for a specific feature, and if the feature value exceeds the default threshold, the sample is determined to belong to a particular emotional state  [85] . However, the threshold is fixed for the subject and depends on experience, leading to a lack of adaptability. Hence, this technical route is not mainstream, and we will not describe much on this route.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Route: 0→1→2→5→11",
      "text": "At present, machine learning have been extensively studied in this field, including the traditional supervised learning models, e.g., discriminant analysis  [101] , support vector machine  [70] , K-nearest neighbor  [78] , Bayesian method  [122] , Random forest  [123] , perceptron  [124] , etc, as well as unsupervised learning methods such as manifold learning  [74] , clustering  [125] , and so on. Traditional statistical machine learning based approaches follow the route 0 → 1 → 2 → 5 → 11 are detailedly illustrated in Figure  3 .\n\nHere are some representative approaches. Wang et al.  [74]  extracted two kinds of power spectrum features, two types of wavelet characteristics, and three types of nonlinear features from EEG. Afterward, they reduced the feature noise by the feature smoothing method and adopted linear discriminant analysis (LDA) to conduct feature dimension reduction. They finally utilized the linear SVM classifier to classify the two types of emotions. Besides, they predicted the change trajectory of emotional states based on the manifold learning method. Jenke et al.  [126]  extracted time-frequency-domain features as well as channel combination features from multichannel EEGs. Furthermore, they utilized feature selection methods based on ReliefF, minimum redundancy maximum relevance (MRMR), and statistical test method with the quadratic discriminant analysis (QDA) modeling method for enhanced emotion classification. In addition, Atkinson and Campos  [115]  combined the MRMR feature selection method and kernel function classifier to promote the emotion recognition performance. Lan et al.  [70]  and Ackermann et al.  [123]  used SVM and random forest models, respectively, to construct recognition models based on statistical features, nonlinear features, spectral features, etc. Simultaneously, they compared the recognition effects of the method for the same period data and different periods data of users. Li et al.  [127]  proposed graph regular linear regression model (GRSLR), where the sparse regularization is introduced for channel selection. Besides, the graph regularization can preserve inherent manifold topology after data embedding, thus preventing model over-fitting. Recently, Cheng et al.  [128]    for EEG-based emotion recognition task. The gcForest algorithm has fewer hyperparameters and is robust to hyperparameter settings. In addition, its model complexity adapts with different sizes of data, thus is worthy of our attention. In parts of DL-based works, the DL models are purely regarded as classifiers that play the same roles as the traditional machine learning models. The advantage of this Route compared with Route: 0 → 1 → 2 → 5 → 11 is the representation learning and the universal approximation property of the DL model that can nonlinearly transform the original features into any vector space  [129] . For example, Zheng and Lu  [60]  and Thammasan et al.  [130]  utilized the deep belief network (DBN) to classify the EEG emotions based on the handcrafted features extracted from EEG, e.g., the PSD and discrete wavelet. The DBN is proposed by Hinton et al.  [131] , it builds on multiple restricted Boltzmann machines (RBMs) to solve the training problem of deep neural networks and promotes the rapid development of Deep Learning. RBMs are a two-layered artificial neural network with generative capabilities. Compared to Boltzmann Machines, RBMs are restricted in terms of the connections between the visible layer and the hidden layer. They are able to learn a probability distribution over the input data. Typically, the DBN training includes three main steps: 1) pre-train the DBN through Gibbs sampling method; 2) the DBN is transformed into an encoder-decoder network, thus fine-tune the DBN through unsupervised back-propagation; and 3) train the DBN through supervised back-propagation, as shown in Figure  8 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Route: 0→1→2→3→11",
      "text": "The implicit correlations over different channels are a significant indicator to recognize emotions. Convolutional neural networks (CNN) are ideally suitable for processing two-dimensional data and extracting inter-channel joint information. Applying CNN to detecting emotions based on multi-channel EEG is worthy of study. Two central problems need to solve: 1) transforming the EEG data into proper representation to fit the input format of the CNN model; 2) building effective representation learning models based on various CNN modules for feature transformation. As shown in Figure  9 , we illustrate two possible representation approaches when applying 2D CNNs. For example, Yang et al.  [90]  proposed one channel-frequency convolutional neural network (CFCNN), which works with the recurrence quantification analysis (RQA). The entropy characteristics in different EEG frequency ranges derived through RQA are taken as the input of the CFCNN model. The input frame does not reserve the channel topology information. The rows of the feature map correspond to the channels, and the columns are the extracted feature in different frequency bands. Similarly, Tripathi et al.  [132]  extracted nine types of statistical EEG characteristics of signals as the input of the CNN model, and finally, the effect of this method reached and exceeded those of mainstream methods. For better data representation, the input feature map can also reserve the channel spatial topology information, for example, Li et al.  [61]  organized the differential entropy features from different channels as 2D sparse graphs, which maintains information of the electrode spatial topology, and finally used for CNN training and inferring. The constructed input feature maps could be comprehended as the input images. Hence the emotion recognition task can be resolved by the approaches adopted in computer vision tasks.  In the PrimaryCaps part, convolution operations are first performed on the data from the s part, and the convolution layers are composed of a convolution unit with a convolution kernel size of 3 × 3 and a stride of 2. After the convolution operation, the output data are reshaped to 256-dimensional (256D) vectors in 7 × 7 arrays. Then, the squash activation function is applied.\n\nThe EmotionCaps part reshapes the 7 × 7 arrays generated from the PrimaryCaps, and forms 49 by the 256D vector. Then, the vector is multiplied by weight matrix i , and the vector ˆi u can be obtained, where i denotes the index of each output class. Then, i c is determined by the dynamic ting algorithm. The dynamic routing algorithm iterates three times in this model, and i c encodes the ˆi u into a 32-dimensional activation vector of instantiation parameters Lastly, the output vector is squashed in order to determine the probability of each emotion state. The values of are 0 and 1, which correspond to the respective emotion classes. The final part attempts to reconstruct the input MFM from the final capsules, which keeps the information from the input as much as possible throughout the network. Thus, it can prevent itting and help to generalize new samples as a normalizer. The structure uses a three-layer feed-forward neural network with 512, 1024, and 324 units, respectively. This model uses L2 for reconstruction and margin loss for classification. The margin loss is shown in Equation (  6 ):\n\nT equals 1 if an emotional class is present and m = 0.1 and m = 0.9. The λ is down-weighting of the loss and it was set to 0.5 by default. e v represents the final output vector of class .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results And Discussions",
      "text": "In order to increase the number of samples, the raw EEG signals of each channel were divided into 20 sections by using a sliding window. The duration of the sliding window was 3 s, and there was no overlap region between adjacent windows. Each section was regarded as an independent nherited the labels of the original sample. The number of EEG samples per subject was 800  (40 × 20) , and the number of all samples was 25,600 (800 × 32). The total number of MFMs of all ects was 25,600, corresponding to the number of samples, as shown in Table  1 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "6.",
      "text": "Architecture of the capsule network (CapsNet)-based model. ReLU: rectified linear unit.\n\nIn the PrimaryCaps part, convolution operations are first performed on the data from the previous are composed of a convolution unit with a convolution kernel size of 3 and a stride of 2. After the convolution operation, the output data are reshaped to 256-dimensional in 7 7 arrays. Then, the squash activation function is applied. reshapes the 7 7 arrays generated from the PrimaryCaps, and forms 49 by the 256D vector. Then, the vector is multiplied by weight matrix , and the vector be of each output class. Then, is determined by the dynamic routing routing algorithm iterates three times in this model, and a 32-dimensional activation vector of instantiation parameters. Lastly, the output vector is squashed in order to determine the probability of each emotion state. The values of are 0 and 1, which correspond to the respective emotion classes.\n\nto reconstruct the input MFM from the final capsules, which keeps the from the input as much as possible throughout the network. Thus, it can prevent overfitting to generalize new samples as a normalizer. The structure uses a three-layer feed-forward respectively. This model uses L2 for reconstruction and is shown in Equation (  6 ):\n\n1 if an emotional class is present and is down-weighting of the loss and it was set to 0.5 by default.\n\nrepresents the final output vector of class",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Results And Discussions",
      "text": "In order to increase the number of samples, the raw EEG signals of each channel were divided 20 sections by using a sliding window. The duration of the sliding window was 3 s, and there no overlap region between adjacent windows. Each section was regarded as an independent of the original sample. The number of EEG samples per subject was of all samples was 25,600 (800 of MFMs of all to the number of samples, as shown in Table The CNNs are not good at recognizing features of input data when they are in different orientations. Specifically, through downsampling, pooling decreases the computation cost and can fit the variations in images. Nevertheless, the advantage of pooling is at the expense of neglecting precise spatial correlations between high-level parts, which is critical for recognizing objects with abundant spatial information  [134, 135] . To tackle this problem, recently, a new type of neural network called Capsule Network (CapsNet) inspired by neuroscience has been proposed. The brain is organized into modules, which can be considered capsules. An artificial neuron processes scalars, a capsule deals with vectors. The CapsNet can model the implicit correlations between local parts and whole objects. Besides, the CapsNet can be trained with a faster speed and requires a fewer amount of training samples compared with the CNN model. Hence, researchers have started to introduce CapsNet into this technical route. For example, Chao et al.  [133]  point out the salient correspondence between the various emotions and cortex regions can be distinguished by the CapsNet. They also proposed one input representation structure, called the multiband feature matrix (MFM), which contains the topology correlation between EEG channels and the distinction of various EEG frequency ranges. Thus, it contributes to mining emotion-related information in spatial and frequency domains. The MFM-CapsNet based approach is illustrated in Figure  10 , where the length and direction of each primary capsule indicate the existence and characteristic of the low-level representations correlated to emotions, respectively. Some researchers also believe that the traditional CNN model may be not optimal for feature learning from EEG, which is discrete in the spatial domain. Besides, a closer spatial relationship may not guarantee a closer functional relationship. Hence, adopting the 2D representation and the CNN model may neglect he complex relationship among different channels, the relationship between the functional brain network patterns and the emotion process. The graph-based description method is advantageous in extracting signals' discriminative features in the discrete spatial domain  [136] , the structural representations learned from the functional connectivity graph could capture those correlation information mentioned above. For example, the graph convolutional neural network (GCNN) allows exploring the implicit correlations among the multiple graph nodes that represent the EEG channels. Similar to the approach shown in Figure  11 (A), Wang et al.  [100]  built one typical GCNN model on the EEG derived graph. The graph is a fusion of the within-frequency functional connectivity graph (FCG) and the cross-frequency FCG. The within-frequency FCG is obtained through computing PLV for every pair of channel signals in the same frequency band, while the cross-frequency FCG does not require the signals to come from the same frequency bands. Those two graph representations are concatenated into a big graph with × nodes, where and denote the counts of the channel and the frequency band, respectively. The experiments verify that GCNN performs better than CNN on the FCG representation. In view of the dynamic process of functional network, similar to the approach shown in Figure  11 (B), Song et al.  [137]  proposed a dynamical graph convolutional neural networks (DGCNN) To further purify pseudo pathways, we proa sparse variational scaling (SVS) module to learn scalfactors imposed on those pathways. In contrast to the local aggregation, the variational scaling is shared on all be understood as a holistical weighting gy. The scaling factors could be not only used for genve features for final emotion prebut also fed-back into those candidate pathways to Pathway Candidates Generation For the input EEG sigevery electrode in each local brain region. Formally, for a random walk of a certain length , the ordered nodes denoted as • • • by the following distribution:\n\ny between value being set to 1 if in Fig.  2 ) or 0 otherwise, is the normalizing constant, and represents the set of of adjacent electrodes enclosed in the region ∈ {F, T, P, O, C ). Note that, in this paper, the symbols abusively used as nodes or node signals for the y could be easily understood to the context statement.\n\nPathway Coding For the candidate pathways, the seve model LSTM (Gers, Schmidhuber, and is employed to encode the dependencies on walking nodes and obtain their embedding representation. Formally, for a given pathway • • • , the encodis defined as follows: Pathways Aggregation To find those salient connections between eletrodes, we use max-aggregation to Fig.  12 . The framework of the heuristic variational pathway reasoning (VPR) method that can adaptively determine salient pathways to facilitate EEG emotion recognition  [138] .\n\nAlso encouraged by the research findings that connections and pathways exist between spatially-adjacent and functional-related areas during emotion expression  [139, 140] . As shown in Figure  12 , Zhang et al.  [138]  proposed a heuristic variational pathway reasoning (VPR) method that introduced random walk to generate candidate pathways along electrodes. LSTM was used to encode their ordered connectivity into high-level features of pathways that indicate between-electrode dependency to represent each pathway. They also proposed a salient pathway reasoning method, which includes two basic modules named pathway aggregation and sparse variational scaling. It can adaptively determine salient pathways to facilitate EEG emotion recognition and provide some explanation for emotion analysis. Based on the interpretable model, they explored salient interaction pathways w.r.t. different emotions. This research sets a new SOTA for the SEED dataset.\n\nAnother similar work is illustrated in Figure  13 , considering the importance of the asymmetrical information between the hemispheres in emotion cognition, one bi-hemispheric discrepancy model (BiHDM) is developed by Li et al.  [141] , in which two individual horizontal and vertical traversing RNNs were employed to scan all left separately-and right-hemisphere channels' EEG features to learn the deep features of two hemispheres. Different from the prior work in Figure  12 , the electrode pathways here were predefined. After the deep representations of each channel above have been obtained, they performed pairwise subtraction, division, and inner product on the paired channels on symmetric locations of the brain region as the asymmetry information between two hemispheres is supposed to be more discriminative recognizing emotions from EEG. Another RNN models the obtained asymmetric representations, and the two-directional streams information are fused for final classification. In addition, they added a domain discriminator into the model to extract domain-invariant data representation. Before this work, Li et al.  [142]  has developed a bi-hemispheres domain adversarial neural network (BiDANN) based on the same neuroscience hypothesis. They fitted the cerebral hemisphere asymmetry information into the framework and took the domain adaptation. The framework has two feature extractors. Two local discriminators reduce the distribution discrepancy between the left and right hemisphere domains, respectively. Then, the global discriminator lessens the overall distribution discrepancy between two domains. The left and right hemispheric features are extracted through LSTM modules. To the best of our knowledge, for the first time, researchers introduced the hemisphere' asymmetry theory into the DL model design and verified that prior neuroscience knowledge is beneficial for guiding the modeling training but usually neglected by prior works. characteristics of EEG emotional signal is a challenging topic in EEG emotion recognition area, explicitly extracting the discrepancy between two brain hemispheres by deep learning models will be meaningful and further improve EEG emotion recognition, and will be explored in this article.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B. Other Existing Methods For Eeg Emotion Recognition",
      "text": "Various machine learning methods have been proposed to solve EEG emotion recognition in the literature. For example, Zheng  [10]  proposed a GSCCA method for simultaneous EEG channel selection and emotion recognition. Li et al.  [24]  proposed a graph regularized sparse linear regression (GRSLR) method to deal with EEG emotion recognition. Recently, using deep learning methods for EEG emotion recognition has been increasingly adopted and demonstrated better performance than traditional methods. Zheng and Lu  [12]  proposed to use DBN for EEG emotion classification. Zhang et al.  [25]  employed a multidirectional RNN layer to capture long-range contextual cues by traversing the spatial regions of each temporal slice along different directions. Song et al.  [26]  used the dynamic graph convolution neural network (DGCNN) to model the multichannel EEG features and they also presented a novel attention-long short-term memory (A-LSTM)  [27]  which strengthens the effectiveness of useful sequences to extract more discriminative features. However, these existing methods only model the relation between EEG signals and the emotion class labels to obtain the deep features. It is better to concentrate on capturing the emotion-specific information, which is more important and will improve EEG emotion recognition. We will compare the proposed methods with the aforementioned ones in the experimental study.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Iii. Proposed Odel For Eeg Emotion Ecognition",
      "text": "To specify the proposed method clearly, we illustrate the framework of the BiHDM model in Fig.  1 . Its goal is to capture the asymmetric differential information between two hemispheres. We adopt three steps to achieve this goal. The first step is to obtain the deep representations of all the electrodes' data. Subsequently, the relationship is characterized between the identified paired electrodes on two hemispheres, and hence a more discriminative and higher level discrepancy feature is generated for final classification. Third, a classifier and a discriminator are leveraged to cooperatively induce the above process to generate the emotion related but domain-invariant features. The overall process is described as follows.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A. Obtaining The Deep Representation For Each Electrode",
      "text": "Note that EEG signals can be treated as a set of sequences since the dimensions of all the electrodes are same. For this reason, the EEG data can be fed into the RNN module to extract high-level deep features, which will contain the spatial relation information. In BiHDM, we separately extract the EEG electrodes' deep features on the left and right brain hemispheres by using two independent RNN modules. Meanwhile, to consider the intrinsic structural information of EEG data that reflects the relations between brain regions, for each hemispheric EEG data, we build the RNN module traversing the spatial regions under two predefined stacks, which are determined with respect to horizontal and vertical directions. These two directional RNNs are complementary to construct a complete relationship of electrodes' locations. Concretely, for an EEG sample , it is split as , . . . , , . . . , , where and   To tackle this issue, similar to the hypothesis of brain asymmetry in emotional processing adopted in the BiHDM and BiDANN models mentioned above, as shown in Figure  14 (A), Huang et al.  [143]  proposed an end-to-end bi-hemisphere discrepancy convolutional neural network model (BiDCNN) that recognize the different emotions based on the asymmetry information between the two hemispheres. They transformed the multi-channel EEGs into 2D frames with a shape of 9 × 9, which reserves the knowledge of channel topology. Three different kinds of feature frames are proposed, namely are the original EEG value matrix (OEFM), the bi-hemisphere symmetric matrix (BiSSM) derived by subtracting the symmetrical electrode pairs' values, and the bi-hemisphere division symmetric matrix (BiDSM) derived by dividing the symmetrical electrode pairs' values. In BiDCNN, a 2D convolutional layer is utilized to learn from each of the three preprocessed data. Another end-to-end regional-asymmetric convolutional neural network model (RACNN) was proposed by Cui et al.  [144] . As shown in 14(B), it consists of three parts of feature learners to extract temporal, regional, and asymmetric features, respectively. Three-dimensional convolution functions are utilized in temporal feature extractors to mine time-frequency characteristics. The regional feature extractor uses two-dimensional convolution functions to mine regional characteristics from neighboring electrodes. At last, the asymmetric differential layer (ADL) is designed to capture long-distance information between symmetric positions. The original multi-channel EEGs are transformed into the 3D tensor , which reserves the topology information of the electrodes. Fig.  15 . The end-to-end 2D convolutional neural networks based spatial-temporal Deep Learning model (EEGNet)  [145] , in which the input EEGs are randomly arranged into a 2D frame.\n\nFurthermore, as shown in Figure  15 , Lawhern et al.  [145]  designed an EEG-specific ConvNet model (EEGNet) by integrating depthwise and separable convolutions. Even though the original EEGNet was only validated on the motor imagery classification tasks, its idea was further verified and compared with the EmotioNet model proposed by Wang et al.  [146] , which adopted the 3D EEG representation method shown in 17(B). Islam et al.  [147]  proposed one efficient recognition method with lower computational complexity, lower memory requirement, and lower time consumption. Only applying the traditional CNN model to the channel correlation matrix of Pearson's correlation coefficient can achieve ideal performance. Inspired by the works shown in Figure  10 , Liu et al.  [135]  developed one end-to-end CapsNet based approach that builds directly on the raw EEG signals and judge the emotions. It has three modules, namely, ConvReLU, PrimaryCaps, and EmotionCaps. Compared with the prior MFM-CapsNet approach, it works without the need for any feature design and extraction stages. It incorporates multi-level learned representations into the primary capsules. One drawback of the approach mentioned above is it's only suitable for processing short input signal segments with only a few second lengths. The dependencies of a long trial signal can not be fully mined. Hence, In addition to the 'end-to-end modeling' problem, researchers also pay attention to the 'context modeling' problem to mine long signal dependencies. Specifically, the works mentioned above are only suitable for modeling global static information. Nevertheless, the human's emotional cognitive process is not static but continuously evolving. As shown in Figure  16 , the subjects' specific emotions generally evolve over the experiment with the fluctuations of the EEGs. Hence, the reported so-called ground truth emotional label of one trial only reflects their overall evaluation of their emotional experience. The temporal and fluctuant property of the EEG has been neglected in several prior related works. Contextual learning ability should be considered in the DL model study. Fig.  17 . Two representative 3D EEG representation methods in emotion recognition when adopting the 3D-CNN-based approaches. A: The raw multichannel EEGs are randomly arranged into a 2D frame and 3D cube  [148] . B: The raw multi-channel EEGs are arranged into a 2D frame and 3D cube according to the 10-20 topology of electrodes  [146, 149] .\n\nVarious types of recurrent neural network (RNN) have been successfully applied in EEG-based emotion recognition, including the GRU, the LSTM, and the simple recurrent units (SRU). For example, Wei et al.  [151]  proposed to use ensemble SRU networks to learn from the features sequences of different EEG rhythms obtained through wavelet transform. Although the recurrent neural networks (RNN) is good as sequential modeling tasks, you also can conduct end-to-end contextual learning only based on CNN without the help of the RNN. Utilizing 3D CNN for sequence modeling has been extensively explored in video analysis, e.g., action recognition  [152] . Hence, inspired by those studies, 3D CNN also has been introduced into this area. For example, Salama et al.  [148]  proposed one 3D-CNN model that multi-channel EEGs are randomly arranged into frames. As shown in Figure  17(A) , consecutive frames are further concatenated together into one 3D cube. Besides, since those current open-source EEG datasets do not collect enough trials for each subject, the data augmentation strategy is adopted by adding white Gaussian noise to the original signals. The effectiveness of the 3D-CNN on long sequence modeling is verified in this work. Wang et al.  [146]  also proposed one 3D CNN model, called EmotioNet, which integrates batch normalization and dense prediction mechanism for resolving issues of covariance shift and the unreliability of ground truth labels. Specifically, as shown in Figure  17(B) , Consequently, the output of this layer only has temporal characteristics, which are fed into the following two layers to conduct high-level temporal representation learning. At the end of the model, a dense prediction is utilized to make a time-varying emotion state prediction. Experiments show that the EmotioNet performs better than the aforementioned 2D EEGNet (see Figure  15 ) proposed by Lawhern et al.  [145] . Jia et al.  [153]  proposed one spatialspectral-temporal-based attention 3D dense network, called SST-EmotionNet, which consists of the spatial-spectral stream and spatial-temporal stream. Each stream consists of several attention-based 3D dense blocks. In the end, the two parallel streams are fused for classification. Although the EmotioNet and SST-EmotionNet seem alike in name, they are different models. In addition, the input of SST-EmotionNet is differential entropy feature-based 3D representation instead of raw EEG signals that the EmotioNet can process. Hence, it may be the weakness of the SST-EmotionNet. Cho and Hwang  [149]  also introduced two types of 3D-CNN models, namely C3D and R(2 + 1)D. They adopted the 3D EEG representation method shown in Figure  17(B) . The input raw EEGs are arranged into 2D frames according to electrode topology, and the interpolated 2D EEG frames are further concatenated into 3D cubes. Unfortunately, the aforementioned 3D CNN-based approaches only verified on the few second long sequences. Conducting consecutive 1D-CNN operations also could effectively extract spatial-temporal information from raw multichannel EEGs. Inspired by the Inception block of GoogleNet, Ding et al.  [150]  proposed the TSception model. As shown in Figure  18 , it consists of two types of 1D convolution-based learners for end-to-end temporalspatial information modeling. Correspondingly, the channels in the input frame are deliberately arranged according to which hemisphere they locate. Then the spatial learner adopts a multi-scale 1D convolution operation to learn the asymmetry features from both hemispheres. The temporal learner adopts multi-scale 1D convolution operations that help to extract multiple temporal and frequency patterns. Integrating both the ability of CNN and LSTM to build hybrid Deep Learning models is a natural choice. For example, as shown in Figure  19 (A), Li et al.  [154]  propose a wavelet transformation-based preprocessing that transforms the multi-channel EEG into scalogram based 2D frame representation. Each frame reflects the energy distribution of the multi-channel EEG in a time slice. Further, they designed one hybrid DL model, called C-RNN, which fuses CNN and RNN. Specifically, the CNN module can decode inter-channel relationships, and the RNN (LSTM) module helps capture contextual information from sequential data. Even though this work does not achieve very high performance, this work contributes to the further development of end-to-end and hybrid EEG emotion recognition models. Zhang et al.  [155]  introduce cascade (see Figure  19(B) ) and parallel (see Figure  19(C )) hybrid DL models integrate CNN and RNN, in which the input is the raw EEG signal arranged according to electrode topology, each input map corresponds to a signal timestamp. The model can effectively learn the joint spatio-temporal representations from raw EEGs, the complex dependencies between adjacent signals, and the contextual information can be fully mined. For cascade model, it follows the same mechanism as the works shown in Figure  19(A) . It first learns the spatial representation from each data frame, and the sequence of the learned spatial representations is further carried to the RNN to learn temporal representations. Unlike the cascade structure-based model, the parallel structure-based model learns the spatial and temporal representations from EEG parallelly. At last, the concatenated representations are utilized for final recognition. Both the cascade and parallel models consistently outperform the SOTA methods. Almost at the same time, a similar parallel hybrid model is proposed by Yang et al.  [156] . They introduce a preprocessing method that removes the nonstimulus pre-trial baseline signal from the stimulus trial signal. Based on the preprocessed data, the parallel hybrid model's accuracy is greatly improved by 32%. Attention is a special mechanism in the information processing of the human brain, hence, inspired by those neuroscience findings, Tao et al.  [157]  introduced attention mechanisms into the aforementioned CNN-LSTM hybrid DL models. They integrated channel-wise Attention and self-attention into the CNN and RNN, respectively, to learn attention characteristics among the channels and the attention characteristics within a sequence. There are several potential weaknesses in the works mentioned above: Firstly, both the cascade and parallel models mentioned above require a 2D representation of EEG channels. If we represent them according to their topology, it may cause information loss because channels are actually arranged in the 3D space, and the 2D frame has multiple positions of the null electrodes that need to be padded with zeros. Secondly, these approaches utilize RNNs to capture inter-channel and inter-time relations. However, each 2D frame corresponds to a time step rather than a time window. Hence, if the signal is extremely long with a high sampling frequency, the model computation burden will be largely increased, especially for RNN models. Thirdly, such a two-stage approach is somewhat inconvenient to implement. The whole process is time-consuming and highly dependent on domain knowledge. Hence, developing approaches modeling directly on the original multi-channel EEGs regardless of considering the topology is worth studying.   10  . The region to global (R2G) process includes two streams. The spatial stream constructs the relation in and among all the brain regions hierarchically, while the temporal stream captures the EEG signal's dynamic information as well as learning from the brain regions' time sequences  [158] .\n\nWhen building a spatial-temporal model, the spatial information can also be effectively processed by the RNN model without the help of CNN. For example, Zhang et al.  [159]  designed a spatial-temporal hybrid DL model called STRNN that only integrated RNN modules. It utilized RNN to learn the temporal dependencies and to capture the spatial dependencies in the multi-channel context. Firstly, a quad-directional spatial RNN (SRNN) layer scans each slice from different angles. Following the SRNN, a bi-directional temporal RNN layer (TRNN) learns the long-term temporal dependencies by the forward and backward processing of the sequences. Li et al.  [158]  proposed one LSTM based regional-to-global brain spatial-temporal neural network model (R2G-STNN) that realizes the regional to global hierarchical feature learning. A bidirectional long short-term memory (BiLSTM) network was adopted to learn spatial characteristics to model the regional correlations among EEG channels. Further, the regional attention layer is also introduced in the R2G-STNN model to differentiate the importance of different brain regions in the emotion process. The attention layer learns and assigns weights to increase or reduce the influence of different regions. At last, BiLSTM is adopted to learn the temporal dependencies of regional and global spatial representations. This work also adds one discriminator to solve the domain shift problem. Lew et al.  restricted Boltzmann machines (generative model), and variational AutoEncoder (generative model) to decode the source signals from the raw EEGs, then further utilized the LSTM for sequence learning and emotion recognition. One weakness of this work is the input sequences for LSTM processing are a sampled sequence of the entire decoded latent source signals that reduce the computation cost meanwhile will produce information loss. A similar AE+LSTM model was also proposed, whereas handcrafted feature extraction is needed to construct sequence before fed into the LSTM model  [162]  that also may lead to information loss. They are not strict end-to-end models, still need extracting intermediate latent EEG source signals. Such a two-stage approach is somewhat inconvenient for practical application. As mentioned in Route: 0 → 1 → 2 → 3 → 11, the graph neural networks (GNN) is capable of decoding the intrinsic correlations among the multi-source signal. Nevertheless, the functional connectivity between two channels is not static but continuously changing with the evolution of the emotional process. Hence, developing GNN based models that can capture the functional connectivity change between EEG channels may greatly enhance the emotion recognition effect. Combining the GNN model with some sequence modeling methods is one direct way, which is similar to the ideas shown in Figure  19 . For example, Yin et al.  [163]  proposed one hybrid DL model (named ECGGCNN) that integrates GCNN and LSTM. The GCNN module helps to learn the channel connectivity within a time slice. A parallel GCNN computing mode is designed to receive data frames in sequential order and transports the learned representations to the LSTM layer, which is used to model the evolution of the channel connectivity. At last, the dense layer predicts final emotions according to the LSTM's learned contextual information.   I .\n\n4) BCI-IV 2b: 2b dataset was collected by Fig.  23 . A temporal-spatial EEG information Riemannian fusion network (RFNet) for affective BCI  [164] .\n\nTraditional BCI solutions rely on Riemannian geometry, in which the spatial covariance matrices (SCM) derived from raw EEGs contribute to developing BCI algorithms. As the SCM is symmetric positive definite, it lies in Riemannian space rather than the Euclidean space, the models designed in Euclidean space can not be directly employed  [165] . To tackle this problem, as shown in Figure  23 , Zhang and Etemad  [164]  proposed one end-to-end Riemannian fusion network (RFNesst), which separately extract spatial representation in the Riemannian space and temporal representation in the Euclidean space. Finally, the attention strategy guides the fusion of different learned embeddings. One key operation is mapping the spatial representations extracted from SCM in Riemannian space to the embeddings in Euclidean space through tangent space learning. Finally, an MLP is connected to process the spatial information. For the temporal representation learning in Euclidean space, they utilize the attention-LSTM network to learn from the EEG frequency sub-bands. The learned temporal representation is transported forward to a fully connected layer to get the high-level representations. At last, the latent spatial and temporal representations in Euclidean space are concatenated for final emotion recognition tasks.\n\nThough lots of works mentioned above adopted the RNN based approaches, they do not talk about the computation cost. Actually, the RNNs tend to be more computationally intensive than CNNs, especially when the target sequence is quite long. The reason for that is because RNNs are very memory intensive with backpropagation through time.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Route: 3→7→11 And 5→7→11",
      "text": "We hope the developed AI system can have consistent and robust performance on a wide range of data domains. Nevertheless, the difference in data distribution among users would result in degraded recognition performance. For example, Kim and André  [166]  firstly built a subject-dependent model with the specific user data, the 4-class emotion recognition rates can reach 95% on intra-subject data. However, when one subject-independent recognition model was established with the mixed data of three users, the data distribution deviation among three users caused the recognition accuracy to be reduced to 70%, suggesting the simple and crudely built user-independent models will inevitably have low robustness. Likewise, Petrantonakis and Hadjileontiadis  [98]  verified the proposed method in the specific individual data and non-specific individual data, respectively. The experimental recognition accuracy of the subject-dependent model was 70%∼100%, while for the subject-independent model, the performance decreased to 10%∼20%. AlZoubi et al.  [167]  analyzed the physiological signals of 27 students under eight kinds of emotions. They found that the consistency of emotional physiological response patterns of different individuals was poor and even proposed that user-independent modeling methods were not feasible in EEG-based emotion recognition research.\n\nThis problem is typically referred to as 'domain shift'. The EEG data exhibit 'domain shift' problems due to various factors. The differences among source users (such as gender, culture, gene, etc.) would lead to different neurophysiological activity patterns. Some studies have shown that the asynchronous activity the brain presents different patterns in different individuals  [168] . Regarding gender factors, there has been a tremendous amount of research on the difference between men and women in processing emotional stimuli. For example, researchers found that men and women showed different scalp activity patterns and distributions in processing emotional information of music  [169] . Bilalpur et al.  [170]  adopted the EEG to examine the gender difference in facial emotion recognition. They found that women were more sensitive and faster to recognize negative emotions than men, irrespective of age, even when only partial information was provided. Goshvarpour and Goshvarpour  [171]  assessed EEG powers in depressing, fun, and sad music videos for women's and men's groups, respectively. They found the mean power of all frequency bands in the women's group was significantly higher than that of the men's group. There were significant gender-related differences in parietal lobe activation for depressing and sad music videos and limbic lobe activation for fun stimuli. It is believed that biological and sociocultural factors cause the differences  [172] . In terms of biological factors, Lee et al.  [173]  revealed that the right insula and left thalamus were consistently activated for men but not for women during the emotional experience. They also suggested that men evaluate current emotional experiences by recalling past emotional experiences, whereas women tended to evaluate current emotional experiences rapidly according to the immediate stimuli. In terms of the genetic factor, Raab et al.  [174]  revealed that serotonin transporter gene (5-HTTLPR) polymorphisms are closed correlated with brain activation during facial emotion processing. In terms of sociocultural factors, women socialize differently than men, which is not decided by genetic factors but by social norms defined by politics, culture, and religion  [175] . In Western culture, at least, women are more emotional than men and more reactive to unpleasant events  [172] . Zhu et al.  [176]  studied the cross-gender EEG modeling, the recognition performance of female models is better than male models. It indicates that women share more stable EEG patterns during emotional experience than men. Pava et al.  [177]  conducted a special study on the gender differences present in the EEG-based emotion recognition system. They found the gender differences in the classification performance, the gender differences in differential entropy features extracted from EEG, and the gender differences in evaluating the emotion experienced in the Valence dimension. Regarding cultural factors, Huang  [178]  studied the evaluation of emotional images by Chinese and foreign groups. The finding showed that the viewpoint difference between Western and Chinese users would lead to large differences in emotion evaluation. Kurbalija et al.  [179]  conducted experiments with several Serbian and Chinese subjects. They observed that cultural differences between the subjects did not significantly impact the recognition tasks and models. Nevertheless, Gan et al.  [180]  found that French has higher mean accuracy on beta frequency band while Chinese tends to perform better on gamma frequency band on tasks of recognizing emotions. They also found similarities and differences in connectivity patterns between Chinese and French subjects. Hence, we can not say the demographic factors will not affect the model building. We should pay more attention to the these factors in developing and assessing EEG-based emotion recognition systems.\n\nThe domain shift problem will appear not only in different sources of EEG data, but they could also appear in the same source of EEG. Take the DEAP dataset for example, as shown in Figure  24(A) , the instantaneous distribution of EEG continuously evolves that causing the data non-stationarity issues. The reason lies in the mental change of a participant or the technical factors, e.g., drying electrode gel changes. Therefore, the distributions of different epochs might be different. As shown in Figure  24 (B), inter-subject variability also is a representative domain shift problem, which indicates there also exist discrepancies among the statistical characteristics of different subjects.\n\nThough much research has studied subject-dependent modeling, the construction of a user-independent recognition system can meet practical application requirements. This drives the relevant researchers to focus on and improve the effect of user-independent approaches. At present, there are mainly four ways to solve the above problems, as follows:\n\n(1) One way is calibrating or aligning the physiological signals among the participants. The calibration-based methods reduce the difference in physiological measurement among the participants by using the baseline characteristics of participants. For example, Mohammad and Nishida  [181]  calibrated the level of physiological signals of each participant and took the physiological characteristics at calm state as baseline characteristics. The physiological characteristics under various emotional states subtract the baseline characteristics to obtain the calibrated characteristics. Then, the relative physiological characteristics were used to establish the prediction model of emotion and obtained a good experimental effect. The alignment-based methods are widely employed in brain-computer interface studies. Researchers proposed to align the multichannel EEGs of the source domain and target domain in the common Riemannian manifold space and judge the states according to the Riemannian distance between each state center and the EEG covariance matrix  [182] . Fernandez et al.  [183]  studied different feature normalization methods combined with the deep neural network. The results show that Fig.  24 . Domain shi problems in DEAP data set 11 . A: non-stationary EEG distribution between two epochs. B: inter-subject EEG variability under the same trial  [146] .\n\nthe proposed stratified normalization-based neural network significantly outperforms batch normalization-based approaches in cross-subject emotion recognition settings.\n\n(2) Another way is to establish aspect-oriented models according to specific factors that cause domain shift. Zhou et al.  [184]  established culture-specific model and gender-specific model for 46 participants, as well as compared their performance with the general models. The experiments showed that the specificities in gender and culture would affect recognition performance. The emotion model built on the user of the same culture or gender specificity improves the recognition accuracy. Similarly, Bailenson et al.  [185]  introduced the individual-specific model, gender-specific model, and general model, respectively. The experiments suggested that the individual-specific model and gender-specific model had a higher recognition accuracy than the universal model. Chen et al.  [186]  proposed a user grouping-based approach, specifically the modeling workflow contains three stages, including user grouping, emotional state pool partition, and final state discrimination. Liu et al.  [187]  also introduced subject clustering into cross-subject emotion recognition, as shown in Figure  25 . Based on the clustering results, cluster selection was conducted to match the target subject with one optimal source cluster, whose source subjects have similar emotional EEG activity patterns. Subspace alignment method is further utilized for selecting the optimal sources with possibly 'positive transfer'. Finally, the emotional state of the target data is decided by majority voting of the optimal sources.\n\n1. The framework of the proposed DASC method of the located classifiers were regarded as the sources ve transfer\" on the target and selected. For each selected source, we aligned the target with it by of the to make cross-subject classification on the aligned , the predictions from the classifiers of selected voting strategy as the of the target.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Iii. Experiments",
      "text": "A. EEG Dataset was evaluated using the DEAP is a well-known publicly available dataset 32 participants. Each participant watched 40 one-minute music videos with simultaneous EEG at a sampling rate of 512 Hz. After watching each ve feelings of valence, on a continuous scale from 1 to 9.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C. Experiment Details",
      "text": "In this work, binary classifications of valence and arousal valence or arousal ratings larger than 5 regarded as positive or active, while the ratings lower 5 were taken as negative or passive respectively. We adopted \"one-subject-out\" cross-validation tests and y of 32 subjects to evaluate the of the proposed DASC method. Each time, one s data were excluded from the training set as the of each subject in the remaining as an independent source domain.\n\nIn source clustering, we compared the values of Silhouette 2 to 5), and divided the source domains into the number clusters with the Fig.  25 . The framework of the subject clustering based cross-subject recognition method 12 . Cluster selection is required for selecting the optimal source cluster. Source selection is utilized for further selecting the optimal sources from the optimal cluster. The emotional state of the target is decided by those optimal sources  [187] .\n\n(3) In recent years, the Transfer Learning method has been paid more and more attention by scholars. For a domain = { , ( )} with a feature space and the corresponding marginal probability distribution ( ). When source domain { , ( )} and target domains = { , ( )} lie in the same feature space, namely = , and modeled for the same type of task, the domain shift issue can be resolved through Transfer Learning or called domain adaptation approaches. Transfer Learning maps the EEG features from source and target domains into the common feature representation space, where the inter-subject or inter-session shifts of the EEG data are adjusted, and distinctive features across subjects or sessions are obtained. For example, Lan et al.  [188]  verified various domain adaptation methods, including maximum independence domain adaption (MIDA) and transfer component analysis (TCA), which is believed able to decrease inter-subject variance as well as the inter-dataset discrepancies. By utilizing those domain adaptation methods, only a simple logistic regression model can have a significant performance improvement in both cross-subject and cross-dataset experimental settings compared to the baselines that have no domain adaptation capabilities.\n\nTransfer Learning techniques have also been extensively studied in Deep Learning. Among them, a naive approach draws lessons from experience in computer vision. They realize cross-domain application by fine-tuning the source domain model on the target domain data, including fine-tuning the 11 Reprinted from  [   whole network model and fine-tuning only part of the network structure in the target domain. For example, Cimtay and Ekmekcioglu  [191]  proposed to use the Inception Resnet model that pre-trained from the multi-subject raw EEG data, and they obtained promising cross-subject recognition performance on three benchmark datasets. Wang et al.  [192]  proposed a residual block-based CNN, which is trained on the electrode-frequency distribution maps (EFDMs) with short-time Fourier transform (STFT), the pre-trained model on SEED dataset can be successfully transferred to apply on DEAP dataset. Integrating domain adaptation mechanisms into the Deep Learning model is increasingly gained attention. For example, as shown in Figure  26 (A), Luo et al.  [189]  proposed the Wasserstein generative adversarial network domain adaptation (WGANDA), transferred the differential entropy characteristics of different domains into the common space, which is helpful to improve the emotion recognition effect across participants. Inspired by the same idea, Li et al.  [193]  also developed one domain adaptation neural network (DANN) based on a deep adversarial network. This model contains components of one feature extractor, one label predictor, and one domain discriminator. The feature extractor is trained in the direction for deceiving the domain discriminator by maximizing the domain discrimination losses. In this way, the feature extractor eliminates the domain-specific characteristics of the input for the purpose of increasing the domain identification loss. The multi-kernel maximum mean discrepancies (MK-MMDs) were utilized for measuring the domain discrepancies. By simultaneously optimizing the loss functions of the MK-MMDs and the task, the DAN can reduce domain shift across domains, meanwhile preserving domain-invariant and task-related features. A multi-source adaptation transfer network (DMATN) for cross-subject emotion recognition is proposed by Wang et al.  [190] , as shown in Figure  26(B) . The mechanism of this model is exactly similar to the model proposed by Li et al.  [193] . The difference between these two approaches is the DMATN needs to select target domain-related source domains before modeling, and the features are automatically learned by the networks instead of handcrafted features. Cai et al.  [194]  also follows the DAN-based approach and proposed one model called maximum classifier discrepancy (MCD) for domain adversarial neural networks (MCD_DA). MCD_DA not only adopts the GAN module to adapt the feature distribution between the source and target domains, but also it maximizes the classifier difference between the source and target domains. Zhao et al.  [195]  proposes a plug-and-play domain adaptation method based on LSTM-Encoder-Decoder, in which the subject-invariant representations are modeled by the shared encoder and the subject-private representations are modeled by the private encoders. The target prediction is the integration of the shared classifier with those of individual classifiers ensemble.\n\nIn the aforementioned methods, the DANN regards each domain as a whole, ignoring the class boundary in each domain. The MCD considers the specific class-boundary and trains adversarially to relocate the target feature to be inside the source features. However, since the original feature space of source and target are related but distinguishable, MCD will eliminate useful features, especially when the two domains are far more than similar. To eliminate the problems of DANN and MCD simultaneously, Ding et al.  [196]  proposed the task-specific domain adversarial neural network (T-DANN). Another problem that needs to be mentioned here is that although many works adopt discriminator-based domain adaptation approaches, it's a challenge to apply on the target domain with few-labeled data. Hence, Wang et al.  [197]  proposed one few-label adversarial domain adaptation (FLADA) approach for cross-subject emotion recognition task. The FLADA originates from Meta-learning, which is to find a feature representation that is broadly suitable for the target subject and source subject with limited labels. This approach can be applied to all Deep Learning models.\n\nRecently, Zhang and Etemad  [198]  proposed a novel knowledge distillation-based knowledge transfer pipeline to distill EEG representations via capsulebased architectures, as shown in Figure  27 , the pipeline contains a teacher network and a student network. They first pre-train a large model (teacher network) on the large amounts of available cross-subject data. Then, using the pre-trained teacher to learn information embedded in capsules with intra-subject data. At last, the training of the lightweight student network on intra-subject data can be guided by the privileged information learned by the teacher via capsules. This knowledge distillation-based approach improves the robustness when faced with limited training samples and maximally compresses the model with minimal loss in performance. This approach follows the modeling idea of 'generalization to concretization'. Zhong et al.  [199]  proposed a regularized graph neural network (RGNN) to simultaneously resolve the domain shift problems of inter-subject variability and inconsistent/noisy emotion labels. Specifically, two regularizers are integrated into the model. One regularizer is the node-wise domain adversarial training (NodeDAT) mechanism, which regularizes RGNN to generalize well in inter-subject recognition scenarios. NodeDAT is a fine-grained regularization method to correct domain shift for each channel. Another regularizer is the emotion-aware distribution learning (EmotionDL) mechanism, which solves the problem of inconsistent emotion labels by learning the label distribution instead of the hard labels. To improve the recognition performance when facing large amounts of noisy labels.\n\nThe transfer process also could be accelerated by applying Meta Learning. For example, Duan et al.  [200]  introduced the meta update mechanism (MUPS-EEG) for cross-subject classification. MUPS-EEG involves interaction between a base learner and a meta learner during meta training, each formed with a representation learning network and a prediction learning network. Duan et al.  [201]  proposed to utilize the model-agnostic meta-learning (MAML) algorithm to perform under limited target data, as shown in Figure  28   is consistent with the number of categories in our classification task and can be empirically tuned for regression tasks. We enable the higher level capsules to have degrees of freedom by setting H > d have been used to lower level caplevel capsules  [5] . The capsule network lower level capsules ('part' into higher level ones ('whole' information) through , a CapsNet used as an attenis established between the prediction vector of higher level capsules ( ). lower level capsule [1, A to level capsule\n\n[1, K . The prediction vector is expressed as the multiplication of weight matrix ij of lower level capsule ij to capsule . The output of higher level capsule is the squashed output of of (0 1) ij of logit ij , where ij be updated by the iterative process ij ij + B.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Proposed Method For Knowledge Distillation",
      "text": "An overview of our novel knowledge distillation framework is illustrated in Figure  1 . We develop a knowledge distillation work to compress the large model without performance degradation. To do so, we first employ the LSTM-CapsNet ve as the teacher network. Next, we on cross-subject data and then fine-tune it on intra-subject data in order to adapt to subject-specific At last, we train the student model with the help of privileged information learned by the evaluate it on intra-subject data. In order we learn the in of lower level capsules, as well in higher level capsules, through Lower Leve level capsules contain local features whe of the For example, when trained with h of lower level cap on digital-specific variat of the capsules' inter-dime we first ca of the lower level capsules as:\n\nG, G covar lower level capsules of the teacher ( ) vely. Dimension is kept of lower level capsu or equal to the student's we compute the square euclidean distance covariance matrices  [27]  as sim",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "|| || || || || ||",
      "text": "|| || is the Frobenius norm.\n\nlevel capsules include global inform of their output vectors to that capsu is further used as 'soft target , we employ of information dist level capsules between teacher ( ) and stu is used:\n\nis the softmax operator, i a logarithm operation to help accelerate the distill = 1 experiments. Teacher Network. We use similar archit as our teacher ne ed LSTM layers with is used and followed by LeakyReLU . For the SEED dataset, we set the num to = 3 to be consistent with the n in order to use margin loss for clas regression task in the SEED-VIG datas = 10 as the number of higher le is then followed by a fully connecte 10 vation. The in Table  I .   [198] .\n\nwhile significantly reducing the number of parameters to transfer. Considering existing domain adaptation approaches may become sensitive where a low discriminative feature space among classes is given. Jiménez-Guarneros and Gómez-Gil  [202]  proposed a Standardization-Refinement Domain Adaptation (SRDA) method, which trains a target neural network model using Adaptive Batch Normalization (AdaBN) and introducing a novel loss based on the Variation of Information (VOI). Using AdaBN, SRDA makes the marginal distributions similar in source and target domains. (4) In addition to calibration, alignment, aspect-oriented modeling, and transfer learning, some researchers directly explore the subject-independent robustness features. For example, Soleymani et al.  [203]  studied the importance of different EEG features in emotion recognition. The findings showed that for Arousal, the PSD extracted from the low-frequency Alpha rhythm in the occipital EEGs is the most distinguishable feature. But for Valence, the key features are mainly from the Beta and Gamma rhythm in the temporal lobe EEGs. Zheng and Lu  [60]  investigated the stable EEG activity patterns to promote the effect of emotion recognition across participants and time periods. The results showed that the Beta and Gamma rhythms of both sides of temporal lobes under positive emotion generated stronger activation than negative ones. Moreover, the subject-independent EEG features were mainly from these scalp channels and frequency bands. Li et al.  [62]  extracted 18 widely used EEG features and studied the contribution of each feature in crosssubject emotion recognition according to the results derived from multiple feature selection methods. After analysis, the features of Hjorth parameters in Beta rhythm yield the best cross-subject recognition results. Yin et al.  [204]  proposed a locally-robust feature selection (LRFS) method for individualindependent emotion recognition. Kernel density estimation (KDE) first modeled the extracted EEG features. The inter-individual consistency of the EEG features is described by evaluating the similarity of all density functions between every two subjects, and the locally-robust EEG features could be further determined.\n\nIn addition, there has been one review paper proposed by Wan et al.  [205]  that focuses on the Transfer Learning techniques in solving the 'domain shift' problem in EEG analysis. As a complement to our review, we recommend the readers reference this review paper for detailed guidance of building EEG-based cross-subject emotion recognition models.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Route: 3→8→11 And 5→8→11",
      "text": "The Ensemble Learning-based recognition approaches are also an effective strategy for getting ideal performance in the EEG-based emotion recognition tasks. Ensemble Learning follows the idea of 'two heads are better than one' by taking advantage of multiple models' decision boundaries. For example, Mehmood et al.  [206]  used four Ensemble Learning strategies (Bagging, Boosting, Stacking, and Voting) to integrate the abilities of multiple machine learning models and then obtained the best recognition effect based on the Voting approach. Stacking is to use the training data to build several base learners and use the probability output of these learners as a new training set to learn a meta learner. The meta learner learns to organize the input and assign weights to the base learners. For example, Yin et al.  [207]  proposed one Deep Learning-based stacking algorithm. The constructed network combines multi-layer stacked AutoEncoders. Each corresponds to a feature subset of multiple time-frequency-domain features. Hence, multiple types of higher-level features can be extracted from those subnetworks that promote the generalization capability and the robustness against data imbalance. Chen et al.  [208]  proposed to apply the Adaboost algorithm to elevate the recognition performance adaptively. As shown in Figure  29 , it works based on the iteration mechanism. In each iteration, a weak classifier is included to be trained on the weighted samples, the importance (weight) of this weak classifier is determined in each iteration, and the sample weight is adjusted according to the classification results. Specifically, the misclassified samples will be assigned a higher weight in the next iteration to get more attention in model training, while the correctly classified samples' weights will decrease. ) 26\n\nto the sum and obtain the ):\n\n)\n\n)) 27 of we set the number of iterations of the weak as 100 and learning rate as 0.05, which can achieve Testing AdaBoost Classifier and Evaluation. After training the AdaBoost classifier, the data of the test set are sent to the trained classifier in Section 2.4.2, and based on 10-fold cross-validation  [42] , binary classification tests are performed on four emotional dimensions of valence, arousal, dominance, and liking, and comparison experiments are performed using random forest and XGBoost classifiers. For the main experiment and comparison experiment, the following five performance indicators are investigated: accuracy, precision, recall, F1-score, area under curve (AUC), and confusion matrix. Meanwhile, we draw the figures of results in order to evaluate the performance of the model from multiple angles. of predicted results and real of these combination is the confusion matrix. In the binary classification 4 different combinations of the above as TP, FP, FN, and TN. us, the as shown in Table  3 , can be obtained.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Route: 0→1→2→6→11",
      "text": "The acquired EEG signal and the corresponding labels might be noisy, imprecise, and uncertain, leading to precise modeling problems. Fuzzy Logic provides a foundation for approximate reasoning based on fuzzy set theory. Hence, in addition to those aforementioned technical routes, some studies adopted the Fuzzy Logic-based technical route, assigning the samples to multiple categories with a certain degree of membership. The Fuzzy C-Means and Fuzzy k-Means clustering methods are two representative Fuzzy Logic methods that have been implemented very early in EEG-based emotion recognition  [209] . Besides, Matiko et al.  [210]  designed fuzzy classification rules based on the asymmetry theory of emotional activities in the left and right brain hemispheres. Then, the classifier outputs the type of emotion and the confidence levels according to various rules. Based on Dempster-Shafer's theory, Soroush et al.  [211]  improved the accuracy of recognition by fusing the feature subsets and multiple MLP classifiers. Additionally, fuzzy cognitive maps (FCMs), which combine aspects of Fuzzy Logic, neural networks, and nonlinear dynamical systems, also has been verified its effectiveness in EEG-based emotion recognition  [212] . As a whole, Fuzzy Logic is rarely studied in this research field.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Route: 3→9→11",
      "text": "From the previous related works, we can see that adopting Deep Learning is the trend in EEG-based emotion recognition. Nevertheless, deep neural network models contain more parameters and rely on sufficient labeled training data to optimize the parameters compared with shallow models. Consequently, we must face one central challenge in EEG-based emotion recognition: acquiring adequate and high-quality training data. Hence, a promising research route is studying the 'data-constrained learning' to address the data limitation problems. Here we introduce two potential ways.\n\n4.8.1 Data Augmentation Techniques. Some researchers focus on studying data augmentation (DA) techniques. We can generate new samples from the existing dataset to increase the number of training samples. Exposing the model to more variable representations of training samples makes it robust to data transformations that are likely to encounter in real applications. Furthermore, increasing the size of the training set facilitates training more complex models with many parameters and reduces over-fitting. Data augmentation is typically conducted in computer vision by applying geometric transformations, e.g., rotation, cropping, etc. Nevertheless, the EEG is non-stationary time series. The geometric transformation methods are not suitable for EEG. One naive way is adding random noise (e.g., Gaussian, Poisson, Salt, Pepper noise, etc.) to the raw EEG signals  [154, 213] . Sliding window-based approaches are also adopted for data augmentation. However, these approaches may introduce modeling and performance evaluation risks that we discuss carefully in Section 5.3. Deep Generative Learning-based data augmentation methods are recently drawing widespread attention, including the generative adversarial network (GAN) based approaches and the variational autoencoder (VAE) based approaches  [214] . For example, during the training process of the adversarial network, the generator tries to generate data that are similar to the real data until the discriminator can not distinguish the fake data. More related works about data augmentation for EEG can be found in the review paper published by Lashgari et al.  [215] .\n\n4.8.2 Few-shot Learning Techniques. Few-shot learning also is potential for dealing with the data limitation problems and has been studied in recent related works  [216] . Few-shot learning is a class of machine learning techniques that build effective models that generalize well on classes unseen during the training process. It works well with limited samples and does not rely on re-training on the data belonging to the new classes. The few-shot learning can also be called a -shot--way learning problem. Most few-shot learning techniques rely on metric learning. As shown in Figure  30 , we need to construct the Support set and Query set, respectively. For the Support set, we sample samples for each of the classes. Further, we again sample the classes to construct the Query set. An embedding function is needed to project these samples to a latent space, in which the model is optimized to reduce the distance between the embeddings of query and support samples belonging to the same class while increasing the distance of the samples belonging to different categories. The Support and Query set construction process and optimization process iterate several times. After iteration, for testing, we only need a few samples belonging to the unseen class to form a support set, whereas the samples of the query set are the target to be inferred.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Route: 3→10→11 And 5→10→11",
      "text": "The processes involved in EEG-based emotion recognition studies are somewhat tedious. The domain knowledge hidden in this task is far beyond the machine learning specialists' knowledge range. Is there a way to automatically build robust recognition models on raw EEG data? In this regard, Automated Machine Learning (AutoML) is drawing attention in this domain. For AI-based approaches, an effective model is primarily decided by the model hyperparameters and the data representations. AutoML refers to end-to-end methodologies and tools for automatic optimization of data preprocessing, feature support set. A Support set S can be de!ned as\n\n, where S is the number of support samples. Next, we create a Query set , de!ned as , where Q is the number of queries, where we sample Q samples emotion recognition task) since the number of classes is !xed, either low or high valence (or arousal or dominance), the motive behind using a few-shot paradigm is mainly to handle the inter-subject variability by  engineering, model selection, model building, and hyperparameter optimization  [217] . It aims to generate the models that provide the best classification performance and minimize the generalization error for a specific problem. Currently, a few researchers have started to introduce the AutoML techniques into EEG-based emotion recognition. For example, He et al.  [218]  proposed one firefly integrated optimization algorithm (FIOA) to simultaneously realize the automatic parameter optimization, feature selection, and classifier selection. For Deep Learning-based technical routes, Aquino-Brítez et al.  [219]  proposed a fully-configurable optimization framework based on multi-objective optimization for Deep Learning architectures. It is not only capable of optimizing the model hyperparameters, but it can also adapt the model architecture, e.g., inserting or removing layers. At present, the main problem it encounters is that the computation burden is high. For example, the neural architecture search algorithm NASNet that was proposed by Google takes 28 days of training on 800 GPUs. Such high computational costs make search algorithms impractical for most researchers. It is encouraged that researchers are devoted to reducing the cost of AutoML training. We believe introducing AutoML techniques into various EEG modeling tasks will be very promising in the future.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Performance Evaluation 5.1 Benchmark Dataset",
      "text": "The proposed recognition algorithms and models should be verified on EEG data with emotional ratings or labels. However, it is impossible for some researchers, especially those in computer science, to build a professional experimental environment and design a scientific user experimental paradigm that needs specialized knowledge of psychology. Most researchers interested in studying recognition models choose to verify their ideas and compare with related works on the recognized benchmark dataset. Hence, developing open-source EEG dataset that can help evaluate recognition models' performance is something the field urgently needs and well worth studying. Among them, the Dataset for Emotion Analysis using EEG, Physiological and video signals (DEAP) is mostly used and cited, which was collected and opened by researchers from Queen Mary University of London, the University of Geneva in Switzerland, etc.  [54] . Thirty-two participants were recruited for the emotional EEG induction experiment. The EEG and several kinds of peripheral physiological signals were acquired while watching forty 60s long music movie clips. Then the subjective emotional experience in induction experiments was self-evaluated and rated on assessment scales that cover multiple emotional dimensions, including Arousal, Valence, Like, Dominance and Familiarity. The ratings are taken as the emotional ratings and labels of the EEG samples for model optimization. Another well-recognized benchmark dataset is the MAHNOB-HCI multi-modal dataset. It not only records the physiological and eye-tracking activities of participants during the emotion induction experiments, but also the videos (face and body) and the audios are also synchronously recorded. This dataset is developed for emotion detection and implicit tagging studies  [14] . SJTU Emotion EEG Dataset (SEED)  [220]  also has a great community influence that was released by the BCMI Research Center in Shanghai Jiaotong University (SJTU). In",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "We choose to outline the review from the perspective of researchers who try to take the first step on this topic. Hence, we review not only the overall current situation in the EEG based emotion recognition research but also provide a tutorial to guide the researchers to start from a very beginning, as well as illustrate the theoretical basis and the research motivation, which will help the readers to understand why those techniques are employed.\n\nFor this prospect, we introduce the preliminaries and basic knowledge of this field. Firstly, we present the definition and quantification methods of emotion. It is the prerequisite for affective computing, and it determines the objective of the modeling tasks (regression, clustering, classification). Then we illustrate the specificities and neural correlates of EEG in the emotional process, and we demonstrate the feasibility of EEG in studying the emotion recognition technologies. Before reviewing the technical routes, we also exhibit the classical research methodologies for EEG-based emotion detection studies, which helps the readers understand the goal of this field and the mainstream methodologies in the past quickly. The section of 'Preliminaries and Basic Knowledge' guides the readers to understand the following sections' contents better. Then, we devote much effort to guiding the newcomers of the EEG preprocessing and the feature engineering methods, which is the basis for most classical methodologies. The remaining parts of this paper mainly focus on the pattern recognition technical routes applied in the field, in which we summarize the mainstream and latest technical routes involved in this field and review plenty of representative works under each route. Finally, we discuss the evaluation methods adopted in this field. In addition to the benchmark datasets, we discuss the potential influence of different data split strategies in modeling and validation, which is a topic that many researchers care about. Considering the rapid development of Deep Learning and its successful application in this field, we select to review as many Deep Learning-based approaches as possible, and the selected works are within the scope of the recent three years. We tend to summarize representative works in this field and conduct empirical comparisons for closely related approaches from a descriptive perspective. We try to list these works in a structured table  (Table 4 ), which presents the methodologies, the validation strategies, and the achieved performance in a direct way.\n\nThough there have been many achievements in this field, there still exists several problems and challenges need to be further studied and resolved, as follows:\n\n• There is still room for research to explore effective EEG representation (transformation) approaches, which rely on EEG preprocessing and feature engineering. EEG preprocessing makes the emotion-related information (components) effectively filtered out from the multi-channel EEGs that contain redundant noisy components. Feature engineering helps to determine the critical variables related to emotions. Current widely utilized features cover various aspects, such as time-frequency characteristics, nonlinear dynamical system characteristics, etc. The feature extraction process may incur a high overhead and depends on subtle parameter settings, especially in nonlinear dynamical system feature computation. Nevertheless, the extracted features at a high cost contain many redundant and irrelevant variables that contribute slightly to the performance improvement. For example, Li et al.  [62]  explored a variety of EEG features in cross-subject emotion recognition, and experimental results indicate that only one or two key features lead to comparative performance to that obtained on the whole feature set. Hence, research on the critical EEG features and variables is still worth conducting. The decided scope of EEG features and variables helps reduce the computation cost in EEG representation, meanwhile, improve the recognition effect. Besides, the decided critical EEG channels help mitigate the difficulties in user experiment, e.g., only selecting to attach fewer electrodes on the cortex help to improve the user engagement and reduce the pre-experiment preparation. The critical EEG features and variables also could provide a new perspective to analyze the mechanism of the emotion cognition process. Though several studies mentioned above have taken prior knowledge into the model design. Take the BiDHM, BiDANN, and BiDCNN shown in Figure  13  and Figure  14  for example, the asymmetry correlation information between the bi-hemispheres is utilized, there still exists gaps between the true emotion process manifested by the EEG and the information processing modeled by the existing models. It will be better to integrate prior knowledge about the users' gender, age, physical condition, mental condition, and prior knowledge about the emotional stimuli, the semantic context, the environment, and the knowledge graph about emotion into the model design. Prior knowledge-guided recognition models must obtain an enhanced and robust performance on cross-domain learning and few-shot learning settings. Meanwhile, prior knowledge introduces interpretability to the constructed models and the obtained results. • Although abundant cutting-edge artificial intelligence models have been studied, developing computational methods for emotion recognition needs a deeper understanding of emotion processes and their neural basis. Psychophysiology-inspired, biology-inspired, and brain-inspired cognitive models based on the principle of how the human brain works in the emotion cognition process should also be taken into consideration for us. The popular Deep Learning models only are a less precise mathematical abstraction of the brain functions. They have limitations in online learning, small-sample learning, modeling the information interaction between different brain regions. The biologically inspired methods are built on the architecture of the neocortex and try to model the process of how the human brain handles complex information about vision, audio, behavior, and emotion. These biologically inspired methods (e.g., the hierarchical temporal memory model based on neocortex theory  [227]  and the spiking neural network) are intuitively suitable for modeling brain imaging data and other behavioral data controlled by the neocortex. For example, Luo et al.  [226]  proposed one spiking neural network (SNN) based model, which makes full use of the spatiotemporal features of the EEG signal. As one kind of brain-inspired computing model, the SNN is able to encode the neural data through the synapses, neurons, and spiking activity. In addition, the Deep Learning model is perceived as one black-box that is hard to understand why they get specific decisions  [228] . How to resolve the problem of the weak statistical interpretability should be taken into consideration for future works, e.g., through the 'inceptions' techniques adopted in Google Brain  [229] , and the model-agnostic explanation approach (LIME)  [230] . • Several works mentioned above face the problem of lengthy signal modeling. The EEG signal acquired in one trial with a high sampling rate will be extremely long. As a result, the model computation burden will be largely increased, especially for RNN based models. Hence, researchers should not only focus on the metric of recognition accuracy but also the computational efficiency should also be reported. At present, the researches mainly focus on the offline data processing scene. Therefore, the algorithm suitable for real-time emotion recognition and monitoring should be extensively explored. • The 'domain shift' problem and 'transfer learning' will still be the hot research topics in the next few years. Here we summarize several potential research approaches in these topics. Currently, researchers mainly focus on studying these topics within one single dataset setting, which we can call the intra-dataset-inter-subject modeling problem, in which the user difference in cultures, ages, gender, and physiology will degrade the performance. Nevertheless, developing domain adaptation techniques in inter-dataset-inter-subject settings is a more challenging task that deserves careful exploration. As we know, only a few open-source datasets about emotion recognition are available nowadays. If more EEG datasets uncorrelated with emotion recognition can be simultaneously employed, a robust model with more complex structure and more parameters could be trained. Considering the recent advances in the large-scale pre-trained model in natural language processing (e.g., GPT, BERT)  [231] , we believe it is possible to develop the EEG-oriented pre-trained model on large-scale open-source EEG datasets, which are not limited to emotion recognition. Nevertheless, the EEG data of multi-sources is acquired with different devices, different experiment designs, different types of stimuli, etc. These factors could further increase the discrepancies caused by inter-subject/session variability. Hence, research on 'modeling on multiple source domains' is another direction worthy of further exploration. In addition, current research on the domain shift problem mainly focuses on domain adaptation of the extracted EEG features. There are lots of room for performance enhancement on the cross-subject, cross-dataset, or cross-session emotion recognition tasks if the raw EEG signals could be calibrated and aligned to common data space in advance before adopting the traditional transfer learning approaches. Researchers who are interested in this research topic can refer to the raw EEG alignment methods based on Riemannian geometry that have been adopted in the motor imagery BCI  [182] . • How effective those models perform in an open environment is still unknown. As in real-world scenarios, the people are continuously in a dynamic condition that they may seldomly be calm, which is quite different from the controlled experimental environment. Realtime EEG signals are inevitably influenced by continually evolving activities, including physical factors such as body movements and environmental noise and psychological factors such as mental workloads and attention. The robustness of the emotion recognition system will be affected when people are executing psychological or physical activities. It raises a great challenge to develop recognition models that can capture robust and distinctive emotion-related features from real-time and dynamic EEGs that generalize well under various people states. Researchers should devote themselves to developing open-environment EEG datasets, the corresponding algorithms, and the evaluation criteria. • The reviewed works in this paper were designed on a single EEG modality. In recent years, increasing published articles manifest a shift of research interest from unimodal to multi-modal information-based emotion recognition tasks, in which the multi-modal approaches fuse two or more modalities for emotion recognition. This shift is based on some problems that unimodal systems mainly faced. Firstly, the unimodal data may be missing or inconsecutive for some reason, e.g., the monitored signal may be blocked or affected by external obstacles, noise, or device instability. In such a circumstance, the data from other modalities complement the single modality properly. Secondly, the exterior behavioral information sometimes may not be consistent with the actual affective state. An individual may conceal their real feelings under social masks. For example, the same facial expressions may represent different psychological activities, so that the single data modality may be insufficient for an accurate recognition task. Last but not least, the recognition performance may be promoted when multi-modal information is fused and utilized, and this is also the ultimate goal of multi modalities-based approaches. For example, Ma et al.  [225]  developed one LSTM based multi-modal recognition framework that successfully learned the joint information from the original EEG and physiological data, and thus significantly improving the recognition effect of the DEAP dataset.\n\nResearchers should take as many data modalities as possible into emotion recognition studies, including the EEG, the facial expression, the gesture, the gait, the peripheral physiological signal, the eye movement, etc., to build a comprehensive recognition model. We will devote ourselves to reviewing relevant multi-modal fusion studies in future work.",
      "page_start": 28,
      "page_end": 29
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Valence-Arousal Bipolar Coordinate System Porposed by Russell (A) and the corresponding Self-Assessment Manikins scale (B) [36].",
      "page": 3
    },
    {
      "caption": "Figure 2: ACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2022.",
      "page": 3
    },
    {
      "caption": "Figure 2: Diﬀerences between high and low Valence on lef and right frontal EEG alpha power in music listening (Note that EEG power is inversely related",
      "page": 4
    },
    {
      "caption": "Figure 2: , the valence and arousal of musical stimuli have been reported to correlate with frontal",
      "page": 4
    },
    {
      "caption": "Figure 3: The classical methods in the EEG based Emotion recognition study1",
      "page": 5
    },
    {
      "caption": "Figure 3: , namely applying pattern recognition models to the handcrafted",
      "page": 5
    },
    {
      "caption": "Figure 4: Examples of 7-level decomposition and corresponding frequency bands of EEG with Sample Frequency of 128Hz.",
      "page": 7
    },
    {
      "caption": "Figure 5: Phase diagram example for diﬀerent types of signals: A. Periodic Signal, B. White Noise Signal, C. EEG Signal",
      "page": 7
    },
    {
      "caption": "Figure 5: , the neurophysiological signals present the",
      "page": 7
    },
    {
      "caption": "Figure 6: Illustration of the brain network construction and the derived network features for classification [104].",
      "page": 8
    },
    {
      "caption": "Figure 6: ). The constructed brain network reﬂects the coupling correlation between two EEG channels. Hence it is not sensitive",
      "page": 8
    },
    {
      "caption": "Figure 3: Here are some representative approaches. Wang et al. [74] extracted two kinds of power spectrum features, two types of wavelet characteristics, and",
      "page": 9
    },
    {
      "caption": "Figure 7: Flow Chart of the Patern Recognition Technical Routes Applied in The Field.",
      "page": 10
    },
    {
      "caption": "Figure 8: Illustration of a deep belief network (DBN) consists of restricted Boltzmann machines (RBMs) and the corresponding training process.",
      "page": 10
    },
    {
      "caption": "Figure 8: The implicit correlations over diﬀerent channels are a signiﬁcant indicator to recognize emotions. Convolutional neural networks (CNN) are ideally",
      "page": 10
    },
    {
      "caption": "Figure 9: , we illustrate",
      "page": 10
    },
    {
      "caption": "Figure 9: Two representative 2D EEG feature representation methods in emotion recognition when adopting the 2D-CNN-based approaches.",
      "page": 11
    },
    {
      "caption": "Figure 10: The architecture of the CapsNet based emotion recognition model proposed by Chao et al. [133], in which the channel signal’s PSD features",
      "page": 11
    },
    {
      "caption": "Figure 10: , where the length and direction of each primary capsule indicate the existence and characteristic of the low-level",
      "page": 11
    },
    {
      "caption": "Figure 11: (A), Wang et al. [100] built one typical GCNN model on the EEG derived graph. The graph is a fusion of the",
      "page": 11
    },
    {
      "caption": "Figure 11: (B), Song et al. [137] proposed a dynamical graph convolutional neural networks (DGCNN)",
      "page": 11
    },
    {
      "caption": "Figure 11: Two representative approaches of applying graph convolutional neural network (GCNN). A: The traditional GCNN model, the input functional",
      "page": 12
    },
    {
      "caption": "Figure 12: The framework of the heuristic variational pathway reasoning (VPR) method that can adaptively determine salient pathways to facilitate EEG",
      "page": 12
    },
    {
      "caption": "Figure 12: , Zhang et al. [138] proposed a heuristic variational pathway reasoning (VPR) method that introduced random",
      "page": 12
    },
    {
      "caption": "Figure 13: , considering the importance of the asymmetrical information between the hemispheres in emotion",
      "page": 12
    },
    {
      "caption": "Figure 12: , the electrode pathways here were predeﬁned. After the deep representations of each channel above have been obtained,",
      "page": 12
    },
    {
      "caption": "Figure 13: Handcrafed feature based bi-hemispheric discrepancy information integrated model (BiHDM)4 [141]. BiHDM utilizes four RNNs to capture",
      "page": 13
    },
    {
      "caption": "Figure 14: End-to-end bi-hemispheric discrepancy information integrated model. A: BiDCNN5 [143], B: RACNN6 [144].",
      "page": 13
    },
    {
      "caption": "Figure 14: (A), Huang et al. [143] proposed an end-to-end bi-hemisphere discrepancy convolutional neural network model (BiDCNN)",
      "page": 13
    },
    {
      "caption": "Figure 15: The end-to-end 2D convolutional neural networks based spatial-temporal Deep Learning model (EEGNet) [145], in which the input EEGs are",
      "page": 13
    },
    {
      "caption": "Figure 15: , Lawhern et al. [145] designed an EEG-speciﬁc ConvNet model (EEGNet) by integrating depthwise and separable",
      "page": 14
    },
    {
      "caption": "Figure 10: , Liu et al. [135] developed one end-to-end CapsNet based approach that builds directly on the raw EEG signals and judge the emotions. It",
      "page": 14
    },
    {
      "caption": "Figure 16: The fluctuation of the ground truth emotion during one stimulus trial that leads to the problem of unreliability of the ground truth emotional",
      "page": 14
    },
    {
      "caption": "Figure 16: , the subjects’ speciﬁc",
      "page": 14
    },
    {
      "caption": "Figure 17: Two representative 3D EEG representation methods in emotion recognition when adopting the 3D-CNN-based approaches. A: The raw multi-",
      "page": 14
    },
    {
      "caption": "Figure 17: (A), consecutive",
      "page": 14
    },
    {
      "caption": "Figure 18: End-to-end 1D-CNN model TSception8 [150], which consists of two types of 1D convolution based learners: temporal learner and spatial learner.",
      "page": 15
    },
    {
      "caption": "Figure 15: ) proposed by Lawhern et al. [145]. Jia et al. [153] proposed one spatial-",
      "page": 15
    },
    {
      "caption": "Figure 18: , it consists of two types of 1D convolution-based learners for end-to-end temporal-",
      "page": 15
    },
    {
      "caption": "Figure 19: Three representative CNN-RNN hybrid models. A: The CRNN hybrid DL model proposed by Li et al. [154] that the input data representation",
      "page": 15
    },
    {
      "caption": "Figure 19: (A), Li et al.",
      "page": 15
    },
    {
      "caption": "Figure 19: (B)) and parallel (see Figure 19(C)) hybrid DL models integrate",
      "page": 16
    },
    {
      "caption": "Figure 19: (A). It ﬁrst learns the spatial",
      "page": 16
    },
    {
      "caption": "Figure 20: The framework of LSTM based model R2G-STNN10. The region to global (R2G) process includes two streams. The spatial stream constructs the",
      "page": 16
    },
    {
      "caption": "Figure 21: Illustration of a hybrid model that fuses unsupervised decoding of latent source factors and the recurrent neural network (RNN).",
      "page": 17
    },
    {
      "caption": "Figure 22: Illustration of a hybrid model that fuses graph convolutional neural network (GCNN) and RNN.",
      "page": 17
    },
    {
      "caption": "Figure 19: For example, Yin et al. [163] proposed one hybrid DL model (named ECGGCNN) that integrates GCNN and LSTM. The GCNN module",
      "page": 17
    },
    {
      "caption": "Figure 23: A temporal-spatial EEG information Riemannian fusion network (RFNet) for aﬀective BCI [164].",
      "page": 17
    },
    {
      "caption": "Figure 23: , Zhang and Etemad [164] proposed one end-to-end Riemannian",
      "page": 17
    },
    {
      "caption": "Figure 24: (A), the instantaneous distribution of EEG continuously evolves that causing the data non-stationarity issues.",
      "page": 18
    },
    {
      "caption": "Figure 24: (B), inter-subject variability also is a representative domain shift problem, which indicates there also",
      "page": 18
    },
    {
      "caption": "Figure 24: Domain shif problems in DEAP data set11. A: non-stationary EEG distribution between two epochs. B: inter-subject EEG variability under the",
      "page": 19
    },
    {
      "caption": "Figure 25: Based on the clustering results, cluster selection was conducted to match the target subject with one optimal source cluster, whose",
      "page": 19
    },
    {
      "caption": "Figure 25: The framework of the subject clustering based cross-subject recognition method12. Cluster selection is required for selecting the optimal source",
      "page": 19
    },
    {
      "caption": "Figure 26: Two representative domain adaptation neural networks (DANN). A: The Wasserstein generative adversarial network based domain adaptation",
      "page": 20
    },
    {
      "caption": "Figure 26: (B). The mechanism of this model is exactly similar to the model proposed by",
      "page": 20
    },
    {
      "caption": "Figure 27: , the pipeline contains a teacher network and a student network. They ﬁrst pre-train a large model (teacher",
      "page": 20
    },
    {
      "caption": "Figure 28: Experiments show it keeps enough ﬂexibility to adapt to the new subject",
      "page": 20
    },
    {
      "caption": "Figure 27: One novel knowledge distillation DL framework for Aﬀective BCI based on LSTM-Capsule structure that compresses the large model without",
      "page": 21
    },
    {
      "caption": "Figure 28: The Workflow of the Meta Learning on constrained Transfer Learning (MLCL) [201].",
      "page": 21
    },
    {
      "caption": "Figure 29: , it works based",
      "page": 21
    },
    {
      "caption": "Figure 29: The working principle diagram of AdaBoost [208].",
      "page": 22
    },
    {
      "caption": "Figure 30: , we need to",
      "page": 22
    },
    {
      "caption": "Figure 30: General few-shot learning paradigm: (a) generating embeddings for the support set S, (b) generating embeddings for the query set Q, (c) Mapping",
      "page": 23
    },
    {
      "caption": "Figure 31: Binary classification confusion matrix.",
      "page": 24
    },
    {
      "caption": "Figure 32: The possible k-fold data split strategy adopted in related works",
      "page": 25
    },
    {
      "caption": "Figure 32: Seven possible K-fold splitting strategies may be",
      "page": 25
    },
    {
      "caption": "Figure 16: , where the subjects’ emotional state may",
      "page": 25
    },
    {
      "caption": "Figure 13: and Figure 14 for example, the asymmetry correlation information between the bi-hemispheres is utilized, there still exists gaps between the true emo-",
      "page": 28
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Confusion \nMatrix": "Actual Class\nActual Value:\nPositive (+)\nActual Value:\nNegative (-)",
          "Predicted Value:\nPositive (+)": "True Positive\nTP\nFalse Positive\nFP",
          "Predicted Value:\nNegative (-)": "False Negative\nFN\nTrue Negative\nTN"
        }
      ],
      "page": 24
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Building hal: Computers that sense, recognize, and respond to human emotion",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2001",
      "venue": "Human Vision and Electronic Imaging VI"
    },
    {
      "citation_id": "3",
      "title": "A survey of ambient intelligence",
      "authors": [
        "Rob Dunne",
        "Tim Morris",
        "Simon Harper"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "4",
      "title": "The neuroscience of emotion: A new synthesis",
      "authors": [
        "Ralph Adolphs",
        "David Anderson"
      ],
      "year": "2018",
      "venue": "The neuroscience of emotion: A new synthesis"
    },
    {
      "citation_id": "5",
      "title": "The james-lange theory of emotions: A critical examination and an alternative theory",
      "authors": [
        "Walter B Cannon"
      ],
      "year": "1927",
      "venue": "The American journal of psychology"
    },
    {
      "citation_id": "6",
      "title": "On identification of driving-induced stress using electroencephalogram signals: A framework based on wearable safety-critical scheme and machine learning",
      "authors": [
        "Zahid Halim",
        "Mahma Rehan"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "7",
      "title": "Multi-modal prediction of ptsd and stress indicators",
      "authors": [
        "Amelio Viktor Rozgic",
        "Michael Vazquez-Reina",
        "Amit Crystal",
        "Veasna Srivastava",
        "Chris Tan",
        "Berka"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Avec 2013: the continuous audio/visual emotion and depression recognition challenge",
      "authors": [
        "Michel Valstar",
        "Björn Schuller",
        "Kirsty Smith",
        "Florian Eyben",
        "Bihan Jiang",
        "Sanjay Bilakhia",
        "Sebastian Schnieder",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM international workshop on Audio/visual emotion challenge"
    },
    {
      "citation_id": "9",
      "title": "Feature-level fusion approaches based on multimodal eeg data for depression recognition",
      "authors": [
        "Hanshu Cai",
        "Zhidiao Qu",
        "Zhe Li",
        "Yi Zhang",
        "Xiping Hu",
        "Bin Hu"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "10",
      "title": "Cognitive therapy and the emotional disorders",
      "authors": [
        "T Aaron",
        "Beck"
      ],
      "year": "1979",
      "venue": "Cognitive therapy and the emotional disorders"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition using eeg and physiological data for robot-assisted rehabilitation systems",
      "authors": [
        "Elif Gümüslü",
        "Duygun Barkana",
        "Hatice Köse"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "12",
      "title": "A personalized music recommendation system based on electroencephalography feedback. Multimedia Tools and Applications",
      "authors": [
        "Hong-Yi Chang",
        "Shih-Chang Huang",
        "Jia-Hao Wu"
      ],
      "year": "2017",
      "venue": "A personalized music recommendation system based on electroencephalography feedback. Multimedia Tools and Applications"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based analysis of the emotional effect of music therapy on palliative care cancer patients",
      "authors": [
        "Rafael Ramirez",
        "Josep Planas",
        "Nuria Escude",
        "Jordi Mercade",
        "Cristina Farriols"
      ],
      "year": "2018",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "14",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Maja Thierry Pun",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "15",
      "title": "An effective implicit relevance feedback technique using affective, physiological and behavioural features",
      "authors": [
        "Yashar Moshfeghi",
        "Joemon M Jose"
      ],
      "year": "2013",
      "venue": "Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval"
    },
    {
      "citation_id": "16",
      "title": "Fusion of facial expressions and eeg for implicit affective tagging",
      "authors": [
        "Sander Koelstra",
        "Ioannis Patras"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "17",
      "title": "Enriching user profiling with affective features for the improvement of a multimodal recommender system",
      "authors": [
        "Ioannis Arapakis",
        "Yashar Moshfeghi",
        "Hideo Joho",
        "Reede Ren",
        "David Hannah",
        "Joemon M Jose"
      ],
      "year": "2009",
      "venue": "Proceedings of the ACM international conference on image and video retrieval"
    },
    {
      "citation_id": "18",
      "title": "Role of emotion in information retrieval",
      "authors": [
        "Yashar Moshfeghi"
      ],
      "year": "2012",
      "venue": "Role of emotion in information retrieval"
    },
    {
      "citation_id": "19",
      "title": "Current state of text sentiment analysis from opinion to emotion mining",
      "authors": [
        "Ali Yadollahi",
        "Ameneh Gholipour Shahraki",
        "Osmar R Zaiane"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "20",
      "title": "Publication date: January 2022. 1:30",
      "authors": [
        "• Li",
        "Zhang"
      ],
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "21",
      "title": "Database for an emotion recognition system based on eeg signals and various computer games-gameemo",
      "authors": [
        "Talha Burak",
        "Murat Gonen",
        "Ibrahim Turkoglu"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "22",
      "title": "Assessing the emotional impact of virtual reality-based teacher training",
      "authors": [
        "Evangelia Kalliopi",
        "Maria Stavroulia",
        "Evangelia Christofi",
        "Despina Baka",
        "Nadia Michael-Grigoriou",
        "Andreas Magnenat-Thalmann",
        "Lanitis"
      ],
      "year": "2019",
      "venue": "The International Journal of Information and Learning Technology"
    },
    {
      "citation_id": "23",
      "title": "Virtual reality in education: Focus on the role of emotions and physiological reactivity",
      "authors": [
        "Mikko Vesisenaho",
        "Merja Juntunen",
        "P Johanna",
        "Janne Fagerlund",
        "Iryna Miakush",
        "Tiina Parviainen"
      ],
      "year": "2019",
      "venue": "Journal For Virtual Worlds Research"
    },
    {
      "citation_id": "24",
      "title": "A review of classification algorithms for eeg-based brain-computer interfaces: a 10 year update",
      "authors": [
        "Fabien Lotte",
        "Laurent Bougrain",
        "Andrzej Cichocki",
        "Maureen Clerc",
        "Marco Congedo",
        "Alain Rakotomamonjy",
        "Florian Yger"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "25",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "M Soraia",
        "Manuel Alarcao",
        "Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Eeg-based emotion recognition: Review of commercial eeg devices and machine learning techniques",
      "authors": [
        "Didar Dadebayev",
        "Wei Goh",
        "Ee Xion"
      ],
      "year": "2021",
      "venue": "Eeg-based emotion recognition: Review of commercial eeg devices and machine learning techniques"
    },
    {
      "citation_id": "27",
      "title": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities. Computational intelligence and neuroscience",
      "authors": [
        "James Nazmi Sofian Suhaimi",
        "Jason Mountstephens",
        "Teo"
      ],
      "year": "2020",
      "venue": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities. Computational intelligence and neuroscience"
    },
    {
      "citation_id": "28",
      "title": "Affect recognition using brain signals: A survey",
      "authors": [
        "Resham Arya",
        "Ashok Kumar",
        "Megha Bhushan"
      ],
      "venue": "Computational Methods and Data Engineering"
    },
    {
      "citation_id": "29",
      "title": "Recognition of human emotions using eeg signals: A review",
      "authors": [
        "Ajay Md Mustafizur Rahman",
        "Md Sarkar",
        "Md Amzad Hossain",
        "Md Rabiul Selim Hossain",
        "Md Islam",
        "Julian Mw Biplob Hossain",
        "Mohammad Quinn",
        "Moni Ali"
      ],
      "year": "2021",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "30",
      "title": "Deep learning for electroencephalogram (eeg) classification tasks: a review",
      "authors": [
        "Alexander Craik",
        "Yongtian He",
        "Jose L Contreras- Vidal"
      ],
      "year": "2019",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "31",
      "title": "Darwin's contributions to our understanding of emotional expressions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1535",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "32",
      "title": "Basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of Cognition and Emotion"
    },
    {
      "citation_id": "33",
      "title": "Emotions and Life: Perspectives from Psychology",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2003",
      "venue": "Emotions and Life: Perspectives from Psychology"
    },
    {
      "citation_id": "34",
      "title": "Theories of emotion",
      "authors": [
        "Robert Plutchik",
        "Henry Kellerman"
      ],
      "year": "2013",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "35",
      "title": "Emotions in Social Psychology: Essential Readings",
      "authors": [
        "W Parrott",
        "Gerrod"
      ],
      "year": "2001",
      "venue": "Emotions in Social Psychology: Essential Readings"
    },
    {
      "citation_id": "36",
      "title": "Affective space is bipolar",
      "authors": [
        "Russell James"
      ],
      "year": "1979",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "37",
      "title": "Effects of organization and disorganization on pleasantness, calmness, and the frontal negativity in the event-related potential",
      "authors": [
        "J Sandra",
        "Langeslag"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "38",
      "title": "Frontal brain electrical activity (eeg) distinguishes valence and intensity of musical emotions",
      "authors": [
        "A Louis",
        "Laurel Schmidt",
        "Trainor"
      ],
      "year": "2001",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "39",
      "title": "Impaired recognition of emotion in facial expressions following bilateral damage to the human amygdala",
      "authors": [
        "Ralph Adolphs",
        "Daniel Tranel",
        "Hanna Damasio",
        "Antonio Damasio"
      ],
      "year": "1994",
      "venue": "Nature"
    },
    {
      "citation_id": "40",
      "title": "Discrete emotions predict changes in cognition, judgment, experience, behavior, and physiology: a meta-analysis of experimental emotion elicitations",
      "authors": [
        "Sarah Heather C Lench",
        "Shane Flores",
        "Bench"
      ],
      "year": "2011",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "41",
      "title": "The brain basis of emotion: a meta-analytic review",
      "authors": [
        "Kristen Lindquist",
        "Tor Wager",
        "Hedy Kober",
        "Eliza Bliss-Moreau",
        "Lisa Barrett"
      ],
      "year": "2012",
      "venue": "The Behavioral and brain sciences"
    },
    {
      "citation_id": "42",
      "title": "Rethinking the emotional brain",
      "authors": [
        "Joseph Ledoux"
      ],
      "year": "2012",
      "venue": "Neuron"
    },
    {
      "citation_id": "43",
      "title": "Neural correlates of social and nonsocial emotions: An fmri study",
      "authors": [
        "K Jennifer C Britton",
        "Stephan Luan Phan",
        "Robert Taylor",
        "Kent Welsh",
        "Israel Berridge",
        "Liberzon"
      ],
      "year": "2006",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "44",
      "title": "Mapping cerebral sites for emotion and emotional expression with direct cortical electrical stimulation and seizure discharges",
      "authors": [
        "Barry Gordon",
        "John Hart",
        "Ronald Lesser",
        "Santiago Arroyo"
      ],
      "year": "1996",
      "venue": "Progress in brain research"
    },
    {
      "citation_id": "45",
      "title": "Subthalamic nucleus stimulation affects fear and sadness recognition in parkinson's disease",
      "authors": [
        "Péron",
        "E Biseul",
        "S Leray",
        "Le Vicente",
        "S Jeune",
        "D Drapier",
        "P Drapier",
        "C Sauleau",
        "M Haegelen",
        "Vérin"
      ],
      "year": "2010",
      "venue": "Neuropsychology"
    },
    {
      "citation_id": "46",
      "title": "Mirth and laughter arising from human temporal cortex",
      "authors": [
        "Satow",
        "M Usui",
        "J Matsuhashi",
        "T Yamamoto",
        "Hiroshi Begum",
        "Shibasaki",
        "N Ikeda",
        "S Mikuni",
        "Miyamoto",
        "Hashimoto"
      ],
      "year": "2003",
      "venue": "Neurosurgery & Psychiatry"
    },
    {
      "citation_id": "47",
      "title": "Mirth and laughter elicited during brain stimulation",
      "authors": [
        "Guadalupe Fernández-Baca Vaca",
        "Maysaa Hans O Lüders",
        "Jonathan Merhi Basha",
        "Miller"
      ],
      "year": "2011",
      "venue": "Epileptic Disorders"
    },
    {
      "citation_id": "48",
      "title": "Anterior cerebral asymmetry and the nature of emotion",
      "authors": [
        "R Davidson"
      ],
      "year": "1992",
      "venue": "Brain and Cognition"
    },
    {
      "citation_id": "49",
      "title": "What does the prefrontal cortex \"do\" in affect: perspectives on frontal eeg asymmetry research",
      "authors": [
        "R Davidson"
      ],
      "year": "2004",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "50",
      "title": "Brain correlates of music-evoked emotions",
      "authors": [
        "Stefan Koelsch"
      ],
      "year": "2014",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "51",
      "title": "Frontal eeg asymmetry of mood: A mini-review",
      "authors": [
        "Massimiliano Palmiero",
        "Laura Piccardi"
      ],
      "year": "2017",
      "venue": "Frontiers in Behavioral Neuroscience"
    },
    {
      "citation_id": "52",
      "title": "Music and emotion: electrophysiological correlates of the processing of pleasant and unpleasant music",
      "authors": [
        "Daniela Sammler",
        "Maren Grigutsch",
        "Thomas Fritz",
        "Stefan Koelsch"
      ],
      "year": "2007",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "53",
      "title": "Neural correlates of emotional responses to music: an eeg study",
      "authors": [
        "Ian Daly",
        "Asad Malik",
        "Faustina Hwang",
        "Etienne Roesch",
        "James Weaver",
        "Alexis Kirke",
        "Duncan Williams",
        "Eduardo Miranda",
        "Slawomir Nasuto"
      ],
      "year": "2014",
      "venue": "Neuroscience letters"
    },
    {
      "citation_id": "54",
      "title": "Neurophysiological correlates of induced discrete emotions in humans: an individually oriented analysis",
      "authors": [
        "Li Aftanas",
        "Reva",
        "Savotina",
        "Makhnev"
      ],
      "year": "2006",
      "venue": "Neuroscience and Behavioral Physiology"
    },
    {
      "citation_id": "55",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "Consciousness and arousal effects on emotional face processing as revealed by brain oscillations. a gamma band analysis",
      "authors": [
        "Michela Balconi",
        "Claudio Lucchiari"
      ],
      "year": "2008",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "57",
      "title": "High gamma band eeg closely related to emotion: evidence from functional network",
      "authors": [
        "Kai Yang",
        "Li Tong",
        "Jun Shu",
        "Ning Zhuang",
        "Bin Yan",
        "Ying Zeng"
      ],
      "year": "2020",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "58",
      "title": "Eeg asymmetry, dispositional mood and personality",
      "authors": [
        "Dirk Hagemann",
        "Ewald Naumann",
        "Alexander Lürken",
        "Gabriele Becker",
        "Stefanie Maier",
        "Dieter Bartussek"
      ],
      "year": "1999",
      "venue": "Personality and Individual Differences"
    },
    {
      "citation_id": "59",
      "title": "Frontal eeg asymmetry as a moderator and mediator of emotion",
      "authors": [
        "A James",
        "John Jb Coan",
        "Allen"
      ],
      "year": "2004",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "60",
      "title": "Emotion classification based on gamma-band eeg",
      "authors": [
        "Mu Li",
        "Bao-Liang Lu"
      ],
      "year": "2009",
      "venue": "2009 Annual International Conference of the IEEE Engineering in medicine and biology society"
    },
    {
      "citation_id": "61",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Weilong Zheng",
        "Baoliang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "62",
      "title": "Hierarchical convolutional neural networks for eeg-based emotion recognition",
      "authors": [
        "Jinpeng Li",
        "Zhaoxiang Zhang",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "63",
      "title": "Exploring eeg features in cross-subject emotion recognition",
      "authors": [
        "Xiang",
        "Dawei Li",
        "Song",
        "Peng",
        "Yazhou Zhang",
        "Yuexian Zhang",
        "Bin Hou",
        "Hu"
      ],
      "year": "2018",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "64",
      "title": "Influence of music liking on eeg based emotion recognition",
      "authors": [
        "Daimi Syed",
        "Goutam Saha"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "65",
      "title": "Investigating patterns for self-induced emotion recognition from eeg signals",
      "authors": [
        "Ning Zhuang",
        "Ying Zeng",
        "Kai Yang",
        "Chi Zhang",
        "Li Tong",
        "Bin Yan"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "66",
      "title": "A cluster-based approach to selecting representative stimuli from the international affective picture system (iaps) database",
      "authors": [
        "Alexandra Constantinescu",
        "Maria Wolters",
        "Adam Moore",
        "Sarah Macpherson"
      ],
      "year": "2017",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "67",
      "title": "Arousal and valence recognition of affective sounds based on electrodermal activity",
      "authors": [
        "Alberto Greco",
        "Gaetano Valenza",
        "Luca Citi",
        "Enzo Scilingo"
      ],
      "year": "2016",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "68",
      "title": "Affective virtual reality system (avrs): design and ratings of affective vr scenes",
      "authors": [
        "Wenzhuo Zhang",
        "Lin Shu",
        "Xiangmin Xu",
        "Dan Liao"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Virtual Reality and Visualization (ICVRV)"
    },
    {
      "citation_id": "69",
      "title": "EEG based Emotion Recognition: A Tutorial and Review",
      "year": "2022",
      "venue": "EEG based Emotion Recognition: A Tutorial and Review"
    },
    {
      "citation_id": "70",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "71",
      "title": "Emotion recognition from brain signals using hybrid adaptive filtering and higher order crossings analysis",
      "authors": [
        "C Panagiotis",
        "Leontios Petrantonakis",
        "Hadjileontiadis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "72",
      "title": "Real-time eeg-based emotion monitoring using stable features",
      "authors": [
        "Zirui Lan",
        "Olga Sourina",
        "Lipo Wang",
        "Yisi Liu"
      ],
      "year": "2016",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "73",
      "title": "Affective picture processing: an integrative review of erp findings",
      "authors": [
        "Steven Jonas K Olofsson",
        "Henrique Nordin",
        "John Sequeira",
        "Polich"
      ],
      "year": "2008",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "74",
      "title": "Event-related brain potentials differentiate positive and negative mood adjectives during both supraliminal and subliminal visual processing",
      "authors": [
        "Edward Bernat",
        "Scott Bunce",
        "Howard Shevrin"
      ],
      "year": "2001",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "75",
      "title": "Toward emotion aware computing: an integrated approach using multichannel neurophysiological recordings and affective visual stimuli",
      "authors": [
        "Christos Frantzidis",
        "Charalampos Bratsas",
        "Christos Papadelis",
        "Evdokimos Konstantinidis",
        "Costas Pappas",
        "Panagiotis Bamidis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "76",
      "title": "Emotional state classification from eeg data using machine learning approach",
      "authors": [
        "Xiaowei Wang",
        "Dan Nie",
        "Baoliang Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "77",
      "title": "Eeg-based emotion recognition in music listening: A comparison of schemes for multiclass support vector machine",
      "authors": [
        "Yuan-Pin Lin",
        "Chi-Hong Wang",
        "Tien-Lin Wu",
        "Shyh-Kang Jeng",
        "Jyh-Horng Chen"
      ],
      "year": "2009",
      "venue": "Proceedings of the 2009 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "78",
      "title": "Real-time movie-induced discrete emotion recognition from eeg signals",
      "authors": [
        "Yong-Jin Liu",
        "Minjing Yu",
        "Guozhen Zhao",
        "Jinjing Song",
        "Yan Ge",
        "Yuanchun Shi"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "79",
      "title": "Emotion detection from eeg signals with continuous wavelet analyzing",
      "authors": [
        "Sorkhabi Majid Memarian"
      ],
      "year": "2014",
      "venue": "American Journal of Computing Research Repository"
    },
    {
      "citation_id": "80",
      "title": "Wavelet-based emotion recognition system using eeg signal",
      "authors": [
        "Zeynab Mohammadi",
        "Javad Frounchi",
        "Mahmood Amiri"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "81",
      "title": "Automatic recognition of alertness level from eeg by using neural network and wavelet coefficients",
      "authors": [
        "A Subasi"
      ],
      "year": "2005",
      "venue": "Automatic recognition of alertness level from eeg by using neural network and wavelet coefficients"
    },
    {
      "citation_id": "82",
      "title": "A comparative study of wavelet families for eeg signal classification",
      "authors": [
        "T Gandhi",
        "B Panigrahi",
        "S Anand"
      ],
      "year": "2011",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "83",
      "title": "Emotion recognition from eeg signals by using multivariate empirical mode decomposition",
      "authors": [
        "Ahmet Mert",
        "Aydin Akan"
      ],
      "year": "2018",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "84",
      "title": "Nonlinear dynamical analysis of eeg and meg",
      "authors": [
        "C J Stam"
      ],
      "year": "2005",
      "venue": "Clinical Neurophysiology"
    },
    {
      "citation_id": "85",
      "title": "Human electroencephalograms seen as fractal time series: Mathematical analysis and visualization",
      "authors": [
        "Vladimir Kulish",
        "Alexei Sourin",
        "Olga Sourina"
      ],
      "year": "2006",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "86",
      "title": "Fractal dimension based neurofeedback in serious games",
      "authors": [
        "Qiang Wang",
        "Olga Sourina",
        "Minh Nguyen"
      ],
      "year": "2011",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "87",
      "title": "Real-time eeg-based human emotion recognition and visualization",
      "authors": [
        "Yisi Liu",
        "Olga Sourina",
        "Minh Nguyen"
      ],
      "year": "2010",
      "venue": "Proceedings of the 2010 International Conference on Cyberworlds"
    },
    {
      "citation_id": "88",
      "title": "Classifying depression patients and normal subjects using machine learning techniques and nonlinear features from eeg signal",
      "authors": [
        "Behshad Hosseinifard",
        "Mohammad Hassan Moradi",
        "Reza Rostami"
      ],
      "year": "2013",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "89",
      "title": "Eeg-based subject-dependent emotion recognition algorithm using fractal dimension",
      "authors": [
        "Yisi Liu",
        "Olga Sourina"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "90",
      "title": "Recurrence plots of dynamical systems",
      "authors": [
        "Jean-Pierre Eckmann",
        "S Oliffson Kamphorst",
        "David Ruelle"
      ],
      "year": "1995",
      "venue": "World Scientific Series on Nonlinear Science Series A"
    },
    {
      "citation_id": "91",
      "title": "Encoding physiological signals as images for affective state recognition using convolutional neural networks",
      "authors": [
        "Guangliang Yu",
        "Xiang Li",
        "Dawei Song",
        "Xiaozhao Zhao",
        "Peng Zhang",
        "Yuexian Hou",
        "Bin Hu"
      ],
      "venue": "2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "92",
      "title": "A recurrence quantification analysis-based channelfrequency convolutional neural network for emotion recognition from eeg",
      "authors": [
        "Yu-Xuan Yang",
        "Zhong-Ke Gao",
        "Xin-Min Wang",
        "Yan-Li Li",
        "Jing-Wei Han",
        "Norbert Marwan",
        "Jürgen Kurths"
      ],
      "year": "2018",
      "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science"
    },
    {
      "citation_id": "93",
      "title": "Differential entropy feature for eeg-based vigilance estimation",
      "authors": [
        "Li-Chen Shi",
        "Ying-Ying Jiao",
        "Bao-Liang Lu"
      ],
      "year": "2013",
      "venue": "2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "94",
      "title": "A review on nonlinear methods using electroencephalographic recordings for emotion recognition",
      "authors": [
        "Beatriz García-Martínez",
        "Arturo Martinez-Rodrigo",
        "Raul Alcaraz",
        "Antonio Fernández-Caballero"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "95",
      "title": "Electroencephalogram asymmetry during emotionally evocative films and its relation to positive and negative affectivity",
      "authors": [
        "Aaron Jones",
        "Nathan Fox"
      ],
      "year": "1992",
      "venue": "Brain and Cognition"
    },
    {
      "citation_id": "96",
      "title": "Investigating models of affect: relationships among eeg alpha asymmetry, depression, and anxiety",
      "authors": [
        "L Mathersul",
        "P Williams",
        "A Hopkinson",
        "Kemp"
      ],
      "year": "2008",
      "venue": "Emotion"
    },
    {
      "citation_id": "97",
      "title": "A surface-based analysis of language lateralization and cortical asymmetry",
      "authors": [
        "Lise Douglas N Greve",
        "Qing Van Der Haegen",
        "Steven Cai",
        "Stufflebeam",
        "R Mert",
        "Bruce Sabuncu",
        "Marc Fischl",
        "Brysbaert"
      ],
      "year": "2013",
      "venue": "Journal of cognitive neuroscience"
    },
    {
      "citation_id": "98",
      "title": "Continuous music-emotion recognition based on electroencephalogram",
      "authors": [
        "Nattapong Thammasan",
        "Koichi Moriyama",
        "Ken Fukui",
        "Masayuki Numao"
      ],
      "year": "2016",
      "venue": "IEICE Transactions on Information and Systems"
    },
    {
      "citation_id": "99",
      "title": "Asymmetric spatial pattern for eeg-based emotion detection",
      "authors": [
        "Dong Huang",
        "Cuntai Guan",
        "Kai Keng Ang",
        "Haihong Zhang",
        "Yaozhang Pan"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2012 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "100",
      "title": "Adaptive emotional information retrieval from eeg signals in the time-frequency domain",
      "authors": [
        "C Panagiotis",
        "Leontios Petrantonakis",
        "Hadjileontiadis"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "101",
      "title": "The economy of brain network organization",
      "authors": [
        "Ed Bullmore",
        "Olaf Sporns"
      ],
      "year": "2012",
      "venue": "Nature Reviews Neuroscience"
    },
    {
      "citation_id": "102",
      "title": "Convolutional neural networks using dynamic functional connectivity for eeg-based person identification in diverse human states",
      "authors": [
        "Min Wang",
        "Heba El-Fiqi",
        "Jiankun Hu",
        "Hussein Abbass"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "103",
      "title": "Classifying different emotional states by means of eeg-based functional connectivity patterns",
      "authors": [
        "Youyun Lee",
        "Shulan Hsieh"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "104",
      "title": "Multi-scale brain networks",
      "authors": [
        "F Richard",
        "Danielle Betzel",
        "Bassett"
      ],
      "year": "2017",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "105",
      "title": "Infants and adults have similar regional functional brain organization for the perception of emotions",
      "authors": [
        "Rotem-Kohavi",
        "N Oberlander",
        "Virji-Babul"
      ],
      "year": "2017",
      "venue": "Neuroscience letters"
    },
    {
      "citation_id": "106",
      "title": "Assessment of driver drowsiness using electroencephalogram signals based on multiple functional brain networks",
      "authors": [
        "Jichi Chen",
        "Hong Wang",
        "Chengcheng Hua"
      ],
      "year": "2018",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "107",
      "title": "Analysis of eeg networks and their correlation with cognitive impairment in preschool children with epilepsy",
      "authors": [
        "Eli Kinney-Lang",
        "Michael Yoong",
        "Matthew Hunter",
        "Krishnaraya Kamath Tallur",
        "Jay Shetty",
        "Ailsa Mclellan",
        "Richard Fm Chin",
        "Javier Escudero"
      ],
      "year": "2019",
      "venue": "Epilepsy & Behavior"
    },
    {
      "citation_id": "108",
      "title": "A longitudinal eeg study of alzheimer's disease progression based on a complex network approach",
      "authors": [
        "Francesco Carlo Morabito",
        "Maurizio Campolo",
        "Domenico Labate",
        "Giuseppe Morabito",
        "Lilla Bonanno",
        "Alessia Bramanti",
        "Simona Salvo",
        "Angela Marra",
        "Placido Bramanti"
      ],
      "year": "2015",
      "venue": "International journal of neural systems"
    },
    {
      "citation_id": "109",
      "title": "Functional neural network analysis in frontotemporal dementia and alzheimer's disease using eeg and graph theory",
      "authors": [
        "Willem De Haan",
        "Yolande Pijnenburg",
        "Rob Strijers",
        "Yolande Van Der Made",
        "M Wiesje",
        "Philip Van Der Flier",
        "Cornelis Scheltens",
        "Stam"
      ],
      "year": "2009",
      "venue": "BMC neuroscience"
    },
    {
      "citation_id": "110",
      "title": "Cortical network topology in prodromal and mild dementia due to alzheimer's disease: graph theory applied to resting state eeg",
      "authors": [
        "Raffaella Franciotti",
        "Nicola Falasca",
        "Dario Arnaldi",
        "Francesco Famà",
        "Claudio Babiloni",
        "Marco Onofrj",
        "Mariano Flavio",
        "Laura Nobili",
        "Bonanni"
      ],
      "year": "2019",
      "venue": "Brain topography"
    },
    {
      "citation_id": "111",
      "title": "The brainweb: phase synchronization and large-scale integration",
      "authors": [
        "Francisco Varela",
        "Jean-Philippe Lachaux",
        "Eugenio Rodriguez",
        "Jacques Martinerie"
      ],
      "year": "2001",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "112",
      "title": "A deep learning framework for identifying children with adhd using an eeg-based brain network",
      "authors": [
        "He Chen",
        "Yan Song",
        "Xiaoli Li"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "113",
      "title": "An introduction to variable and feature selection",
      "authors": [
        "Isabelle Guyon",
        "André Elisseeff"
      ],
      "year": "2003",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "114",
      "title": "A filter approach to feature selection based on mutual information",
      "authors": [
        "Jinjie Huang",
        "Yunze Cai",
        "Xiaoming Xu"
      ],
      "year": "2006",
      "venue": "Proceedings of 5th IEEE International Conference on Cognitive Informatics",
      "doi": "10.1109/COGINF.2006.365681"
    },
    {
      "citation_id": "115",
      "title": "A wrapper method for feature selection using support vector machines",
      "authors": [
        "Sebastián Maldonado",
        "Richard Weber",
        "; • Li",
        "Zhang"
      ],
      "year": "2008",
      "venue": "Information Sciences",
      "doi": "10.1016/j.ins.2009.02.014"
    },
    {
      "citation_id": "116",
      "title": "Minimum redundancy feature selection from microarray gene expression data",
      "authors": [
        "Chris Ding",
        "Hanchuan Peng"
      ],
      "year": "2005",
      "venue": "Journal of bioinformatics and computational biology"
    },
    {
      "citation_id": "117",
      "title": "Improving bci-based emotion recognition by combining eeg feature selection and kernel classifiers",
      "authors": [
        "John Atkinson",
        "Daniel Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "118",
      "title": "Gene selection for cancer classification using support vector machines",
      "authors": [
        "Isabelle Guyon",
        "Jason Weston",
        "Stephen Barnhill",
        "Vladimir Vapnik"
      ],
      "year": "2002",
      "venue": "Machine Learning",
      "doi": "10.1023/A:1012487302797"
    },
    {
      "citation_id": "119",
      "title": "Feature selection, L1 vs. L2 regularization, and rotational invariance",
      "authors": [
        "Y Andrew",
        "Ng"
      ],
      "year": "2004",
      "venue": "Proceedings of the 21st International Conference on Machine Learning",
      "doi": "10.1145/1015330.1015435"
    },
    {
      "citation_id": "120",
      "title": "On the interpretation of weight vectors of linear models in multivariate neuroimaging",
      "authors": [
        "Stefan Haufe",
        "Frank Meinecke",
        "Kai Görgen",
        "Sven Dähne",
        "J Haynes",
        "Benjamin Blankertz",
        "Felix Bießmann"
      ],
      "year": "2014",
      "venue": "Neuroimage",
      "doi": "10.1016/j.neuroimage.2013.10.067"
    },
    {
      "citation_id": "121",
      "title": "Enhancing performance of eeg-based emotion recognition systems using feature smoothing",
      "authors": [
        "Trung Duy Pham",
        "Dat Tran",
        "Wanli Ma",
        "Nga Thuy"
      ],
      "year": "2015",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "122",
      "title": "Eeg-based emotion recognition via fast and robust feature smoothing",
      "authors": [
        "Cheng Tang",
        "Di Wang",
        "Ah-Hwee Tan",
        "Chunyan Miao"
      ],
      "year": "2017",
      "venue": "International Conference on Brain Informatics"
    },
    {
      "citation_id": "123",
      "title": "An investigation of deep learning models for eeg-based emotion recognition",
      "authors": [
        "Yaqing Zhang",
        "Jinling Chen",
        "Jen Hong Tan",
        "Yuxuan Chen",
        "Yunyi Chen",
        "Dihan Li",
        "Lei Yang",
        "Jian Su",
        "Xin Huang",
        "Wenliang Che"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "124",
      "title": "Affective classification using bayesian classifier and supervised learning",
      "authors": [
        "Seong Youb",
        "Hyun Yoon"
      ],
      "year": "2012",
      "venue": "Proceedings of the 2012 12th International Conference on Control, Automation and Systems"
    },
    {
      "citation_id": "125",
      "title": "Eeg-based automatic emotion recognition: Feature extraction, selection and classification methods",
      "authors": [
        "Pascal Ackermann",
        "Christian Kohlschein",
        "Agila Jó",
        "Klaus Bitsch",
        "Sabina Wehrle",
        "Jeschke"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 18th International Conference on e-Health Networking, Applications and Services (Healthcom)"
    },
    {
      "citation_id": "126",
      "title": "Human emotion recognition and analysis in response to audio music using brain signals",
      "authors": [
        "Adnan Mehmood Bhatti",
        "Muhammad Majid",
        "Syed Muhammad Anwar",
        "Bilal Khan"
      ],
      "year": "2016",
      "venue": "Computers in Human Behavior"
    },
    {
      "citation_id": "127",
      "title": "Lifting scheme for human emotion recognition using eeg",
      "authors": [
        "M Murugappan",
        "R Rizon",
        "S Nagarajan",
        "I Yaacob",
        "Zunaidi",
        "Hazry"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 International symposium on information technology"
    },
    {
      "citation_id": "128",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "Robert Jenke",
        "Angelika Peer",
        "Martin Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "129",
      "title": "Eeg emotion recognition based on graph regularized sparse linear regression",
      "authors": [
        "Yang Li",
        "Wenming Zheng",
        "Zhen Cui",
        "Yuan Zong",
        "Sheng Ge"
      ],
      "year": "2019",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "130",
      "title": "Emotion recognition from multi-channel eeg via deep forest",
      "authors": [
        "Juan Cheng",
        "Meiyao Chen",
        "Chang Li",
        "Yu Liu",
        "Rencheng Song",
        "Aiping Liu",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "131",
      "title": "Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results",
      "authors": [
        "Franco Scarselli",
        "Ah Chung"
      ],
      "year": "1998",
      "venue": "Neural networks"
    },
    {
      "citation_id": "132",
      "title": "Application of deep belief networks in eeg-based dynamic music-emotion recognition",
      "authors": [
        "Nattapong Thammasan",
        "Ken-Ichi Fukui",
        "Masayuki Numao"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "133",
      "title": "A fast learning algorithm for deep belief nets",
      "authors": [
        "Geoffrey Hinton",
        "Simon Osindero",
        "Yee-Whye Teh"
      ],
      "year": "2006",
      "venue": "Neural computation"
    },
    {
      "citation_id": "134",
      "title": "Using deep and convolutional neural networks for accurate emotion classification on deap dataset",
      "authors": [
        "Samarth Tripathi",
        "Shrinivas Acharya",
        "Dev Ranti",
        "Sudhanshi Sharma",
        "Samit Mittal",
        "Bhattacharya"
      ],
      "year": "2017",
      "venue": "Proceedings of the 21st AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "135",
      "title": "Emotion recognition from multiband eeg signals using capsnet",
      "authors": [
        "Hao Chao",
        "Liang Dong",
        "Yongli Liu",
        "Baoyun Lu"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "136",
      "title": "Dynamic routing between capsules",
      "authors": [
        "Sara Sabour",
        "Nicholas Frosst",
        "Geoffrey Hinton"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "137",
      "title": "Multi-channel eeg-based emotion recognition via a multi-level features guided capsule network",
      "authors": [
        "Yu Liu",
        "Yufeng Ding",
        "Chang Li",
        "Juan Cheng",
        "Rencheng Song",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "138",
      "title": "Robust spatial filtering with graph convolutional neural networks",
      "authors": [
        "Felipe Petroski Such",
        "Shagan Sah",
        "Miguel Dominguez",
        "Suhas Pillai",
        "Chao Zhang",
        "Andrew Michael",
        "Nathan Cahill",
        "Raymond Ptucha"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "139",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song",
        "Zhen Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "140",
      "title": "Variational pathway reasoning for eeg emotion recognition",
      "authors": [
        "Tong Zhang",
        "Zhen Cui",
        "Chunyan Xu",
        "Wenming Zheng",
        "Jian Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "141",
      "title": "Neural systems for recognizing emotion",
      "authors": [
        "Ralph Adolphs"
      ],
      "year": "2002",
      "venue": "Current opinion in neurobiology"
    },
    {
      "citation_id": "142",
      "title": "Complex brain networks: graph theoretical analysis of structural and functional systems",
      "authors": [
        "Ed Bullmore",
        "Olaf Sporns"
      ],
      "year": "2009",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "143",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Yang Li",
        "Lei Wang",
        "Wenming Zheng",
        "Yuan Zong",
        "Lei Qi",
        "Zhen Cui",
        "Tong Zhang",
        "Tengfei Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "144",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Yang Li",
        "Wenming Zheng",
        "Zhen Cui",
        "Tong Zhang",
        "Yuan Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "145",
      "title": "Differences first in asymmetric brain: A bi-hemisphere discrepancy convolutional neural network for eeg emotion recognition",
      "authors": [
        "Dongmin Huang",
        "Sentao Chen",
        "Cheng Liu",
        "Lin Zheng",
        "Zhihang Tian",
        "Dazhi Jiang"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "146",
      "title": "Eeg-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "Heng Cui",
        "Aiping Liu",
        "Xu Zhang",
        "Xiang Chen",
        "Kongqiao Wang",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "147",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "148",
      "title": "Emotionet: A 3-d convolutional neural network for eeg-based emotion recognition",
      "authors": [
        "Yi Wang",
        "Zhiyi Huang",
        "Brendan Mccane",
        "Phoebe Neo"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "149",
      "title": "Md Saiful Islam, and Mohammad Ali Moni. Eeg channel correlation based model for emotion recognition",
      "year": "2021",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "150",
      "title": "Eeg-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "Reda Elham S Salama",
        "Mahmoud El-Khoribi",
        "Mohamed A Wahby Shoman",
        "Shalaby"
      ],
      "year": "2018",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "151",
      "title": "Spatio-temporal representation of an electoencephalogram for emotion recognition using a three-dimensional convolutional neural network",
      "authors": [
        "Jungchan Cho",
        "Hyoseok Hwang"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "152",
      "title": "Tsception: a deep learning framework for emotion detection using eeg",
      "authors": [
        "Yi Ding",
        "Neethu Robinson",
        "Qiuhao Zeng",
        "Duo Chen"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "153",
      "title": "Eeg-based emotion recognition using simple recurrent units network and ensemble learning",
      "authors": [
        "Chen Wei",
        "Lan-Lan Chen",
        "Zhen-Zhen Song",
        "Xiao-Guang Lou",
        "Dong-Dong Li"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "154",
      "title": "3d convolutional neural networks for human action recognition",
      "authors": [
        "Shuiwang Ji",
        "Wei Xu",
        "Ming Yang",
        "Kai Yu"
      ],
      "year": "2012",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "155",
      "title": "Sst-emotionnet: Spatial-spectral-temporal based attention 3d dense network for eeg emotion recognition",
      "authors": [
        "Ziyu Jia",
        "Youfang Lin",
        "Xiyang Cai",
        "Haobin Chen",
        "Haijun Gou",
        "Jing Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "156",
      "title": "Emotion recognition from multi-channel eeg data through convolutional recurrent neural network",
      "authors": [
        "Xiang Li",
        "Dawei Song",
        "Peng Zhang",
        "Guangliang Yu",
        "Yuexian Hou",
        "Bin Hu"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on bioinformatics and biomedicine (BIBM)"
    },
    {
      "citation_id": "157",
      "title": "Robert Boots, and Boualem Benatallah. Cascade and parallel convolutional recurrent neural networks on eeg-based intention recognition for brain computer interface",
      "authors": [
        "Dalin Zhang",
        "Lina Yao",
        "Xiang Zhang",
        "Sen Wang",
        "Weitong Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "158",
      "title": "EEG based Emotion Recognition: A Tutorial and Review",
      "year": "2022",
      "venue": "EEG based Emotion Recognition: A Tutorial and Review"
    },
    {
      "citation_id": "159",
      "title": "Emotion recognition from multi-channel eeg through parallel convolutional recurrent neural network",
      "authors": [
        "Yilong Yang",
        "Qingfeng Wu",
        "Ming Qiu",
        "Yingdong Wang",
        "Xiaowei Chen"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "160",
      "title": "Eeg-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "Wei Tao",
        "Chang Li",
        "Rencheng Song",
        "Juan Cheng",
        "Yu Liu",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "161",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Yang Li",
        "Wenming Zheng",
        "Lei Wang",
        "Yuan Zong",
        "Zhen Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "162",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "Tong Zhang",
        "Wenming Zheng",
        "Zhen Cui",
        "Yuan Zong",
        "Yang Li"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "163",
      "title": "Eeg-based emotion recognition using spatial-temporal representation via bi-gru",
      "authors": [
        "Wai-Cheong Lincoln Lew",
        "Di Wang",
        "Katsiaryna Shylouskaya",
        "Zhuo Zhang",
        "Joo-Hwee Lim",
        "Kai Keng Ang",
        "Ah-Hwee Tan"
      ],
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "164",
      "title": "Latent factor decoding of multi-channel eeg for emotion recognition through autoencoder-like neural networks",
      "authors": [
        "Xiang Li",
        "Zhigang Zhao",
        "Dawei Song",
        "Yazhou Zhang",
        "Jingshan Pan",
        "Lu Wu",
        "Jidong Huo",
        "Chunyang Niu",
        "Di Wang"
      ],
      "year": "2020",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "165",
      "title": "Sae+ lstm: A new framework for emotion recognition from multi-channel eeg",
      "authors": [
        "Xiaofen Xing",
        "Zhenqi Li",
        "Tianyuan Xu",
        "Lin Shu",
        "Bin Hu",
        "Xiangmin Xu"
      ],
      "year": "2019",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "166",
      "title": "Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Yongqiang Yin",
        "Xiangwei Zheng",
        "Bin Hu",
        "Yuang Zhang",
        "Xinchun Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "167",
      "title": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "authors": [
        "Guangyi Zhang",
        "Ali Etemad"
      ],
      "year": "2020",
      "venue": "Rfnet: Riemannian fusion network for eeg-based brain-computer interfaces",
      "arxiv": "arXiv:2008.08633"
    },
    {
      "citation_id": "168",
      "title": "From euclidean to riemannian means: Information geometry for ssvep classification",
      "authors": [
        "Sylvain Emmanuel K Kalunga",
        "Quentin Chevallier",
        "Karim Barthélemy",
        "Yskandar Djouani",
        "Eric Hamam",
        "Monacelli"
      ],
      "year": "2015",
      "venue": "International Conference on Geometric Science of Information"
    },
    {
      "citation_id": "169",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "Jonghwa Kim",
        "Elisabeth André"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "170",
      "title": "Detecting naturalistic expressions of nonbasic affect using physiological signals",
      "authors": [
        "Omar Alzoubi",
        "K D' Sidney",
        "Rafael Mello",
        "Calvo"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "171",
      "title": "Individual differences in emotion processing",
      "authors": [
        "S Hamann",
        "Canli"
      ],
      "year": "2004",
      "venue": "Current Opinion in Neurobiology"
    },
    {
      "citation_id": "172",
      "title": "Differential alpha coherence hemispheric patterns in men and women during pleasant and unpleasant musical emotions",
      "authors": [
        "Enrique Flores-Gutiérrez",
        "José-Luis Díaz",
        "Fernando Barrios",
        "Miguel Guevara",
        "Yolanda Del Río-Portilla",
        "María Corsi-Cabrera",
        "Enrique Del Flores-Gutiérrez"
      ],
      "year": "2009",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "173",
      "title": "Discovering gender differences in facial emotion recognition via implicit behavioral cues",
      "authors": [
        "Maneesh Bilalpur",
        "Seyed Mostafa Kia",
        "Tat-Seng Chua",
        "Ramanathan Subramanian"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "174",
      "title": "Eeg spectral powers and source localization in depressing, sad, and fun music videos focusing on gender differences",
      "authors": [
        "Atefeh Goshvarpour",
        "Ateke Goshvarpour"
      ],
      "year": "2019",
      "venue": "Cognitive neurodynamics"
    },
    {
      "citation_id": "175",
      "title": "Emotion and motivation ii: sex differences in picture processing",
      "authors": [
        "Margaret M Bradley",
        "Maurizio Codispoti",
        "Dean Sabatinelli",
        "Peter Lang"
      ],
      "year": "2001",
      "venue": "Emotion"
    },
    {
      "citation_id": "176",
      "title": "Neural activities associated with emotion recognition observed in men and women",
      "authors": [
        "Tmc Lee",
        "Cch Liu",
        "Chan",
        "Sy Fang",
        "Gao"
      ],
      "year": "2005",
      "venue": "Molecular psychiatry"
    },
    {
      "citation_id": "177",
      "title": "Understanding the impact of 5-httlpr, antidepressants, and acute tryptophan depletion on brain activation during facial emotion processing: A review of the imaging literature",
      "authors": [
        "Kyeon Raab",
        "Peter Kirsch",
        "Daniela Mier"
      ],
      "year": "2016",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "178",
      "title": "Gender and culture differences in emotion",
      "authors": [
        "Agneta Fischer",
        "Patricia Rodriguez Mosquera",
        "E Annelies",
        "Antony Van Vianen",
        "Manstead"
      ],
      "year": "2004",
      "venue": "Emotion"
    },
    {
      "citation_id": "179",
      "title": "Cross-subject and cross-gender emotion classification from eeg",
      "authors": [
        "Jia-Yi Zhu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "In World Congress on Medical Physics and Biomedical Engineering"
    },
    {
      "citation_id": "180",
      "title": "Gender effects on an eeg-based emotion level classification system",
      "authors": [
        "Pava",
        "Paula Álvarez",
        "Germán Herrera",
        "Castellanos-Domínguez",
        "Orozco"
      ],
      "year": "2018",
      "venue": "Iberoamerican Congress on Pattern Recognition"
    },
    {
      "citation_id": "181",
      "title": "Native assessment of international affective picture system",
      "authors": [
        "Y Huang"
      ],
      "year": "2004",
      "venue": "Chinese Mental Health Journal"
    },
    {
      "citation_id": "182",
      "title": "Emotion perception and recognition: an exploration of cultural differences and similarities",
      "authors": [
        "Vladimir Kurbalija",
        "Mirjana Ivanović",
        "Miloš Radovanović",
        "Zoltan Geler",
        "Weihui Dai",
        "Weidong Zhao"
      ],
      "year": "2018",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "183",
      "title": "A cross-culture study on multimodal emotion recognition using deep learning",
      "authors": [
        "Lu Gan",
        "Wei Liu",
        "Yun Luo",
        "Xun Wu",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "184",
      "title": "Using physiological signals to detect natural interactive behavior",
      "authors": [
        "Yasser Mohammad",
        "Toyoaki Nishida"
      ],
      "year": "2010",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "185",
      "title": "Transfer learning: A riemannian geometry framework with applications to brain-computer interfaces",
      "authors": [
        "Paolo Zanini",
        "Marco Congedo",
        "Christian Jutten",
        "Salem Said",
        "Yannick Berthoumieu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "186",
      "title": "Cross-subject eeg-based emotion recognition through neural networks with stratified normalization",
      "authors": [
        "Javier Fernandez",
        "Nicholas Guttenberg",
        "Olaf Witkowski",
        "Antoine Pasquali"
      ],
      "year": "2021",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "187",
      "title": "Affect prediction from physiological measures via visual stimuli",
      "authors": [
        "Feng Zhou",
        "Xingda Qu",
        "Martin Helander",
        "Jianxin Roger"
      ],
      "year": "2011",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "188",
      "title": "Real-time classification of evoked emotions using facial feature tracking and physiological responses",
      "authors": [
        "Emmanuel Jeremy N Bailenson",
        "Iris Pontikakis",
        "James Mauss",
        "Maria Gross",
        "Jabon",
        "A Cendri",
        "Clifford Hutcherson",
        "Oliver Nass",
        "John"
      ],
      "year": "2008",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "189",
      "title": "Subject-independent emotion recognition based on physiological signals: a three-stage decision method",
      "authors": [
        "Jing Chen",
        "Bin Hu",
        "Yue Wang",
        "Philip Moore",
        "Yongqiang Dai",
        "Lei Feng",
        "Zhijie Ding"
      ],
      "year": "2017",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "190",
      "title": "Domain adaptation for cross-subject emotion recognition by subject clustering",
      "authors": [
        "Jin Liu",
        "Xinke Shen",
        "Sen Song",
        "Dan Zhang"
      ],
      "year": "2021",
      "venue": "2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "191",
      "title": "Domain adaptation techniques for eeg-based emotion recognition: a comparative study on two public datasets",
      "authors": [
        "Zirui Lan",
        "Olga Sourina",
        "Lipo Wang",
        "Reinhold Scherer",
        "Gernot R Müller-Putz"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "192",
      "title": "Wgan domain adaptation for eeg-based emotion recognition",
      "authors": [
        "Yun Luo",
        "Si-Yang Zhang",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "193",
      "title": "A deep multi-source adaptation transfer network for cross-subject electroencephalogram emotion recognition",
      "authors": [
        "Fei Wang",
        "Weiwei Zhang",
        "Zongfeng Xu",
        "Jingyu Ping",
        "Hao Chu"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "194",
      "title": "Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset eeg emotion recognition",
      "authors": [
        "Yucel Cimtay",
        "Erhan Ekmekcioglu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "195",
      "title": "Emotion recognition with convolutional neural network and eeg-based efdms",
      "authors": [
        "Fei Wang",
        "Shichao Wu",
        "Weiwei Zhang",
        "Zongfeng Xu",
        "Yahui Zhang",
        "Chengdong Wu",
        "Sonya Coleman"
      ],
      "year": "2020",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "196",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "He Li",
        "Yi-Ming Jin",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2018",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "197",
      "title": "Cross-subject electroencephalogram emotion recognition based on maximum classifier discrepancy. Sheng wu yi xue gong cheng xue za zhi=",
      "authors": [
        "Ziliang Cai",
        "Miaomiao Guo",
        "Xinsheng Yang",
        "Xintong Chen",
        "Guizhi Xu"
      ],
      "year": "2021",
      "venue": "Journal of biomedical engineering= Shengwu yixue gongchengxue zazhi"
    },
    {
      "citation_id": "198",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "Li-Ming Zhao",
        "Xu Yan",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 35th AAAI Conference on Artificial Intelligence. sn"
    },
    {
      "citation_id": "199",
      "title": "Eeg emotion enhancement using task-specific domain adversarial neural network",
      "authors": [
        "Ke-Ming Ding",
        "Tsukasa Kimura",
        "Ken-Ichi Fukui",
        "Masayuki Numao"
      ],
      "year": "2021",
      "venue": "2021 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "200",
      "title": "Cross-subject eeg emotion classification based on few-label adversarial domain adaption",
      "authors": [
        "Yingdong Wang",
        "Jiatong Liu",
        "Qunsheng Ruan",
        "Shuocheng Wang",
        "Chen Wang"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "201",
      "title": "Distilling eeg representations via capsules for affective computing",
      "authors": [
        "Guangyi Zhang",
        "Ali Etemad",
        "; • Li",
        "Zhang"
      ],
      "year": "2021",
      "venue": "Distilling eeg representations via capsules for affective computing",
      "arxiv": "arXiv:2105.00104"
    },
    {
      "citation_id": "202",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "203",
      "title": "Ultra efficient transfer learning with meta update for cross subject eeg classification",
      "authors": [
        "Tiehang Duan",
        "Mihir Chauhan",
        "Mohammad Shaikh",
        "Jun Chu",
        "Sargur Srihari"
      ],
      "year": "2020",
      "venue": "Ultra efficient transfer learning with meta update for cross subject eeg classification",
      "arxiv": "arXiv:2003.06113"
    },
    {
      "citation_id": "204",
      "title": "Meta learn on constrained transfer learning for low resource cross subject eeg classification",
      "authors": [
        "Tiehang Duan",
        "Mohammad Shaikh",
        "Mihir Chauhan",
        "Jun Chu",
        "K Rohini",
        "Archita Srihari",
        "Sargur Pathak",
        "Srihari"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "205",
      "title": "Standardization-refinement domain adaptation method for cross-subject eeg-based classification in imagined speech recognition",
      "authors": [
        "Magdiel Jiménez",
        "Pilar Gómez-Gil"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "206",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "Mohammad Soleymani",
        "Maja Pantic",
        "Thierry Pun"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "207",
      "title": "Locally robust eeg feature selection for individual-independent emotion recognition",
      "authors": [
        "Lei Zhong Yin",
        "Jianing Liu",
        "Boxi Chen",
        "Yongxiong Zhao",
        "Wang"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "208",
      "title": "A review on transfer learning in eeg signal analysis",
      "authors": [
        "Zitong Wan",
        "Rui Yang",
        "Mengjie Huang",
        "Nianyin Zeng",
        "Xiaohui Liu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "209",
      "title": "Optimal feature selection and deep learning ensembles method for emotion recognition from human brain eeg sensors",
      "authors": [
        "Raja Majid Mehmood",
        "Ruoyu Du",
        "Hyo Lee"
      ],
      "year": "2017",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "210",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Mengyuan Zhong Yin",
        "Yongxiong Zhao",
        "Jingdong Wang",
        "Jianhua Yang",
        "Zhang"
      ],
      "year": "2017",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "211",
      "title": "Emotion recognition of eeg signals based on the ensemble learning method: Adaboost. Mathematical Problems in Engineering",
      "authors": [
        "Yu Chen",
        "Rui Chang",
        "Jifeng Guo"
      ],
      "year": "2021",
      "venue": "Emotion recognition of eeg signals based on the ensemble learning method: Adaboost. Mathematical Problems in Engineering"
    },
    {
      "citation_id": "212",
      "title": "Eeg feature extraction for classifying emotions using fcm and fkm",
      "authors": [
        "Murugappn Murugappan",
        "M Rizon",
        "R Nagarajan",
        "S Yaacob",
        "I Zunaidi",
        "Hazry"
      ],
      "year": "2007",
      "venue": "International journal of Computers and Communications"
    },
    {
      "citation_id": "213",
      "title": "Fuzzy logic based emotion classification",
      "authors": [
        "Stephen Joseph W Matiko",
        "John Beeby",
        "Tudor"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "214",
      "title": "A novel method of eeg-based emotion recognition using nonlinear features variability and dempster-shafer theory",
      "authors": [
        "Zangeneh Morteza",
        "Keivan Soroush",
        "Seyed Maghooli",
        "Ali Kamaledin Setarehdan",
        "Nasrabadi"
      ],
      "year": "2018",
      "venue": "Biomedical Engineering: Applications, Basis and Communications"
    },
    {
      "citation_id": "215",
      "title": "A hybrid fuzzy cognitive map/support vector machine approach for eeg-based emotion classification using compressed sensing",
      "authors": [
        "Kairui Guo",
        "Rifai Chai",
        "Henry Candra",
        "Ying Guo",
        "Rong Song",
        "Hung Nguyen",
        "Steven Su"
      ],
      "year": "2019",
      "venue": "International Journal of Fuzzy Systems"
    },
    {
      "citation_id": "216",
      "title": "Data augmentation for eeg-based emotion recognition with deep convolutional neural networks",
      "authors": [
        "Fang Wang",
        "Sheng-Hua Zhong",
        "Jianfeng Peng",
        "Jianmin Jiang",
        "Yan Liu"
      ],
      "year": "2018",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "217",
      "title": "Data augmentation for enhancing eeg-based emotion recognition with deep generative models",
      "authors": [
        "Yun Luo",
        "Li-Zhen Zhu",
        "Zi-Yu Wan",
        "Bao-Liang Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "218",
      "title": "Data augmentation for deep-learning-based electroencephalography",
      "authors": [
        "Elnaz Lashgari",
        "Dehua Liang",
        "Uri Maoz"
      ],
      "year": "2020",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "219",
      "title": "Calibration free meta learning based approach for subject independent eeg emotion recognition",
      "authors": [
        "Swapnil Bhosale",
        "Rupayan Chakraborty",
        "Sunil Kumar"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "220",
      "title": "Automl: A survey of the state-of-the-art",
      "authors": [
        "Xin He",
        "Kaiyong Zhao",
        "Xiaowen Chu"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "221",
      "title": "Strengthen eeg-based emotion recognition using firefly integrated optimization algorithm",
      "authors": [
        "Hong He",
        "Yonghong Tan",
        "Jun Ying",
        "Wuxiong Zhang"
      ],
      "year": "2020",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "222",
      "title": "Optimization of deep architectures for eeg signal classification: An automl approach using evolutionary algorithms",
      "authors": [
        "Diego Aquino-Brítez",
        "Andrés Ortiz",
        "Julio Ortega",
        "Javier León",
        "Marco Formoso",
        "John Gan",
        "Juan Escobar"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "223",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "Weilong Zheng",
        "Jiayi Zhu",
        "Baoliang Lu"
      ],
      "year": "2017",
      "venue": "Identifying stable patterns over time for emotion recognition from eeg",
      "arxiv": "arXiv:1601.02197"
    },
    {
      "citation_id": "224",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Biomedical & Health Informatics"
    },
    {
      "citation_id": "225",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "226",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Cheng Lu",
        "Yuan Zong",
        "Xilei Zhang",
        "Zhen Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "227",
      "title": "Reducing the subject variability of eeg signals with adversarial domain generalization",
      "authors": [
        "Bo-Qun Ma",
        "He Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "228",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "Jiaxin Ma",
        "Hao Tang",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "229",
      "title": "Eeg-based emotion classification using spiking neural networks",
      "authors": [
        "Yuling Luo",
        "Qiang Fu",
        "Juntao Xie",
        "Yunbai Qin",
        "Guopei Wu",
        "Junxiu Liu",
        "Frank Jiang",
        "Yi Cao",
        "Xuemei Ding"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "230",
      "title": "The htm spatial pooler-a neocortical algorithm for online sparse distributed coding",
      "authors": [
        "Yuwei Cui",
        "Subutai Ahmad",
        "Jeff Hawkins"
      ],
      "year": "2017",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "231",
      "title": "A survey on deep learning: Algorithms, techniques, and applications",
      "authors": [
        "Samira Pouyanfar",
        "Saad Sadiq",
        "Yilin Yan",
        "Haiman Tian",
        "Yudong Tao",
        "Maria Reyes",
        "Mei-Ling Shyu",
        "Shu-Ching Chen",
        "Iyengar"
      ],
      "year": "2018",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "232",
      "title": "Inceptionism: Going deeper into neural networks",
      "authors": [
        "Alexander Mordvintsev",
        "Christopher Olah",
        "Mike Tyka"
      ],
      "year": "2015",
      "venue": "Inceptionism: Going deeper into neural networks"
    },
    {
      "citation_id": "233",
      "title": "Model-agnostic interpretability of machine learning",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Model-agnostic interpretability of machine learning",
      "arxiv": "arXiv:1606.05386"
    },
    {
      "citation_id": "234",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": [
        "Xipeng Qiu",
        "Tianxiang Sun",
        "Yige Xu",
        "Yunfan Shao",
        "Ning Dai",
        "Xuanjing Huang"
      ],
      "year": "2020",
      "venue": "Science China Technological Sciences"
    }
  ]
}