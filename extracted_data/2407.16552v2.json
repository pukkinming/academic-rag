{
  "paper_id": "2407.16552v2",
  "title": "Microemo: Time-Sensitive Multimodal Emotion Recognition With Micro-Expression Dynamics In Video Dialogues",
  "published": "2024-07-23T15:05:55Z",
  "authors": [
    "Liyun Zhang"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Computing methodologies ‚Üí Neural networks",
    "‚Ä¢ Artifcial intelligence",
    "‚Ä¢ Computer vision",
    "‚Ä¢ Natural language processing",
    "Multimodal emotion recognition, Multimodal Large language models (MLLMs), Micro-expressions, Time-sensitive, Video Q-Former, Open-vocabulary emotion recognition, Global-local attention visual encoder, Utterance-aware"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable multimodal emotion recognition capabilities, integrating multimodal cues from visual, acoustic, and linguistic contexts in the video to recognize human emotional states. However, existing methods ignore capturing local facial features of temporal dynamics of micro-expressions and do not leverage the contextual dependencies of the utterance-aware temporal segments in the video, thereby limiting their expected effectiveness to a certain extent. In this work, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention to the local facial micro-expression dynamics and the contextual dependencies of utterance-aware video clips. Our model incorporates two key architectural contributions: (1) a global-local attention visual encoder that integrates global frame-level timestamp-bound image features with local facial features of temporal dynamics of micro-expressions; (2) an utteranceaware video Q-Former that captures multi-scale and contextual dependencies by generating visual token sequences for each utterance segment and for the entire video then combining them. Preliminary qualitative experiments demonstrate that in a new Explainable Multimodal Emotion Recognition (EMER) task that exploits multi-modal and multi-faceted clues to predict emotions in an open-vocabulary (OV) manner, MicroEmo demonstrates its effectiveness compared with the latest methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities in integrating multimodal information (e.g., visual, acoustic, and linguistic modalities) and understanding the context to generate text descriptions  [29] . Many MLLMs with outstanding performance can be adapted to different tasks by tuning on specific datasets. However, there still remains a gap in the expected effectiveness. This is because video understanding requires a full perception and deep reasoning of visual/acoustic details and temporal dynamics for models  [7, 33] , particularly in the multimodal emotion recognition task.\n\nFor example, in a new Explainable Multimodal Emotion Recognition (EMER) task  [16]  that exploits multi-modal and multi-faceted clues to predict emotions in an open-vocabulary (OV) manner. This not only requires the integration of multimodal information but also demands the in-depth capture of emotion-related subtle information, e.g., micro-expressions. Emotion is a highly sensitive state form that undergoes dynamic changes. The emotions in each utterance of the video evolve dynamically over time, which is crucial information that cannot be overlooked. It necessitates the design of specialized modules within the model to capture and leverage these elements, enabling the provision of detailed recognition of subtle emotional changes without compromising contextual accuracy. However, current existing methods ignore capturing local facial features of temporal dynamics of micro-expressions and do not leverage the contextual dependencies of the utterance-aware temporal segments in the video, thereby limiting their expected effectiveness to a certain extent when tuning MLLMs to Multimodal Emotion Recognition tasks.\n\nTo this end, we propose MicroEmo, a novel time-sensitive MLLM aimed at directing attention to the local facial micro-expression dynamics and the contextual dependencies of utterance-aware video clips. MicroEmo incorporates two key modules, which consists of i) a global-local attention visual encoder that first extracts and integrates global frame-level timestamp-bound image features with local facial features of temporal dynamics of micro-expressions, and ii) an utterance-aware video Q-Former that that models each utterance time segment feature via an utterance-aware sliding window and combines it with whole features to produce multi-scale fused tokens for the image.\n\nBenefiting from the attention to local facial features of temporal dynamics of micro-expressions and fully capturing multi-scale and contextual dependencies of utterances in videos, MicroEmo shows its superiority in capturing more subtle and deeper emotional information to enable the provision of detailed recognition of subtle emotional changes without compromising contextual accuracy. Finally, audio is also processed by a pre-trained encoder and whole video Q-Former to obtain audio tokens, these multimodal tokens are concatenated with transcribed speech and instructions to feed into the LLM for result generation. Our contributions to this paper are listed as follows:",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Multimodal Large Language Model (MLLM) has attracted much attention due to its adaptability to adapt to different tasks by tuning on specific datasets, e.g., tuning on EMER  [16]  for multimodal emotion recognition. Numerous studies have endeavored to integrate LLMs with a video encoder for video understanding  [18, 21, 24, 25] .\n\nThey typically employ open-source LLMs  [1, 28]  to encode the video into vision tokens compatible with the LLMs. Such as VideoChat  [12]  uses a video transformer to encode video features and then implements a Query Transformer (Q-Former)  [11]  for video tokens. Video-llama  [33]  uses a vision transformer (ViT)  [4]  with an image Q-Former to encode frames and employs a video Q-Former for temporal modeling. TimeChat  [24]  incorporates frame timestamps to make the model time-sensitive and uses a video Q-Former to establish inter-frame temporal dependencies. However, these methods ignore capturing local features of temporal dynamics and do not leverage the contextual dependencies of important segments in videos, thereby limiting their expected effectiveness to a certain extent. In contrast, our MicroEmo proposes a global-local attention visual encoder and an utterance-aware video Q-Former to capture subtle local features of temporal dynamics and contextual dependencies in videos, such as micro-expressions and utterance segment context of emotion recognition, enabling the provision of subtle reasoning results without compromising contextual accuracy.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "We present MicroEmo, a time-sensitive MLLM featuring two novel modules: a global-local attention visual encoder and an utteranceaware video Q-Former. These modules enable MicroEmo to capture more subtle and deeper emotional information in videos. We verify the effectiveness of our model on the new task EMER and the new style dataset EMER-Fine  [16]  it pioneered.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview",
      "text": "MicroEmo is mainly composed of a global-local attention visual encoder, an utterance-aware video Q-Former for image, and a large language model, as depicted in Figure  1 . Given an input video, the global-local attention visual encoder first extracts and integrates global frame-level timestamp-bound image features with local facial features of temporal dynamics of micro-expressions. Next, the utterance-aware video Q-Former models each utterance time segment feature via an utterance-aware sliding window and combines it with whole features to produce multi-scale fused tokens for the image. Audio is also passed through a pre-trained encoder and whole video Q-Former to obtain audio tokens. Finally, these multimodal tokens are concatenated with transcribed speech and instructions, which are then fed into the LLM to generate responses.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Global-Local Attention Visual Encoder",
      "text": "First, for the global frame-level features, we use the time-aware frame encoder inspired by InstructBLIP  [3]  and Timechat  [24]  as shown in Figure  1 . Given an input video, we use a pre-trained ViT  [26]  to encode each frame and obtain frame features. Subsequently, we use the keypoint indices of the MediaPipe  [20]  face mesh via extracting landmarks of each frame to define different facial regions as micro-expression regions, which are predefined on facial anatomy. Then, the patches are combined to create masks associated with micro-expression regions. we implemented a localized micro-expression attention mechanism based on the generated mask. This allows each patch to selectively attend to pertinent micro-expression regions, facilitating the extraction of fine-grained facial local features. By emphasizing spatially relevant areas, we enhance the model's capacity to discern subtle facial cues and microexpressions, which are crucial for accurate emotion recognition.\n\nFurthermore, the image Q-Former compresses the frame tokens. The Q-Former takes learnable queries as input, which interacts with the frame features and facial local features via cross-attention and updates the initial queries respectively. The global and local results are fused and further processed to obtain final visual tokens  [11] .\n\nDuring visual token extraction, we add the frame-level timestamp, e.g., \"This frame is sampled at 0.0s.\", as a condition to the Q-Former to fuse the visual and timestamp information for binding temporal dynamics to the visual tokens.\n\nThe proposed global-local attention visual encoder can fully integrate global timestamp-bound frame features with local facial features of temporal dynamics of micro-expressions. Benefiting from the attention to temporal dynamics of micro-expressions of the videos, MicroEmo shows its superiority in capturing more subtle and deeper emotional information to facilitate richer and more reliable emotion recognition results.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Utterance-Aware Video Q-Former",
      "text": "Since frames are encoded independently, we can use a sliding video Q-Former to model temporal relationships across frames and enhance the feature fusion in the temporal dimension  [24] . Here, we propose a novel utterance-aware video Q-Former (yellow block in Figure  1 ), which combines utterance-level timestamps to design a dynamic sliding window with a length of each utterance segment and within each window utilizing the video Q-Former extract video 0.0 -0.8 seconds, look at me. 1.7 -3.5 seconds, Do you think there is any relationship between the two of us? ...",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Large Language Model (Llama) Image Encoder",
      "text": "This frame is sampled at 0.0s.  tokens from frame features. Meanwhile, we also extract video tokens from whole frame features via a global video Q-Former and then combine them to produce multi-scale fused tokens.\n\nIt is crucial to capture the context of each conversation segment, which is the key to the model's understanding of emotional fluctuations and changes. We use this way to fully capture multi-scale and contextual dependencies, maintaining the accuracy and consistency of contextual dependencies when the model provides detailed recognition of subtle emotional changes. Notably, the method maintains its effectiveness even in videos with minimal or no dialogue content, thus ensuring robustness across diverse types of video material. Finally, we use a linear layer to transform the dimension of video tokens to match the dimension of the LLM embedding space.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Large Language Model",
      "text": "We concatenate inputs from different modalities, i.e., the video tokens ùëá ùëâ , audio tokens ùëá ùê¥ , and text query tokens ùëá ùêø ùëû including transcribed speech and instruction. They are fed into a large language model to generate reasonable and expected responses ùëá ùêø ùëé . They have the same token embedding dimension. The training of the MLLM typically utilizes a two-stage training framework. The first stage pre-trains the model using large-scale image-audio-text pairs for vision-audio-language mapping  [14, 19, 23, 30] . The second stage finetunes the model with instruction data for instruction following. Considering computing efficiency, we reuse the checkpoints of the existing open-source models after the first stage of training, e.g., the vision-language and audio-language branches from  [33] , conducting only instruction tuning. During the training procedure, we utilize the language modeling loss for generating target answers ùëá ùêø ùëé and Œò is the trainable parameter, which serves as the objective function:\n\nTo better adapt the LLM to multimodal video tasks, we apply the parameter-efficient fine-tuning method, i.e., LoRA  [8]  to train.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments 4.1 Implementation Details",
      "text": "We use ViT-G/14 from EVA-CLIP  [26]  as the image encoder, Imagebind  [5]  as the audio encoder, and LLaMA-2 (7B)  [29]  as the language foundation model. The parameters of the image Q-Former are initialized from InstructBLIP's checkpoint, while the video Q-Former for vision and audio is initialized from Video-LLaMA's checkpoint. We finetune our MicroEmo on train-set of EMER-Fine  [16]  dataset for 3 epochs, using a batch size of 1, with a single NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and LLM are frozen, while those of image Q-Former, micro-expression attention module, video Q-Former, and linear layer (not shown in the diagram due to space constraints) are tuned.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Setup",
      "text": "Tasks, Datasets and Evaluation Metrics We evaluate our model on a new Explainable Multimodal Emotion Recognition (EMER) task  [16] , test-set of EMER-Fine as the test dataset, the new ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ ùë† and ùëÖùëíùëêùëéùëôùëô ùë† in EMER as metrics, and MLLM such as VideoChat  [12] , Video-LLaMA  [33]  and Chat-UniVi  [9] , etc. as baselines for evaluation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Main Results",
      "text": "We evaluate our MicroEmo and some existing MLLMs for EMER task as shown in Table  1 , where, the baseline methods except for AffectGPT  [16]  are zero-shot performance results. AffectGPT is only tuned on the EMER-Fine  [16]  dataset in the second stage without pre-training of the first stage, i.e., pre-training the model using large-scale image-audio-text pairs for vision-audio-language mapping, which is the same as our experimental setting. EMER (Multi)  [16]  can be viewed as a post-processed upper bound approximating the ground truth, serving as a reference point here. The experimental results indicate that our proposed model outperforms",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted an ablation study to evaluate the key novel modules, i.e., the global-local attention visual encoder (AVE) and utteranceaware video Q-Former (V-QF). As shown in Table  2 , removing either of these modules decreases the overall performance, highlighting their effectiveness and reasonability in the architecture.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We propose MicroEmo that directs attention to the local facial micro-expression dynamics and fully captures multi-scale and contextual dependencies of utterances in videos, and tuning on EMER. MicroEmo demonstrates capabilities in capturing subtle emotion to output richer and more reliable results without compromising contextual accuracy. In the future, we will apply the utterance-aware video Q-Former to acoustic modality and integrate emotion in audio into multimodal emotion recognition task based on MLLM.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Given an input video,",
      "page": 2
    },
    {
      "caption": "Figure 1: Given an input video, we use a pre-trained",
      "page": 2
    },
    {
      "caption": "Figure 1: ), which combines utterance-level timestamps to design a",
      "page": 2
    },
    {
      "caption": "Figure 1: The overall architecture of MicroEmo. Input a sequence of video frames along with their timestamps, (a) Global-local",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Japan": "liyun.zhang@lab.ime.cmc.osaka-u.ac.jp"
        },
        {
          "Japan": "ABSTRACT"
        },
        {
          "Japan": "Multimodal Large Language Models (MLLMs) have demonstrated"
        },
        {
          "Japan": "remarkable multimodal emotion recognition capabilities, integrat-"
        },
        {
          "Japan": "ing multimodal cues from visual, acoustic, and linguistic contexts"
        },
        {
          "Japan": "in the video to recognize human emotional states. However, ex-"
        },
        {
          "Japan": "isting methods ignore capturing local facial features of temporal"
        },
        {
          "Japan": "dynamics of micro-expressions and do not\nleverage the contex-"
        },
        {
          "Japan": "tual dependencies of the utterance-aware temporal segments in"
        },
        {
          "Japan": "the video, thereby limiting their expected effectiveness to a certain"
        },
        {
          "Japan": "extent. In this work, we propose MicroEmo, a time-sensitive MLLM"
        },
        {
          "Japan": "aimed at directing attention to the local facial micro-expression dy-"
        },
        {
          "Japan": "namics and the contextual dependencies of utterance-aware video"
        },
        {
          "Japan": "clips. Our model incorporates two key architectural contributions:"
        },
        {
          "Japan": "(1) a global-local attention visual encoder that integrates global"
        },
        {
          "Japan": "frame-level timestamp-bound image features with local facial fea-"
        },
        {
          "Japan": "tures of temporal dynamics of micro-expressions; (2) an utterance-"
        },
        {
          "Japan": "aware video Q-Former that captures multi-scale and contextual"
        },
        {
          "Japan": "dependencies by generating visual token sequences for each ut-"
        },
        {
          "Japan": "terance segment and for the entire video then combining them."
        },
        {
          "Japan": "Preliminary qualitative experiments demonstrate that\nin a new"
        },
        {
          "Japan": "Explainable Multimodal Emotion Recognition (EMER)\ntask that"
        },
        {
          "Japan": "exploits multi-modal and multi-faceted clues to predict emotions"
        },
        {
          "Japan": "in an open-vocabulary (OV) manner, MicroEmo demonstrates its"
        },
        {
          "Japan": "effectiveness compared with the latest methods."
        },
        {
          "Japan": ""
        },
        {
          "Japan": ""
        },
        {
          "Japan": "CCS CONCEPTS"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "‚Ä¢ Computing methodologies ‚Üí Neural networks; ‚Ä¢ Artifcial"
        },
        {
          "Japan": "intelligence; ‚Ä¢ Computer vision; ‚Ä¢ Natural language process-"
        },
        {
          "Japan": "ing;"
        },
        {
          "Japan": ""
        },
        {
          "Japan": ""
        },
        {
          "Japan": "KEYWORDS"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "Multimodal emotion recognition, Multimodal Large language mod-"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "els (MLLMs), Micro-expressions, Time-sensitive, Video Q-Former,"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "Open-vocabulary emotion recognition, Global-local attention visual"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "encoder, Utterance-aware."
        },
        {
          "Japan": ""
        },
        {
          "Japan": ""
        },
        {
          "Japan": ""
        },
        {
          "Japan": "Permission to make digital or hard copies of all or part of this work for personal or"
        },
        {
          "Japan": "classroom use is granted without fee provided that copies are not made or distributed"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "for profit or commercial advantage and that copies bear this notice and the full citation"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "on the first page. Copyrights for components of this work owned by others than the"
        },
        {
          "Japan": "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or"
        },
        {
          "Japan": "republish, to post on servers or to redistribute to lists, requires prior specific permission"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "and/or a fee. Request permissions from permissions@acm.org."
        },
        {
          "Japan": ""
        },
        {
          "Japan": "ACM MM, 2024, Melbourne, Australia"
        },
        {
          "Japan": "¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM."
        },
        {
          "Japan": "ACM ISBN 978-x-xxxx-xxxx-x/YY/MM"
        },
        {
          "Japan": ""
        },
        {
          "Japan": "https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "into the LLM for result generation. Our contributions to this paper"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "are listed as follows:"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "‚Ä¢ We propose a novel global-local attention visual encoder,"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "which integrates global frame-level timestamp-bound image"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "features with local facial features of temporal dynamics of"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "micro-expressions. This facilitates the model to capture more"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "subtle and deeper emotional\ninformation to output richer"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "and more reliable results."
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "‚Ä¢ We propose a novel utterance-aware video Q-Former, which"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "captures multi-scale and contextual dependencies by gener-"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "ating visual token sequences for each utterance segment and"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "for the entire video then combining them. This ensures the"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "accuracy and consistency of contextual dependencies when"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "the model provides detailed recognition of subtle emotional"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "changes."
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "‚Ä¢ We conduct preliminary qualitative experiments to demon-"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "strate the effectiveness of MicroEmo compared with the lat-"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "est methods in the Explainable Multimodal Emotion Recog-"
        },
        {
          "are concatenated with transcribed speech and instructions to feed": "nition (EMER) task."
        },
        {
          "are concatenated with transcribed speech and instructions to feed": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACM MM, 2024, Melbourne, Australia": "video Q-Former to obtain audio tokens, these multimodal tokens",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "3.1\nOverview"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "are concatenated with transcribed speech and instructions to feed",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "MicroEmo is mainly composed of a global-local attention visual"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "into the LLM for result generation. Our contributions to this paper",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "encoder, an utterance-aware video Q-Former for image, and a large"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "are listed as follows:",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "language model, as depicted in Figure 1. Given an input video,"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "the global-local attention visual encoder first extracts and inte-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "‚Ä¢ We propose a novel global-local attention visual encoder,",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "grates global\nframe-level timestamp-bound image features with"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "which integrates global frame-level timestamp-bound image",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "local\nfacial\nfeatures of temporal dynamics of micro-expressions."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "features with local facial features of temporal dynamics of",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "Next, the utterance-aware video Q-Former models each utterance"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "micro-expressions. This facilitates the model to capture more",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "time segment feature via an utterance-aware sliding window and"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "subtle and deeper emotional\ninformation to output richer",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "combines it with whole features to produce multi-scale fused tokens"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "and more reliable results.",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "for the image. Audio is also passed through a pre-trained encoder"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "‚Ä¢ We propose a novel utterance-aware video Q-Former, which",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "and whole video Q-Former to obtain audio tokens. Finally, these"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "captures multi-scale and contextual dependencies by gener-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "multimodal tokens are concatenated with transcribed speech and"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ating visual token sequences for each utterance segment and",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "instructions, which are then fed into the LLM to generate responses."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "for the entire video then combining them. This ensures the",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "accuracy and consistency of contextual dependencies when",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "the model provides detailed recognition of subtle emotional",
          "Liyun Zhang": "3.2\nGlobal-Local Attention Visual Encoder"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "changes.",
          "Liyun Zhang": "First, for the global frame-level features, we use the time-aware"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "‚Ä¢ We conduct preliminary qualitative experiments to demon-",
          "Liyun Zhang": "frame encoder inspired by InstructBLIP [3] and Timechat [24] as"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "strate the effectiveness of MicroEmo compared with the lat-",
          "Liyun Zhang": "shown in Figure 1. Given an input video, we use a pre-trained"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "est methods in the Explainable Multimodal Emotion Recog-",
          "Liyun Zhang": "ViT [26] to encode each frame and obtain frame features. Subse-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "nition (EMER) task.",
          "Liyun Zhang": "quently, we use the keypoint indices of the MediaPipe [20] face"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "mesh via extracting landmarks of each frame to define different"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "facial regions as micro-expression regions, which are predefined"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "2\nRELATED WORKS",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "on facial anatomy. Then, the patches are combined to create masks"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Multimodal Large Language Model (MLLM) has attracted much",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "associated with micro-expression regions. we implemented a local-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "attention due to its adaptability to adapt to different tasks by tuning",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "ized micro-expression attention mechanism based on the generated"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "on specific datasets, e.g., tuning on EMER [16] for multimodal emo-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "mask. This allows each patch to selectively attend to pertinent"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "tion recognition. Numerous studies have endeavored to integrate",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "micro-expression regions, facilitating the extraction of fine-grained"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "LLMs with a video encoder for video understanding [18, 21, 24, 25].",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "facial\nlocal features. By emphasizing spatially relevant areas, we"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "They typically employ open-source LLMs [1, 28] to encode the video",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "enhance the model‚Äôs capacity to discern subtle facial cues and micro-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "into vision tokens compatible with the LLMs. Such as VideoChat",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "expressions, which are crucial for accurate emotion recognition."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[12] uses a video transformer to encode video features and then",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "Furthermore, the image Q-Former compresses the frame tokens."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "implements a Query Transformer (Q-Former) [11] for video tokens.",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "The Q-Former takes learnable queries as input, which interacts with"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Video-llama [33] uses a vision transformer (ViT) [4] with an image",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "the frame features and facial local features via cross-attention and"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Q-Former to encode frames and employs a video Q-Former for",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "updates the initial queries respectively. The global and local results"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "temporal modeling. TimeChat [24] incorporates frame timestamps",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "are fused and further processed to obtain final visual tokens [11]."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "to make the model time-sensitive and uses a video Q-Former to",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "During visual token extraction, we add the frame-level timestamp,"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "establish inter-frame temporal dependencies.",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "e.g., \"This\nframe\nis\nsampled\nat\n0.0s.\", as a condition to the"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "However, these methods ignore capturing local features of tem-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "Q-Former to fuse the visual and timestamp information for binding"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "poral dynamics and do not leverage the contextual dependencies of",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "temporal dynamics to the visual tokens."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "important segments in videos, thereby limiting their expected effec-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "The proposed global-local attention visual encoder can fully"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "tiveness to a certain extent. In contrast, our MicroEmo proposes a",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "integrate global timestamp-bound frame features with local facial"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "global-local attention visual encoder and an utterance-aware video",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "features of\ntemporal dynamics of micro-expressions. Benefiting"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Q-Former to capture subtle local features of temporal dynamics",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "from the attention to temporal dynamics of micro-expressions of the"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "and contextual dependencies in videos, such as micro-expressions",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "videos, MicroEmo shows its superiority in capturing more subtle"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "and utterance segment context of emotion recognition, enabling",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "and deeper emotional\ninformation to facilitate richer and more"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "the provision of subtle reasoning results without compromising",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "reliable emotion recognition results."
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "contextual accuracy.",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "3.3\nUtterance-Aware Video Q-Former"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "3\nMETHODOLOGY",
          "Liyun Zhang": "Since frames are encoded independently, we can use a sliding video"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "We present MicroEmo, a time-sensitive MLLM featuring two novel",
          "Liyun Zhang": "Q-Former to model temporal relationships across frames and en-"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "modules: a global-local attention visual encoder and an utterance-",
          "Liyun Zhang": "hance the feature fusion in the temporal dimension [24]. Here, we"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "aware video Q-Former. These modules enable MicroEmo to capture",
          "Liyun Zhang": "propose a novel utterance-aware video Q-Former (yellow block in"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "more subtle and deeper emotional information in videos. We verify",
          "Liyun Zhang": "Figure 1), which combines utterance-level timestamps to design a"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "the effectiveness of our model on the new task EMER and the new",
          "Liyun Zhang": "dynamic sliding window with a length of each utterance segment"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "style dataset EMER-Fine [16] it pioneered.",
          "Liyun Zhang": "and within each window utilizing the video Q-Former extract video"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": "region masks"
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": "Q-Former"
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        },
        {
          "Utterance-aware Video Q-Former": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "a pre-trained encoder and whole video Q-Former to obtain audio tokens. Finally, these multimodal tokens are concatenated"
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "with transcribed speech and instructions, which are then fed into a (c) Large Language Model to generate responses."
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "tokens from frame features. Meanwhile, we also extract video to-"
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "kens from whole frame features via a global video Q-Former and"
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "then combine them to produce multi-scale fused tokens."
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "It is crucial to capture the context of each conversation segment,"
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "which is the key to the model‚Äôs understanding of emotional fluctu-"
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "ations and changes. We use this way to fully capture multi-scale"
        },
        {
          "with whole tokens extracted by a global video Q-Former to produce multi-scale fused visual tokens. Audio is also passed through": "and contextual dependencies, maintaining the accuracy and consis-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "LLM are frozen, while those of image Q-Former, micro-expression"
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "attention module, video Q-Former, and linear layer (not shown in"
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "the diagram due to space constraints) are tuned."
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": ""
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "4.2\nExperiment Setup"
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": ""
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "Tasks, Datasets and Evaluation Metrics We evaluate our model"
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": ""
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "on a new Explainable Multimodal Emotion Recognition (EMER) task"
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": ""
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "[16], test-set of EMER-Fine as the test dataset, the new ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ùë†"
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": ""
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "in EMER as metrics, and MLLM such as VideoChat\nand ùëÖùëíùëêùëéùëôùëôùë†"
        },
        {
          "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and": "[12], Video-LLaMA [33] and Chat-UniVi [9], etc. as baselines for"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "3.4\nLarge Language Model",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "We concatenate inputs from different modalities,\ni.e.,\nthe video",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "including",
          "Liyun Zhang": "RecallùëÜ"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "tokens ùëáùëâ , audio tokens ùëáùê¥, and text query tokens ùëáùêøùëû",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "transcribed speech and instruction. They are fed into a large lan-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "guage model to generate reasonable and expected responses ùëáùêøùëé",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "30.35"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "They have the same token embedding dimension. The training of",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "38.56"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "the MLLM typically utilizes a two-stage training framework. The",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "44.19"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "first stage pre-trains the model using large-scale image-audio-text",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "48.38"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "pairs for vision-audio-language mapping [14, 19, 23, 30]. The sec-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ond stage finetunes the model with instruction data for instruction",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "following. Considering computing efficiency, we reuse the check-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "33.22"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "points of the existing open-source models after the first stage of",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "51.19"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "training, e.g., the vision-language and audio-language branches",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "42.50"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "from [33], conducting only instruction tuning. During the training",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "40.13"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "procedure, we utilize the language modeling loss for generating",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "38.42"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "target answers ùëáùêøùëé",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "48.14"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "as the objective function:",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "50.11"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "49.34"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "(1)",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "ùêø = ‚àíùëôùëúùëîùëÉŒò (ùëáùêøùëé |ùëáùëâ ,ùëáùê¥,ùëáùêøùëû )",
          "Liyun Zhang": "42.25"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "To better adapt the LLM to multimodal video tasks, we apply the",
          "Liyun Zhang": "52.11"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "parameter-efficient fine-tuning method, i.e., LoRA [8] to train.",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "70.90"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "4\nEXPERIMENTS",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "65.93"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "4.1\nImplementation Details",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "65.26"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "We use ViT-G/14 from EVA-CLIP [26] as the image encoder, Im-",
          "Liyun Zhang": "69.37"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "agebind [5] as the audio encoder, and LLaMA-2 (7B) [29] as the",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "75.03"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "language foundation model. The parameters of the image Q-Former",
          "Liyun Zhang": "69.44"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "are initialized from InstructBLIP‚Äôs checkpoint, while the video Q-",
          "Liyun Zhang": "61.46"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Former for vision and audio is initialized from Video-LLaMA‚Äôs",
          "Liyun Zhang": "68.59"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "checkpoint. We finetune our MicroEmo on train-set of EMER-Fine",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "77.70"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[16] dataset\nfor 3 epochs, using a batch size of 1, with a single",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "NVIDIA A40 (49G) machine. The parameters of ViT, Imagebind and",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "LLM are frozen, while those of image Q-Former, micro-expression",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "attention module, video Q-Former, and linear layer (not shown in",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "the diagram due to space constraints) are tuned.",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "RecallùëÜ"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "4.2\nExperiment Setup",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "68.59"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "Tasks, Datasets and Evaluation Metrics We evaluate our model",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "59.49"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "on a new Explainable Multimodal Emotion Recognition (EMER) task",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "58.71"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[16], test-set of EMER-Fine as the test dataset, the new ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ùë†",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": "47.98"
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "in EMER as metrics, and MLLM such as VideoChat\nand ùëÖùëíùëêùëéùëôùëôùë†",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "[12], Video-LLaMA [33] and Chat-UniVi [9], etc. as baselines for",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "evaluation.",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "4.3\nMain Results",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "We evaluate our MicroEmo and some existing MLLMs for EMER",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "task as shown in Table 1, where, the baseline methods except for",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "AffectGPT [16] are zero-shot performance results. AffectGPT is",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "only tuned on the EMER-Fine [16] dataset\nin the second stage",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "without pre-training of the first stage, i.e., pre-training the model",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "using large-scale image-audio-text pairs for vision-audio-language",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "mapping, which is the same as our experimental setting. EMER",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "(Multi) [16] can be viewed as a post-processed upper bound approx-",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "imating the ground truth, serving as a reference point here. The",
          "Liyun Zhang": ""
        },
        {
          "ACM MM, 2024, Melbourne, Australia": "experimental results indicate that our proposed model outperforms",
          "Liyun Zhang": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[18] Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang,\n5\nCONCLUSION"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Ran He, and Hongxia Yang. 2023. Video-Teller: Enhancing Cross-Modal Genera-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "We propose MicroEmo that directs attention to the local\nfacial"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "tion with Fusion and Decoupling.\n(Oct 2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[19] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu\nmicro-expression dynamics and fully captures multi-scale and con-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Sun, and Lu Hou. 2024. Fetv: A benchmark for fine-grained evaluation of open-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "textual dependencies of utterances in videos, and tuning on EMER."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "domain text-to-video generation. Advances in Neural\nInformation Processing"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "MicroEmo demonstrates capabilities in capturing subtle emotion to"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Systems 36 (2024)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[20] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja,\noutput richer and more reliable results without compromising con-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "MichaelD. Hays, Fan Zhang, C.K. Chang, Ming Yong,\nJuhyun Lee, Wan-Teh"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "textual accuracy. In the future, we will apply the utterance-aware"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Chang, Hua Wang, Manfred Georg, and Matthias Grundmann. 2019. MediaPipe:"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "video Q-Former to acoustic modality and integrate emotion in audio\nA Framework for Building Perception Pipelines. Cornell University - arXiv,Cornell"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "University - arXiv (Jun 2019).\ninto multimodal emotion recognition task based on MLLM."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[21] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Tao Wang, and Zhongyu Wei. 2023. Valley: Video Assistant with Large Language"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "model Enhanced abilitY.\n(Jun 2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "REFERENCES\n2023. Video-chatgpt: Towards detailed video understanding via large vision and"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[1] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nlanguage models. arXiv preprint arXiv:2306.05424 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,\n[23]\nShuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, and Xu Sun. 2022. Delv-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with\ning into the Openness of CLIP.\n(Jun 2022)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "90%* ChatGPT Quality.\nhttps://lmsys.org/blog/2023-03-30-vicuna/\n[24]\nShuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. 2024. Timechat: A"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan,\ntime-sensitive multimodal large language model for long video understanding. In\n[2] Yunfei Chu,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "understanding via unified large-scale audio-language models.\n14313‚Äì14323.\narXiv preprint"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[25]\nEnxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou,\narXiv:2311.07919 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang.\n[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.\nInstructBLIP: Towards\n2023. 6,MovieChat: From Dense Token to Sparse Memory for Long Video Under-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "standing.\n(Jul 2023).\nGeneral-purpose Vision-Language Models with Instruction Tuning. In Thirty-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "https://openreview.\n[26] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023. Eva-clip:\nseventh Conference on Neural Information Processing Systems."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "net/forum?id=vvoWPYqZJA\nImproved training techniques for clip at scale. arXiv preprint arXiv:2303.15389"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\n(2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\n[27] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. 2020.\nAn Image\nZejun Ma, and Chao Zhang. 2023. Salmonn: Towards generic hearing abilities"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "is Worth 16x16 Words: Transformers for Image Recognition at Scale.\narXiv:\nfor large language models. arXiv preprint arXiv:2310.13289 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nComputer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nRecognition (Oct 2020)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[5] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, KalyanVasudev\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Alwala, Armand Joulin, and Ishan Misra. 2023. ImageBind: One Embedding Space\npreprint arXiv:2302.13971 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "To Bind Them All.\n(May 2023).\n[29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[6]\nJiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. 2024. Onellm: One framework to\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "align all modalities with language. In Proceedings of the IEEE/CVF Conference on\npreprint arXiv:2307.09288 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[30] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan,\nComputer Vision and Pattern Recognition. 26584‚Äì26595."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP:\n[7] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. 2024. Vtimellm:"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference\nContrastive Pre-training for Zero-shot Video-Text Understanding. In Proceedings"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "on Computer Vision and Pattern Recognition. 14271‚Äì14280.\nof\nthe 2021 Conference on Empirical Methods in Natural Language Processing."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[8] HuEdward J., Yulong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nhttps://doi.org/10.18653/v1/2021.emnlp-main.544"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language\n[31]\nYaoxun Xu, Hangting Chen, Jianwei Yu, Qiaochu Huang, Zhiyong Wu, Shi-Xiong"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Models.\nZhang, Guangzhi Li, Yi Luo, and Rongzhi Gu. 2024.\nSecap: Speech emotion\narXiv: Computation and Language,arXiv: Computation and Language"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "(Jun 2021).\ncaptioning with large language model. In Proceedings of the AAAI Conference on"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. 2024.\n[9]\nArtificial Intelligence, Vol. 38. 19323‚Äì19331."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Chat-univi: Unified visual representation empowers large language models with\n[32] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modular-\nimage and video understanding. In Proceedings of the IEEE/CVF Conference on"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "ization empowers large language models with multimodality.\nComputer Vision and Pattern Recognition. 13700‚Äì13710.\narXiv preprint"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[10] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei\narXiv:2304.14178 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Liu. 2023. Otter: A Multi-Modal Model with In-Context\nInstruction Tuning.\nVideo-llama: An instruction-\n[33] Hang Zhang, Xin Li, and Lidong Bing. 2023."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "arXiv:2305.03726 [cs.CV] https://arxiv.org/abs/2305.03726\ntuned audio-visual\nlanguage model\nfor video understanding.\narXiv preprint"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[11]\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping\narXiv:2306.02858 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "language-image pre-training with frozen image encoders and large language"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "models. In International conference on machine learning. PMLR, 19730‚Äì19742."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[18] Haogeng Liu, Qihang Fan, Tingkai Liu, Linjie Yang, Yunzhe Tao, Huaibo Huang,\n5\nCONCLUSION"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Ran He, and Hongxia Yang. 2023. Video-Teller: Enhancing Cross-Modal Genera-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "We propose MicroEmo that directs attention to the local\nfacial"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "tion with Fusion and Decoupling.\n(Oct 2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[19] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu\nmicro-expression dynamics and fully captures multi-scale and con-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Sun, and Lu Hou. 2024. Fetv: A benchmark for fine-grained evaluation of open-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "textual dependencies of utterances in videos, and tuning on EMER."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "domain text-to-video generation. Advances in Neural\nInformation Processing"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "MicroEmo demonstrates capabilities in capturing subtle emotion to"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Systems 36 (2024)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[20] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja,\noutput richer and more reliable results without compromising con-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "MichaelD. Hays, Fan Zhang, C.K. Chang, Ming Yong,\nJuhyun Lee, Wan-Teh"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "textual accuracy. In the future, we will apply the utterance-aware"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Chang, Hua Wang, Manfred Georg, and Matthias Grundmann. 2019. MediaPipe:"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "video Q-Former to acoustic modality and integrate emotion in audio\nA Framework for Building Perception Pipelines. Cornell University - arXiv,Cornell"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "University - arXiv (Jun 2019).\ninto multimodal emotion recognition task based on MLLM."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[21] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Tao Wang, and Zhongyu Wei. 2023. Valley: Video Assistant with Large Language"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "model Enhanced abilitY.\n(Jun 2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "REFERENCES\n2023. Video-chatgpt: Towards detailed video understanding via large vision and"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[1] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nlanguage models. arXiv preprint arXiv:2306.05424 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,\n[23]\nShuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, and Xu Sun. 2022. Delv-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with\ning into the Openness of CLIP.\n(Jun 2022)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "90%* ChatGPT Quality.\nhttps://lmsys.org/blog/2023-03-30-vicuna/\n[24]\nShuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. 2024. Timechat: A"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan,\ntime-sensitive multimodal large language model for long video understanding. In\n[2] Yunfei Chu,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "understanding via unified large-scale audio-language models.\n14313‚Äì14323.\narXiv preprint"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[25]\nEnxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou,\narXiv:2311.07919 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang.\n[3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023.\nInstructBLIP: Towards\n2023. 6,MovieChat: From Dense Token to Sparse Memory for Long Video Under-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "standing.\n(Jul 2023).\nGeneral-purpose Vision-Language Models with Instruction Tuning. In Thirty-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "https://openreview.\n[26] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. 2023. Eva-clip:\nseventh Conference on Neural Information Processing Systems."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "net/forum?id=vvoWPYqZJA\nImproved training techniques for clip at scale. arXiv preprint arXiv:2303.15389"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\n(2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\n[27] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Heigold, Sylvain Gelly,\nJakob Uszkoreit, and Neil Houlsby. 2020.\nAn Image\nZejun Ma, and Chao Zhang. 2023. Salmonn: Towards generic hearing abilities"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "is Worth 16x16 Words: Transformers for Image Recognition at Scale.\narXiv:\nfor large language models. arXiv preprint arXiv:2310.13289 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nComputer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nRecognition (Oct 2020)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[5] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, KalyanVasudev\nAzhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Alwala, Armand Joulin, and Ishan Misra. 2023. ImageBind: One Embedding Space\npreprint arXiv:2302.13971 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "To Bind Them All.\n(May 2023).\n[29] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[6]\nJiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. 2024. Onellm: One framework to\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "align all modalities with language. In Proceedings of the IEEE/CVF Conference on\npreprint arXiv:2307.09288 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[30] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan,\nComputer Vision and Pattern Recognition. 26584‚Äì26595."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. 2021. VideoCLIP:\n[7] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. 2024. Vtimellm:"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Empower llm to grasp video moments. In Proceedings of the IEEE/CVF Conference\nContrastive Pre-training for Zero-shot Video-Text Understanding. In Proceedings"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "on Computer Vision and Pattern Recognition. 14271‚Äì14280.\nof\nthe 2021 Conference on Empirical Methods in Natural Language Processing."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[8] HuEdward J., Yulong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nhttps://doi.org/10.18653/v1/2021.emnlp-main.544"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language\n[31]\nYaoxun Xu, Hangting Chen, Jianwei Yu, Qiaochu Huang, Zhiyong Wu, Shi-Xiong"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Models.\nZhang, Guangzhi Li, Yi Luo, and Rongzhi Gu. 2024.\nSecap: Speech emotion\narXiv: Computation and Language,arXiv: Computation and Language"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "(Jun 2021).\ncaptioning with large language model. In Proceedings of the AAAI Conference on"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. 2024.\n[9]\nArtificial Intelligence, Vol. 38. 19323‚Äì19331."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Chat-univi: Unified visual representation empowers large language models with\n[32] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023. mplug-owl: Modular-\nimage and video understanding. In Proceedings of the IEEE/CVF Conference on"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "ization empowers large language models with multimodality.\nComputer Vision and Pattern Recognition. 13700‚Äì13710.\narXiv preprint"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[10] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei\narXiv:2304.14178 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Liu. 2023. Otter: A Multi-Modal Model with In-Context\nInstruction Tuning.\nVideo-llama: An instruction-\n[33] Hang Zhang, Xin Li, and Lidong Bing. 2023."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "arXiv:2305.03726 [cs.CV] https://arxiv.org/abs/2305.03726\ntuned audio-visual\nlanguage model\nfor video understanding.\narXiv preprint"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[11]\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping\narXiv:2306.02858 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "language-image pre-training with frozen image encoders and large language"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "models. In International conference on machine learning. PMLR, 19730‚Äì19742."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[12] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Limin Wang, and Yu Qiao. 2023. Videochat: Chat-centric video understanding."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "arXiv preprint arXiv:2305.06355 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[13] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Xu, Guo Chen, Ping Luo, et al. 2024. Mvbench: A comprehensive multi-modal"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "video understanding benchmark. In Proceedings of the IEEE/CVF Conference on"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Computer Vision and Pattern Recognition. 22195‚Äì22206."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[14]\nShicheng Li, Lei Li, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun,"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "and Lu Hou. 2023.\nVITATECS: A Diagnostic Dataset\nfor Temporal Concept"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Understanding of Video-Language Models.\n(Nov 2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[15]\nYanwei Li, Chengyao Wang, and Jiaya Jia. 2023. Llama-vid: An image is worth 2"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "tokens in large language models. arXiv preprint arXiv:2311.17043 (2023)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[16] Zheng Lian, Haiyang Sun, Licai Sun,\nJiangyan Yi, Bin Liu, and Jianhua Tao."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "2024. AffectGPT: Dataset and Framework for Explainable Multimodal Emotion"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Recognition. arXiv preprint arXiv:2407.07653 (2024)."
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "[17] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. 2023. Video-llava:"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "Learning united visual representation by alignment before projection.\narXiv"
        },
        {
          "MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues\nACM MM, 2024, Melbourne, Australia": "preprint arXiv:2311.10122 (2023)."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez",
        "Ion Stoica",
        "Eric Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality"
    },
    {
      "citation_id": "2",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "3",
      "title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning",
      "authors": [
        "Wenliang Dai",
        "Junnan Li",
        "Dongxu Li",
        "Anthony Tiong",
        "Junqi Zhao",
        "Weisheng Wang",
        "Boyang Li",
        "Pascale Fung",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "Thirtyseventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2020",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "ImageBind: One Embedding Space To Bind Them All",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyanvasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": "2023",
      "venue": "ImageBind: One Embedding Space To Bind Them All"
    },
    {
      "citation_id": "6",
      "title": "Onellm: One framework to align all modalities with language",
      "authors": [
        "Jiaming Han",
        "Kaixiong Gong",
        "Yiyuan Zhang",
        "Jiaqi Wang",
        "Kaipeng Zhang",
        "Dahua Lin",
        "Yu Qiao",
        "Peng Gao",
        "Xiangyu Yue"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Vtimellm: Empower llm to grasp video moments",
      "authors": [
        "Bin Huang",
        "Xin Wang",
        "Hong Chen",
        "Zihan Song",
        "Wenwu Zhu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models. arXiv: Computation and Language,arXiv: Computation and Language",
      "authors": [
        "J Huedward",
        "Yulong Shen",
        "Phillip Wallis",
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li",
        "Shean Wang",
        "Weizhu Chen"
      ],
      "year": "2021",
      "venue": "LoRA: Low-Rank Adaptation of Large Language Models. arXiv: Computation and Language,arXiv: Computation and Language"
    },
    {
      "citation_id": "9",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Wancai Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Jingkang Yang",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Otter: A Multi-Modal Model with In-Context Instruction Tuning",
      "arxiv": "arXiv:2305.03726[cs.CV"
    },
    {
      "citation_id": "11",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "12",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "13",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models",
      "authors": [
        "Shicheng Li",
        "Lei Li",
        "Shuhuai Ren",
        "Yuanxin Liu",
        "Yi Liu",
        "Rundong Gao",
        "Xu Sun",
        "Lu Hou"
      ],
      "year": "2023",
      "venue": "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models"
    },
    {
      "citation_id": "15",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Yanwei Li",
        "Chengyao Wang",
        "Jiaya Jia"
      ],
      "year": "2023",
      "venue": "Llama-vid: An image is worth 2 tokens in large language models",
      "arxiv": "arXiv:2311.17043"
    },
    {
      "citation_id": "16",
      "title": "AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition",
      "arxiv": "arXiv:2407.07653"
    },
    {
      "citation_id": "17",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "18",
      "title": "Ran He, and Hongxia Yang. 2023. Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling",
      "authors": [
        "Haogeng Liu",
        "Qihang Fan",
        "Tingkai Liu",
        "Linjie Yang",
        "Yunzhe Tao",
        "Huaibo Huang"
      ],
      "year": "2023",
      "venue": "Ran He, and Hongxia Yang. 2023. Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling"
    },
    {
      "citation_id": "19",
      "title": "Fetv: A benchmark for fine-grained evaluation of opendomain text-to-video generation",
      "authors": [
        "Yuanxin Liu",
        "Lei Li",
        "Shuhuai Ren",
        "Rundong Gao",
        "Shicheng Li",
        "Sishuo Chen",
        "Xu Sun",
        "Lu Hou"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "MediaPipe: A Framework for Building Perception Pipelines",
      "authors": [
        "Camillo Lugaresi",
        "Jiuqiang Tang",
        "Hadon Nash",
        "Chris Mcclanahan",
        "Esha Uboweja",
        "Michaeld",
        "Fan Hays",
        "C Zhang",
        "Ming Chang",
        "Juhyun Yong",
        "Wan-Teh Lee",
        "Hua Chang",
        "Manfred Wang",
        "Matthias Georg",
        "Grundmann"
      ],
      "year": "2019",
      "venue": "MediaPipe: A Framework for Building Perception Pipelines"
    },
    {
      "citation_id": "21",
      "title": "Valley: Video Assistant with Large Language model Enhanced abilitY",
      "authors": [
        "Ruipu Luo",
        "Ziwang Zhao",
        "Min Yang",
        "Junwei Dong",
        "Minghui Qiu",
        "Pengcheng Lu",
        "Tao Wang",
        "Zhongyu Wei"
      ],
      "year": "2023",
      "venue": "Valley: Video Assistant with Large Language model Enhanced abilitY"
    },
    {
      "citation_id": "22",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "23",
      "title": "Delving into the Openness of CLIP",
      "authors": [
        "Lei Shuhuai Ren",
        "Xuancheng Li",
        "Guangxiang Ren",
        "Xu Zhao",
        "Sun"
      ],
      "year": "2022",
      "venue": "Delving into the Openness of CLIP"
    },
    {
      "citation_id": "24",
      "title": "Timechat: A time-sensitive multimodal large language model for long video understanding",
      "authors": [
        "Linli Shuhuai Ren",
        "Shicheng Yao",
        "Xu Li",
        "Lu Sun",
        "Hou"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding",
      "authors": [
        "Enxin Song",
        "Wenhao Chai",
        "Guanhong Wang",
        "Yucheng Zhang",
        "Haoyang Zhou",
        "Feiyang Wu",
        "Xun Guo",
        "Tian Ye",
        "Yan Lu",
        "Jenq-Neng Hwang",
        "Gaoang Wang"
      ],
      "year": "2023",
      "venue": "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding"
    },
    {
      "citation_id": "26",
      "title": "Eva-clip: Improved training techniques for clip at scale",
      "authors": [
        "Quan Sun",
        "Yuxin Fang",
        "Ledell Wu",
        "Xinlong Wang",
        "Yue Cao"
      ],
      "year": "2023",
      "venue": "Eva-clip: Improved training techniques for clip at scale",
      "arxiv": "arXiv:2303.15389"
    },
    {
      "citation_id": "27",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "28",
      "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timoth√©e Lacroix",
        "Baptiste Rozi√®re",
        "Naman Goyal",
        "Eric Hambro"
      ],
      "year": "2023",
      "venue": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "29",
      "title": "Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava"
      ],
      "year": "2023",
      "venue": "Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "30",
      "title": "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding",
      "authors": [
        "Hu Xu",
        "Gargi Ghosh",
        "Po-Yao Huang",
        "Dmytro Okhonko",
        "Armen Aghajanyan",
        "Florian Metze",
        "Luke Zettlemoyer",
        "Christoph Feichtenhofer"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.544"
    },
    {
      "citation_id": "31",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Yaoxun Xu",
        "Hangting Chen",
        "Jianwei Yu",
        "Qiaochu Huang",
        "Zhiyong Wu",
        "Shi-Xiong Zhang",
        "Guangzhi Li",
        "Yi Luo",
        "Rongzhi Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "33",
      "title": "Video-llama: An instructiontuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instructiontuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    }
  ]
}