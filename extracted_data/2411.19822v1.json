{
  "paper_id": "2411.19822v1",
  "title": "Sdr-Gnn: Spectral Domain Reconstruction Graph Neural Network For Incomplete Multimodal Learning In Conversational Emotion Recognition",
  "published": "2024-11-29T16:31:50Z",
  "authors": [
    "Fangze Fu",
    "Wei Ai",
    "Fan Yang",
    "Yuntao Shou",
    "Tao Meng",
    "Keqin Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal Emotion Recognition in Conversations (MERC) aims to classify utterance emotions using textual, auditory, and visual modal features. Most existing MERC methods assume each utterance has complete modalities, overlooking the common issue of incomplete modalities in real-world scenarios. Recently, graph neural networks (GNNs) have achieved notable results in Incomplete Multimodal Emotion Recognition in Conversations (IMERC). However, traditional GNNs focus on binary relationships between nodes, limiting their ability to capture more complex, higher-order information. Moreover, repeated message passing can cause over-smoothing, reducing their capacity to preserve essential high-frequency details. To address these issues, we propose a Spectral Domain Reconstruction Graph Neural Network (SDR-GNN) for incomplete multimodal learning in conversational emotion recognition. SDR-GNN constructs an utterance semantic interaction graph using a sliding window based on both speaker and context relationships to model emotional dependencies. To capture higher-order and high-frequency information, SDR-GNN utilizes weighted relationship aggregation, ensuring consistent semantic feature extraction across utterances. Additionally, it performs multifrequency aggregation in the spectral domain, enabling efficient recovery of incomplete modalities by extracting both high-and low-frequency information. Finally, multi-head attention is applied to fuse and optimize features for emotion recognition. Extensive experiments on various real-world datasets demonstrate that our approach is effective in incomplete multimodal learning and outperforms current state-of-the-art methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Emotion Recognition in Conversations (ME RC)  [67, 46, 41, 45, 48, 49]  aims to identify the emotions expressed by each multimodal utterance in conversation scenes. Unlike traditional Unimodal Emotion Recognition in Conversations (UERC)  [31, 28, 47, 6, 5] , MERC can use textual, auditory, and visual modal information from the utterance to reveal more realistic emotions of the speaker by capturing the consistency and complementary semantics within and between modalities  [18, 61, 27, 43, 42, 37] . With the development of human-computer interaction, MERC has attracted significant attention from researchers because it can be widely used to understand and generate conversation  [15, 62, 39, 38] . However, most existing MERC methods usually assume that each utterance has complete modalities, ignoring the incomplete modality problem  [21, 16, 30, 50] . Unfortunately, obtaining complete multimodal data is incredibly challenging in practical conversation scenarios  [33] . For example, auditory data may not be available due to noise interference, visual data may not be available due to light or occlusion, and even more modal data may not be available due to sensor failure  [60, 7, 67] . Fig.  1  presents a sample conversation between two speakers, where each â‹† Our code is publicly available at https://github.com/fufangze/SDR-GNN.\n\n* Corresponding author ORCID(s): utterance contains three modalities. The conversation on the right side illustrates the condition when modalities are incomplete.\n\nThe problem of incomplete modalities poses significant challenges for MERC tasks. To this end, researchers have proposed various methods to solve this problem mainly from how to perform modal recovery  [33, 60, 53, 22] . For instance, Pham et al.  [33, 8, 29, 1]  proposed the MCTN model considering the semantic consistency between modalities.\n\nMCTN constructs cyclic transformations between modalities through sequence modeling to learn robust joint representations and uses cyclic consistency loss to achieve modality recovery. Wang et al.  [60]  considered the consistency of distribution between modalities and proposed the DiCMoR model. DiCMoR reduces the distribution gap by mapping different modalities to a latent space with Gaussian distribution and samples the characteristic distribution of the latent space to achieve modal recovery. Sun et al.  [53, 40, 3, 44, 51]  considered the consistency of long-range semantics between modalities and proposed the EMT-DLFR model. EMT-DLFR captures consistent semantics in the global dialogue context by building a multi-modal Transformer and achieves modality recovery through feature reconstruction. Lian et al.  [22]  considered the complex relationship between multi-modal utterances and proposed the GCNet model. GC-Net uses graph neural networks to model context and speaker relationships separately to capture consistent semantics for missing modality recovery. Although these methods show good performance, they still have some limitations:\n\n(i) Limitations in capturing higher-order information. In single-modal or multi-modal emotion recognition, the distribution of modalities in conversations is typically fixed. However, in conversations with incomplete modalities, the absence of modalities is often unpredictable. Models need to adapt to modalities absence of varying degrees and under different circumstances. While existing graph-based models, including GCNet  [22, 2, 4] , do capture higher-order information through information propagation, they rely on traditional graph structures, which are limited to binary relationships between nodes. These fixed structure graphs often struggle to capture complex semantic dependencies in conversations, especially when adapting to various missing modalities. MMIN  [68]  proposed six possible missingmodality conditions, but it can only learn for individual utterances. In contrast, our approach utilizes a hypergraph structure, which effectively models higher-order relationships among multiple nodes  [10] . This allows the model to capture more complex and nuanced dependencies, overcoming the limitations of conventional graphs that GNN-based models face. Therefore, how to capture the complex semantic dependencies between utterances, adapt to different situations, and optimize the recovery of incomplete modalities is an issue that cannot be ignored.\n\n(ii) Limitations in handling high-frequency information. Much research shows that high-frequency signals that reflect dissimilarity are as crucial as low-frequency signals that reflect consistency in MERC tasks  [21, 16] . Because the message propagation of GNN  [12]  has lowpass filtering characteristics, node representation is achieved by aggregating consistent low-frequency information in the neighborhood and suppressing differential high-frequency information. This inclination towards low-frequency components results in over-smoothing, where distinctive emotional transitions-the high-frequency signals-are suppressed, masking important intra-modal shifts. Regrettably, the constructed utterances-emotion interaction graphs often have semantic inconsistencies, and it is crucial to retain highfrequency information. Our proposed SDR-GNN addresses this by preserving and leveraging high-frequency information to capture rapid transitions and local changes, which are integral for comprehensive emotional analysis. Consequently, simultaneously retaining and fusing high-and lowfrequency information to guide the recovery of incomplete modalities is a challenge that must be overcome.\n\nInspired by the above analysis, this paper proposes a novel Spectral Domain Reconstruction Graph Neural Network for Incomplete Multimodal Learning in Conversational Emotion Recognition, named SDR-GNN. SDR-GNN can capture the complex emotional dependencies between utterances while learning multi-frequency information in multimodal features for incomplete modal recovery to obtain better emotion recognition results. Specifically, SDR-GNN first simulates the modal missing problem in real conversation scenarios by randomly discarding some modal features, and adding speaker information to the discourse features to form multimodal nodes. Subsequently, to model the complex semantic dependencies between multimodal utterances, SDR-GNN constructs the emotional interaction graph from the context and speaker relationships based on a sliding window, where the nodes in the sliding window are fully connected and construct the context and speaker hyperedges separately. Next, to capture the complex emotional dependence between far and near utterances and learn multifrequency information in multimodal features, SDR-GNN uses a neighborhood relationship awareness layer, a hyperedge relationship awareness layer, and a multi-frequency information awareness layer separately for information propagation. Finally, SDR-GNN reconstructs based on the learned features to guide the recovery of incomplete modalities and uses multi-head attention for feature fusion to achieve emotion recognition. We conducted experiments on three conversational datasets, verifying the effectiveness of our method. The experimental results demonstrate that our SDR-GNN outperforms existing approaches. The main contributions of this paper can be summarized as follows:\n\nâ€¢ Existing graph neural networks (GNNs) are constrained by their inherent limitations, which may lead to over-smoothing and the erasure of high-frequency signals, making it difficult to fully utilize multifrequency information. We have not only addressed this limitation but also applied our approach to multimodal emotion recognition under incomplete modalities, thereby filling the gap in current works.\n\nâ€¢ We propose a novel framework, SDR-GNN, to deal with incomplete conversational data in the MERC task, which jointly considers the higher-order information of modalities and multi-frequency features, and fully utilizes the semantic dependence in both speaker and context for missing modality recovery and emotion recognition.\n\nâ€¢ Experimental results on three benchmark datasets verify the effectiveness of our method. SDR-GNN outperforms existing state-of-the-art approaches in the domain of incomplete multimodal learning in conversational emotion recognition.",
      "page_start": 1,
      "page_end": 14
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Multimodal Emotion Recognition In Conversations",
      "text": "Multimodal Emotion Recognition in Conversations has gained significant attention in recent years due to its potential applications in various fields. Multimodal ERC leverages multiple data modalities, including text, audio, and visual data, to capture and analyze emotions more comprehensively during conversational exchanges.\n\nTo better utilize multimodal information to address the ERC problem, researchers have proposed various methods. MulT  [55]  model uses cross-modal transformers to capture long-range dependencies. MMGCN  [21]  constructs a comprehensive graph to handle multimodal and extensive contextual information, and includes speaker embeddings to encode speaker-specific details. M2FNet  [17] , a multimodal network based on multi-head attention layers to capture crossmodal interactions. MultiEMO  [36]  model incorporates bidirectional multi-head cross-attention layers for effective fusion. What's more, CBERL  [28]  using a multimodal generative adversarial network to address the imbalanced distribution of emotion categories in raw data.\n\nOne major assumption in MERC is that data from all modalities are complete and continuous. However, in the real world, data from modalities are often incomplete due to various reasons, making learning under incomplete modalities a promising area of research.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Incomplete Multimodal Learning",
      "text": "Multimodal learning aims to utilize information from a variety of data modalities to improve generalization performance. However, in some conditions, modalities may be missing or unavailable. A straightforward method is to use existing data for classification. Additionally, there are strategies that conduct data imputation aiming to reconstruct missing data. We divide the existing methods into two categories: non-reconstruction and reconstruction methods.\n\nExisting non-reconstruction approaches primarily focus on the analysis of incomplete data, such as through maximizing correlations  [9, 24, 23] . Hotelling et al.  [20]  introduced CCA, which maximizes canonical correlations by linearly mapping multimodal features into a low-dimensional space. In contrast to CCA's linear focus, Andrew et al.  [9]  developed DCCA, which enhances traditional CCA by addressing its limitations related to linear associations. It employs deep neural networks to uncover more intricate, non-linear relationships across different modalities. Additionally, Wang et al.  [59]  introduced DCCAE. DCCAE advances CCA by incorporating autoencoders, which are designed to extract latent features from each modality. This approach optimizes both the reconstruction accuracy of the autoencoders and the canonical correlations, effectively balancing the integrity of modality-specific structures with the connectivity between modalities.\n\nReconstruction methods, on the other hand, aim to ensure data completeness, primarily through data imputation  [32, 25, 66] , generating missing data  [57, 54, 14] , or reconstructing incomplete data by learning feature representations. Parthasarathy et al.  [32]  proposed an attention-based model that fills missing video data with zero vectors. Zhang et al.  [66]  developed CPM-Net, which integrates an encoderless model with a clustering-like classification loss to learn features and pads missing modalities with average values. Moreover, several DNN-based models have been developed, including autoencoders  [54] , GAN  [58] , and Transformers  [63] .\n\nTo better reconstruct incomplete data, researchers started to explore feature representations. For example, Lian et al.\n\nproposed GCNet  [22] , which utilizes GNN-based models to capture different types of information in conversations to reconstruct missing modalities. Wang et al.  [60]  considered the consistency of data distributions to recover missing features.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "The main objective of MERC is to assign an appropriate emotion label to each utterance within a dialogue. This paper specifically addresses scenarios where multimodal data is incomplete-common in real-world applications where some modalities might be unavailable or lost due to technical issues. We introduce a novel framework, SDR-GNN, designed to effectively manage and process these incomplete datasets. Our approach leverages the intrinsic structure of conversational data and employs graph neural networks to interpolate or reconstruct the missing modalities, ensuring robust emotion recognition even with partial information. Fig.  2  in the paper provides a visual overview of the SDR-GNN framework, illustrating its key components and operational flow in handling missing multimodal features across conversational utterances.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Node Construction",
      "text": "We define each conversation consists of a series of utterances ğ¶ = {ğ‘¢ 1 , ğ‘¢ 2 , â‹¯ , ğ‘¢ ğ‘› }, where ğ‘› is the number of utterances. Each conversation involves ğ‘ speakers ğ‘ƒ = {ğ‘ 1 , ğ‘ 2 , â‹¯ , ğ‘ ğ‘ }(ğ‘ â‰¥ 2). Each utterance ğ‘¢ ğ‘– is spoken by ğ‘ ğ‘ (ğ‘¢ ğ‘– ) , where the function ğ‘ (â‹…) maps the index of utterance into its corresponding speaker. For each utterance ğ‘¢ ğ‘– , we extract multimodal features ğ‘¢ ğ‘– = {ğœ‚ğ‘“ ğ‘š ğ‘– } ğ‘šâˆˆ{ğ‘,ğ‘£,ğ‘¡} . Here, ğ‘“ ğ‘ ğ‘– âˆˆ â„ ğ‘‘ ğ‘ , ğ‘“ ğ‘£ ğ‘– âˆˆ â„ ğ‘‘ ğ‘£ and ğ‘“ ğ‘¡ ğ‘– âˆˆ â„ ğ‘‘ ğ‘¡ represent the audio, visual and text features of the utterance, respectively. {ğ‘‘ ğ‘š } ğ‘šâˆˆ{ğ‘,ğ‘£,ğ‘¡} is the feature dimension of each modality. Each ğœ‚ of ğ‘¢ ğ‘– is defined as follows: The overall structure of the framework. First, we encode features of the utterance using a Bi-GRU to obtain the contextual embedding of each node. Then, we apply the SDR-GNN to capture features, jointly considering higher-order and multi-frequency information. Finally, we reconstruct the incomplete features and classify the emotion labels.\n\nIn this paper, we assume at least one modality-complete data is available for analysis. Therefore, an incomplete ğ‘€modal dataset has\n\ndifferent missing patterns, in line with previous works  [66, 22] .\n\nWe employ a bidirectional Gated Recurrent Unit (GRU) to extract contextual features and dynamically analyze dependency relationships. The computation is performed as follows:\n\nğ» is the matrix containing all hidden states â„ ğ‘– for (ğ‘– = 1) to ğ‘›. Each hidden state is a vector that captures contextual information up to the ğ‘–-th position from both directions of the sequence.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Spectral Domain Reconstruction Graph Neural Network",
      "text": "The main idea of Spectral Domain Reconstruction Graph Neural Network is to capture the multivariate relationships between domain nodes, resulting in better aggregation effects for the following reconstruction task. We first construct relation graph convolutional networks (R-GCN)  [35]  in capturing node features, capturing both contextual and speaker features. In addition, considering the dynamic absence of modalities, we construct a hypergraph with edge-dependent node weights to flexibly aggregate node information. Recent works has verified the effectiveness of multi-frequency emotional information in the ERC task  [16, 12] , therefore we design a frequency-aware module specifically to capture this information.\n\nWe have developed speaker interaction graphs and context interaction graphs as the primary modules for extracting emotion cues. In these graphs, edges measure the significance of connections between nodes, where the type of edge determines the propagation method of various information. While both the speaker and context graphs use identical edges, each edge represents a distinct dependency.\n\nEdges: Considering the overwhelming number of connections when each node interacts with all others, we streamline this by limiting node interactions to a fixed-size context window ğ‘¤, following insights from previous research that emphasize the importance of local context. Therefore, a node ğ‘£ ğ‘– only connects with nearby nodes within the context window {ğ‘£ ğ‘— } ğ‘—âˆˆ[ğ‘šğ‘ğ‘¥(ğ‘–-ğ‘¤,1),ğ‘šğ‘–ğ‘›(ğ‘–+ğ‘¤,ğ¿)] , significantly reducing complexity. We select ğ‘¤ from the set {1, 2, 3, 4} and denote the edge from node ğ‘£ ğ‘– to ğ‘£ ğ‘— as ğ‘’ ğ‘–ğ‘— âˆˆ îˆ± (|îˆ±| = ğ‘› + 2ğ‘¤ -1).\n\nSpeaker Interaction Graph: The speaker interaction graph leverages the various speakers and their corresponding utterances to map out the dependencies among speakers within a conversation. Each edge ğ‘’ ğ‘–ğ‘— in the graph is tagged with a speaker identifier ğ›¼ ğ‘–ğ‘— from the set ğ›¼, which encompasses all speaker types present in the dialogue. The cardinality of ğ›¼, represented as ğ›¼, indicates the number of distinct speaker types. For each connection ğ‘’ ğ‘–ğ‘— , ğ›¼ ğ‘–ğ‘— denotes the directional flow from speaker ğ‘ ğ‘ (ğ‘¢ ğ‘– ) to speaker ğ‘ ğ‘ (ğ‘¢ ğ‘— ) , where ğ‘ ğ‘ (ğ‘¢ ğ‘– ) and ğ‘ ğ‘ (ğ‘¢ ğ‘— ) are the speaker identifiers for ğ‘¢ ğ‘– and ğ‘¢ ğ‘— , respectively.\n\nContext Interaction Graph: Context interaction graph utilizes contextual information to delineate the contextual dependencies within a conversation. Each edge ğ‘’ ğ‘–ğ‘— is assigned a context type identifier ğ›½ ğ‘–ğ‘— âˆˆ |ğ›½|, which contains all possible context types in the discussion. The determination of ğ›½ values is influenced by the relative positioning of ğ‘¢ ğ‘– and ğ‘¢ ğ‘— within the dialogue, with possible values including backward, present, forward. Therefore, the total number of context types, |ğ›½|, is three.\n\nWeighted HyperGraph: In hypergraphs, we define two types of weights: an edge weight, ğœ†(ğ‘’), for each edge ğ‘’, and a node weight, ğ›¾ ğ‘’ (ğ‘£), for each node ğ‘£ incident to edge ğ‘’, also known as edge-dependent node weight. Intuitively, ğ›¾ ğ‘’ (ğ‘£) represents the contribution of node ğ‘£ to the hyperedge ğ‘’, enriching the representation of detailed multimodal and contextual dependencies. Consequently, edge-dependent node weights are expressed using a weighted incidence matrix. Ä¤ âˆˆ â„ ğ‘›Ã—|îˆ±| :\n\nGraph learning: We use R-GCN to aggregate the local information in the graph, then use hypergraph for weighted aggregation. The calculation is shown as follows:\n\nwhere\n\nis the input at layer ğ‘™. ğ– ğ‘’ =diag(ğœ†(ğ‘’ 1 ), â‹¯ , ğœ†(ğ‘’ ğ‘–ğ‘— )) is the edge weight matrix.\n\nFrequency-Aware Graph: Although speaker graph and context graph can capture feature dependencies, they still follow the generic graph learning protocol, which aggregates and smooths signals from the local neighborhood, thereby erasing high-frequency signals  [16, 12] . These signals can be crucial for ERC tasks. To effectively learn different types of frequency information between the central node and its neighbors, we designed a self-gating mechanism. Specifically, it calculates the correlation between the central node and its neighbors, learning the multi-frequency information of multimodal features. Mathematically:\n\nHere, ğ‘Š ğ‘Ÿ 3 , ğ‘Š ğ‘Ÿ 4 âˆˆ â„ 2â„ are trainable weight matrices, and tanh(â‹…) is the hyperbolic tangent function, which scales the input to the range [-1,1]. In the context of graph neural networks, low-frequency signals can be thought of as generalized information propagated across large areas of the network, indicating similarity or commonality among nodes. High-frequency signals, conversely, emphasize differences or specific characteristics distinct to neighboring nodes. These signals are derived through the spectral decomposition of the graph Laplacian, which allows us to separate these frequency components mathematically. Through this mechanism, the outputs of ğ‘Š ğ‘Ÿ  3 ğ‘£ ğ‘ ğ‘ ğ‘–ğ‘— and ğ‘Š ğ‘Ÿ 4 ğ‘£ ğ‘ğ‘œ ğ‘–ğ‘— effectively gauge the significance of various frequency components. The selfgating mechanism, as proposed in our SDR-GNN, enables dynamic differentiation and integration of these frequency signals, helping retain essential low-frequency information while preserving critical high-frequency details crucial for tasks involving nuanced data. For example, if ğ‘Š ğ‘Ÿ  3 ğ‘£ ğ‘ ğ‘ ğ‘–ğ‘— < 0, high-frequency messages are prominent, signifying a greater difference between node ğ‘– and its neighbor ğ‘—, and vice versa.\n\nTo aggregate these representations, we concatenate them to form the final representation of the Local Enhancement Graph:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Reconstruction & Self Optimization",
      "text": "To better utilize multi-frequency data, we input the extracted features into a linear transformation layer for predicting missing data and achieving data recovery. Then input the recovered data into a multi-head attention layer  [56]  to fuse and optimize reconstructed modalities, which can be shown as follows:\n\nwhere ğ¿ = {ğ‘™ ğ‘– } ğ‘› ğ‘–=1 âˆˆ â„ ğ‘›Ã—ğ‘‘ â„ is the matrix containing all hidden states ğ‘™ ğ‘– and F ğ‘š = { fğ‘– } ğ‘› ğ‘–=1 âˆˆ â„ ğ¿Ã—ğ‘‘ ğ‘š is the estimated complete data. ğ‘Š ğ‘š âˆˆ â„ ğ‘‘Ã—ğ‘‘ ğ‘š and ğ‘ ğ‘š âˆˆ â„ ğ‘‘ ğ‘š are the trainable parameters, where ğ‘‘ ğ‘š is the feature dimension for each modality. For the attention layers, ğ‘„ = F ğ‘Š ğ‘„ , ğ¾ = F ğ‘Š ğ¾ , ğ‘‰ = F ğ‘Š ğ‘‰ . ğ‘„ = F ğ‘Š ğ‘„ , ğ¾ = F ğ‘Š ğ¾ , ğ‘‰ = F ğ‘Š ğ‘‰ are the trainable parameter matrices. In this approach, multiple attentions are combined to obtain the output results of the multi-head attention layer as follows:\n\nwhere ğ¹ 1 , â€¦ , ğ¹ ğ‘˜ is the output of each attention layer, ğ‘˜ is the number of attention layers, and ğ‘Š is the trainable parameter matrix.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Classifier",
      "text": "To enhance the learning of more discriminative features for conversation understanding, we input the latent representations ğ¿ = {ğ‘™ ğ‘– } ğ‘› ğ‘–=1 into a fully-connected layer, subsequently followed by a softmax layer to compute the classification probabilities:\n\nhere\n\nWhere ğ‘¦ ğ‘– is the true labels and ğ‘ is the number of discrete labels in the corpus, ğ‘Š ğ‘ âˆˆ â„ ğ‘‘Ã—ğ‘ and ğ‘ ğ‘ âˆˆ â„ ğ‘ are the trainable parameters. ğ‘Š ğ‘ âˆˆ â„ ğ‘‘Ã—ğ‘ and ğ‘ ğ‘ âˆˆ â„ ğ‘ are the trainable parameters.\n\nOur loss function consists of two parts, the reconstruction function and the cross entropy function. The reconstruction function is used to calculate the difference between the original data and the filled data, while the cross entropy function is used for label classification. The calculation is illustrated as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we describe the three benchmark conversational datasets employed in our experiments, explain the evaluation metrics and multimodal features used, and introduce a variety of advanced baselines for comparison in the context of incomplete multimodal learning.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets",
      "text": "To assess the efficacy of the SDR-GNN model, we conducted experiments using three benchmark conversational datasets: IEMOCAP  [13] , CMU-MOSI  [64] , and CMU-MOSEI  [65] . These datasets are widely recognized in the research community for their comprehensive coverage of emotional and multimodal human interactions, making them ideal for testing new models in multimodal learning contexts.\n\nIEMOCAP includes multiple conversations between two speakers, segmented into short utterances each annotated with discrete emotion labels. For consistency in comparisons, we employ two prevalent labeling methods, generating datasets with either four or six classes. The fourclass dataset includes the emotions: anger, happiness (where excitement is merged with happiness), sadness, and neutral  [34] . The six-class dataset encompasses: anger, happiness, sadness, neutral, excitement, and frustration  [26] .\n\nCMU-MOSI features a collection of movie review videos from online platforms, comprising 2,199 short monologue clips. Each clip is rated with a sentiment intensity score on a scale from -3 (most negative) to +3 (most positive).\n\nCMU-MOSEI extends CMU-MOSI by incorporating a wider range of topics with 22,856 movie review clips from YouTube, maintaining the same sentiment scoring method from -3 to +3.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details And Evaluation Metrics",
      "text": "We evaluate the performance of various methods on multimodal datasets with different missing rates, defined as\n\nHere, ğ‘  ğ‘– represents the number of available modalities for the ğ‘– ğ‘¡â„ sample, ğ¿ is the total number of samples, and ğ‘€ is the total number of modalities. For each sample, modalities are randomly masked according to îˆ¹, ensuring at least one modality per sample. This constraint results in îˆ¹ â©½ ğ‘€-1 ğ‘€ . For ğ‘€ = 3, îˆ¹ ranges from 0.0 to 0.7, the latter approximating ğ‘€-1 ğ‘€ . In line with prior research  [60, 22] , the missing rate remains constant across training, validation, and testing phases.\n\nWe utilize the datasets IEMOCAP, CMU-MOSI, and CMU-MOSEI, which are equipped with predefined splits for training, validation, and testing. The model configuration that performs optimally is identified using the validation set and subsequently evaluated on the test set. Our methodology involves adjusting two key parameters: the dimension of latent representations, labeled as â„, and the size of the interaction window, labeled as ğ‘¤. Our experiments involve values of â„ âˆˆ {100, 150, 200, 250} and ğ‘¤ âˆˆ {1, 2, 3, 4}, applied across all datasets. For optimization, the Adam optimizer is employed, with a learning rate of 0.001 and a weight decay of 0.00001. Additionally, we incorporate a multi-head attention mechanism with ğ‘˜ = 256 heads. To mitigate overfitting, Dropout  [52]  is applied at a rate of ğ‘ = 0.5. The reliability of our results is ensured by averaging the performance over ten trials on the test set.\n\nTo verify our method, we select the following evaluation metrics to fair compete with different approaches.\n\nFor IEMOCAP, we choose weighted average F1-score (WAF1) as the evaluation metric. WAF1 is calculated as a weighted mean F1 over different emotion categories with weights proportional to the number of utterances in each emotion class, which can be shown as follows, in line with previous works  [26, 22] .\n\nwhere ğ¸ is the total number of emotion categories, ğ‘ ğ‘— is the number of samples in category ğ‘—, and ğ¹ 1 ğ‘— is the F1 score for category ğ‘—.\n\nFor CMU-MOSI and CMU-MOSEI, we focus on the negative/positive classification task, with scores assigned to less than 0 for negative and greater than 0 for positive, respectively. We choose WAF1 as the primary metric and the accuracy (ACC) of the classification task as the secondary metric.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baselines",
      "text": "CCA  [20] : CCA aims to find the linear relationships with the maximum correlation between two multimodal datasets. By linearly mapping them into a low-dimensional common space, CCA learns the relationships between different modalities. It is a strong benchmark model, especially suitable for scenarios where linear relationships can capture the interactions between modalities well. DCCA  [9] : DCCA enhances traditional CCA by addressing its limitations related to linear associations. It employs deep neural networks to uncover more intricate, non-linear relationships across different modalities. DCCAE  [59] : DCCAE advances CCA by incorporating autoencoders, which are designed to extract latent features from each modality. This approach optimizes both the reconstruction accuracy of the autoencoders and the canonical correlations, effectively balancing the integrity of modalityspecific structures with the connectivity between modalities. AE  [11] : In incomplete multimodal learning, autoencoders are widely used to impute missing data from partially observed inputs. By jointly optimizing the reconstruction loss of autoencoders and the classification loss of downstream tasks, this method supports a trade-off in implementation.\n\nCRA  [54] : CRA extends AE by integrating a series of residual autoencoders into a cascaded architecture for data imputation. During implementation, CRA optimizes both imputation and downstream tasks in an end-to-end manner, enhancing the quality of data completion and the performance of tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Mmin[68]:",
      "text": "The MMIN model integrates CRA with cycle consistency learning to predict the latent representations of missing modalities. This approach makes MMIN a robust benchmark model, demonstrating excellent performance under a range of missing conditions. This dual-component strategy enhances the model's ability to handle incomplete data, ensuring more accurate and reliable predictions across different scenarios. CPM-Net[66]: CPM-Net accounts for both completeness and versatility in multi-view representation to learn discriminative latent features. The framework is constructed to optimize the use of multiple partial views by defining and theoretically proving \"completeness\" and \"versatility\" in multi-view representations. MCTN  [33] : MCTN is a method designed to learn robust joint representations by translating between modalities. It combines an autoencoder with a cycle consistency loss to achieve modality reconstruction. GCNet  [22] : GCNet is a state-of-the-art method that utilizes graph neural networks to capture different types of features and recover missing modalities, further improving the performance of downstream tasks. DiCMoR  [60] : DiCMoR is also a state-of-the-art method which considers the consistency of data distributions to recover the missing features, in order to obtain better recovered data.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Classification Performance",
      "text": "Table  2  and Table  3  presents the classification performance compared with different approaches under various missing rate. From these results, we can observe:\n\n1. On average, SDR-GNN consistently outperforms other methods across all datasets. For IEMOCAP and CMU-MOSEI, SDR-GNN shows an absolute improvement from 0.77% to 8.6% on WAF1. Compared with non-reconstructive approaches, reconstructive techniques, including our SDR-GNN, demonstrate superior performance. This improvement is attributed to the ability of reconstructive methods to estimate and rebuild modalities from existing modalities. Compared with the reconstruction methods  [33, 60, 22, 68] , our SDR-GNN perform better. We argue that these baselines do not use the multi-frequency information in conversation. Our method utilizes multi-frequency signals to reconstruct missing modalities, resulting in better classification performance.\n\n2. Our method exhibits less performance degradation with increasing missing rates compared to others. For example, in IEMOCAP (four-class), while other methods see performance drops between 6.98% and 37.70% as the missing rate increases to 0.7, our SDR-GNN declines by only 5.17%. Moreover, SDR-GNN shows greater improvement as the missing rate rises; in CMU-MOSEI, the improvement is 3.21% at a missing rate of 0.1, reaching 8.6% at 0.7, indicating robustness in scenarios with high missing rates. 3. Experimental results demonstrate SDR-GNN also exhibits better performance when multimodal data is complete (îˆ¹ = 0.0), For all datasets, our SDR-GNN improve 0.7% âˆ¼ 2.7%. These results validate the effectiveness of our method on both complete and incomplete multimodal data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "To study the necessity of different components in SDR-GNN to model performances, we conduct ablation studies on IEMOCAP(four-class) and IEMOCAP(six-class). Experimental results are shown in Table  4 .\n\nâ€¢ SDR-GNN: Our proposed method that considers both features relationships and multi-frequency information.\n\nâ€¢ SDR-GNN w/o Sp : It is derived from SDR-GNN, but ignores the information comes from spearker interaction graph.\n\nâ€¢ SDR-GNN w/o Co : It is derived from SDR-GNN, but ignores the information comes from context interaction graph.\n\nâ€¢ SDR-GNN w/o Fre : It is derived from SDR-GNN, but replaces the frequency-aware graph learning with a GNN-based model from DialogueGCN  [19] , a currently advanced graphical model for conversation understanding.\n\nâ€¢ SDR-GNN w/o Op : It is derived from SDR-GNN, but remove self optimizing multi-head attention layer used for data reconstruction.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Speaker Interaction Graph:",
      "text": "To study the effect of speaker interaction graph. We remove the information that comes from the speaker graph. Experimental results show that performance decreases in most cases on both IEMOCAP (four-class) and IEMOCAP (six-class). The inferior performance of SDR-GNN w/o Sp on both datasets proves the effectiveness of speaker information.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Impact Of Context Interaction Graph:",
      "text": "We remove the information that comes from the context interaction graph to investigate its effectiveness. Experimental results show that performance decreases at all missing rates. Meanwhile, compared with SDR-GNN w/o Sp , SDR-GNN w/o Co deceases more. The results show that contextual information is more important than speaker information, which also proves the significance of contextual information.\n\nImpact of Frequency-aware Graph Learning: To investigate the impact of frequency-aware graph learning, we replace the frequency-aware graph learning with a graph convolution network from DialogueGCN, which captures speaker and context dependencies on one graph. Results from Table  4  show that performance drop considerably at all missing rates. This proves the importance and superiority of capturing frequency information using Frequency-aware Graph Learning, especially when modalities are incomplete. Impact of Self Optimization: We use multi-head attention layers to optimize the reconstructed data. To study the effect of this model, we remove the multi-head attention layers during training. Experimental results demonstrate that the performances of SDR-GNN w/o Op decline on all datasets. The results of SDR-GNN w/o Op prove the effectiveness of self optimizing reconstructed data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion Categories Analysis",
      "text": "We investigate the classification performance of different emotional categories under various missing rates. Fig.  3aâˆ¼3h  show confusion matrices on IEMOCAP (four-class) and IEMOCAP (six-class) under different missing rates. The rows represent the predicted labels, and the columns represent the actual emotional labels.\n\nFig.  3aâˆ¼3d  depict the confusion matrices on IEMOCAP (four-class). From these matrices, we observe no significant decrease in the accuracy of recognizing various emotion categories as the missing rate increases. This indicates that our SDR-GNN can effectively recognize conversations with high missing rates. However, as the missing rate increases, we notice that conversations truly labeled as 'happy' are more likely to be misclassified as 'angry'. We attribute this to the possibility that, with significant data loss, the model may struggle to capture subtle features distinguishing between happy and angry emotions. For instance, incomplete tone and emphasis information in audio may hinder the model's ability to differentiate between excited high tones and angry high tones. In the expression of happy and angry emotions, certain expressions may appear similar to some extent, particularly when multimodal information is incomplete. Without contextual support from other modalities, the model may fail to interpret these subtle differences accurately. Fig.  3eâˆ¼3h  depict the confusion matrix on IEMOCAP (six-class). Similar to IEMOCAP (four-class), the model can maintain recognition accuracy even as the loss rate increases. However, unlike IEMOCAP (four-class), IEMO-CAP (six-class) introduces two additional labels: \"excited\" and \"frustrated,\" which adds complexity to the model's recognition task. From these confusion matrices, it's evident that statements labeled as \"happy\" are more prone to being misclassified as \"angry\" or \"excited.\" This is reasonable since the model struggles to differentiate statements with intense emotions, particularly when the loss rate is high. Moreover, statements with a true label of \"neutral\" are often mistakenly identified as \"frustrated\". From a frequency perspective, we believe that emotions such as \"neutral\" exhibit low-frequency signals that tend towards zero. However, when the loss rate increases, some low-frequency signals are erroneously amplified, leading to misjudgments by the model. Additionally, \"frustrated\" is frequently misclassified as \"anger,\" likely due to similarities in language features between frustrated and anger, such as negative emotions and vocabulary. This similarity poses challenges in accurately distinguishing between these two emotions.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Model Complexity Analysis",
      "text": "To analyze the complexity of our model, we compared SDR-GNN and SDR-GNNmini with other state-of-the-art models.\n\nâ€¢ SDR-GNN: Our original version that considers both features relationships and multi-frequency information..\n\nâ€¢ SDR-GNN mini : A derivative of SDR-GNN that retains all core functionalities of SDR-GNN but reduces the number of neurons and network layers.\n\nFrom the experimental results in Table  5 , SDR-GNN performs the best, but its parameter size and training speed are inferior to other methods. SDR-GNN mini outperforms other methods in terms of parameter size and training speed, but its performance is slightly lower than SDR-GNN.\n\nWe believe that the higher parameter size of SDR-GNN enhances the model's learning capacity, thereby improving its performance, but this also results in longer training times.  SDR-GNN mini , on the other hand, sacrifices some performance in exchange for faster training speed.\n\nIn conclusion, SDR-GNN mini outperforms other solutions in terms of parameter size, training time, and performance, which also validates the effectiveness of our method.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Importance Of Incomplete Data",
      "text": "Our proposed SDR-GNN not only utilizes complete multimodal data, but also make full use of incomplete multimodal data. To investigate the importance of incomplete data, in Fig.  3 , we compare the performance of different methods under various missing rates.\n\nâ€¢ SDR-GNN: The method we proposed that fully utilizes both complete and incomplete modality data for conversational learning.\n\nâ€¢ Lower bound: It comes from SDR-GNN, but abandons the incomplete multimodal utterances. This method is a straightforward strategy that only focus on complete data, which is regarded as the lower bound  [25] .\n\nAccording to Fig.  3 , SDR-GNN consistently outperforms the lower bound across all missing rates and datasets. Meanwhile, as the missing rate increases, the disparity in performance between SDR-GNN and the comparison system widens significantly.This observation underscores the significance of leveraging incomplete data to enhance the performance of conversational learning models. By effectively incorporating incomplete information, SDR-GNN demonstrates superior adaptability and robustness in handling incomplete multimodal data.\n\nThe experimental results demonstrate that despite the incompleteness of the data modality, it retains significant utility. It is imperative to concurrently leverage both complete and incomplete modal data to enhance contextual understanding and improve recognition outcomes. Our belief stems from the comprehensive utilization of both data types by SDR-GNN in establishing contextual connections, enabling it to maintain recognition accuracy even under high missing rates. This underscores the efficacy and superiority of leveraging both data types simultaneously.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Reconstruction Performance",
      "text": "Our approach employs SDR-GNN to reconstruct the data in order to meet the requirements of downstream emotion classification. Therefore, the quality of the reconstructed data will directly impact the performance of the classification task. To validate the effectiveness of our approach, we compared it with two advanced data reconstruction models, GCNet  [22]  and CRA  [54] . To evaluate the reconstruction performance of different methods, we calculated the mean square error (MSE) between the reconstructed data of the missing modalities and the real data, in line with previous works.\n\nFig.  5  shows the performance of the reconstructed data under different missing rates. A lower MSE indicates a smaller difference between the reconstructed data and the real data, implying better reconstruction performance. We observe that as the missing rate increases, the MSE also increases. This is because a higher missing rate leads to a reduction in data volume, making it more difficult for the model to reconstruct the data.\n\nThe experimental results demonstrate that SDR-GNN outperforms other methods in most cases. Compared to GNN-based models, SDR-GNN performs better because we utilize multi-frequency signals of different frequencies in our reconstruction method, further proving the importance of",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Parameter Tuning",
      "text": "Our SDR-GNN model includes four hyper-parameters: the interaction window size ğ‘¤, the hidden layer dimension â„, reconstruction loss function weight ğ‘’ and the number of hypergraphs layer ğ‘™. We assessed the impact of these parameters through experiments on the IEMOCAP (fourclass) dataset under various missing rates, selecting ğ‘¤ from {1, 2, 3, 4} and â„ from {100, 150, 200, 250}, ğ‘’ ranges from 0.1 to 0.9 and ğ‘™ from {1, 2, 3, 4, 5, 6}. The results of the experiments are displayed in Table  6 , Fig.  7  and Fig.  8 .\n\nIn most cases, the classification performance improves first the degrades as ğ‘¤ increase. This can be explained from two aspects. On one hand, a bigger window size can contain more utterances, which helps capturing and learning contextual information. On the other hand, a large window size will contain a large number of edges, which may include  more irrelevant information. This will increase the difficulty of model learning.\n\nSimilarly, an increase in the hidden layer dimension â„ generally results in improved performance. At the same time, we also observed that when â„ becomes too large, it leads to a decline in the model's performance. as seen in Table  6 . A larger â„ provides a greater number of trainable parameters, thereby enhancing the model's ability to capture and represent complex feature interactions. This is particularly beneficial for discerning subtle patterns and distinctions in the data, which are crucial for accurate classification. However, this increase in parameters also heightens the risk of overfitting. Therefore, choosing appropriate parameters is crucial for improving the performance of the model.\n\nTo investigate the impact of the hyperparameters ğ‘’ and ğ‘™, we conducted experiments on the IEMOCAP (Four-class) dataset with a missing rate of îˆ¹ = 0.4.\n\nAs shown in Fig.  6 , when the weight of the reconstruction loss function ğ‘’ increases from 0.1 to 0.9, the model's performance first rises and then declines, with the best performance observed around ğ‘’ = 0.5. We believe that the reconstruction task and the classification task should have similar weights. If the reconstruction task dominates, the classification results deteriorate; conversely, if the model focuses too much on the classification task, the quality of the reconstructed data decreases, which negatively impacts classification performance. Therefore, in our actual experiments, we set ğ‘’ to 0.5 to balance the weights between the classification and reconstruction tasks, achieving good results in most cases. From Fig.  7 , we can observe that as the number of layers ğ‘™ increases, the model's performance also first improves and then declines. This is because with fewer layers, the number of propagated nodes is limited, while with more layers, redundant data propagation does not further enhance feature extraction. Additionally, it increases the risk of over-fitting.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Case Study",
      "text": "In this section, we compare the prediction results of different methods under the condition of missing modalities. The dialogue example is taken from IEMOCAP (Four-class), and Fig.  8  shows the experimental results. In this dialogue, Speaker B tells Speaker A that he intends to propose to Annie. We observe that as the degree of missing modalities increases, the performance of all models decreases, as it becomes more challenging to predict the outcome with less data.\n\nIn the dialogue shown in Fig.  8 , ğ‘¢5 can be considered a high-frequency signal sentence because SpeakerA shifts from Neutral to Anger, displaying intense emotion that stands out distinctly from the surrounding context. Most models perform poorly in predicting ğ‘¢5, except for SDR-GNN and MMIN. MMIN is designed to analyze individual utterances, so the surrounding context does not influence its predictions. In contrast, other models that rely on context for feature extraction or clustering algorithms may lose ğ‘¢5's high-frequency signal during the feature capturing and signal propagation process. However, SDR-GNN effectively differentiates multi-frequency information for feature aggregation, preventing this issue.\n\nOur method consistently achieves high accuracy across all situations, demonstrating its effectiveness. SDR-GNN comprehensively leverages multi-frequency information, further improving prediction accuracy, which also highlights the importance of multi-frequency information.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we introduce a novel framework, SDR-GNN, designed for addressing the challenges of incomplete multimodal learning in conversational emotion recognition. This approach leverages the dependencies between speakers and contexts, utilizing multi-frequency information within conversations effectively. Our framework specifically addresses the higher-order information of modalities and exploits multi-frequency data, bridging the gap in existing methods. We validate our method through experiments on three benchmark datasets, with results showing that SDR-GNN outperforms current methods in handling incomplete multimodal data for emotion recognition. Additionally, we dissect the critical role of each component within SDR-GNN and examine the influence of various hyper-parameters. Furthermore, After that, we analyze emotion categories at various missing rates and show the importance of incomplete data.\n\nIn the future, we will explore ways to better use the multifrequency information in conversations and the relationships between the frequency signals and emotions.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Credit Authorship Contribution Statement",
      "text": "",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A toy example of complete multimodal features",
      "page": 1
    },
    {
      "caption": "Figure 2: in the paper provides a visual overview of the SDR-",
      "page": 3
    },
    {
      "caption": "Figure 2: The overall structure of the framework. First, we encode features of the utterance using a Bi-GRU to obtain the",
      "page": 4
    },
    {
      "caption": "Figure 3: aâˆ¼3h show confusion matrices on IEMOCAP (four-class)",
      "page": 9
    },
    {
      "caption": "Figure 3: aâˆ¼3d depict the confusion matrices on IEMOCAP",
      "page": 9
    },
    {
      "caption": "Figure 3: Confusion matrices of the test set on IEMOCAP at varying missing rates. The matrices present the true labels along",
      "page": 10
    },
    {
      "caption": "Figure 3: eâˆ¼3h depict the confusion matrix on IEMOCAP",
      "page": 10
    },
    {
      "caption": "Figure 4: Classification performance comparison between SDR-GNN and Lower bound under different missing rates.",
      "page": 11
    },
    {
      "caption": "Figure 3: , we compare the performance of different",
      "page": 11
    },
    {
      "caption": "Figure 3: , SDR-GNN consistently outper-",
      "page": 11
    },
    {
      "caption": "Figure 5: Reconstruction performance comparison between SDR-GNN and other methods under different missing rates. Lower",
      "page": 12
    },
    {
      "caption": "Figure 5: shows the performance of the reconstructed data",
      "page": 12
    },
    {
      "caption": "Figure 7: and Fig. 8.",
      "page": 13
    },
    {
      "caption": "Figure 6: Parameter tuning with various number of hypergraph",
      "page": 13
    },
    {
      "caption": "Figure 7: Parameter tuning with various weight of reconstruc-",
      "page": 13
    },
    {
      "caption": "Figure 6: , when the weight of the reconstruc-",
      "page": 13
    },
    {
      "caption": "Figure 8: Prediction results on incomplete conversational data from IEMOCAP (Four-class).",
      "page": 14
    },
    {
      "caption": "Figure 7: , we can observe that as the number of layers",
      "page": 14
    },
    {
      "caption": "Figure 8: shows the experimental results. In this dialogue,",
      "page": 14
    },
    {
      "caption": "Figure 8: , ğ‘¢5 can be consid-",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table 1: modality.Fortheattentionlayers,ğ‘„ = ğ¹Ì‚ğ‘Š ğ‘„ ,ğ¾ = ğ¹Ì‚ğ‘Š ğ¾ , Statistical information on IEMOCAP, CMU-MOSI and CMU-",
      "data": [
        {
          "IEMOCAP(four-class)": "IEMOCAP(six-class)",
          "4290": "5810",
          "1241": "1623",
          "120": "120",
          "31": "31"
        },
        {
          "IEMOCAP(four-class)": "CMU-MOSI",
          "4290": "1513",
          "1241": "686",
          "120": "62",
          "31": "31"
        },
        {
          "IEMOCAP(four-class)": "CMU-MOSEI",
          "4290": "18197",
          "1241": "4659",
          "120": "2549",
          "31": "676"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: Parameter tuning with various missing rates.",
      "data": [
        {
          "0.0": "0.1",
          "100\n150\n200\n250": "100\n150\n200\n250",
          "77.85\n78.02\n77.87\n78.15\n79.52\n78.75\n78.55\n79.03\n78.76\n79.10\n78.99\n79.45\n78.68\n79.32\n79.12\n78.50": "77.95\n77.50\n77.94\n77.61\n79.05\n77.69\n77.75\n78.38\n78.50\n78.37\n78.58\n78.57\n78.43\n78.80\n78.33\n78.24"
        },
        {
          "0.0": "0.2",
          "100\n150\n200\n250": "100\n150\n200\n250",
          "77.85\n78.02\n77.87\n78.15\n79.52\n78.75\n78.55\n79.03\n78.76\n79.10\n78.99\n79.45\n78.68\n79.32\n79.12\n78.50": "77.26\n77.34\n77.74\n77.21\n77.62\n77.71\n77.85\n77.44\n78.12\n77.36\n76.67\n78.00\n77.54\n77.43\n77.88\n77.32"
        },
        {
          "0.0": "0.3",
          "100\n150\n200\n250": "100\n150\n200\n250",
          "77.85\n78.02\n77.87\n78.15\n79.52\n78.75\n78.55\n79.03\n78.76\n79.10\n78.99\n79.45\n78.68\n79.32\n79.12\n78.50": "76.80\n76.63\n77.04\n77.04\n76.96\n77.18\n77.32\n76.95\n77.63\n76.92\n76.67\n77.44\n76.60\n76.03\n76.78\n76.66"
        },
        {
          "0.0": "0.4",
          "100\n150\n200\n250": "100\n150\n200\n250",
          "77.85\n78.02\n77.87\n78.15\n79.52\n78.75\n78.55\n79.03\n78.76\n79.10\n78.99\n79.45\n78.68\n79.32\n79.12\n78.50": "75.69\n75.81\n75.97\n76.40\n75.66\n75.88\n76.23\n76.55\n77.11\n75.93\n76.74\n76.70\n75.98\n75.50\n75.65\n76.23"
        },
        {
          "0.0": "0.5",
          "100\n150\n200\n250": "100\n150\n200\n250",
          "77.85\n78.02\n77.87\n78.15\n79.52\n78.75\n78.55\n79.03\n78.76\n79.10\n78.99\n79.45\n78.68\n79.32\n79.12\n78.50": "74.99\n75.23\n75.51\n75.64\n75.22\n75.44\n75.61\n75.67\n76.09\n75.73\n75.32\n75.92\n75.02\n74.87\n75.56\n75.62"
        },
        {
          "0.0": "0.6",
          "100\n150\n200\n250": "100\n150\n200\n250",
          "77.85\n78.02\n77.87\n78.15\n79.52\n78.75\n78.55\n79.03\n78.76\n79.10\n78.99\n79.45\n78.68\n79.32\n79.12\n78.50": "74.41\n74.26\n74.33\n74.64\n74.54\n74.36\n74.76\n74.82\n75.55\n74.84\n74.74\n74.88\n74.34\n74.02\n73.89\n74.44"
        },
        {
          "0.0": "0.7",
          "100\n150\n200\n250": "100\n150\n200\n250",
          "77.85\n78.02\n77.87\n78.15\n79.52\n78.75\n78.55\n79.03\n78.76\n79.10\n78.99\n79.45\n78.68\n79.32\n79.12\n78.50": "74.33\n73.90\n74.30\n74.18\n74.43\n73.98\n74.77\n73.69\n74.78\n74.28\n74.09\n74.10\n72.13\n73.34\n73.23\n74.02"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modalities": "",
          "Conversation\nSpeaker A\nSpeaker B": "",
          "Emotion": "",
          "Prediction": "CCA"
        },
        {
          "Modalities": "u1\nu2\nu3\nu4\nu5\nu6",
          "Conversation\nSpeaker A\nSpeaker B": "I've got an idea, but what's the story?\nI'm gonna ask her to marry me.\nWell, that's only your business, Chris.\nYou know it's not only my business.\nWhat do you want me to do?  You're old enough to know your \nown mind.\nSo it's all right then?",
          "Emotion": "Sadness\nNeutral\nNeutral\nNeutral\nAnger\nNeutral",
          "Prediction": "Neutral\nNeutral\nAnger\nNeutral\nNeutral\nNeutral"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mcsff: Multi-modal consistency and specificity fusion framework for entity alignment",
      "authors": [
        "W Ai",
        "W Deng",
        "H Chen",
        "J Du",
        "T Meng",
        "Y Shou"
      ],
      "year": "2024",
      "venue": "Mcsff: Multi-modal consistency and specificity fusion framework for entity alignment",
      "arxiv": "arXiv:2410.14584"
    },
    {
      "citation_id": "2",
      "title": "Seg: Seeds-enhanced iterative refinement graph neural network for entity alignment",
      "authors": [
        "W Ai",
        "Y Gao",
        "J Li",
        "J Du",
        "T Meng",
        "Y Shou",
        "K Li"
      ],
      "year": "2024",
      "venue": "Seg: Seeds-enhanced iterative refinement graph neural network for entity alignment",
      "arxiv": "arXiv:2410.20733"
    },
    {
      "citation_id": "3",
      "title": "Graph contrastive learning via cluster-refined negative sampling for semi-supervised text classification",
      "authors": [
        "W Ai",
        "J Li",
        "Z Wang",
        "J Du",
        "T Meng",
        "Y Shou",
        "K Li"
      ],
      "year": "2024",
      "venue": "Graph contrastive learning via cluster-refined negative sampling for semi-supervised text classification",
      "arxiv": "arXiv:2410.18130"
    },
    {
      "citation_id": "4",
      "title": "Contrastive multi-graph learning with neighbor hierarchical sifting for semi-supervised text classification",
      "authors": [
        "W Ai",
        "J Li",
        "Z Wang",
        "Y Wei",
        "T Meng",
        "Y Shou",
        "K Lib"
      ],
      "year": "2024",
      "venue": "Contrastive multi-graph learning with neighbor hierarchical sifting for semi-supervised text classification",
      "arxiv": "arXiv:2411.16787"
    },
    {
      "citation_id": "5",
      "title": "Der-gcn: Dialog and event relation-aware graph convolutional neural network for multimodal dialog emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "6",
      "title": "2023a. Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "N Yin",
        "K Li"
      ],
      "venue": "2023a. Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "arxiv": "arXiv:2312.10579"
    },
    {
      "citation_id": "7",
      "title": "Edgeenhanced minimum-margin graph attention network for short text classification",
      "authors": [
        "W Ai",
        "Y Wei",
        "H Shao",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "8",
      "title": "2023b. A two-stage multimodal emotion recognition model based on graph contrastive learning",
      "authors": [
        "W Ai",
        "F Zhang",
        "T Meng",
        "Y Shou",
        "H Shao",
        "K Li"
      ],
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "9",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "G Andrew",
        "R Arora",
        "J Bilmes",
        "K Livescu"
      ],
      "year": "2013",
      "venue": "International conference on machine learning, PMLR"
    },
    {
      "citation_id": "10",
      "title": "Hypergraph convolution and hypergraph attention",
      "authors": [
        "S Bai",
        "F Zhang",
        "P Torr"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Greedy layer-wise training of deep networks",
      "authors": [
        "Y Bengio",
        "P Lamblin",
        "D Popovici",
        "H Larochelle"
      ],
      "year": "2007",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Beyond low-frequency information in graph convolutional networks",
      "authors": [
        "D Bo",
        "X Wang",
        "C Shi",
        "H Shen"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "13",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "14",
      "title": "Deep adversarial learning for multi-modality missing data completion",
      "authors": [
        "L Cai",
        "Z Wang",
        "H Gao",
        "D Shen",
        "S Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "15",
      "title": "Proceedings of the 13th international workshop on semantic evaluation",
      "authors": [
        "A Chatterjee",
        "K Narahari",
        "M Joshi",
        "P Agrawal"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th international workshop on semantic evaluation"
    },
    {
      "citation_id": "16",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "F Chen",
        "J Shao",
        "S Zhu",
        "H Shen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Fusing pairwise modalities for emotion recognition in conversations",
      "authors": [
        "C Fan",
        "J Lin",
        "R Mao",
        "E Cambria"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "19",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Relations between two sets of variates",
      "authors": [
        "H Hotelling"
      ],
      "year": "1992",
      "venue": "Breakthroughs in statistics"
    },
    {
      "citation_id": "21",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Gcnet: Graph completion network for incomplete multimodal learning in conversation"
    },
    {
      "citation_id": "23",
      "title": "Completer: Incomplete multi-view clustering via contrastive prediction",
      "authors": [
        "Y Lin",
        "Y Gou",
        "Z Liu",
        "B Li",
        "J Lv",
        "X Peng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "2021a. An efficient approach for audio-visual emotion recognition with missing labels and missing modalities",
      "authors": [
        "F Ma",
        "S Huang",
        "L Zhang"
      ],
      "venue": "2021 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "25",
      "title": "2021b. Maximum likelihood estimation for multimodal learning with missing modality",
      "authors": [
        "F Ma",
        "X Xu",
        "S Huang",
        "L Zhang"
      ],
      "venue": "2021b. Maximum likelihood estimation for multimodal learning with missing modality",
      "arxiv": "arXiv:2108.10513"
    },
    {
      "citation_id": "26",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "2024a. A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "J Du",
        "H Liu",
        "K Li"
      ],
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "T Meng",
        "F Zhang",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    },
    {
      "citation_id": "30",
      "title": "Masked graph learning with recurrent alignment for multimodal emotion recognition in conversation",
      "authors": [
        "T Meng",
        "F Zhang",
        "Y Shou",
        "H Shao",
        "W Ai",
        "K Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "31",
      "title": "2021. I-gcn: Incremental graph convolution network for conversation emotion detection",
      "authors": [
        "W Nie",
        "R Chang",
        "M Ren",
        "Y Su",
        "A Liu"
      ],
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "33",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "H Pham",
        "P Liang",
        "T Manzini",
        "L Morency",
        "B PÃ³czos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "34",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "M Schlichtkrull",
        "T Kipf",
        "P Bloem",
        "Van Den",
        "R Berg",
        "I Titov",
        "M Welling"
      ],
      "year": "2018",
      "venue": "European Semantic Web Conference"
    },
    {
      "citation_id": "36",
      "title": "Multiemo: An attention-based correlationaware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "T Shi",
        "S Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "2024a. Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "authors": [
        "Y Shou",
        "W Ai",
        "J Du",
        "T Meng",
        "H Liu"
      ],
      "venue": "2024a. Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "arxiv": "arXiv:2407.00119"
    },
    {
      "citation_id": "38",
      "title": "2023a. Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "K Li"
      ],
      "venue": "2023a. Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "39",
      "title": "Graph information bottleneck for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "N Yin"
      ],
      "year": "2023",
      "venue": "Graph information bottleneck for remote sensing segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "40",
      "title": "Graphunet: Graph make strong encoders for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "F Zhang",
        "K Li"
      ],
      "year": "2023",
      "venue": "2023 IEEE 29th International Conference on Parallel and Distributed Systems (ICPADS)"
    },
    {
      "citation_id": "41",
      "title": "Masked contrastive graph representation learning for age estimation",
      "authors": [
        "Y Shou",
        "X Cao",
        "H Liu",
        "D Meng"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "42",
      "title": "2024b. Spegcl: Self-supervised graph spectrum contrastive learning without positive samples",
      "authors": [
        "Y Shou",
        "X Cao",
        "D Meng"
      ],
      "venue": "2024b. Spegcl: Self-supervised graph spectrum contrastive learning without positive samples",
      "arxiv": "arXiv:2410.10365"
    },
    {
      "citation_id": "43",
      "title": "Contrastive graph representation learning with adversarial cross-view reconstruction and information bottleneck",
      "authors": [
        "Y Shou",
        "H Lan",
        "X Cao"
      ],
      "year": "2024",
      "venue": "Contrastive graph representation learning with adversarial cross-view reconstruction and information bottleneck",
      "arxiv": "arXiv:2408.00295"
    },
    {
      "citation_id": "44",
      "title": "A lowrank matching attention based cross-modal feature fusion method for conversational emotion recognition",
      "authors": [
        "Y Shou",
        "H Liu",
        "X Cao",
        "D Meng",
        "B Dong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Object detection in medical images based on hierarchical transformer and mask mechanism",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "C Xie",
        "H Liu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "46",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "47",
      "title": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "Adversarial representation with intra-modal and inter-modal graph contrastive learning for multimodal emotion recognition",
      "arxiv": "arXiv:2312.16778"
    },
    {
      "citation_id": "48",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "49",
      "title": "2024e. Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "venue": "Information Fusion"
    },
    {
      "citation_id": "50",
      "title": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "authors": [
        "Y Shou",
        "T Meng",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "51",
      "title": "2024g. Graph domain adaptation with dual-branch encoder and two-level alignment for whole slide image-based survival prediction",
      "authors": [
        "Y Shou",
        "P Yan",
        "X Yuan",
        "X Cao",
        "Q Zhao",
        "D Meng"
      ],
      "venue": "2024g. Graph domain adaptation with dual-branch encoder and two-level alignment for whole slide image-based survival prediction",
      "arxiv": "arXiv:2411.14001"
    },
    {
      "citation_id": "52",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "53",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Missing modalities imputation via cascaded residual autoencoder",
      "authors": [
        "L Tran",
        "X Liu",
        "J Zhou",
        "R Jin"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference"
    },
    {
      "citation_id": "56",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Extracting and composing robust features with denoising autoencoders",
      "authors": [
        "P Vincent",
        "H Larochelle",
        "Y Bengio",
        "P Manzagol"
      ],
      "year": "2008",
      "venue": "Proceedings of the 25th international conference on Machine learning"
    },
    {
      "citation_id": "58",
      "title": "Partial multi-view clustering via consistent gan",
      "authors": [
        "Q Wang",
        "Z Ding",
        "Z Tao",
        "Q Gao",
        "Y Fu"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "59",
      "title": "On deep multiview representation learning",
      "authors": [
        "W Wang",
        "R Arora",
        "K Livescu",
        "J Bilmes"
      ],
      "year": "2015",
      "venue": "International conference on machine learning, PMLR"
    },
    {
      "citation_id": "60",
      "title": "Distribution-consistent modal recovering for incomplete multimodal learning",
      "authors": [
        "Y Wang",
        "Z Cui",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "61",
      "title": "Emotion recognition in conversation based on a dynamic complementary graph convolutional network",
      "authors": [
        "Z Yang",
        "X Li",
        "Y Cheng",
        "T Zhang",
        "X Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "62",
      "title": "Prediction model of dow jones index based on lstm-adaboost",
      "authors": [
        "R Ying",
        "Y Shou",
        "C Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "63",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "64",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "65",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "66",
      "title": "Deep partial multi-view learning",
      "authors": [
        "C Zhang",
        "Y Cui",
        "Z Han",
        "J Zhou",
        "H Fu",
        "Q Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "67",
      "title": "A multi-level alignment and cross-modal unified semantic graph refinement network for conversational emotion recognition",
      "authors": [
        "X Zhang",
        "W Cui",
        "B Hu",
        "Y Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "68",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "J Zhao",
        "R Li",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    }
  ]
}