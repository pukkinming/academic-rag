{
  "paper_id": "2404.09113v3",
  "title": "Extending Mean-Field Variational Inference Via Entropic Regularization: Theory And Computation",
  "published": "2024-04-14T01:40:11Z",
  "authors": [
    "Bohan Wu",
    "David Blei"
  ],
  "keywords": [
    "Variational inference",
    "optimal transport",
    "mean-field approximation",
    "statisticalcomputational tradeoff",
    "high-dimensional Bayesian inference"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Variational inference (VI) has emerged as a popular method for approximate inference for high-dimensional Bayesian models. In this paper, we propose a novel VI method that extends the naive mean field via entropic regularization, referred to as Ξ-variational inference (Ξ-VI). Ξ-VI has a close connection to the entropic optimal transport problem and benefits from the computationally efficient Sinkhorn algorithm. We show that Ξ-variational posteriors effectively recover the true posterior dependency, where the likelihood function is downweighted by a regularization parameter. We analyze the role of dimensionality of the parameter space on the accuracy of Ξ-variational approximation and the computational complexity of computing the approximate distribution, providing a rough characterization of the statistical-computational trade-off in Ξ-VI, where higher statistical accuracy requires greater computational effort. We also investigate the frequentist properties of Ξ-VI and establish results on consistency, asymptotic normality, high-dimensional asymptotics, and algorithmic stability. We provide sufficient criteria for our algorithm to achieve polynomialtime convergence. Finally, we show the inferential benefits of using Ξ-VI over mean-field VI and other competing methods, such as normalizing flow, on simulated and real datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Variational inference (VI) is a widely used method for approximate probabilistic inference. VI approximates a difficult-to-compute distribution by positing a family of simpler distributions and minimizing the KL divergence between the family and the target. In Bayesian modeling, the target is a posterior distribution of latent variables given observations p(θ | x) and the variational family is of distributions of the latent variables q(θ) ∈ Q(Θ). VI approximates the posterior with q * (θ) = arg min q∈Q D kl (q(θ) ∥ p(θ | x)) .\n\n(1.1)\n\nTo set up variational inference, we need to select the family of distributions over which to optimize. In many applications, practitioners use the mean-field or fully factorized family. This is the family of product distributions, where each variable is independent and endowed with its own distributional factor. Consider a model with D latent variables θ = {θ 1 , . . . , θ D }. The corresponding mean-field family is q(θ) = D i=1 q i (θ i ),\n\n(1.2)\n\nwhere each q i (θ i ) is the variational factor for θ i . Thanks to this simple family, the variational optimization is computationally efficient (to a local optimum). But this efficiency comes at a cost. Mean-field VI suffers in accuracy because it cannot capture posterior dependencies between the elements of θ  [Blei et al., 2017] .\n\nIn this paper, we develop a new way of doing variational inference. The idea is to optimize over all distributions of the latent variables, i.e., q ∈ P(Θ), but to regularize the variational objective function to encourage simpler distributions that are \"more like the mean-field.\" At one end of the regularization path we effectively optimize over the meanfield family, providing traditional mean-field VI (MFVI). At the other end we optimize over all distributions, providing exact inference (but at prohibitive cost). Between these extremes, our method smoothly trades off efficiency and accuracy.\n\nIn detail, consider a probabilistic model p(θ, x) = p(θ)p(x | θ) and the goal to approximate the posterior p(θ | x). Denote the prior π(θ) := p(θ) and the log likelihood ℓ(x ; θ) := log p(x | θ). We propose to approximate the posterior by optimizing an expressivity-regularized variational objective over the entire space of distributions q ∈ P(Θ).\n\nTake an arbitrary distribution q(θ) with marginal distributions denoted q j (θ j ). We define the expressivity functional as the KL divergence between q(θ) and the product of its marginals: Ξ(q) = D kl q(θ) ∥ D i=1 q i (θ i ) .\n\n(1.3) Expressivity measures the dependence among the D latent variables under q(θ). In the language of information theory, it is the multivariate mutual information of θ ∼ q(θ)  [Cover and Thomas, 2006 ]. Intuitively, it quantifies how \"un-mean-field\" the distribution q is. A larger Ξ(q) indicates that the distribution is further from a factorized distribution. We define Ξ-variational inference (Ξ-VI) as an expressivity-regularized optimization problem: q * λ (θ) = arg max q∈P(Θ)\n\nE q [ℓ(x ; θ)] -D kl (q(θ) ∥ π(θ))\n\nELBO(q) -λ Ξ(q)\n\nExpressivity penalty\n\n.\n\n(1.4)\n\nThe first two terms comprise the evidence lower bound (ELBO)  [Jordan et al., 1999 , Blei et al., 2017] , which is the usual objective functon for variational inference. When optimized relative to the full set of distributions of θ, maximizing the ELBO recovers the exact posterior  [Zellner, 1988 , Knoblauch et al., 2022] . The third term, however, is a penalty term. It encourages the optimal q to resemble a product distribution, i.e., a member of the mean-field family. By varying λ > 0, we interpolate between the exact posterior and its mean-field approximation.\n\nWe will study the theory and application of Eq. (1.4), which we call Ξ-VI (pronounced \"ksee VI\"). First we show that we can solve this optimization by iterating between (1) calculating approximate posterior marginals for each variable and (2) solving a problem of entropic optimal transport (EOT) with a multi-marginal Sinkhorn algorithm  [Cuturi, 2013 , Lin et al., 2022] . We then develop expressivity-corrected mean field. It first approximates marginals using traditional VI (e.g., black-box VI  [Ranganath et al., 2014]  or expectation propagation  [Minka, 2013] ), and then optimizes Eq. (1.4) with the Sinkhorn algorithm to model dependencies in the variational approximation.\n\nWe prove that Ξ-VI gives frequentist guarantees including posterior consistency and a Bernstein-von Mises theorem. Further, we theoretically characterize how to choose the regularization parameter λ to balance accuracy and efficiency. Specifically, we characterize the regions of possible λ values where the resulting variational approximation is either mean-field or Bayes-optimal.\n\nEmpirically, we apply Ξ-VI correction to multivariate Gaussians, linear regression with a Laplace prior, and hierarchical Bayesian modeling. The results demonstrate the competitive performance of Ξ-VI over other variational inference methods, including mean-field and fullrank ADVI  [Kucukelbir et al., 2017] , normalizing flow  [Rezende and Mohamed, 2015] , and Stein variational gradient descent  [Liu and Wang, 2016] . To set the regularization strength λ, our empirical findings suggest that λ = D is a reasonable choice, marking a phase transition between the computationally efficient and statistically accurate regimes.\n\nThe rest of the paper is organized as follows. Section 2 introduces Ξ-VI and the Ξ-VI correction algorithm. Section 3 provides an empirical study. Section 4 establishes theoretical guarantees for the Ξ-variational posterior, including posterior consistency, Bernstein-von Mises theorem, high-dimensional bounds, finite-sample convergence, and algorithmic stability. Section 5 concludes the paper with a discussion of limitations and further research.\n\nRelated Work. This paper proposes Ξ-VI, a new way to relax the mean-field assumption in variational inference. With this new algorithm, we also add to two existing areas of VI research: statistical guarantees and computational guarantees.\n\nMean-field VI is efficient, but it also has limitations. It poorly approximates posteriors in settings such as multivariate Gaussian models  [Blei et al., 2017] , state-space models  [Wang and Titterington, 2004] , piecewise-constant models  [Zhang and Gao, 2020] , and spike covariance models  [Ghorbani et al., 2019] . To address these shortcomings, researchers have proposed a variety of solutions, including structural VI  [Xing et al., 2012 , Ranganath et al., 2016] , copula-based methods  [Tran et al., 2015 [Tran et al., , 2017]] , linear response corrections  [Giordano et al., 2018, Raymond and Ricci-Tersenghi, 2017] , TAP corrections  [Opper and Saad, 2001 , Fan et al., 2021 , Celentano et al., 2023a,b] , and variational boosting  [Miller et al., 2017 , Locatello et al., 2018] . Our method makes a contribution to this landscape of research, providing a principled and theoretically supported approach to capture dependencies among latent variables and to manage the statistical-computation tradeoff.\n\nSeveral lines of recent research examine the statistical properties of VI approximations. This work includes results on asymptotic normality  [Hall et al., 2011b ,a, Bickel et al., 2013 , Wang and Blei, 2019] , posterior contraction rates  [Zhang and Gao, 2020, Zhang and Zhou, 2020] , finite-sample bounds  [Alquier et al., 2016 , Alquier and Ridgway, 2020 , Yang et al., 2020] , and performance in high-dimensional settings  [Basak and Mukherjee, 2017 , Ray et al., 2020 , Ray and Szabó, 2022 , Mukherjee and Sen, 2022 , Mukherjee et al., 2023 , Qiu, 2024] . We contribute to this research by proving frequentist guarantees-posterior consistency and a Bernstein-von Mises theorem-for our proposed class of variational approximations.\n\nOther research examines computational aspects of VI, including convergence rates for coordinate ascent methods  [Mukherjee and Sarkar, 2018 , Plummer et al., 2020 , Zhang and Zhou, 2020 , Xu and Campbell, 2022 , Bhattacharya et al., 2023] , black-box optimization  [Kim et al., 2023] , and the trade-off between statistical accuracy and computational complexity  [Bhatia et al., 2022] . Related work also analyzes VI through gradient flow techniques  [Yao and Yang, 2022 , Lambert et al., 2022 , Diao et al., 2023 , Jiang et al., 2023] . Our paper contributes to these computational analyses by explicitly characterizing the trade-off of accuracy for computational simplicity. We also expand the interface between VI and optimal transport, in using entropic optimal transport methods  [Cuturi, 2013 , Lin et al., 2022]  in VI optimization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ξ-Variational Inference",
      "text": "Again, we consider a general probabilistic model p(θ, x) = π(θ) exp{ℓ(x ; θ)},\n\n(2.1)\n\nwhere π(θ) is the prior of the unknown parameter and ℓ(x ; θ) is the log likelihood of the data under θ. We assume the prior π to be a product distribution of the form π(θ) = d i=1 π i (θ i ). Our goal is to approximate the posterior p(θ | x).\n\nIn this section, we formally define Ξ-VI and analyze its structure. We reformulate Ξ-VI as a nested optimization, separating the problem into an outer optimization over marginals and an inner optimization over their couplings, i.e., a representation of the dependency structure in the variational approximation. We focus on solving the inner optimization, showing how to correct mean-field (factorized) solutions using entropic optimal transport (EOT). We present a computationally practical algorithm and discuss its interpretation.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "The Ξ Variational Objective And Its Nested Formulation",
      "text": "We aim to find the distribution q * λ that solves the problem in Eq. (1.4). In this problem, λ ≥ 0 is a user-defined regularization parameter, and the optimal q * λ (θ) is called the Ξvariational posterior. When it is not unique, q * λ is one of the optimizers of Eq. (1.4). We make two observations about the Ξ-VI problem: (1) When λ = 0, q * 0 is the exact posterior. When λ = ∞, q * ∞ is a mean-field variational posterior.\n\n(2) By the standard duality theory, the Ξ-VI problem is equivalent to optimizing the standard ELBO over a neighborhood of the mean-field family: q * λ = arg max q∈P(Θ):Ξ(q)≤δ ELBO (q) .\n\nThe Ξ-VI posterior is the distribution over the latent variables closest to the posterior, but within the neighborhood of expressivity.\n\nWe can rewrite Ξ-VI as a nested minimization problem. Let m i (θ i ) denote a marginal distribution of θ i and let M(Θ) denote the space of product distributions over Θ,\n\n(2.2)\n\nGiven a set of D marginals let C (m 1 , . . . , m D ) denote the set of D-dimensional joint distributions where m j (θ j ) is the j th marginal,\n\n(2.\n\n3)\n\nAs shorthand, we write C(m) as the set of couplings over the marginal distributions of m(θ).\n\nNote the set C(m) is convex and closed in the Wasserstein distance  [Nutz, 2021] , and we assume that there exists q ∈ C(m) with finite (Boltzmann) entropy. With these definitions in place, we write Ξ-VI as a double minimization problem, min\n\n(2.4)\n\nThe equation follows from expressing the minimization set P(Θ) as {q ∈ C(m), m ∈ M(Θ)}, while the objective stays the same. In Eq. (2.4), the outer variational problem minimizes the objective with respect to the space of marginal distributions. Given a set of marginals, the inner variational problem minimizes the objective over its set of couplings. Here we will focus on the inner variational problem. Given fixed marginal distributions-such as those produced by mean-field VIthe inner problem finds the optimal coupling that corrects these marginals, i.e., the λregularized optimal dependencies between the variables. We show this problem is solvable using entropic optimal transport tools. Our method improves a given mean-field solution to capture dependencies in the latent variables and better approximate the posterior.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Expressivity-Corrected Mean-Field Vi",
      "text": "We now derive an algorithm to correct mean-field variational inference using Ξ-VI. We fix the solution to the outer variational problem with a product distribution m(θ), obtained from mean-field VI or another approximate method. We then solve Eq. (2.4) for the optimal distribution q(θ) that matches these marginals, i.e., by solving the inner variational problem with respect to the coupling q ∈ C(m). As we will see, we can optimize over the set of couplings using tools from entropic optimal transport (EOT)  [Villani, 2009 , Nutz, 2021] .\n\nSolving the inner variational problem with the Sinkhorn algorithm. We view the inner variational problem as an EOT problem  [Nutz, 2021] . Again, we fix m and optimize q. A simple calculation (in Section C) shows that\n\n(2.5)\n\nTheorem 9 (Section C) shows the unique solution to Eq. (2.5) has the following form:\n\nwhere each ϕ * i : Θ i → R is a measurable function, called an EOT potential. The set of EOT potentials ϕ * := (ϕ * 1 , • • • , ϕ * D ) are identifiable up to an additive constant. So we identify the solution by imposing D-1 constraints that each one has mean-zero under the marginal,\n\nLet E(m) denote the space of ϕ such that Eq. (2.7) holds. We find the optimal potentials from Eq. (2.6) by maximizing the Lagrangian dual problem,\n\nSee Section C for the derivation. Finally, we solve Eq. (2.8) with a block coordinate ascent algorithm called the Sinkhorn algorithm  [Cuturi, 2013] . Given the marginals m t at time t, the Sinkhorn algorithm iteratively updates each ϕ i ,\n\nwhere\n\nTo solve for Eq. (2.9), the update has an explicit formula:\n\nwhere\n\nThe updated EOT potentials satisfy the identifiability constraints (2.7). The solution q * λ calculated with these EOT potentials is a valid probability distribution.\n\nIn practice, the expectations required of Eq. (2.10) might be difficult to compute. In our algorithm, we approximate them with Monte Carlo.\n\nSpecifically, given samples\n\nwe approximate ϕ t+1 i with the Monte Carlo estimate φt+1 i :  (2.11)  where\n\nand\n\nOne-step expressivity-corrected mean-field VI. Algorithm 1 implements the entropic correction in a single round of updates. In the first stage, it computes a set of pseudomarginals { mi } i∈  [D]  , and draws samples from them. In the second stage, it uses those samples in a multi-marginal Sinkhorn algorithm to compute the optimal EOT coupling.\n\nAlgorithm 1: Expressivity-corrected mean-field variational inference Input:\n\nCompute marginals from a mean-field algorithm m1 , • • • , mD ← mean field inference(x, ℓ, π).\n\nEqs. (2.9) and (2.10)\n\nNote that in the first stage, we can use any algorithm for approximating the posterior marginals, e.g., variational inference  [Blei et al., 2017] , expectation propagation (EP)  [Minka, 2013] , or MCMC  [Robert and Casella, 2004] . Ideally, the first step of Algorithm 1 would produce accurate estimates of the marginals of the exact posterior.\n\nIn practice, we recommend using an approximate method that yields overdispersed marginals, such as EP, because the additional variability often improves downstream coupling approximations. Intuitively, it produces more variation in the initial samples of\n\nfor the Monte Carlo step (2.11), which leads to better downstream approximations. We demonstrate this empirically in Section 3.\n\nAlgorithm 1 only outputs an approximate solution to the full Ξ-VI problem in Eq. (2.4). However, by coupling the marginals m1 , • • • , mD , the final estimate qλ (θ) is guaranteed to be at least as good as the initial approximation m(θ) := D j=1 m j (θ j ) in terms of KL divergence to the exact posterior. The reason is that qλ maximizes the regularized ELBO in Eq. (1.4) over the coupling C( m).\n\nWith a large number of variables, Algorithm 1 is computationally challenging because the complexity of step (2.11) scales exponentially in D. But in Section A, we outline conditions on the likelihood for the algorithm to be polynomial-time solvable. Specifically, we provide polynomial-time complexity guarantees in two settings: (i) graphical models with bounded treewidth, and (ii) models in which the likelihood evaluated at the sample points\n\nforms a low-rank and sparse tensor. In the first setting, we show that the algorithm converges in time polynomial in the dimension D, but exponential in the treewidth and inversely proportional to the regularization parameter λ.",
      "page_start": 5,
      "page_end": 8
    },
    {
      "section_name": "Ξ-Vi Solution And Connection To Generalized Bayes",
      "text": "In this section, we discuss the structure of the Ξ-VI solution and its connection to existing theories of generalized Bayesian methods  [Knoblauch et al., 2022] .\n\nAs shown in Eq. (2.6), the Ξ-VI solution consists of three components: (i) a scaled loglikelihood term, (ii) a set of potential functions {ϕ * λ,i } D i=1 , and (iii) a product of marginals\n\nThe regularization parameter λ controls the temperature of the likelihood term ℓ(x; θ).\n\nIntuitively, λ divides a sample size of n between the true posterior and the mean-field solution by a factor of 1/(λ + 1) and λ/(λ + 1), respectively. It thus quantifies the tradeoff between the likelihood and a product distribution. Higher λ allows the variational posterior to be close to the mean field, while lower λ allows the solution to better approximate the exact posterior (but at computational cost). When λ = 0, the likelihood term is untempered-the variational solution is the exact posterior. When λ = ∞, the solution matches the mean-field variational posterior. The curve of measures {q * λ , λ ∈ R+ } smoothly interpolates between the mean-field variational posterior and the true posterior.\n\nWe can also view q * λ as a nonlinear tilt of the 1/(λ + 1)-tempered posterior  [Miller and Dunson, 2018, Bhattacharya et al., 2019] . Write\n\nThen we can represent q * λ as a nonlinear tilt of the tempered posterior q\n\n(2.12) Wainwright et al.  [2008]  shows that the mean-field variational posterior of the quadratic interaction model amounts to a linear tilting of the prior. Eq. (2.12) extends this result, where f * λ,i (θ i ) is the tilting function.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Examples",
      "text": "We apply Ξ-VI to three statistical models: a multivariate Gaussian model, a high-dimensional Bayesian linear regression, and a hierarchical Bayesian model on the 8-schools data  (Gelman et al. [1995] , Section 5.5).\n\n• In the multivariate Gaussian example, Ξ-VI is explicitly solvable. This example illustrates the limitations of mean-field VI  [Blei et al., 2017] , and demonstrates how Ξ-VI improves it.\n\n• In high-dimensional Bayesian linear regression, mean-field VI produces valid inference under weak covariate interactions  [Mukherjee and Sen, 2022, Mukherjee et al., 2023] , but fail when the interaction among the covariates is strong  [Qiu, 2024 , Celentano et al., 2023a] . Again, Ξ-VI improves on the classical approach.\n\n• Our analysis of the Bayesian hierarchical model shows how Ξ-VI provides more accurate posterior inferences on a real-world dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multivariate Gaussian Distributions",
      "text": "We first apply Ξ-VI to approximating a multivariate Gaussian with the family of all Gaussian distributions. In this demonstration, no algorithm is needed because Ξ-VI admits a closed form solution. In general, it is well known that mean-field VI underestimates the marginal variance of its target posterior  [Blei et al., 2017] . Here we show how Ξ-VI interpolates between the mean-field and the target posterior, and strictly outperforms mean-field VI in covariance estimation. Assume that the exact posterior is a multivariate normal, q * 0 := N (µ 0 , Σ 0 ) with Ddimensional mean vector µ 0 and a D × D full-rank covariance matrix Σ 0 . The Ξ-VI formulation is q * λ = arg min q=N (µ,Σ)\n\nThe next result establishes the self-consistency equations for the Ξ-VI solution and establishes upper and lower bounds for the approximating covariance:\n\nProposition 1. Suppose we solve the Gaussian Ξ-VI problem (3.1) with N (µ 0 , Σ 0 ) the exact posterior and λ > 0. Then the minimizer q * λ = N (µ * , Σ * ) where µ * , Σ * satisfy the following fixed point equations:\n\nFor any matrix norm ∥.∥, the following bounds hold:\n\nThe proof can be found in Section E. Our result shows that Λ * is a convex combination of the true precision Λ 0 and the inverse of the variational marginal variances. As the regularizer λ → ∞, the off-diagonal elements of Λ * converge to 0 while the diagonal elements approach those of Λ 0 .\n\nThe weight λ controls the approximation error of a variational posterior covariance by combining the marginal precisions of the exact posterior and the mean-field precision with weights 1 λ+1 and λ λ+1 , respectively. For any λ < ∞, the Ξ-variational posterior offers a tighter approximation than the naive mean field. λ from closely approximating the exact posterior (at low λ) to resembling the mean-field approximation (at high λ). The right panel shows the covariance between the two normal coordinates versus λ on a log scale. Note that the Ξ-variational approximation to the covariance is very accurate up to a critical λ (≈ 10 -1 ), after which it degrades rapidly to 0.\n\nAs a concrete demonstration of these ideas, we study a bivariate Gaussian posterior. Here, the Ξ-variational posterior has an analytical solution that can be exactly computed (see Proposition 4 in the Section E).\n\nFigure  1illustrates  the interpolation, where the regularization downweights the offdiagonal entries of the precision matrix by a factor of 1/(λ + 1). It shows qλ fitted to a bivariate Gaussian, for different values of λ. The left panel shows qλ as a smooth interpolation between the true posterior and the mean-field variational posterior. Increasing λ smoothly reduces posterior dependence, with a sharp structural change only at λ = ∞. The right panel paints a quantitative picture of this interpolation: when λ ≤ 10 -1 , the Ξ-variational posterior closely approximates the covariance values of the exact bivariate Gaussian posterior. For λ ≥ 10 1 , the covariance is close to zero, which indicates proximity to the mean-field variational posterior. Both plots suggest that qλ undergoes a \"phase transition\" phenomenon at λ ∈ [10 -1 , 10 1 ].",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Bayesian Linear Regression With Laplace Prior",
      "text": "Ξ-VI involves a tradeoff between statistical accuracy and computational complexity: as the regularization increases away from the mean-field solution, the quality of VI approximation improves at the cost of increased computational complexity.\n\nTo study this, we consider a a Bayesian linear model with Laplace prior,\n\nThe Laplace prior has density π(θ i ) =  We simulate a dataset consisting of n = 100 observations and d = 12 features. The true regression coefficients is drawn randomly from a 12-dimensional standard Gaussian distribution, and σ 2 = 1. Columns (1, 2, 3, 8, 9) of X are generated from a standard Gaussian distribution. Then we set each of features  (4, 5, 6, 11, 12)  equal to each of features (1, 2, 3, 8, 9) plus a standard Gaussian noise. This setup aims to simulate realistic multicollinearity. Finally, we generate the response y using model (3.2). With this simulated data, we calculate an \"exact\" posterior with a long-run MCMC algorithm of 3,000 iterations. The MCMC draws produce an R of below 1.01 across coefficients  [Gelman et al., 1995] , which is below the typical threshold of 1.1 for satisfactory mixing.\n\nSince coupling all 12 coefficients is computationally expensive, we couple groups of coefficients in the EOT step. We adopt a naive grouping approach where features (1, 2, 3), (4, 5, 6),  (7, 8, 9), (10, 11, 12)  are grouped together. This effectively reduces the computational cost by reducing a twelve-dimensional coupling problem into a four-dimensional one. While it is beneficial to use an informed grouping, any choice of grouping will improve the approximation accuracy of MFVI. For each dimension, we use M = 20 support points to represent the marginal distributions ( see details in Section A).\n\nWith this simulated data, we use Algorithm 1 to compute the Ξ-VI approximation. In the first step, we use expectation propagation (EP) to compute the pseudomarginals. For the analysis, we chose 100 λ values on a logarithmic scale from 10 -4 to 10 6 , and represented the variational posterior for each λ by 2, 000 sample points. Figure  2(a)  shows the approximation errors of Ξ-VI as a function of λ, measured using the Wasserstein distance (W 2 ). These distances are computed between the posterior distributions sampled via MCMC and those obtained from Ξ-VI. The Ξ-VI approximation errors are benchmarked against the baseline errors of EP at λ = ∞, mean-field VI, and the theoretical lower bound at λ = 0. A vertical line at λ = D, the number of features, marks an inflection point where the posterior variational approximation error transitions from rapidly converging to the EP error (λ ≤ D) to relatively stable (λ > D). shows the runtime of the approximate coordinate ascent algorithm for Laplace linear regression, measured in the number of iterations until convergence. The λ values are shown on a logarithmic scale to highlight the performance over several orders of magnitude. The plot shows a sharp decline for λ ≤ D before it becomes stable at λ > D. The inflection in both the approximation error and runtime plots suggests that a regularization strength around λ = D offers an optimal balance in the tradeoff between approximation accuracy and computational complexity.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "Hierarchical Model",
      "text": "The 8-schools model  (Gelman et al. [1995] , Section 5.5) is a classical example of a hierarchical Bayesian model. Each of the 8 schools run a randomized trial to assess the effect of tutoring on a standardized test. Each school provides separate estimates for the mean y i and standard deviation σ i of their respective treatment effects.\n\nLet θ j be the treatment effect in school j. We treat the outcomes from each school as independent:\n\nwhere µ and τ are the global parameters common to all schools, θ j is a local parameter specific to school j. The target of posterior inference are {θ j } 8 j=1 , µ and τ 2 . To match the Ξ-VI formulation in Section 2.2, we define z j := (θ j -µ)/τ and rewrite the model as follows: y j | µ, z j , τ ∼ N (µ + τ z j , σ 2 j ), z j ∼ N (0, 1), µ ∼ N (0, 5), τ ∼ halfCauchy(0, 5).\n\n(3.4)\n\nThis reparameterization transforms the joint prior of z j 's, µ, and τ into a product distribution. We apply Algorithm 1 to solve the Ξ-VI problem for this model, expressed as:\n\nIn the first step, we use automatic differentiation variational inference (ADVI,  [Kucukelbir et al., 2017 , Carpenter et al., 2015] ) to compute a set of pseudomarginals. In the second step, we use the Sinkhorn algorithm to solve the EOT problem:\n\nThe problem (3.6) is solvable in polynomial time using the Sinkhorn algorithm, as detailed in Proposition 2 of Section A. Assumption 1 of Proposition 2 is upheld due to efficient storage of the cost tensor as third-order tensors. Ultimately, we derive the joint distribution q * λ (θ 1 , • • • , θ 8 , µ, τ ) by setting θ j = µ + τ z j based on the optimal coupling in (3.6).\n\nTo benchmark the performance of our VI methods, we compute the true posterior using MCMC draws with 4 chains for 1000 tune and 5000 draw iterations. For each of the VI methods, we represent the approximate posterior with 10, 000 sample points. Ξ-VI captures the dependency among the variables in the posterior. Figure  3  compares the strength of association between θ 1 and θ 7 under the true posterior, mean-field variational posterior and Ξ-variational posteriors when λ ∈ {0, 1, 10, 1000}. The true posterior shows a strong positive correlation between θ 1 and θ 7 , which is effectively captured by Ξ-VI at small λ. As λ increases, the correlation decreases to the MFVI level that attains a slope estimate of 0.19.\n\nΞ-VI excels in inference that involves multiple variables in the posterior. Figure  4  illustrates credible intervals for maximum and minimum treatment effects across schools, comparing Ξ-VI with MFVI, normalizing flow variational inference (NFVI), Stein variational gradient descent (SVGD), and full-rank ADVI. Ξ-VI achieves more accurate interval width and coverage accuracy for both max and min treatment effects compared to other VI methods. Specifically, for the maximum treatment effect, while MFVI, NFVI, and fullrank ADVI produce overly large or small intervals, SVGD results in overly small intervals. In contrast, Ξ-VI closely approximates the true 95% posterior credible interval. For the minimum treatment effect, none of the VI methods precisely capture the true posterior interval. MFVI, NFVI, and full-rank ADVI produce intervals with a downward-shifted center, SVGD offers considerably undersized intervals, and Ξ-VI generates reasonably-sized intervals with less downward shift compared to MFVI. Now we show the computation-statistical tradeoff of Ξ-VI in the 8-schools model. We evaluate our procedure on 100 λ values logarithmically spaced from 10 -3 to 10 5 . Figure  5(a)  illustrates the approximation errors of the Ξ-variational posterior relative to the exact posterior, measured using KL divergence and W 2 distance. These errors are benchmarked against those of MFVI at λ = ∞ and a theoretical lower bound at λ = 0. A vertical line at λ = D = 10 marks a critical transition: errors remain relatively stable for λ < 1 and approach MFVI for λ ≥ 100. Notably, the normalizing flow VI also peforms a reasenable well for this model and matches the performance of Ξ-VI at λ = 1 in W 2 distance and at λ = D in KL distance. Figure  5 (b) shows the runtime of Algorithm 1 for the 8-schools model, measured in the number of iterations to reduce the Sinkhorn error (Algorithm 2) below 10 -4 . The regularization strength λ is plotted on a logarithmic scale. The plot shows a sharp decline right before and right after λ = D. The phase transition in both plots confirms that a choice of λ = D offers a balance in the tradeoff between computational efficiency and approximation accuracy. However, a computational-statistical gap exists in this model: while λ < 1 yields a closer approximation to the exact posterior, optimal runtime is only achieved for λ > 10.\n\nFinally, note that in the 8-schools model, the MFVI produces overdispersed results after we apply the reparametrization. Generally, we recommend using overdispersed pseudomarginals in Algorithm 1. The advantage comes from an intuitive understanding of the one-step EOT correction: it seeks overlaps between the pseudomarginals and the exact posterior to effectively capture the dependency information present in the exact posterior. When the pseudomarginals are underdispersed, the one-step EOT correction still leads to underdispersed samples. With overdispersed pseudomarginals, the one-step EOT coupling compensates for the overdispersion by subsampling points from the marginals that reflect the dependency structure of the exact posterior distributions, as seen in Figure  3 .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Asymptotic Theory",
      "text": "In this section, we study the asymptotic theory of Ξ-variational posterior q * λn in two regimes. • In the fixed-dimensional setting (Section 4.1), we prove standard frequentist guarantees for inference under Ξ-VI such as consistency and a Bernstein-von Mises (BvM) theorem, establishing that q * λn converges to a Gaussian distribution as the sample size grows, under standard regularity conditions.\n\n• In the high-dimensional regime where the parameter dimension D increases with n (Section 4.2), we characterize the behavior of q * λ n beyond the mean-field setting and characterize the choices of λ n for which Ξ-VI behaves like the exact posterior inference or mean-field VI.\n\nWe define some useful notations for the theory. Let P p (Θ) := {q ∈ P(Θ) : E q [∥θ∥ p ] < ∞}. For p ≥ 1, the (p th )-Wasserstein distance is defined as W p (q 0 , q 1 ) := (inf q∈C(q 0 ,q 1 ) E q [∥X-Y ∥ p ]) 1/p . The space (P 2 (Θ), W 2 ) forms a metric space  [Villani, 2009] . We denote BW(R D ) as the subspace of P 2 (R D ) consisting of Gaussian distributions, known as the Bures-Wasserstein space  [Bhatia et al., 2019] .\n\nWe assume that observations X 1 , . . . , X n iid ∼ P θ 0 for a true parameter θ 0 ∈ Θ, and we assume that the exact posterior q * 0 belongs to the space (P 2 (Θ), W 2 ). We also make explicit the dependence on n of the regularizer λ n and the data x (n) . Under this setup, the Ξ-variational posterior is given by q * λn = arg min",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Asymptotic Normality In Finite Dimension",
      "text": "The first part of the theory deals with posterior consistency and asymptotic normality of Ξvariational posteriors for finite-dimensional models. The asymptotic normality results state that depending on the limit of λ n , Ξ-variational posterior converges in the limit to one of three quantities: the mean-field minimizer of a normal distribution, the normal distribution itself, or a Ξ-variational normal approximation.\n\nFor two positive sequences a n and b n , we write\n\nWe make the following assumptions.\n\nAssumption 1 (Prior Mass). The prior π has Lebesgue-density π(θ) := exp(ν 0 (θ)), where the function ν 0 : Θ → R is twice continuously differentiable and bounded in a neighborhood of θ 0 . For some C > 0, we have\n\nAssumption 2 (Consistent Testability Assumptions). For every ϵ > 0, there exists a sequence of tests ϕ n such that\n\nAssumption 3 (Local Asymptotic Normality (LAN) Assumptions). For every compact set K ⊂ R D , there exists random vectors ∆ n,θ 0 bounded in probability and nonsingular matrix\n\nwhere δ n is a D × D diagonal matrix. For D = 1, we commonly have\n\nThe first assumption ensures the prior is light-tailed. It is satisfied by, for example, the flat prior or the sub-Gaussian prior.\n\nThe second assumption guarantees the existence of a sequence of uniformly consistent tests for H 0 : θ = θ 0 versus H 1 : ∥θ -θ 0 ∥ 2 ≥ ϵ based on the data. This condition is satisfied if there exists a consistent sequence of estimators T n for θ and set ϕ n (θ) := I{T n -θ ≥ ϵ/2}, or when the Hellinger distance between {p θ , ∥θ -θ 0 ∥ 2 ≥ ϵ} and p θ 0 is lower bounded by some positive constant δ  [Ghosal and van der Vaart, 2017] .\n\nThe third assumption states that the log-likelihood is locally well-approximated (up to a vanishing error) by that of a normal location model centered at θ 0 under an appropriate rescaling. The rescaling sequence δ n is exactly the posterior contraction rate. In standard finite-dimensional, correctly specified models, we typically have δ n = n -1/2  [Ghosal and van der Vaart, 2017] .\n\nIn line with Assumption 3, we consider a change of variable:\n\nOur first result states that under this change of variable, Ξ-variational posterior satisfies a Bernstein-von-Mises phenomenon with a phase transition.\n\nTheorem 1 (Bernstein von-Mises Theorem). Let qλn be the distribution of the rate-adjusted parameter h defined in Eq. (4.2). The distribution qλn converges in the Wasserstein metric to a normal distribution under the following three regimes:\n\nThe result aligns well with intuition. When λ n diverges, q * λn converges to the meanfield approximation. When λ n approaches zero, the constraint set in the Lagrangian dual problem increases to include the true limiting posterior N (0, V -1 θ 0 ). When λ n converges to some finite value λ ∞ , the Ξ-variational posterior converges to the Gaussian limit of the exact posterior. In the regime where lim n→∞ λ n does not exists but λ n = O(1), the Ξ-variational posterior converges to a a \"biased\" estimate of the true Gaussian posterior N (0, V -1 θ 0 ) along a subsequence of λ n that converges as n → ∞.\n\nThe Bernstein von-Mises Theorem implies the (weak) posterior consistency for q * λn .\n\nCorollary 1. The Ξ-variational posterior is consistent in [P θ 0 ]-probability, i.e. W 2 (q * λn , δ θ 0 )\n\nThe convergence in Corollary 1 is stated in the Wasserstein metric, which is slightly stronger than the typical metric used in posterior consistency results. The convergence in the Wasserstein metric is equivalent to weak convergence plus the convergence of the second moments  (Theorem 5.11, Santambrogio [2015] ). Thus, posterior consistency and the Bernstein-von Mises theorem (Theorem 1) can be framed in terms of the weak convergence and L 2 convergence for the corresponding measures.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Asymptotic Results With Growing Dimension",
      "text": "Motivated by our empirical findings in Section 3.2, which involves a high-dimensional model where the model dimension increases with the sample size, we now develop an asymptotic theory for the Ξ-variational posterior q * λn in the regime where D = D n → ∞ as n → ∞. Our primary objective is to characterize the scaling behavior of the regularization parameter λ n under which q * λn behaves either like a mean-field variational posterior or closely approximates the true posterior. We begin with a theorem for the general model and then focus on the high-dimensional linear regression model. We assume that the parameter space is a unit cube, i.e. Θ = [-1, 1] D , but the results are extendable to any compact set in R D . Let the prior be of the form π(θ) = exp(ν 0 (θ)). We make the following assumptions on the model and prior: Assumption 4 (Prior Assumptions). The function ν 0 : Θ → R is twice continuously differentiable on Θ.\n\nAssumption 5 (Model Assumptions). The log-likelihood function ℓ(x (n) ; •) is twice continuously differentiable on Θ.\n\nWe quantify the fluctuation of the log-likelihood function using oscillation, defined as\n\nWe now state the main result: Theorem 2. Let Assumption 4 and Assumption 5 hold. Let q * λn be an optimizer of Eq. (4.1). Define:\n\n, there exists a sequence of product distributions m * λn such as, for any\n\nThe proof uses the theory nonlinear large deviation  [Yan, 2020]  and the properties of displacement convex functionals  [Ambrosio et al., 2005] . See Section E.\n\nEq. (4.4) defines a mean-field regime, where a product measure matches q * λn in any 1-Lipschitz statistics (first-order statistics). This regime characterizes when Ξ-VI can be replaced by MFVI. The critical scaling term in the threshold is i,j c 2 ij as the other terms are typically well controlled. Roughly, the equivalence Ξ-VI and MFVI is determined by comparing λ n to (D -1/2 )-scaled Frobenius norm of the Fisher information.\n\nEq. (4.5) defines a Bayes optimal regime, where Ξ-VI asymptotically recovers 1-Lipschitz statistic of the exact posterior. If the dimension D = O(1) as n increases and the exact posterior achieves consistency, then Ξ(q * 0 ) converges to zero, and Eq. (4.5) holds for any bounded sequence of λ n . When D grows with n but at a slow rate (e.g. D ≲ n -1/3 ), we may still expect a form of posterior consistency to hold the Bayesian optimal regime to contain non-trivial choices of λ n .\n\nTo match the computational complexity in Section A, we provide sufficient conditions for λ n ≻ D to be in the mean-field regime.\n\nThe result establishes the equivalence of statistical behavior between q * λn and a product measure for λ ≻ D. The implication of Corollary 2 is a computational insight on Ξ-VI: when λ n is large, the Ξ-variational posterior can be replaced by the computationally efficient mean-field approximation to the posterior.\n\nThe setting of Corollary 2 are met under Assumption 5 with a compact Θ. It also suffices that 1) the gradient and diagonal Hessian of the log-likelihood scale slower than D entry-wise and 2) the off-diagonal Hessian is uniformly bounded.\n\nNext, we specialize to the example of high-dimensional linear regression models empirically studied in Section 3.2.\n\nHigh-Dimensional Linear Model. We observe {(x i , y i ) :\n\nWe consider a high-dimensional Bayesian linear regression model where both n, D are tending to infinity:\n\n(4.7)\n\nFor matrix B := 1 σ 2 X T X, let B diag and B off denote the diagonal and off-diagonal submatrix. Define B ′ diag as diagonal matrix with [B ′ diag ] ii := 1/([B] -1 ) ii . The Ξ-VI for Bayesian linear model is given by q * λn = arg min\n\nFor the linear model, we make the following model curvature assumption.\n\nAssumption 6 (Curvature Assumption). There exist κ 1 ≥ 0 and\n\nAssumption 6 implicitly assumes Assumption 4 and Assumption 5 by requiring the log prior ν 0 to be twice continuously differentiable with Hessian ∇ 2 ν 0 bounded above by -κ 1 I D . Note that for the linear model y = Xθ + ϵ with Gaussian noise, the Hessian of the negative log-likelihood is X ⊤ X/(2σ 2 ), so the second condition ensures the likelihood is strongly convex and twice continously differentiable.\n\nOur main result characterizes the asymptotic properties of q * λn for model (4.7).\n\nTheorem 3. Let Assumption 6 hold. Let q * λn be an optimizer of Eq. (4.1). Then the following holds:\n\nWhen λ n ≻ tr(B 2 off ), there exists a sequence of product distributions m * λn such as, as n → ∞, sup\n\nWhen λ n ≻ D -1 tr(B 2 off ), there exists a sequence of product distributions m * λn such as, for any 1-Lipschitz function ψ : R → R, as n → ∞, sup\n\nEq. (4.9) and Eq. (4.10) form the mean-field regimes, which refine the regime of Theorem 2.\n\nWhen λ n scales faster than tr(B 2 off ), q * λn converges in Wasserstein metric to a product distribution. This means all moment statistics can be asymptotically transported between Ξ-VI and MFVI. When λ n scales faster than tr(B 2 off )/D, we can transport any 1-Lipschitz statistic between Ξ-VI and MFVI. As λ n increases, q * λn shares more distributional information with m * λn . Eq. (4.11) and Eq. (4.12) define the Bayes optimal regimes. When λ n increases slower than (κ 1 +κ 2 ) tr Cov q * 0 (B off θ) -1 , q * λn converges to the exact posterior in the Wasserstein metric. By relaxing a factor of D, q * λn achieves asymptotic Bayes optimality for all 1-Lipschitz statistics. As with the mean-field regimes, the term tr Cov q * 0 (B off θ) , which involves B off , controls the discrepancy between Ξ-variational posterior and the exact posterior. In particular, it is upper bounded by ∥B off ∥ 2 tr Cov q * 0 (θ) , thus the Bayes optimal regime is large when ∥B off ∥ 2 is small. When B off = 0, any choice of λ n falls automatically inside the Bayes optimal regime. For sufficiently regular models, we could use the Bernstein von-Mises results D ≲ n 1/3 [Panov and Spokoiny, 2014] or D ≲ n 1/2  [Katsevich, 2023]  to approximate the posterior covariance with the inverse Fisher information matrix, namely (Cov q * 0 (θ)) -1 ≈ tr(B). Thus, when the dimension D grows slowly with n, the Bayes optimal regimes in Eq. (4.11) and Eq. (4.12) are satisfied with\n\n2 . The Bayes optimal regime (Eq. (4.9)) and the mean-field regime (Eq. (4.11)) can both hold for sufficiently large λ n . In that case, the Ξ-VI solution q * λn can be computed efficiently via MFVI while still closely approximating the exact posterior. For instance, in a linear regression setup where X ⊤ X is diagonal, the exact posterior q * 0 is itself a product measure. Consequently, the upper bound in (4.11) is infinite, whereas the lower bound in (4.9) is zero. Hence, any choice of λ n satisfies both (4.9) and (4.11) simultaneously. We have clarified this point in the revised manuscript. For 1-Lipschitz statistics, the overlap regime between Eq. (4.10) and Eq. (4.12) holds if\n\nThis criterion is satisfied, for example, when tr(B 2 off ) ≺ D, ∥B diag ∥ 2 ≲ 1 and tr(Cov q * 0 (θ)) ≲ D, which recovers the Bayes optimal condition for MFVI  [Mukherjee and Sen, 2022, Mukherjee et al., 2023] . But our criterion is more flexible: for example, it is also satisfied tr(B 2 off ) ≲ D, ∥B -B ′ diag ∥ 2 ≲ 1 and tr(Cov q * 0 (θ)) ≺ D. When no choice of λ n satisfies the overlap criterion, there is a gap between the mean-field and Bayes optimal regimes. Achieving accurate posterior inference thus requires paying an additional computational cost that scales inversely with λ n , as discussed in Section A.\n\nLet the eigenvalues of B off as η D ≥ . . . ≥ η 1 . Then tr(B 2 off ) = D i=1 η 2 i , and the meanfield regime Eq. (4.9) corresponds to λ n ≻ D i=1 η 2 i . To match the complexity bound in Section A, we provide sufficient conditions for λ n ≻ D to be in the mean-field regime Eq. (4.9).\n\nData preprocessing often involves normalizing features to have unit variances. Thus, the requirement that D i=1 η 2 i ≲ D 2 is often met in practice.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Discussion",
      "text": "We introduced Ξ-VI, a new way of doing variational inference that extends MFVI through entropic regularization. We characterize the asymptotic normality of Ξ-variational posteriors in lower-dimensional scenarios and the tradeoff between computational complexity and statistical fidelity in higher-dimensional settings. On both simulated and empirical datasets, we demonstrated its advantages over traditional MFVI. Further, our method explicitly connects VI to entropic optimal transport, using the Sinkhorn algorithm to improve the fidelity of a VI approximation.\n\nOne question prompted by our work is to understand the fundamental limits of highdimensional Bayesian models. It is known that many high-dimensional problems show a gap between what is statistically achievable (in a minimax sense) and what is achievable via a polynomial-time algorithm, such as sparse PCA  [Wang et al., 2016]  and denoising problems  [Chandrasekaran and Jordan, 2013] . However, characterization of a statisticalcomputational gap is a new topic in probabilistic machine learning.\n\nThe theoretical results in Section 4 identify distinct asymptotic regimes that correspond to the exact posterior and the mean-field approximation. The transition between these regimes echoes classical phase transitions in spin glass models  [Montanari and Sen, 2022] . While our analysis focuses on a regression setting, similar techniques could be extended to models such as the Ising model or the quadratic interaction model. It would also be interesting to investigate the connection between Ξ-VI and the rich literature on PAC-Bayes and generalized Bayes learning  [Alquier and Guedj, 2018 , Alquier, 2021 , Husain and Knoblauch, 2022 , Wild et al., 2022 , 2023] , and to explore how such results in Pac-Bayes  [Alquier, 2024]  interact with the observed phase transitions-especially given that the target distribution is a general Gibbs measure.\n\nAnother challenge is scaling Ξ-VI to high-dimensional settings. The main hurdle lies in implementing the multimarginal Sinkhorn algorithm efficiently for a large number of marginals while maintaining polynomial-time complexity. Advances in distributed computing and stochastic optimization could help mitigate these computational costs. Future work may focus on developing accessible and efficient algorithmic tools to enable scalable applications of Ξ-VI.\n\nWhile this paper implemented the examples using EP and BBVI for approximating posterior marginals, advanced mean-field methods such as the TAP approach may be preferable in certain contexts, such as spiked covariance models  [Fan et al., 2021]  and highdimensional Bayesian linear regression  [Celentano et al., 2023a] . Exploring Ξ-VI combined with the TAP method is a promising avenue for future research, potentially providing a more accurate approximation to the true posterior.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Algorithm 2: (Multmarginal) Sinkhorn Algorithm",
      "text": "Input: Cost tensor C, marginals M , tolerance ϵ, regularization parameter λ.\n\nInitialize:\n\npolynomial-time solvability of the Sinkhorn algorithm requires additional assumptions on the cost tensor.  Altschuler and Boix-Adsera [2021]  shows that if the cost has a graphical structure with bounded treewidth or if the cost tensor is low rank plus sparse, then multimarginal EOT is solvable in poly(M, D) time. Their result is stated as follows:\n\nProposition 2  (Altschuler and Boix-Adsera [2023] ). Consider cost tensor C ∈ (R M ) ⊗D that satisfies one of the following:\n\n1. C has graphical structure with constant junction tree width ω; or 2. C = R + S where R ∈ (R M ) ⊗D has constant multilinear rank and S has poly(M, D) sparsity.\n\nThen for any λ ≥ 0, the Algorithm 2 terminates in poly(M, D, C max /ϵ, 1 λ+1 ) time. Remark 1. The bounded treewidth assumption guarantees polynomial-time solvability of the junction tree algorithm  [Wainwright et al., 2008] . Models that satisfy the bounded treewidth assumption include state space models, topic models, and linear regression models with sparse design.\n\nRemark 2. The assumption of low-rank plus sparsity is less used in the Bayesian literature. Loosely speaking, the low-rank assumption requires the true posterior to be a mixture of product distributions. The error factor means that the exact posterior need only match a mixture of product distributions up to a poly(M, D) sparse remainder.\n\nFor general graphs G with bounded treewidth,  Fan et al. [2022]  proposed implementing the Sinkhorn algorithm using the junction tree method. It has the following complexity: Corollary 4. Assume the cost tensor C ∈ (R M ) ⊗D has constant treewidth ω. Consider Algorithm 2 implemented with the junction tree method  [Fan et al., 2022] . For any λ ≥ 0, it converges in O(D 3 M ω+1 (λ + 1) -1 ϵ -1 ) iterations.\n\nThis result adapts Theorem 4 in  Fan et al. [2022] , which shows the computational complexity drops with increasing λ. Also observe that achieving polynomial dependence on D only requires the graph's treewidth ω(G) to grow slower than log(D). Consequently, the polynomial-time solvability might be achievable for \"locally tree-like\" graphs.\n\nCounterintuitively, for λ scaling faster than D, the computational complexity decreases as D increases. Yet, as Corollary 2 and Corollary 3 show in Section 4, when λ scales faster than D, the variational posterior effectively reduces to the naive mean-field approximation.\n\nRemark 3. Well-known examples exist that violate the conditions in Proposition 2. For example, an Ising model over a complete D × D graph has a treewidth of D, and its cost tensor is neither low rank nor sparse. In fact, implementing the Sinkhorn algorithm for an Ising model on a complete graph is known to be NP-hard  [Altschuler and Boix-Adsera, 2021] . For these problems, we further approximate by grouping the variables and coupling marginals represented by the group. For example, if we have an Ising model with 100 variables, we could group first 50 and last 50 variables and perform an EOT computation with two 50-dimensional marginals instead of 50 one-dimensional marginals. This still produces a strict improvement over MFVI, the procedure runs in polynomial time when the group number is fixed since the complexity of the Sinkhorn algorithm is blind to the dimension of each marginal  [Altschuler et al., 2017] .",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "A.3 Finite-Sample Convergence",
      "text": "Here we analyze convergence properties of the Ξ-variational posterior q * λ when λ tends to 0 or ∞, while keeping n fixed. Understanding this setting justifies the stability of the algorithmic output after we replace the marginals of q * λ with a set of pseudomarginals in Algorithm 1. Moreover, the convergence results of the Ξ-variational posterior for both large and small λ values are useful from a classical Bayesian perspective that treats the observed data as known  [Berger, 2013] .\n\nWe show that Ξ-variational posterior converges to the mean-field variational posterior as λ tends to infinity, and converges to the exact posterior as λ tends to zero. Then we establish a stability property for q * λ when we replace its marginals with another set of marginals, which helps justify Algorithm 1.\n\nLet us define a cost function C λ over λ ∈ R+ as\n\nELBO(q) -λΞ(q).\n\nLimits as λ → ∞ or λ → 0. We start with the convergence of q * λ and C λ as λ tends to infinity.\n\nTheorem 4. Assume that D KL (q ∥ q * 0 ) < ∞ for some q ∈ M(Θ). For each λ ∈ R+ , define the set Q λ as the set of minimizers for the functional q → D KL (q ∥ q * 0 ) + λΞ(q\n\nThe result shows that every mean-field variational posterior is an accumulation point of some sequence of Ξ-variational posteriors. This type of result is called \"large-time limits\" in the optimal transport literature. When the likelihood is quadratic, it is possible to prove an exponential rate of convergence for C λ under more restrictive conditions  [Conforti and Tamanini, 2021] . However, this setting is uninteresting for Bayesian inference and we do not pursue it in this paper.\n\nAs λ tends to zero, we provide analog results to show that Ξ-variational posterior converges to the exact posterior in the Wasserstein metric.\n\nTheorem 5. Assume that Ξ(q * 0 ) < ∞ [P θ 0 ]-almost surely. For each λ ∈ R+ , define the set Q λ as the set of minimizers for the functional q → D KL (q ∥ q * 0 )+λΞ(q) in P 2 (Θ). If q * λ ∈ Q λ converges as λ → 0 in the Wasserstein metric, then lim λ→0 W 2 (q * 0 , q * λ ) = 0. Furthermore, the Ξ-VI cost converges to the true posterior ELBO, i.e. lim λ→0 |C λ -C 0 | = 0. Algorithmic Stability. Let m * λ be the product of marginals of q * λ . In Section 2.2, we produce to replace idealized Algorithm 3 with a simple, efficient approximate Algorithm 1. A natural question to ask is whether the solution is stable after we replace m * λ with pseudomarginals m. To answer this question, we leverage the tools of quantitative stability from OT theory  [Eckstein and Nutz, 2022] . We make two assumptions: a Lipschitz cost assumption, and transportation cost inequality.\n\nAssumption 7 (Lipschitz Cost Assumption). We assume that there exists a constant L ≥ 0 and ϕ i : Θ i → R such that for all q ∈ C(m * λ ) and q ∈ C( m),\n\nThis assumption is slightly more general than the Lipschitzness of ℓ(x (n) ; •) minus additive correction factors. As an example, the Gaussian likelihood satisfies Assumption 7 (Lemma 3.5,  Eckstein and Nutz [2022] ).\n\nAssumption 8 (Transportation Cost Inequality). A product distribution m over Θ satisfies the transportation cost inequality if there exists a constant C such that W 2 (q 1 , q 2 ) ≤ D KL (q 1 ∥ q 2 ), for all q 1 , q 2 ∈ C(m).\n\nAssumption 8 is standard in high-dimensional statistics  [Wainwright, 2019] . When Θ is compact, the assumption follows from Pinsker's inequality. Otherwise, this assumption holds when each marginal has a finite exponential moment.\n\nWe now state the main stability result which upper bounds the approximation error of Algorithm 1 using the approximation error of the pseudomarginals.\n\nTheorem 6 (Stability of Algorithm 1). Let Assumption 7 hold with a Lipschitz constant L. Let m * λ be the marginals of Ξ-variational posterior q * λ and m ∈ M (Θ) be another product distribution. Suppose m * λ satisfies Assumption 8 with a fixed constant C. Then for the one-step approximation qλ defined in Algorithm 1 with pseudomarginals m, the following upper bound holds:\n\nThe proof uses an OT technique called shadowing. See Section E for details. The result highlights the stability of Algorithm 1, as the approximation error of q * λ is only Lipschitz in the approximation error of the pseudomarginals. If m is close enough to m * λ in terms of the W 2 metric, the output of Algorithm Algorithm 1 is guaranteed to well approximate true variational posterior q * λ . Corollary 5. Assume Assumption 7 with Lipschitz constant L, and Assumption 8 for the pseudomarginals. Then the following limits hold:\n\n1. Let q * (∞) λ be the optimizer of Eq. (2.5) with marginals {q * ∞,i } i∈  [D]  . Then lim λ→∞ W 2 (q * (∞) λ , q * λ ) = 0.\n\n2. Let q * (0) λ be the optimizer of Eq. (2.5) with marginals {q * 0\n\nThe Corollary is a consequence of Theorem 4, Theorem 5, and Theorem 6. As λ tends to 0 or ∞, the error of replacing the idealized Algorithm 3 with Algorithm 1 vanishes when we use exact posterior marginals or mean-field variational posteriors, respectively. If we plug in a consistent estimate of the exact posterior marginals (e.g. TAP approximation of a linear model with i.i.d. Gaussian design  [Celentano et al., 2023a] ), then Algorithm 1 asymptotically recovers the exact posterior as λ tends to zero.",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "A.4 Full Coordinate Ascent Algorithm",
      "text": "In this section, we present a full coordinate descent algorithm to exactly optimize the Ξ-VI objective.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "A.4.1 Outer Variational Problem",
      "text": "We now derive steps to solve the outer variational problem of Eq. (2.4). Treating ϕ 1 , • • • , ϕ D as fixed, we optimize over the marginals m i 's, min\n\nEq. (A.3) is equivalent to a mean-field VI problem with a surrogate log-likelihood. To solve Eq. (A.3), we use a method based on coordinate ascent variational inference (CAVI)  [Blei et al., 2017] .\n\nDenote Θ -i := j̸ =i Θ j , θ -i := (θ [D]\\{i} ), and m t -i := j<i m t+1 j (θ j ) j>i m t j (θ j ). Now define ν t+1 i (θ i ) as follows:\n\nThis leads to an explicit formula for the update of each marginal,",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "A.4.2 Full Coordinate Ascent Algorithm",
      "text": "Algorithm 3 presents the full coordinate ascent algorithm. It monitors change in ELBO as the criterion of convergence, which is equivalent (up to a scalar) to the KL divergence between the variational posterior and the exact posterior.\n\nUnfortunately, Algorithm 3 is difficult to implement because we cannot calculate the expectations needed in Eq. (2.10) or Eq. (A.4). When we represent ϕ i 's implicitly, there is no practically stable MFVI for implicit log-likelihood, especially when the model is highdimensional.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Algorithm 3: Coordinate Ascent Algorithm",
      "text": "Input: Log-likelihood ℓ (x; θ), prior π, tolerance ϵ, regularization parameter λ.\n\nProof. Given that A is symmetric and positive definite, there exists another symmetric positive definite matrix B such that B 2 = A. We note that A jj = e T j Ae j = e T j B T Be j = ∥Be j ∥ 2 2 and similarly, (A -1 ) jj = ∥B -1 e j ∥ 2 2 . By the Cauchy-Schwarz inequality, we have ⟨Be j , B -1 e j ⟩ 2 ≤ ∥Be j ∥ 2 2 ∥B -1 e j ∥ 2 2 = A jj (A -1 ) jj .\n\nHowever, ⟨Be j , B -1 e j ⟩ = e T j (B -1 ) T Be j = e T j B -1 Be j = e T j e j = 1. Therefore, we have A jj (A -1 ) jj ≥ 1, which simplifies to (A -1 ) jj ≥ 1\n\nA jj . This completes the proof.\n\nLemma 3. For q ∈ P(Θ), the following variational characterization of its expressivity holds:\n\nProof. Apply Donsker-Vardhan lemma. We obtain\n\nApply Donsker-Vardhan lemma again to D KL (q(θ i , θ -i ) Theorem 7 (Theorem 5.11,  Santambrogio [2015] ). In the space P p (R d ), we have W p (µ n , µ) → 0 if and only if µ n → µ weakly and\n\nwhere p > 0 is a given exponent.\n\nLemma 4. Let q n be a sequence of measures in P 2 (Θ). If W 2 (q n , q) → 0 for some q ∈ P 2 (Θ), then lim inf n→∞ Ξ(q n ) ≥ Ξ(q). Let q 0 be another measure in P 2 (Θ). We have lim inf n→∞ D KL (q n ∥ q 0 ) ≥ D KL (q ∥ q 0 ).\n\nProof. The second property follows from the fact that functional D KL (• ∥ q * 0 ) is continuous in the Wasserstein metric (Proposition 7.1,  Santambrogio [2015] ). For any q n W 2 → q, Theorem 7 implies that q n weakly converge to q 0 . The convergence W 2 (q n , q 0 ) → 0 implies the convergence W 2 (q n,i , q 0,i ) → 0 for each i ∈ [D]. Since D KL is lower semicontinuous in both arguments (Theorem 4.8,  Polyanskiy and Wu [2023] ), we get\n\nwhere D is fixed with respect to n.\n\nDefinition 1 (Shadow). Let p ∈ [1, ∞] and m, m be product measures within P p (Θ). Assume κ i ∈ C(m i , mi ) is a coupling that achieves W p (m i , mi ) and let κ i = m i ⊗ K i represent a disintegration. For a given q ∈ C(m), its shadow q s ∈ C( m) is defined as the second marginal of q ⊗ K ∈ P(Θ × Θ), where the kernel\n\nGiven a coupling q ∈ C(m), its shadow q s satisfied the following properties.\n\nLemma 5 (Lemma 3.2,  Eckstein and Nutz [2022] ). For product distributions m, m ∈ P 2 (Θ) and coupling q ∈ C(m), its shadow q s ∈ C( m) satisfies\n\nwhere D f (•) is any f -divergence.\n\nTheorem 8 (Theorem 12,  Lin et al. [2022] ). Let {ϕ t } t≥0 be the iterates generated by Algorithm 2. The number of iterations t required to reach the stopping criterion E ≤ ϵ ′ is upper bounded by:",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Appendix C. Proofs Of Section 2",
      "text": "Derivation of Eq. (2.5). Let m be given. Then to optimize q * , we have\n\nThe first line uses the fact that m is a product distribution. The third line drops D KL (m ∥ π) as it does not depend on q.\n\nThe next result states that the solution to the EOT problem (2.5) has a unique representation.\n\nTheorem 9 (Structure Theorem for Multi-Marginal EOT). Assume that inf q∈C(m)\n\n{-E q [ℓ(x; θ)] + (λ + 1)Ξ(q)} < ∞ and sup θ∈Θ ℓ(x; θ) < ∞.\n\nThen there exists a unique minimizer q * to the inner variational problem (2.5) that is absolutely continuous with respect to m (denoted q * ≪ m), and the following hold:\n\n(1) There exist measurable functions ϕ *\n\nm-almost surely. The collection ϕ * := (ϕ * 1 , . . . , ϕ * D ) is referred to as the EOT potentials. Each ϕ * i is m i -almost surely measurable and unique up to an additive constant. Moreover, if\n\n(2) Conversely, suppose q ∈ C(m) admits a density of the form in Eq. (C.1), m-almost surely, for some functions ϕ i : Θ i → R. Then q minimizes the inner variational problem in Eq. (2.5), and the functions ϕ i are the EOT potentials.\n\nThis result first appears heuristically in  Carlier [2022] . For D = 2, the uniform boundedness assumption can be relaxed to ℓ(x; •) being integrable  (Theorem 4.2, Nutz [2021] ).\n\nProof of Theorem 9. Define the auxiliary distribution q aux ∈ P(Θ) as\n\nwhere Z n (λ) is the normalizing constant. Since sup θ∈Θ ℓ(x; θ) < ∞, Z n (λ) < ∞, and hence q aux is well-defined and absolutely continuous with respect to m.\n\nMinimizing the objective function in Eq. (2.5) is equivalent to minimizing the KL loss to q aux . min\n\nSince C(m) is displacement convex and the KL loss is displacement convex  [Villani, 2009] , the solution is unique. Let q * be the optimizer. Then, by the method of Lagrange multipliers, there exist dual variables\n\nThis is equivalent to\n\nafter normalizing. Since the potentials ϕ * i are uniformly bounded, the resulting normalization constant is finite. By adding the normalizing constant to ϕ D , we obtain\n\n\"only if\" direction: Assume that the optimal coupling q * is given by\n\nwhere\n\n) are some potential functions. Plugging the solution in the EOT primal problem, for each i and [m i ]-a.s.θ i , the potentials satisfy a set of fixed point equations called the Schrödinger system:\n\nThe Schrödinger system (Eq. (C.5)) satisfies the Euler-Lagrange optimality condition for the primal EOT problem  [Carlier, 2022] . Precisely, the EOT potentials solve max\n\nwhich is the dual problem to the multimarginal EOT problem (Eq. (2.5)). Since the EOT problem is convex  [Nutz, 2021] , the primal-dual gap closes, which means the probability measure q defined under ϕ solves Eq. (2.5).\n\nTo see that\n\nD KL (q ∥ q aux ) ≥ 0 By Eq. (C.5), we apply Jensen's inequality to obtain that\n\nFor the other direction, since sup θ∈Θ l (x; θ) < ∞, we have\n\nSince the right-hand side of the inequality does not depend on θ i , inf\n\nProof of Eq. (A.3). We make the following derivation, min",
      "page_start": 36,
      "page_end": 37
    },
    {
      "section_name": "Appendix D. Proofs Of Section A",
      "text": "Proof of Proposition 2. By Theorem 14 of  Lin et al. [2022] , Algorithm 2 reaches the stopping criterion E ≤ ϵ in t iterations, where t satisfies\n\nThis implies that t ≍ poly(D, C max /ϵ, 1 λ + 1\n\n). (D.1)\n\nAlgorithm 2 calls the following oracle D times:\n\nThe other steps are computed in linear time. By Theorem 5.5 and Theorem 7.4 of  Altschuler and Boix-Adsera [2023] , the oracle can be computed in poly(M, D) iterations. Repeating the oracle complexity Dt times, by Eq. (D.1), the algorithm terminates in poly(M, D, C max /ϵ, 1 λ+1 ) time.\n\nProof of Corollary 4. We consider Algorithm 1 from  Fan et al. [2022] . The algorithm implements the marginalization in Algorithm 2 using the sum-product method. Consider a graph G = ([D], E, K), where [D], E, K represent the set of nodes, edges, and maximal cliques. If the log-likelihood ℓ(x (n) ; θ) factorizes according to G, by the Hammersley-Clifford theorem, we get\n\nwith ℓ α is defined over j∈α Θ j .\n\nDefine C kα as the tensor of ℓ α (θ α ) values at support points (θ\n\n. The cost tensor decomposes as follows:\n\nLet t be the iteration count for Algorithm 1 of  Fan et al. [2022]  to terminate with criterion ϵ. By Theorem 1 of  Fan et al. [2022] , we get\n\nWith T as the minimal junction tree for G, marginalizing over each factor in T takes O(M ω(G) ) iterations, and message passing takes O(d(T )M ω(G) ) iterations, where d(T ) is the average leaf distance in T . Since max α∈K ∥C kα ∥ ∞ is uniformly bounded, we conclude that sum-product implementation of the Sinkhorn algorithm\n\nLemma 6 (Transformation Identities). For h := δ -1 n (θ -θ 0 -δ n ∆ n,θ 0 ) where θ ∼ q, we have q(θ) = |det(δ n )| -1 q(h), and q i (θ) = δ -1 n,ii qi (h) for i ∈ [D], Moreover, we have\n\nand for any distribution q 1 , q 2 over Θ, we have\n\nwhere q1 , q2 are densities defined via Eq. (4.2).\n\nProof of Lemma 6. We obtain the first equality by applying the change of variable formul to Eq. (4.2). q(θ) = |det(δ n )| -1 q(h), and q i (θ\n\nFor the second equality, we have\n\nThe univariate case follows from this.\n\nFor the third equality, we can write\n\nFor the fourth equality, we have\n\nThis concludes the proof.\n\nTo establish the Bernstein von-Mises theorem, we introduce the tool of Γ-convergence  [Braides, 2014] .\n\nDefinition 2 (Γ-Convergence). Let X be a metric space and consider a set of functionals F ε : X → R indexed by ε > 0. A limiting functional F 0 exists and is called the Γ-limit of F ε as ε → 0, if the following conditions are met:\n\n1. Liminf Inequality: For all x ∈ X and for every sequence\n\n2. Limsup Inequality / Existence of a Recovery Sequence: For each x ∈ X, there exists a sequence xε → x such that\n\nThe first condition requires F 0 to be asymptotically upper bounded by F ε . When paired with the second condition, it ensures that F 0 (x) = lim ε→0 F ε (x ε ), thereby confirming that the lower bound is tight.\n\nDefinition 3 (Equi-Coerciveness of Functionals). A sequence of functionals F ε : X → R is said to be equi-coerciv if for every bounded sequence x ε with F ε (x ε ) ≤ t, there exists a subsequence x j of x ϵ and a converging sequence x ′ j satisfies F ε j (x ′ j ) ≤ F ε j (x j ) + o(1). Equi-coerciveness ensures the existence of a precompact minimizing sequence for F ε , which helps establish the convergence x ε → x.\n\nTheorem 10 (Fundamental Theorem of Γ-Convergence). Let X be a metric space and F ε an equi-coercive sequence of functionals.\n\nThis theorem implies that if minimizers x ε for all F ε exist, the sequence converges, potentially along a subsequence, to a minimizer of F . We note that the converse is not necessarily true; there may exist minimizers for F which are not limits of minimizers for F ε .\n\nNote that when\n\nn δ n,ii . We can explicitly characterize the transformed variational posterior:\n\nwhere q * λ is the original Ξ-variational posterior. Lemma 7. Under Definition Eq. (E.1), the distribution qλ solves the following variational problem qλ = arg min\n\nThis Lemma is a direct consequence of the transformation identities (Lemma 6) and Eq. (4.1), thus the proof is omitted.\n\nProof of Theorem 1. WLOG, we assume that Θ = R D . Otherwise, we use the same proof by adding an indicator of the minimizing set to the sequence of functionals.\n\nRegime 1: λ n → ∞. It suffices to show\n\nBy Theorem 10, Γ convergence implies W 2 (q λn , arg min q∈P 2 (Θ) F 0 (q)) P θ 0 → 0, where q 0 is the minimizer of F 0 .\n\nTo prove the Γ-convergence, we rewrite F n .\n\nApplying LAN expansion and Laplace approximation to the log-normalizer, we have\n\nAfter cancellation, we have\n\nUsing Assumption 1 to bound the prior tail via Taylor expansion, we have an expression for F n\n\nNow we rewrite F 0 (q).\n\nNow we prove the Γ convergence.\n\nFirst, we verify the liminf inequality. Let q n W 2 → q. When q is not mean-field, we have:\n\nThe second inequality follows from the definition of liminf. The third line is due to Lemma 4, which states that the KL functional and Ξ functional are lower semicontinuous.\n\nSince this holds for all ϵ, we verified that lim inf n→∞ F n (q n ) ≥ F 0 (q). Next, we show the existence of a recovery sequence. When q is not mean-field, F 0 (q) = +∞, and the limsup inequality is automatically satisfied. When q is mean-field, choose q n := q, then:\n\nThus, F 0 is the Γ-limit of the sequence F n .\n\nNext we prove that the sequence F n is eqi-coercive. Take n j → ∞ and q n j such that F n j (q n j ) ≤ t for all j. Then λ n j Ξ(q n j ) is bounded as λ n j → ∞, thus Ξ(q n j ) = o(1). Using this and Eq. (E.2), we have\n\nSince D KL . ∥ N (0, V -1 θ 0 ) is a Wasserstein (geodastically) convex functional, it is coercive by Lemma 2.4.8 of  Ambrosio et al. [2005] . This implies that the set {q ∈ P 2 (Θ) | D KL (q ∥ q0 ) ≤ t + 1} is compact under the Wasserstein metric, thus q n j has a subsequence q ′ n j that converges to q * in the Wasserstein metric of and D KL (q * ∥ q0 ) ≤ t + 1. Thus we have  1 ) by Eq. (E.2) where q ′ n j is a converging subsequence of q n j . This verifies the equi-coercivity of F n .\n\nLastly, we note that F 0 attains its minimum at\n\nis the MFVI covariance. As a result of Theorem 10, we conclude that the desired convergence takes place:\n\nRegime 2: λ n → 0.\n\nIn this regime, we show that the functionals\n\nGiven that F n is defined analogous to Regime 1, we will skip the derivation:\n\nNow we prove the Γ convergence. First, we verify the liminf inequality. Let q n W 2 → q. We have:\n\nSince this holds for all ϵ, we verified that lim inf n→∞ F n (q n ) ≥ F 0 (q). For the recovery sequence, we take q n := q. Since q is absolutely continuous with respect to the product of its marginals, Ξ(q) is finite. Then we have:\n\nThe equicoercivity of F n follows from the argument in regime 1. By Theorem 10, we conclude with the desired convergence:\n\nIn this regime, we show that the functionals\n\nRecall that\n\nNow we prove the Γ convergence. First, we verify the liminf inequality. Let q n W 2 → q. We have:\n\nThe second inequality follows from the definition of liminf, and the last inequality is due to Lemma 4, which states that the KL functional and Ξ functional are lower semicontinuous.\n\nFor the recovery sequence, we take q n := q. As long as Ξ(q) is finite, we have:\n\nThe equicoercivity of F n follows from the argument in regime 1. By Theorem 10, we have the convergence: W 2 (q λn , arg min\n\nProof of Corollary 1. Recall the definition of Wasserstein distance:\n\nGiven the change of variable definition (Eq. (4.2)), we have\n\nIf W 2 (q λn , N (µ, Σ)) tends to 0, then W 2 (q * λn , N (δ n µ + θ 0 + δ n ∆ n,θ 0 , δ T n Σδ n )) tends to 0. Since N (δ n µ+θ 0 +δ n ∆ n,θ 0 , δ T n Σδ n ) weakly converge to δ θ 0 , it converges to δ θ 0 in Wasserstein metric. By Theorem 1, we have q * λn converges in Wasserstein metric to δ θ 0 , as desired.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Proofs Of Section 4.2",
      "text": "We first prove a useful proposition.\n\nProposition 3 (Optimality to fixed point). Let Assumption 4 hold. Let m * λ (θ) = D i=1 m * λ,i (θ i ) be the product of optimal marginals, and ϕ * λ be the optimal EOT potentials. Then m * λ and ϕ * λ satisfy the fixed point equations:\n\nwhere Z i 's are the normalizing constants.\n\nProof of Proposition 3. Define f λ (θ) as follows,\n\n(E.4) By Theorem 9, Assumption 4 and the uniform boundedness of ℓ x (n) ; • , the function f λ is integrable with respect to m * λ . From the derivation in Section 2.2, the distribution m * λ attain the minimum, min\n\nand by the tower property, we have\n\nBy the Gibbs variational principle (Lemma 1), the minimum is uniquely attained by\n\nRecall that the optimal EOT potentials satisfy the Schrödinger system:\n\nThis allows us to simplify fλ,i (θ i ):\n\nThe last equality uses the fact that m * λ,i is the i th marginal of q * λ . Since j̸ =i E m * λ,j -(λ + 1)ϕ * λ,j (θ j ) + log π j (θ j ) does not depend on θ i , we obtain\n\nUsing Eq. (E.10), we conclude\n\nProof of Theorem 2. We define constants u i , v i , and w based on the partial derivatives of the log-likelihood ℓ(x (n) ; θ). Since the parameter space Θ is compact and ℓ(x (n) ; •) is twice continuously differentiable, both the gradient ∇ℓ(x (n) ; •) and the Hessian ∇ 2 ℓ(x (n) ; •) are uniformly bounded over Θ.\n\nLet\n\nWe define a new log-likelihood l(x (n) ; θ) that shift ℓ(x (n) ; θ) by a quadratic function:\n\nCalculation yields that\n\nGiven the optimal m * λn , the inner variational (EOT) problem has the following formulation.\n\nq * λn = arg min\n\nwhere we use l(x (n) ; θ) instead of ℓ(x (n) ; θ) because the subtracting a tensorized function w + D i=1 v i θ i + D i=1 u i θ 2 i from the cost does not change the optimal EOT coupling. By Theorem 9, we can write q * λn using the EOT solution structure.\n\nq * λn (θ) = exp\n\nwhere m * λn,i 's are the marginals of q * λn and ϕ * λn,i 's are the EOT potentials. Define another product distribution mλn (θ) ∝ exp D i=1 ϕ * λn,i (θ i ) m * λn,i (θ). We can rewrite q * λn as the product of a tempered likelihood and a mλn .\n\nwhere the normalizing constant is given by n) ; θ) mλn (θ)dθ. (E.12)\n\nFirst, we want to show that\n\n(E.13) Let ∥f ∥ ∞ denote the supremum norm of a function f . Fix some ϵ > 0. Let S λn (ϵ) ⊂ Θ be a finite set such that for any θ ∈ Θ, there exists s ∈ S λn (ϵ) satisfying\n\nDenote by |S λn (ϵ)| the cardinality of S λn (ϵ). Theorem 1.1 of  Yan [2020]  implies that Thus,\n\nPlugging the definition of ϵ n into Eq. (E.16), we get\n\n(E.18) For any m ∈ M(Θ), we have\n\nEq. (E.18) implies that for m * λn ∈ arg min m∈M(Θ) D KL (m ∥ q * λn ), we have\n\nFor any 1-Lipschitz function f under the L 1 norm, consider the random variable f (θ), where θ ∼ q * λn . This variable satisfies the inequality log\n\n, which is derived from the assumption that Θ = [-1, 1] D . Thus, q * λn is (4D)-subGaussian. By the T 1 -transportation inequality  (Theorem 4.8, Van Handel [2014] ), for any m ∈ M(Θ), the following upper bound holds:\n\nwhere W 1 is the 1-Wasserstein distance.\n\nLet m * λn denote the minimizer of the left hand side Eq. (E.19). Consider a function ψ that is 1-Lipschitz on R. The function θ → D i=1 ψ(θ i ) is also 1-Lipschitz with respect to the L 1 norm. This follows from the inequality:\n\nApplying Kantorovich duality, we obtain the bound:\n\nSince the bound in Eq. (E.15) does not depend on the value of x (n) , we have sup\n\nConsider the regime λ n ≺ DΞ -1 (q * 0 ). Recall that the the Ξ-VI has the Lagrangian formulation as min Ξ(q)≤r(λn) D KL (q ∥ q * 0 ) for some constant r(λ n ) depending on λ n . If Ξ(q * 0 ) ≤ r(λ n ), then q * λn = q * 0 , which implies Ξ(q * 0 ) ≥ Ξ(q * λn ) for all λ n . For fixed n, we have\n\nBy the T 1 -transportation inequality and Kantorovich duality, we have sup\n\nProof of Corollary 2. Under the assumptions,\n\nWhen we plug these terms in the upper bounds (2), Eq. (4.4) follows as the desired result.\n\nFor the linear model, denote w := σ -2 X T y and\n\nii is the i th diagonal entry of matrix B diag . The next result shows Ξ-VI respects log-concavity of the exact posterior.\n\nLemma 8. Let Assumption 6 hold. For λ n ∈ R+ , the solution q * λn to Eq. (4.1) is (κ 1 + κ 2 )log-concave. Moreover, for each i, the optimal EOT potential ϕ * λn,i is κ 2 /(λ n + 1)-convex and marginal m * λn,i is (κ 1 + κ 2 )-log-concave.\n\nProof of Lemma 8. We first prove existence. By Lagragian duality, Ξ-VI (Eq. (4.1)) is equivalent to min Ξ(q)≤r(λn) D KL (q ∥ q * 0 ). An optimizer of the latter problem exists because Ξ(•) has weakly closed sublevel set in P 2 (Θ) and because D KL (• ∥ q * 0 ) has weakly compact sub-level sets.\n\nRecall the Ξ-variational posterior be represented in term of optimal marginals m * λn and optimal EOT potentials ϕ * λn :\n\nBy Proposition 3, m * λn and ϕ * λn satisfy the following fixed point equations:\n\nexp -(λ n + 1)ϕ * λ,j (θ j ) π j (θ j )dθ j .\n\n(E.26)\n\nUsing equations Eq. (E.26) to replace m * λn in Eq. (E.25), the variational posterior q * λn satisfies q * λn (θ) ∝ exp\n\nWe now establish the log-concavity of q * λn . Applying Eq. (E.26) to Eq. (E.27), we get\n\n(E.28) For α ∈ [0, 1] and θ 0 i , θ 1 i ∈ Θ i , we have\n\nThe log-likelihood is κ 2 -concave, thus\n\nBy the Prékopa-Leindler inequality  (Theorem 19.16, Villani [2009] ), we have\n\nSince the logarithmic function is concave, we conclude φλn,i (αθ\n\nThus, the function -φλn,i (•) is κ 2 /(λ n +1)-concave. By the fixed point representation (E.26), m * λn is (κ 2 + κ 1 )-log-concave. Using the representation (E.27), we conclude that the distribution q * λn is (κ 2 + κ 1 )-log-concave.\n\nWe introduce some notations to streamline the two subsequent proofs.\n\nDefinition 4 (Nonlinear quadratic tilt). Let µ be a probability measure on R.\n\nand define the probability distribution µ ϕ,γ on R by setting\n\nFor any probability measure µ, we have c µ (ϕ, γ) < ∞ for any (ϕ, γ) ∈ L 1 (R) ∈ (0, ∞). Given the base measure µ, the tilted measure µ ϕ,γ (θ) has an exponential family density that has (ϕ(θ), θ 2 ) as the sufficient statistics. We call µ ϕ,γ a nonlinear quadratic tilt of µ.\n\nUsing Theorem 9 and Proposition 3, we have\n\nis defined in Eq. (E.29), and π i,λnϕ * λn,i ,\n\nWhen λ n = 0, Z D (0) is the normalizing constant of the exact posterior. When λ n > 0, we can view Z D (λ n ) as an approximation to Z D (0).\n\nThe log-concavity of q * λn induces an upper bound of Ξ(q * λn ) using the covariance matrix, the design matrix, and the regularization parameter. Lemma 9. Let Assumption 6 hold. the solution q * λn to Eq. (4.1) satisfies\n\nProof of Lemma 9. Any constant shift in q * λn is preserved by its marginal distribution m * λn . Since the KL divergence is invariant to constant shift, Ξ(q * λn ) is the same if we shift q * λn by a constant. WLOG, we can assume that E q\n\n, and m(θ) = D i=1 mi (θ i ). By the variational representation of mutual information, we have\n\nBy Lemma 8, ϕ * λn,i is κ 2 /(λ n + 1)-convex. Since π is κ 1 -log-concave, m is (κ 1 + κ 2 )-logconcave. By the log-Sobolev inequality, we have:\n\nUnder the assumed constraint E q * λn [θ] = 0, we conclude with the desired inequality:\n\nProof of Theorem 3. Define Boff := B off λn+1 and πi := π i,λnϕ * λn,i ,\n\nBy Lemma 8, q * λn is a (κ 1 + κ 2 )-log-concave. By Theorem 1 of  Lacker et al. [2024] , we have:\n\nFor any m ∈ M(Θ), we have\n\nWe invokve the upper bound on the log normalizer Eq. (E.32):\n\ninf\n\nBy the T 2 -transportion inequality (Theorem 1 and 2,  Otto and Villani [2000] ), we upper bound the Wasserstein metric with the square root of KL divergence:\n\nFor λ n ≻ tr(B 2 off ), we have inf m∈M(Θ) W 2 (q * λn , m)\n\nConsider the second regime λ n ≻ tr(B 2 off )/D. By the triangle inequality, we have sup\n\n.\n\n(E.35) Since ψ is 1-Lipschitz, we apply Katorovich duality to bound the second term.\n\nBy the subadditivity inequality of Wasserstein distance and Eq. (E.34), we have inf\n\nThe Lipschitzness implies ∥∇ψ∥ 2 ≤ 1. To bound the first term, we apply Poincaré inequality to the function\n\n.\n\n(E.38)\n\nCombining bounds Eq. (E.37) and Eq. (E.38), we have sup\n\nFor λ n ≻ tr(B 2 off )/D, the bounds implies the Eq. (4.10). Consider the third regime λ n ≺ (κ 1 + κ 2 ) tr Cov q * 0 (B off θ) -1 . Recall that Ξ-VI has a dual problem of the form min Ξ(q)≤r(λn) D KL (q ∥ q * 0 ) for some constant r(λ n ) depending on λ n . If Ξ(q * 0 ) ≤ r(λ n ), then q * λn = q * 0 , hence Ξ(q * 0 ) ≥ Ξ(q * λn ). For t < λ n and fixed n, we apply Lemma 9 to obtain an upper bound,\n\nFinally, consider the fourth regime λ n ≺ D(κ 1 + κ 2 ) tr Cov q * 0 (B off θ) -1 . We follow an analogous derivation as the third regime:\n\nSince q * 0 is (κ 1 + κ 2 )-log-concave, we invoke the T 2 -transportion inequality:\n\nThe remaining proofs for the third and fourth regimes are the same as the first two regimes, where we plug in the upper bounds for the KL divergence to upper-bound the Wasserstein distance. We skip repeating the details.\n\nProof of Corollary 3. Given tr(B 2 off ) = D i=1 η 2 i , Theorem 3 ensures that the convergence of W 2 (q * λn , m * λn ) holds for λ n ≻ D i=1 η 2 i . Since D i=1 η 2 i ≲ D 2 , we have W 2 (q * λn , m * λn ) converges in probability to zero, for any choice of λ n ≻ D.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Proofs Of Section A.3",
      "text": "We first state an auxiliary lemma to Theorem 4.\n\nLemma 10. Let q * λ be the Ξ-variational posterior. Then ELBO(q * λ ) and C λ are monotonically decreasing function of λ.\n\nProof of Lemma 10. Since q * λ is a maximizer of ELBO(q) -λΞ(q), we have\n\nFor λ 1 < λ 2 , we have\n\nBy Langragian duality, we have ELBO(q * λ ) = max Ξ(q)≤t(λ) ELBO(q) for t(λ) monotonically decreasing in λ.\n\nFor λ 1 < λ 2 , t(λ 1 ) ≥ t(λ 2 ) hence ELBO(q * λ 1 ) = max Ξ(q)≤t(λ 1 ) ELBO(q) ≥ max Ξ(q)≤t(λ 2 )\n\nELBO(q) = ELBO(q * λ 2 ).\n\nProof of Theorem 4. Let (P 2 (Θ), W 2 ) be the metric space. We want to show that the functionals F λ (q) := D KL (q ∥ q * 0 ) + λΞ(q). Γ-converge to F ∞ (q) := D KL (q ∥ q * 0 ) + ∞Ξ(q), as λ → ∞.\n\nTo verify Γ convergence, we make use of the property that the KL divergence functional D KL (• ∥ q * 0 ) and Ξ(.) functional are lower semicontinuous (l.s.c.) in Wasserstein metric. This is provided in Lemma 4.\n\nLet q ∈ P 2 (Θ) and W 2 (q λ , q) → 0. If q is a product measure, then\n\nThe first inequality holds because D KL (. ∥ q * 0 ) is l.s.c. If q is not a product measure, we have lim inf n→∞ Ξ(q n ) ≥ Ξ(q) > 0 by the lower semicontinuity of Ξ. Since the KL term is nonnegative, we have\n\nThus the liminf inequality is verified.\n\nNext we show the existence of a recovery sequence. For any q ∈ P 2 (Θ), we take q λ = q. If q is a product measure, then\n\nThis verifies the limsup inequality. Combining the liminf and limsup inequalities, we obtain that F ∞ = Γ -lim λ→∞ F λ .\n\nNext we prove that the sequence F λ is eqi-coercive. Take λ j → ∞ and q λ j such that F λ j (q λ j ) ≤ t for all j. Then Ξ(q λ j ) = o(1) because λ j Ξ(q λ j ) is bounded as λ j → ∞. Moreover, D KL (q λ j ∥ q * 0 ) is upper bounded by t. Since D KL (. ∥ q * 0 ) is Wasserstein (geodastically) convex, it is coercive by Lemma 2.4.8 of  Ambrosio et al. [2005] . Thus, there exists a converging sequence q ′ λ j such that D KL (q ′ λ j ∥ q * 0 ) ≤ D KL (q λ j ∥ q * 0 ) + o(1). Since Ξ(q λ j ) = o(1), we obtain that F λ j (q ′ j ) ≤ F λ j (q j ) + o(1). This verifies the equi-coercivity of F λ . ≤ F λ j (q λ j ) + λ j Ξ(q 0 ) = F λ j (q λ j ) + o(1), Finally, by the fundamental theorem of Γ convergence, we conclude that W 2 (q * 0 , q * λ ) → 0, as λ → 0, where q 0 is a minimizer of F 0 , and |C λ -C 0 | → 0, as λ → 0.\n\nSince q * 0 is the unique minimizer of F 0 , we conclude that q 0 = q * 0 . To prove the convergence of optimal cost, we note that F λ (q * λ ) ≤ F λ (q * 0 ) = F 0 (q * 0 ) + λΞ(q * 0 ).\n\nThus, |C λ -C 0 | = |F λ (q * λ ) -F 0 (q * 0 )| ≤ λΞ(q * 0 ).\n\nDefine a functional Φ λ that combines the objective functional of the inner variational objective problem and Assumption 7: Φ λ (q) := E q -ℓ(x (n) ; θ) + For proving Theorem 6, we introduce a Pythagorean theorem for the inner variational problem.\n\nLemma 11. Let q λ ∈ C(m) be a optimizer of Φ λ over C(m). Then D KL (q, q λ ) ≤ Φ λ (q) -Φ λ (q λ ), for all q ∈ C(m).\n\nProof of Lemma 11. We recall definition of the auxiliary measure q aux in the proof of Theorem 9, q aux (θ) = α -1 e ℓ(x (n) ;θ)-D i=1 ϕ i (θ i ) λ+1 m(θ), where α is the normalizing constant. Then Φ λ (q) = D KL (q ∥ q aux ) -log α, (E.41) so that the entropic optimal transport problem is equivalent to minimizing D KL (• ∥ q aux ).\n\nIn particular, q λ = arg min C(m) D KL (q ∥ q aux ) and the Pythagorean theorem for relative entropy (Theorem 2.2,  [Csiszár, 1975] ) yields D KL (q ∥ q aux ) ≥ D KL (q λ ∥ q aux ) + D KL (q ∥ q λ ) for all q ∈ C(m).\n\nIn view of Eq. (E.41), the desired claim holds.\n\nThe next Lemma is also auxiliary to the proof of Theorem 6.\n\nLemma 12. Let q * λ ∈ C(m * ) be a optimizer of Φ λ over C(m * ), and q s λ ∈ C( m) be its shadow. Then |Φ λ (q * λ ) -Φ λ (q s λ )| ≤ LW 2 (q * λ , q s λ ).\n\nProof of Lemma 12. Using the Lipschitz cost assumption and Lemma 5, we have\n\nϕ i (θ i ) + (λ + 1)Ξ(q * λ ).\n\nϕ i (θ i ) -LW 2 (q * λ , q s λ ) + (λ + 1)Ξ(q s λ )\n\n= Φ λ (q s λ ) -LW 2 (q * λ , q s λ ).\n\nThe claim follows by a symmetric argument.\n\nProof of Theorem 6. Consider the optimizers qλ ∈ C( m) and q * λ ∈ C(m * λ ). Let q s λ ∈ C( m) be the shadow of q * λ . By Lemma 5 and the Lipschitz cost assumption, we have:\n\nLemma 12 implies Φ λ (q λ ) -Φ λ (q * λ ) ≤ LW 2 (m * λ , m). Adding the inequalities shows:\n\n|Φ λ (q λ ) -Φ λ (q s λ )| ≤ 2LW 2 (m * λ , m).\n\nBy Lemma 11, we have that D KL (π, π * ) ≤ 2LW 2 (m * λ , m), and the transport inequality assumption implies:\n\nW ρ (q s λ , qλ ) ≤ C ρ (2LW 2 (m * λ , m))\n\n.\n\nBy Lemma 5, we get W 2 (q * λ , q s λ ) = W 2 (m * λ , m). We conclude the proof via the triangle inequality, W 2 (q * λ , qλ ) ≤ W 2 (q * λ , q s λ ) + W 2 (q s λ , qλ ) ≤ W 2 (m * λ , m) + C q (2LW 2 (m * λ , m)) First, we confirm that the optimal µ * is equal to the true µ 0 . The first-order optimality condition of f with respect to µ yields ∂ µ f (µ, Σ) = 0 =⇒ Λ 0 (µ -µ 0 ) = 0.\n\nSince Σ 0 is full-rank, its inverse Λ 0 is full-rank hence the equality above yields µ * = µ 0 . Now we turn to Σ * . The first-order optimality yields the following characterization of the optimal precision matrix Λ * :\n\n, (E.42) where Σ * diag , Σ * off denote the diagonal and off-diagonal minor of Σ * , respectively. By Eq. (E.42), we have\n\nBy Lemma 2, we have the inequality\n\nBy Hua's identity, we have\n\n.\n\nTaking the diagonal elements on both sides, we have\n\nNote that B is a matrix with zero diagonal entries. By Lemma 2, we have\n\nThis implies that\n\nwhich after simplification yields Σ * diag ≤ Σ 0,diag . (E.44) By Hua's identity, we have\n\n.\n\nBy Eq. (E.44), the matrix Σ * -Σ 0 is negative semidefinite. By Eq. (E.43), we have Σ * diag ≥ Λ -1 0,diag . Then\n\n.\n\nSince Σ * diag ≤ Σ 0,diag , we obtain a lower bound with analogous techniques.\n\nThis lower bound holds when the matrix on the right hand side is negative semidefinite. To see that, we have\n\n, Since Λ 0,diag ≥ Σ -1 0,diag , we have\n\nThis completes the proof.\n\nNext, we provide the explicit formula for the Ξ-VI solution when the exact posterior is bivariate Gaussian. where ψ(λ) = 1 2 λ + 1 + λ - Proof of Corollary 5. We use the well known property of the 2-Wasserstein metric that for any q 0 , q 1 ∈ P 2 (Θ),\n\nW 2 2 (q 0,i , q 1,i ) ≤ W 2 2 (q 0 , q 1 ). (E.45)\n\nBy Theorem 4, W 2 (q * λ , q * ∞ ) → 0 as λ → ∞, hence W 2 (m * λ , q * ∞ ) → 0 by the identity Eq. (E.45) which implies W 2 (q * (∞) λ , q * λ ) → 0 by Theorem 6. An analogous derivation holds for λ → 0.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Additional Simulation Results",
      "text": "",
      "page_start": 40,
      "page_end": 40
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Ξ-VI solutions for a bivariate Gaussian posterior for varying λ. The left panel",
      "page": 10
    },
    {
      "caption": "Figure 1: illustrates the interpolation, where the regularization downweights the off-",
      "page": 10
    },
    {
      "caption": "Figure 2: Left. accuracy of Ξ-VI for Laplace linear regression, measured in W2 across",
      "page": 11
    },
    {
      "caption": "Figure 2: (b) shows the runtime of the approximate coordinate ascent algorithm for",
      "page": 12
    },
    {
      "caption": "Figure 3: Contour plots for the joint distribution of θ1",
      "page": 13
    },
    {
      "caption": "Figure 4: illustrates credible intervals for maximum and minimum treatment effects across schools,",
      "page": 13
    },
    {
      "caption": "Figure 4: Comparison of the 95% posterior credible intervals for the maximum and minimum",
      "page": 14
    },
    {
      "caption": "Figure 5: (b) shows the runtime of Algorithm 1 for the 8-schools",
      "page": 14
    },
    {
      "caption": "Figure 3: 4. Asymptotic Theory",
      "page": 14
    },
    {
      "caption": "Figure 5: Left. approximation accuracy for the Eight School model of Ξ-VI across varying",
      "page": 15
    },
    {
      "caption": "Figure 6: Approximation errors for posterior covariance for Laplace linear regression. The",
      "page": 65
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "MFVI\nTrue\nλ = 0\nλ = 1\nλ = 10\nλ = 1000",
          "θ2 − θ5": "[-12.08, 18.48]\n[-8.50, 14.90]\n[-7.81, 13.77]\n[-9.02, 13.54]\n[-10.43, 13.01]\n[-10.51, 12.93]",
          "θ6 − θ7": "[-20.12, 11.37]\n[-17.74, 7.30]\n[-16.17, 6.49]\n[-16.71, 7.44]\n[-15.77, 8.75]\n[-15.86, 9.53]",
          "θ2 − θ4": "[-13.99, 15.79]\n[-11.28, 12.40]\n[-10.42, 12.06]\n[-11.57, 11.87]\n[-12.57, 12.82]\n[-12.55, 13.19]",
          "θ4 − θ8": "[-15.37, 14.97]\n[-13.02, 12.52]\n[-11.19, 12.70]\n[-11.73, 13.33]\n[-12.21, 14.50]\n[-12.71, 14.47]",
          "θ1 − θ2": "[-13.47, 17.10]\n[-9.21, 16.55]\n[-8.12, 15.81]\n[-9.09, 15.97]\n[-10.54, 15.41]\n[-10.79, 15.64]"
        }
      ],
      "page": 66
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "MFVI\nTrue\nλ = 0\nλ = 1\nλ = 10\nλ = 1000",
          "θ2 − θ8": "[-14.83, 17.07]\n[-12.09, 12.73]\n[-10.15, 13.41]\n[-11.26, 13.57]\n[-11.58, 14.47]\n[-12.14, 14.34]",
          "θ3 − θ8": "[-17.71, 13.88]\n[-16.08, 11.05]\n[-14.54, 11.56]\n[-14.31, 12.75]\n[-14.69, 13.74]\n[-14.48, 14.32]",
          "θ5 − θ6": "[-16.77, 14.19]\n[-12.62, 10.31]\n[-11.12, 9.70]\n[-11.49, 10.27]\n[-11.15, 10.96]\n[-12.00, 11.09]",
          "θ2 − θ7": "[-17.30, 13.41]\n[-14.97, 9.20]\n[-13.66, 8.31]\n[-15.28, 8.95]\n[-15.09, 10.55]\n[-14.75, 10.63]",
          "θ3 − θ4": "[-17.62, 13.49]\n[-14.79, 10.45]\n[-14.33, 10.41]\n[-14.95, 11.25]\n[-15.18, 12.35]\n[-15.21, 13.12]"
        }
      ],
      "page": 66
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Non-exponentially weighted aggregation: regret bounds for unbounded loss functions",
      "authors": [
        "Pierre Alquier"
      ],
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "2",
      "title": "User-friendly introduction to pac-bayes bounds",
      "authors": [
        "Pierre Alquier"
      ],
      "year": "2024",
      "venue": "Foundations and Trends in Machine Learning",
      "doi": "10.1561/2200000100"
    },
    {
      "citation_id": "3",
      "title": "Simpler pac-bayesian bounds for hostile data",
      "authors": [
        "Pierre Alquier",
        "Benjamin Guedj"
      ],
      "year": "2018",
      "venue": "Machine Learning",
      "doi": "10.1007/s10994-017-5690-0"
    },
    {
      "citation_id": "4",
      "title": "Concentration of tempered posteriors and of their variational approximations",
      "authors": [
        "Pierre Alquier",
        "James Ridgway"
      ],
      "year": "2020",
      "venue": "Ann. Statist",
      "doi": "10.1214/19-AOS1855"
    },
    {
      "citation_id": "5",
      "title": "On the properties of variational approximations of Gibbs posteriors",
      "authors": [
        "Pierre Alquier",
        "James Ridgway",
        "Nicolas Chopin"
      ],
      "year": "2016",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "6",
      "title": "Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration",
      "authors": [
        "Jason Altschuler",
        "Jonathan Niles-Weed",
        "Philippe Rigollet"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Hardness results for multimarginal optimal transport problems",
      "authors": [
        "Jason Altschuler",
        "Enric Boix-Adsera"
      ],
      "year": "2021",
      "venue": "Discrete Optimization"
    },
    {
      "citation_id": "8",
      "title": "Polynomial-time algorithms for multimarginal optimal transport problems with structure",
      "authors": [
        "Jason Altschuler",
        "Enric Boix-Adsera"
      ],
      "year": "2023",
      "venue": "Mathematical Programming"
    },
    {
      "citation_id": "9",
      "title": "Gradient Flows",
      "authors": [
        "Luigi Ambrosio",
        "Nicola Gigli",
        "Giuseppe Savaré"
      ],
      "year": "2005",
      "venue": "Metric Spaces and In the Space of Probability Measures"
    },
    {
      "citation_id": "10",
      "title": "Universality of the mean-field for the Potts model",
      "authors": [
        "Anirban Basak",
        "Sumit Mukherjee"
      ],
      "year": "2017",
      "venue": "Probability Theory and Related Fields"
    },
    {
      "citation_id": "11",
      "title": "Statistical Decision Theory and Bayesian Analysis",
      "authors": [
        "O James",
        "Berger"
      ],
      "year": "2013",
      "venue": "ISBN"
    },
    {
      "citation_id": "12",
      "title": "Statistical and computational trade-offs in variational inference: A case study in inferential model selection",
      "authors": [
        "Kush Bhatia",
        "Nikki Kuang",
        "Yi-An Ma",
        "Yixin Wang"
      ],
      "year": "2022",
      "venue": "Statistical and computational trade-offs in variational inference: A case study in inferential model selection",
      "arxiv": "arXiv:2207.11208"
    },
    {
      "citation_id": "13",
      "title": "On the Bures-Wasserstein distance between positive definite matrices",
      "authors": [
        "Rajendra Bhatia",
        "Tanvi Jain",
        "Yongdo Lim"
      ],
      "year": "2019",
      "venue": "Expositiones Mathematicae"
    },
    {
      "citation_id": "14",
      "title": "Bayesian fractional posteriors",
      "authors": [
        "Anirban Bhattacharya",
        "Debdeep Pati",
        "Yun Yang"
      ],
      "year": "2019",
      "venue": "Annals of Statistics"
    },
    {
      "citation_id": "15",
      "title": "On the convergence of coordinate ascent variational inference",
      "authors": [
        "Anirban Bhattacharya",
        "Debdeep Pati",
        "Yun Yang"
      ],
      "year": "2023",
      "venue": "On the convergence of coordinate ascent variational inference",
      "arxiv": "arXiv:2306.01122"
    },
    {
      "citation_id": "16",
      "title": "Asymptotic normality of maximum likelihood and its variational approximation for stochastic blockmodels",
      "authors": [
        "Peter Bickel",
        "David Choi",
        "Xiangyu Chang",
        "Hai Zhang"
      ],
      "year": "1922",
      "venue": "Ann. Statist",
      "doi": "10.1214/13-AOS1124"
    },
    {
      "citation_id": "17",
      "title": "Variational inference: A review for statisticians",
      "authors": [
        "Alp David M Blei",
        "Jon Kucukelbir",
        "Mcauliffe"
      ],
      "year": "2017",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "18",
      "title": "Local minimization, variational evolution and Γ-convergence",
      "authors": [
        "Andrea Braides"
      ],
      "year": "2014",
      "venue": "Local minimization, variational evolution and Γ-convergence"
    },
    {
      "citation_id": "19",
      "title": "On the linear convergence of the multimarginal Sinkhorn algorithm",
      "authors": [
        "Guillaume Carlier"
      ],
      "year": "2022",
      "venue": "SIAM Journal on Optimization"
    },
    {
      "citation_id": "20",
      "title": "Stan: A probabilistic programming language",
      "authors": [
        "Bob Carpenter",
        "Andrew Gelman",
        "Matt Hoffman",
        "Daniel Lee",
        "Ben Goodrich",
        "Michael Betancourt",
        "Marcus Brubaker",
        "Jiqiang Guo",
        "Peter Li",
        "Allen Riddell"
      ],
      "year": "2015",
      "venue": "Journal of Statistical Software"
    },
    {
      "citation_id": "21",
      "title": "Mean-field variational inference with the TAP free energy: Geometric and statistical properties in linear models",
      "authors": [
        "Michael Celentano",
        "Licong Zhou Fan",
        "Song Lin",
        "Mei"
      ],
      "year": "2023",
      "venue": "Mean-field variational inference with the TAP free energy: Geometric and statistical properties in linear models",
      "arxiv": "arXiv:2311.08442"
    },
    {
      "citation_id": "22",
      "title": "Local convexity of the TAP free energy and AMP convergence for Z 2 -synchronization",
      "authors": [
        "Michael Celentano",
        "Fan Zhou",
        "Song Mei"
      ],
      "year": "2023",
      "venue": "Annals of Statistics"
    },
    {
      "citation_id": "23",
      "title": "Computational and statistical tradeoffs via convex relaxation",
      "authors": [
        "Venkat Chandrasekaran",
        "Michael I Jordan"
      ],
      "year": "2013",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "24",
      "title": "A formula for the time derivative of the entropic cost and applications",
      "authors": [
        "Giovanni Conforti",
        "Luca Tamanini"
      ],
      "year": "2021",
      "venue": "Journal of Functional Analysis"
    },
    {
      "citation_id": "25",
      "title": "Elements of Information Theory",
      "authors": [
        "T Cover",
        "J Thomas"
      ],
      "year": "2006",
      "venue": "Elements of Information Theory"
    },
    {
      "citation_id": "26",
      "title": "I-divergence geometry of probability distributions and minimization problems",
      "authors": [
        "Imre Csiszár"
      ],
      "year": "1975",
      "venue": "Annals of Probability"
    },
    {
      "citation_id": "27",
      "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
      "authors": [
        "Marco Cuturi"
      ],
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Forwardbackward Gaussian variational inference via JKO in the Bures-Wasserstein space",
      "authors": [
        "Krishna Michael Ziyang Diao",
        "Sinho Balasubramanian",
        "Adil Chewi",
        "Salim"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "29",
      "title": "Quantitative stability of regularized optimal transport and convergence of Sinkhorn's algorithm",
      "authors": [
        "Stephan Eckstein",
        "Marcel Nutz"
      ],
      "year": "2022",
      "venue": "SIAM Journal on Mathematical Analysis"
    },
    {
      "citation_id": "30",
      "title": "On the complexity of the optimal transport problem with graph-structured cost",
      "authors": [
        "Jiaojiao Fan",
        "Isabel Haasler",
        "Johan Karlsson",
        "Yongxin Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "31",
      "title": "TAP free energy, spin glasses and variational inference",
      "authors": [
        "Song Zhou Fan",
        "Andrea Mei",
        "Montanari"
      ],
      "year": "2021",
      "venue": "Annals of Probability",
      "doi": "10.1214/20-AOP1443"
    },
    {
      "citation_id": "32",
      "title": "Bayesian Data Analysis",
      "authors": [
        "Andrew Gelman",
        "Hal John B Carlin",
        "Donald Stern",
        "Rubin"
      ],
      "year": "1995",
      "venue": "Bayesian Data Analysis"
    },
    {
      "citation_id": "33",
      "title": "An instability in variational inference for topic models",
      "authors": [
        "Behrooz Ghorbani",
        "Hamid Javadi",
        "Andrea Montanari"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "34",
      "title": "Fundamentals of Nonparametric Bayesian Inference",
      "authors": [
        "Subhashis Ghosal",
        "Aad Van Der",
        "Vaart"
      ],
      "year": "2017",
      "venue": "Fundamentals of Nonparametric Bayesian Inference"
    },
    {
      "citation_id": "35",
      "title": "Covariances, robustness and variational Bayes",
      "authors": [
        "Ryan Giordano",
        "Tamara Broderick",
        "Michael Jordan"
      ],
      "year": "2018",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "36",
      "title": "Theory of Gaussian variational approximation for a Poisson mixed model",
      "authors": [
        "Peter Hall",
        "John Ormerod",
        "M P Wand"
      ],
      "year": "2011",
      "venue": "Statistica Sinica"
    },
    {
      "citation_id": "37",
      "title": "Asymptotic normality and valid inference for Gaussian variational approximation",
      "authors": [
        "Peter Hall",
        "Tung Pham",
        "M Wand",
        "S Wang"
      ],
      "year": "2011",
      "venue": "Ann. Statist",
      "doi": "10.1214/11-AOS908"
    },
    {
      "citation_id": "38",
      "title": "Adversarial interpretation of bayesian inference",
      "authors": [
        "Hisham Husain",
        "Jeremias Knoblauch"
      ],
      "year": "2022",
      "venue": "International Conference on Algorithmic Learning Theory"
    },
    {
      "citation_id": "39",
      "title": "Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space",
      "authors": [
        "Yiheng Jiang",
        "Sinho Chewi",
        "Aram-Alexandre Pooladian"
      ],
      "year": "2023",
      "venue": "Algorithms for mean-field variational inference via polyhedral optimization in the Wasserstein space",
      "arxiv": "arXiv:2312.02849"
    },
    {
      "citation_id": "40",
      "title": "An introduction to variational methods for graphical models",
      "authors": [
        "Zoubin Michael I Jordan",
        "Tommi Ghahramani",
        "Lawrence Jaakkola",
        "Saul"
      ],
      "year": "1999",
      "venue": "Machine learning"
    },
    {
      "citation_id": "41",
      "title": "Improved scaling with dimension in the Bernstein-von Mises theorem for two statistical models",
      "authors": [
        "Anya Katsevich"
      ],
      "year": "2023",
      "venue": "Improved scaling with dimension in the Bernstein-von Mises theorem for two statistical models",
      "arxiv": "arXiv:2308.06899"
    },
    {
      "citation_id": "42",
      "title": "On the convergence of black-box variational inference",
      "authors": [
        "Kyurae Kim",
        "Jisu Oh",
        "Kaiwen Wu",
        "Yian Ma",
        "Jacob Gardner"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems (to Appear)"
    },
    {
      "citation_id": "43",
      "title": "An optimization-centric view on bayesŕule: Reviewing and generalizing variational inference",
      "authors": [
        "Jeremias Knoblauch",
        "Jack Jewson",
        "Theodoros Damoulas"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "44",
      "title": "On the complexity of approximating Wasserstein barycenters",
      "authors": [
        "Alexey Kroshnin",
        "Nazarii Tupitsa",
        "Darina Dvinskikh",
        "Pavel Dvurechensky",
        "Alexander Gasnikov",
        "Cesar Uribe"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "45",
      "title": "Automatic differentiation variational inference",
      "authors": [
        "Alp Kucukelbir",
        "Dustin Tran",
        "Rajesh Ranganath",
        "Andrew Gelman",
        "David Blei"
      ],
      "year": "2017",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "46",
      "title": "Mean field approximations via log-concavity. International Mathematics Research Notices",
      "authors": [
        "Daniel Lacker",
        "Sumit Mukherjee",
        "Lane Yeung"
      ],
      "year": "2024",
      "venue": "Mean field approximations via log-concavity. International Mathematics Research Notices"
    },
    {
      "citation_id": "47",
      "title": "Silvère Bonnabel, and Philippe Rigollet. Variational inference via Wasserstein gradient flows",
      "authors": [
        "Marc Lambert",
        "Sinho Chewi",
        "Francis Bach"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "On the complexity of approximating multimarginal optimal transport",
      "authors": [
        "Tianyi Lin",
        "Nhat Ho",
        "Marco Cuturi",
        "Michael Jordan"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "49",
      "title": "Stein variational gradient descent: A general purpose Bayesian inference algorithm",
      "authors": [
        "Qiang Liu",
        "Dilin Wang"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "Boosting variational inference: an optimization perspective",
      "authors": [
        "Locatello",
        "Khanna",
        "Ghosh"
      ],
      "year": "2018",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "51",
      "title": "Variational boosting: Iteratively refining posterior approximations. International Conference on Machine Learning",
      "authors": [
        "C Andrew",
        "Nicholas Miller",
        "Ryan Foti",
        "Adams"
      ],
      "year": "2017",
      "venue": "Variational boosting: Iteratively refining posterior approximations. International Conference on Machine Learning"
    },
    {
      "citation_id": "52",
      "title": "Robust Bayesian inference via coarsening",
      "authors": [
        "W Jeffrey",
        "David Miller",
        "Dunson"
      ],
      "year": "2018",
      "venue": "Journal of the American Statistical Association"
    },
    {
      "citation_id": "53",
      "title": "Expectation propagation for approximate Bayesian inference",
      "authors": [
        "Thomas P Minka"
      ],
      "year": "2013",
      "venue": "Expectation propagation for approximate Bayesian inference",
      "arxiv": "arXiv:1301.2294"
    },
    {
      "citation_id": "54",
      "title": "A short tutorial on mean-field spin glass techniques for non-physicists",
      "authors": [
        "Andrea Montanari",
        "Subhabrata Sen"
      ],
      "year": "2022",
      "venue": "A short tutorial on mean-field spin glass techniques for non-physicists",
      "arxiv": "arXiv:2204.02909"
    },
    {
      "citation_id": "55",
      "title": "Mean field for the stochastic blockmodel: Optimization landscape and convergence issues",
      "authors": [
        "Soumendu Sundar",
        "Purnamrita Sarkar"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "56",
      "title": "Variational inference in high-dimensional linear regression",
      "authors": [
        "Sumit Mukherjee",
        "Subhabrata Sen"
      ],
      "year": "2022",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "57",
      "title": "A mean field approach to empirical Bayes estimation in high-dimensional linear regression",
      "authors": [
        "Sumit Mukherjee",
        "Bodhisattva Sen",
        "Subhabrata Sen"
      ],
      "year": "2023",
      "venue": "A mean field approach to empirical Bayes estimation in high-dimensional linear regression",
      "arxiv": "arXiv:2309.16843"
    },
    {
      "citation_id": "58",
      "title": "Introduction to Entropic Optimal Transport. Lecture notes",
      "authors": [
        "Marcel Nutz"
      ],
      "year": "2021",
      "venue": "Introduction to Entropic Optimal Transport. Lecture notes"
    },
    {
      "citation_id": "59",
      "title": "Advanced Mean Field Methods: Theory and Practice",
      "authors": [
        "Manfred Opper",
        "David Saad"
      ],
      "year": "2001",
      "venue": "Advanced Mean Field Methods: Theory and Practice"
    },
    {
      "citation_id": "60",
      "title": "Generalization of an inequality by Talagrand and links with the logarithmic Sobolev inequality",
      "authors": [
        "Felix Otto",
        "Cédric Villani"
      ],
      "year": "2000",
      "venue": "Journal of Functional Analysis"
    },
    {
      "citation_id": "61",
      "title": "Critical dimension in the semiparametric Bernstein-von Mises theorem",
      "authors": [
        "E Maxim",
        "Vladimir Panov",
        "Spokoiny"
      ],
      "year": "2014",
      "venue": "Proceedings of the Steklov Institute of Mathematics"
    },
    {
      "citation_id": "62",
      "title": "Dynamics of coordinate ascent variational inference: A case study in 2D Ising models",
      "authors": [
        "Sean Plummer",
        "Debdeep Pati",
        "Anirban Bhattacharya"
      ],
      "year": "2020",
      "venue": "Entropy"
    },
    {
      "citation_id": "63",
      "title": "",
      "authors": [
        "Yury Polyanskiy",
        "Yihong Wu"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "64",
      "title": "Sub-optimality of the naive mean field approximation for proportional highdimensional linear regression",
      "authors": [
        "Jiaze Qiu"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "65",
      "title": "Black box variational inference. Artificial intelligence and statistics",
      "authors": [
        "Rajesh Ranganath",
        "Sean Gerrish",
        "David Blei"
      ],
      "year": "2014",
      "venue": "Black box variational inference. Artificial intelligence and statistics"
    },
    {
      "citation_id": "66",
      "title": "Hierarchical variational models. International Conference on Machine Learning",
      "authors": [
        "Rajesh Ranganath",
        "Dustin Tran",
        "David Blei"
      ],
      "year": "2016",
      "venue": "Hierarchical variational models. International Conference on Machine Learning"
    },
    {
      "citation_id": "67",
      "title": "Variational Bayes for high-dimensional linear regression with sparse priors",
      "authors": [
        "Kolyan Ray",
        "Botond Szabó"
      ],
      "year": "2022",
      "venue": "J. Amer. Statist. Assoc",
      "doi": "10.1080/01621459.2020.1847121"
    },
    {
      "citation_id": "68",
      "title": "Spike and slab variational Bayes for high dimensional logistic regression",
      "authors": [
        "Kolyan Ray",
        "Botond Szabo",
        "Gabriel Clara"
      ],
      "year": "2020",
      "venue": "Spike and slab variational Bayes for high dimensional logistic regression",
      "arxiv": "arXiv:2010.11665"
    },
    {
      "citation_id": "69",
      "title": "Improving variational methods via pairwise linear response identities",
      "authors": [
        "Jack Raymond",
        "Federico Ricci-Tersenghi"
      ],
      "year": "2017",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "70",
      "title": "Variational inference with normalizing flows. 32nd International Conference on Machine Learning, ICML 2015",
      "authors": [
        "Danilo Jimenez",
        "Shakir Mohamed"
      ],
      "year": "2015",
      "venue": "Variational inference with normalizing flows. 32nd International Conference on Machine Learning, ICML 2015"
    },
    {
      "citation_id": "71",
      "title": "Monte Carlo Statistical Methods",
      "authors": [
        "C Robert",
        "G Casella"
      ],
      "year": "2004",
      "venue": "Monte Carlo Statistical Methods"
    },
    {
      "citation_id": "72",
      "title": "Optimal Transport for Applied Mathematicians",
      "authors": [
        "Filippo Santambrogio"
      ],
      "year": "2015",
      "venue": "Optimal Transport for Applied Mathematicians"
    },
    {
      "citation_id": "73",
      "title": "Copula variational inference",
      "authors": [
        "Dustin Tran",
        "David Blei",
        "Edo Airoldi"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "74",
      "title": "Hierarchical implicit models and likelihood-free variational inference",
      "authors": [
        "Dustin Tran",
        "Rajesh Ranganath",
        "David Blei"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "75",
      "title": "Probability in high dimension",
      "authors": [
        "Ramon Van Handel"
      ],
      "year": "2014",
      "venue": "Lecture Notes"
    },
    {
      "citation_id": "76",
      "title": "Optimal Transport: Old and New",
      "authors": [
        "Villani"
      ],
      "year": "2009",
      "venue": "Optimal Transport: Old and New"
    },
    {
      "citation_id": "77",
      "title": "High-Dimensional Statistics: A Non-Asymptotic Viewpoint",
      "authors": [
        "Wainwright Martin"
      ],
      "year": "2019",
      "venue": "High-Dimensional Statistics: A Non-Asymptotic Viewpoint"
    },
    {
      "citation_id": "78",
      "title": "Graphical Models, Exponential Families, and Variational Inference",
      "authors": [
        "J Martin",
        "Michael Wainwright",
        "Jordan"
      ],
      "year": "2008",
      "venue": "Foundations and Trends® in Machine Learning"
    },
    {
      "citation_id": "79",
      "title": "Lack of consistency of mean field and variational Bayes approximations for state space models",
      "authors": [
        "Bo Wang",
        "D M Titterington"
      ],
      "year": "2004",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "80",
      "title": "Statistical and computational trade-offs in estimation of sparse principal components",
      "authors": [
        "Tengyao Wang",
        "Quentin Berthet",
        "Richard Samworth"
      ],
      "year": "2016",
      "venue": "The Annals of Statistics"
    },
    {
      "citation_id": "81",
      "title": "Frequentist consistency of variational Bayes",
      "authors": [
        "Yixin Wang",
        "David Blei"
      ],
      "year": "2019",
      "venue": "J. Amer. Statist. Assoc",
      "doi": "10.1080/01621459.2018.1473776"
    },
    {
      "citation_id": "82",
      "title": "Generalized variational inference in function spaces: Gaussian measures meet bayesian deep learning",
      "authors": [
        "D Veit",
        "Robert Wild",
        "Dino Hu",
        "Sejdinovic"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "83",
      "title": "A rigorous link between deep ensembles and (variational) bayesian methods",
      "authors": [
        "David Veit",
        "Sahra Wild",
        "Dino Ghalebikesabi",
        "Jeremias Sejdinovic",
        "Knoblauch"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "84",
      "title": "A generalized mean field algorithm for variational inference in exponential families",
      "authors": [
        "Eric Xing",
        "Michael Jordan",
        "Stuart Russell"
      ],
      "year": "2012",
      "venue": "A generalized mean field algorithm for variational inference in exponential families",
      "arxiv": "arXiv:1212.2512"
    },
    {
      "citation_id": "85",
      "title": "The computational asymptotics of Gaussian variational inference and the Laplace approximation",
      "authors": [
        "Zuheng Xu",
        "Trevor Campbell"
      ],
      "year": "2022",
      "venue": "Statistics and Computing",
      "doi": "10.1007/s11222-022-10125-y"
    },
    {
      "citation_id": "86",
      "title": "Nonlinear large deviations: Beyond the hypercube",
      "authors": [
        "J Yan"
      ],
      "year": "2020",
      "venue": "Annals of Applied Probability",
      "doi": "10.1214/19-AAP1516"
    },
    {
      "citation_id": "87",
      "title": "α-variational inference with statistical guarantees",
      "authors": [
        "Yun Yang",
        "Debdeep Pati",
        "Anirban Bhattacharya"
      ],
      "year": "2020",
      "venue": "Ann. Statist",
      "doi": "10.1214/19-AOS1827"
    },
    {
      "citation_id": "88",
      "title": "Mean field variational inference via Wasserstein gradient flow",
      "authors": [
        "Rentian Yao",
        "Yun Yang"
      ],
      "year": "2022",
      "venue": "Mean field variational inference via Wasserstein gradient flow",
      "arxiv": "arXiv:2207.08074"
    },
    {
      "citation_id": "89",
      "title": "Optimal information processing and bayes's theorem",
      "authors": [
        "Arnold Zellner"
      ],
      "year": "1988",
      "venue": "The American Statistician"
    },
    {
      "citation_id": "90",
      "title": "Theoretical and computational guarantees of mean field variational inference for community detection",
      "authors": [
        "Anderson Zhang",
        "Harrison Zhou"
      ],
      "year": "2020",
      "venue": "Ann. Statist",
      "doi": "10.1214/19-AOS1898"
    },
    {
      "citation_id": "91",
      "title": "Convergence rates of variational posterior distributions",
      "authors": [
        "Fengshuo Zhang",
        "Chao Gao"
      ],
      "year": "2020",
      "venue": "Annals of Statistics"
    }
  ]
}