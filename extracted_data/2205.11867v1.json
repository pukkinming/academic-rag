{
  "paper_id": "2205.11867v1",
  "title": "Building A Dialogue Corpus Annotated With Expressed And Experienced Emotions",
  "published": "2022-05-24T07:40:11Z",
  "authors": [
    "Tatsuya Ide",
    "Daisuke Kawahara"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In communication, a human would recognize the emotion of an interlocutor and respond with an appropriate emotion, such as empathy and comfort. Toward developing a dialogue system with such a human-like ability, we propose a method to build a dialogue corpus annotated with two kinds of emotions. We collect dialogues from Twitter and annotate each utterance with the emotion that a speaker put into the utterance (expressed emotion) and the emotion that a listener felt after listening to the utterance (experienced emotion). We built a dialogue corpus in Japanese using this method, and its statistical analysis revealed the differences between expressed and experienced emotions. We conducted experiments on recognition of the two kinds of emotions. The experimental results indicated the difficulty in recognizing experienced emotions and the effectiveness of multi-task learning of the two kinds of emotions. We hope that the constructed corpus will facilitate the study on emotion recognition in a dialogue and emotionaware dialogue response generation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Text-based communication has become indispensable as society accelerates online. In natural language processing, communication between humans and machines has attracted attention, and the development of dialogue systems has been a hot topic. Through the invention of Transformer  (Vaswani et al., 2017)  and the success of transfer learning (e.g.,  Radford et al. (2018) ;  Devlin et al. (2019) ), the performance of natural language understanding models and dialogue systems continues to improve. In recent years, there have been studies toward building open-domain neural chatbots that can generate a human-like response  (Zhou et al., 2020; Adiwardana et al., 2020; Roller et al., 2021) .\n\nOne of the keys to building more human-like chatbots is to generate a response that takes into account the emotion of the interlocutor. A human would recognize the emotion of the interlocutor and respond with an appropriate emotion, such as empathy and comfort, or give a response that promotes positive emotion of the interlocutor. Accordingly, developing a chatbot with such a human-like ability  (Rashkin et al., 2019; Lubis et al., 2018 Lubis et al., , 2019) )  is essential. Although several dialogue corpora with emotion annotation have been proposed, an utterance is annotated only with a speaker's emotion  (Li et al., 2017; Hsu et al., 2018)  or a dialogue as a whole is annotated  (Rashkin et al., 2019) , all of which are not appropriate for enabling the above ability.\n\nIn this paper, we propose a method to build an emotion-annotated multi-turn dialogue corpus, which is necessary for developing a dialogue system that can recognize the emotion of an interlocutor and generate a response with an appropriate emotion. We annotate each utterance in a dialogue with an expressed emotion, which a speaker put into the utterance, and an experienced emotion, which a listener felt when listening to the utterance.\n\nTo construct a multi-turn dialogue corpus annotated with these emotions, we collect dialogues from Twitter and crowdsource their emotion annotation. As a dialogue corpus, we extract tweet sequences where two people speak alternately. For the emotion annotation, we adopt Plutchik's wheel of emotions  (Plutchik, 1980)  as emotion labels and ask crowdworkers whether an utterance indicates each emotion label for expressed and experienced emotion categories. Each utterance is allowed to have multiple emotion labels and has an intensity, strong and weak, according to the number of crowdworkers' votes. We build a Japanese dialogue corpus as a testbed in this paper, but our proposed method can be applied to any language.\n\nUsing the above method, we constructed a Japanese emotion-tagged dialogue corpus consisting of 3,828 dialogues and 13,806 utterances. 1  Statistical analysis of the constructed corpus revealed the characteristics of words for each emotion and the relationship between expressed and experienced emotions. We further conducted experiments to recognize expressed and experienced emotions using BERT  (Devlin et al., 2019) . We defined the task of emotion recognition as regression and evaluated BERT's performance using correlation coefficients. The experimental results showed that it was more difficult to infer experienced emotions than expressed emotions, and that multi-task learning of both emotion categories improved the overall performance of emotion recognition. From these results, we can see that expressed and experienced emotions are different, and that it is meaningful to annotate both. We expect that the constructed corpus will facilitate the study on emotion recognition in dialogue and emotion-aware response generation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion-Tagged Corpora",
      "text": "Many non-dialogue corpora annotated with emotions have been constructed. EmoBank  (Buechel and Hahn, 2017 ) is a corpus of social media or reviews with emotion annotation. They annotate sentences with the emotions of a person who read them and a person who wrote them. WRIME  (Kajiwara et al., 2021)  is an emotion-annotated corpus in Japanese, where SNS posts are tagged with both subjective and objective emotions. The concept of this corpus is similar to EmoBank. However, they emphasize the subjectivity of annotation and ask writers to annotate their own sentences with emotions. Furthermore, EmoInt  (Mohammad and Bravo-Marquez, 2017)  aims at the task of detecting emotion intensity. They annotate Twitter posts with anger, fear, joy, and sadness and give each emotion a real value between 0 and 1 as the intensity level.\n\nSome corpora are tagged with non-emotional factors, along with emotions. EmotionStimulus  (Ghazi et al., 2015)  and GroundedEmotions  (Liu et al., 2017)  are corpora that focus on the reason for an expressed emotion. The former uses FrameNet to detect a cause, while the latter treats weather and news as external emotion factors. In terms of emotion labels, the two corpora adopt seven emotions (Ekman's six emotions  (Ekman, 1992)  and shame) and two emotions (only happiness and sadness), respectively. In StoryCommonsense  (Rashkin et al., 2018) , a series of sentences comprising of a short story is tagged with motivation and emotional reaction for each character. For emotion labels, they use some theories of psychology, including Plutchik's wheel of emotions  (Plutchik, 1980) .\n\nNone of the above corpora, however, are relevant to dialogue. StoryCommonsense is similar to ours but differs in that characters in a story are annotated instead of speakers' utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialogue Corpora",
      "text": "Several dialogue corpora annotated with emotions are available. DailyDialog  (Li et al., 2017)  is one collected from educational websites and tagged with emotions and intentions. EmotionLines  (Hsu et al., 2018)  is a multi-turn dialogue corpus with annotation of emotions. Both of them use seven labels for tagging: Ekman's six emotions  (Ekman, 1992)  and an other/neutral emotion. MELD  (Poria et al., 2019)  is an extension of EmotionLines, tagged with not only emotions but also visual and audio modalities. EmpatheticDialogues  (Rashkin et al., 2019)  is a dialogue-level emotion-tagged corpus, considering two participants as a speaker and a listener, and tagged with the speaker's emotion and its context.\n\nIn EmpatheticDialogues, not each utterance but each dialogue is annotated, which is not suitable for recognizing emotional transition throughout a dialogue. For Japanese, there is a Japanese version of EmpatheticDialogues called JEmpathetic-Dialogues  (Sugiyama et al., 2021)   and EmotionLines. Although these corpora contain only the speaker's emotion (expressed emotion), we also annotate an utterance with the emotion of a person who hears it (experienced emotion). Furthermore, while an utterance has only one emotion label in these corpora, we allow multiple emotion labels to be tagged per utterance and also consider their strength.\n\nThere are also some studies toward developing emotion-aware dialogue systems.  Smith et al. (2020)  propose three skills for a human-like dialogue system: recognizing emotions, using knowledge  (Dinan et al., 2019) , and considering personality  (Zhang et al., 2018) . Furthermore, Roller et al. (  2021 ) build a dialogue system capable of blending these three skills.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Corpus Building",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialogue Collection",
      "text": "We collect dialogue texts from Twitter by considering the interaction between tweets and their replies by two users as a dialogue. To improve the text quality, we exclude tweets that contain images or hashtags and set the maximum number of utterances included in a dialogue to nine. We also apply several filters: excluding dialogues that contain special symbols, emojis, repeated characters, and utterances that are too short. Note that the reason why we exclude emojis is that they are relatively explicit emotional factors, and we intend to analyze emotions implied from usual textual expressions.\n\nWe collected Japanese dialogues using this method. The numbers of dialogues and utterances are shown in Table  1 . We obtained 3,828 dialogues that correspond to 13,806 utterances in total. Regarding the length of dialogues, the number of dialogues tends to decrease as that of utterances per dialogue increases.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Annotation",
      "text": "We adopt Plutchik's wheel of emotions  (Plutchik, 1980)  as annotation labels. 2  Specifically, our annotation labels consist of eight emotions: anger, anticipation, joy, trust, fear, surprise, sadness, and disgust. We annotate each utterance with two emotion categories: an expressed emotion, which is expressed by a speaker of the utterance, and an experienced emotion, which is experienced by a listener of the utterance. In other words, an utter-   ance is annotated with both subjective and objective emotions, which is similar to EmoBank  (Buechel and Hahn, 2017)  for non-dialogue texts. By annotating expressed and experienced emotions, we can trace the changes in the emotion surrounding both an utterance and a participant in a dialogue. As a crowdsourcing platform, we use Yahoo! Crowdsourcing.  3  By showing the target utterance and its context, we ask seven workers whether the target utterance has a specified emotion or not about each emotion label for expressed and experienced emotion categories. For the expressed emotions, we ask which emotion a speaker expressed when saying the utterance. For the experienced emotions, we ask which emotion a listener experienced when hearing the utterance. Workers are allowed to select multiple emotion labels or none of them. An interface of the crowdsourcing task for expressed emotions is shown in Figure  2 .\n\nBecause a view of expressed and experienced emotions can vary among annotators, we employ many workers per an utterance and aggregate their votes to obtain highly reliable annotations.  4  We consider strength for each emotion according to the number of workers' votes; emotions selected by more than half of the workers are regarded as strong, and ones selected by more than a quarter are regarded as weak. Note that the set of strong emotions is a subset of the set of weak ones. We expect that providing the emotions with intensity enables us to handle their granularity.\n\nWe applied the above emotion annotation method to our dialogue corpus. The number of utterances for each emotion is shown in Table  2 . For the expressed emotion, 46.15% and 88.48% of the utterances are tagged with at least one strong and weak emotion, respectively. For the experienced emotion, the percentages are 34.08% and 90.65%, respectively. Approximately 90% of the utterances are accompanied by one or more emotion labels, and thus our corpus is consequently suitable for recognizing emotions in dialogues and analyzing their changes. In contrast to ours, for example, less than 20% of utterances are tagged with a specific emotion in DailyDialog  (Li et al., 2017) . Hence it is difficult to analyze emotion changes using such corpora with a small amount of emotion annotation. In addition, we can see a bias among the emotion labels for both expressed and experienced emotions, with more instances of anticipation and joy and fewer instances of trust and fear. An example of a dialogue with the annotation is shown in Table  3 . Figure  3 : The confusion matrices of the relationship between expressed and experienced emotions. In this analysis, we focus on only the strong labels. Note that the matrices' elements are normalized in the row direction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Corpus Analysis",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Frequent Words For Emotion Categories And Labels",
      "text": "To investigate the characteristics of utterances with different emotions, we count words for each strong emotion label in our corpus. In this analysis, we identify words by the Japanese morphological analyzer Juman++  (Tolmachev et al., 2018) . To exclude common words likely to appear for all emotions, we apply an IDF filtering. Specifically, words with IDF less than half of the maximum are ignored.\n\nTop-3 words appearing for strong emotion labels are shown in Table  4 . The same words tend to appear in the two emotion categories for joy and sadness. In contrast, the frequent words in the two categories are different for anticipation, trust, and surprise.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Relationship Between Expressed And Experienced Emotions",
      "text": "We annotated utterances with the expressed and experienced emotions. Here, we focus on the relationship between these two emotion categories. Specifically, we investigate the following two relationships:\n\na. The expressed emotion and the experienced emotion for the same utterance (different persons).\n\nb. The experienced emotion for an utterance and the expressed emotion for the next utterance (the same person).\n\nThe confusion matrices for the strong emotion labels are shown in Figure  3 , where the elements are normalized in the row direction. First, diagonal components of the two confusion matrices have large values, indicating that the same emotions are likely to occur both for the same utterance and for the same person. Figure  3a  shows that people are likely to experience joy for an utterance of anticipation, trust, and surprise in addition to the same emotion. People also tend to experience disgust and sadness for anger and disgust, respectively. Figure  3b  shows that after experiencing trust, people are more likely to express joy than trust. For an anger experience, people are more likely to express disgust than anger. Figures  3a  and 3b  reveal that the relationship of sadness is particularly different. For a certain utterance, sadness makes the other person feel sad in most cases, but for a certain person, anticipation in addition to sadness can be expressed after experiencing sadness. We speculate that when a person experiences sadness from the interlocutor, the person brings an utterance with anticipation to comfort them. Figure  4 : The confusion matrices for the emotion labels at the beginning and end of dialogue. In this analysis, we consider only the emotions of a person who begins a dialogue. Note that the targets are limited to the dialogues containing six to nine utterances, and the elements are normalized in the row direction.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotions At The Beginning And End Of A Dialogue",
      "text": "To analyze the emotion changes through a dialogue, we compare emotions at the beginning and end of a dialogue. In other words, we see how the emotions of a person who starts the dialogue change through the dialogue. In this analysis, we focus on the following two relationships:\n\na. The emotion expressed first and the emotion expressed last by the same person.\n\nb. The emotion expressed first and the emotion experienced last by the same person.\n\nThe confusion matrices for the strong emotion labels are shown in Figure  4 . The targets are limited to dialogues containing six to nine utterances to analyze the emotion changes in long dialogues. Figure  4a  shows that a speaker of the first utterance is likely to finally express anticipation and joy regardless of the first emotion. A speaker who first expresses surprise can express sadness through the dialogue. Figure  4b  also shows that the first speaker can experience anticipation at the end of a dialogue. A person who first expresses anger and disgust tends to finally experience trust. From these two figures, we can see that a dialogue causes a person who first expresses fear to finally feel either a positive or negative emotion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model Setup",
      "text": "We conduct experiments on expressed and experienced emotion recognition using our corpus. We solve a regression task of each emotion intensity for an utterance with its context for the emotion recognition task. We assign 0, 1, and 2 for none, weak, and strong emotion labels, respectively, and let a model regress these values for each emotion. As such, we train two separate models for expressed and experienced emotions with the mean squared error loss:\n\nwhere N is the number of samples and K is the number of emotion labels. y ij is the output from the model for the jth label of the ith sample, and t ij is its gold label. We adopt a Japanese pre-trained BERT model and fine-tune it. We compare two pre-trained models from Kyoto University 5  and one from NICT 6  . We use the WWM and BPE versions for Kyoto University's and NICT's BERT models, respectively. Input utterances are segmented into words with Juman++  (Tolmachev et al., 2018)  and tokenized into subwords by applying BPE. We join utterances with [SEP] and append [CLS] and [SEP] to the beginning and end, respectively. As there are two participants in a dialogue, we give each utterance a segment ID of 0 or 1. It provides the models with the information about the speaker of an utterance. Based on a series of utterances joined with [SEP], we predict an emotion label for the last utterance. The vector corresponding to [CLS] is passed to a fully-connected layer, and an eight-dimensional vector representing the eight emotions is obtained. Each of the elements is supposed to regress the intensity of each emotion.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model",
      "text": "Since we are dealing with a regression task, Pearson's and Spearman's correlation coefficients are used as evaluation metrics. The dialogues in our corpus are split into 8:1:1, corresponding to training, validation, and test sets. We fine-tune our models for three epochs and evaluate them on the test set. The implementation of the models is based on HuggingFace Transformers 7  . The models are trained using NVIDIA Tesla V100 SXM2 GPU.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "For the regression task defined in Section 5.1, the correlation coefficients for each model are shown in Table  5 . In terms of performance, the NICT model achieved the best score across all values. For the values regarding expressed and experienced emotions, the performance of the experienced emotion is inferior to that of expressed emotion in all models. This result indicated that it is more difficult to recognize the experienced emotion than the expressed emotion.\n\nThe correlation coefficients for each emotion inferred by the NICT model are shown in Table  6 . For both the expressed and experienced emotions, the highest scores were achieved for anticipation and joy. In contrast, the emotions with lower values were trust and fear for the expressed emotion and anger and disgust for the experienced emotion. From Tables  6  and 2 , we can see that the larger the number of the samples for an emotion is, the higher the correlation coefficient becomes. As a case study, we show example dialogues and their emotions predicted by the NICT model in Table  7 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "Our analysis in Section 4.2 indicated that there is a correlation between expressed and experienced emotions. Therefore, we consider training a single model for recognizing both the emotion categories. The information for solving the two similar tasks is expected to allow a model to improve the performance of each other  (Liu et al., 2019) . We provide a model with two separate fully-connected layers for the tasks and train them simultaneously, where the inputs are the same as those in Section 5.1. Here, the mean of the losses for expressed and experienced emotions is optimized:\n\nBased on Figures  3a  and 3b , we consider multi-task learning of expressed and experienced emotions for a certain utterance and a certain person. For the relationship in a certain person, we use the experienced emotion of an utterance and the expressed emotion of the following utterance. We also conduct experiments on the cases where the training and test sets are different from each other. In such a case, for example, expressed emotions are used Dialogue Predicted Gold A1: ゲームの検証してる人が検証してほしいことあれば言ってください的な こと言ってたから依頼したら無視されて悲しくなったのはいい思い出 (I have a good memory of a guy who was verifying a game and said if there was anything he wanted verified, please let him know, so I made a request and he ignored it, which made me sad.) B1: それは悲しいね (That's sad.)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Strong Sadness",
      "text": "Strong sadness A1: youtubeでバーのマスターが氷砕いてる動画見てボーッとしてる (I've been watching videos of bar masters crushing ice on youtube and I'm in a daze.) B1: なんかしてよ (Do something.) A2: そのうちこういうときにツイキャスをしようかなと思っておる (One of these days I'm going to do a tweak for this.) B2: 天才の発想 スマホでも見やすいから助かる (It's a genius idea, and it's easy to watch on my phone.) for training, but experienced emotions are used for testing.\n\nThe correlation coefficients for an utterance and a speaker by multi-task learning are shown in Tables 8 and 9, respectively. First, the scores when the training and test sets are different from each other are lower than those when they are the same. This gap indicates the significance of annotating utterances with expressed and experienced emotions separately. In all columns, the multi-task models achieved higher performance than the singletask models. Especially, in Table  9 , the multi-task scores for both the two tasks are higher than the single-task baselines by one point. In other words, expressed, experienced, and next expressed emotions have the information for helping the recogni-tion of each other.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a method to build an emotion-tagged multi-turn dialogue corpus to help machines recognize emotional transition in a dialogue. Dialogues between two speakers are collected from Twitter, and each utterance is annotated with emotions by crowdsourcing. In the annotation process, we consider the emotions expressed by a speaker who said the utterance and the emotion experienced by a listener who heard the utterance. In addition, the labels are provided with their intensity, representing the granularity of emotions.\n\nWe built a Japanese emotion-tagged dialogue corpus and analyzed it. The results showed the characteristics of words for each emotion, the correlation between the emotions about a certain utterance and speaker, and the tendency for speakers to become positive through a dialogue. We also developed emotion recognition models for expressed and experienced emotions based on the Japanese pretrained BERT models. The experimental results indicated that it is more difficult to recognize a listener's emotion than a speaker's emotion. Multitask learning of expressed and experienced emotions improved the performance of the two emotion recognition tasks about an utterance and a speaker.\n\nFor our future work, we will tackle response generation based on predicted emotions. With our corpus, a dialogue system is expected to predict which emotion it experiences from a given utterance and which emotion it should express for the next utterance. Once such emotions are recognized, the dialogue system should be able to generate an appropriate response depending on the predicted expressed emotion.\n\nThe corpus in this work is annotated only with expressed and experienced emotions about an utterance. In addition to the emotion annotation, we should also consider dialogue situations  (Rashkin et al., 2019) . The cause of a dialogue or an utterance helps recognize a speaker's emotion and how it changes. We can also consider non-emotional annotation, such as a dialogue's topic and an utterance's intention  (Li et al., 2017) . The relationship between emotions and non-emotional factors is also important for machines to better recognize a speaker's emotion.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example dialogue with expressed and ex-",
      "page": 1
    },
    {
      "caption": "Figure 2: An example of the crowdsourced task.",
      "page": 3
    },
    {
      "caption": "Figure 2: Because a view of expressed and experienced",
      "page": 4
    },
    {
      "caption": "Figure 3: The confusion matrices of the relationship between expressed and experienced emotions. In this analysis,",
      "page": 5
    },
    {
      "caption": "Figure 3: , where the elements",
      "page": 5
    },
    {
      "caption": "Figure 3: a shows that people are",
      "page": 5
    },
    {
      "caption": "Figure 4: The confusion matrices for the emotion labels at the beginning and end of dialogue. In this analysis, we",
      "page": 6
    },
    {
      "caption": "Figure 4: The targets are lim-",
      "page": 6
    },
    {
      "caption": "Figure 4: a shows that a speaker of the ﬁrst utter-",
      "page": 6
    },
    {
      "caption": "Figure 4: b also shows that the ﬁrst",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The statistics of utterances for each emotion",
      "data": [
        {
          "Label": "Anger\nAnticipation\nJoy\nTrust\nFear\nSurprise\nSadness\nDisgust",
          "Expressed\nStrong Weak": "430\n1,349\n1,906\n4,229\n1,629\n3,672\n247\n1,732\n252\n942\n602\n2,018\n1,227\n2,936\n476\n1,979",
          "Experienced\nStrong Weak": "124\n870\n1,215\n4,068\n1.553\n4,549\n520\n3,455\n123\n846\n434\n2,798\n889\n3,037\n186\n1,535"
        },
        {
          "Label": "Any",
          "Expressed\nStrong Weak": "6,371 12,215",
          "Experienced\nStrong Weak": "4,705 12,515"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 8: The results of multi-task learning with ex- multi-turndialoguecorpustohelpmachinesrecog-",
      "data": [
        {
          "Dialogue": "A1: ゲームの検証してる人が検証してほしいことあれば言ってください的な\nこと言ってたから依頼したら無視されて悲しくなったのはいい思い出 (I have a\ngood memory of a guy who was verifying a game and said if there was anything he wanted\nveriﬁed, please let him know, so I made a request and he ignored it, which made me sad.)\nB1: それは悲しいね (That’s sad.)",
          "Predicted\nGold": "Strong\nsad-\nStrong\nsad-\nness\nness"
        },
        {
          "Dialogue": "A1: youtubeでバーのマスターが氷砕いてる動画見てボーッとしてる (I’ve been\nwatching videos of bar masters crushing ice on youtube and I’m in a daze.)\nB1: なんかしてよ (Do something.)\nA2: そのうちこういうときにツイキャスをしようかなと思っておる (One of these\ndays I’m going to do a tweak for this.)\nB2: 天才の発想 スマホでも見やすいから助かる (It’s a genius idea, and it’s easy to\nwatch on my phone.)",
          "Predicted\nGold": "joy\nWeak\nantic-\nStrong\nipation\nand\nand\nweak\njoy\ntrust"
        },
        {
          "Dialogue": "A1: 今、部活終わって帰るとこやけど 雨やばいしかっぱ持ってきてないし 最悪\n(I’m on my way home after club activities, but it’s raining and I didn’t bring my hat, so\nthat sucks.)\nB1: わたしも学校出た瞬間大雨降ってきた (I’m going back to school now, but it’s\nraining really hard and I didn’t bring my jacket.)",
          "Predicted\nGold": "Strong\nStrong\nsad-\nsurprise\nness"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Daniel Adiwardana",
        "Minh-Thang Luong",
        "David So",
        "Jamie Hall",
        "Noah Fiedel",
        "Romal Thoppilan",
        "Zi Yang",
        "Apoorv Kulshreshtha",
        "Gaurav Nemade",
        "Yifeng Lu",
        "V Quoc"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "EmoBank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "authors": [
        "Sven Buechel",
        "Udo Hahn"
      ],
      "year": "2017",
      "venue": "Proceedings of the 15th Conference of the European Chapter"
    },
    {
      "citation_id": "3",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "4",
      "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
      "authors": [
        "Emily Dinan",
        "Stephen Roller",
        "Kurt Shuster",
        "Angela Fan",
        "Michael Auli",
        "Jason Weston"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "5",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "6",
      "title": "Detecting emotion stimuli in emotion-bearing sentences",
      "authors": [
        "Diman Ghazi",
        "Diana Inkpen",
        "Stan Szpakowicz"
      ],
      "year": "2015",
      "venue": "Computational Linguistics and Intelligent Text Processing"
    },
    {
      "citation_id": "7",
      "title": "Emotion-Lines: An emotion corpus of multi-party conversations",
      "authors": [
        "Chao-Chun",
        "Sheng-Yeh Hsu",
        "Chuan-Chun Chen",
        "Ting-Hao Kuo",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "8",
      "title": "WRIME: A new dataset for emotional intensity estimation with subjective and objective annotations",
      "authors": [
        "Tomoyuki Kajiwara",
        "Chenhui Chu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.169"
    },
    {
      "citation_id": "9",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Grounded emotions",
      "authors": [
        "Vicki Liu",
        "Carmen Banea",
        "Rada Mihalcea"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII.2017.8273642"
    },
    {
      "citation_id": "11",
      "title": "Multi-task deep neural networks for natural language understanding",
      "authors": [
        "Xiaodong Liu",
        "Pengcheng He",
        "Weizhu Chen",
        "Jianfeng Gao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1441"
    },
    {
      "citation_id": "12",
      "title": "Eliciting positive emotion through affect-sensitive dialogue response generation: A neural network approach",
      "authors": [
        "Nurul Lubis",
        "Sakriani Sakti",
        "Koichiro Yoshino",
        "Satoshi Nakamura"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Positive emotion elicitation in chat-based dialogue systems",
      "authors": [
        "Nurul Lubis",
        "Sakriani Sakti",
        "Koichiro Yoshino",
        "Satoshi Nakamura"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2019.2900910"
    },
    {
      "citation_id": "14",
      "title": "Emotion intensities in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez"
      ],
      "year": "2017",
      "venue": "Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)",
      "doi": "10.18653/v1/S17-1007"
    },
    {
      "citation_id": "15",
      "title": "A general psychoevolutionary theory of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of emotion"
    },
    {
      "citation_id": "16",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "17",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "18",
      "title": "Modeling naive psychology of characters in simple commonsense stories",
      "authors": [
        "Antoine Hannah Rashkin",
        "Maarten Bosselut",
        "Kevin Sap",
        "Yejin Knight",
        "Choi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1213"
    },
    {
      "citation_id": "19",
      "title": "Towards empathetic opendomain conversation models: A new benchmark and dataset",
      "authors": [
        "Eric Hannah Rashkin",
        "Margaret Smith",
        "Y-Lan Li",
        "Boureau"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1534"
    },
    {
      "citation_id": "20",
      "title": "Recipes for building an open-domain chatbot",
      "authors": [
        "Stephen Roller",
        "Emily Dinan",
        "Naman Goyal",
        "Da Ju",
        "Mary Williamson",
        "Yinhan Liu",
        "Jing Xu",
        "Myle Ott",
        "Eric Smith",
        "Y-Lan Boureau",
        "Jason Weston"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume"
    },
    {
      "citation_id": "21",
      "title": "Can you put it all together: Evaluating conversational agents' ability to blend skills",
      "authors": [
        "Eric Michael",
        "Mary Williamson",
        "Kurt Shuster",
        "Jason Weston",
        "Y-Lan Boureau"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.183"
    },
    {
      "citation_id": "22",
      "title": "Hideharu Nakajima, and Toyomi Meguro. 2021. Empirical analysis of training strategies of transformer-based japanese chit-chat systems",
      "authors": [
        "Hiroaki Sugiyama",
        "Masahiro Mizukami",
        "Tsunehiro Arimoto",
        "Hiromi Narimatsu",
        "Yuya Chiba"
      ],
      "venue": "Hideharu Nakajima, and Toyomi Meguro. 2021. Empirical analysis of training strategies of transformer-based japanese chit-chat systems"
    },
    {
      "citation_id": "23",
      "title": "Juman++: A morphological analysis toolkit for scriptio continua",
      "authors": [
        "Arseny Tolmachev",
        "Daisuke Kawahara",
        "Sadao Kurohashi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/D18-2010"
    },
    {
      "citation_id": "24",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Personalizing dialogue agents: I have a dog, do you have pets too?",
      "authors": [
        "Saizheng Zhang",
        "Emily Dinan",
        "Jack Urbanek",
        "Arthur Szlam",
        "Douwe Kiela",
        "Jason Weston"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1205"
    },
    {
      "citation_id": "26",
      "title": "The design and implementation of XiaoIce, an empathetic social chatbot",
      "authors": [
        "Li Zhou",
        "Jianfeng Gao",
        "Di Li",
        "Heung-Yeung Shum"
      ],
      "year": "2020",
      "venue": "Computational Linguistics",
      "doi": "10.1162/coli_a_00368"
    }
  ]
}