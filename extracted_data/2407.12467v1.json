{
  "paper_id": "2407.12467v1",
  "title": "Bsc-Upc At Emospeech-Iberlef2024: Attention Pooling For Emotion Recognition ‚ãÜ",
  "published": "2024-07-17T10:37:28Z",
  "authors": [
    "Marc Casals-Salvador",
    "Federico Costa",
    "Miquel India",
    "Javier Hernando"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Deep Learning",
    "Attention",
    "Transformers"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The domain of speech emotion recognition (SER) has persistently been a frontier within the landscape of machine learning. It is an active field that has been revolutionized in the last few decades and whose implementations are remarkable in multiple applications that could affect daily life. Consequently, the Iberian Languages Evaluation Forum (IberLEF) of 2024 held a competitive challenge to leverage the SER results with a Spanish corpus. This paper presents the approach followed with the goal of participating in this competition. The main architecture consists of different pre-trained speech and text models to extract features from both modalities, utilizing an attention pooling mechanism. The proposed system has achieved the first position in the challenge with an 86.69% in Macro F1-Score.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are undoubtedly fundamental parts of our idiosyncrasy. They play an important role in interpersonal relationships and decision-making and generally take part in the evolution and consciousness of any mental process  [1] . Moreover, there is empirical evidence that emotions immensely influence human health  [2] , which creates the necessity to monitor them. Affective computing is of great interest in medical health fields  [3] . Consequently, developing a system capable of discerning various emotions is highly valuable. Researchers have attempted to predict emotions using machine learning approaches, but their effectiveness depends on the quality and quantity of available data.\n\nIn the field of Natural Language Processing (NLP), numerous emotion recognition models rely exclusively on text-based features. Since the creation of Transformers  [4] , multiple approaches  [5, 6, 7]  seek to use pre-trained Transformer encoders, such as BERT  [8]  as text feature extractors. These encoders are pre-trained models using self-supervised learning techniques on extensive datasets. This pre-training enables the models to project words in a latent space with a rich semantic representation, from which classifiers can then be employed to predict the corresponding emotion classes. On the other hand, speech is crucial for expressing emotions. Elements such as pitch, prominence, and phrasing contribute generously to providing emotion information. As explained in  [9] , the human brain is capable of recognizing emotions pan-culturally and independently of the language they are expressed with. Following this principle, the researchers have proposed different architectures to process and extract information from speech signals. In the realm of Speech Emotion Recognition (SER), Machine Learning (ML) and Deep Learning (DL) models often utilize hand-crafted features such as Mel-Frequency Cepstral Coefficients (MFCC)  [10, 11, 12, 13] . Nevertheless, recent advances in Deep Learning allow cutting-edge architectures to combine text and speech to provide better results. Multimodal emotion Recognition (MER) is a complex task since it requires models to be able to learn complex patterns of the data. For this reason, the usage of text and audio pre-trained models significantly improves the embedding representation of their features, allowing them to be combined in their latent space  [14, 15, 16] .\n\nIn summary, the exceptional progress that has been made in this field is appreciable, yet there also exists a disparity in the amount of work carried out in Spanish. Developing these models using Spanish data is crucial for several reasons. On the one hand, Spanish is one of the languages with the most native speakers worldwide. On the other hand, it is necessary to leverage the engineering opportunities of Spanish-speaking countries, opening the door to developing new technologies that could satisfy their population needs. However, one of the difficulties this presents is the lack of labelled data in Spanish needed to train any supervised learning approach.\n\nIn order to encourage the creation of an emotion recognition model trained with Spanish data, the Iberian Languages Evaluation Forum (IberLEF) of 2024 created the challenge EmoSPeech 2024. This competition evaluates the Macro F1-Score of the participants in two tasks: Emotion Recognition with text and with speech and text. The training corpus is Spanish MEACorpus 2023  [17] . This dataset contains 13.16 h of speech, and its transparent methodology distinguishes it from other datasets of the same task. The speech samples are collected from YouTube videos and are labelled using the categorical taxonomy proposed by P. Ekman  [18] , which include emotions such as surprise, disgust, anger, joy, sadness, fear, and neutral expressions. Nevertheless, it was impossible for the annotators to find any sample that contained speech expressing surprise emotion, though this class is not represented in the dataset.\n\nThis paper endeavours to leverage the research of Spanish models with cutting-edge technologies in the current state of the art by making use of the MEACorpus 2023. The current state of the art is the usage of Transformers-based pre-trained models with high capacity that, leveraging the enormous datasets they are trained with, are capable of describing complex patterns in both text and speech. Following this lead, the system combines a speech pre-trained model, the XLSR-wav2vec 2.0  [19] , that is trained with 436,000h and a RoBERTA text model fine-tuned in Spanish  [20] . Both models are used as feature extractors for speech and text respectively, and they output a vector with the relevant information of the utterances. The two vectors are then concatenated into a single vector that contains information about text and speech. Subsequently, this vector is reduced to lower-dimension representation by making use of an attention pooling mechanism. Finally, dense layers are utilized to project this reduced vector, determining the classification of the utterance. This approach has reached an F1-Score of 86.69%, achieving the first position in the multimodal task of the EmoSPeech 2024 challenge.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Challenge",
      "text": "The EmoSPeech 2024  [21]  is a Challenge proposed by the Iberian Languages Evaluation Forum (IberLEF) of 2024  [22] , which is a Spanish workshop hosted by the Sociedad Espa√±ola para el Procesamiento del Lenguaje Natural (SEPLN). This event is dedicated to producing models in the frame of the Iberian peninsula, encompassing different national languages such as Spanish, Portuguese, Catalan, Basque, or Galician.\n\nTo develop the competition, the organisers proposed a dataset named Spanish MEACorpus 2023  [17] . The dataset, comprising 13.16h of speech divided into 5,129 audios, was meticulously labelled by the research team of the article. As explained in their paper, the procedure to extract the audio files is as follows: The authors of the dataset selected YouTube videos according to their topic and extracted audio segments considering the noise of the audio files and the silence gaps. Once this part was done, the audio files were classified using the emotion taxonomy developed by P. Ekman  [18] . It comprehended the basic emotions of disgust, anger, joy, sadness, fear, surprise, and neutral emotion. Nevertheless, despite the efforts, finding any speech audio that contained the surprise emotion was impossible.\n\nAs is common in the field, the dataset exhibits an unequal representation of the emotion classes. Figure  1  (left)shows that neutrality and disgust are the most prevalent emotions, while fear is notably scarce. Another important aspect is the length of the audio files, which can directly affect the performance of the network by providing more contextual information about the speech. Therefore, audio duration is an essential characteristic of any speech dataset and must be considered a quality metric. A histogram of the audio file duration is shown in Figure  1 (right). The mean of the duration is 9.24 s. Lastly, another fundamental consideration is the variability of the recordings. The fact that this competition uses third-party audio files makes it more difficult to control other parameters, such as noise or the magnitude of the audio. Some videos are recorded outdoors and are more likely to have background noise or exhibit poorer recording quality, while others are studio-recorded and of higher quality. Furthermore, the dataset's paper detailed that 46% of the speech segments are attributed to female voices, with the remainder belonging to males. The paper affirmed that the text transcriptions were extracted directly from the raw audio file using Whisper  [23]  followed by a manual revision by the researchers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Architecture",
      "text": "The system created is a multimodality model that combines text and speech and, trained with the Spanish MEACorpus 2023 dataset  [17] , effectuates a classification of the emotion. In Figure  2 , it is possible to see a global representation of the system. As can be seen, both text and speech are fed to the network and processed with a self-supervised learning (SSL) model that works as a feature extractor. Then, the model concatenates these features and pools them into one single vector. Finally, a classification is performed by processing this vector with multilayer perceptrons that serve as a classifier.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech",
      "text": "It is a standard practice to apply regularisation techniques to preprocess the data before its utilisation in a machine learning model. In the case of this system, the overall mean and standard deviation of all the audio files were calculated and used to normalise the values in the dataset. This technique is widespread in Deep Learning literature for speech due to the fact that it makes backpropagation more efficient and reduces the impact of the outliers. The normalisation process was extended to the validation and test sets, whereby the mean and standard deviation values obtained from the training set were utilised. It is essential to mention that, during the training stage, the samples were randomly cropped with a window of 5.5 seconds. The shorter utterances were enlarged using repetition padding.\n\nGiven the limited size of the dataset, it became necessary to use data augmentation to mitigate overfitting to the training set. The specific techniques utilized included speed perturbation, which alters the speed of the audio; reverberation, which simulates a reverberant environment; and background noise, which adds ambient sounds to the audio. PyTorch implements data augmentation using the following scheme: first of all, defining the transformations used to create synthetic data. Then, for each sample, a probability decides whether the model will see the original data or the one created after applying these transformations. This process occurs in every epoch, so data augmentation is applied to different samples in each epoch. This streaming data augmentation guarantees the model is exposed to a wide diversity of data without physically increasing the dataset size. As is common in the field, when doing inference is not desired to perturb the data, so in the validation and test sets, the probability of applying these transformations is zero.\n\nRecent advancements in the field of Deep Learning have highlighted the importance of using Transformer-based pre-trained models. It has been proven that their ability to adapt to changes in the domain makes them suitable for extracting features from audio files in any dataset. In this work, the following models were experimented with:\n\n‚Ä¢ WavLM  [24] : This cutting-edge model is trained with 80,000 hours and encompasses datasets such as Libri-Light  [25] , GigaSpeech  [26]  and VoxPopuli  [27] . Two versions of this model were tried, the Large version and the Base. The output vector is 768 in the case of the base version and 1,024 in the case of the large. ‚Ä¢ XLSR-wav2vec 2.0  [19] : This model is based on wav2vec 2.0  [28]  and it is trained with the datasets Common Voice  [29] , BABEL  [30]  and Multilingual LibriSpeech  [31] , which makes a total of 436,000 hours of audio in 128 languages. This model outputs a vector of dimension 1,024. ‚Ä¢ HuBERT  [32] : This model is trained with 60,000 hours of Libri-Light  [25] . The output vector is of dimension 1,024.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Text",
      "text": "The text domain was the first to employ pre-trained large language models (LLMs) for different tasks.\n\nSince the creation of the BERT model  [33] , different approaches have emerged in the state of the art, following the same idea with some variations. In the approach of this work, the following SSL models were experimented with:\n\n‚Ä¢ BERT  [33] : The large uncased version was used, with an an output dimension of 1,024.\n\n‚Ä¢ XLM-RoBERTa Spanish  [34] : It is a pre-trained model based on XLM-RoBERTa  [35]  and trained with Spanish Unannotated Corpora  [36] . This model outputs a vector with 1,024 dimensions. ‚Ä¢ BETO  [37] : BETO is one of the first pre-trained models produced with Spanish data. It follows the same structure as the BERT base. Consequently, it outputs a hidden vector of 768 dimensions. It is trained using Wikipedia data and all of the sources of the OPUS Project  [38] . Additionally, a fine-tuned version of BETO for emotions was used in the system.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Classifier",
      "text": "Following the attention pooling process, the resultant vector undergoes further processing via a classifier module. This module is designed to include a layer that adjusts the vector's dimension to fit the desired hidden layer width. It also consists of a stack of hidden layers and an output layer that has a dimension equivalent to the number of classes being considered.\n\nThe model architecture consists of several linear layers, each followed by a dropout, layer normalization, and a Gaussian Error Linear Unit (GELU) activation function  [39] . The Softmax activation function is used in the output layer to select the predicted class.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Attention Pooling",
      "text": "Different approaches have emerged for integrating information extracted from pre-trained models in the field of multimodal learning. Attention mechanisms, particularly Multi-Head Attention (MHA), have been popular in recent years for combining text and speech utterances. This study used an alternative Attention Pooling mechanism used in works such as  [40, 41, 42]  to reduce the dimensionality of the hidden state vector created by concatenating the outputs of the two pre-trained models.\n\n... Considering the embedding dimension ùê∏ and a batch size of one, we define the hidden states as the sequences of the extracted features {‚Ñé ùë° ‚àà R ùê∏ |ùë° = 1, ..., ùëá }. Then, for each hidden state ‚Ñé ùë° we calculate its weight as described in Equation (1):",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Softmax Softmax Softmax",
      "text": "where ùë¢ ‚àà R ùê∏ is a trainable parameter initialized with the Xavier initialization  [43]  and ùë§ ùë° is the weight associated at the hidden vector ‚Ñé ùë° . Then, the pooled representation of the hidden vector is calculated using Equation  (2) .\n\nThe vector ùëê encapsulates the relevant information of the features extracted in the text and speech systems. This approach is computationally more efficient compared to the general Attention mechanism, where the key, query, and values are calculated. This characteristic is especially convenient for this system due to the scarce data provided. Figure  3  demonstrates graphically the functioning of the attention pooling mechanism.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "PyTorch requires all samples in the batch to have the same dimensions. Therefore, during training, the audio files were cropped using a window of 5.5 seconds, which was the optimal value found. In inference, the whole audio waveform was used. As detailed in Section 3, the audio waveforms were normalized using the mean and standard deviation of the training set. The values extracted were -33.62 and 56.15, respectively. These same values are applied to the other sets when doing inference. Data augmentation techniques were applied by varying the probability based on the capacity of the model. The optimal value was found to be 0.3.\n\nA batch size of 16 samples was selected, as it provided an optimal trade-off between minimizing the duration of each epoch and avoiding GPU memory exhaustion. To further accelerate computation, data parallelization across two GPUs was utilized. In particular, the GPUs employed were two NVIDIA GeForce RTX 2080Ti. The optimizer selected was the AdamW  [44] , with a learning rate of 0.00005, which decayed by 10% after five epochs without improvements in the validation F1-score. The dropout rate, set at 0.1, was adjusted according to the network's complexity. The number of epochs utilized also depended on the model's capacity. Although model parameters were only stored when the F1-score improved, early stopping was necessary due to the noisy and variable learning curves. This variability could lead to an overfitting model being saved based on local improvements in the F1-Score during validation. Each experiment lasted one to two days, depending on the configurations.\n\nTo improve our position on the leaderboard, we made Macro F1-Score our top priority since it was the metric used to evaluate the participants' submissions. This metric combines the Precision and the Recall into one single number by applying their harmonic mean. Specifically, the Macro F1-Score is the average of the F1-Score of each class. This metric treats all classes equally, regardless of their amount of data, making it a fair measure of overall performance. Given that the metric of interest is the macro F1-Score, it is imperative to mitigate the class imbalance present in this dataset. A wide variety of losses try to palliate this disparity. Beyond these possibilities, the loss criterion finally chosen is the weighted cross-entropy loss.\n\nAfter doing the hyperparameter search, two classical machine learning techniques were employed with the aim of improving the results. The first approach involved applying thresholds to modify the final decision over the logits. However, this did not enhance the results. The second strategy leveraged the variability of different models by using a 3-model ensemble. Hard voting was chosen as the ensemble technique, where the most voted prediction among the three models was selected. In the event of a tie, the prediction from the model with the highest F1-Score on the validation set was chosen. The code of the project is available here: https://github.com/marccasals98/BSC-UPC_EmoSPeech",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "In the initial stages of the competition, it was necessary to evaluate various pre-trained self-supervised models to determine the most suitable one for the data. At this moment, only the training corpus was available; therefore, it was necessary to make a validation partition to evaluate the performances of the different self-supervised models. Table  1  presents the best results obtained with the diverse text and feature extractors. The best configuration used RoBERTa for text and XLSR-wav2vec 2.0 for audio, achieving an F1-score of 89.73% on the validation set. This superior performance is likely because RoBERTa was trained on Spanish data, making it more effective for this domain than other models trained in English. In addition, XLS-wav2vec 2.0 was trained by using 436,000 hours of audio in 128 languages, including Spanish, which possibly contributed to the improvement of this metric score. It is worth noting that, despite BETO being a Spanish version of BERT, the results with this encoder were poor. This fact could be attributed to WavLM not being trained with as much Spanish data as XLSR-wav2vec 2.0 or that BETO's vector dimension is 768 instead of 1,024, resulting in fewer features captured by the model. Different results were obtained during the validation test with the different feature extractor models. All of these configurations had their corresponding hyperparameter tuning, and the best of each one was selected.\n\nIt is remarkable that, in this initial stage, some experiments were conducted with other architectures that involved more parameters. For instance, attempts were made to combine features extracted from the encoder models using multi-head attention (MHA) with one and two heads. These experiments yielded unsatisfactory results, with F1-scores of 84.4% and 82.2%, and the models exhibited significant overfitting. Consequently, it was decided to discontinue these lines of experimentation and concentrate all efforts on the hyperparameter tuning using RoBERTa and XLSR-wav2vec 2.0 and the attention pooling.\n\nThe top three different models that obtained the best score achieved 86.20%, 85.96%, and 82.43%. Their confusion matrices over the test set are displayed in Figure  4 . As can be seen, anger is the most difficult emotion to classify, normally getting confused with disgust. It is remarkable that, despite being very scarce in the dataset, fear is very separable from the rest of the emotional spectrum.\n\nSection 5 remarked that thresholding techniques failed to outperform the models and were, therefore, discarded. Consequently, the only non-trainable approach used to improve the model's F1-Score was model ensembling. Initially, models with different feature extractors were employed to leverage the diversity of features and create a robust system. However, this approach proved to not be effective. Instead, the three best-performing models on the validation set were ensembled, improving the F1-score to 86.69%.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "This study presented a multimodal model for the emotion recognition challenge EmoSPeech 2024 within the IberLEF 2024 framework, aimed at recognizing emotions from speech and text inputs. The architecture comprised two pre-trained models, one dedicated to speech and the other to text. They extracted feature vectors that the model concatenates into a unified hidden representation vector. On the one hand, for the audio side, different experiments were conducted with WavLM, XLSR-wav2vec 2.0, and HuBERT. On the other hand, the optimization of the textual component of the architecture involved exploration with the models BERT, XLM-RoBERTa for Spanish, BETO, and its finetuned version for emotion. The best performance is achieved by jointly combining RoBERTa and XLSR-wav2vec 2.0. After the model concatenates the text and speech feature vectors, it employs a dimensionality reduction via reduced attention pooling. This mechanism, with fewer parameters than its standard counterpart, facilitates the seamless integration of text and audio while mitigating the risk of overfitting to the training set. Subsequently, a stack of dense layers processed the output vector, using its compressed information to extract the class prediction. Additionally, to optimize performance and maximize the F1-Score in the competition, model ensembling techniques were adopted, employing hard voting on the top three models. In summary, the system was capable of achieving an F1-Score of 86.69%, an absolute increment of 33.61% compared to the baseline, and securing the first position in the challenge.\n\nAfter the conclusion of this competition, continuing the research of new paradigms for Speech Emotion Recognition (SER) could be a captivating line of research. One of the lines is improving the efficiency of speech feature extractors. In the paper of WavLM, the authors claim that, in general, most self-supervised learning models (SSL) models focus primarily on Automatic Speech Recognition (ASR) tasks. However, by training an SSL model to jointly learn masked speech prediction and denoising in the pretraining stage, the model's capabilities extend beyond ASR, outperforming other SSL in fields such as SER. This statement could seem contradictory because, in Section 6, it is proven that XLSR-wav2vec 2.0 outperforms WavLM Large. Nevertheless, this outcome is likely due to XLSR-wav2vec2.0 being trained with a very extensive multilingual dataset. If WavLM was trained with a comparable volume of data, it could potentially outperform XLSR-wav2vec 2.0, leveraging its joint learning of masked speech prediction and denoising to achieve superior performance in various tasks, including Speech Emotion Recognition.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (left)shows that neutrality and disgust are the most prevalent emotions, while fear is notably scarce.",
      "page": 2
    },
    {
      "caption": "Figure 1: Some of the characteristics of the MEACorpus 2023. On the left is the distribution of the number of",
      "page": 3
    },
    {
      "caption": "Figure 1: (right). The mean of the duration is 9.24 s. Lastly, another",
      "page": 3
    },
    {
      "caption": "Figure 2: Diagram Attention Pooling for the Multimodal Emotion Recognition System. The speech utterances",
      "page": 4
    },
    {
      "caption": "Figure 3: Diagram of the Attention Pooling operation. The hidden vector ‚Ñé= ‚Ñé1, ..., ‚Ñéùëáis pooled into a lower",
      "page": 5
    },
    {
      "caption": "Figure 3: demonstrates graphically the functioning of the",
      "page": 6
    },
    {
      "caption": "Figure 4: As can be seen, anger is the most difficult",
      "page": 7
    },
    {
      "caption": "Figure 4: Confusion matrices in the test set. The drop-out was set to 0.1 and the data augmentation probability",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classi\u0000er": "Attention Pooling",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "9",
          "Column_2": "2",
          "Column_3": "0.",
          "Column_4": "69",
          "Column_5": "2.",
          "Column_6": "1",
          "Column_7": "2.",
          "Column_8": "4",
          "Column_9": "2.",
          "Column_10": "4",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "1.",
          "Column_2": "1",
          "Column_3": "7",
          "Column_4": "9",
          "Column_5": "1",
          "Column_6": "4",
          "Column_7": "4.",
          "Column_8": "5",
          "Column_9": "1.",
          "Column_10": "7",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "4",
          "Column_2": "",
          "Column_3": "1",
          "Column_4": "4",
          "Column_5": "7",
          "Column_6": "7",
          "Column_7": "1",
          "Column_8": "",
          "Column_9": "4",
          "Column_10": "",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "3.",
          "Column_2": "5",
          "Column_3": "5.",
          "Column_4": "8",
          "Column_5": "3.",
          "Column_6": "5",
          "Column_7": "8",
          "Column_8": "1",
          "Column_9": "5.",
          "Column_10": "8",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "4.",
          "Column_2": "4",
          "Column_3": "2.",
          "Column_4": "2",
          "Column_5": "6.",
          "Column_6": "7",
          "Column_7": "2.",
          "Column_8": "2",
          "Column_9": "8",
          "Column_10": "2",
          "Column_11": "2.",
          "Column_12": "2"
        },
        {
          "Column_1": "0",
          "Column_2": "",
          "Column_3": "0",
          "Column_4": "",
          "Column_5": "0",
          "Column_6": "",
          "Column_7": "0",
          "Column_8": "",
          "Column_9": "0",
          "Column_10": "",
          "Column_11": "1e+",
          "Column_12": "02"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "9",
          "Column_2": "3",
          "Column_3": "1",
          "Column_4": "",
          "Column_5": "1",
          "Column_6": "",
          "Column_7": "2.",
          "Column_8": "4",
          "Column_9": "2.",
          "Column_10": "4",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "1.",
          "Column_2": "7",
          "Column_3": "8",
          "Column_4": "4",
          "Column_5": "1",
          "Column_6": "0",
          "Column_7": "2.",
          "Column_8": "8",
          "Column_9": "1.",
          "Column_10": "1",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "5",
          "Column_2": "",
          "Column_3": "1",
          "Column_4": "3",
          "Column_5": "7",
          "Column_6": "8",
          "Column_7": "2",
          "Column_8": "",
          "Column_9": "2",
          "Column_10": "",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "4.",
          "Column_2": "7",
          "Column_3": "7",
          "Column_4": "",
          "Column_5": "1.",
          "Column_6": "2",
          "Column_7": "8",
          "Column_8": "1",
          "Column_9": "5.",
          "Column_10": "8",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "3.",
          "Column_2": "3",
          "Column_3": "5.",
          "Column_4": "6",
          "Column_5": "5.",
          "Column_6": "6",
          "Column_7": "3.",
          "Column_8": "3",
          "Column_9": "8",
          "Column_10": "2",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "0",
          "Column_2": "",
          "Column_3": "0",
          "Column_4": "",
          "Column_5": "0",
          "Column_6": "",
          "Column_7": "0",
          "Column_8": "",
          "Column_9": "0",
          "Column_10": "",
          "Column_11": "1e+",
          "Column_12": "02"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "9",
          "Column_2": "4",
          "Column_3": "0.",
          "Column_4": "69",
          "Column_5": "1.",
          "Column_6": "4",
          "Column_7": "2.",
          "Column_8": "1",
          "Column_9": "2.",
          "Column_10": "1",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "2.",
          "Column_2": "3",
          "Column_3": "8",
          "Column_4": "2",
          "Column_5": "9.",
          "Column_6": "6",
          "Column_7": "4",
          "Column_8": "",
          "Column_9": "1.",
          "Column_10": "7",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "6",
          "Column_2": "",
          "Column_3": "1",
          "Column_4": "4",
          "Column_5": "7",
          "Column_6": "5",
          "Column_7": "2",
          "Column_8": "",
          "Column_9": "3",
          "Column_10": "",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "5.",
          "Column_2": "8",
          "Column_3": "7",
          "Column_4": "",
          "Column_5": "1.",
          "Column_6": "2",
          "Column_7": "8",
          "Column_8": "0",
          "Column_9": "5.",
          "Column_10": "8",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "3.",
          "Column_2": "3",
          "Column_3": "2.",
          "Column_4": "2",
          "Column_5": "6.",
          "Column_6": "7",
          "Column_7": "2.",
          "Column_8": "2",
          "Column_9": "8",
          "Column_10": "6",
          "Column_11": "0",
          "Column_12": ""
        },
        {
          "Column_1": "0",
          "Column_2": "",
          "Column_3": "0",
          "Column_4": "",
          "Column_5": "0",
          "Column_6": "",
          "Column_7": "0",
          "Column_8": "",
          "Column_9": "0",
          "Column_10": "",
          "Column_11": "1e+",
          "Column_12": "02"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion Theory and Research: Highlights, Unanswered Questions, and Emerging Issues",
      "authors": [
        "C Izard"
      ],
      "year": "2009",
      "venue": "Emotion Theory and Research: Highlights, Unanswered Questions, and Emerging Issues",
      "doi": "10.1146/annurev.psych.60.110707.163539"
    },
    {
      "citation_id": "2",
      "title": "The role of discrete emotions in health outcomes: A critical review",
      "authors": [
        "N Consedine",
        "J Moskowitz"
      ],
      "year": "2007",
      "venue": "Applied and Preventive Psychology",
      "doi": "10.1016/j.appsy.2007.09.001"
    },
    {
      "citation_id": "3",
      "title": "Predicting Tomorrow√¢‚Ç¨‚Ñ¢s Mood, Health, and Stress Level using Personalized Multitask Learning and Domain Adaptation",
      "authors": [
        "N Jaques",
        "O Rudovic",
        "S Taylor",
        "A Sano",
        "R Picard"
      ],
      "year": "2017",
      "venue": "Proceedings of IJCAI 2017 Workshop on Artificial Intelligence in Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Attention Is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2023",
      "venue": "Attention Is All You Need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "5",
      "title": "Comparative Analyses of Bert, Roberta, Distilbert, and Xlnet for Text-Based Emotion Recognition",
      "authors": [
        "A Adoma",
        "N.-M Henry",
        "W Chen"
      ],
      "year": "2020",
      "venue": "17th International Computer Conference on Wavelet Active Media Technology and Information Processing",
      "doi": "10.1109/ICCWAMTIP51612.2020.9317379"
    },
    {
      "citation_id": "6",
      "title": "A BERT based dual-channel explainable text emotion recognition system",
      "authors": [
        "P Kumar",
        "B Raman"
      ],
      "year": "2022",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2022.03.017"
    },
    {
      "citation_id": "7",
      "title": "BERT-ERC: Fine-Tuning BERT Is Enough for Emotion Recognition in Conversation",
      "authors": [
        "X Qin",
        "Z Wu",
        "T Zhang",
        "Y Li",
        "J Luan",
        "B Wang",
        "L Wang",
        "J Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v37i11.26582"
    },
    {
      "citation_id": "8",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "9",
      "title": "Recognizing Emotions in a Foreign Language",
      "authors": [
        "M Pell",
        "L Monetta",
        "S Paulmann",
        "S Kotz"
      ],
      "year": "2009",
      "venue": "Journal of Nonverbal Behavior",
      "doi": "10.1007/s10919-008-0065-7"
    },
    {
      "citation_id": "10",
      "title": "Speech Emotion Recognition",
      "authors": [
        "A Ingale",
        "D Chaudhari"
      ],
      "year": "2012",
      "venue": "Speech Emotion Recognition"
    },
    {
      "citation_id": "11",
      "title": "Head Fusion: Improving the Accuracy and Robustness of Speech Emotion Recognition on the IEMOCAP and RAVDESS Dataset",
      "authors": [
        "M Xu",
        "F Zhang",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3067460"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition system using gender dependent convolution neural network",
      "authors": [
        "V Singh",
        "S Prasad"
      ],
      "year": "2023",
      "venue": "Procedia Computer Science",
      "doi": "10.1016/j.procs.2023.01.227"
    },
    {
      "citation_id": "13",
      "title": "Speech Emotion Classification using Ensemble Models with MFCC",
      "authors": [
        "M Mohan",
        "P Dhanalakshmi",
        "R Kumar"
      ],
      "year": "2023",
      "venue": "Speech Emotion Classification using Ensemble Models with MFCC",
      "doi": "10.1016/j.procs.2023.01.163"
    },
    {
      "citation_id": "14",
      "title": "On the Use of Self-Supervised Pre-Trained Acoustic and Linguistic Features for Continuous Speech Emotion Recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Est√£¬®ve",
        "A Rousseau"
      ],
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT), 2021",
      "doi": "10.1109/SLT48900.2021.9383456"
    },
    {
      "citation_id": "15",
      "title": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings",
      "doi": "10.48550/arXiv.2104.03502",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "16",
      "title": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Multi-level Fusion of Wav2vec 2.0 and BERT for Multimodal Emotion Recognition",
      "doi": "10.48550/arXiv.2207.04697",
      "arxiv": "arXiv:2207.04697"
    },
    {
      "citation_id": "17",
      "title": "Spanish MEACorpus 2023: A multimodal speech-text corpus for emotion analysis in spanish from natural environments",
      "authors": [
        "R Pan",
        "J Garc√≠a-D√≠az",
        "M Rodr√≠guez-Garc√≠a",
        "R Valencia-Garc√≠a"
      ],
      "year": "2024",
      "venue": "Computer Standards & Interfaces"
    },
    {
      "citation_id": "18",
      "title": "Facial Expressions of Emotion, Annual Review of Psychology",
      "authors": [
        "P Ekman",
        "H Oster"
      ],
      "year": "1979",
      "venue": "Facial Expressions of Emotion, Annual Review of Psychology",
      "doi": "10.1146/annurev.ps.30.020179.002523"
    },
    {
      "citation_id": "19",
      "title": "Unsupervised Cross-lingual Representation Learning for Speech Recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised Cross-lingual Representation Learning for Speech Recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "20",
      "title": "Boosting transformers for job expression extraction and classification in a low-resource setting",
      "authors": [
        "L Lange",
        "H Adel",
        "J Str√∂tgen"
      ],
      "year": "2021",
      "venue": "Boosting transformers for job expression extraction and classification in a low-resource setting",
      "arxiv": "arXiv:2109.08597"
    },
    {
      "citation_id": "21",
      "title": "Overview of EmoSPeech 2024@IberLEF: Multimodal Speech-text Emotion Recognition in Spanish, Procesamiento del Lenguaje Natural",
      "authors": [
        "R Pan",
        "J Garc√≠a-D√≠az",
        "M Rodr√≠guez-Garc√≠a",
        "F Garc√≠a-S√°nchez",
        "R Valencia-Garc√≠a"
      ],
      "year": "2024",
      "venue": "Overview of EmoSPeech 2024@IberLEF: Multimodal Speech-text Emotion Recognition in Spanish, Procesamiento del Lenguaje Natural"
    },
    {
      "citation_id": "22",
      "title": "Overview of IberLEF 2024: Natural Language Processing Challenges for Spanish and other Iberian Languages",
      "authors": [
        "L Chiruzzo",
        "S Jim√©nez-Zafra",
        "F Rangel"
      ],
      "year": "2024",
      "venue": "Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2024), co-located with the 40th Conference of the Spanish Society for Natural Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "24",
      "title": "Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing",
      "doi": "10.1109/JSTSP.2022.3188113",
      "arxiv": "arXiv:2110.13900"
    },
    {
      "citation_id": "25",
      "title": "Librilight: A benchmark for asr with limited or no supervision, in: ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "authors": [
        "J Kahn",
        "M Riviere",
        "W Zheng",
        "E Kharitonov",
        "Q Xu",
        "P Mazare",
        "J Karadayi",
        "V Liptchinsky",
        "R Collobert",
        "C Fuegen",
        "T Likhomanenko",
        "G Synnaeve",
        "A Joulin",
        "A Mohamed",
        "E Dupoux"
      ],
      "year": "2020",
      "venue": "Librilight: A benchmark for asr with limited or no supervision, in: ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/icassp40776.2020.9052942"
    },
    {
      "citation_id": "26",
      "title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "G Chen",
        "S Chai",
        "G Wang",
        "J Du",
        "W.-Q Zhang",
        "C Weng",
        "D Su",
        "D Povey",
        "J Trmal",
        "J Zhang",
        "M Jin",
        "S Khudanpur",
        "S Watanabe",
        "S Zhao",
        "W Zou",
        "X Li",
        "X Yao",
        "Y Wang",
        "Y Wang",
        "Z You",
        "Z Yan"
      ],
      "year": "2021",
      "venue": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "arxiv": "arXiv:2106.06909"
    },
    {
      "citation_id": "27",
      "title": "Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
      "authors": [
        "C Wang",
        "M Rivi√®re",
        "A Lee",
        "A Wu",
        "C Talnikar",
        "D Haziza",
        "M Williamson",
        "J Pino",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
      "arxiv": "arXiv:2101.00390"
    },
    {
      "citation_id": "28",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "29",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2020",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "30",
      "title": "Speech recognition and keyword spotting for lowresource languages: Babel project research at cued, in: Workshop on Spoken Language Technologies for Under-resourced Languages",
      "authors": [
        "M Gales",
        "K Knill",
        "A Ragni",
        "S Rath"
      ],
      "year": "2014",
      "venue": "Speech recognition and keyword spotting for lowresource languages: Babel project research at cued, in: Workshop on Spoken Language Technologies for Under-resourced Languages"
    },
    {
      "citation_id": "31",
      "title": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "authors": [
        "V Pratap",
        "Q Xu",
        "A Sriram",
        "G Synnaeve",
        "R Collobert"
      ],
      "venue": "MLS: A Large-Scale Multilingual Dataset for Speech Research",
      "doi": "10.21437/Interspeech.2020-2826",
      "arxiv": "arXiv:2012.03411"
    },
    {
      "citation_id": "32",
      "title": "Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "arxiv": "arXiv:2106.07447"
    },
    {
      "citation_id": "33",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "34",
      "title": "Boosting transformers for job expression extraction and classification in a low-resource setting",
      "authors": [
        "L Lange",
        "H Adel",
        "J Str√∂tgen"
      ],
      "year": "2021",
      "venue": "Proceedings of The Iberian Languages Evaluation Forum"
    },
    {
      "citation_id": "35",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzm√°n",
        "E Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning at scale",
      "arxiv": "arXiv:1911.02116"
    },
    {
      "citation_id": "36",
      "title": "Compilation of large spanish unannotated corpora",
      "authors": [
        "J Ca√±ete"
      ],
      "year": "2019",
      "venue": "Compilation of large spanish unannotated corpora",
      "doi": "10.5281/zenodo.3247731"
    },
    {
      "citation_id": "37",
      "title": "Spanish pre-trained bert model and evaluation data",
      "authors": [
        "J Ca√±ete",
        "G Chaperon",
        "R Fuentes",
        "J.-H Ho",
        "H Kang",
        "J P√©rez"
      ],
      "year": "2020",
      "venue": "Spanish pre-trained bert model and evaluation data"
    },
    {
      "citation_id": "38",
      "title": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
      "authors": [
        "J Tiedemann",
        "N Calzolari",
        "K Choukri",
        "T Declerck",
        "M Doƒüan",
        "B Maegaard",
        "J Mariani"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "39",
      "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "40",
      "title": "Self multi-head attention for speaker recognition",
      "authors": [
        "M India",
        "P Safari",
        "J Hernando"
      ],
      "year": "2019",
      "venue": "Self multi-head attention for speaker recognition",
      "arxiv": "arXiv:1906.09890"
    },
    {
      "citation_id": "41",
      "title": "Double multi-head attention multimodal system for odyssey 2024 speech emotion recognition challenge",
      "authors": [
        "F Costa",
        "M India",
        "J Hernando"
      ],
      "year": "2024",
      "venue": "Double multi-head attention multimodal system for odyssey 2024 speech emotion recognition challenge",
      "arxiv": "arXiv:2406.10598"
    },
    {
      "citation_id": "42",
      "title": "Double multi-head attention for speaker verification",
      "authors": [
        "M India",
        "P Safari",
        "J Hernando"
      ],
      "year": "2021",
      "venue": "Double multi-head attention for speaker verification",
      "arxiv": "arXiv:2007.13199"
    },
    {
      "citation_id": "43",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "44",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2019",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    }
  ]
}