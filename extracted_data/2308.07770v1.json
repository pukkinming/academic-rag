{
  "paper_id": "2308.07770v1",
  "title": "Multi-Scale Promoted Self-Adjusting Correlation Learning For Facial Action Unit Detection",
  "published": "2023-08-15T13:43:48Z",
  "authors": [
    "Xin Liu",
    "Kaishen Yuan",
    "Xuesong Niu",
    "Jingang Shi",
    "Zitong Yu",
    "Huanjing Yue",
    "Jingyu Yang"
  ],
  "keywords": [
    "facial AU detection",
    "AU correlation learning",
    "self-adjusting graph structure",
    "multi-scale learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Action Unit (AU) detection is a crucial task in affective computing and social robotics as it helps to identify emotions expressed through facial expressions. Anatomically, there are innumerable correlations between AUs, which contain rich information and are vital for AU detection. Previous methods used fixed AU correlations based on expert experience or statistical rules on specific benchmarks, but it is challenging to comprehensively reflect complex correlations between AUs via hand-crafted settings. There are alternative methods that employ a fully connected graph to learn these dependencies exhaustively. However, these approaches can result in a computational explosion and high dependency with a large dataset. To address these challenges, this paper proposes a novel self-adjusting AU-correlation learning (SACL) method with less computation for AU detection. This method adaptively learns and updates AU correlation graphs by efficiently leveraging the characteristics of different levels of AU motion and emotion representation information extracted in different stages of the network. Moreover, this paper explores the role of multi-scale learning in correlation information extraction, and design a simple yet effective multi-scale feature learning (MSFL) method to promote better performance in AU detection. By integrating AU correlation information with multi-scale features, the proposed method obtains a more robust feature representation for the final AU detection. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on widely used AU detection benchmark datasets, with only 28.7% and 12.0% of the parameters and FLOPs of the best method, respectively. The code for this method is available at https://github.com/linuxsino/Self-adjusting-AU.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "•",
      "text": "We explore the role of multi-scale learning in correlation information learning, and design a simple yet effective MSFL method to promote SACL and obtain a more informative and robust representation of AUs for final AU detection.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "•",
      "text": "Extensive experiments show that the proposed method outperforms the state-of-the-art methods on the widely used AU detection benchmark datasets, namely, the BP4D  [26]  and DISFA  [27] .\n\nIn the rest of the paper, Section 2 reviews related work focusing on AU correlation learning, region learning and using additional relevant data. Section 3 introduces the details of our method and the loss functions utilized in training. Section 4 provides comparisons of experimental results between our method and the state-of-the-art methods, demonstrating the superiority of our method, and also includes the details of ablation studies. Finally, conclusions and future work are given in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Due to the rapid development of computer vision in recent years, facial AU detection has attracted more and CVPR 2019 using LSTM for local features without using graphs FAUDT  [15]  CVPR 2021 using Transformer for AU-specific features FAN-Trans  [16]  WACV 2023 using Transformer with a drop mechanism for AU-specific features DSIN  [13]  ECCV 2018 using a message passing algorithm between AUs SRERL  [2]  AAAI 2019 using a fixed graph based on the statistical rules correlation learning AU-GCN  [3]  MMM 2020 using a fixed graph based on the statistical rules using graphs UGN-B  [5]  AAAI 2021 using a fully connected graph with a probabilistic mask HMP-PS  [14]  CVPR 2021 using a performance-driven hybrid message passing algorithm ME-GraphAU  [4]  IJCAI 2022 using a fully connected graph with multi-dimensional edges region learning JPML  [17]  CVPR 2015 selecting a sparse subset of face patches to exploit group sparsity DRML  [18]  CVPR 2016 using the region layer to focus on important face regions EAC-Net  [19]  TPAMI 2018 using enhancing layers and cropping layers for AU-specific facial regions ARL  [24]  TAC 2019 using channel-wise and spatial attentions for learning AU region features J ÂA-Net  [23]  IJCV 2021 using a hierarchical and multi-scale region layer for multi-scale learning PIAP  [20]  ICCV 2021 using a pixel-wise interest learning to obtaining fine-grained features using additional SEV-Net  [28]  CVPR 2021 introducing the AU semantic text descriptions relevant data KDSRL  [1]  CVPR 2022 using a larger dataset BP4D+ for pre-training AUFM  [29]  CVPR 2023 introducing the biomechanical guidance based on 3D surface mesh more researchers, and several effective methods have been proposed. Firstly, we focus on the related AU correlation learning. Then, we give an overview of the literature related to region learning. Finally, we present some recent competitive methods that use additional relevant data. Table  1  shows a rough summary and classification of representative methods.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Au Correlation Learning",
      "text": "According to the analysis of various facial expressions, there are symbiosis or mutual exclusion correlations between AUs. Therefore, AU correlation learning is very necessary. In the early works  [9] ,  [10] ,  [11] , researchers explored the correlations between AUs using DBN or RBM. However, these works are limited because they are based on hand-crafted features. Niu et al.  [12]  used LSTM for correlation learning of local features generated by ResNet  [30] . Jacob and Stenger  [15]",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Region Learning",
      "text": "In addition, AU is known to be active in diverse regions of the face. Focusing on these AU-related regions is also helpful for AU detection. Zhao et al.  [17]  explored patches centered at facial landmarks and selected a sparse subset of face patches to exploit group sparsity. Zhao et al.  [18]  proposed a region layer, which induces important facial regions by uniformly slicing the feature map from shallow layers into patches and using independent convolution kernels on each patch, forcing the learned weights to capture the structural information of the face. Li et al.  [19]  introduced enhancing layers and cropping layers into a pretrained CNN to perform deeper feature learning for AU-specific facial regions. Besides, the size of ROIs corresponding to each AU is varied, which is not taken into account when only extracting features from regions of fixed size. Therefore, multi-scale learning is introduced to extract richer features. Shao et al.  [22] ,  [23]  improved the region layer in  [18]  and proposed a hierarchical and multi-scale region learning layer. The novel region learning layer is composed of three region layers in  [18] , and each layer uniformly sliced the feature maps into a different number of patches. The feature maps extracted from the three region layers are concatenated to obtain the hierarchical and multi-scale features. Shao et al.  [24]  adopted the hierarchical and multi-scale features proposed in  [22] ,  [23]  and introduced channel-wise and spatial attentions to adaptively learn region features related to AUs. Recently, Tang et al.  [20]  proposed a pixelwise interest learning method with pixel-level attention for each AU to extract fine-grained features.\n\nFig.  2 : An overview of our method for AU detection, which mainly consists of stem network, MSFL, LP, and SACL. The face image is firstly fed into the stem network to obtain basic features. Then, the basic features are further processed through the remaining three modules. Finally, the multi-scale features and AU correlation information are integrated for the final AU detection. 'C' denotes concatenation operation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Using Additional Relevant Data",
      "text": "In recent years, there are also some methods to introduce additional relevant data to improve the performance of AU detection. Yang et al.  [28]  summarized the semantically rich text descriptions of AUs in FACS  [6]  and used text description information to assist in the generation of attention maps to capture AU-specific regional features. Chang et al.  [1]  built an AU correlation graph based on the summary of FACS  [6]  and relied on this prior knowledge to perform self-supervised representation learning, where the larger BP4D+  [33]  dataset was used for pre-training. Cui et al.  [29]  constructed a 3D physical branch, introducing biomechanical guidance based on 3D surface mesh to enhance the performance of AU detection.\n\nIn contrast to these existing methods, our proposed SACL adaptively adjusts the AU correlation graph as the network deepens, effectively leveraging the characteristics of different levels of AU motion and emotion representation information extracted in different stages of the network. SACL captures more accurate AU correlation information than fixed graphs and saves computational effort compared to data-driven fully connected graphs. Additionally, we introduce MSFL to further improve SACL performance, which is rarely explored by existing methods. By integrating the features output from SACL and MSFL, we can obtain a more robust representation for final AU detection. It is worth noting that our method does not use additional relevant data during the training process.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overview",
      "text": "The architecture of the proposed method is shown in Figure  2 . It is mainly composed of four modules: stem network, MSFL, landmarks predictor (LP), and SACL. Firstly, the face image is input to the stem network for extracting the basic features. Then, the basic features are fed into the following three modules. Since different AUs have various sizes, MSFL is designed to obtain multi-scale global features of faces and explore its role in AU correlation learning. LP is used to predict the locations of facial landmarks, which are further utilized to calculate AU-specific centers. According to the centers of AUs, the corresponding ROIs are cut out from the basic features extracted by stem network, which are fed into SACL. SACL is designed to capture correlation information between AUs, which updates the graph structure after each feature mapping to mine more implicit correlation information. Finally, the features from MSFL and SACL are integrated to realize the final AU detection.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Stem Network And Multi-Scale Feature Learning",
      "text": "Our method adopts a flexible framework design pattern so that a variety of different pyramid Transformer models  [34] ,  [35] ,  [36] ,  [37] ,  [38]  can be used as the backbone for building the stem network and MSFL to extract global information of the entire face. In general, the overall architecture of the common pyramid Transformer is mainly composed of a stem network and four stages that output features with various receptive fields, as shown at the bottom of Figure  2 .\n\nTo extract basic features from the input face image with size of H × W × 3, the stem network is simply consistent with the selected pyramid Transformer model consisting of several convolutional layers with overlapping. Then, these obtained basic features F ∈ R H 4 × W 4 ×d0 will be fed into subsequent modules for further processing. Considering that different AUs correspond to various region sizes, MSFL is introduced into the four stages of pyramid Transformer to extract more informative features and enrich the receptive field of features. Specifically, the features extracted by the four stages of the pyramid Transformer can be represented as and\n\n, respectively. The features extracted by the last three stages are interpolated to align the spatial size with the features extracted by the first stage, and then, these aligned features are concatenated to form multi-scale features\n\nThis process is depicted in the blue area in Figure  2  and can be expressed as\n\nWhere, 2×, 4×, and 8× represent two-fold, four-fold, and eight-fold interpolation in the spatial dimension, respectively.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Landmarks Predictor",
      "text": "In order to locate the AU-specific centers for cropping ROIs, Landmarks Predictor is designed to extract the shape information of the face and predict the locations of the facial landmarks. LP contains three consecutive blocks, each consisting of two convolutional layers and a max-pooling layer (shown in the orange area of Figure  2 ). After each convolution operation, the features are processed with Batch Normalization  [39]  and non-linearly transformed by the ReLU activation function  [40] . The features obtained from the last max-pooling layer are flattened and fed into the fully connected layer (omitted in Figure  2 ) for landmarks prediction. The output dimension of the fully connected layer is N land , which is the number of landmark coordinates.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Self-Adjusting Au-Correlation Learning",
      "text": "Referring to  [22] ,  [23] , based on the predicted landmark coordinates, the AU-specific centers are determined. The definition rules and calculation formula of each AU-specific center location are shown in Table  2 , and the visualization of AU-specific center location is shown in Figure  3 (a) .\n\nBecause the spatial size of features obtained by the stem network differs from the input image, the coordinates of the landmarks need to be multiplied by a factor η for scaling. Then, the ROIs are cropped from the basic features according to the centers of AUs, where the spatial size ratio of the ROIs to the basic features is ξ. The size of each ROI is\n\n, where d 0 is the number of basic The proposed SACL is illustrated in the green area of Figure  2 . Firstly, the features of obtained ROIs are flattened, and each ROI is taken as a graph node v i ∈ R d1 , where d 1 = (ξ × H 4 )×(ξ × W 4 )×d 0 . Thus, the node set can be represented as\n\nwhere N ROI is the number of ROIs. In the feature domain, the correlations between nodes are negatively correlated with the feature distance, that is, the smaller the feature distance, the greater the correlation, and vice versa. Based on the above characteristics, the Knearest neighbors (via the KNN algorithm) are calculated for each node to get the edge set E. Finally, the graph of AUs G = (V, E) can be constructed.\n\nGiven a graph structure data G ∈ R N ROI ×d1 , the graph convolution can be utilized to aggregate and communicate information between adjacent nodes for learning the correlation information between AUs. In order to increase the diversity of features, fully connected layers are added before and after graph convolution, and the activation function for nonlinear mapping is applied. In addition, the residual connection is introduced to avoid a vanishing gradient. Specifically, this process can be expressed as\n\nwhere G ′ ∈ R N ROI ×d1 , σ is the GeLU  [41]  activation function, W bef ore and W af ter are the learnable weights of the fully connected layers before and after graph convolution, respectively.\n\nInspired by  [42] , a feed forward network with two fully connected layers is introduced to alleviate the over smoothing phenomenon in the graph convolution. We also introduce residual connections. Specifically, this process can be expressed as\n\nwhere G ′′ ∈ R N ROI ×d1 , W 1 and W 2 are the learnable weights of the fully connected layers. After each feed forward network, the K-nearest neighbors are recalculated using the obtained features to update the graph structure, and thereby, self-adjusting learning can be realized. After several graph processing, a fully connected layer is introduced for feature dimension expansion, and then the graph is re-initialized.\n\nIt can be seen from Figure  2  that SACL consists of S stages, each of which includes initializing of graph structure, L i graph processing blocks and a fully connected layer. The final features of the S th stage is B ∈ R N ROI ×D . By constantly updating the graph structure, SACL can make use of the similarities and differences between shallow layer muscle motion information and deep layer emotion representation information to construct different graph structures. In that way, the information between related AUs can be sufficiently communicated and aggregated, and more effective and implicit correlation information can be extracted.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Au Detection",
      "text": "Considering the inconsistent sizes of the multi-scale features A and the AU correlation features B, a transformation is performed on A to integrate the features of two branches. Since the tokens in the Transformer features are treated as nodes, A is flattened in the spatial dimension to get\n\nFinally, as illustrated in Figure  2 , A ′ and B are concatenated together to obtain C ∈ R (N M S +N ROI )×D with comprehensive effective information, which is finally fed into the classifier with fully connected layers for the final AU detection.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Loss Function",
      "text": "AU detection is regarded as a multi-label binary classification problem, and many previous methods  [3] ,  [12] ,  [15] ,  [22] ,  [23]  adopted a weighted multi-label cross-entropy loss function for supervision. However, the difficulty level of detecting various AUs is different, but this loss function did not fully take into account these differences among AUs. For example, a well-designed loss function could push the network to learn the features of AUs which are hard to be identified correctly. In this paper, the weighted asymmetric loss function  [4]  is employed to focus on activate AUs and inactivate AUs that are hard to be correctly recognized. It can be formulated as\n\nwhere y i is the ground truth of the i th AU, p i is the corresponding prediction probability, and N AU is the number of AUs. ω i is the weight for alleviating the imbalance problem in the dataset, and it can be formulated as\n\n, where r i is the occurrence rate of the i th AU.\n\nMoreover, considering that AU detection strongly biases towards non-occurrence, a weighted multi-label dice loss  [22] ,  [23]  is introduced. It can be formulated as\n\nwhere ε is a smooth term. Additionally, a face alignment loss is utilized to supervise the LP. It can be formulated as\n\nwhere c 2i-1 and c 2i are the ground truth of x coordinate and y coordinate of the i th landmark, respectively, and ĉ2i-1 and ĉ2i are the corresponding predictions. d o is the inter-ocular distance of ground truth for normalization  [22] ,  [23] ,  [43] ,  [44] ,  [45] ,  [46] . Combined with the above loss functions, our overall loss function can be formulated as\n\nwhere λ 1 , λ 2 and λ 3 are the trade-off parameters.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we provide evaluation results on several datasets widely used for AU detection and present a detailed analysis of the experimental results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Settings",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "We evaluate the proposed method on two benchmark datasets, namely, the BP4D  [26]  and DISFA  [27] .\n\n• BP4D contains 2D and 3D data of spontaneous facial expressions from 41 subjects (23 females and 18 males). Each subject was given well-validated emotion induction through 8 different tasks. There are totally 328 videos, and about 140,000 frames are annotated with AU labels. Each frame is also annotated with 49 landmarks detected by SDM  [47] .\n\n• DISFA records facial expression data from 27 subjects (12 females and 15 males) while watching video clips intended to elicit spontaneous emotion expression. About 130,000 frames are available in DISFA, and each frame is annotated with AU intensity from 0 to 5. According to  [18] , if the AU intensity is equal or greater than 2, it is considered to be present; otherwise, it is not present. Each frame in DISFA is also annotated with 66 landmarks detected by AAM  [48] , and we map them to 49 landmarks for consistency with BP4D  [23] .\n\nFollowing the experiment setting of  [4] ,  [18] ,  [22] ,  [23] , a subject-exclusive 3-fold cross-validation is performed on TABLE 3: The partition details of BP4D  [26]  and DISFA  [27]  in the subject-exclusive 3-fold cross-validation, where two folds are used for training and the remaining fold is used for testing. BP4D Fold-1 F001, F002, F008, F009, F010, F018, F016, F023, M001, M004, M007, M008, M012, M014.\n\nFold-2 F003, F005, F011, F013, F020, F022, M002, M005, M010, M011, M013, M016, M017, M018.\n\nFold-3 F004, F006, F007, F012, F014, F015, F017, F019, F021, M003, M006, M009, M015.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Disfa",
      "text": "Fold-1 SN001, SN002, SN009, SN010, SN016, SN026, SN027, SN030, SN032.\n\nFold-2 SN006, SN011, SN012, SN013, SN018, SN021, SN024, SN028, SN031.\n\nFold-3 SN003, SN004, SN005, SN007, SN008, SN017, SN023, SN025, SN029.\n\nBP4D, and the best-trained model on BP4D is then finetuned on DISFA. The partition details of each fold of BP4D and DISFA are shown in Table  3 .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "For each original face image, we conduct similarity transformation using landmarks. This transformation includes inplane rotation, uniform scaling, and translation, which preserves the shape and expression information while reducing the variations of pose and scale. Then, the aligned faces are resized to 256 × 256. To increase the diversity of the data, we randomly crop these face images to 224 × 224 as the inputs. Furthermore, random horizontal flipping and random color jittering (contrast, brightness) for data augmentation are also introduced.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "We choose the ResT-Lite  [35]  (pretrained on ImageNet-1k  [49] ) as the backbone (for our stem network and MSFL) due to its lightweight. Also, it has a similar structure with ResNet  [30] , which has been widely used in previous AU detection methods  [2] ,  [4] ,  [12] ,  [14] . During the training, we set batch size to 16 and employ an SGD optimizer  [50]  with a Nesterov momentum  [51]  of 0.9 and a weight decay of 0.0005. We use a cosine decay learning rate scheduler with a maximum learning rate of 0.001 for 12 epochs, and the first epoch is used for linear warm-up. Besides, We use gradient clipping  [52]  with a max norm of 5. Table  4  shows the parameter settings of our model. The parameters about the loss function are set as λ 1 = 1, λ 2 = 1, λ 3 = 0.5. All our experiments are conducted using an Nvidia RTX 3090 GPU based on the open-source PyTorch  [53]  platform.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Similar to previous methods, the F-measure (F1) score  [55]  is applied to evaluate the effectiveness of methods. The F1 score is a harmonic mean of precision and recall, with a Max-Relative GraphConv  [54]  maximum value of 1 and a minimum value of 0, which can be formulated as\n\nBesides, the accuracy is also taken into account for a more comprehensive comparison.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "Sixteen state-of-the-art image-based AU detection methods are compared with the proposed method, including JPML  [17] , DRML  [18] , EAC-Net  [19] , ARL  [24] , J ÂA-Net  [23] , PIAP  [20] , LP-Net  [12] , FAUDT  [15] , FAN-Trans  [16] , AU-GCN  [3] , SRERL  [2] , UGN-B  [5] , HMP-PS  [14] , ME-GraphAU  [4] , KDSRL  [1]  and AUFM  [29] . The summary and classification of each method is shown in Table  1 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation On Bp4D",
      "text": "Table  5  shows the F1 score and accuracy results of different methods on BP4D. It can be seen that our method outperforms all methods based solely on region learning, including JPML, DRML, EAC-Net, ARL, J ÂA-Net, and PIAP, which is attributed to the capture of AU correlation information in our method. Our method also shows superiority compared to LP-Net, FAUDT and FAN-Trans, which model AU correlations without using the graph model. In particular, we compare the proposed model with graph-based methods for extracting AU correlations, including AU-GCN, SRERL, UGN-B, HMP-PS, and ME-GraphAU. As shown in Table  5 , our method still shows superior performance compared to other graph-based methods. The reason behind this is that the proposed SACL can mine more implicit correlation information between AUs, and the designed MSFL plays a facilitating role. Moreover, we compare our method to those that use additional data for training, namely AUFM and KDSRL. Although more relevant data (3D surface mesh or BP4D+  [33] ) are applied in AUFM and KDSRL, our method can obtain better results. In addition, we also compare the accuracy with popular AU detection models of EAC-Net, ARL, UGN-B and J ÂA-Net, and our method achieves a great improvement of 2.3%.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluation On Disfa",
      "text": "Table  6  shows the F1 score and accuracy results of different methods on DISFA. It can be observed that our method outperforms all state-of-the-art methods by at least 1.0% on F1 score and obtains comparable results on accuracy. This demonstrates the strong generalization ability of our method.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation studies on BP4D to investigate the impact of each part of the proposed method on the results, the computational complexity of the model, also, the choice of some important parameters and settings.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Each Part Of The Proposed Method",
      "text": "Table  7  shows the F1 score results of different architecture implementations. We set original ResT-Lite without data augmentation as the baseline. Its result is shown in the first row of Table  7 .\n\nEffectiveness of Data Augmentation. Because the collection of datasets is time-consuming and labor-intensive, the data fall into the dilemma of small numbers and limited scenarios. Data augmentation is an effective measure to increase the diversity of data. Therefore, we use random cropping, random horizontal flipping, and random color jittering for data augmentation. It can be seen from Table  7  that the introduction of data augmentation brings a 0.8% improvement in F1 score, which indicates that richer data is indeed beneficial for model training.\n\nEffectiveness of MSFL. We verify the effectiveness of MSFL, as shown in the third row of Table  7 . It can be observed that Baseline * +MSFL achieves an improvement in F1 score compared to Baseline * , which indicates that preserving spatial information and combining features of different scales obtained from different stages to enrich the receptive field is helpful for AU detection. In addition, we remove the features from the first, first two, and first three Effectiveness of SACL. To demonstrate the superiority of the proposed SACL, we firstly introduce it alone into Baseline * and get the variant Baseline * +SACL, which brought a 1.3% improvement, as shown in the fourth row of Table  7 . Then, we take MSFL into account and construct several variants by introducing different graph structures. Please note that Baseline * +MSFL+FBG, Baseline * +MSFL+SBG, and Baseline * +MSFL+SACL represent the introduction of the FACS-based graph, statistics-based graph, and self-adjusting AU-correlation learning graph into Baseline * +MSFL, respectively, so the Baseline * +MSFL+SACL is the proposed method. Baseline * +MSFL+FBG adopts the AU correlation graph proposed in  [1] , and Baseline * +MSFL+SBG adopts the AU correlation graph proposed in  [3] . For fairness, the above three   The role of MSFL in SACL. We explore the role of MSFL in SACL, which has rarely been mentioned in previous work. It can be observed from Table  7  that in the absence of MSFL, Baseline * +SACL brings a 1.3% improvement compared with Baseline * . After introducing MSFL, Baseline * +MSFL+SACL can improve Baseline * +MSFL by 1.8%. The above reveals the promotion effect of MSFL on SACL. With the introduction of MSFL, SACL can perform better. Besides, we also discuss this conclusion through the perspective of visualization, see Section 4.4 for details.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model Complexity Comparison",
      "text": "In order to demonstrate the lightness and convenience of our method, we compare the model complexity with the current optimal method ME-GraphAU (based on a fully connected graph)  [4] . Table  8  shows the number of parameters and FLOPs of our model and its variants as well as ME-GraphAU. It can be observed that the number of parameters and FLOPs of our entire model are only 28.7% and 12.0% of  ME-GraphAU, respectively. The reason is that ME-graphAU calculated the multi-dimensional edge features between each AU pairs ergodically based on the fully connected graph structure, which is computationally expensive. In contrast, we utilize SACL (with fewer edge connections) to adaptively learn and update AU correlation graphs by efficiently leveraging the characteristics of different levels of AU motion and emotion representation information extracted in different stages of the network, thereby mining more implicit AU correlation information with less model complexity. Importantly, the proposed network is trained in an end-to-end manner rather than a two-stage training done in ME-GraphAU.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Choice Of Important Parameters And Settings",
      "text": "Table  9  and Figure  5  shows the F1 score results obtained by making different choices for important parameters and settings. Next, we will explain them one by one.\n\nChoice of feature fusion mechanism. To maximize the promotion of SACL by MSFL, it is crucial to explore the fusion method of the two. Therefore, we compare the two fusion mechanisms: cross-attention and concatenation. The implementation of cross-attention is similar to that in  [4] . The F1 score results are shown in the first group of the Table  9 . It can be seen that the effect of directly concatenating is better than applying cross-attention. We think that the reason is the feature information obtained by the two branches has different meanings, and the cross-attention may induce confusion. Conversely, concatenating the features directly could preserve the feature information to the greatest extent.\n\nChoice of correlation learning way. Due to the powerful long-distance dependency modeling capability of the Transformer  [56] , it is also considered for correlation learning, such as in  [15] ,  [16] . We replace the GCN in the SACL with six consecutive vanilla Transformer blocks, and compare the performance of the two ways, see the second group of Table  9 . It can be observed that GCN outperforms Transformer. We suppose that this is because Transformer has large hunger on labelled data, and struggles to capture the correct correlations with AU-related small-scale datasets. In addition, the computational complexity of Transformer is higher, and just six blocks lead to 74.0M parameters and 3.41G FLOPs, which increase by 173% and 33% than our method, respectively. Therefore, GCN is more portable and practical for AU correlation learning.\n\nChoice of distance metric. When updating AU correlation graph with K-nearest neighbor algorithm, it is worth exploring how to measure the distance between features. Therefore, we try the Manhattan distance, Cosine similarity, and Euclidean distance, as shown in the third group in Table  9 . It should be noted that we use 1 minus cosine similarity to represent cosine distance. It can be observed that the Euclidean distance achieved the best results, followed by the Manhattan distance. Therefore, when calculating the Knearest neighbors, we choose the Euclidean distance.\n\nChoice of GCN type. The type of GCN determines the way information is aggregated and communicated between nodes. In order to choose the appropriate GCN type, we compare several representative GCN variants, including EdgeConv  [57] , GIN  [58] , GraphSAGE  [59] , and Max- Relative GraphConv  [54] . It can be observed from the fourth group of Table  9  that, Max-Relative GraphConv achieves the best average F1 score, so we choose it. In the other experiments in this paper, we use Max-Relative GraphConv by default.\n\nSetting of the number of stages of SACL (S). To explore the optimal structure, we ablate S from 1 to 7, as shown in Figure  5  (a). When S is small, the depth of network is insufficient, and the extracted features contain less deep emotion representation information of AUs, thus achieving suboptimal performance. When S is large, the network tends to overfit and its performance decreases gradually. In summary, the results show that S equal to 4 is the most suitable setting.\n\nSetting of the number K in KNN algorithm. The number K of K-nearest neighbors is related to the construction of the AU correlation graph. As shown in Figure  5  (b), when K is equal to 9, the F1 score reaches the highest result. As K gradually decreases, the F1 score decreases. This is because if too few neighbors are considered, the communication of information between nodes will be destroyed, thus losing important implicit correlation information and affecting the following construction of the AU correlation graph. Besides, when K increases gradually, the F1 score also decreases. This is because if too many nodes are considered, redundant and noisy information will inevitably be introduced, which will interfere with valid information and thus make the final result worse. To summarize, we set the number K in Knearest neighbors to 9.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Visualization",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Au Correlation Graph",
      "text": "To better demonstrate the superiority of SACL, Figure  6    ods; here, we utilize the AU1 and AU7 as the central node for illustration. It can be seen that the AU correlation graphs based on the FACS summary  [1]  and the statistical rules of the dataset  [2]  are limited and inconsistent, as shown in Figure  6  (a) and (b). In contrast, our SACL not only considers the correlation information captured by previous methods but also obtains more implicit AU correlation information. Specifically, in the shallow layers, compared to the previous methods, more potential correlations are captured based on the similarity of the muscle motion information, such as AU1 and AU15, AU7 and AU10, as shown in Figure  6  (c). Moreover, in the deep layers, according to the similarity of emotion representation information, we not only correct the correlations obtained in the shallow layers (such as supplementing the ignored AU7 and AU6, and removing the redundant AU7 and AU15), but also capture more implicit correlations (such as AU1 and AU17, AU7 and AU24), as shown in Figure  6 (d) . From a psychological perspective, AU1, AU15, and AU17 show a symbiotic relationship in sadness, and AU7, AU10, and AU24 always co-occur in anger  [8] , which indicates that the obtained correlation information is meaningful.\n\nIn addition, we also visualize the deep AU correlation graph obtained by Baseline * +SACL, and observe that in the absence of MSFL, the variant will miss some basic and crucial correlation information, as shown in Figure  6 (e) . This finding confirms the promoting effect of MSFL on SACL.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "T-Sne",
      "text": "In order to demonstrate the effectiveness and discriminability of the features extracted by our method, we sample some images to visualize the final output of our model using t-SNE  [60] . Figure  7  shows the t-SNE distribution of some common AU combinations in BP4D. The legends in Figure  7 , such as 'AUs  (6, 7, 10, 12, 14) ', represent that AU6, 7, 10, 12, and 14 are all activated in the sample, and other legends follow the same convention. We can observe that samples with the same AU combination are clustered closely, while those with different AU combinations are farther apart. Moreover, we found that the blue, pink, and yellow dots are closer to each other than to other dots. This is because the AU combinations of the latter two (AUs  (6, 7, 10, 12)  and AUs  (10, 12) ) are subsets of the former (AUs  (6, 7, 10, 12, 14) ), while the green dots with independent labels (AU4) are farther from other dots. In summary, it can be shown that the feature distribution of our model's output makes sense.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper proposes a multi-scale promoted self-adjusting correlation learning network for AU detection. We propose a novel SACL method, which can efficiently utilize the AU motion and emotion representation information obtained in the different stages of the network to adaptively update the AU correlation graph, enabling more implicit correlation information to be mined with less computational effort. Additionally, we introduce a simple yet effective MSFL method, which has a boost to SACL. Finally, the features from SACL and MSFL are integrated to obtain a more robust representation for the final AU detection. Extensive experiments have demonstrated the superiority of our method on the widely used datasets, namely, the BP4D and DISFA.\n\nIn this paper, when the SACL module updates the AU correlation graph structure using the KNN algorithm, K is set to a fixed value. In future work, gradually increasing or decreasing the value of K as the network deepens is a direction worth exploring. Besides, we will further explore the interactive effects of multi-scale learning and AU correlation learning to seek the optimal solution for information fusion between the two.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a)). However,",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison between our proposed SACL method",
      "page": 2
    },
    {
      "caption": "Figure 1: (b)), such as [4],",
      "page": 2
    },
    {
      "caption": "Figure 6: ). Meantime, our method is less computationally",
      "page": 2
    },
    {
      "caption": "Figure 1: In addition, we introduce a simple yet effective",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of our method for AU detection, which mainly consists of stem network, MSFL, LP, and SACL. The face",
      "page": 4
    },
    {
      "caption": "Figure 2: It is mainly composed of four modules: stem network,",
      "page": 4
    },
    {
      "caption": "Figure 2: To extract basic features from the input face image with",
      "page": 4
    },
    {
      "caption": "Figure 2: ). After each",
      "page": 5
    },
    {
      "caption": "Figure 2: ) for landmarks predic-",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) The AU centers calculated based on the landmark",
      "page": 5
    },
    {
      "caption": "Figure 3: (b) visualizes this operation. Here,",
      "page": 5
    },
    {
      "caption": "Figure 2: Firstly, the features of obtained ROIs are flattened,",
      "page": 5
    },
    {
      "caption": "Figure 2: that SACL consists of S",
      "page": 6
    },
    {
      "caption": "Figure 2: , A′ and B are concatenated together to obtain C ∈",
      "page": 6
    },
    {
      "caption": "Figure 4: Average F1 score of removing the features from",
      "page": 8
    },
    {
      "caption": "Figure 4: It can be observed that the more features are",
      "page": 8
    },
    {
      "caption": "Figure 5: (a) Average F1 score of choosing different number of stages of SACL. (b) Average F1 score of choosing different",
      "page": 10
    },
    {
      "caption": "Figure 5: shows the F1 score results obtained",
      "page": 10
    },
    {
      "caption": "Figure 6: Visualization of AU correlation graph obtained by different methods: (a) AU correlation graph based on FACS",
      "page": 11
    },
    {
      "caption": "Figure 5: (a). When S is small, the depth of network",
      "page": 11
    },
    {
      "caption": "Figure 5: (b), when",
      "page": 11
    },
    {
      "caption": "Figure 7: t-SNE distribution of our model output. We choose",
      "page": 11
    },
    {
      "caption": "Figure 6: (a) and (b). In contrast, our SACL not only considers",
      "page": 11
    },
    {
      "caption": "Figure 6: (d). From a psychological perspective,",
      "page": 12
    },
    {
      "caption": "Figure 7: shows the t-SNE distribution of some",
      "page": 12
    },
    {
      "caption": "Figure 7: , such as ’AUs (6,7,10,12,14)’, represent that AU6, 7, 10, 12,",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 8: ), echoing the",
      "page": 2
    },
    {
      "caption": "Table 1: Summary of the representative AU detection methods in terms of AU correlation learning, region learning, using",
      "page": 3
    },
    {
      "caption": "Table 1: shows a rough summary and classification of representative",
      "page": 3
    },
    {
      "caption": "Table 2: The definitions of the AU center locations and the formula for calculating them through Landmark coordinates.",
      "page": 5
    },
    {
      "caption": "Table 2: , and the visualization",
      "page": 5
    },
    {
      "caption": "Table 2: and donates the distance",
      "page": 5
    },
    {
      "caption": "Table 3: The partition details of BP4D [26] and DISFA [27]",
      "page": 7
    },
    {
      "caption": "Table 4: The parameter settings about the model.",
      "page": 7
    },
    {
      "caption": "Table 5: shows the F1 score and accuracy results of different",
      "page": 7
    },
    {
      "caption": "Table 5: , our method still shows superior performance compared",
      "page": 7
    },
    {
      "caption": "Table 6: shows the F1 score and accuracy results of different",
      "page": 7
    },
    {
      "caption": "Table 5: F1-score and accuracy results for 12 AUs on BP4D [26]. The best results for each column are bolded. % is omitted.",
      "page": 8
    },
    {
      "caption": "Table 7: shows the F1 score results of different architecture",
      "page": 8
    },
    {
      "caption": "Table 7: Effectiveness of Data Augmentation. Because the col-",
      "page": 8
    },
    {
      "caption": "Table 7: that the introduction of data augmentation brings a 0.8%",
      "page": 8
    },
    {
      "caption": "Table 7: It can be",
      "page": 8
    },
    {
      "caption": "Table 7: Then, we take MSFL into account",
      "page": 8
    },
    {
      "caption": "Table 6: F1-score and accuracy results for 8 AUs on DISFA [27]. The best results for each column are bolded. % is omitted.",
      "page": 9
    },
    {
      "caption": "Table 7: Average F1 score of different architecture implementations. For convenience, DA represents Data Augmentation,",
      "page": 9
    },
    {
      "caption": "Table 8: Comparison of the model complexity of our model",
      "page": 9
    },
    {
      "caption": "Table 8: shows the number of parame-",
      "page": 9
    },
    {
      "caption": "Table 9: F1 score results of choosing different feature fusion mechanisms, correlation learning ways, distance metrics and",
      "page": 10
    },
    {
      "caption": "Table 9: and Figure 5 shows the F1 score results obtained",
      "page": 10
    },
    {
      "caption": "Table 9: It can be seen that the effect of directly concatenating is",
      "page": 10
    },
    {
      "caption": "Table 9: It can be observed that GCN outperforms Trans-",
      "page": 10
    },
    {
      "caption": "Table 9: It should be noted that we use 1 minus cosine similarity",
      "page": 10
    },
    {
      "caption": "Table 9: that, Max-Relative GraphConv achieves",
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Knowledge-driven self-supervised representation learning for facial action unit recognition",
      "authors": [
        "Y Chang",
        "S Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Relation modeling with graph convolutional networks for facial action unit detection",
      "authors": [
        "Z Liu",
        "J Dong",
        "C Zhang",
        "L Wang",
        "J Dang"
      ],
      "year": "2020",
      "venue": "MultiMedia Modeling: 26th International Conference, MMM 2020"
    },
    {
      "citation_id": "4",
      "title": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "authors": [
        "C Luo",
        "S Song",
        "W Xie",
        "L Shen",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition"
    },
    {
      "citation_id": "5",
      "title": "Uncertain graph neural networks for facial action unit detection",
      "authors": [
        "T Song",
        "L Chen",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "E Friesen",
        "P Ekman"
      ],
      "year": "1978",
      "venue": "Palo Alto"
    },
    {
      "citation_id": "7",
      "title": "imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "X Liu",
        "H Shi",
        "H Chen",
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "8",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "B Martinez",
        "M Valstar",
        "B Jiang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "9",
      "title": "Facial action unit recognition by exploiting their dynamic and semantic relationships",
      "authors": [
        "Y Tong",
        "W Liao",
        "Q Ji"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Capturing global semantic relationships for facial action unit recognition",
      "authors": [
        "Z Wang",
        "Y Li",
        "S Wang",
        "Q Ji"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Multiple facial action unit recognition by learning joint features and label relations",
      "authors": [
        "S Wu",
        "S Wang",
        "Q Ji"
      ],
      "year": "2016",
      "venue": "2016 23rd International Conference on Pattern Recognition (ICPR"
    },
    {
      "citation_id": "12",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "C Corneanu",
        "M Madadi",
        "S Escalera"
      ],
      "year": "2018",
      "venue": "Proceedings of the european conference on computer vision (ECCV)"
    },
    {
      "citation_id": "14",
      "title": "Hybrid message passing with performance-driven structures for facial action unit detection",
      "authors": [
        "T Song",
        "Z Cui",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Fan-trans: Online knowledge distillation for facial action unit detection",
      "authors": [
        "J Yang",
        "J Shen",
        "Y Lin",
        "Y Hristov",
        "M Pantic"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Joint patch and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "F De La Torre",
        "J Cohn",
        "H Zhang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Eac-net: Deep nets with enhancing and cropping for facial action unit detection",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu",
        "L Yin"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "20",
      "title": "Piap-df: Pixelinterested and anti person-specific facial action unit detection net with discrete feedback learning",
      "authors": [
        "Y Tang",
        "W Zeng",
        "D Zhao",
        "H Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Joint action unit localisation and intensity estimation through heatmap regression",
      "authors": [
        "E Sánchez-Lozano",
        "G Tzimiropoulos",
        "M Valstar"
      ],
      "year": "2018",
      "venue": "british machine vision conference"
    },
    {
      "citation_id": "22",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Jaa-net: joint facial action unit detection and face alignment via adaptive attention",
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "Y Wu",
        "L Ma"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "Optimizing filter size in convolutional neural networks for facial action unit recognition",
      "authors": [
        "S Han",
        "Z Meng",
        "Z Li",
        "J O'reilly",
        "J Cai",
        "X Wang",
        "Y Tong"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "27",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Exploiting semantic embedding and visual feature for facial action unit detection",
      "authors": [
        "H Yang",
        "L Yin",
        "Y Zhou",
        "Gu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "29",
      "title": "Biomechanics-guided facial action unit detection through force modeling",
      "authors": [
        "Z Cui",
        "C Kuang",
        "T Gao",
        "K Talamadupula",
        "Q Ji"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "Gated graph sequence neural networks",
      "authors": [
        "Y Li",
        "D Tarlow",
        "M Brockschmidt",
        "R Zemel"
      ],
      "year": "2015",
      "venue": "Gated graph sequence neural networks",
      "arxiv": "arXiv:1511.05493"
    },
    {
      "citation_id": "32",
      "title": "Residual gated graph convnets",
      "authors": [
        "X Bresson",
        "T Laurent"
      ],
      "year": "2017",
      "venue": "Residual gated graph convnets",
      "arxiv": "arXiv:1711.07553"
    },
    {
      "citation_id": "33",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang",
        "J Cohn",
        "Q Ji",
        "L Yin"
      ],
      "year": "2016",
      "venue": "Multimodal spontaneous emotion corpus for human behavior analysis"
    },
    {
      "citation_id": "34",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "35",
      "title": "Rest: An efficient transformer for visual recognition",
      "authors": [
        "Q Zhang",
        "Y.-B Yang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "Mpvit: Multi-path vision transformer for dense prediction",
      "authors": [
        "Y Lee",
        "J Kim",
        "J Willette",
        "S Hwang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Crossformer: A versatile vision transformer hinging on cross-scale attention",
      "authors": [
        "W Wang",
        "L Yao",
        "L Chen",
        "B Lin",
        "D Cai",
        "X He",
        "W Liu"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations, ICLR"
    },
    {
      "citation_id": "38",
      "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "authors": [
        "W Wang",
        "E Xie",
        "X Li",
        "D.-P Fan",
        "K Song",
        "D Liang",
        "T Lu",
        "P Luo",
        "L Shao"
      ],
      "year": "2021",
      "venue": "Proceedings"
    },
    {
      "citation_id": "39",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift"
    },
    {
      "citation_id": "40",
      "title": "Rectified linear units improve restricted boltzmann machines",
      "authors": [
        "V Nair",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "Rectified linear units improve restricted boltzmann machines"
    },
    {
      "citation_id": "41",
      "title": "Gaussian error linear units (gelus),\" arXiv: Learning",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus),\" arXiv: Learning"
    },
    {
      "citation_id": "42",
      "title": "Vision gnn: An image is worth graph of nodes",
      "authors": [
        "K Han",
        "Y Wang",
        "J Guo",
        "Y Tang",
        "E Wu"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Learning deep representation from coarse to fine for face alignment",
      "authors": [
        "Z Shao",
        "S Ding",
        "Y Zhao",
        "Q Zhang",
        "L Ma"
      ],
      "year": "2016",
      "venue": "arXiv: Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "Deep multi-center learning for face alignment",
      "authors": [
        "Z Shao",
        "H Zhu",
        "X Tan",
        "Y Hao",
        "L Ma"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "45",
      "title": "3d skeletal gesture recognition via sparse coding of time-warping invariant riemannian trajectories",
      "authors": [
        "X Liu",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Mul-tiMedia Modeling: 25th International Conference, MMM 2019, Thessaloniki"
    },
    {
      "citation_id": "46",
      "title": "3d skeletal gesture recognition via hidden states exploration",
      "authors": [
        "X Liu",
        "H Shi",
        "X Hong",
        "H Chen",
        "D Tao",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "47",
      "title": "Supervised descent method and its applications to face alignment",
      "authors": [
        "X Xiong",
        "F De"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "48",
      "title": "Active appearance models",
      "authors": [
        "T Cootes",
        "G Edwards",
        "C Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "49",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "50",
      "title": "A stochastic approximation method",
      "authors": [
        "H Robbins",
        "S Monro"
      ],
      "year": "1951",
      "venue": "The annals of mathematical statistics"
    },
    {
      "citation_id": "51",
      "title": "On the importance of initialization and momentum in deep learning",
      "authors": [
        "I Sutskever",
        "J Martens",
        "G Dahl",
        "G Hinton"
      ],
      "year": "2013",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "52",
      "title": "Why gradient clipping accelerates training: A theoretical justification for adaptivity",
      "authors": [
        "J Zhang",
        "T He",
        "S Sra",
        "A Jadbabaie"
      ],
      "year": "2019",
      "venue": "Why gradient clipping accelerates training: A theoretical justification for adaptivity",
      "arxiv": "arXiv:1905.11881"
    },
    {
      "citation_id": "53",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z Devito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": "Automatic differentiation in pytorch"
    },
    {
      "citation_id": "54",
      "title": "Deepgcns: Can gcns go as deep as cnns",
      "authors": [
        "G Li",
        "M Muller",
        "A Thabet",
        "B Ghanem"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "55",
      "title": "Facing imbalanced datarecommendations for the use of performance metrics",
      "authors": [
        "L Jeni",
        "J Cohn",
        "F De",
        "La Torre"
      ],
      "year": "2013",
      "venue": "2013 Humaine association conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "56",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "57",
      "title": "Dynamic graph cnn for learning on point clouds",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Z Liu",
        "S Sarma",
        "M Bronstein",
        "J Solomon"
      ],
      "year": "2019",
      "venue": "Acm Transactions On Graphics (tog)"
    },
    {
      "citation_id": "58",
      "title": "How powerful are graph neural networks?",
      "authors": [
        "K Xu",
        "W Hu",
        "J Leskovec",
        "S Jegelka"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "59",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "60",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}