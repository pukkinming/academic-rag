{
  "paper_id": "2308.02190v1",
  "title": "Emo-Dna: Emotion Decoupling And Alignment Learning For Cross-Corpus Speech Emotion Recognition",
  "published": "2023-08-04T08:15:17Z",
  "authors": [
    "Jiaxin Ye",
    "Yujie Wei",
    "Xin-Cheng Wen",
    "Chenglong Ma",
    "Zhizhong Huang",
    "Kunhong Liu",
    "Hongming Shan"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Computing methodologies â†’ Unsupervised learning",
    "Transfer learning",
    "â€¢ Information systems â†’ Sentiment analysis Unsupervised Domain Adaptation, Decoupled Representation Learning, Contrastive Learning, Speech Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-corpus speech emotion recognition (SER) seeks to generalize the ability of inferring speech emotion from a well-labeled corpus to an unlabeled one, which is a rather challenging task due to the significant discrepancy between two corpora. Existing methods, typically based on unsupervised domain adaptation (UDA), struggle to learn corpus-invariant features by global distribution alignment, but unfortunately, the resulting features are mixed with corpus-specific features or not class-discriminative. To tackle these challenges, we propose a novel Emotion Decoupling aNd Alignment learning framework (Emo-DNA) for cross-corpus SER, a novel UDA method to learn emotion-relevant corpus-invariant features. The novelties of Emo-DNA are two-fold: contrastive emotion decoupling and dual-level emotion alignment. On one hand, our contrastive emotion decoupling achieves decoupling learning via a contrastive decoupling loss to strengthen the separability of emotionrelevant features from corpus-specific ones. On the other hand, our dual-level emotion alignment introduces an adaptive threshold pseudo-labeling to select confident target samples for class-level alignment, and performs corpus-level alignment to jointly guide model for learning class-discriminative corpus-invariant features across corpora. Extensive experimental results demonstrate the superior performance of Emo-DNA over the state-of-the-art methods in several cross-corpus scenarios. Source code is available at https://github.com/Jiaxin-Ye/Emo-DNA.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) aims to automatically recognize human emotions from speech signals  [42] , which has attracted much attention in human-computer interaction (HCI), mental diagnostic tools, in-car board systems, etc  [2, 47] . However, in these practical applications, significant discrepancies exist between training and real-world corpora, arising from different languages, cultures, speakers, contents, data scales, etc. These discrepancies across corpora lead to significant idiosyncratic variations impeding the generalization of current SER technology. Therefore, SER faces enormous challenges when being applied to cross-corpus scenarios, which require a strong generalization of inferring speech emotion from a well-labeled corpus to an unlabeled one.\n\nTo solve these challenges, many researchers have explored unsupervised domain adaptation (UDA)  [14, 46, 52]  to match global distributions of source and target data, which aims to learn domaininvariant representations. Recent advances in UDA have proven effective in addressing the challenge of corpus 1  misalignment for the cross-corpus SER  [11] . One key idea to achieve corpus alignment is to reduce distribution shifts between the source and target corpus, which is commonly implemented through statistic divergence alignment and adversarial learning  [25] . Statistic divergence alignment methods  [23, 30]  utilize various divergence measures, such as maximum mean discrepancy (MMD)  [38] , to minimize domain discrepancy in a latent feature space. Lu et al.  [30]  propose a hierarchical alignment framework based on Multi-Kernel MMD (MK-MMD) criterion to learn corpus-invariant features. Instead of introducing statistic divergence measures, adversarial learning methods  [12, 14, 22]  adaptively learn a measure of divergence under the guidance of adversarial domain discriminators. Feng et al.  [12]  present a noise representation learning framework that leveraged adversarial training to retain emotion-relevant information.\n\nWhile these dominant UDA methods can effectively align the distributions across corpora at feature level and improve the generalization ability of models to some extent, they still suffer from the following two dilemmas in cross-corpus SER. 1) False alignment refers to falsely aligning features of different attributes (e.g., emotion, corpus, language, and speaker) during domain adaptation, leading to performance degradation. Due to the nonlinear manifold structures underlying data distributions  [6] , the emotionrelevant and corpus-specific features might be highly entangled in corpus-invariant feature space. Most previous efforts focus on matching global marginal distributions between source and target corpora  [31] , which fail to decouple corpus information and preserve emotion information from the aligned distribution. This may lead to the false alignment problem, i.e., aligning emotion-irrelevant features (e.g., corpus-specific ones) with emotion-relevant ones  2  . 2) Class confusion refers to ignoring class consistency when aligning data distributions of different domains, leading to performance degradation. Although matching global source and target data distributions can pull the features of source and target corpora closer, it also mixes features of different classes. This can cause class confusion since the class-discriminative information from the source corpus cannot be easily transferred to the target corpus. In other words, if the model fails to maintain class consistency during corpus alignment, it ultimately results in the erroneous mapping of target corpus features (i.e., negative transfer).\n\nIn this paper, we propose a novel Emotion Decoupling aNd Alignment learning framework (Emo-DNA) to tackle the aforementioned issues for cross-corpus SER. For Emo-DNA, we hope that feature decoupling and alignment are two key techniques to cross-corpus SER, like the complementary paired duplexes in deoxyribonucleic acid. On one hand, for contrastive emotion decoupling, we first leverage two encoders to learn emotion-relevant and corpus-specific features of each corpus, guided by source emotion labels and corpora labels, respectively. Then we propose a novel contrastive decoupling loss based on emotion-relevant and corpusspecific prototypes, which encourages decoupling emotion features from corpus-specific features by pushing emotion prototypes away from the corpus-specific ones. On the other hand, dual-level emotion alignment is based on class-level and corpus-level to achieve effective alignment. We leverage an adaptive threshold pseudolabeling to select confident samples of the target corpus, and then devise a contrastive class alignment loss based on pseudo-labeled target samples and the source samples to encourage class alignment across corpora by maximizing the mutual information of representations of the same class. We further introduce an explicit corpus alignment loss to minimize corpus discrepancy across corpora considering the significant domain shifts between different corpora. During the training process, the contrastive emotion decoupling enforces the features to be emotion-relevant for the corpus alignment, and dual-level emotion alignment encourages the features to be emotion-relevant corpus-invariant with class discrimination. Decoupling and alignment promote progressive learning with each other and enable the model to achieve robust adaptability in the target corpus, establishing new state-of-the-art performance on several benchmark datasets.\n\nThe main contributions of this paper are summarized as follows:\n\n1) We propose a novel UDA framework for cross-corpus SER, called Emo-DNA, which decouples emotion-relevant features from highly coupled distorted feature space and learns the emotion-relevant corpus-invariant features. To the best of our knowledge, Emo-DNA makes the first attempt at identifying the false alignment and class confusion problems as the key limiting factors for cross-corpus SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Unsupervised domain adaptation. The unsupervised domain adaptation (UDA) attempts to make the models generalize well from the source and target domains. Most previous works in cross-corpus SER aim to transfer the ability of inferring speech emotion from a labeled source corpus to an unlabeled target corpus. These methods can be summarized into two categories: statistic divergence alignment and adversarial learning. Statistic divergence alignment methods align distributions by minimizing domain discrepancy measurement. Recent UDA approaches minimize some measurements of corpus shift, such as Maximum Mean Discrepancy (MMD)  [23]  and multi-kernel MMD  [30] . Another predominant approach to UDA is adversarial learning, which uses a domain discriminator that distinguishes whether the features are from the source or target corpus to encourage learning corpus-invariant feature in adversarial manner  [12, 14, 46] . For example, Wen et al.  [46]  propose an adversarial training framework with the margin disparity discrepancy loss to align feature distribution. Feng et al.  [12]  present a noise representation learning architecture with adversarial training to keep emotion-relevant information. These explicit feature alignment methods fail to decouple emotion-relevant features from the highly entangled feature space and maintain class consistency, potentially resulting in false alignment and class confusion problems. Alternatively, we perform contrastive emotion decoupling and dual-level emotion alignment to alleviate these issues. Decoupled representation learning. The core idea of decoupled representation learning follows a prior that the model can learn to embed representations in a corpus-specific information space and a corpus-invariant emotion space. It allows the emotion information available for both corpora by assuming a shared corpusinvariant space. Recently, Tjandra et al.  [44]  present unsupervised",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Prediction",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pseudo-Labeling",
      "text": "Update < l a t e x i t s h a 1 _ b a s e 6 4 = \" W 7 W j n e x a 2 b a X y G 5 0 5 m H X n c m j z / c = \" > A A A B 7 n i c b V B N S 8 N A E J 3 U r x q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c K 9 g P a U D b b T b t 0 s w m 7 E 6 G E / g g v H h T x 6 u / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 z t 7 u z u 7 R 9 U D o 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P x 3 c x v P 3 F t R K I e c Z L y I K Z D J S L B K F q p 7 e Y 9 p N m 0 X 6 l 6 N W 8 O s k r 8 g l S h Q K N f + e o N E p b F X C G T 1 J i u 7 6 U Y 5 F S j Y J J P 3 V 5 m e E r Z m A 5 5\n\nx K W h c 1 / 6 p 2 + X B Z r d 8 W c Z T h B E 7 h H H y 4 h j r c Q w O a w G A M z / A K b 0 7 q v D j v z s e i t e Q U M 8 f w B 8 7 n D x 5 O j 3 E = < / l a t e x i t >       learning for speech synthesis by decoupling contents and style of speech representation. Xi et al.  [48]  propose frontend attributes decoupling for single-corpus SER. However, these methods still focus on the individual corpus tasks rather than improving generalizability across corpora. In contrast to previous approaches, we explore the decoupled representation learning for UDA, which can enhance generalizability under different adaptation scenarios. Contrastive learning. Contrastive learning  [7, 16, 17]  aims to learn generalized features by leveraging positive and negative views of samples. Unfortunately, it is challenging to apply contrastive learning directly in UDA due to the domain gap and unavailable class labels of the target domain. To address this issue, many UDA methods  [9, 18, 32]  combine contrastive learning with pseudolabeling techniques to implicitly reduce the domain discrepancy. Ma et al.  [32]  propose a contrastive-based alignment strategy on two domain-oriented feature spaces with a target pseudo-label refinement. Chen et al.  [9]  integrate self-supervised pretext tasks and UDA in a contrastive manner to improve generalizability of the conventional self-supervised frameworks. However, these implicit alignment methods do not guarantee that the aligned features via contrastive learning are only relevant to the semantic information (i.e., emotion-relevant information), leading to the false alignment. Instead, we propose the contrastive emotion decoupling to learn emotion-relevant features decoupled from corpus-specific features.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Method 3.1 Problem Definition",
      "text": "Given a source corpus D S = {(ğ‘¿ S ğ‘– , ğ‘¦ S ğ‘– )} ğ‘› S ğ‘–=1 with ğ‘› S well-labeled samples and an unlabeled target corpus D T = {ğ‘¿ T ğ‘— } ğ‘› T ğ‘—=1 with ğ‘› T unlabeled samples, where ğ‘¿ S , ğ‘¦ S and ğ‘¿ T denote the source speech signals (or features), source emotion label and target speech signals (or features), respectively, the goal of cross-corpus SER is to learn an emotion classifier ğœ‘ e that is trained on the source corpus D S with source labels and minimizes the empirical risk on the target corpus D T . That is, the cross-corpus SER task aims to generalize the ability of inferring speech emotion from the labeled source corpus to the unlabeled target corpus, which is a typical unsupervised domain adaption problem. In this paper, ğ‘¿ S and ğ‘¿ T denote the pre-processed features, and the source corpus and target corpus are characterized by probability distributions ğ‘ƒ and ğ‘„, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Overview Of Emo-Dna",
      "text": "Figure  1  presents the overview of the proposed Emo-DNA framework for cross-corpus SER. In a nutshell, Emo-DNA performs contrastive emotion decoupling to learn decoupled emotion-relevant features and the dual-level emotion alignment to further learn class-discriminative corpus-invariant features. Specifically, Emo-DNA alleviates the false alignment and class confusion problem in two ways: 1) Contrastive emotion decoupling: the emotion encoder E e and corpus encoder E c first learn emotion-relevant and corpus-specific features, respectively, and then the contrastive decoupling loss guides model to decouple emotion-relevant features from corpus-specific features by pushing emotion-relevant prototypes away from the corpus-specific prototypes. 2) Dual-level emotion alignment: the adaptive threshold pseudo-labeling first selects confident samples of the target corpus, and the contrastive class alignment loss based on pseudo-labeled target samples and the source samples encourages model to align features by maximizing the mutual information of representations sharing the same emotion. Moreover, the explicit corpus alignment loss aims to reduce domain shifts between two corpora by minimizing MK-MMD measure across corpora. In the following subsections, we describe each of the proposed components in detail.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Contrastive Emotion Decoupling",
      "text": "Feature decoupling. To transfer knowledge effectively, it is crucial for the model to learn useful emotion representations from pre-processed Mel-Frequency Cepstral Coefficients (MFCCs) features. As shown in Figure  1 , labeled samples in D S and unlabeled samples in D T are fed to the backbone E, followed by emotion encoder E e and corpus encoder E c . These two individual encoders for encoding emotion-relevant and corpus-specific features are introduced to avoid the false alignment problem caused by highly coupled features. Specifically, we first introduce a convolution neural network (CNN) based model  [14, 40]  as the backbone to extract high-level features from low-level MFCCs. For emotion encoder E e , since the emotion typically spans the short utterance and remains constant, we introduce temporal convolution network (TCN)  [3]  to effectively learn affective representations through local extraction and global integration with multi-scale receptive fields. For corpus encoder E c , considering that corpus-specific property of corpus is context-dependent  [37] , we use bi-directional long short term memory (Bi-LSTM)  [39]  as the corpus encoder E c for learning contextual information.\n\nFormally, we feed MFCCs features ğ‘¿ S to the encoders to encode emotion features ğ‘¬ S and corpus-specific features ğ‘ª S from the source corpus as follows:\n\nSimilarly, ğ‘¿ T from the target corpus is passed through the same encoders to obtain the ğ‘¬ T and ğ‘ª T . After obtaining these features, we apply the widely-used cross-entropy loss on the ğ‘¬ S for discrimination of emotion-relevant features and on the ğ‘ª = ğ‘ª S âˆª ğ‘ª T for the corpus-specific features, which are detailed as follows:\n\nwhere ğœ‘ e and ğœ‘ c are emotion and corpus classifiers with the softmax outputs, respectively, and ğ‘‘ ğ‘– denotes the binary corpus label for ğ‘–-th sample, which indicates whether ğ¶ ğ‘– come from the source corpus or from the target corpus. Contrastive decoupling loss. Since a prototype is very close to an representative observation in a feature space  [21] , to decouple emotion and corpus features, we devise two kinds of prototypes for each corpus: emotion-relevant prototype ğ’‘ S e representing the center of ğ‘¬ S from the source corpus, and corpus-specific prototype ğ’‘ S c representing the center of ğ‘ª S from the source corpus, which are defined as the mean representation from a projection head ğ‘” in a mini-batch:\n\nwhere ğ‘¬ S ğ‘– and ğ‘ª S ğ‘– are the emotion and corpus features of ğ‘–-th samples, and ğ’‘ T e and ğ’‘ T c can be obtained similarly. Note that prototypes are â„“ 2 -normalized. We then can leverage these prototypes to achieve decoupling by pushing emotion-relevant prototypes away from the corpus-specific prototypes and alignment by aggregating the emotion-relevant prototypes of different corpora, formally defined as follows:\n\nwhere (ğ‘š, ğ‘›) âˆˆ {(S, T), (T, S)}, S and T denote the source and target corpus, respectively. ğœ p is the temperature hyper-parameter of prototype-level loss, with 'â€¢' denoting the inner product.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dual-Level Emotion Alignment",
      "text": "Adaptive threshold pseudo-labeling. Since the label information of the target corpus is unknown in UDA, it is difficult to maintain class consistency during corpus alignment. Inspired by  [45] , we introduce an adaptive threshold pseudo-labeling module based on the confidence of model prediction on the target corpus to avoid class confusion while aligning corpus. Concretely, we estimate the threshold ğœ ğ‘¡ as the exponential moving average (EMA) of the confidence at each training time step ğ‘¡ (iteration) as follows:\n\nwhere ğœ† âˆˆ (0, 1) is the momentum decay of EMA, ğ‘› ğ¶ is the number of classes, and ğ’’ ğ‘— indicates the confidence (i.e., probability) of the ğ‘—-th sample in a mini-batch of the target corpus. The low threshold during initial training stages can involve more potentially correct samples. As the model attains greater confidence through ongoing learning, the threshold is adaptively increased. Finally, we obtain the most probable class of the target corpus predicted from the source network and then build a pseudo-labeled target corpus DT according to the threshold ğœ ğ‘¡ :\n\nwhere Å·T ğ‘— is the pseudo label of the ğ‘—-th sample ğ‘¿ T ğ‘— . Contrastive class alignment loss. The contrastive class alignment loss aims to align emotion features at the class level, which is based on the pseudo labels to maximize the mutual information of emotion features sharing the same label. Specifically, after getting the pseudo-labeled target corpus DT with nT samples, we treat the features ğ‘¬ S and ğ‘¬ T in DT as positive pairs if they have same emotion (pseudo) labels, and negative pairs if they have different labels. We then construct a new labeled data set H = D S âˆª DT with ğ‘› H = ğ‘› S + nT samples. We can get the projected representation ğ’› ğ‘– of ğ‘–-th feature ğ’‰ ğ‘– in H as follows:\n\nwhere ğ’› ğ‘– is â„“ 2 -normalized. Then we maximize the mutual information of representations from the positive pairs to enrich the class-discriminative representation. However, since calculating mutual information is notoriously difficult, we employs the following supervised contrastive loss (SCL)  [20]  as a proxy objective to maximize this mutual information:\n\nwhere I ğ‘– = { ğ‘— âˆˆ {1, . . . , ğ‘› ğ» } | ğ‘— â‰  ğ‘–, ğ‘¦ ğ‘— = ğ‘¦ ğ‘– } is the set of positive samples indexes to ğ‘–-th representation ğ’› ğ‘– from H , ğ‘¦ ğ‘– is the class label of ğ‘–-th sample, and ğœ s is the temperature hyper-parameter for SCL.\n\nExplicit corpus alignment loss. Due to the significant domain shifts between two corpora, it is challenging to achieve class alignment with a few confident samples. To enable more efficient class alignment and learn the corpus-invariant features, we conduct the explicit corpus alignment loss at the corpus level through multiple kernels of maximum mean discrepancies (MK-MMD)  [15] . Concretely, the MK-MMD is defined as the reproducing kernel Hillbert space (RKHS) H ğ‘˜ distance between the mean emotion representation of ğ‘¬ S and ğ‘¬ T from the D S and D T as follows:\n\nwhere ğœ™ (â€¢) denotes the kernel feature map of RKHS, and the characteristic kernel ğ‘˜ (ğ‘¬ S , ğ‘¬ T ) = âŸ¨ğœ™ (ğ‘¬ S ), ğœ™ (ğ‘¬ T )âŸ© associated with ğœ™ (â€¢) is defined as the linear combination of ğ‘š positive-definite kernels (i.e., Gaussian kernels)\n\nHere, ğ›½ ğ‘¢ is the weight. In this paper, we optimize ğ‘‘ 2 ğ‘˜ according to  [27]  in practice. Then, the dual-level emotion alignment loss L align can guide model to learn the corpus-invariant with discrimination by combining explicit corpus alignment loss ğ‘‘ 2  ğ‘˜ and contrastive class alignment loss L SCL as follows:\n\nwhere ğ›¼ 1 is the trade-off hyper-parameter balancing corpus alignment and class alignment. Note that L SCL is computed from the source corpus and pseudo-labeled target corpus.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Objective Function For Emo-Dna",
      "text": "Overall, the proposed domain adaptation framework Emo-DNA is illustrated in Algorithm 1, and the objective is defined as follows:\n\nNote that the L emotion is computed only on the source corpus and L corpus is computed on both corpora.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments 4.1 Experimental Setup",
      "text": "Datasets. To sufficiently compare the performance with stateof-the-art (SOTA) methods on the cross-corpus SER task, we conduct experiments on 6 benchmark SER corpora in different languages, including Chinese corpus CASIA  [43] , German corpus EMODB  [4] , Italian corpus EMOVO  [10] , English corpora IEMO-CAP  [5] , RAVDESS  [26] , and SAVEE  [19] . For these SER datasets available, using shared fine-grained categorical emotion labels (e.g., angry, happy, neutral, etc.) could harm the learning of generalizable representations due to insufficient  data. Therefore, we follow the previous studies  [14, 22, 35]  to exploit the available data by mapping categorial emotion to dimensional emotion to aggregate the emotion information and ensure better learning of generalizable representations. Specifically, we introduce common theoretical emotional mapping Geneva Emotion Wheel  [41] , which is used in most works regarding emotion measurement. In this way, we define the cross-corpus SER task as two binary classification tasks by mapping the categorical emotion into to dimensional emotion, as summarized in Table  1 . Preprocessing. We utilize Librosa toolbox  [33]  to extract the most commonly-used Mel-Frequency Cepstral Coefficients (MFCCs) features  [1, 49, 50]  as the inputs to Emo-DNA. We extract the first 40 coefficients to obtain the low-frequency envelope and highfrequency details in this paper. Task setup. For the cross-corpus SER tasks, 6 corpora form 30 cross-corpus SER scenarios; e.g., a scenario like CASIAâ†’EMODB indicates the model is trained with labeled corpus CASIA and unlabeled corpus EMODB, and then be tested on EMODB. Evaluation metrics. We use two widely-used metrics for the cross-corpus SER task: weighted accuracy (WAR) and F1-score (F1). WAR uses the class probabilities to balance the recall metric of different classes, and F1-score is a metric that combines precision and recall by taking their harmonic mean. Baselines. To prove the effectiveness of our method, we compare our method with the following methods: the baselines for unsupervised domain adaptation methods are DANN  [13] , DAN  [27] , JAN  [29] , CDAN  [28] , and BSP  [8] ; the cross-corpus SER baseline methods are CLSTM  [36] , ADDI  [22] , DIFL  [14] , and CAAM  [46] . Implementation details. Our experiments are performed on the PyTorch platform and each experiment is repeated 3 times to show the robustness of the methods. For the proposed method, the backbone E has 3 local feature learning blocks, each of which consists of a 1D convolution operation (with kernel size 3 and output channel size 64) followed by batch normalization, ReLU activation function, dropout (dropout rate is 0.1) and a max pooling layer. Then the emotion encoder E e is composed of 5 residual blocks of temporal convolution network  [3] . We set the kernel size 2 and replace the weight normalization with the batch normalization. We use Bi-LSTM  [39]  as the E c with 64 hidden units. The temperature hyper-parameters ğœ p and ğœ s are both set to 1e-2. The trade-off hyperparameter ğ›¼ 1 is 0.1. In addition, we adopt RAdam  [24]  to optimize the model with a learning rate 1e-3, and a mini-batch size ğ‘€ of 64. For the UDA baselines, we use the same inputs and encoder (i.e., E + E e ) to extract emotion features for a fair comparison. We reproduce all cross-corpus SER baselines following their original paper and default settings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With State-Of-The-Arts",
      "text": "Quantitative comparison. We compare the proposed Emo-DNA with current commonly used UDA methods and cross-corpus SER SOTA methods. In the experiment, we take each corpus as the source corpus and each of the other five corpora as the target corpus respectively, and we report the average performance of the five cross-corpus scenarios for each source corpus. Moreover, we show the mean and standard deviation of the results of three runs. The results are reported in Tables  2  and 3 . Overall, Emo-DNA achieves the best performance on almost all cross-corpus adaptation scenarios against the SOTA methods and gains consistent improvements on each scenario.\n\nFor arousal recognition, compared with methods that learn corpusinvariant features, Emo-DNA outperforms DANN, DAN, JAN, ADDI, DIFL, and CAAM by 13.3%, 9.5%, 9.3%, 7.2%, 7.7%, and 6.0% WAR on average, respectively. The inferior performance of these methods may be attributed to their tendency to overlook class consistency while focusing solely on aligning the distributions of the source and target corpora. Note that although DIFL additionally utilizes labels of corpus, gender and language to learn corpus-invariant features, but fails to align ambiguous target samples with correct classes due to the lack of class-discriminative information, leading to performance degradation.\n\nIn addition, compared with methods that learn class-discriminative corpus-invariant features, Emo-DNA outperforms CDAN and BSP by 7.5% and 8.3% WAR on average. We believe that they may suffer from the false alignment problem and cause a performance drop.\n\nFor valence recognition, all baselines show relatively poor performance. It indicates that corpus-invariant feature learning is more challenging to achieve for valence than for arousal, as concluded in  [14, 51] . However, Emo-DNA outperforms almost all baselines for valence recognition by alleviating the false alignment and class confusion problems.\n\nOverall, Emo-DNA achieves greater performance improvements on both arousal and valence recognition scenarios by exploiting the emotion decoupling and alignment to learn emotion-relevant corpus-invariant features. Feature visualization. To give an intuitive understanding of Emo-DNA, we visualize the features of a cross-corpus adaptation scenario CASIA â†’ EMODB through the popular uniform manifold approximation and projection (UMAP) technique  [34] . We visualize the features learned by CDAN, ADDI, and DIFL and the proposed Emo-DNA in Figure  2  for comparison. Both ADDI and DIFL can learn good corpus-invariant features but fail to capture discriminative information, suffering from class confusion problem. For instance, their target high arousal features are aligned with the low arousal features rather than with the correct ones. On the    contrary, the target features learned by Emo-DNA demonstrate higher intra-class compactness and much larger inter-class margin. These suggest that our Emo-DNA produces more emotion-relevant corpus-invariant features for the target corpus and substantiates our improvement shown in Tables  2  and 3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "Ablation study on each component. To validate the effectiveness of each component in Emo-DNA, we conduct ablation studies on these 30 cross-corpus adaptation scenarios of arousal and valence recognition. We construct the five variations of Emo-DNA for comparison. As shown in Table  4 , each result denotes the average performance over the 30 cross-corpus cases, and we can get the following conclusions: First, we only introduce the corpus encoder, whose results are reported in V2. From the comparison of Table  5 : Experimental results of applying L SCL to other UDA baselines. \"â†‘\" denotes a better result compared to the method without L SCL and \"â†“\" denotes a worse result.\n\nof mixed emotions on both source and target corpora. It suggests that L decouple (i.e., contrastive emotion decoupling) can provide decoupled emotion-relevant features for the corpus alignment to avoid the false alignment problem. (4) Figure  3 (d) shows that, combined with all the proposed components, the learned distributions achieve significant inter-class margin with high intra-class compactness. Therefore, the emotion feature decoupling and alignment exhibit mutually incremental effects and jointly enable the model to achieve robust adaptability in the target corpus.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Analysis",
      "text": "Effectiveness of contrastive class alignment. The contrastive class alignment loss L SCL is proposed for aligning emotion features at the class level, which can be easily integrated into other UDA methods as a plug-and-play approach. To assess its efficacy, we incorporate L SCL into several typical UDA baselines. As shown in Table  5 , DANN, DAN, and JAN trained with L SCL demonstrate substantial performance gains and improved generalizability. These methods concentrate on learning corpus-invariant features while overlooking class consistency, whereas the contrastive class alignment facilitates perceiving class discrimination information and achieving corpus-invariant class-discriminative features. However, we notice that CDAN and BSP with L SCL show a degraded performance on arousal recognition. This could potentially be attributed to their emphasis on learning class-discriminative information that may be disrupted by L SCL . Thus it may be more appropriate to combine L SCL with the corpus-invariant methods than those likewise for class discrimination. Overall, our proposed contrastive class alignment effectively enhances the ability of corpus-invariant UDA methods to learn class discrimination information. Effectiveness of pseudo-labeling module. The efficacy of our contrastive class alignment loss L SCL relies on the quality of the target pseudo-labels, as using noisy pseudo-labels can exacerbate class confusion issues. Table  6  shows that when directly applying L SCL without the proposed threshold (\"w/o threshold\"), performance even declines compared to Emo-DNA without the proposed loss (\"w/o L SCL \"), which indicates that a threshold is necessary for pseudo-labeling. Additionally, setting a fixed threshold of 0.9 produces worse results, as a high threshold may cause the model to disregard potentially correct samples during early training stages, and a fixed threshold is also unsuitable for flexible cross-corpus adaptation scenarios. Hence, we conclude that the adaptive threshold pseudo-labeling module enhances the robustness of Emo-DNA to class alignment.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose Emo-DNA, a novel emotion decoupling and alignment learning framework to learn emotion-relevant corpusinvariant features for cross-corpus SER. The proposed contrastive emotion decoupling module decouple emotion-relevant features from corpus-specific features in a latent distorted manifold. The proposed dual-level emotion alignment module performs corpuslevel alignment and class-level alignment for effective alignment between source and target corpora. The decoupling and alignment promote progressive learning with each other and enable the model to achieve robust adaptability in the target corpus. Experimental results demonstrate the superiority of Emo-DNA over state-of-theart methods in several adaptation scenarios across six widely-use corpora. Ablation studies and visualization further validate the advantages of the synergy between decoupling and alignment. However, there are still some limitations in the existing experimental setup. Because most of the current SER corpora lack dimensional emotion labels, it is a rough approach to map categorical emotion to dimensional one. In the future, we plan to incorporate more modal information for affective analysis and make an attempt to mitigate the gap in mapping categorical emotions into dimensional one.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overview of the proposed Emotion Decoupling aNd Alignment (Emo-DNA). We use different colours to indicate",
      "page": 3
    },
    {
      "caption": "Figure 1: presents the overview of the proposed Emo-DNA frame-",
      "page": 3
    },
    {
      "caption": "Figure 1: , labeled samples in DS and unlabeled",
      "page": 4
    },
    {
      "caption": "Figure 2: for comparison. Both ADDI and DIFL can",
      "page": 6
    },
    {
      "caption": "Figure 2: Feature visualizations by UMAP. The learned features by different SOTA methods under the adaptation scenario",
      "page": 7
    },
    {
      "caption": "Figure 3: and draw the following intuitive observations:",
      "page": 7
    },
    {
      "caption": "Figure 3: (a) shows the features learned without the guidance of",
      "page": 7
    },
    {
      "caption": "Figure 3: (c) shows that",
      "page": 7
    },
    {
      "caption": "Figure 3: The learned features with different variants of Emo-DNA under the adaptation scenario CASIAâ†’EMODB.",
      "page": 8
    },
    {
      "caption": "Figure 3: (d) shows that, com-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source": "Method",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "WAR\nF1\nWAR\nF1\nWAR\nF1\nWAR\nF1\nWAR\nF1\nWAR\nF1"
        },
        {
          "Source": "Source Only",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "50.4Â±0.7\n32.4Â±1.3\n52.3Â±1.6\n49.0Â±5.8\n64.8Â±0.3\n66.1Â±1.3\n67.5Â±0.3\n79.5Â±0.2\n62.3Â±1.2\n72.2Â±0.7\n62.8Â±0.5\n75.9Â±0.2"
        },
        {
          "Source": "DANN [13]\nDAN [27]\nJAN [29]\nCDAN [28]\nBSP [8]",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "60.7Â±5.7\n68.5Â±9.7\n48.9Â±6.9\n57.3Â±8.5\n56.9Â±1.4\n67.1Â±3.1\n68.9Â±1.2\n77.3Â±0.7\n64.1Â±0.8\n72.5Â±1.9\n66.2Â±2.5\n72.8Â±4.2\n67.2Â±0.8\n74.6Â±1.2\n58.6Â±1.7\n64.4Â±2.1\n61.4Â±5.2\n70.2Â±5.5\n66.7Â±0.8\n72.8Â±0.8\n65.4Â±1.6\n73.4Â±1.0\n64.1Â±3.4\n71.9Â±1.8\n70.0Â±2.7\n76.2Â±1.3\n53.8Â±4.4\n59.5Â±4.1\n63.3Â±3.0\n70.7Â±1.0\n68.2Â±3.2\n74.6Â±2.6\n68.0Â±3.9\n74.8Â±3.6\n62.2Â±4.5\n69.1Â±4.2\n80.0Â±0.9\n63.9Â±3.0\n75.5Â±2.3\n63.4Â±3.5\n70.8Â±7.5\n67.5Â±6.7\n73.7Â±7.2\n69.2Â±0.9\n67.5Â±4.3\n77.3Â±1.6\n69.4Â±4.1\n78.0Â±2.1\n67.2Â±1.5\n76.2Â±1.9\n58.5Â±7.7\n69.2Â±10.3\n59.1Â±2.8\n69.5Â±5.7\n69.8Â±1.3\n76.7Â±1.5\n72.6Â±1.6\n79.1Â±1.3\n69.0Â±1.9\n77.4Â±2.2"
        },
        {
          "Source": "CLSTM [36]\nADDI [22]\nDIFL [14]\nCAAM [46]",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "69.0Â±1.8\n73.1Â±2.8\n56.9Â±0.6\n50.0Â±1.1\n56.7Â±1.1\n60.0Â±2.1\n70.8Â±0.1\n79.1Â±0.3\n68.3Â±0.3\n78.1Â±0.2\n64.8Â±1.6\n75.3Â±0.8\n65.3Â±1.7\n66.4Â±2.3\n60.0Â±3.7\n56.0Â±6.0\n65.8Â±0.4\n71.2Â±1.0\n71.2Â±1.9\n77.2Â±3.0\n69.3Â±0.8\n75.7Â±0.4\n70.8Â±0.7\n77.7Â±0.3\n65.4Â±0.5\n74.5Â±1.2\n63.8Â±3.9\n66.3Â±8.1\n61.6Â±2.6\n72.8Â±4.9\n71.7Â±1.5\n79.4Â±0.6\n68.4Â±0.2\n77.5Â±0.6\n68.5Â±3.2\n76.3Â±4.2\n66.5Â±4.5\n75.3Â±2.9\n68.1Â±0.7\n73.6Â±0.3\n65.8Â±1.0\n75.3Â±0.7\n68.9Â±1.7\n79.1Â±0.5\n69.7Â±2.5\n75.2Â±2.9\n70.5Â±0.5\n79.0Â±0.3"
        },
        {
          "Source": "Emo-DNA",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "73.8Â±3.2\n79.9Â±2.1\n72.7Â±0.5\n78.2Â±0.9\n77.0Â±1.3\n82.3Â±0.9\n75.1Â±1.1\n73.5Â±0.8\n80.0Â±0.4\n73.9Â±1.2\n79.4Â±0.9\n79.8Â±1.3"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source": "Method",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "WAR\nF1\nWAR\nF1\nWAR\nF1\nWAR\nF1\nWAR\nF1\nWAR\nF1"
        },
        {
          "Source": "Source Only",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "48.6Â±1.5\n31.9Â±1.5\n51.9Â±0.4\n13.4Â±0.9\n52.6Â±1.5\n13.0Â±3.5\n55.4Â±0.3\n26.2Â±3.6\n57.0Â±0.5\n51.2Â±4.1\n47.3Â±1.1\n40.4Â±2.0"
        },
        {
          "Source": "DANN [13]\nDAN [27]\nJAN [29]\nCDAN [28]\nBSP [8]",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "53.6Â±1.0\n49.4Â±6.1\n52.5Â±4.6\n41.8Â±13.0\n53.1Â±1.7\n42.0Â±3.8\n54.9Â±1.7\n58.2Â±2.5\n53.8Â±2.2\n50.4Â±3.8\n47.2Â±1.6\n44.8Â±2.2\n54.7Â±1.7\n52.3Â±0.2\n54.6Â±0.4\n39.6Â±1.1\n56.8Â±1.2\n51.6Â±1.4\n53.8Â±1.8\n57.9Â±0.9\n56.0Â±0.5\n55.1Â±2.0\n54.3Â±1.7\n54.5Â±1.9\n57.0Â±1.0\n57.2Â±1.7\n54.2Â±1.4\n49.2Â±4.0\n54.1Â±1.7\n52.2Â±3.7\n54.6Â±0.7\n59.2Â±1.9\n58.4Â±1.6\n57.4Â±3.2\n53.9Â±1.4\n52.6Â±3.3\n63.1Â±0.8\n56.0Â±1.1\n53.9Â±1.1\n55.4Â±2.1\n62.0Â±6.6\n55.4Â±2.6\n53.1Â±9.7\n55.6Â±2.6\n57.5Â±2.9\n52.6Â±9.5\n53.9Â±0.3\n49.4Â±6.3\n52.2Â±1.5\n55.8Â±5.0\n56.2Â±1.7\n56.0Â±4.6\n52.8Â±0.5\n50.2Â±6.9\n55.3Â±2.2\n56.2Â±1.8\n56.4Â±0.9\n54.5Â±3.9\n53.9Â±1.1\n51.4Â±7.8"
        },
        {
          "Source": "CLSTM [36]\nADDI [22]\nDIFL [14]\nCAAM [46]",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "61.4Â±0.4\n57.7Â±1.6\n56.8Â±0.4\n54.2Â±0.9\n60.0Â±4.0\n51.4Â±0.4\n41.8Â±4.2\n53.8Â±0.1\n56.6Â±0.3\n57.1Â±0.7\n56.9Â±0.5\n51.3Â±5.2\n65.0Â±1.3\n57.7Â±1.3\n62.2Â±1.3\n55.3Â±0.6\n52.4Â±2.3\n51.3Â±0.5\n55.9Â±0.3\n58.9Â±0.5\n58.1Â±1.2\n56.8Â±1.8\n55.3Â±2.2\n50.0Â±5.2\n52.6Â±0.4\n60.1Â±1.5\n54.4Â±3.3\n52.3Â±8.2\n56.1Â±0.3\n50.7Â±3.5\n44.4Â±1.3\n60.8Â±1.1\n53.8Â±2.0\n56.2Â±7.3\n52.5Â±4.6\n54.9Â±2.6\n58.7Â±2.4\n55.0Â±1.5\n57.3Â±2.1\n56.6Â±1.8\n45.5Â±4.0\n54.2Â±2.0\n54.8Â±3.9\n54.2Â±1.3\n60.5Â±1.6\n50.1Â±4.4\n51.5Â±7.0\n52.3Â±2.3"
        },
        {
          "Source": "Emo-DNA",
          "CASIA\nEMODB\nEMOVO\nIEMOCAP\nRAVDESS\nSAVEE\nAverage": "60.2Â±1.0\n56.9Â±0.2\n61.1Â±1.9\n57.0Â±1.9\n59.5Â±0.4\n58.0Â±1.4\n58.1Â±0.8\n59.2Â±1.3\n53.9Â±2.3\n55.9Â±1.3\n58.8Â±0.9\n58.3Â±2.6"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "V1: Source-only",
          "Arousal\nValence\nWARavg\nF1avg\nWARavg\nF1avg": "60.0Â±0.5\n62.5Â±1.4\n52.1Â±0.0\n29.4Â±0.3"
        },
        {
          "Method": "V2: w/o Ldecouple + Lalign\nV3: w/o Ldecouple\nV4: w/o Lalign",
          "Arousal\nValence\nWARavg\nF1avg\nWARavg\nF1avg": "60.6Â±0.1\n66.0Â±0.1\n54.1Â±0.2\n49.7Â±0.2\n71.5Â±0.7\n77.7Â±0.5\n56.8Â±0.2\n55.8Â±0.3\n64.4Â±0.1\n69.8Â±0.2\n56.4Â±0.2\n56.8Â±0.2"
        },
        {
          "Method": "V5: w/o Decoupling",
          "Arousal\nValence\nWARavg\nF1avg\nWARavg\nF1avg": "65.1Â±0.2\n72.1Â±0.1\n55.1Â±0.3\n50.6Â±0.4"
        },
        {
          "Method": "Emo-DNA",
          "Arousal\nValence\nWARavg\nF1avg\nWARavg\nF1avg": "74.3Â±0.3\n79.9Â±0.4\n58.6Â±0.2\n57.5Â±0.2"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "Arousal\nValence": "WARavg\nF1avg\nWARavg\nF1avg"
        },
        {
          "Method": "w/o LSCL",
          "Arousal\nValence": "64.8Â±0.4\n70.5Â±0.7\n54.1Â±0.6\n49.3Â±0.8"
        },
        {
          "Method": "w/o threshold\nFixed threshold",
          "Arousal\nValence": "63.3Â±0.2\n72.0Â±0.3\n53.7Â±0.4\n44.7Â±0.2\n58.3Â±0.1\n67.5Â±3.2\n76.4Â±2.3\n55.5Â±0.2"
        },
        {
          "Method": "Adaptive threshold",
          "Arousal\nValence": "74.3Â±0.3\n79.9Â±0.4\n58.5Â±0.2\n57.4Â±0.1"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "LIGHT-SERNET: A Lightweight Fully Convolutional Neural Network for Speech Emotion Recognition",
      "authors": [
        "Arya Aftab",
        "Alireza Morsali",
        "Shahrokh Ghaemmaghami",
        "BenoÃ®t Champagne"
      ],
      "year": "2022",
      "venue": "ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Moataz",
        "Mohamed Ayadi",
        "Fakhri Kamel",
        "Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "3",
      "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
      "authors": [
        "J Shaojie Bai",
        "Vladlen Kolter",
        "Koltun"
      ],
      "year": "2018",
      "venue": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
      "doi": "10.48550/arXiv.1803.01271",
      "arxiv": "arXiv:1803.01271"
    },
    {
      "citation_id": "4",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes"
      ],
      "year": "2005",
      "venue": "INTERSPEECH 2005"
    },
    {
      "citation_id": "5",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "6",
      "title": "Learning Disentangled Semantic Representation for Domain Adaptation",
      "authors": [
        "Ruichu Cai",
        "Zijian Li",
        "Pengfei Wei",
        "Jie Qiao",
        "Kun Zhang",
        "Zhifeng Hao"
      ],
      "year": "2019",
      "venue": "IJCAI 2019"
    },
    {
      "citation_id": "7",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020"
    },
    {
      "citation_id": "8",
      "title": "Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation",
      "authors": [
        "Xinyang Chen",
        "Sinan Wang",
        "Mingsheng Long",
        "Jianmin Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "9",
      "title": "Transferrable Contrastive Learning for Visual Domain Adaptation",
      "authors": [
        "Yang Chen",
        "Yingwei Pan",
        "Yu Wang",
        "Ting Yao",
        "Xinmei Tian",
        "Tao Mei"
      ],
      "year": "2021",
      "venue": "MM '21: ACM Multimedia Conference, Virtual Event"
    },
    {
      "citation_id": "10",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "LREC 2014"
    },
    {
      "citation_id": "11",
      "title": "Universum Autoencoder-Based Domain Adaptation for Speech Emotion Recognition",
      "authors": [
        "Jun Deng",
        "Xinzhou Xu",
        "Zixing Zhang",
        "Sascha FrÃ¼hholz",
        "BjÃ¶rn Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "12",
      "title": "Enhancing Privacy Through Domain Adaptive Noise Injection For Speech Emotion Recognition",
      "authors": [
        "Tiantian Feng",
        "Hanieh Hashemi",
        "Murali Annavaram",
        "Shrikanth Narayanan"
      ],
      "year": "2022",
      "venue": "ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "13",
      "title": "Unsupervised Domain Adaptation by Backpropagation",
      "authors": [
        "Yaroslav Ganin",
        "Victor Lempitsky"
      ],
      "year": "2015",
      "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015"
    },
    {
      "citation_id": "14",
      "title": "Domain-Invariant Feature Learning for Cross Corpus Speech Emotion Recognition",
      "authors": [
        "Yuan Gao",
        "Shogo Okada",
        "Longbiao Wang",
        "Jiaxing Liu",
        "Jianwu Dang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "15",
      "title": "Optimal kernel choice for large-scale two-sample tests",
      "authors": [
        "Arthur Gretton",
        "Dino Sejdinovic",
        "Heiko Strathmann",
        "Sivaraman Balakrishnan",
        "Massimiliano Pontil",
        "Kenji Fukumizu",
        "Bharath Sriperumbudur"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held"
    },
    {
      "citation_id": "16",
      "title": "Bootstrap your own latent-a new approach to self-supervised learning",
      "authors": [
        "Jean-Bastien Grill",
        "Florian Strub",
        "Florent AltchÃ©",
        "Corentin Tallec",
        "Pierre Richemond",
        "Elena Buchatskaya",
        "Carl Doersch",
        "Bernardo Avila Pires",
        "Zhaohan Guo",
        "Mohammad Gheshlaghi Azar"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020"
    },
    {
      "citation_id": "18",
      "title": "Category Contrast for Unsupervised Domain Adaptation in Visual Tasks",
      "authors": [
        "Jiaxing Huang",
        "Dayan Guan",
        "Aoran Xiao",
        "Shijian Lu",
        "Ling Shao"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Surrey Audio-Visual Expressed Emotion (SAVEE) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey Audio-Visual Expressed Emotion (SAVEE) database"
    },
    {
      "citation_id": "20",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification",
      "authors": [
        "Been Kim",
        "Cynthia Rudin",
        "Julie Shah"
      ],
      "year": "1952",
      "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "22",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and crosslanguage speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Bjorn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "23",
      "title": "Cross-Corpus Speech Emotion Recognition Based on Deep Domain-Adaptive Convolutional Neural Network",
      "authors": [
        "Jiateng Liu",
        "Wenming Zheng",
        "Yuan Zong",
        "Cheng Lu",
        "Chuangao Tang"
      ],
      "year": "2020",
      "venue": "IEICE Trans. Inf. Syst. 103-D"
    },
    {
      "citation_id": "24",
      "title": "On the Variance of the Adaptive Learning Rate and Beyond",
      "authors": [
        "Liyuan Liu",
        "Haoming Jiang",
        "Pengcheng He",
        "Weizhu Chen",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Jiawei Han"
      ],
      "year": "2020",
      "venue": "8th International Conference on Learning Representations"
    },
    {
      "citation_id": "25",
      "title": "Deep Unsupervised Domain Adaptation: A Review of Recent Advances and Perspectives",
      "authors": [
        "Xiaofeng Liu",
        "Chae Yoo",
        "Fangxu Xing",
        "Hyejin Oh",
        "Georges Fakhri",
        "Je-Won Kang",
        "Jonghye Woo"
      ],
      "year": "2022",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "26",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "27",
      "title": "Learning Transferable Features with Deep Adaptation Networks",
      "authors": [
        "Mingsheng Long",
        "Yue Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2015",
      "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015"
    },
    {
      "citation_id": "28",
      "title": "Conditional Adversarial Domain Adaptation",
      "authors": [
        "Mingsheng Long",
        "Zhangjie Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Deep Transfer Learning with Joint Adaptation Networks",
      "authors": [
        "Mingsheng Long",
        "Han Zhu",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Domain Invariant Feature Learning for Speaker-Independent Speech Emotion Recognition",
      "authors": [
        "Cheng Lu",
        "Yuan Zong",
        "Wenming Zheng",
        "Yang Li",
        "Chuangao Tang",
        "Bjorn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "31",
      "title": "Taking a Closer Look at Domain Shift: Category-Level Adversaries for Semantics Consistent Domain Adaptation",
      "authors": [
        "Yawei Luo",
        "Liang Zheng",
        "Tao Guan",
        "Junqing Yu",
        "Yi Yang"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Making The Best of Both Worlds: A Domain-Oriented Transformer for Unsupervised Domain Adaptation",
      "authors": [
        "Wenxuan Ma",
        "Jinming Zhang",
        "Shuang Li",
        "Chi Liu",
        "Yulin Wang",
        "Wei Li"
      ],
      "year": "2022",
      "venue": "MM '22: The 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "Proceedings of the 14th Python in Science Conference"
    },
    {
      "citation_id": "34",
      "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
      "authors": [
        "Leland Mcinnes",
        "John Healy"
      ],
      "year": "2018",
      "venue": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
      "arxiv": "arXiv:1802.03426"
    },
    {
      "citation_id": "35",
      "title": "Cross-lingual and Multilingual Speech Emotion Recognition on English and French",
      "authors": [
        "Michael Neumann",
        "Ngoc Vu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Jack Parry",
        "Dimitri Palaz",
        "Georgia Clarke",
        "Pauline Lecomte",
        "Rebecca Mead",
        "Michael Berger",
        "Gregor Hofer"
      ],
      "year": "2019",
      "venue": "INTERSPEECH 2019, 20th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "37",
      "title": "A robust model for domain recognition of acoustic communication using Bidirectional LSTM and deep neural network",
      "authors": [
        "Sandeep Rathor",
        "Sanket Agrawal"
      ],
      "year": "2021",
      "venue": "Neural Comput. Appl"
    },
    {
      "citation_id": "38",
      "title": "Beyond Sharing Weights for Deep Domain Adaptation",
      "authors": [
        "Artem Rozantsev",
        "Pascal Mathieu Salzmann",
        "Fua"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "39",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep BiLSTM",
      "authors": [
        "Muhammad Sajjad",
        "Soonil Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "40",
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "INTERSPEECH 2017, 18th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "41",
      "title": "What are emotions? And how can they be measured?",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2005",
      "venue": "Social science information"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM"
    },
    {
      "citation_id": "43",
      "title": "Design of speech corpus for Mandarin text to speech",
      "authors": [
        "Jianhua Tao",
        "Fangzhou Liu",
        "Meng Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge"
    },
    {
      "citation_id": "44",
      "title": "Unsupervised Learning of Disentangled Speech Content and Style Representation",
      "authors": [
        "Andros Tjandra",
        "Ruoming Pang",
        "Yu Zhang",
        "Shigeki Karita"
      ],
      "year": "2021",
      "venue": "INTERSPEECH 2021"
    },
    {
      "citation_id": "45",
      "title": "FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning",
      "authors": [
        "Yidong Wang",
        "Hao Chen",
        "Qiang Heng",
        "Wenxin Hou",
        "Yue Fan",
        "Zhen Wu",
        "Marios Savvides",
        "Takahiro Shinozaki"
      ],
      "year": "2023",
      "venue": "Bhiksha Raj, and Bernt Schiele"
    },
    {
      "citation_id": "46",
      "title": "CTL-MTNet: A Novel CapsNet and Transfer Learning-Based Mixed Task Net for Single-Corpus and Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Xincheng Wen",
        "Jiaxin Ye",
        "Yan Luo",
        "Yong Xu",
        "Xuanze Wang",
        "Changli Wu",
        "Kunhong Liu"
      ],
      "year": "2022",
      "venue": "IJCAI 2022"
    },
    {
      "citation_id": "47",
      "title": "Automatic depression recognition by intelligent speech signal processing: A systematic survey",
      "authors": [
        "Pingping Wu",
        "Ruihao Wang",
        "Han Lin",
        "Fanlong Zhang",
        "Juan Tu",
        "Miao Sun"
      ],
      "year": "2023",
      "venue": "CAAI Trans. Intell. Technol"
    },
    {
      "citation_id": "48",
      "title": "Frontend Attributes Disentanglement for Speech Emotion Recognition",
      "authors": [
        "Yuxuan Xi",
        "Yan Song",
        "Li-Rong Dai",
        "Ian Mcloughlin",
        "Lin Liu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "49",
      "title": "GM-TCNet: Gated Multi-Scale Temporal Convolutional Network Using Emotion Causality for Speech Emotion Recognition",
      "authors": [
        "Jiaxin Ye",
        "Xincheng Wen",
        "Xuanze Wang",
        "Yong Xu",
        "Yan Luo",
        "Changli Wu",
        "Liyan Chen",
        "Kunhong Liu"
      ],
      "year": "2022",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "50",
      "title": "Temporal Modeling Matters: A Novel Temporal Emotional Modeling Approach for Speech Emotion Recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng Wen",
        "Yujie Wei",
        "Yong Xu",
        "Kunhong Liu",
        "Hongming Shan"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Cross-Corpus Acoustic Emotion Recognition with Multi-Task Learning: Seeking Common Ground While Preserving Differences",
      "authors": [
        "Biqiao Zhang",
        "Emily Provost",
        "Georg Essl"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "52",
      "title": "Unsupervised Domain Adaptation Integrating Transformer and Mutual Information for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Shiqing Zhang",
        "Ruixin Liu",
        "Yijiao Yang",
        "Xiaoming Zhao",
        "Jun Yu"
      ],
      "year": "2022",
      "venue": "MM '22: The 30th ACM International Conference on Multimedia"
    }
  ]
}