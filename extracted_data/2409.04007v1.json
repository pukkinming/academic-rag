{
  "paper_id": "2409.04007v1",
  "title": "Searching For Effective Preprocessing Method And Cnn-Based Architecture With Efficient Channel Attention On Speech Emotion Recognition",
  "published": "2024-09-06T03:17:25Z",
  "authors": [
    "Byunggun Kim",
    "Younghun Kwon"
  ],
  "keywords": [
    "speech emotion recognition (SER)",
    "convolutional neural network (CNN)",
    "efficient channel attention (ECA)",
    "log-Mel spectrogram",
    "data augmentation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) classifies human emotions in speech with a computer model. Recently, performance in SER has steadily increased as deep learning techniques have adapted. However, unlike many domains that use speech data, data for training in the SER model is insufficient. This causes overfitting of training of the neural network, resulting in performance degradation. In fact, successful emotion recognition requires an effective preprocessing method and a model structure that efficiently uses the number of weight parameters. In this study, we propose using eight dataset versions with different frequency-time resolutions to search for an effective emotional speech preprocessing method. We propose a 6-layer convolutional neural network (CNN) model with efficient channel attention (ECA) to pursue an efficient model structure. In particular, the well-positioned ECA blocks can improve channel feature representation with only a few parameters. With the interactive emotional dyadic motion capture (IEMOCAP) dataset, increasing the frequency resolution in preprocessing emotional speech can improve emotion recognition performance. Also, ECA after the deep convolution layer can effectively increase channel feature representation. Consequently, the best result (79.37UA 79.68WA) can be obtained, exceeding the performance of previous SER models. Furthermore, to compensate for the lack of emotional speech data, we experiment with multiple preprocessing data methods that augment trainable data preprocessed with all different settings from one sample. In the experiment, we can achieve the highest result (80.28UA 80.46WA).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH emotion recognition (SER) is the technique in which a computer can recognize the inherent emotional features of a human's speech signal  [1] . In particular, interest in the human-computer interaction (HCI) systems has arisen  [2] , and SER has noticed that some applications, such as psychotherapy  [3]  and consultation calls  [4] , require emotional labor. However, it is hard to understand how human speech emotions can be represented in terms of the exact values using common standards. This is because each person's method of recognizing emotions differs depending on their personality and culture  [5] . The ambiguity of human emotions is one of the main challenges in developing an accurate SER model  [6] .\n\nTherefore, many studies have attempted to use the deep learning-based models  [7] -  [11] , that can be trained directly from the emotional speech data. Among them, in recent, the convolutional neural network (CNN) based models trained with speech spectral features are proposed  [12] -  [18] . A CNNbased model has two advantages. First, owing to the convolutional layer's weight sharing, a relatively small number of trainable parameters can be used. Second, a deep CNN layer model makes it possible to learn global context features using filters of only a small size  [19] -  [21] . For the SER, it is essential to learn the linguistic features and the paralinguistic features of speech  [22] . Therefore, the CNN model, which can learn the context of emotional speech utterances, performs better than the other structures.\n\nHowever, to learn the overall context of the input data with a CNN-based model, a sufficient number of deep layers must be stacked, or a larger filter kernel must be used. Therefore, attention modules have been proposed to cover the CNN layer's weakness for the SER  [23] -  [28] . Xu et al.  [25]  proposed a multiscale area attention, which applies the transformer-type attention mechanism  [29]  to the CNNbased model. This significantly improves the recognition performance by dividing the time-frequency spatial information into granular perspectives. Guo et al.  [27]  proposed spectral temporal channel attention, which is a modified version of bottleneck attention module (BAM)  [30] -  [32] . Therefore, it used not only focus on spatial features but also attention to channel features. In addition, it has an independent attention learning structure in all the axes of the input features. However, channel attention requires more learning parameters than spatial attention because of the two multi-layer perceptron (MLP) layers.\n\nMore trainable parameters are required when examining the attention structure and considering the more diverse aggregated input features  [33] . However, an increase in trainable parameters causes overfitting problems when trainable samples are leaked, such as in SER  [34] . Therefore, in this study, we search for an efficient attention structure that can improve emotional feature expression with only a few learning parameters while maintaining a deep CNN-based model. Through experiments, we observe that CNN's channel features are essential for emotion classification performance. Therefore, the efficient channel attention (ECA)  [35]  that can learn how to focus on the important channel features is applied to the SER problem for the first time. With the interactive emotional dyadic motion capture (IEMOCAP) corpus  [36] , we experiment to look for methods to use the ECA with a CNNbased model that is more suitable. The ECA can find important channel features by focusing on the relationship between neighboring channels for the emotion classification. To achieve this, the ECA uses a 1-D convolution layer. Therefore, it is highly efficient because it requires only a few trainable parameters equal to the kernel size  [37] .\n\nWe also search for a more appropriate preprocessing method to better represent the emotional features. Previous studies have preprocessed emotional speech signals using different methods; therefore, it is difficult to compare the results. Therefore, we prepare the eight different types of preprocessed datasets. Specifically, we choose the log-Mel spectrogram preprocessing method first. We preprocess the speech signals with different and overlapping window sizes using shortterm Fourier transformation (STFT). We also evaluate the emotion classification performances of various CNN-based models. As a result, we can observe that a preprocessing method with a large window size can accurately represent the emotional features. Fig.  1  shows the overall pipeline used in the experiments.\n\nIn summary, our contributions to this paper are below:\n\n1) Several experiments have shown that a CNN's channel complexity significantly affects the SER. Therefore, the ECA was applied to the SER domain for the first time.\n\nThe ECA efficiently improved the emotion classification results with only a few training parameters by extracting the relationship between the features of the neighboring channel. It was shown, in particular, that using ECA is effective in the deep layer of the CNN model, which has many channels. 2) We conducted experiments using eight datasets preprocessed under various settings to determine the most effective preprocessing method for accurately expressing emotional features. The precision of our results, a testament to the reliability of our experiment's outcomes, was evident. Training with a log-Mel spectrogram, rep-resenting a relatively high-frequency resolution, proved significantly more effective in emotion classification. 3) In the field of SER, the need to learn data poses a significant challenge. To address this, we proposed STFT data augmentation, which uses various preprocessed settings in STFT to supplement the expression of emotional features. Our proposed STFT data augmentation had a profound impact, resulting in a substantial enhancement in emotion classification performance, demonstrating the efficacy of this approach. When using a CNNbased model with ECA applied, we achieved the highest performance to date (80.28UA 80.46WA 80.37ACC), inspiring confidence in this method's potential. This paper's overall composition is as follows: Section II describes the several CNN-based models with attention modules proposed for SER. Section III and Section IV introduce our proposed method. Section V presents the details of the experimental settings and evaluation results when the ECA is applied to our CNN-based model. Finally, Section VI concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Many different attention methods have been proposed. In this section, we present an overview of the development flow of CNN-based models using several attention methods. We divide contents whether the recurrent neural network (RNN) is used or not in the CNN-based model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Cnn-Rnn Models With Attention Mechanism",
      "text": "The CNN can effectively learn local spatial features from the data. It is also possible to learn the global spatial features, such as the context of the data when stacking multiple CNNs. However, if the model stacks more layers, its complexity increases. In the SER problem, increasing the model complexity is critical. Therefore, to train the emotional context from the speech signals while sustaining the model complexity, most of studies have proposed a combination model a CNN and RNN  [12] -  [14] . Although it is possible to learn the temporal features of speech using RNN, there are limitations to learning long sequences. Therefore, to compensate for the limitations, many models that combine attention mechanisms with RNN have been proposed.\n\nM. Chen et al.  [15]  proposed a CNN-LSTM-Attention model to aggregate the hidden states in each time step. This enables effective learning even for long sequences. In addition, for more comprehensive context learning, ADRNN  [16] , which uses residual connections and dilated convolution layers together, and ASRNN  [17] , which compensates for the shortcomings of RNN through a sliding RNN method, have been proposed.\n\nHowever, models using CNN-RNN-Attention layers forcefully learn the spatio-temporal features together. However, models consisting of CNN and RNN models in parallel are proposed to independently learn spatio-temporal features  [38] . Zhao et al.  [39]  proposed a structure that separates LSTM and CNN in parallel and applies independent attention. Furthermore, Z. Chen et al.  [28]  proposed AMSnet, which is a parallel model that effectively synthesizes features through a connection attention mechanism.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Cnn-Based Models With Attention Mechanism",
      "text": "Recently, there has been a trend to use only CNN and attention layers without an RNN. Because RNN requires many more computational and training parameters than others, they focus on developing attention mechanism methods that help learn the context of the spatial features of speech spectrogram. Xu et al.  [23]  demonstrated effective emotional feature learning only through self-attention after the CNN layer for the spatial context learning.\n\nAnother attention mechanism method was proposed to enhance the feature learning of the CNN layers. Li et al.  [24]  demonstrated the importance of frequency features that use frequential attention, in addition to spatial attention. Xu et al.  [26]  proposed the ATDA, which applied independent self-attention to all feature axes (temporal, frequential, and channel) to compensate for the weakness of temporal feature learning owing to the lack of an RNN. Guo et al.  [27]  proposed STC, which is a more efficient attention method for all feature axes of the CNN structure.\n\nWe further explore this trend and attempt to find an attention structure that is efficient and effective in learning emotional features while using a deep-layer CNN structure. In particular, for efficient attention, we focused on learning the channel features that contained the context information of the input data in the CNN layer. Therefore, we applied the ECA module to the SER problem and obtained improved emotion classification performance than before.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Preprocessing Method",
      "text": "In this section, we explain the preprocessing method used to extract crucial emotional speech signal features. Speech preprocessing suitable for a specific purpose is necessary to effectively learn a neural network model that may provide better performance. It is not yet known which speech preprocessing method is the best for emotion recognition in speech.  Therefore, we need to determine which speech preprocessing method is the best way to recognize the emotion of speech. For this purpose, we selected the log-Mel Spectrogram, which is frequently used for speech recognition. Then, we check the suitable windowing and overlap times in the log-Mel Spectrogram to obtain the best result for the emotional recognition of speech.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Stft",
      "text": "The STFT is a method used to obtain the feature in frequency features by dividing a signal into short time periods. Even though STFT is used in speech processing, the setting in STFT that is the most suitable for the emotional recognition of speech has yet to be discovered. Therefore, we want to determine the value of the best setting when a neural network based on a CNN is applied to speech emotion recognition. The STFT can be described as follows:\n\nThe output X t (f ) is obtained by applying a windowing function (w) to a signal x(τ ) in an interval of the windowing function. The output X t (f ) then becomes a feature of the frequency. The windowing function moves according to stride (s), which is determined by the windowing length (l) and overlapping length (o).\n\nNote that the output X t (f ) has a resolution limit. If the windowing length is longer, the frequency resolution increases; however, the resolution in time decreases. If the windowing length is shorter, the frequency resolution decreases, however, the time resolution increases. Therefore, we need to determine which features are more important in terms of time or frequency. For this purpose, we performed our experiment by using eight different settings during preprocessing.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Speech Emotion Discrimination With Log-Mel Spectrogram",
      "text": "Because the amount of data for emotion recognition in speech is limited, it is advantageous for the size of the features to become small. The log-Mel spectrogram is effective for emotion recognition in speech because it can reduce the size of the features expressed in frequency. In addition, the log-Mel spectrogram displays speech characteristics in 2D images.\n\nFig.  2  shows an example of a log-Mel spectrogram for each emotion class. Fig.  2  (a) and Fig.  2 (c ) show the characteristics of speech in sadness and angry emotions. In the case of sadness, the utterance time was short, and the speech was distributed in the low-frequency regions. Meanwhile, some parts of the high-frequency region were observed in the speech of angry.\n\nHowever, recognizing the differences between different emotions is challenging. In addition, some images are ambiguous when characterizing the corresponding emotions. Therefore, a deep neural network model based on a CNN must be introduced to recognize the emotions of speech. In this study, we suggest the structure of a CNN model based on efficient channel attention, which is effective and efficient in emotion recognition of speech.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Model Architecture",
      "text": "Because there is little training data for speech-emotional recognition, we need to use as few parameters as possible to improve the successful recognition of emotions in speech. Therefore, we consider a neural network model for images based on a CNN. In addition, to effectively learn speech emotions in context, we set up a model to elevate the learning ability in the channel. Therefore, we apply efficient channel attention (ECA) to our neural network model-based CNN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Convolution Layer",
      "text": "The Convolution layer learns the input features included in the local region by using various filters. The 2D Convolution layer learns the spatial information of the 2D images. When a 2D convolution layer is applied to a spectrogram for recognition for recognizing speech emotions, it can learn the relationship between time and frequency.\n\nEquations (  3 ) and (4) show the method for calculating the convolution when a spectrogram is used as the input. The input X l ∈ R T ×F ×C is convoluted with weight filter\n\nto transform the C ′ size of channel dimension. In this calculation, the weight filters are used with kernel size (K ×K) of the local spatial information.\n\nWe must set an appropriate number of the weight filters and kernel sizes to better train with this convolution layer. Therefore, in this study, we explored the trainable parametric efficiency in a CNN-based model for speech emotion recognition. We mainly focus on some weight filters called \"channel size\". The channel size in the convolution layer is a hyperparameter related to the information capacity of the neural network. If we use many weight filters, spatial information can be sensitively extracted from the input data. However, this leads to a more robust overfitting of the training dataset. We design a 6-layer CNN architecture to determine an adequate channel size for speech emotion recognition. This is explained in detail in the following section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Cnn-Based Architecture",
      "text": "We proposed a deep CNN-based model as a baseline for developing an effective SER model. The structure of the CNNbased model was based on a previously proposed model for SER. Using this model, we focused on CNN's channel features of the CNN, which effectively trained the speech emotion features. Fig.  3  shows the overall architecture of the CNNbased model. It is composed of six Convolutional blocks and The detailed model structure is as follows. First, we chose a convolution block, which is commonly used in image classification models. The convolution block consists of three layers: convolution layers with (3 × 3) kernel size, batch normalization, and ReLU activation. To find the adequate channel size in each convolution layer, we selected the initial channel size in multiples of 16. We multiplied by a scaling factor to scale up the channel size. The details of the parameter settings are listed in Table  I .\n\nNext, we used the average pooling layer after all the convolution blocks. It can efficiently subsample the hidden features without vanishing the features. Exceptionally, in the sixth pooling layer, we used global average pooling to connect with the next layer. Finally, two fully connected layers were used in the classification layer. The weight of each fully connected layer was set to be the same as the output channel size of the last convolution layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. The Design Of Eca Module",
      "text": "The convolution layer can extract the local spatial features using the number of trainable filters from the input data. However, when more filters were used in the model, the representation capacity of the filters weakened. To overcome this situation, we adopt the ECA  [35]  in the CNN-based model, which can effectively improve the representation of the filters. The ECA is a channel attention architecture proposed by Wang et al.. Learning the relationship between different filters and focusing on the important ones is possible with fewer trainable parameters. We experimented with the application of an ECA suitable for a CNN-based SER model. As a result, increasing the filter's representation in the convolution layer helps to extract emotional features.\n\nThe ECA is a type of layer that applies a self-attention mechanism  [29] . The self-attention mechanism comprises three components: query (Q), key (K), and value (V ). These components are used to attend to the crucial information from input features. Each component's role is as follows. A query is a \"question\" about what is essential in the input features. The key is \"hint,\" which uses how similar the query is. This helps to find the most appropriate information in the input features.\n\nThe value is the \"real answer\" that exists in pairs with keys and is used as the output features of self-attention.\n\nThe most common self-attention mechanism in (  5 ) is the inner-product attention. In the first step, each query is multiplied with the multiple keys to obtain the relevant score matrix. In the second step, the score matrix is represented by a probability distribution within each query using the Softmax function. In the final step, the output is represented by a linear combination of all the values with a probability distribution. In summary, the output is the weighted value obtained that is most closely related to the query indirectly through the keys.\n\nECA is simplified by omitting the key from the existing self-attention structure. Equation (  6 ) shows the progress of the ECA. The most significant difference from self-attention in (  5 ) is that there is no key; however, the importance of the value is judged by the score learned from the query itself. This simplified attention mechanism is suitable for application in the SER.\n\nThe ECA structure is shown in Fig.  4 . The ECA structure can be divided into three steps. The first involves the preparation of a query. The query represents the channel features in each input. We used global average pooling, which can contain temporal-frequency features in each channel without trainable parameters. Please refer  (7) .\n\nThe next step is to make the score that represents the importance of each channel feature. We trained the channel feature's relation with a 1-D convolution layer to obtain the score. Specifically, the number of neighboring channel queries that train the relation with the target channel query is determined by the kernel size of the 1-D convolution layer.\n\nMoreover, with the sigmoid function, we score within 0 to 1. Next, a key difference from the original ECA is the omission of batch normalization. This change is made because batch normalization tends to extract global features of speech data rather than individual emotional features.\n\nThe final step is the re-representation of the input features. An element-wise product is performed on the input features and the attention channel score.\n\nECA can efficiently improve the channel feature of the convolution layer, which is essential for classifying speech emotions. Fig.  5  shows the ECA block used after the convolution block. We searched for more appropriate ECA block settings for the CNN-based model. To achieve this, we compared the different experiments using several kernel sizes and block positions in the model. Consequently, unlike the original ECA method used in all layers after the convolution block, using some convolution blocks with many channel features can be effective for emotion recognition performance. This shows that the ECA works well when the complexity of the filter increases. In particular, the effectiveness of ECA can be increased when augmentation data are used for training. Typically, sufficient data are required to use an attention-structured neural network effectively. Therefore, we used eight different datasets as the augmentation data, as listed in Table  II . Consequently, the performance difference between the model combined with the ECA and the model without the ECA is visible.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Weighted Focal Loss",
      "text": "Most speech emotion datasets have an unbalanced distribution, depending on the emotion labels. Therefore, when we use cross entropy as a loss function, the model can be focused on a relatively large number of specific emotion labels (neutral and happiness). In addition, emotion classification is complex, depending on the label. For example, \"happiness\" and \"angry\" are often misclassified. Therefore, we used weighted focal loss to deal with those specific situations. The weighted focal loss can be expressed as follows.\n\nIt has two properties. First, the focal loss  [40]  achieves flexible learning rates in different emotional classes (n class ). The focal loss function is a slightly more generalized function of the cross entropy weighted by the predicted probability of each emotion class (p i ). For emotion classes with low probability values, the loss increases. This causes relatively more learning in backward updates. Conversely, emotion classes   [41]  log-Mel 46 ms 23 ms STC  [27]  spectrogram 16 ms 8 ms HNSD  [11]  log-Mel 25 ms 10 ms TIM  [42]  MFCC 50 ms 38.5 ms AMSNet  [28]  spectrogram 50 ms 25 ms\n\nwith high probability values resulted in relatively less learning. The hyperparameter gamma (γ) controls how dramatically the loss value changes. In our experiment, we set γ = 1.\n\nSecond, to avoid imbalanced learning owing to the number of emotion classes, the loss function is multiplied by the learning rate weight (w i ) based on the number of labels in the training dataset. The learning rate weight was obtained as the reciprocal ratio of the number of data for each label.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Experiment Setting And Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Dataset (Iemocap)",
      "text": "The IEMOCAP  [36]  is the most popular dataset used in the SER problem. In this dataset, ten individual actors (five men and five women) recorded their voices, facial movements, and overall behavior for five sessions to understand human emotions in various situations. The IEMOCAP's data are divided into an improvised set, which contains improvised acting, and a script, which acts through dialogue. Three or more annotators manually classified the emotional speeches, and the final decision was made through a majority vote.\n\nThe data samples that we used in the experiment are as follows. We selected only the Improvised set, considering situations in which human emotions can appear naturally. Next, we choose five emotional classes corresponding to \"angry\", \"sadness\", \"happiness\", \"neutral\" and \"excited\". These classes are commonly used in various experiments. Moreover, to compensate for the data sample's number of \"happiness,\" \"excited\" is considered as \"happiness\". The experiments were conducted using 2943 speech data samples containing four emotional classes (angry:289, sadness:608, happiness:947, neutral:1099).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Data Preprocessing",
      "text": "To efficiently represent the input data, we extracted the log-Mel spectrogram features from the speech signal. The data preprocessing steps are as follows.\n\nFirst, signal segmentation is performed to equalize the length of the input data. The IEMOCAP dataset speech samples had an average length of 4.5 s and varied from short (∼ 0.5s) to long (∼ 30s) speech. Therefore, the lengths of the speech samples were consistent at 6 s, which was slightly longer than the average. Specifically, if the speech data were shorter than 6 s, zero padding was performed at the beginning and end of the signal with the same length for the signal position in the center. If the speech was longer than 6 s, both the beginning and ends of the signal were cut to the same size to contain as long an utterance as possible.\n\nNext, we prepared the different versions of the datasets to search for more effective preprocessing settings with different window sizes and overlaps in the STFT. Therefore, an interval was set based on previous studies. As listed in Table  III , most previous studies set the window size from 16 ms to 50 ms. Based on this, we chose eight different window sizes at 5ms intervals within a slightly wider range of 15 ms to 50 ms. The overlap size was adjusted to obtain the same size of input data.\n\nIn the final step, log-Mel filters were applied to effectively decrease the input data size Specifically, we use 64-number Mel filters to increase frequency resolution. The detailed settings for each dataset version of the dataset are listed in Table  II . All preprocessing steps were conducted using MATLAB.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Experimental Setup And Evaluation",
      "text": "We use 5-fold cross-validation for the entire dataset to ensure general SER performance. Samples from all the sets were randomly selected. To evaluate the model performances, we used the unweighted average accuracy (UA) and weighted average accuracy (WA). These two metrics are commonly used in the SER. Additionally, to analyze the balanced evaluation, we used the mean of UA and WA (ACC) values.\n\nThe PyTorch framework  [43] , a deep learning framework, was implemented in all the experiments in this study. The specific hyperparameters of the models were as follows: All weight parameters were initialized with He initialization  [44] . For the model's optimization, we use the Adam optimizer  [45]  with 10 -4 initial learning rates and 10 -6 decay rates. The batch size was set to 32, and the focal loss parameter was set to 1. Finally, the models were trained for 150 epochs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Searching The Proper Channel Size For Cnn-Based Model Architecture",
      "text": "To extract emotional features from speech data more effectively, we first experimented with the number of channel sizes used in a CNN-based model. The channel size is the most critical hyperparameter in the convolution layer. Also, the proper channel size is crucial to efficiently reduce the number of trainable weight parameters. The optimal number of channels can improve the classification performance of the SER model.\n\nFor this experiment, we individually trained and evaluated the eight different CNN-based models with variant channel sizes. The detailed number of channel sizes is set with the parameter n, as listed in Table  I . Parameter n was selected as an integer ranging from 1 to 8. In addition, we used eight different datasets to analyze which version of the preprocessing methods best represents the emotional features. The preprocessing methods are listed in Table  II .\n\nFig.  6  shows the performance of the entire model for the different versions of datasets. From the perspective of channel size change, the model performance changes drastically as the channel size increases in range from 1 to 3. In particular, when comparing the performance of the n = 1 (blue line) and n = 3 (purple line) models, there was a 2% ∼ 3% gap.\n\nNext, the best result was obtained when we used the n = 4 (khaki line) model with the dataset of version 6 (79.16 UA 79.27 WA 79.22 ACC). In addition, the model with n = 4 (khaki line) shows the best results for datasets of versions 2, 6, and 8. This shows that an appropriate channel size can lead to a decent performance in the emotion recognition possible. However, no improvement was observed in the models using a larger channel size (4 ≤ n ≤ 8). Therefore, a larger channel size can be inefficient for training.\n\nIn experiments with different versions of datasets, except n = 1, the best performance of each model can be observed in the higher versions of the datasets (5 ∼ 8). This implies that a larger window size can effectively represent emotional features.\n\nIn summary, the channel size is an important factor in the performance of a CNN-based model. Therefore, in the next experiments, we explored how to improve the training channel features using the ECA.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Using Original Eca Method In Cnn-Based Models",
      "text": "To efficiently increase the channel feature representation, we use the ECA blocks in a CNN-based model. We first experimented with the original ECA block. For the experiments, we selected the CNN-based model (n = 4) that achieved the best performance in a channel size search experiment in section V-D. Moreover, the original ECA blocks in this model were applied. The original ECA is positioned after each convolution layer. Therefore six ECA blocks were added to the CNN-based model. Next, the ECA's kernel sizes (k) were set based on the channel size of each layer. Fig.  7  shows the performance of each dataset before and after applying ECA. Compared with the CNN-based model,  the original ECA method (red line) showed an overall decrease in emotion recognition performance from 0.1% to 1%. This implies that the original ECA method is not suitable for a direct application. Therefore, we experimented to determine a more appropriate way to use the ECA block.\n\nThe differences between the original ECA and our ECA methods are as follows: First, we applied the ECA blocks after the 5th and 6th convolution layers, which were relatively deeper than the other layers. In addition, a larger kernel size (k = 7) was used compared to the original kernel size (k = 3). By applying our ECA, the number of trainable parameters increased by only 14.\n\nConsequently, unlike the original ECA method, the proposed ECA method (purple line) obtained a 0.3% higher performance than that without an ECA block. In particular, the best performance (79.37 UA 79.68 WA 79.53 ACC) was obtained when we trained with dataset version 8. This result indicates that the ECA method can improve performance using SER properly. The channel features of deeper layers are particularly important for extracting the speech emotion features.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "F. Searching The Proper Eca Block Usage With Different Version Of Datasets",
      "text": "To effectively use ECA blocks, we experimented with how the model performance changes when using the ECA blocks in different positions on a CNN-based model with different kernel sizes (k). A CNN-based model has a structure in which the channel size increases as the layer deepens; therefore, the complexity of the channel features in deep layers is relatively high. Based on that situation, we want to determine where the ECA block can be more effective in helping to train the channel features. Specifically, the experiment was conducted by sequentially adding the ECA blocks starting from the sixth convolution layer, which was the deepest layer in the model.\n\nSubsequently, we changed the kernel size of the 1-d convolution layer of the ECA block. The kernel size is the length of the local region in which the relationships between neighboring channel features are learned. In our experiments, four different kernel sizes (3, 5, 7, and 9) were used to verify the change in performance according to the kernel size. Moreover, we excluded the cases in which the kernel size was larger than nine because of poor performance. However, to determine the best kernel size, we must train (4 + 1) 6 times to consider all case sizes according to the ECA block. Therefore, we skipped some cases of the experiments owing to limitations in computational resources. In all the experiments, we used the dataset of version 8.\n\nFig.  8  shows that the performance decreases when ECA blocks are used after most convolution layers, whereas the performance improves when ECA blocks are used for relatively deep layers (3 to 6 layers). In particular, when ECA blocks are used on the fifth and sixth floors, the highest performance of the model without the ECA block (orange line, 79.16 UA 79.27 WA 79.22 ACC) was improved by approximately 0.3% based on the ACC. In addition, it shows high performance in many cases compared to the best result when the original ECA blocks are used (pink line, 78.71 UA 78.80 WA 78.76 ACC). These results show that the ECA block positioned in the deep-layer channel features was effective.\n\nNext, if the results are analyzed according to the kernel size, the kernel size that shows the best performance is k = 7 (purple line). Compared with the other kernel sizes (3, 5, and 9), k = 7 had an overall high accuracy range (78.59 ∼ 79.53 ACC). However, k = 7 did not always exhibit the best performance in any of the cases. For example, the four ECA blocks used in model k = 9 (khaki line) performed better than the others (1, 3, and 5). In the next experiment, we examined the effect of the ECA block using eight different emotional speech preprocessing datasets. For this purpose, a CNN-based model with the ECA blocks on the fifth and sixth convolution layers was used, which were the cases with the highest performance in the previous experiment. In addition, four kernel sizes (3, 5, 7, and 9) were used.\n\nAs shown in Fig.  9 , the model performance tended to increase from dataset versions 1 to 8 In particular, as in the previous experiment, the version 8 dataset showed better results than the other version datasets in most cases. This indicates that a large-sized window in emotional speech preprocessing is effective. However, for most dataset versions, the performance was lower than that of CNN-based models alone. Eventually, it will be essential to train the model using fined emotional features to obtain an effect from the attention layer.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "G. Augmentation Method With Different Version Of Stft Datasets",
      "text": "To overcome the limitations of representing speech emotional features obtained using only one preprocessing method, multiple preprocessing data augmentation experiments were performed. For this purpose, only the training dataset was added from the eight different versions of the datasets obtained by setting listed in Table  I . Because each of the eight preprocessing methods has a different window size and overlap size, the model can train with richer emotional features.\n\nWe conducted two different experiments depending on the dataset selection methods to determine out the effect of multiple preprocessing data augmentation on SER. In the first case, we selected version 1 as the test set and collected training data samples from version 2 to version 8 in ascending order. Second, in contrast to the first, we selected dataset version 8 as the test set and collected the training data samples in descending order from version 7 to version 1. The models used in the experiment are CNN-based models with ECA blocks and models without an ECA block. Fig.  10  shows the augmentation experiments in ascending order. In most cases, the results were higher than those in the cases where the augmentation method was not applied to either model. Specifically, the best results in the CNNbased model were obtained using all the preprocessing datasets",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model Name",
      "text": "Evaluation Results STC  [27]  59.1 UA 60.5 WA AMSNet  [28]  70.5 UA 69.2 WA MHA  [41]  70.1 UA 76.4 WA HNSD  [11]  72.5 UA 70.5 WA ATDA  [26]  75. were obtained when versions 1 and 2 were used. These results show that the multiple preprocessing augmentation method can improve performance when a small amount of data is available. Fig.  11  shows the augmentation experiments in descending order. An interesting result shown in Fig.  11  is that the CNN model (79.66 UA 80.50 WA 80.08 ACC) and the ECA CNN (80.28 UA 80.46 WA 80.37 ACC) model exhibited the highest performance. In particular, the CNN-based model with ECA blocks showed 0.8% higher based on ACC than the ascending experimental result. In addition, the CNN-based model with ECA blocks (red line) is usually 0.2% to 0.82% higher than that of the CNN model (blue line). In other words, the multiple preprocessing augmentation method can significantly improve the learning of emotional features using ECA blocks.\n\nFrom these two experiments, we can observe that the multiple preprocessing augmentation method can compensate for the training problem with a few speech-emotional data samples, which is one of the difficulties in SER. In addition, the ECA blocks can work more effectively using the data augmentation method. This method achieved the highest performance (80.28 UA 80.46 WA 80.37 ACC).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "H. Comparison With Other Attention Models",
      "text": "Next, we compare with other proposed models' performance that used attention methods. For this, we chose our best results models that contained ECA blocks and STFT data augmentation. Table IV lists the results of the UA and WA evaluations. All models compared with our method use spectrogram data and attention methods used for feature aggregation or extraction in the independent axis of the data. As listed in Table  IV , the proposed model shows a significantly better performance than the other models. this is because the combination of deep CNN layers and ECA is an efficient structure for extracting emotional context. In addition, the insufficient expressions of emotional features can be reinforced using the STFT augmentation method. Therefore, for the SER, it is important to increase the representation of the emotional feature data and effectively learn the context within it.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "I. Analysis Of The Ablation Studies",
      "text": "For a detailed analysis of the proposed methods, the results of the ablation models were compared. Table V lists performance depending on whether or not the ECA block and STFT data augmentation were used. From the overall results, we can observe that the emotion classification performance is improved when applying our proposed ECA block. In particular, the effect of the ECA block can significantly improve performance when used together with the STFT data augmentation method. Subsequently, we compared the classification performance for each emotion according to the ablation models. Fig.  12  and Fig.  13  show the confusion matrices of the ablation models. As shown in Fig.  12  and Fig.  13 , you can see that in most models, the classification performance of angry and sadness was high; however, the classification performance for angry, happiness, and neutral tended to be low. Compared with Fig.  12 (a), Fig.  13(b)  shows that the classification performance of all emotions improved when the ECA and STFT augmentation methods were used.\n\nSpecifically, in Fig.  12 (a) and (b), the happiness classification accuracy improved by 3.5%, and the neutral classification accuracy decreased by 2% using the ECA. It means that the ambiguity of classifying happiness and neutral was resolved through the ECA block. As shown in Fig.  13 , the neutral classification accuracy improved by approximately 2% ∼ 4%. In particular, as shown in Fig.  13 (b), both angry and neutral classification accuracies were significantly improved.\n\nThe results show that channel feature extraction with the ECA blocks is effective for SER. Subsequently, to understand how the ECA block works for classifying each emotion, we checked the channel weights for each emotion. Fig.  14  shows a plot of the channel weight of the ECA blocks learned in the 5th and 6th layers when using the STFT augmentation method. To plot the channel weights, the weights from the test set were averaged.\n\nAs shown in Fig.  14 (a), the ECA weights of the fifth layer are not related to any emotion. However, as shown in Fig.  14(b ), the ECA weights of the last layer are noticeably different. In particular, channel weights were distinguishable between angry (blue line) and sadness (orange line). In addition, neutral (red line) is distinct from angry (blue line) and sadness (orange line). However, it is slightly different from the neutral (red line) to happiness (green line), because it is difficult to distinguish between them.\n\nIn summary, it is difficult to distinguish between all emotions, especially angry, happiness, and neutral emotions. However, the proposed method can increase the accuracy of all emotions. Therefore, adopting channel attention in the CNNbased model can effectively extract the emotional context.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study proposes a promising and more effective preprocessing method and an ECA module for SER, offering the potential for significant advancements in the field. Our experiment, conducted with eight different preprocessing datasets from the IEMOCAP corpus, revealed a significant finding: a spectrogram with a higher frequency resolution is more effective in training emotional features, providing valuable insight for future research in the field. Our study is the first study to apply an ECA to the SER. We achieved significantly better results than previous models by applying ECA to our CNN-based model with an effective preprocessing method.\n\nConsidering these results, correctly understanding the relationship between the channel features in the CNN structure can be a clue to understanding how to distinguish emotions. However, the ECA is limited in that it only considers the relationship between neighboring channels. In future work, we will look for attention structures that are efficient but can learn the relationships between channel features more broadly. In addition, it is necessary to determine a better preprocessing method for emotion recognition by analyzing the frequency features associated with each emotion.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall pipeline of speech emotion recognition with CNN-based efficient channel attention architectures.",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the overall pipeline used in",
      "page": 2
    },
    {
      "caption": "Figure 2: The log-Mel spectrogram images of each category of emotions",
      "page": 3
    },
    {
      "caption": "Figure 2: shows an example of a log-Mel spectrogram for each",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) and Fig. 2 (c) show the characteristics",
      "page": 4
    },
    {
      "caption": "Figure 3: CNN-based model architecture. It consists of six convolution blocks",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the overall architecture of the CNN-",
      "page": 4
    },
    {
      "caption": "Figure 4: The design of efficient channel attention module in this study.",
      "page": 5
    },
    {
      "caption": "Figure 4: The ECA structure",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the ECA block used after the convo-",
      "page": 5
    },
    {
      "caption": "Figure 5: The ECA block position in CNN-based model.",
      "page": 6
    },
    {
      "caption": "Figure 6: The comparison of ACC results from models using different channel",
      "page": 7
    },
    {
      "caption": "Figure 6: shows the performance of the entire model for the",
      "page": 7
    },
    {
      "caption": "Figure 7: shows the performance of each dataset before and",
      "page": 7
    },
    {
      "caption": "Figure 7: The comparison of ACC results of the ECA block used or not in",
      "page": 8
    },
    {
      "caption": "Figure 8: The comparison of ACC results with the ECA blocks used in",
      "page": 8
    },
    {
      "caption": "Figure 9: The comparison of ACC results used different versions of datasets",
      "page": 8
    },
    {
      "caption": "Figure 8: shows that the performance decreases when ECA",
      "page": 8
    },
    {
      "caption": "Figure 10: The comparison of ACC results used the STFT data augmentations.",
      "page": 9
    },
    {
      "caption": "Figure 9: , the model performance tended to",
      "page": 9
    },
    {
      "caption": "Figure 10: shows the augmentation experiments in ascending",
      "page": 9
    },
    {
      "caption": "Figure 11: The comparison of ACC results used the STFT data augmentations.",
      "page": 9
    },
    {
      "caption": "Figure 11: shows the augmentation experiments in descending",
      "page": 9
    },
    {
      "caption": "Figure 11: is that the CNN",
      "page": 9
    },
    {
      "caption": "Figure 12: The confusion matrices whether the ECA block is used or not in",
      "page": 10
    },
    {
      "caption": "Figure 13: The confusion matrices whether the ECA block is used or not in",
      "page": 10
    },
    {
      "caption": "Figure 13: show the confusion matrices of the ablation models.",
      "page": 10
    },
    {
      "caption": "Figure 12: and Fig. 13, you can see that in most",
      "page": 10
    },
    {
      "caption": "Figure 12: (a), Fig. 13(b) shows that the classification performance of",
      "page": 10
    },
    {
      "caption": "Figure 12: (a) and (b), the happiness classifica-",
      "page": 10
    },
    {
      "caption": "Figure 13: , the neutral",
      "page": 10
    },
    {
      "caption": "Figure 13: (b), both angry and neutral",
      "page": 10
    },
    {
      "caption": "Figure 14: (a), the ECA weights of the fifth",
      "page": 10
    },
    {
      "caption": "Figure 14: (b), the ECA weights of the last layer are noticeably",
      "page": 10
    },
    {
      "caption": "Figure 14: The ECA’s channel weight plots of each emotion class. (a) The",
      "page": 11
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "2",
      "title": "Emotion in human-computer interaction",
      "authors": [
        "S Brave",
        "C Nass"
      ],
      "year": "2007",
      "venue": "The human-computer interaction handbook"
    },
    {
      "citation_id": "3",
      "title": "Detecting depression severity from vocal prosody",
      "authors": [
        "Y Yang",
        "C Fairbairn",
        "J Cohn"
      ],
      "year": "2012",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Call redistribution for a call center based on speech emotion recognition",
      "authors": [
        "M Bojanić",
        "V Delić",
        "A Karpov"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "5",
      "title": "Culture and the categorization of emotions",
      "authors": [
        "J Russell"
      ],
      "year": "1991",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "6",
      "title": "Deep learning approaches for speech emotion recognition: state of the art and research challenges",
      "authors": [
        "R Jahangir",
        "Y Teh",
        "F Hanif",
        "G Mujtaba"
      ],
      "year": "2021",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "8",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "9",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Y Xie",
        "R Liang",
        "Z Liang",
        "C Huang",
        "C Zou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks,\" in 2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "Speech emotion recognition using convolutional and recurrent neural networks,\" in 2016 Asia-Pacific signal and information processing association annual summit and conference (APSIPA)"
    },
    {
      "citation_id": "13",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "15",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition using 3d convolutions and attention-based sliding recurrent networks with auditory front-ends",
      "authors": [
        "Z Peng",
        "X Li",
        "Z Zhu",
        "M Unoki",
        "J Dang",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Lightsernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "A Aftab",
        "A Morsali",
        "S Ghaemmaghami",
        "B Champagne"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "20",
      "title": "How much position information do convolutional neural networks encode?",
      "authors": [
        "M Islam",
        "S Jia",
        "N Bruce"
      ],
      "year": "2020",
      "venue": "How much position information do convolutional neural networks encode?",
      "arxiv": "arXiv:2001.08248"
    },
    {
      "citation_id": "21",
      "title": "Contextnet: Improving convolutional neural networks for automatic speech recognition with global context",
      "authors": [
        "W Han",
        "Z Zhang",
        "Y Zhang",
        "J Yu",
        "C.-C Chiu",
        "J Qin",
        "A Gulati",
        "R Pang",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Contextnet: Improving convolutional neural networks for automatic speech recognition with global context",
      "arxiv": "arXiv:2005.03191"
    },
    {
      "citation_id": "22",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "23",
      "title": "Improve accuracy of speech emotion recognition with attention head fusion",
      "authors": [
        "M Xu",
        "F Zhang",
        "S Khan"
      ],
      "year": "2020",
      "venue": "2020 10th annual computing and communication workshop and conference (CCWC)"
    },
    {
      "citation_id": "24",
      "title": "Spatiotemporal and frequential cascaded attention networks for speech emotion recognition",
      "authors": [
        "S Li",
        "X Xing",
        "W Fan",
        "B Cai",
        "P Fordson",
        "X Xu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition with multiscale area attention and data augmentation",
      "authors": [
        "M Xu",
        "F Zhang",
        "X Cui",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "26",
      "title": "Atda: Attentional temporal dynamic activation for speech emotion recognition",
      "authors": [
        "L.-Y Liu",
        "W.-Z Liu",
        "J Zhou",
        "H.-Y Deng",
        "L Feng"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "27",
      "title": "Dstcnet: Deep spectrotemporal-channel attention network for speech emotion recognition",
      "authors": [
        "L Guo",
        "S Ding",
        "J Wang",
        "Dang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "28",
      "title": "Learning multi-scale features for speech emotion recognition with connection attention mechanism",
      "authors": [
        "Z Chen",
        "J Li",
        "H Liu",
        "X Wang",
        "H Wang",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "29",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "Bam: Bottleneck attention module",
      "authors": [
        "J Park",
        "S Woo",
        "J.-Y Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "Bam: Bottleneck attention module",
      "arxiv": "arXiv:1807.06514"
    },
    {
      "citation_id": "32",
      "title": "A simple and lightweight attention module for convolutional neural networks",
      "authors": [
        "J Park",
        "S Woo",
        "J.-Y Lee",
        "I Kweon"
      ],
      "year": "2020",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "33",
      "title": "Model complexity of deep learning: A survey",
      "authors": [
        "X Hu",
        "L Chu",
        "J Pei",
        "W Liu",
        "J Bian"
      ],
      "year": "2021",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "34",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "Eca-net: Efficient channel attention for deep convolutional neural networks",
      "authors": [
        "Q Wang",
        "B Wu",
        "P Zhu",
        "P Li",
        "W Zuo",
        "Q Hu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "36",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "1d convolutional neural networks and applications: A survey",
      "authors": [
        "S Kiranyaz",
        "O Avci",
        "O Abdeljaber",
        "T Ince",
        "M Gabbouj",
        "D Inman"
      ],
      "year": "2021",
      "venue": "Mechanical systems and signal processing"
    },
    {
      "citation_id": "38",
      "title": "Parallelized convolutional recurrent neural network with spectral features for speech emotion recognition",
      "authors": [
        "P Jiang",
        "H Fu",
        "H Tao",
        "P Lei",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "39",
      "title": "Exploring deep spectrum representations via attentionbased recurrent and convolutional neural networks for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Y Zhao",
        "Z Zhang",
        "N Cummins",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "40",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "41",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X.-C Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification"
    },
    {
      "citation_id": "45",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    }
  ]
}