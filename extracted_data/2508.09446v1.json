{
  "paper_id": "2508.09446v1",
  "title": "Mpt: Motion Prompt Tuning For Micro-Expression Recognition",
  "published": "2025-08-13T02:57:43Z",
  "authors": [
    "Jiateng Liu",
    "Hengcan Shi",
    "Feng Chen",
    "Zhiwen Shao",
    "Yaonan Wang",
    "Jianfei Cai",
    "Wenming Zheng"
  ],
  "keywords": [
    "Micro-expression recognition",
    "vision transformer",
    "parameter-efficient fine-tuning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-expression recognition (MER) is crucial in the affective computing field due to its wide application in medical diagnosis, lie detection, and criminal investigation. Despite its significance, obtaining micro-expression (ME) annotations is challenging due to the expertise required from psychological professionals. Consequently, ME datasets often suffer from a scarcity of training samples, severely constraining the learning of MER models. While current large pre-training models (LMs) offer general and discriminative representations, their direct application to MER is hindered by an inability to capture transitory and subtle facial movements-essential elements for effective MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to adapting LMs for MER, representing a pioneering method for subtle motion prompt tuning. Particularly, we introduce motion prompt generation, including motion magnification and Gaussian tokenization, to extract subtle motions as prompts for LMs. Additionally, a group adapter is carefully designed and inserted into the LM to enhance it in the target MER domain, facilitating a more nuanced distinction of ME representation. Furthermore, extensive experiments conducted on three widely used MER datasets demonstrate that our proposed MPT consistently surpasses state-of-the-art approaches and verifies its effectiveness.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Micro-expressions (MEs) are comprised of transitory and subtle facial muscle movements, as illustrated in Fig.  1 (a ). Haggard and Isaacs  [7]  have shown that micro-expressions can unveil authentic human emotions that individuals may attempt to conceal, and accurate micro-expression recognition (MER) stands as a crucial step for many real-world applications, such as lie detection, criminal investigation, and clinical diagnosis  [8] -  [10] . Thus, MER has become a research hotspot in the field of affective computing, and many novel approaches have The key to MER is to capture subtle facial movements. (b) Compared with previous methods, our proposed MPT achieves the best recognition accuracy (Acc) while containing fewer tunable parameters. Here we compare our MPT with SRMCL  [1] , TSCNN  [2] , LFBVT  [3] , SLSTT  [4] , AlexNet  [5] , and VGG-16  [6]  been proposed in recent years to cope with it. Nevertheless, MER is still a very challenging task. This is because of the low intensity and short duration of MEs that may lead to subtle and irregular muscle movements in facial regions and temporal segments, which also makes it extremely difficult to capture the comprehensive spatial-temporal representation of MEs. Consequently, it is desirable to learn a reliable representation of MEs to recognize the MEs more robustly.\n\nCurrent MER methods can be generally divided into two types. The first type is hand-crafted methods  [11] -  [14] , which manually extract facial texture and movements for recognition. However, hand-crafted features are usually hard to capture complex patterns in facial movements, and thus limit their recognition ability. The second type is deep learningbased methods, such as CNN and Transformer-based solutions  [2] ,  [4] ,  [15] ,  [16] . They utilize learnable deep features arXiv:2508.09446v1 [cs.CV] 13 Aug 2025\n\nto capture more comprehensive spatio-temporal relations in micro-expression sequences, generally outperforming handcrafted methods. Nonetheless, deep-learning-based methods, especially Transformer-based ones, typically carry a mass of learnable parameters and require enormous training data. This brings in the challenge for MER since MER datasets usually only contain a small number of training samples, e.g., 247 and 164 samples in the CASME II and SMIC datasets, respectively, which are hard to train heavy deep MER models.\n\nOn the other hand, large pre-training models (LMs) have been widely developed in recent years, which are trained with millions of vision and/or language data to obtain general representations  [17] -  [21] , and have shown excellent performance on various downstream tasks like multi-modal tracking  [22] , action recognition  [23] , face generation  [24] , etc. A straightforward way to address the few-sample issue is to leverage the general representations in LMs as prior knowledge and accelerate convergence  [24] -  [26] . However, LMs are usually trained with macroscopical data, while MER is required to recognize subtle and transitory facial movements as shown in Fig.  1 (a) . Therefore, the key to leveraging LMs for MER is to incorporate subtle motion cues so that the LMs can capture more distinguishable ME representations.\n\nMotivated by the above discussion, in this paper, we propose an effective and efficient method termed motion prompt tuning (MPT) for adapting LMs for the MER task. MPT only finetunes a small number of parameters rather than the entire LM to preserve the knowledge of the LM, which can train the model more efficiently and reduce overfitting in small MER datasets. As shown in Fig.  1  (b), MPT achieves an excellent performance using a smaller tunable parameter number than other MER methods. Specifically, our MPT consists of a large pre-training vision model, a well-designed motion prompt generation module, and group adapters inserted in the LMs. The motion prompt generation module first magnifies and captures subtle facial motions in the ME sequence. Then, it models temporal expression flows by Gaussian tokenization to aggregate ME salient snapshots. Motion prompt tokens are generated, combined with facial appearance RGB tokens and a classification token, as the inputs of the LM. To transfer the knowledge learned by LM to the MER task, we further propose to adapt different types of tokens differently in the transformer layer by parameter-efficient group adapters. Specifically, the group adapter divides the classification token, motion prompts, and RGB tokens into separate groups in transformer layers and uses three learnable type-aware adapter operations to cope with them. Compared with the previous common adapter structure  [25] , the group adapter can aggregate different types of information in LMs more effectively. Extensive experiments on three widely used benchmarks, CASME II, SAMM, and SMIC datasets, show that our MPT significantly outperforms state-of-the-art methods, demonstrating its effectiveness.\n\nOur main contributions are summarized as follows: 1) To the best of our knowledge, this is the first work proposing to adapt LMs to the task of MER. The rest of the paper is organized as follows. Section II briefly reviews the existing works on micro-expression recognition, large models, and parameter-efficient transfer learning. Section III introduces the proposed motion prompt tuning (MPT) in detail. Section IV reports extensive experimental results on several widely used datasets. Finally, we draw a conclusion in Section V.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Micro-Expression Recognition",
      "text": "Current MER approaches can be mainly divided into two categories, including hand-crafted and deep-learning-based methods. On the one hand, Hand-crafted works manually design and extract features, such as local binary pattern (LBP) and its variants (LBP-TOP), histogram of oriented gradient (HOG), and histogram of optical flow (HOOF) to represent the spatial and temporal features of MEs  [10] ,  [27] . For example, Zong et al.  [8]  leverage LBP-TOP features and propose a hierarchical spatial grid division scheme and a kernelized group sparse learning (KGSL) model for MER. Liu et al.  [28]  use sparse main directional mean optical flow (MDMO) to capture expression structures. In  [10] , handcrafted features of MEs are used for conducting cross-database recognition for ME. However, the hand-crafted features struggle to capture complex facial patterns to recognize various MEs in various scenes. On the other hand, the deep-learning-based methods can learn more comprehensive expression representations to contain more accurate MER results  [9] ,  [29] ,  [30] . For example, Lei et al.  [16]  extract the shape information of MEs and construct facial graph structures to model the intrinsic relations among important facial regions to help feature learning Verma et al. present several neural architecture searchbased models to handle MER  [31] ,  [32] . Wei et al.  [15]  use contrastive-learning-based distillation loss to encode explicit movement features and enforce a Wilcoxon rank sum test loss to calibrate the extracted intensity clues. These methods are based on CNNs, while many recent works  [4] ,  [33] ,  [34]  leverage Transformers to recognize MEs. Li et al.  [35]  employ multi-instance learning (MIL) to detect essential information on faces automatically and design a local maximum and global context joint learning to obtain a discriminative facial representation. In  [36] , Chen et al. present a block division convolutional network (BDCNN) combined with an implicit semantic data augmentation loss to enhance ME feature learning. Zhang et al.  [4]  model short-and long-range relations to capture the local and global spatiotemporal patterns in MEs. They also utilize long-term optical flows to describe motions. FRL-DGT  [33]  uses a convolutional displacement generation module with self-supervised learning to extract the motion features between onset and apex frames. In SelfME  [34] , Fan et al. also use a self-supervised method combined with Fig.  2 . Pipeline of the proposed MPT. It consists of a motion prompt generation module, a large model backbone inserted with group adapters, and a learnable classification head. Specifically, the input micro-expression sequence is processed by rank-pooling and patch-embedding operations to obtain class and vision tokens x 0 and V 0 , and the motion prompt generation module generates the motion prompt tokens P 0 at the same time. The concatenated tokens are then fed into a frozen L-layer transformer-based LM, in which each layer is inserted by a group adapter to help with the adaptation between different types of tokens. Finally, the classification head predicts the micro-expression type based on the output class token x L . a symmetric contrastive vision transformer to extract facial motions in MEs.\n\nNevertheless, deep-learning-based methods require a mass of training data, while ME data is limited. Unlike these approaches, we propose motion prompt tuning and a group adapter that are specifically designed to adapt LMs to MER, which only need to learn a few parameters and thus reduce the requirements of training data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Large Model And Parameter-Efficient Transfer Learning",
      "text": "In the rapidly evolving landscape of machine learning, large models (LMs)  [18] ,  [21] ,  [37]  have garnered significant attention, emerging as a cornerstone in various tasks like natural language processing, image processing, and multimodal. These models are often based on transformer architectures like ViT  [38]  and trained on millions of data. They have shown great performance in various downstream tasks, including image classification  [38] , object detection  [39] ,  [40] , facial expression analysis  [24] , and segmentation  [19] . Given their superior performance and much larger parameter scale, the question of efficiently adapting such models to the data-limited task becomes crucial. Existing efficient methods for adapting LMs into downstream tasks fall into two categories. The first type is prompt tuning, which refers to designing instructions and prepending them to the input of LMs. Recently, prompt techniques  [17] ,  [19] ,  [41]  have attracted increasing research attention, which only fine-tune a few additional tokens in the input of LMs. VPT  [17]  adds a few randomly initialized vision tokens and achieves better performance than fully fine-tuned methods. ViPT  [42]  learns modal-related prompt tokens to adapt LMs to downstream tracking tasks. STPN  [43]  generates dynamic visual prompts gathering spatial and temporal information for robust video representation. These methods focus on capturing semantics and appearances as prompts, while we propose to generate motion prompts that can help to represent ME. Another type of parameter-efficient transfer learning method is adapter tuning  [25] ,  [44] ,  [45] , which adds a small number of learnable parameters to fixed LMs for adaptation. Sung et al.  [45]  leverage residual connections to add several linear layers to pretrained LMs, and thus they can fine-tune the additional linear layers while fixing LMs. Pan et al.  [46]  and Yang et al.  [23]  propose spatio-temporal adapters for imagevideo transfer learning tasks. They introduce frameworks that jointly spatial adaptation and temporal adaptation to gradually improve the spatiotemporal reasoning capability of the image model. CLIP-Adapter  [26]  combines both visual and textual feature adapters to adapt the pretrained CLIP  [18]  model to target tasks.\n\nNevertheless, existing fine-tuning methods can not adapt to the MER domain well since they rely on naive prompts or simple adapter structures, which can not capture the subtle motions in MEs. To handle this issue, we combine motion prompt tuning with unique group adapters, which are designed based on the characteristics of MEs for further improving the adaptation of LMs for ME.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Method",
      "text": "We propose MPT to effectively adapt LMs for the MER task. Instead of training the whole LM from scratch, MPT only fine-tunes a few parameters while freezing the parameters of the LM at the same time, which (1) reserves the pre-trained knowledge in the LMs, (2) trains the model more efficiently, and (3) reduces overfitting in small-scale ME databases. In this section, we will introduce the proposed MPT in detail.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Overview",
      "text": "As shown in Fig  2 , our MPT consists of three major components, i.e., (1) a Large Model (e.g., a ViT-16  [38] ),  (2)  the Motion Prompt Generation, and (3) the Group Adapter. Specifically, consider a micro-expression sequence S = {I 0 , I 1 , ...I T } ∈ R T ×H×W ×C , where T is the number of frames, I t is the t-th frame in the sequence, H and W are the height and width of each frame, and C is the number of channels. Firstly, we follow  [32]  to aggregate the sequence S into a single RGB image S * ∈ R H×W ×C by rankpooling  [47] . The basic idea of the rank-pooling approach is to aggregate the facial texture and appearance of a video sequence into a single RGB image by learning a ranking function to capture the temporal ordering of the sequence. By focusing on the order of features, rank-pooling decreases noise and variations in ME, and so it can effectively capture the temporal dynamics of MEs. Here we utilize the fast version for calculation  [47] :\n\nwhere ψ(I t ) denotes the feature representation of I t .\n\nConsequently, for a pre-trained L-layer Vision Transformer model  [38] , the calculated RGB image S * ∈ R H×W ×C is divided into fixed-sized patches, each of which is embedded into a D-dimensional vector with positional embedding:\n\nwhere Embed(•) is the patch-embedding operation in vision transformer, N v is the number of visual tokens and D is the dimension of each token. In addition, a class token x 0 ∈ R D is concatenated to the patch tokens sequence. At the same time, motion prompts P 0 ∈ R Np×D are generated by our motion prompt generation module to help the pre-trained LM to recognize subtle motion movements, where N p represents the number of motion tokens. The entire input of the LM is a concatenation of vision tokens V 0 , motion prompts P 0 and an additional [CLS] token x 0 ∈ R D . i.e.,\n\nwhere [•, •, •] is the concatenation operation and I ∈ R (Np+Nv+1)×D is fed into the LM as input.\n\nThe LM comprehensively analyzes vision, motion, as well as class information and updates these tokens. As in the operation in ViT  [38] , we pass the updated [CLS] token into a classification head to generate the final recognition result. However, LMs are usually trained on macroscopically RGB data, while inputs for MER are a facial RGB image and motion prompts. To alleviate the domain gaps, we propose a group adapter to further adapt LM representations to the MER domain. Different from the vanilla adapter  [25] , our group adapter individually deals with each type of token to better adapt each category of data and reduce computational costs. During our training, the large model is fixed, while only motion prompt generation, group adapter, as well as classification head are updated. Below, we introduce the details of our motion prompt generation and group adapter.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Motion Prompt Generation",
      "text": "The proposed motion prompt generation contains two steps, i.e., motion magnification to capture subtle movements for MER, as well as temporal Gaussian tokenization to further aggregate temporal movements and embed them into motion prompt tokens. Motion Magnification and Extraction. In terms of  [35] ,  [48] ,  [49] , the rate of motion changes in an ME sequence can be represented by the special frequency components. To this end, we first magnify and extract the motion cues using an Eulerian approach, in which we analyze video variations at fixed pixel locations over time, and capture subtle changes in the frequency domain. Specifically, let I(x, y, t) represent pixel intensity at time t and spatial coordinates (x, y), we first decompose each frame I t in the ME sequence into K-level spatial frequency bands by using Laplacian pyramids Lp(•):\n\nThen we apply temporal filtering for each spatial frequency level k:\n\nwhere H(t) denotes the temporal filter. The filtered signals are magnified by a factor β:\n\nFinally, the magnified signals are reconstructed to obtain the motion sequence S:\n\nwhere Rec(•) denotes the Laplacian reconstruction function to form the final magnified signal. As shown in Fig.  2 , the motion magnification and extraction make it possible to observe the invisible changes over the ME sequence by the captured temporal motion sequence S ∈ R T ×H×W ×C . Temporal Gaussian Tokenization. We then integrate the captured temporal motions S and generate motion prompt tokens P 0 . Since ME sequences have several salient snapshots, like onset and apex frames, which contain more ME motion information  [35] ,  [50]  and play more important roles in representing ME, the temporal Gaussian tokenization is proposed to achieve the goal that adaptively aggregating these significant ME motion patterns over temporal dynamics. In particular, we generate N p motion prompt tokens, and for each token, we use a temporal Gaussian kernel to integrate motions. Therefore, there are N p Gaussian kernels. Firstly, a 3 × 3 convolution layer Conv(•) and an average pooling Avg(•) are used to embed S and fuse spatial information:\n\nwhere S ′ ∈ R T ×D is the embeded feature map. Subsequently, two linear layers W µ i and W σ i are used to generate the means and variances for i-th temporal Gaussian kernels:\n\nwhere µ i ∈ R D and σ 2 i ∈ R D denote a pair of mean and variance of Gaussian distributions. Then, a motion prompt token can be generated as:\n\nwhere W p ∈ R D is a learnable linear layer. In the end, N p motion tokens are generated as the motion prompts of LMs.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Group Adapter",
      "text": "We input the concatenation of vision tokens V 0 , motion prompts P 0 as well as the [CLS] token x 0 into the LM. The LM containing L layers learns and updates representations as\n\nwhere T rans l (•) represents the l-th Transformer layer, and l = 1, ..., L. [x l-1 , V l-1 , P l-1 ] are tokens from the previous block, and [x l , V l , P l ] are the ones updated by the l-th layer.\n\nTo transfer the knowledge learned by LM to the MER task, a straightforward insight is to inject domain-specific adapters to learn the information from different domains effectively  [25] . Adapters are learnable bottlenecks consisting of a downsampling layer, an upsampling layer, and an activation function to process different types of tokens in the transformer layer in a parameter-efficient way. However, the primitive adapter  [25]  processes all the tokens together, which cannot explore the complex modality-specific nuances. To this end, we divide the classification token, motion prompts, and RGB tokens into separate groups in transformer layers and use three learnable type-aware adapter operations to cope with them.\n\nTo this end, we propose a group adapter to reduce the gaps between MER and the LM. Specifically, as shown in Fig  3 , group adapter is inserted after the feed-forward layer to maximize the benefits of the transformer block's processing. Furthermore, to facilitate efficient cross-modal learning, the group adapter is divided into three branches: a class branch, a motion branch, and a visual branch. Each branch consists of a downsample linear layer, an activation function, and an upsample linear layer. Formally, given the input feature matrix at the l-th layer, the feature adaptation process can be written as:\n\nwhere f (•) denotes the GELU  [51]  activation function, W xd l , W Vd l , W Pd l ∈ R D× D η refer to the downsample linear layers, and\n\n×D refer to the upsample linear layers, η denotes the reduction factor. xl , Vl and Pl denote the average of the class, vision, and motion tokens in the l-th transformer layer, respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Classification Head And Loss Function",
      "text": "The final [CLS] token x L is fed into a classification head containing a linear layer and a Softmax normalization. The classification head predicts the MER results in y p . The proposed method is optimized as follows:\n\nwhere CE(  IV. EXPERIMENTS",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Experimental Settings",
      "text": "Three widely used databases were utilized during the experiments, including CASME II  [61] , SAMM  [62] , and SMIC  [63]   frames. The Butterworth band-pass filter is used for temporal filtering in the motion prompt generation, and the cut-off frequency for motion magnification is set between 0.4 and 4 Hz, corresponding to heart rate per minute. Additionally, the leave-one-subject-out (LOSO) protocol is utilized for evaluation, whereby one subject is reserved for validation while the remaining subjects are used for training. Two common evaluation metrics are involved in the experiments, i.e., the recognition accuracy (Acc) and F1-score. To be specific, assume that T P c , F P c , and F N c denote the true positive, false positive, and false negative rates for the c-th class, the recognition accuracy and F1-score can be calculated as:\n\nin which\n\nwhere N is the total number of samples of all subjects, C is the number of categories in the datasets. Besides these, we also calculate the tunable parameters (#Tparas) and GFLOPs during some comparison experiments to compare the model complexity and efficiency.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Comparison With State-Of-The-Art Methods",
      "text": "In this section, we conduct a series of comparison experiments and list the results to evaluate the proposed MPT. Recognition results comparison. We present the results achieved in recognition accuracy (Acc) and mean F1-score under the leave-one-subject-out (LOSO) protocol on the CASME II, SAMM, and SMIC databases, which can be seen in Table  I , Table  II  and Table  III , respectively. Meanwhile, a lot of state-of-the-art methods are chosen for comparison, including handcrafted methods (LBP-TOP  [14]  LBP-SIP  [12] , STCLQP  [60] ) as well as recent CNN and Transformerbased deep methods (e.g., Alexnet  [5] , AUGCN  [59] , Knowledge Distillation (KD)  [54] , DSSN  [53] , TSCNN  [2] , Graph-TCN  [16] , SLSTT  [4] , CMNET  [15] , LTR3O  [58] , µ-BERT  [55] , PLMaM-Net  [56]  and SRMCL  [1] ).\n\nAccording to the results, the proposed MPT achieves the best performance in all cases. Specifically, on the CASME II database, MPT outperforms the previous state-of-theart method FFDIN by a large margin of 2.08%/2.95% in Acc and F1-score. On the SAMM database, our method gains improvements of 7.35% and 9.30% in Acc and F1score than the state-of-the-art LTR3O, respectively. Our MPT also achieves the best performance in the SMIC-HS database and outperforms the state-of-the-art PLMaM-Net  [56]  for 4.27% and 4.69% in Acc and F1-score. The reasons for these superior results can be attributed to several factors. On the one hand, our MPT leverages pre-trained knowledge of LMs and proposes motion prompts to enhance subtle motion recognition, the strong power of LMs successfully improves the MER performance. On the other hand, a group adapter is presented in our MPT to alleviate domain gaps between LMs and MEs further, which can handle information from different domains efficiently. Furthermore, in contrast to the previous methods, the proposed MPT uses a small number of tunable parameters that can prevent the data-limited MER model from being trapped in overfitting while preserving the rich knowledge of LMs, thus it can achieve more effective performance.  Comparison with other transfer learning-based MER methods. Several transfer learning-based MER methods are listed for comparison, including KD  [54] , MicroNet  [65] , M2MTNet  [66] , NSPT  [67] , and PLMaM-Net  [56] . As shown in Table  IV , the proposed MPT outperforms all the transfer learning-based methods. The main reason is that all the existing transfer learning methods just construct similar network structures to mimic the facial appearance and texture representation of macro-expressions, which will inevitably fall into overfitting due to the limited ME data number thus they can not narrow the gap between MaE and ME to obtain ME features. However, our MPT uses a parameter-efficient way to fine-tune the LMs, making them adapt to the MER domain, which utilizes the rich information in LMs without knowledge loss and copes with the issue of limited data number of ME successfully.\n\nEfficiency comparison. Table V provides a comprehensive evaluation of recent SOTA models based on the number of tunable parameters (#Tparas), computational complexity (GFLOPs), accuracy (Acc), and F1-score. In terms of the results, MPT achieves the highest accuracy of 86.59% and F1 score of 86.01% with only 5.5 M tunable parameters and 18.7 GFLOPs, showing its parameter efficiency. This performance is excellent given its moderate complexity, as MPT outperforms many large models, including VGG-16 (with 138.4 M tunable parameters) and SRMCL (with 315.7 M parameters), both of which have much lower accuracy results. In a word, MPT demonstrates a superior balance between computational demand and performance compared to other methods trained from scratch, which proves the effectiveness of the presented strategies in MPT, as it successfully utilizes the powerful ability of LMs and adapts them to the MER domain to handle the limited data issue efficiently.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Ablation Study",
      "text": "Extensive experiments are conducted to make a more indepth analysis of the proposed MPT. Effects of different learning strategies. In Table  VI , we investigate different learning strategies. In the first row, we select a ViT-B/16 model  [38]  as the backbone without any pretraining. Compared with this setting, the ViT-B/16 pretrained on ImageNet-21K  [69]  and fully fine-tuned on SMIC (the second row) achieves improvements of 4.27% in Acc and 4.30% in F1-Score, which shows that the rich knowledge in LMs does benefit the MER issue. However, in the third row, we only fine-tune the classification head rather than the whole model, declining the performance over the baseline method. A reason is that MER datasets only contain limited data, and thus, large models are easy to overfit. Meanwhile, some pretrained knowledge is forgotten during full fine-tuning, while partial fine-tuning can not adapt to the MER domain well. In the fourth row, we add our motion prompts to the partly fine-tuned model, which yields very large improvements in both Acc and F1-Score. Compared with the partly fine-tuned model, our group adapter also achieves gains of 7.32% in Acc. and 8.16% in F1-Score. These demonstrate the effectiveness of our motion prompts and group adapter. Finally, our MPT with both motion prompts and the group adapter shows the best performance, which outperforms the partly fine-tuned model with important improvements by 14.64% in Acc and 15.74% in F1-Score. In a word, both the presented motion prompts and group adapters can improve the ME feature learning a lot. Effects of different prompts. We compare the proposed MPT with the classic visual prompt methods VPT-Shallow and VPT-Deep  [17] , which insert randomly initialized prompts to the LMs. The results can be seen in Table  VII , and we can get several conclusions. Firstly, all prompt-based methods outperform the model without prompts, which proves that using prompt tuning can effectively adapt LMs to MER. Secondly, the proposed MPT significantly improves performance compared to the two classic visual prompt tuning methods. The main reason is that since the common visual prompts used in VPT are just randomly initialized, it can not model the subtle motion of MEs and capture the robust ME representation well. However, our MPT introduces parameterlimited motion cues as visual prompts for LMs and gains an important improvement over the other methods, which shows it can effectively model the subtle motion of MEs.\n\nEffects of different adapters. We compare the proposed group adapter with the primitive adapter  [17]  and MPT without the adapter. According to the results in Table  VIII , the addition of adapters improves the performance, and the proposed group adapter outperforms the primitive one by 3.67% and 3.68% in Acc and F1-score, respectively. The reason is that our group adapter more specifically adapts each type of feature since it utilizes the group structure to process different types of information in the transformer layer effectively.\n\nEffects of the number of motion prompts. We compare different numbers of our motion prompts, and the results are shown in Fig.  5 . Models with 1, 3, 5, and 7 prompts outperform the model without motion prompts. Both accuracy and F1-score show an increasing trend as the number of prompts increases from 0 to 5. This indicates that the inclusion of more prompts generally enhances the model's performance. After 5 prompts, there is a slight decrease in both metrics when the number of prompts is increased to 7. This    [38]  and CLIP  [18] . ViT-16-based models achieve better performance than ViT-B/32-based ones since smaller patches retain more fine-grained details of the facial regions, which can be crucial for recognizing subtle features and textures that contribute to the MER task, while the larger patches might miss these details, leading to lower performance. Meanwhile, the results of CLIP-based models are lower than those pretrained on ImageNet21K. Furthermore, most of the cases outperform the state-of-the-art methods on the SMIC database shown in Table  III . This can be the result of the MER issue with a stable set of classes, while the common pretrained ViT-B/16 is likely to be more beneficial since it may contain more visual information like texture and appearances. This fine-tuning can help the model learn to detect the subtle features specific to MEs. Effects of hyper-parameters.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Visualization Analysis",
      "text": "In this section, we conduct a series of visualization experiments to further evaluate the proposed modules in MPT. Prediction results visualization. To further evaluate the effectiveness of the proposed modules, the overall F1-score and detailed breakdown of class accuracy of the proposed MPT and baseline on each emotion type on CASME II, SAMM, and SMIC-HS databases are visualized in Fig.  6 . Specifically, the model fine-tunes the pretrained primitive ViT-B/16 without inserting the proposed motion prompt, and the group adapter From top to bottom, there are samples selected from CASME II  [61]  related to happiness, surprise, and disgust, respectively. is selected as a baseline comparison. In terms of the results, we can demonstrate the performance of the proposed MPT at the class level, and it significantly improves the performance in the confusable regression type in the CASME II database and the contempt type in the SAMM database. That is to say, the proposed modules successfully represent the subtle motions in these types of MEs. Furthermore, several samples from the CASME II database predicted by the proposed MPT and baseline method are shown in Fig.  7 . According to the figure, we can find that the proposed MPT achieves a higher prediction score on the distinguishable happiness sample and also predicts the right results on the surprise and disgust samples, while the baseline method obtains the wrong predictions on the confusable surprise and disgust samples. Hence, the proposed MPT is more robust since it introduces additional distinguishable facial movement information to the LMs that would benefit from representing subtle MEs. Visualization results of feature heatmaps. To better evaluate the learned features, we visualize the feature heatmaps using GradCAM  [70] . As shown in  Fig 8,   we select the common ViT  [38]  as a baseline and compare its activation heatmap visualization results with our MPT. It can be seen from the results that the proposed MPT focuses more on the AU region relating to MEs than the baseline method. For example, in the first row, the activation map of the proposed MPT focuses more on the brows and lips, which is consistent with the provided AU ground truth (Brow Lowerer and Upper Lip Raiser). In the second row, the region near the lips is highly activated, which is consistent with the AU 25 (Lips Part) annotation. The results prove that our proposed MPT has a stronger ability to capture the subtle motion cues in MEs than the common vit structure since it introduces magnified motion information as prompts to adapt the LM to learn the ME feature, and the introduced motion prompts are effectively absorbed in the structure of the LM through the light-weight adapter layers. In contrast, the common ViT can not represent the subtle motion of ME well since it is trained on the RGB image directly without introducing extra motion cues. Feature distribution. To evaluate the effectiveness of the proposed MPT, we apply t-SNE  [71]  to visualize the feature embedding under different settings as shown in Fig 9 . To be specific, a pretrained ViT-B/16 is utilized as the backbone.  experiments that conduct fine-tuning on the pretrained ViT-B/16 with the proposed group adapter, motion prompt, and the whole MPT, respectively. According to the results, both motion prompt and group adapter extract more distinguishable feature boundaries with three class clusters. Feature embedding of part fine-tuning on serious overlap among the three classes, while the confusable surprise and positive samples are separated clearly in Fig.  9 (c) and (d), which means the generation of motion prompts plays a significant role in assisting the LMs to capture the subtle motion cues. Furthermore, the proposed MPT can obtain a clearer class boundary which demonstrates that the proposed MPT can learn more representative characteristics of the MER issue.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we have proposed a novel motion prompt tuning network model (MPT) for adapting large pre-training models (LMs) to the data-limited MER problem. Based on a frozen LM, MPT first utilizes a motion prompt generation module to magnify subtle motions and then generates motion prompts by aggregating the temporal salient snapshots. Furthermore, a simple yet effective group adapter (GA) is presented to reduce the gaps between pretrained LMs and MER data efficiently. Extensive experiments conducted on several databases show that the proposed MPT outperforms state-of-the-art MER methods, which demonstrates that the proposed motion prompt generation and tuning methods can help the LMs focus on subtle motion cues and enhance the MER performance effectively and efficiently.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) An example of micro-expression of Happiness. The key to MER is",
      "page": 1
    },
    {
      "caption": "Figure 1: (a). Therefore, the key to leveraging LMs for MER is",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), MPT achieves an excellent",
      "page": 2
    },
    {
      "caption": "Figure 2: Pipeline of the proposed MPT. It consists of a motion prompt generation module, a large model backbone inserted with group adapters, and a learnable",
      "page": 3
    },
    {
      "caption": "Figure 2: , our MPT consists of three major",
      "page": 4
    },
    {
      "caption": "Figure 3: Overview of the group adapter. It contains three branches, including the class, prompt, and vision branches to separate the transfer learning in the",
      "page": 5
    },
    {
      "caption": "Figure 3: , group adapter is inserted after the feed-forward layer to",
      "page": 5
    },
    {
      "caption": "Figure 4: Confusion matrices of vision transformer-based SRMCL [1] and the proposed MPT on CASME II, SAMM, and SMIC databases.",
      "page": 7
    },
    {
      "caption": "Figure 4: , both of which are built on the vision transformer",
      "page": 8
    },
    {
      "caption": "Figure 5: Models with 1, 3, 5, and 7 prompts",
      "page": 9
    },
    {
      "caption": "Figure 5: Recognition accuracy (Acc) and F1-score achieved by different",
      "page": 9
    },
    {
      "caption": "Figure 6: Comparison of the proposed MPT with baseline at class level. The F1-score results are visualized on CASME II, SAMM, and SMIC databases,",
      "page": 10
    },
    {
      "caption": "Figure 6: Specifically,",
      "page": 10
    },
    {
      "caption": "Figure 7: Comparison of the prediction cases of the proposed MPT and baseline",
      "page": 10
    },
    {
      "caption": "Figure 7: According",
      "page": 10
    },
    {
      "caption": "Figure 8: Visualization results of feature heatmaps of several samples in",
      "page": 11
    },
    {
      "caption": "Figure 8: , we select the common",
      "page": 11
    },
    {
      "caption": "Figure 9: t-SNE visualization comparison results on the SMIC-HS database.",
      "page": 11
    },
    {
      "caption": "Figure 9: (c) and (d), which means the generation of",
      "page": 11
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Boosting microexpression recognition via self-expression reconstruction and memory contrastive learning",
      "authors": [
        "Y Bao",
        "C Wu",
        "P Zhang",
        "C Shan",
        "Y Qi",
        "X Ben"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Recognizing spontaneous micro-expression using a three-stream convolutional neural network",
      "authors": [
        "B Song",
        "K Li",
        "Y Zong",
        "J Zhu",
        "W Zheng",
        "J Shi",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Late fusion-based video transformer for facial micro-expression recognition",
      "authors": [
        "J Hong",
        "C Lee",
        "H Jung"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "4",
      "title": "Short and long range relation based spatio-temporal transformer for micro-expression recognition",
      "authors": [
        "L Zhang",
        "X Hong",
        "O Arandjelović",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Computer Science"
    },
    {
      "citation_id": "7",
      "title": "Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy",
      "authors": [
        "E Haggard",
        "K Isaacs"
      ],
      "year": "1966",
      "venue": "Methods of research in psychotherapy"
    },
    {
      "citation_id": "8",
      "title": "Learning from hierarchical spatiotemporal descriptors for micro-expression recognition",
      "authors": [
        "Y Zong",
        "X Huang",
        "W Zheng",
        "Z Cui",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Micromamba: State space model with partitioned window scan for micro-expression recognition",
      "authors": [
        "T Zhou",
        "J Liu",
        "Y Jin",
        "L Yao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 6th ACM International Conference on Multimedia in Asia"
    },
    {
      "citation_id": "10",
      "title": "Cross-database micro-expression recognition based on transfer double sparse learning",
      "authors": [
        "J Liu",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "11",
      "title": "Optical strain based recognition of subtle emotions",
      "authors": [
        "S Liong",
        "R Phan",
        "J See",
        "Y Oh",
        "K Wong"
      ],
      "year": "2014",
      "venue": "ISPACS"
    },
    {
      "citation_id": "12",
      "title": "Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "W Phan",
        "Y Oh"
      ],
      "year": "2015",
      "venue": "Accv"
    },
    {
      "citation_id": "13",
      "title": "Eulerian emotion magnification for subtle expression recognition",
      "authors": [
        "A Ngo",
        "Y Oh",
        "R Phan",
        "J See"
      ],
      "year": "2016",
      "venue": "ICASSP"
    },
    {
      "citation_id": "14",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Cmnet: contrastive magnification network for micro-expression recognition",
      "authors": [
        "M Wei",
        "X Jiang",
        "W Zheng",
        "Y Zong",
        "C Lu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "A novel graph-tcn with a graph structured representation for micro-expression recognition",
      "authors": [
        "L Lei",
        "J Li",
        "T Chen",
        "S Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Visual prompt tuning",
      "authors": [
        "M Jia",
        "L Tang",
        "B.-C Chen",
        "C Cardie",
        "S Belongie",
        "B Hariharan",
        "S.-N Lim"
      ],
      "year": "2022",
      "venue": "Proceedings of the European Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "19",
      "title": "Proposalclip: Unsupervised open-category object proposal generation via exploiting clip cues",
      "authors": [
        "H Shi",
        "M Hayat",
        "Y Wu",
        "J Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Transformer scale gate for semantic segmentation",
      "authors": [
        "H Shi",
        "M Hayat",
        "J Cai"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Llmformer: Large language model for open-vocabulary semantic segmentation",
      "authors": [
        "H Shi",
        "S Dao",
        "J Cai"
      ],
      "year": "2024",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Maple: Multi-modal prompt learning",
      "authors": [
        "M Khattak",
        "H Rasheed",
        "M Maaz",
        "S Khan",
        "F Khan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Aim: Adapting image models for efficient video action recognition",
      "authors": [
        "T Yang",
        "Y Zhu",
        "Y Xie",
        "A Zhang",
        "C Chen",
        "M Li"
      ],
      "year": "2023",
      "venue": "Aim: Adapting image models for efficient video action recognition",
      "arxiv": "arXiv:2302.03024"
    },
    {
      "citation_id": "24",
      "title": "Faceclip: Facial imageto-video translation via a brief text description",
      "authors": [
        "J Guo",
        "H Manukyan",
        "C Yang",
        "C Wang",
        "L Khachatryan",
        "S Navasardyan",
        "S Song",
        "H Shi",
        "G Huang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "25",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Clip-adapter: Better vision-language models with feature adapters",
      "authors": [
        "P Gao",
        "S Geng",
        "R Zhang",
        "T Ma",
        "R Fang",
        "Y Zhang",
        "H Li",
        "Y Qiao"
      ],
      "year": "2024",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "27",
      "title": "Seeking salient facial regions for cross-database micro-expression recognition",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "J Liu",
        "M Wei"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "28",
      "title": "Sparse mdmo: Learning a discriminative feature for spontaneous micro-expression recognition",
      "authors": [
        "Y.-J Liu",
        "B.-J Li",
        "Y.-K Lai"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Bi-modality fusion for emotion recognition in the wild",
      "authors": [
        "S Li",
        "W Zheng",
        "Y Zong",
        "C Lu",
        "C Tang",
        "X Jiang",
        "J Liu",
        "W Xia"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "30",
      "title": "A novel microexpression recognition approach using attention-based magnificationadaptive networks",
      "authors": [
        "M Wei",
        "W Zheng",
        "Y Zong",
        "X Jiang",
        "C Lu",
        "J Liu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Efficient neural architecture search for emotion recognition",
      "authors": [
        "M Verma",
        "M Mandal",
        "S Reddy",
        "Y Meedimale",
        "S Vipparthi"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "32",
      "title": "Learnet: Dynamic imaging network for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh",
        "S Murala"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "33",
      "title": "Feature representation learning with adaptive displacement generation and transformer fusion for micro-expression recognition",
      "authors": [
        "Z Zhai",
        "J Zhao",
        "C Long",
        "W Xu",
        "S He",
        "H Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Selfme: Self-supervised motion learning for micro-expression recognition",
      "authors": [
        "X Fan",
        "X Chen",
        "M Jiang",
        "A Shahid",
        "H Yan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Joint local and global information learning with single apex frame detection for micro-expression recognition",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "36",
      "title": "Block division convolutional network with implicit deep features augmentation for microexpression recognition",
      "authors": [
        "B Chen",
        "K.-H Liu",
        "Y Xu",
        "Q.-Q Wu",
        "J.-F Yao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "38",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "39",
      "title": "End-to-end object detection with transformers",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "End-to-end object detection with transformers"
    },
    {
      "citation_id": "40",
      "title": "Benchmarking detection transfer learning with vision transformers",
      "authors": [
        "Y Li",
        "S Xie",
        "X Chen",
        "P Dollar",
        "K He",
        "R Girshick"
      ],
      "year": "2021",
      "venue": "Benchmarking detection transfer learning with vision transformers",
      "arxiv": "arXiv:2111.11429"
    },
    {
      "citation_id": "41",
      "title": "Dual modality prompt tuning for vision-language pre-trained model",
      "authors": [
        "Y Xing",
        "Q Wu",
        "D Cheng",
        "S Zhang",
        "G Liang",
        "P Wang",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "Visual prompt multimodal tracking",
      "authors": [
        "J Zhu",
        "S Lai",
        "X Chen",
        "D Wang",
        "H Lu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "43",
      "title": "Spatiotemporal prompting network for robust video feature extraction",
      "authors": [
        "G Sun",
        "C Wang",
        "Z Zhang",
        "J Deng",
        "S Zafeiriou",
        "Y Hua"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Vision transformer adapter for dense predictions",
      "authors": [
        "Z Chen",
        "Y Duan",
        "W Wang",
        "J He",
        "T Lu",
        "J Dai",
        "Y Qiao"
      ],
      "year": "2022",
      "venue": "Vision transformer adapter for dense predictions",
      "arxiv": "arXiv:2205.08534"
    },
    {
      "citation_id": "45",
      "title": "Vl-adapter: Parameter-efficient transfer learning for vision-and-language tasks",
      "authors": [
        "Y.-L Sung",
        "J Cho",
        "M Bansal"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "St-adapter: Parameterefficient image-to-video transfer learning",
      "authors": [
        "J Pan",
        "Z Lin",
        "X Zhu",
        "J Shao",
        "H Li"
      ],
      "year": "2022",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "47",
      "title": "Action recognition with dynamic image networks",
      "authors": [
        "H Bilen",
        "B Fernando",
        "E Gavves",
        "A Vedaldi"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "48",
      "title": "Can micro-expression be recognized based on single apex frame?",
      "authors": [
        "Y Li",
        "X Huang",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "49",
      "title": "Eulerian video magnification for revealing subtle changes in the world",
      "authors": [
        "H.-Y Wu",
        "M Rubinstein",
        "E Shih",
        "J Guttag",
        "F Durand",
        "W Freeman"
      ],
      "year": "2012",
      "venue": "ACM transactions on graphics (TOG)"
    },
    {
      "citation_id": "50",
      "title": "Off-apexnet on micro-expression recognition system",
      "authors": [
        "Y Gan",
        "S Liong",
        "W Yau",
        "Y Huang",
        "T Ken"
      ],
      "year": "2019",
      "venue": "Signal Process. Image Commun"
    },
    {
      "citation_id": "51",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "52",
      "title": "Micro-expression recognition with expression-state constrained spatio-temporal feature representations",
      "authors": [
        "D Kim",
        "W Baddar",
        "Y Ro"
      ],
      "year": "2016",
      "venue": "Acm on Multimedia Conference"
    },
    {
      "citation_id": "53",
      "title": "Dual-stream shallow networks for facial micro-expression recognition",
      "authors": [
        "H Khor",
        "J See",
        "S Liong",
        "R Phan",
        "W Lin"
      ],
      "year": "2019",
      "venue": "ICIP"
    },
    {
      "citation_id": "54",
      "title": "Dynamic micro-expression recognition using knowledge distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Micron-bert: Bert-based facial micro-expression recognition",
      "authors": [
        "X.-B Nguyen",
        "C Duong",
        "X Li",
        "S Gauch",
        "H.-S Seo",
        "K Luu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "56",
      "title": "Progressively learning from macro-expressions for micro-expression recognition",
      "authors": [
        "F Wang",
        "Y Zong",
        "J Zhu",
        "M Wei",
        "X Xu",
        "C Lu",
        "W Zheng"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "57",
      "title": "Structure representation with adaptive and compact facial graph for micro-expression recognition",
      "authors": [
        "C Li",
        "R Ba",
        "X Wang",
        "M Yu",
        "X Li",
        "D Huang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "58",
      "title": "Learning to rank onset-occurring-offset representations for micro-expression recognition",
      "authors": [
        "J Zhu",
        "Y Zong",
        "J Shi",
        "C Lu",
        "H Chang",
        "W Zheng"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Micro-expression recognition based on facial graph representation learning and facial action unit fusion",
      "authors": [
        "L Lei",
        "T Chen",
        "S Li",
        "J Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "60",
      "title": "Spontaneous facial micro-expression analysis using spatiotemporal local radon-based binary pattern",
      "authors": [
        "X Huang",
        "G Zhao"
      ],
      "year": "2017",
      "venue": "FADS"
    },
    {
      "citation_id": "61",
      "title": "Casme ii: An improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "62",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "A spontaneous micro-expression database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "64",
      "title": "Improved adam optimizer for deep neural networks",
      "authors": [
        "Z Zhang"
      ],
      "year": "2018",
      "venue": "2018 IEEE/ACM 26th international symposium on quality of service (IWQoS)"
    },
    {
      "citation_id": "65",
      "title": "Learning from macroexpression: A micro-expression recognition framework",
      "authors": [
        "B Xia",
        "W Wang",
        "S Wang",
        "E Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "66",
      "title": "Micro-expression recognition enhanced by macroexpression from spatial-temporal domain",
      "authors": [
        "B Xia",
        "S Wang"
      ],
      "year": "2021",
      "venue": "IJCAI"
    },
    {
      "citation_id": "67",
      "title": "N-step pre-training and décalcomanie data augmentation for micro-expression recognition",
      "authors": [
        "C Lee",
        "J Hong",
        "H Jung"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "68",
      "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
      "authors": [
        "L Zhou",
        "Q Mao",
        "X Huang",
        "F Zhang",
        "Z Zhang"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "69",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "70",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju"
      ],
      "year": "2017",
      "venue": "Proceedings"
    },
    {
      "citation_id": "71",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}