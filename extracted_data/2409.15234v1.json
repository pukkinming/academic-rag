{
  "paper_id": "2409.15234v1",
  "title": "Ca-Mhfa: A Context-Aware Multi-Head Factorized Attentive Pooling For Ssl-Based Speaker Verification",
  "published": "2024-09-23T17:30:30Z",
  "authors": [
    "Junyi Peng",
    "Ladislav Mošner",
    "Lin Zhang",
    "Oldřich Plchot",
    "Themos Stafylakis",
    "Lukáš Burget",
    "Jan Černocký"
  ],
  "keywords": [
    "Self-supervised learning",
    "speaker verification",
    "speaker extractor",
    "pooling mechanism",
    "speech classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Self-supervised learning (SSL) models for speaker verification (SV) have gained significant attention in recent years. However, existing SSL-based SV systems often struggle to capture local temporal dependencies and generalize across different tasks. In this paper, we propose context-aware multi-head factorized attentive pooling (CA-MHFA), a lightweight framework that incorporates contextual information from surrounding frames. CA-MHFA leverages grouped, learnable queries to effectively model contextual dependencies while maintaining efficiency by sharing keys and values across groups. Experimental results on the VoxCeleb dataset show that CA-MHFA achieves EERs of 0.42%, 0.48%, and 0.96% on Vox1-O, Vox1-E, and Vox1-H, respectively, outperforming complex models like WavLM-TDNN with fewer parameters and faster convergence. Additionally, CA-MHFA demonstrates strong generalization across multiple SSL models and tasks, including emotion recognition and anti-spoofing, highlighting its robustness and versatility. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Large-scale self-supervised learning (SSL) speech models, such as Wav2vec  [1] , HuBERT  [2] , WavLM  [3] , and their variants  [4] , have significantly advanced various speech-related tasks, including speech recognition (ASR)  [5] , emotion recognition (ER)  [6] , deepfake detection  [7] , and speaker verification (SV)  [8] ,  [9] . These SSL models are pre-trained on extensive speech datasets, which allow them to learn universal representations that are more robust to channel mismatch and noisy conditions compared to traditional acoustic features like Mel-frequency cepstral coefficients (MFCCs) and FBank, especially under low-resource scenarios  [10] ,  [11] .\n\nThe most common approach to adapting these general-purpose models to specific downstream tasks is by fine-tuning the entire pre-trained SSL model with a task-oriented back-end module using labeled downstream data. In the field of SV, since the outputs of SSL models are layer-wise frame-by-frame representations, the taskoriented module (known as the speaker extractor back-end) processes these features to generate utterance-level speaker embeddings, which are then used for speaker identity prediction. It typically includes a frame-level feature extractor, a pooling mechanism, and an utterancelevel feature extractor. For example, in  [8] ,  [12] , a weighted sum of layer-wise SSL outputs replaces traditional acoustic features as input to speaker extractors, such as the x-vector  [13]  and the ECAPA-TDNN models  [14] . These SSL-based SV systems significantly accelerate convergence and boost performance compared to those 1 Code is available at https://github.com/BUTSpeechFIT/wespeaker ssl public based on MFCC features. To further explore the potential of SSL models, a lighter back-end, called multi-head factorized attentive pooling (MHFA)  [15] , was proposed. MHFA employs two sets of weights to model different aspects of input features. This approach has been shown to outperform systems that integrate ECAPA-TDNN on the VoxCeleb dataset.\n\nAlthough MHFA has achieved promising results on the SV task, it typically operates at the utterance level. By processing all frames simultaneously, it ignores the detailed relationships between surrounding frames. This lack of local context limits its ability to capture dynamic and fine-grained temporal dependencies across frames. Additionally, since pre-trained SSL models already possess strong frame-level modeling capabilities by leveraging self-attention mechanisms, incorporating complex and randomly initialized framelevel modules in the back-end can mislead the optimization process, leading to suboptimal performance  [16] . Furthermore, studies on the generalizability of these lightweight back-end modules to other speech classification tasks, such as emotion recognition (ER) and spoofing detection, remain limited. This raises the concern that these modules might be over-optimized for a single task, such as SV  [17] .\n\nTo address the aforementioned challenges, in this paper, we aim to develop a lightweight framework considering context information for SV and can be applied to broader speech classification tasks and various SSL models. The proposed framework, named context-aware multi-head factorized attentive pooling (CA-MHFA), utilizes both past and future speech frames as contextual information to extract utterance-level representations from SSL layer-wise features. Specifically, similar to the self-attention mechanism used in MHFA  [15] , we employ two sets of learnable weights along with a linear layer to generate keys and values. Unlike MHFA, we introduce a set of learnable queries that are grouped, allowing the model to focus on the surrounding frames and better capture contextual dependencies. The attention weights are computed using convolution between the grouped queries and keys. Finally, an attentive pooling layer aggregates all frames from each group, followed by a concatenation and linear layer to compute the speaker embedding. Moreover, we share the single key and value with the grouped queries to simplify the entire pipeline.\n\nThe contributions of our work are as follows: • Compatible architecture: Besides MHFA, our context-aware extension is compatible with other pooling methods, like mean pooling and attentive pooling  [18] , making it extensible for future integration and improvements to existing ones. • State-of-the-art SV performance: We achieve SOTA results on the VoxCeleb dataset using fewer model parameters. With the same pre-trained SSL model, the proposed system outperforms the WavLM-TDNN  [3]  and yields 0.42%, 0.48% and 0.96% EER on Vox1-O, Vox1-E and Vox1-H, respectively. • Strong generalization: We demonstrate the effectiveness of the proposed lightweight back-end module across various speech classification tasks and SSL models, including SV, ER, and deepfake detection, following the SUPERB principle. This is evaluated by integrating with nine popular pre-trained SSL models, including Wav2Vec 2.0, HuBERT, Data2vec, and WavLM.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Context-Aware Multi-Head Factorized Attentive Pooling",
      "text": "In this section, we describe the proposed CA-MHFA in detail. As shown in Fig.  1 , it consists of three main components: a frame-level extractor, a context-aware attentive pooling module, and an utterancelevel extractor.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Frame-Level Extractor With Compression",
      "text": "The goal of text-independent speaker embedding extractors is to capture speaker-related characteristics independent of content. However, SSL models are designed as general-purpose models, not specifically for speaker embedding extraction. As a result, the representations they extract often contain a mix of both speaker-related and content-related information  [12] . Although the weighted sum method  [12] , which uses learnable weights for each layer, can help mitigate this issue, it treats content and speaker information within the same layer equally. Consequently, when the method reduces the weights for layers rich in content-related information, it also unintentionally reduces the speaker-related information from those layers. To effectively utilize and refine speaker-related information from all layers, the proposed CA-MHFA is designed to factorize SSL representations into multiple subsets, focusing on extracting speakerspecific features while minimizing interference from content.\n\nTo describe the proposed CA-MHFA, we use the query/key/value abstraction for the attention mechanism  [19] . For a SSL model, we define outputs from its different layers as Z = {z0, ..., zN }, zn ∈ R T ×F , where T is the dimention of the frame level, F is the feature dimension of outputs from Transformer blocks, and N denotes the number of Transformer blocks inside the SSL model. To construct multiple subsets, we employ two sets of normalized weights (factors) ω k n and ω v n along with a linear layer to separately aggregate and compress layer-wise outputs to produce the matrices of keys K and values V, respectively:\n\nwhere S k , S v ∈ R F ×D denote transformation matrices responsible for compressing the weighted frame-level features into keys and values, respectively, and D represents the feature dimension after compression.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Context-Aware Attentive Pooling",
      "text": "To effectively model the information within the key flow, we introduce a set of global, input-agnostic, learnable vectors, denoted as queries Q ∈ R LG×D . For contextual modeling, queries Q are divided into G groups, with each group's vectors denoted as q g = [q g 1 , .., q g L ], q g ∈ R L×D , where g ∈ [1, G] indexes the groups (heads) and each group q g comprises L trainable vectors. For computational efficiency, each of the groups q g shares the same key and value.\n\nTo model contextual information, we consider R frames on either side of the current frame, resulting in a total of L neighboring frames used to compute attention weights for the current frame. The current frame refers to the frame in focus during attention calculation, with neighboring frames providing additional context information. Here, R = Floor((L -1)/2). The attention weights of each group a g ∈ R T ×1 are further computed by q g and K as follows:\n\nwhere a g t represents the attentive weight given to the current t-th frame after considering the surrounding frames L, and ⊤ denotes transpose. From an implementation point of view, this process can be viewed as a 2D convolution operation. Specifically, the input K consists of D channels, and the output channels correspond to the number of groups G. The kernel size is (L, D). In this way, Q is reshaped into grouped vectors and serves further as the G convolutional kernels, each with the shape (L, D).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Utterance-Level Extractor",
      "text": "Next, each group's speaker representations c g ∈ R 1×D are aggregated via attention weights a g and then concatenated as:\n\nwhere c ∈ R 1×GD aggregates the speaker representation at the utterance level from all subspaces (heads). Finally, the prediction of the speaker label is performed by passing the representation through a subsequent linear layer, L2 normalization, and a classification layer, which is needed only during training. During testing, the output of the L2 normalized linear layer is considered as the speaker embedding e.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Generalization Of Other Works",
      "text": "The CA-MHFA extends the MHFA framework by introducing flexibility in the attention mechanism. When we set L = 1, CA-MHFA simplifies to the original MHFA as presented in  [15] .\n\nIn scenarios utilizing average pooling, setting q g l to a constant non-trainable zero vector implies that the similarity of each frame to every group query equals zero. After softmax normalization, this results in equal weights of 1/T for each frame feature, in this case, CA-MHFA degenerates to average mean pooling.\n\nCompared to self-attentive pooling  [18] , setting G = 1 implies that Q comprises a single group that contains only one D-dimensional trainable query. In this configuration, only a single set of weights is computed, making CA-MHFA equivalent to self-attentive pooling. We explored two types of evaluation: 1) Full fine-tuning condition: Both the SSL and back-end modules are trainable during fine-tuning. 2) SUPERB-style: We follow the SUPERB principle  [12]  and only update the back-end module during the fine-tuning stage, while keeping the SSL model frozen.\n\n2) Datasets: We first focus on exploring the efficiency of the lightweight CA-MHFA for the SV task, involving two of the aforementioned evaluation methods: 1) For full fine-tuning conditions, to ensure we have enough data for finetuning SSL model, we use the development set of VoxCeleb2 for training, then use Vox1-O, Vox1-E, and Vox1-H trials for evaluating. 2) For SUPERB-style, the training set is VoxCeleb1 and the evaluation set is Vox1-O following  [12] . We also explored the generalizability of CA-MHFA to two other speech classification tasks based on the SUPERB-style evaluation: For R, the models are trained and evaluated on the IEMOCAP corpus  [23] , which consists of approximately 12 hours of recordings in five sessions. Regarding deepfake detection, we use the ASVspoof 2019 LA  [24]  dataset.\n\n3) Implementation details: For all three aforementioned tasks, we utilize two scales of pre-trained SSL models listed in Table  III:  1) The BASE models, consisting of a CNN encoder and 12 layers of Transformers with approximately 94M parameters. The dimension of the Transformer output F is 768. 2) The Large models, which include 24 layers of Transformers with approximately 316M parameters. In the full fine-tuning configurations, the extracted speaker embedding dimension is 256. For the SV task under full fine-tune condition, we employ the AAM-softmax  [25]  loss function with a margin of 0.2 and a scaling factor of 32. During gradient updates, the gradients of the SSL model are scaled by 0.1. To further boost performance, we adopt large margin tuning  [26]  with longer (5-second) segments and a margin of 0.5 for an additional 3 training epochs. The initial learning rate is set to 1e-4 and decreases to 1e-6 by the 10th epoch using the AdamW optimizer. All fine-tuning datasets are augmented with with MUSAN  [27]  and room impulse responses. In addition, we used speaker-wise adaptive score normalization and Quality Measure Functions  [26]  to calibrate the scores.\n\nWhen evaluating the generalizability of the model across different tasks and upstream models, we adopt the SUPERB-style evaluation: For SV, all systems use the AM-softmax loss function. To keep For ER, cross-entropy (CE) loss is employed for optimization, and mean pooling is performed via a weighted sum across layer-wise features, followed by mean pooling along the time dimension and a final linear layer. For deepfake detection, CE loss is used, with a learning rate set to 1e-4 using the Adam optimizer, following  [7] . For the backend module, we use the weighted sum with LLGF (LCNN-BLSTM) when MHFA is not utilized, as this structure has shown the best performance when the SSL model is frozen  [7] . No augmentation methods are applied. 4) Performance Metrics: For SV and deepfake detection, equal error rate (EER) is employed to measure the performancesER, we use a leave-one-session-out 5-fold cross-validation to report averaged accuracy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Hyper-Parameter Analysis Of Ca-Mhfa",
      "text": "We present here a hyper-parameter analysis of CA-MHFA for the SV task under the full fine-tuning evaluation. We focus on analyzing different context lengths (L) and numbers of heads (G) in terms of EER (%) on the VoxCeleb dataset. WavLM BASE Plus is used as the pre-trained SSL model  [3] . The results are shown in Table  I .\n\nIt is observed that increasing the context length and the number of heads improves the performance of CA-MHFA, especially on Vox1-E and Vox1-H. This may be due to the fact that while the SV task does not require extensive context, considering adjacent surrounding frames can be beneficial. Compared to MHFA, CA-MHFA consistently outperforms MHFA in most configurations.\n\nWe also compared systems that applied convolution operations to both the keys and values branches, as shown in the last row of Table  I . Specifically, the values branches used 1D convolution operations with a kernel size of 5, where the input and output channels were set to D. The results show that this configuration performs worse compared to systems that applied convolution only to the keys branch. This suggests that contextual information may be more beneficial for the content-related subspace than for the speaker-related subspace. One possible explanation is that context in the content subspace helps capture the speaker's speaking style, such as the structuring of words, while the speaker subspace focuses on features that are invariant to the speaker's identity and, thus, requires less contextual attention.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Comparison With State-Of-The-Art Sv Systems",
      "text": "Under the optimal hyperparameters identified in the previous section (G = 64, L = 9), we compared the proposed CA-MHFA with other state-of-the-art SV systems, including both SSL-based models and speaker extractors trained from scratch, as shown in Table  II .\n\nIt is observed that WavLM Large combined with CA-MHFA outperforms the SOTA model trained from scratch like ECAPA-TDNN and deep ResNet models (e.g., ResNet221 and ResNet293) with fewer training steps (23 epochs vs. 150 epochs). Additionally, despite the WavLM Large model having a much larger number of parameters than ResNet293, the FLOPs are comparable when using 2 seconds of speech as input. This indicates that after convergence, the inference speed of WavLM Large + CA-MHFA is comparable to that of ResNet293. Furthermore, when comparing SSL models of similar scales, WavLM + CA-MHFA consistently outperforms other SSL-based SV systems with a lighter speaker extractor back-end. For example, under the same WeSpeaker framework, the proposed CA-MHFA outperforms ECAPA-TDNN systems when using both WavLM Large and WavLM Base Plus.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Ca-Mhfa On Other Tasks",
      "text": "In this section, we explored the generalizability of the proposed CA-MHFA across three downstream tasks: SV, ER, and deepfake detection. The evaluation follows the SUPERB-style as we introduced in section III-A. The results are summarized in Table  III .\n\nIn general, the proposed CA-MHFA consistently improves performance in multiple tasks and upstream models over baseline systems and MHFA. Specifically, for SV, when using the same SSL features as input, CA-MHFA consistently outperforms the x-vector and ECAPA-TDNN back-ends with fewer parameters (2.3M vs. 9.2M and 7.0M, respectively). In addition, CA-MHFA shows notable performance improvements in ER tasks. In the deepfake detection, compared to LLGF, CA-MHFA demonstrates Considerable improvements in most cases. Our results demonstrate consistent improvements across different model scales, highlighting its robustness and versatility.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "In this paper, we propose a novel lightweight back-end module named CA-MHFA for pre-trained SSL-based speaker verification, which can be easily extended to broader speech classification tasks, including emotion recognition and deepfake detection. CA-MHFA enhances contextual modeling by considering frames surrounding the current frame when computing attention weights, leading to more stable and discriminative utterance-level representations. We conducted comprehensive experiments on the VoxCeleb, IEMOCAP, and ASVspoof2019 datasets. The results demonstrate that CA-MHFA outperforms other downstream back-end modules while maintaining lower parameter counts.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of the pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,",
      "page": 2
    },
    {
      "caption": "Figure 1: , it consists of three main components: a frame-level",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Greece\nCzechia\nCzechia": "Abstract—Self-supervised learning (SSL) models\nfor speaker verifica-\nbased on MFCC features. To further\nexplore\nthe potential of SSL"
        },
        {
          "Greece\nCzechia\nCzechia": "tion (SV) have\ngained significant\nattention in recent\nyears. However,"
        },
        {
          "Greece\nCzechia\nCzechia": "models,\na\nlighter\nback-end,\ncalled multi-head\nfactorized\nattentive"
        },
        {
          "Greece\nCzechia\nCzechia": "existing SSL-based SV systems often struggle to capture local\ntemporal"
        },
        {
          "Greece\nCzechia\nCzechia": "pooling (MHFA)\n[15], was proposed. MHFA employs\ntwo sets of"
        },
        {
          "Greece\nCzechia\nCzechia": "dependencies and generalize across different tasks. In this paper, we pro-"
        },
        {
          "Greece\nCzechia\nCzechia": "weights\nto model different aspects of\ninput\nfeatures. This approach"
        },
        {
          "Greece\nCzechia\nCzechia": "pose context-aware multi-head factorized attentive pooling (CA-MHFA),"
        },
        {
          "Greece\nCzechia\nCzechia": "has been shown to outperform systems that\nintegrate ECAPA-TDNN\na lightweight\nframework that\nincorporates contextual\ninformation from"
        },
        {
          "Greece\nCzechia\nCzechia": "surrounding frames. CA-MHFA leverages grouped,\nlearnable queries to\non the VoxCeleb dataset."
        },
        {
          "Greece\nCzechia\nCzechia": "effectively model\ncontextual dependencies while maintaining\nefficiency"
        },
        {
          "Greece\nCzechia\nCzechia": "Although MHFA has\nachieved\npromising\nresults\non\nthe\nSV"
        },
        {
          "Greece\nCzechia\nCzechia": "by sharing keys and values across groups. Experimental results on the"
        },
        {
          "Greece\nCzechia\nCzechia": "task,\nit\ntypically operates\nat\nthe utterance\nlevel. By processing all"
        },
        {
          "Greece\nCzechia\nCzechia": "VoxCeleb dataset show that CA-MHFA achieves EERs of 0.42%, 0.48%,"
        },
        {
          "Greece\nCzechia\nCzechia": "frames simultaneously,\nit\nignores the detailed relationships between\nand 0.96% on Vox1-O, Vox1-E, and Vox1-H, respectively, outperforming"
        },
        {
          "Greece\nCzechia\nCzechia": "complex models\nlike WavLM-TDNN with fewer parameters and faster\nsurrounding\nframes. This\nlack\nof\nlocal\ncontext\nlimits\nits\nability"
        },
        {
          "Greece\nCzechia\nCzechia": "convergence. Additionally, CA-MHFA demonstrates strong generalization"
        },
        {
          "Greece\nCzechia\nCzechia": "to capture dynamic\nand fine-grained temporal dependencies\nacross"
        },
        {
          "Greece\nCzechia\nCzechia": "across multiple SSL models and tasks,\nincluding emotion recognition and"
        },
        {
          "Greece\nCzechia\nCzechia": "frames. Additionally,\nsince pre-trained SSL models already possess"
        },
        {
          "Greece\nCzechia\nCzechia": "anti-spoofing, highlighting its robustness and versatility. 1"
        },
        {
          "Greece\nCzechia\nCzechia": "strong frame-level modeling capabilities by leveraging self-attention"
        },
        {
          "Greece\nCzechia\nCzechia": "Index Terms—Self-supervised learning,\nspeaker verification,\nspeaker"
        },
        {
          "Greece\nCzechia\nCzechia": "mechanisms,\nincorporating complex and randomly initialized frame-\nextractor, pooling mechanism, speech classification"
        },
        {
          "Greece\nCzechia\nCzechia": "level modules in the back-end can mislead the optimization process,"
        },
        {
          "Greece\nCzechia\nCzechia": "leading\nto\nsuboptimal\nperformance\n[16]. Furthermore,\nstudies\non\nI.\nINTRODUCTION"
        },
        {
          "Greece\nCzechia\nCzechia": "the generalizability of\nthese lightweight back-end modules\nto other"
        },
        {
          "Greece\nCzechia\nCzechia": "Large-scale self-supervised learning (SSL) speech models, such as"
        },
        {
          "Greece\nCzechia\nCzechia": "speech classification tasks,\nsuch as\nemotion recognition (ER)\nand"
        },
        {
          "Greece\nCzechia\nCzechia": "Wav2vec [1], HuBERT [2], WavLM [3], and their variants [4], have"
        },
        {
          "Greece\nCzechia\nCzechia": "spoofing detection, remain limited. This raises the concern that\nthese"
        },
        {
          "Greece\nCzechia\nCzechia": "significantly advanced various speech-related tasks,\nincluding speech"
        },
        {
          "Greece\nCzechia\nCzechia": "modules might be over-optimized for a single task, such as SV [17]."
        },
        {
          "Greece\nCzechia\nCzechia": "recognition (ASR) [5], emotion recognition (ER) [6], deepfake detec-"
        },
        {
          "Greece\nCzechia\nCzechia": "To address\nthe aforementioned challenges,\nin this paper, we aim"
        },
        {
          "Greece\nCzechia\nCzechia": "tion [7], and speaker verification (SV) [8], [9]. These SSL models are"
        },
        {
          "Greece\nCzechia\nCzechia": "to develop a lightweight\nframework considering context\ninformation"
        },
        {
          "Greece\nCzechia\nCzechia": "pre-trained on extensive speech datasets, which allow them to learn"
        },
        {
          "Greece\nCzechia\nCzechia": "for SV and can be applied to broader speech classification tasks and"
        },
        {
          "Greece\nCzechia\nCzechia": "universal\nrepresentations\nthat are more robust\nto channel mismatch"
        },
        {
          "Greece\nCzechia\nCzechia": "various SSL models. The proposed framework, named context-aware"
        },
        {
          "Greece\nCzechia\nCzechia": "and noisy conditions compared to traditional acoustic features\nlike"
        },
        {
          "Greece\nCzechia\nCzechia": "multi-head\nfactorized\nattentive\npooling\n(CA-MHFA),\nutilizes\nboth"
        },
        {
          "Greece\nCzechia\nCzechia": "Mel-frequency cepstral coefficients (MFCCs) and FBank, especially"
        },
        {
          "Greece\nCzechia\nCzechia": "past and future speech frames as contextual\ninformation to extract"
        },
        {
          "Greece\nCzechia\nCzechia": "under\nlow-resource scenarios [10],\n[11]."
        },
        {
          "Greece\nCzechia\nCzechia": "utterance-level representations from SSL layer-wise features. Specif-"
        },
        {
          "Greece\nCzechia\nCzechia": "The most\ncommon\napproach\nto\nadapting\nthese\ngeneral-purpose"
        },
        {
          "Greece\nCzechia\nCzechia": "ically,\nsimilar\nto the self-attention mechanism used in MHFA [15],"
        },
        {
          "Greece\nCzechia\nCzechia": "models\nto\nspecific\ndownstream tasks\nis\nby fine-tuning\nthe\nentire"
        },
        {
          "Greece\nCzechia\nCzechia": "we employ two sets of\nlearnable weights along with a linear\nlayer"
        },
        {
          "Greece\nCzechia\nCzechia": "pre-trained SSL model with a task-oriented back-end module using"
        },
        {
          "Greece\nCzechia\nCzechia": "to generate keys and values. Unlike MHFA, we introduce a set of"
        },
        {
          "Greece\nCzechia\nCzechia": "labeled downstream data.\nIn the field of SV,\nsince\nthe outputs of"
        },
        {
          "Greece\nCzechia\nCzechia": "learnable queries\nthat are grouped, allowing the model\nto focus on"
        },
        {
          "Greece\nCzechia\nCzechia": "SSL models are layer-wise frame-by-frame representations,\nthe task-"
        },
        {
          "Greece\nCzechia\nCzechia": "the surrounding frames and better capture contextual dependencies."
        },
        {
          "Greece\nCzechia\nCzechia": "oriented module (known as the speaker extractor back-end) processes"
        },
        {
          "Greece\nCzechia\nCzechia": "The\nattention weights\nare\ncomputed\nusing\nconvolution\nbetween"
        },
        {
          "Greece\nCzechia\nCzechia": "these features to generate utterance-level speaker embeddings, which"
        },
        {
          "Greece\nCzechia\nCzechia": "queries\nthe\ngrouped\nand\nkeys. Finally,\nan\nattentive\npooling\nlayer"
        },
        {
          "Greece\nCzechia\nCzechia": "are then used for speaker\nidentity prediction.\nIt\ntypically includes a"
        },
        {
          "Greece\nCzechia\nCzechia": "aggregates all\nframes from each group,\nfollowed by a concatenation"
        },
        {
          "Greece\nCzechia\nCzechia": "frame-level feature extractor, a pooling mechanism, and an utterance-"
        },
        {
          "Greece\nCzechia\nCzechia": "and linear\nlayer\nto compute the speaker embedding. Moreover, we"
        },
        {
          "Greece\nCzechia\nCzechia": "level\nfeature extractor. For example,\nin [8],\n[12], a weighted sum of"
        },
        {
          "Greece\nCzechia\nCzechia": "share the single key and value with the grouped queries to simplify"
        },
        {
          "Greece\nCzechia\nCzechia": "layer-wise SSL outputs replaces traditional acoustic features as input"
        },
        {
          "Greece\nCzechia\nCzechia": "the entire pipeline."
        },
        {
          "Greece\nCzechia\nCzechia": "to speaker\nextractors,\nsuch as\nthe x-vector\n[13]\nand the ECAPA-"
        },
        {
          "Greece\nCzechia\nCzechia": "The contributions of our work are as follows:"
        },
        {
          "Greece\nCzechia\nCzechia": "TDNN models\n[14].\nThese\nSSL-based\nSV systems\nsignificantly"
        },
        {
          "Greece\nCzechia\nCzechia": "• A lightweight back-end module: We propose\na\nlightweight"
        },
        {
          "Greece\nCzechia\nCzechia": "accelerate\nconvergence\nand\nboost\nperformance\ncompared\nto\nthose"
        },
        {
          "Greece\nCzechia\nCzechia": "back-end, CA-MHFA,\nto extract\nrepresentations\nthat\nconsider"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "CA-MHFA). During the fine-tuning,\nthe SSL model and cascaded speaker extractor are jointly optimized."
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "• Compatible\narchitecture: Besides MHFA, our\ncontext-aware\nTo describe the proposed CA-MHFA, we use the query/key/value"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "extension is compatible with other pooling methods,\nlike mean\nabstraction for\nthe attention mechanism [19]. For a SSL model, we"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "pooling\nand\nattentive\npooling\n[18], making\nit\nextensible\nfor\ndefine outputs\nfrom its different\nlayers as Z = {z0, ..., zN }, zn ∈"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "future integration and improvements to existing ones.\nRT ×F , where T is the dimention of the frame level, F is the feature"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "• State-of-the-art SV performance: We achieve SOTA results on\ndimension of outputs\nfrom Transformer blocks, and N denotes\nthe"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "the VoxCeleb dataset using fewer model parameters. With the\nnumber of Transformer blocks\ninside the SSL model. To construct"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "same pre-trained SSL model,\nthe proposed system outperforms\nmultiple subsets, we employ two sets of normalized weights (factors)"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "ωk\nthe WavLM-TDNN [3]\nand yields 0.42%, 0.48% and 0.96%\nand ωv\nalong with a\nlinear\nlayer\nto separately aggregate\nand\nn\nn"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "EER on Vox1-O, Vox1-E and Vox1-H,\nrespectively.\ncompress layer-wise outputs to produce the matrices of keys K and"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "• Strong generalization: We demonstrate the effectiveness of the\nvalues V,\nrespectively:"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "proposed lightweight back-end module\nacross various\nspeech"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "(cid:33)\n(cid:32) N"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "classification\ntasks\nand SSL models,\nincluding SV, ER,\nand"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "Sk\nK =\nωk\nnzn"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "(cid:88) n\ndeepfake detection,\nfollowing the SUPERB principle. This\nis"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "=0"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "evaluated by integrating with nine popular pre-trained SSL mod-\n(1)"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "(cid:32) N\n(cid:33)"
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "els,\nincluding Wav2Vec 2.0, HuBERT, Data2vec, and WavLM."
        },
        {
          "Fig. 1.\nThe architecture of\nthe pre-trained model and attached lightweight speaker extractor back-end (context-aware multi-head factorized attentive pooling,": "V =\nSv,\nωv"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": "HYPER-PARAMETER ANALYSIS OF CA-MHFA WITH DIFFERENT CONTEXT",
          "TABLE II": ""
        },
        {
          "TABLE I": "LENGTHS L AND NUMBERS OF HEADS G IN TERMS OF EER (%) ON",
          "TABLE II": ""
        },
        {
          "TABLE I": "VOXCELEB DATASET. † DENOTES THE SYSTEM APPLYING CONVOLUTION",
          "TABLE II": ""
        },
        {
          "TABLE I": "IN BOTH keys AND values.",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": ""
        },
        {
          "TABLE I": "Methods\n#Param\n#Context\n#Head\nVox1-O\nVox1-E",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": ""
        },
        {
          "TABLE I": "MHFA\n0.72M\n1\n16\n0.79\n0.85",
          "TABLE II": "FLOPs"
        },
        {
          "TABLE I": "",
          "TABLE II": ""
        },
        {
          "TABLE I": "MHFA\n1.25M\n1\n32\n0.79\n0.81",
          "TABLE II": ""
        },
        {
          "TABLE I": "MHFA\n2.30M\n1\n64\n0.76\n0.79",
          "TABLE II": "1.04G"
        },
        {
          "TABLE I": "",
          "TABLE II": "21.29G"
        },
        {
          "TABLE I": "CA-MHFA\n1.25M\n3\n32\n0.76\n0.76",
          "TABLE II": "28.10G"
        },
        {
          "TABLE I": "CA-MHFA\n1.26M\n5\n32\n0.73\n0.76",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "-"
        },
        {
          "TABLE I": "CA-MHFA\n2.30M\n3\n64\n0.73\n0.74",
          "TABLE II": ""
        },
        {
          "TABLE I": "",
          "TABLE II": "-"
        },
        {
          "TABLE I": "0.69\nCA-MHFA\n2.31M\n5\n64\n0.74",
          "TABLE II": "-"
        },
        {
          "TABLE I": "0.72\nCA-MHFA\n2.36M\n9\n64\n0.70",
          "TABLE II": "-"
        },
        {
          "TABLE I": "CA-MHFA\n2.41M\n17\n64\n0.69\n0.74",
          "TABLE II": "-"
        },
        {
          "TABLE I": "",
          "TABLE II": "-"
        },
        {
          "TABLE I": "CA-MHFA†\n2.32M\n5\n64\n0.72\n0.76",
          "TABLE II": "25.79G"
        },
        {
          "TABLE I": "",
          "TABLE II": "11.05G"
        },
        {
          "TABLE I": "",
          "TABLE II": "11.05G"
        },
        {
          "TABLE I": "",
          "TABLE II": "25.79G"
        },
        {
          "TABLE I": "RT ×1 are further computed by qg and K as follows:",
          "TABLE II": "25.79G"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RT ×1 are further computed by qg and K as follows:": "(cid:16) 1\n(cid:80)R",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "exp\nt+j\nj k⊤\nj=−R qg\nL",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "ag\n(2)",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "III. EXPERIMENTS"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "(cid:17) ,",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "(cid:16) 1\n(cid:80)T\n(cid:80)R",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "mk⊤\ni=1 exp\nm=−R qg",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "i+m\nL",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "A.\nSetup"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "1) Two Types of Evaluation: We explored two types of evaluation:"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "where ag\nrepresents\nthe\nattentive weight given to the\ncurrent\nt-th\nt",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "1) Full fine-tuning condition: Both the SSL and back-end modules"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "frame\nafter\nconsidering the\nsurrounding frames L,\nand ⊤ denotes",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "are\ntrainable during fine-tuning. 2) SUPERB-style: We\nfollow the"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "transpose. From an implementation point of view,\nthis process can",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "SUPERB principle [12] and only update the back-end module during"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "be\nviewed\nas\na\n2D convolution\noperation. Specifically,\nthe\ninput",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "the fine-tuning stage, while keeping the SSL model\nfrozen."
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "K consists of D channels,\nand the output\nchannels\ncorrespond to",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "2) Datasets: We first\nfocus\non\nexploring\nthe\nefficiency\nof\nthe"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "the number of groups G. The kernel\nsize\nis\n(L, D).\nIn this way,",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "lightweight CA-MHFA for\nthe SV task,\ninvolving two of\nthe afore-"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "Q is\nreshaped\ninto\ngrouped\nvectors\nand\nserves\nfurther\nas\nthe G",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "mentioned evaluation methods: 1) For\nfull fine-tuning conditions,\nto"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "convolutional kernels, each with the shape (L, D).",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "ensure we have enough data for finetuning SSL model, we use the"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "development set of VoxCeleb2 for training,\nthen use Vox1-O, Vox1-E,"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "C. Utterance-Level Extractor",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "and Vox1-H trials for evaluating. 2) For SUPERB-style,\nthe training"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "R1×D",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "set\nis VoxCeleb1 and the\nevaluation set\nis Vox1-O following [12]."
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "cg\n∈\nNext,\neach\ngroup’s\nspeaker\nrepresentations\nare",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "We\nalso explored the generalizability of CA-MHFA to two other"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "aggregated via attention weights ag and then concatenated as:",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "speech classification tasks based on the SUPERB-style\nevaluation:"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "(cid:88)T",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "For R,\nthe models\nare\ntrained\nand\nevaluated\non\nthe\nIEMOCAP"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "ag\ncg =\nt vt,",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "t=1",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "(3)",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "corpus [23], which consists of approximately 12 hours of recordings"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "c1, ..., cG(cid:17)\n,\nc = concat",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "in five sessions. Regarding deepfake detection, we use the ASVspoof"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "2019 LA [24] dataset."
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "where\nc ∈ R1×GD aggregates\nthe\nspeaker\nrepresentation\nat\nthe",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "3)\nImplementation details: For all\nthree aforementioned tasks, we"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "utterance level\nfrom all subspaces (heads). Finally,\nthe prediction of",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "utilize two scales of pre-trained SSL models\nlisted in Table III: 1)"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "the speaker label\nis performed by passing the representation through",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "The BASE models, consisting of a CNN encoder and 12 layers of"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "a subsequent linear layer, L2 normalization, and a classification layer,",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "Transformers with approximately 94M parameters. The dimension of"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "which is needed only during training. During testing, the output of the",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "the Transformer output F is 768. 2) The Large models, which include"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "L2 normalized linear layer is considered as the speaker embedding e.",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "24 layers of Transformers with approximately 316M parameters.\nIn"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "the full fine-tuning configurations,\nthe extracted speaker embedding"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "dimension is 256. For the SV task under full fine-tune condition, we"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "D. Generalization of Other Works",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": ""
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "employ the AAM-softmax [25]\nloss\nfunction with a margin of 0.2"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "The CA-MHFA extends\nthe MHFA framework\nby\nintroducing",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "and a\nscaling factor of 32. During gradient updates,\nthe gradients"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "flexibility in the\nattention mechanism. When we\nset L = 1, CA-",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "of\nthe SSL model are scaled by 0.1. To further boost performance,"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "MHFA simplifies to the original MHFA as presented in [15].",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "we adopt\nlarge margin tuning [26] with longer\n(5-second) segments"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "In scenarios utilizing average pooling,\nsetting qg\nto a\nconstant\nl",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "and a margin of 0.5 for an additional 3 training epochs. The initial"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "non-trainable zero vector\nimplies\nthat\nthe similarity of each frame",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "learning rate is set\nto 1e-4 and decreases to 1e-6 by the 10th epoch"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "to every group query equals zero. After softmax normalization,\nthis",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "using the AdamW optimizer. All fine-tuning datasets are augmented"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "results in equal weights of 1/T for each frame feature,\nin this case,",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "with with MUSAN [27] and room impulse responses. In addition, we"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "CA-MHFA degenerates to average mean pooling.",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "used speaker-wise adaptive score normalization and Quality Measure"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "Compared to self-attentive pooling [18], setting G = 1 implies that",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "Functions [26]\nto calibrate the scores."
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "Q comprises a single group that contains only one D-dimensional",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "When evaluating the generalizability of the model across different"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "trainable query.\nIn this configuration, only a single set of weights is",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "tasks and upstream models, we adopt\nthe SUPERB-style evaluation:"
        },
        {
          "RT ×1 are further computed by qg and K as follows:": "computed, making CA-MHFA equivalent\nto self-attentive pooling.",
          "+ LMF/QMF\n25.79G\n316M + 2.3M\n0.42\n0.48\n0.96": "For SV,\nall\nsystems\nuse\nthe AM-softmax\nloss\nfunction. To\nkeep"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "WavLM Large\n4.87\n2.17\n1.78\n1.77",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "67.92\n69.72\n71.52\n1.54\n2.23\n1.21"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "consistent with\nthe\nx-vector\nimplementation within\nthe SUPERB,",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "other state-of-the-art SV systems,\nincluding both SSL-based models"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "the speaker embedding dimensions of ECAPA-TDNN, MHFA, and",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "and speaker extractors trained from scratch, as shown in Table II."
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "CA-MHFA, are set\nto 512. The MHFA model uses 8 heads, while",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "It\nis\nobserved\nthat WavLM Large\ncombined with CA-MHFA"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "the CA-MHFA model uses 8 heads with a context\nlength of 9. For",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "outperforms\nthe SOTA model\ntrained\nfrom scratch\nlike ECAPA-"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "ER, cross-entropy (CE) loss is employed for optimization, and mean",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "TDNN and deep ResNet models\n(e.g., ResNet221 and ResNet293)"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "pooling is performed via a weighted sum across layer-wise features,",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "with fewer\ntraining steps\n(23 epochs vs. 150 epochs). Additionally,"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "followed by mean pooling along the time dimension and a final linear",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "despite the WavLM Large model having a much larger number of"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "layer. For deepfake detection, CE loss\nis used, with a learning rate",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "parameters than ResNet293,\nthe FLOPs are comparable when using"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "set\nto 1e-4 using the Adam optimizer,\nfollowing [7]. For\nthe back-",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "2 seconds of speech as input. This indicates that after convergence,"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "end module, we use the weighted sum with LLGF (LCNN-BLSTM)",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "the inference speed of WavLM Large + CA-MHFA is comparable"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "when MHFA is not utilized,\nas\nthis\nstructure has\nshown the best",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "to that of ResNet293. Furthermore, when comparing SSL models of"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "performance when the SSL model\nis\nfrozen [7]. No augmentation",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "similar scales, WavLM + CA-MHFA consistently outperforms other"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "methods are applied.",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "SSL-based SV systems with a\nlighter\nspeaker\nextractor back-end."
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "4) Performance Metrics:\nFor SV and deepfake detection,\nequal",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "For example, under\nthe same WeSpeaker\nframework,\nthe proposed"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "error\nrate (EER)\nis employed to measure the performancesER, we",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "CA-MHFA outperforms ECAPA-TDNN systems when\nusing\nboth"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "use a leave-one-session-out 5-fold cross-validation to report averaged",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "WavLM Large and WavLM Base Plus."
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "accuracy.",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "D.\nCA-MHFA on Other Tasks"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "B. Hyper-Parameter Analysis of CA-MHFA",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "In this\nsection, we explored the generalizability of\nthe proposed"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "We present here a hyper-parameter analysis of CA-MHFA for\nthe",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "CA-MHFA across\nthree downstream tasks: SV, ER,\nand deepfake"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "SV task under the full fine-tuning evaluation. We focus on analyzing",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "detection. The evaluation follows the SUPERB-style as we introduced"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "different context\nlengths (L) and numbers of heads (G)\nin terms of",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "in section III-A. The results are summarized in Table III."
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "EER (%) on the VoxCeleb dataset. WavLM BASE Plus\nis used as",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "In general,\nthe proposed CA-MHFA consistently improves perfor-"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "the pre-trained SSL model\n[3]. The results are shown in Table I.",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "mance in multiple tasks and upstream models over baseline systems"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "It\nis observed that\nincreasing the context\nlength and the number",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "and MHFA. Specifically, for SV, when using the same SSL features as"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "of\nheads\nimproves\nthe\nperformance\nof CA-MHFA,\nespecially\non",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "input, CA-MHFA consistently outperforms the x-vector and ECAPA-"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "Vox1-E and Vox1-H. This may be due\nto the\nfact\nthat while\nthe",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "TDNN back-ends with fewer parameters (2.3M vs. 9.2M and 7.0M,"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "SV task\ndoes\nnot\nrequire\nextensive\ncontext,\nconsidering\nadjacent",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "respectively).\nIn\naddition, CA-MHFA shows\nnotable\nperformance"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "surrounding\nframes\ncan\nbe\nbeneficial. Compared\nto MHFA, CA-",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "improvements\nin ER tasks.\nIn\nthe\ndeepfake\ndetection,\ncompared"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "MHFA consistently outperforms MHFA in most configurations.",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "to LLGF, CA-MHFA demonstrates Considerable\nimprovements\nin"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "We also compared systems that applied convolution operations to",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "most cases. Our\nresults demonstrate consistent\nimprovements across"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "both the keys and values branches, as shown in the last row of Table I.",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "different model scales, highlighting its robustness and versatility."
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "Specifically, the values branches used 1D convolution operations with",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "a kernel size of 5, where the input and output channels were set\nto",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "IV. CONCLUSION"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "D. The results show that this configuration performs worse compared",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "In this paper, we propose\na novel\nlightweight back-end module"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "to systems\nthat\napplied convolution only to the\nkeys branch. This",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "named CA-MHFA for\npre-trained SSL-based\nspeaker\nverification,"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "suggests that contextual\ninformation may be more beneficial\nfor\nthe",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "which can be easily extended to broader speech classification tasks,"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "content-related subspace than for\nthe speaker-related subspace. One",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "including emotion recognition and deepfake detection. CA-MHFA"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "possible\nexplanation is\nthat\ncontext\nin the\ncontent\nsubspace helps",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "enhances\ncontextual modeling\nby\nconsidering\nframes\nsurrounding"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "capture the speaker’s speaking style, such as the structuring of words,",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "the\ncurrent\nframe when\ncomputing\nattention weights,\nleading\nto"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "while the speaker subspace focuses on features that are invariant\nto",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "more\nstable\nand discriminative utterance-level\nrepresentations. We"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "the speaker’s identity and,\nthus,\nrequires less contextual attention.",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "conducted comprehensive experiments on the VoxCeleb, IEMOCAP,"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "C. Comparison with State-of-the-art SV Systems",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": ""
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "and ASVspoof2019 datasets. The results demonstrate that CA-MHFA"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "Under\nthe\noptimal\nhyperparameters\nidentified\nin\nthe\nprevious",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "outperforms other downstream back-end modules while maintaining"
        },
        {
          "Data2vec Large\n7.62\n3.11\n2.59\n2.62": "section (G = 64, L = 9), we compared the proposed CA-MHFA with",
          "64.97\n64.88\n64.79\n4.26\n1.41\n1.32": "lower parameter counts."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE, 2022, pp. 6147–6151.": "J. Peng, T. Stafylakis, R. Gu, O. Plchot, L. Moˇsner, L. Burget,\nand"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "ˇ\nJ.\nCernock`y, “Parameter-efficient\ntransfer\nlearning of pre-trained trans-"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "former models\nfor\nspeaker verification using adapters,” arXiv preprint"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "arXiv:2210.16032, 2022."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "J. Peng, O. Plchot, T. Stafylakis, L. Mosner, L. Burget,\nand\nJ. H."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "ˇ\nCernock´y, “Improving Speaker Verification with Self-Pretrained Trans-"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "former Models,” in Proc.\nINTERSPEECH 2023, 2023, pp. 5361–5365."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "P.-c. Hsu, A. Elkahky, W.-N. Hsu, Y. Adi, T. A. Nguyen,\nJ. Copet,"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "E. Dupoux, H.-y. Lee, and A. Mohamed, “Low-resource self-supervised"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "learning with ssl-enhanced tts,” arXiv preprint arXiv:2309.17020, 2023."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin,"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K.\ntik"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed,"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "and H. yi Lee, “SUPERB: Speech Processing Universal PERformance"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Benchmark,” in Proc.\nInterspeech 2021, 2021, pp. 1194–1198."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": ""
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "vectors: Robust dnn embeddings for speaker recognition,” in 2018 IEEE"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "international\nconference\non\nacoustics,\nspeech\nand\nsignal\nprocessing"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "(ICASSP).\nIEEE, 2018, pp. 5329–5333."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Emphasized Channel Attention, Propagation and Aggregation in TDNN"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Based Speaker Verification,” in Proc. Interspeech 2020, 2020, pp. 3830–"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "3834."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "ˇ\nJ. Peng, O. Plchot, T. Stafylakis, L. Moˇsner, L. Burget, and J.\nCernock`y,"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "“An attention-based backend allowing efficient fine-tuning of transformer"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "models for speaker verification,” in 2022 IEEE Spoken Language Tech-"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "nology Workshop (SLT).\nIEEE, 2023, pp. 555–562."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": ""
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "A. H. Abdelaziz, S. Watanabe, and B.-J. Theobald, “Can you remove the"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "downstream model\nfor speaker\nrecognition with self-supervised speech"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "features?” arXiv preprint arXiv:2402.00340, 2024."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "S. Zaiem, Y. Kemiche, T. Parcollet, S. Essid, and M. Ravanelli, “Speech"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Self-Supervised Representation Benchmarking: Are We Doing it Right?”"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "in Proc.\nINTERSPEECH 2023, 2023, pp. 2873–2877."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Snyder, B. Mak,\nand D.\nPovey,\n“Self-Attentive"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Speaker Embeddings\nfor Text-Independent\nSpeaker Verification,”\nin"
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "Proc.\nInterspeech 2018, 2018, pp. 3573–3577."
        },
        {
          "IEEE, 2022, pp. 6147–6151.": ""
        },
        {
          "IEEE, 2022, pp. 6147–6151.": "in\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "Emphasized Channel Attention, Propagation and Aggregation in TDNN"
        },
        {
          "REFERENCES": "[1] A. Baevski, Y.\nZhou, A. Mohamed,\nand M. Auli,\n“wav2vec\n2.0:",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "Based Speaker Verification,” in Interspeech2020, 2020, pp. 1–5."
        },
        {
          "REFERENCES": "A framework for\nself-supervised learning of\nspeech representations,”",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "[21] H. Wang, C. Liang, S. Wang, Z. Chen, B. Zhang, X. Xiang, Y. Deng,"
        },
        {
          "REFERENCES": "Advances in neural\ninformation processing systems, vol. 33, pp. 12 449–",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "and Y. Qian, “Wespeaker: A research and production oriented speaker"
        },
        {
          "REFERENCES": "12 460, 2020.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "IEEE\nInternational Conference\non\nembedding\nlearning\ntoolkit,”\nin"
        },
        {
          "REFERENCES": "[2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2023, pp."
        },
        {
          "REFERENCES": "A. Mohamed, “HuBERT: Self-Supervised Speech Representation Learn-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "1–5."
        },
        {
          "REFERENCES": "ing by Masked Prediction of Hidden Units,” IEEE/ACM Transactions on",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "[22] D. Cai and M. Li, “Leveraging asr pretrained conformers\nfor\nspeaker"
        },
        {
          "REFERENCES": "Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "verification through transfer\nlearning and knowledge distillation,” arXiv"
        },
        {
          "REFERENCES": "[3]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "preprint arXiv:2309.03019, 2023."
        },
        {
          "REFERENCES": "T. Yoshioka, X. Xiao et al., “WavLM: Large-scale self-supervised pre-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "[23] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "REFERENCES": "IEEE Journal\nof\nSelected\ntraining\nfor\nfull\nstack\nspeech\nprocessing,”",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional"
        },
        {
          "REFERENCES": "Topics in Signal Processing, 2022.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "dyadic motion capture database,” Language resources and evaluation,"
        },
        {
          "REFERENCES": "[4] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, “Data2vec:",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "vol. 42, pp. 335–359, 2008."
        },
        {
          "REFERENCES": "A general\nframework for self-supervised learning in speech, vision and",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "[24] X. Wang, J. Yamagishi, M. Todisco, H. Delgado, A. Nautsch, N. Evans,"
        },
        {
          "REFERENCES": "language,” in International Conference on Machine Learning.\nPMLR,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "M. Sahidullah, V. Vestman, T. Kinnunen, K. A. Lee et al., “Asvspoof"
        },
        {
          "REFERENCES": "2022, pp. 1298–1312.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "2019: A large-scale\npublic\ndatabase\nof\nsynthesized,\nconverted\nand"
        },
        {
          "REFERENCES": "[5] Z. Li, T. Graave, J. Liu, T. Lohrenz, S. Kunzmann, and T. Fingscheidt,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "replayed speech,” Computer Speech & Language, vol. 64, p. 101114,"
        },
        {
          "REFERENCES": "“Parameter-efficient\ncross-language\ntransfer\nlearning\nfor\na\nlanguage-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "2020."
        },
        {
          "REFERENCES": "2023\nIEEE\nAutomatic\nmodular\naudiovisual\nspeech\nrecognition,”\nin",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "[25]",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular"
        },
        {
          "REFERENCES": "Speech Recognition and Understanding Workshop (ASRU).\nIEEE, 2023,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "the IEEE/CVF\nmargin loss for deep face recognition,” in Proceedings of"
        },
        {
          "REFERENCES": "pp. 1–8.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "conference on computer vision and pattern recognition, 2019, pp. 4690–"
        },
        {
          "REFERENCES": "[6] E. Morais, R. Hoory, W. Zhu, I. Gat, M. Damasceno, and H. Aronowitz,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "4699."
        },
        {
          "REFERENCES": "“Speech emotion recognition using self-supervised features,” in ICASSP",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "[26]",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "J. Thienpondt, B. Desplanques, and K. Demuynck, “The idlab voxsrc-20"
        },
        {
          "REFERENCES": "2022 - 2022 IEEE International Conference on Acoustics, Speech and",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "submission: Large margin fine-tuning and quality-aware score calibra-"
        },
        {
          "REFERENCES": "Signal Processing (ICASSP), 2022, pp. 6922–6926.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "tion in dnn based speaker verification,”\nin ICASSP 2021-2021 IEEE"
        },
        {
          "REFERENCES": "[7] X. Wang and J. Yamagishi, “Investigating Self-Supervised Front Ends for",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "REFERENCES": "Speech Spoofing Countermeasures,” in Proc. Odyssey, 2022, pp. 100–",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "(ICASSP).\nIEEE, 2021, pp. 5814–5818."
        },
        {
          "REFERENCES": "106.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "[27] D. Snyder, G. Chen, and D. Povey, “Musan: A music, speech, and noise"
        },
        {
          "REFERENCES": "[8] Z. Chen, S. Chen, Y. Wu, Y. Qian, C. Wang, S. Liu, Y. Qian,\nand",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": "corpus,” arXiv preprint arXiv:1510.08484, 2015."
        },
        {
          "REFERENCES": "M. Zeng,\n“Large-scale\nself-supervised\nspeech\nrepresentation\nlearning",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "for automatic speaker verification,” in ICASSP 2022-2022 IEEE Interna-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "tional Conference on Acoustics, Speech and Signal Processing (ICASSP).",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "IEEE, 2022, pp. 6147–6151.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[9]\nJ. Peng, T. Stafylakis, R. Gu, O. Plchot, L. Moˇsner, L. Burget,\nand",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "ˇ\nJ.\nCernock`y, “Parameter-efficient\ntransfer\nlearning of pre-trained trans-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "former models\nfor\nspeaker verification using adapters,” arXiv preprint",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "arXiv:2210.16032, 2022.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[10]\nJ. Peng, O. Plchot, T. Stafylakis, L. Mosner, L. Burget,\nand\nJ. H.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "ˇ\nCernock´y, “Improving Speaker Verification with Self-Pretrained Trans-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "former Models,” in Proc.\nINTERSPEECH 2023, 2023, pp. 5361–5365.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[11]\nP.-c. Hsu, A. Elkahky, W.-N. Hsu, Y. Adi, T. A. Nguyen,\nJ. Copet,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "E. Dupoux, H.-y. Lee, and A. Mohamed, “Low-resource self-supervised",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "learning with ssl-enhanced tts,” arXiv preprint arXiv:2309.17020, 2023.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[12]\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K.\ntik",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "and H. yi Lee, “SUPERB: Speech Processing Universal PERformance",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "Benchmark,” in Proc.\nInterspeech 2021, 2021, pp. 1194–1198.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[13] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "vectors: Robust dnn embeddings for speaker recognition,” in 2018 IEEE",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "international\nconference\non\nacoustics,\nspeech\nand\nsignal\nprocessing",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "(ICASSP).\nIEEE, 2018, pp. 5329–5333.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[14] B. Desplanques,\nJ. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "Emphasized Channel Attention, Propagation and Aggregation in TDNN",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "Based Speaker Verification,” in Proc. Interspeech 2020, 2020, pp. 3830–",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "3834.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "ˇ\n[15]\nJ. Peng, O. Plchot, T. Stafylakis, L. Moˇsner, L. Burget, and J.\nCernock`y,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "“An attention-based backend allowing efficient fine-tuning of transformer",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "models for speaker verification,” in 2022 IEEE Spoken Language Tech-",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "nology Workshop (SLT).\nIEEE, 2023, pp. 555–562.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[16] Z. Aldeneh, T. Higuchi, J.-w. Jung, S. Seto, T. Likhomanenko, S. Shum,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "A. H. Abdelaziz, S. Watanabe, and B.-J. Theobald, “Can you remove the",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "downstream model\nfor speaker\nrecognition with self-supervised speech",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "features?” arXiv preprint arXiv:2402.00340, 2024.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[17]\nS. Zaiem, Y. Kemiche, T. Parcollet, S. Essid, and M. Ravanelli, “Speech",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "Self-Supervised Representation Benchmarking: Are We Doing it Right?”",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "in Proc.\nINTERSPEECH 2023, 2023, pp. 2873–2877.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[18] Y. Zhu, T. Ko, D.\nSnyder, B. Mak,\nand D.\nPovey,\n“Self-Attentive",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "Speaker Embeddings\nfor Text-Independent\nSpeaker Verification,”\nin",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "Proc.\nInterspeech 2018, 2018, pp. 3573–3577.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "[19] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "in\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        },
        {
          "REFERENCES": "neural\ninformation processing systems, vol. 30, 2017.",
          "[20] B. Desplanques,": "",
          "J. Thienpondt,\nand K. Demuynck,\n“ECAPA-TDNN:": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "WavLM: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Parameter-efficient cross-language transfer learning for a languagemodular audiovisual speech recognition",
      "authors": [
        "Z Li",
        "T Graave",
        "J Liu",
        "T Lohrenz",
        "S Kunzmann",
        "T Fingscheidt"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Investigating Self-Supervised Front Ends for Speech Spoofing Countermeasures",
      "authors": [
        "X Wang",
        "J Yamagishi"
      ],
      "year": "2022",
      "venue": "Proc. Odyssey"
    },
    {
      "citation_id": "8",
      "title": "Large-scale self-supervised speech representation learning for automatic speaker verification",
      "authors": [
        "Z Chen",
        "S Chen",
        "Y Wu",
        "Y Qian",
        "C Wang",
        "S Liu",
        "Y Qian",
        "M Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Parameter-efficient transfer learning of pre-trained transformer models for speaker verification using adapters",
      "authors": [
        "J Peng",
        "T Stafylakis",
        "R Gu",
        "O Plchot",
        "L Mošner",
        "L Burget",
        "J Černockỳ"
      ],
      "year": "2022",
      "venue": "Parameter-efficient transfer learning of pre-trained transformer models for speaker verification using adapters",
      "arxiv": "arXiv:2210.16032"
    },
    {
      "citation_id": "10",
      "title": "Improving Speaker Verification with Self-Pretrained Transformer Models",
      "authors": [
        "J Peng",
        "O Plchot",
        "T Stafylakis",
        "L Mosner",
        "L Burget",
        "J Černocký"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "11",
      "title": "Low-resource self-supervised learning with ssl-enhanced tts",
      "authors": [
        "P.-C Hsu",
        "A Elkahky",
        "W.-N Hsu",
        "Y Adi",
        "T Nguyen",
        "J Copet",
        "E Dupoux",
        "H -Y. Lee",
        "A Mohamed"
      ],
      "year": "2023",
      "venue": "Low-resource self-supervised learning with ssl-enhanced tts",
      "arxiv": "arXiv:2309.17020"
    },
    {
      "citation_id": "12",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H Yi Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "13",
      "title": "Xvectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "An attention-based backend allowing efficient fine-tuning of transformer models for speaker verification",
      "authors": [
        "J Peng",
        "O Plchot",
        "T Stafylakis",
        "L Mošner",
        "L Burget",
        "J Černockỳ"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "16",
      "title": "Can you remove the downstream model for speaker recognition with self-supervised speech features?",
      "authors": [
        "Z Aldeneh",
        "T Higuchi",
        "J.-W Jung",
        "S Seto",
        "T Likhomanenko",
        "S Shum",
        "A Abdelaziz",
        "S Watanabe",
        "B.-J Theobald"
      ],
      "year": "2024",
      "venue": "Can you remove the downstream model for speaker recognition with self-supervised speech features?",
      "arxiv": "arXiv:2402.00340"
    },
    {
      "citation_id": "17",
      "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?",
      "authors": [
        "S Zaiem",
        "Y Kemiche",
        "T Parcollet",
        "S Essid",
        "M Ravanelli"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "18",
      "title": "Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification",
      "authors": [
        "Y Zhu",
        "T Ko",
        "D Snyder",
        "B Mak",
        "D Povey"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "venue": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification"
    },
    {
      "citation_id": "21",
      "title": "Wespeaker: A research and production oriented speaker embedding learning toolkit",
      "authors": [
        "H Wang",
        "C Liang",
        "S Wang",
        "Z Chen",
        "B Zhang",
        "X Xiang",
        "Y Deng",
        "Y Qian"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Leveraging asr pretrained conformers for speaker verification through transfer learning and knowledge distillation",
      "authors": [
        "D Cai",
        "M Li"
      ],
      "year": "2023",
      "venue": "Leveraging asr pretrained conformers for speaker verification through transfer learning and knowledge distillation",
      "arxiv": "arXiv:2309.03019"
    },
    {
      "citation_id": "23",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "24",
      "title": "Asvspoof 2019: A large-scale public database of synthesized, converted and replayed speech",
      "authors": [
        "X Wang",
        "J Yamagishi",
        "M Todisco",
        "H Delgado",
        "A Nautsch",
        "N Evans",
        "M Sahidullah",
        "V Vestman",
        "T Kinnunen",
        "K Lee"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "25",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "N Xue",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "The idlab voxsrc-20 submission: Large margin fine-tuning and quality-aware score calibration in dnn based speaker verification",
      "authors": [
        "J Thienpondt",
        "B Desplanques",
        "K Demuynck"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Musan: A music, speech, and noise corpus",
      "authors": [
        "D Snyder",
        "G Chen",
        "D Povey"
      ],
      "year": "2015",
      "venue": "Musan: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    }
  ]
}