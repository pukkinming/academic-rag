{
  "paper_id": "2201.03967v3",
  "title": "Emotion Intensity And Its Control For Emotional Voice Conversion",
  "published": "2022-01-10T02:11:25Z",
  "authors": [
    "Kun Zhou",
    "Berrak Sisman",
    "Rajib Rana",
    "Björn W. Schuller",
    "Haizhou Li"
  ],
  "keywords": [
    "Emotional voice conversion",
    "emotion intensity",
    "sequence-to-sequence",
    "perceptual loss",
    "limited data",
    "relative attribute"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotional voice conversion (EVC) seeks to convert the emotional state of an utterance while preserving the linguistic content and speaker identity. In EVC, emotions are usually treated as discrete categories overlooking the fact that speech also conveys emotions with various intensity levels that the listener can perceive. In this paper, we aim to explicitly characterize and control the intensity of emotion. We propose to disentangle the speaker style from linguistic content and encode the speaker style into a style embedding in a continuous space that forms the prototype of emotion embedding. We further learn the actual emotion encoder from an emotion-labelled database and study the use of relative attributes to represent fine-grained emotion intensity. To ensure emotional intelligibility, we incorporate emotion classification loss and emotion embedding similarity loss into the training of the EVC network. As desired, the proposed network controls the fine-grained emotion intensity in the output speech. Through both objective and subjective evaluations, we validate the effectiveness of the proposed network for emotional expressiveness and emotion intensity control.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E MOTIONAL Voice Conversion (EVC) is a technique that seeks to manipulate the emotional state of an utterance while keeping other vocal states unchanged  [1] . It allows for the projection of the desired emotion into the synthesized voice. Emotional voice conversion poses a tremendous potential for human-computer interaction, such as enabling emotional intelligence into a dialogue system  [2] ,  [3] ,  [4] .\n\nVoice conversion aims to convert the speaker-dependent vocal attributes such as the speaker identity while preserving the linguistic information  [5] . Since the speaker information is characterized by the physical structure of the vocal tract and manifested in the spectrum  [6] , spectral mapping has been the main focus of voice conversion  [7] . However, speech also conveys emotions with various intensity levels that can be perceived by the listener  [8] . For example, happy can be perceived as happy or elation  [9] , while angry can be divided into a 'mild' angry and the 'full-blown' angry  [10] . In particular, intensity of emotion is described as the magnitude of factor to attain the goal of the emotion  [11] . Therefore, emotion intensity is not just the loudness of a voice, but correlates to all the acoustic cues that contribute to achieving an emotion  [12] . Moreover, speech emotion is hierarchical and supra-segmental in nature, varying from syllables to utterances  [13] ,  [14] ,  [15] ,  [16] . Thus, it is insufficient to only focus on frame-wise spectral mapping for emotional voice conversion. Both intensity variations and prosodic dynamics need to be considered for speech emotion modelling.\n\nSynthesizing various intensities of an emotion is a challenging task for emotional voice conversion studies. One of the reasons is the lack of explicit intensity labels in most emotional speech datasets. Besides, emotion intensity is even more subjective and complex than just considering discrete emotion categories, which makes it challenging to model  [12] . There are generally two types of methods in the literature for emotion intensity control. One uses auxiliary features such as a state of voiced, unvoiced, and silence (VUS)  [17] , attention weights or a saliency map  [18] . Another manipulates the internal emotion representations through interpolation  [19]  or scaling  [20] . Despite these methods, emotion intensity control is still an under-explored topic in emotional voice conversion.\n\nPrevious emotional voice conversion studies mainly focus on learning a feature mapping between different emotion types. Most of them, model the mappings of spectral and prosody parameters with a Gaussian mixture model (GMM)  [21] ,  [22] , sparse representation  [23] , or hidden Markov model (HMM)  [24] . Recent deep learning methods such as deep neural networks (DNN)  [25] ,  [26]  and deep bi-directional long-short-term memory network (DBLSTM)  [27]  have advanced the state-of-the-art. New techniques using generative adversarial network (GAN)-based  [28] ,  [29] ,  [30]  or auto-encoder-based models  [31] ,  [32] ,  [33]  make it possible for non-parallel training. We note that these frameworks convert the emotion on a frame basis, so speech duration cannot be modified. Moreover, since the spectrum and prosody are not independent of each other, a separate study of them may cause a mismatch during the conversion  [34] ,  [35] . It would be advantageous to have a model to transfer the correlated vocal factors end-to-end, producing more realistic emotions in synthetic speech.\n\nRecently, sequence-to-sequence (Seq2Seq) models have attracted much interest in speech synthesis  [36] ,  [37]  and voice conversion  [38] ,  [39] ,  [40] ,  [41] . With the attention mechanism, Seq2Seq frameworks jointly learn the feature mapping and alignment and automatically predict the speech duration at run-time. Inspired by these successful attempts, researchers introduce Seq2Seq modelling into emotional voice conversion. For example, a Seq2Seq model to jointly model pitch and duration is proposed in  [42] . In  [43] , a multi-task learning for both emotional voice conversion and emotional text-to-speech is studied. We note two limitations of these studies: First, they learn an averaged emotional pattern during the training, while emotional expressive speech presents abundant variations of emotion intensity in real life. Second, these frameworks require enormous emotional speech data to train. But in practice, such a large emotional speech database is not widely available, which limits the scope of applications.\n\nIn this article, we aim to address the above challenges. The main contributions of this paper are listed as follows.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "•",
      "text": "We introduce Emovox, a Seq2Seq emotional voice conversion framework, which jointly transfers the spectrum and duration in an end-to-end way for emotional voice conversion.\n\n• Emovox automatically learns the abundant variations of intensity that are exhibited in an emotional speech dataset, without the need for any explicit intensity labels and enables effective control of the emotion intensity in the converted emotional speech at the run-time;\n\n• Emovox eliminates the need for a large amount of emotional speech data for Seq2Seq EVC training and still achieves remarkable performance under limited data conditions;\n\n• We present a comprehensive evaluation to show the effectiveness of Emovox for emotional expressiveness and emotion intensity control. This paper is organized as follows: In Section 2, we motivate our study by introducing the background and related work. In Section 3, we present the details of our proposed Emovox framework and we introduce our experiments in Section 4. In Section 5, we report the experimental results and conclude in Section 6.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Background And Related Work",
      "text": "This work is built on several previous studies spanning emotion intensity, expressive speech synthesis, and emotional voice conversion. We briefly introduce the related studies to set the stage for our research and summarize the gaps in current literature to place our novel contributions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Intensity In Vocal Expression",
      "text": "The most straightforward way to characterize emotion is to categorize it into several different groups  [44] ,  [45] ; however, the choice of emotion labels is mostly intuitive and inconsistent in the literature. One key reason is that",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep Features",
      "text": "Fig.  1 : An example of a speech emotion recognizer (SER)  [47] , where the deep features are obtained before the last fully-connected (FC) layer to describe the emotion styles  [48] ,  [49] .\n\nemotion intensity can affect our perception of emotions  [46] . For example, happy can be perceived as happy or elation, which are similar in voice quality but different in intensity  [9] . Thus, correlating the emotion intensity to the loudness of the voice is a rather oversimplification. Emotion intensity can be observed in various acoustic cues, not only in speech energy but also in speech rate and fundamental frequency  [12] . The differences in these cue levels could be larger between different intensities of the same emotion than between different emotions  [46] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sequence-To-Sequence Conversion Models",
      "text": "The sequence-to-sequence model with attention mechanism was first studied in machine translation  [50]  and then found effective in speech synthesis  [36] ,  [37] . In text-to-speech, sequence-to-sequence modelling achieves remarkable performance by learning an attention alignment between the text and acoustic sequence, such as Tacotron  [37] . Similar to text-to-speech, voice conversion aims to generate realistic speech from internal representations; therefore, sequenceto-sequence models are applied to various voice conversion and emotional voice conversion studies.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sequence-To-Sequence Voice Conversion",
      "text": "Sequence-to-sequence voice conversion frameworks such as SCENT  [38] , AttS2S-VC  [39] , and ConvS2S-VC  [51] , jointly convert the duration and prosody components, and achieve higher naturalness and similarity than conventional framebased methods. To address the conversion issues such as the deletion and repetition caused by the misalignment, various approaches are proposed, such as a monotonic attention mechanism  [40] , non-autoregressive training  [52] ,  [53] , and the use of pre-training models  [54]  or text supervision  [55] ,  [56] ,  [57] . These successful attempts further motivate the study of sequence-to-sequence modelling for emotional voice conversion.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sequence-To-Sequence Emotional Voice Conversion",
      "text": "Compared with conventional frame-based models, sequence-to-sequence models are more suitable for emotional voice conversion. First, the sequence-to-sequence models allow for the prediction of speech duration at the run-time, which is an important aspect of the speech rhythm and strongly affects the emotional prosody  [58] . Besides, a joint transfer of spectrum and prosody in sequence-to-sequence models addresses the mismatch issues in conventional analysis-synthesis-based emotional voice conversion systems  [28] ,  [33] ,  [34] . Also, emotional prosody is supra-segmental and can be only associated with a few words  [47] . Learning an attention alignment makes it possible to focus on emotion-relevant regions during the conversion. Hence, sequence-to-sequence modelling for emotional voice conversion will be our primary focus in this paper.\n\nThere are only few studies on sequence-to-sequence emotional voice conversion  [20] ,  [42] ,  [43] ,  [59] . In  [42] , the authors jointly model pitch and duration with parallel data, where the model is conditioned on the syllable position in the phrase. In  [43] , a multi-task learning framework of emotional voice conversion and emotional text-to-speech is built with a large-scale emotional speech database. In  [20] , the authors introduce an emotion encoder and a speaker encoder into the sequence-to-sequence training for emotional voice conversion. We note that these frameworks require tens of hours of parallel emotional speech data, which is hard to collect. A recent work  [59]  proposes a 2-stage training strategy for sequence-to-sequence emotional voice conversion leveraging text-to-speech to eliminate the need for a large emotional speech database. However, none of these frameworks study emotion intensity variations, and the converted emotional utterances lack the controllability of emotion intensity. Only  [20]  attempts to scale the emotion embedding by multiplying it with a factor to control the emotion intensity at run-time. However, the authors do not explicitly model emotion intensity variations during the training, and their intensity control method lacks interpretability.\n\nThis work aims to bridge this gap in the current literature and study emotion intensity modelling for emotional voice conversion. We aim to build a sequence-to-sequence emotional voice conversion framework with effective emotion intensity control using a limited amount of emotional speech data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Expressive Speech Synthesis With Prosody Style Control",
      "text": "Speech emotion is highly related to speech prosody and influenced by several prosodic cues embedded in acoustic speech such as intonation, rhythm, and energy  [60] ,  [61] . The most straightforward way to model and control prosody style is to use explicit annotations, or labels  [62] ,  [63] ,  [64] . Besides explicitly labelling, researchers use a reference encoder to imitate and transplant the reference style in an unsupervised way  [65] . Global style token (GST)  [66]  is an example to learn interpretable style embeddings from the reference audio. By choosing specific tokens, the model could control the style of synthesized speech. Other studies  [67] ,  [68] ,  [69] ,  [70]  mainly replace the global style embedding with fine-grained prosody embedding. Some other studies based on Variational Autoencoders (VAE)  [71]  show the effectiveness of controlling the speech style by learning, scaling, or combining disentangled representations  [72] ,  [73] .\n\nEmotion expressive speech is even more complex, which has subtle dynamic variations associated with multiple prosodic attributes  [74] ,  [75] ,  [76] . Inspired by the successful attempts in prosody style control, several studies control the emotion intensity for emotional speech synthesis. For example, in  [19] , an inter-to-intra distance ratio algorithm is applied to the learnt style tokens for emotional speech synthesis, where an interpolation technique is used to control emotion intensity. In  [18] , the authors show that a speech emotion recognizer is capable of generating a meaningful intensity representation via attention or saliency. In  [77] ,  [78] , a relative attribute scheme is introduced to learn the emotion intensity for emotional speech synthesis. None of these frameworks explicitly models prosody style, but rather encodes the association between input text and its emotional prosody style end-to-end.\n\nThis contribution studies explicit modelling of emotion intensity variations with a relative attribute method for emotional voice conversion. We believe that the relative attributes scheme provides a straightforward way to model intensity variants, which will be discussed later.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional Prosody Modelling With A Speech Emotion Recognizer",
      "text": "Emotional prosody is prominently exhibited in emotional expressive speech  [79] ,  [80] ,  [81] , which can be characterized by either categorical  [45]  or dimensional representations  [82] . Recent studies  [83]  in speech emotion recognition provide valuable insights into emotional prosody modelling. Instead of categorical or dimensional attributes, they characterize the emotion styles with the latent representations learnt by the deep neural network. Compared with humancrafted features, deep features learnt by a speech emotion recognizer (SER) are data-driven and less dependent on human knowledge  [84] ,  [85] , which we believe is more suitable for emotion style transfer.\n\nSome studies are leveraging a speech emotion recognizer to improve the prosody modelling for expressive speech synthesis. In  [86] , an emotion recognizer is used to extract the style embedding for style transfer. In  [49] , a speech emotion recognizer is further used as the style descriptor to evaluate the style reconstruction performance. In  [48] , researchers use the deep emotional features from a pretrained speech emotion recognizer to transfer both seen and unseen emotion styles. These studies show the capability of a speech emotion recognizer to describe emotion styles with their latent representations.\n\nA speech emotion recognizer also shows a potential to supervise the emotional speech synthesis system to generate the speech with desirable emotion styles  [87] . In  [88] , a reinforcement learning paradigm for emotional speech synthesis is proposed, where the classification accuracy of the speech emotion recognizer is used as the reward function to the system. In  [89] , the authors use emotion classifiers to enhance the emotion-discrimination of the emotion embedding and the predicted Mel-spectrum. In  [90] , an emotional speech synthesis system is built on an expressive TTS corpus with the assistance of a cross-domain emotion recognizer. These studies show remarkable performance by incorporating the supervision from the pre-trained emotion recognizer into the emotional speech synthesis systems, which motivates our study. We further study the use of perceptual losses in EVC training to improve the intelligibility of the converted emotion.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Research Gap (Summary)",
      "text": "Below, we summarise the gaps in the literature of emotional voice conversion that we aim to address in this paper.\n\n1) There are very few studies on emotion intensity control, which is crucial to achieving emotional intelligence.\n\n2) Despite the tremendous potential, emotion intensity control is still not a well-explored research direction for emotional voice conversion. 3) There is a lack of focus on modelling prosody style to achieve improved emotion intensity control. 4) Feasibility of using a pre-trained speech emotion recognizer as an emotion supervisor for EVC training poses tremendous potential but is not well understood.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emovox : Emotional Voice Conversion With Emotion Intensity Control",
      "text": "The proposed emotional voice conversion framework: Emovox consists of four modules, 1) a recognition encoder, which derives the linguistic embedding from the source speech; 2) an emotion encoder, which encodes the reference emotion style into an emotion embedding; 3) an intensity encoder, which encodes a fine-grained intensity input into an intensity embedding, and 4) a Seq2Seq decoder, which generates the converted speech from a combination of linguistics, emotion, and intensity embeddings. At run-time, Emovox preserves the source linguistic content (\"linguistic transplant\"), while transferring the reference emotion to a source utterance (\"emotion transfer\"), as illustrated in Figure  2 . Emovox also allows users to manipulate/control the emotion intensity of the output speech (\"intensity control\").\n\nTo train Emovox, we propose a Seq2Seq framework that disentangles the speech elements from input acoustic features, and reconstructs the acoustic features from the speech elements. To reduce the amount of training data for Emovox, we introduce two pre-training strategies, i. e., 1) style pre-training with a large TTS corpus, and 2) emotion supervision training with an SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Seq2Seq Emotional Voice Conversion",
      "text": "Human speech can be viewed as a combination of speech style, and linguistic content  [75] ,  [91] . If the speech style that represents the emotion can be disentangled from the linguistic content, emotion conversion can be achieved by manipulating the speech style at run-time while keeping the linguistic content and speaker identity unchanged  [33] ,  [92] .\n\nThere are various ways to disentangle the speech elements. In  [56] , text information and adversarial learning are used in a sequence-level autoencoder. This framework achieves strong disentanglement between linguistic and speaker representations and enables duration modelling for voice conversion. We adopt this framework in Emovox to model emotion styles and intensity, as shown in Figure  3 . To overcome the issues such as deletion and repetition with the Seq2Seq approach, we include a text input as the supervision signal to augment the linguistic embedding, which are shown effective in recent studies  [56] ,  [57] ,  [59] .  Emovox aims to transfer the reference emotion to the source speech (\"emotion transfer\") while controlling its emotion intensity (\"intensity control\") and preserving the source linguistic information (\"linguistic transplant\").\n\nGiven the phoneme sequences and acoustic features as the input, the text encoder and the recognition encoder learn to predict the linguistic embedding from the text (H text ) and the audio input (H audio ), respectively. The emotion encoder learns the emotion representations from the speech, while the emotion classifier further eliminates the residual emotion information in the linguistic embedding H audio . The Seq2Seq decoder Dec learns to reconstruct the acoustic features Â from the combination of the emotion embedding h emo , the intensity embedding h inten , and the linguistic embedding either from the text encoder: H text or recognition encoder: H audio as shown in  (1) .\n\nwhere\n\nDuring the training, H text and H audio are taken by the decoder alternately, depending on whether the epoch number is odd or even. A contrastive loss is employed to ensure the similarity between H text and H audio as in  [56] . We believe the proposed Emovox learns an effective disentanglement between linguistic and emotional elements and provides a straightforward way to model and control both emotion and its intensity, which will be discussed next.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modelling Emotion And Its Intensity",
      "text": "To model emotion intensity, one of the difficulties is the lack of annotated intensity labels. Inspired by the idea of attribute  [93]  in computer vision, we regard emotion intensity as an attribute of the emotional speech. Combining the emotion representations with the intensity information allows the framework to jointly learn abundant emotion styles and intensity levels from any emotional speech database.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Formulation Of Emotion Intensity Using Relative Attributes",
      "text": "In computer vision, there are various ways  [94] ,  [95]  to model the relative difference between different data categories. Instead of predicting the presence of a specific attribute, relative attributes  [96]  offer more informative descriptions to unseen data, thus closer to detailed human supervision. Motivated by the success in various computer vision tasks  [97] ,  [98] ,  [99] , we believe that relative attributes bridge between the low-level features and high-level semantic meanings, which is appropriate for emotion intensity modelling.\n\nEmotion intensity can be viewed as how well the emotion can be perceived in its type. Since the neutral speech does not contain any emotional variance, the emotion intensity of a neutral utterance should be zero. Therefore, we regard the emotion intensity as a relative difference between neutral speech and emotional speech. Emotion intensity can be represented by relative attributes learnt with a rich set of emotion-related acoustic features from each emotion pair. The learning process of relative attributes can be formulated as a max-margin optimization problem as explained below:\n\nGiven a training set T = {x t }, where x t is the acoustic features of the t th training sample, and T = N ∪ E, where N and E are the neutral and emotional set respectively. We aim to learn a ranking function given as below:\n\nwhere W is a weighting matrix indicating the emotion intensity. To learn the ranking function, we have to satisfy the following constraints:\n\nwhere O and S are the ordered and similar sets respectively. We pair an emotional sample of E with a neutral sample from N to form an ordered set O, where the emotion intensity of E is higher than in that of N . We then randomly create pairs of neutral-neutral and emotionalemotional samples in the similar set S, where the emotion intensity of the pair is similar. The weighting matrix W is estimated by solving the following problem similar with that of a support vector machine  [100] :\n\nwhere C is the trade-off between the margin and the size of slack variables ξ a,b and γ a,b . Through Eq. (  6 ) -(  9 ), we learn a wide-margin ranking function that enforces the desired ordering on each training point. Once it is learnt, the relative ranking function can estimate the order of unseen data.\n\nIn practice, we learn a ranking function for each emotion category. As shown in Figure  3 , the learnt ranking function predicts a relative attribute normalized to [0, 1] for each sample in the training set. A larger value of relative attribute represents a stronger intensity of an emotion.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Modelling Emotion Styles And Its Intensity",
      "text": "As shown in Figure  3 , we obtain the relative attribute from the learnt ranking function, which passes through a fully connected layer to derive an intensity embedding. The emotion encoder learns to generate the emotion embedding from the input speech features. The Seq2Seq decoder combines a linguistic embedding sequence, an emotion and an intensity embedding to reconstruct the acoustic features of the emotional speech.\n\nDuring the training process, Emovox jointly learns the emotion style and its intensity from the speech samples that are referred to as emotion training hereafter. With the explicit intensity modelling, we are able to manipulate the level of intensity at run-time for intensity control. The intended emotion intensity can be predicted from the reference or given manually at run-time. In theory, Emovox may perform both emotional text-to-speech and emotional voice conversion. In this paper, the text encoder is not used at run-time since we are only interested in voice conversion.\n\nAs shown in Figure  4 , we first use the emotion encoder to generate the emotion embeddings from a set of reference utterances belonging to the same emotional category. Next, we use the averaged reference emotion embedding to represent an emotion category. Finally, the recognition encoder derives a linguistic embedding sequence from the source speech utterance at run-time. By assigning an intended emotion category and a level of emotion intensity, the Seq2Seq decoder generates the emotional speech of the same content as the source but with the target emotion style at an appropriate intensity.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Pre-Training",
      "text": "During training, a large amount of emotional speech is always required to achieve robust attention alignment and deliver high emotional intelligibility in a Seq2Seq model  [101] . To reduce the reliance on emotional speech, we propose two pre-training strategies, 1) style pre-training with a large TTS corpus and 2) emotion supervision training with a SER.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Style Pre-Training With A Multi-Speaker Tts Corpus",
      "text": "It is known that speech style contains speaker-dependent elements related to speaker characteristics, called speaker style. Speaker style is exhibited in most TTS corpora containing multi-speaker speech data. Unlike emotional speech databases, there are abundant speech databases for TTS  [102] ,  [103] ,  [104]  with a neutral tone, which allows us to build a multi-speaker Seq2Seq TTS framework, and train a network to disentangle speaker style from the linguistic content. We call this stage \"style pre-training\".\n\nDuring the style pre-training, the style encoder learns abundant speaker styles through a multi-speaker TTS corpus while excluding the linguistic information from the acoustic features. As a result, even though the style encoder does not learn to encode any specific emotion style during training, it learns to discriminate different emotion styles during emotion training, as shown in Figure  5 (a). We, therefore, use the style encoder trained on a TTS corpus as the pre-trained model for an emotion encoder.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Modelling Emotion With Perceptual Loss",
      "text": "We would like the converted emotional speech to be perceived with the intended emotion category. However, this is not easily achieved, especially with a limited emotional training data, for several reasons: 1) The pre-trained emotion decoder in Figure  3  is not explicitly trained for characterisation of emotions, and 2) frame-level style reconstruction loss Fig.  4 : An illustration of the run-time conversion phase. By combining a source linguistic embedding sequence, an averaged reference emotion embedding, and an intensity embedding, the Seq2Seq decoder generates the acoustic features with the reference emotion type and the manually defined intended intensity.\n\nis not always consistent with human perception because it does not capture speech's prosodic and temporal patterns.\n\nFollowing the success of perceptual loss in speech synthesis  [105] , we introduce a perceptual loss as the emotion supervision in the training process, as shown in Figure  3 . We first use a pre-trained SER to predict the emotion category from the reconstructed acoustic features. We then calculate two perceptual loss functions: 1) emotion classification loss L cls , and 2) emotion embedding similarity loss L sim . We incorporate these two loss functions into the training and update all the trainable modules. For detail of SER pretraining, readers are referred to  [106] .\n\nThe emotion classification loss L cls is introduced to ensure the perceptual similarity between the reconstructed acoustic features and the intended emotion category at the utterance level,\n\nwhere l is the target one-hot emotion label, p is the predicted emotion probabilities at the utterance level, and CE(•) denotes the cross-entropy loss function.\n\nThe pre-trained SER is considered text-independent. To ensure that the emotion encoder characterizes emotions independent of linguistic content, we introduce an emotion style descriptor, derived from the pre-trained SER  [48] ,  [49] , as a learning objective for emotion encoder with a loss function L sim , between the emotion encoder output, i. e., emotion embedding, and the emotion style descriptor, as illustrated in Figure  3 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Effect Of Perceptual Loss",
      "text": "To validate the effectiveness of two perceptual loss functions, we evaluate the emotion-discriminative ability of the emotion encoder with an ablation study. We believe that the emotion encoder demonstrates a better performance by producing more discriminative emotional representations. We use the t-SNE algorithm  [107]  to visualize the emotion embeddings in a two-dimensional plane, as shown in Figure  5 (e). It is observed that emotion embeddings form different emotion clusters in terms of the feature distribution. To get a more intuitive understanding of the clustering performance, we consider performing a clustering evaluation to evaluate the discriminability of the emotion embeddings.\n\nThe typical objective function of clustering formalizes the goal of attaining high intra-cluster similarity, and low inter-cluster similarity  [108] ,  [109] . There are studies to use different measurements for the quality of a clustering  [110] ,  [111] ,  [112] ,  [113] . Our study considers a simplified and effective solution for clustering evaluation. We first compute a centroid for each of K emotion classes, c i , i ∈ [1, K] by taking the average of all N i embeddings e in class i as follows  [114] :\n\nwhere E i is the set of embeddings in class i. We then calculate the inter-class distance dist inter by computing the Euclidean distance between each embedding e ∈ E i and the other embedding centres c j;j =i as follows:\n\nand intra-class distance dist intra as follows:\n\nA clustering ratio r is calculated from the ratio of intraclass distance dist intra and inter-class distance dist inter as follows:\n\nA lower value of ratio r represents a better clustering effect of emotion embeddings.\n\nWe perform an ablation experiment on the ESD evaluation dataset  [1] . We visualize the distribution of emotion embeddings and report the clustering ratios in Figure  5 . As the style encoder is pre-trained without the emotion intensity mechanism, we report the results of Emovox without intensity control for a fair comparison, which is denoted as Emovox w/o intensity. We first observe that Emovox w/o intensity always achieves a better clustering performance than the style encoder in Figure  5 . From Figure  5 (b) -(d), it is observed that both loss functions L cls and L sim contribute to a lower r, which suggests a better clustering performance. From Figure  5 (e), we further observe a more distinct separation between the emotions with different energy (such as neutral, sad vs angry, or happy). With both L cls and L sim , we obtain the lowest clustering ratio at 0.514. It shows that these two losses can help the emotion encoder to generate more discriminative emotional representations.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we report our experimental settings. For all the experiments, we conduct emotion conversion from neutral to angry, neutral to happy, and neutral to sad, which we denote as Neu-Ang, Neu-Hap, and Neu-Sad, respectively. We have made the source codes and speech samples available to the public 1 . We encourage readers to listen to the speech samples to understand this work.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Reference Methods And Setups",
      "text": "We implement 3 state-of-the-art emotional voice conversion methods as the reference baselines, that are summarized as follows:\n\n• CycleGAN-EVC  [28]  (baseline): CycleGAN-based emotional voice conversion with WORLD vocoder  [115] , where the fundamental frequency (F0) is analyzed with continuous wavelet transform;\n\n• StarGAN-EVC  [30]  (baseline): StarGAN-based emotional voice conversion with WORLD vocoder  [115] ;\n\n• Seq2Seq-EVC  [59]  (baseline): Sequence-to-sequence emotional voice conversion with a Parallel Wave-GAN vocoder  [116] ;\n\n• Emovox (proposed): Our proposed sequence-tosequence emotional voice conversion framework with a Parallel WaveGAN vocoder  [116]  shown in Figure  3 .\n\nNote that emotion intensity control is only available with Emovox. For a fair comparison among the methods, we obtain an intensity value for Emovox, by passing a reference set of speech data through the learnt ranking function, as shown in Figure  4  (\"Emotion Intensity Transfer\") . Besides, none of these frameworks require any parallel training data or frame alignment procedures.\n\nFor a contrastive study, we replace the intensity control module in Emovox with two other competing intensity control methods: Scaling Factor and Attention Weights through comprehensive experiments.\n\n• Emovox w/ Scaling Factor (proposed): where the emotion embedding is multiplied by a scaling factor  [20] ;\n\n• Emovox w/ Attention Weights (proposed): where the attention weight vector obtained from a pre-trained SER is used to represent the intensity  [18] ;\n\n• Emovox w/ Relative Attributes (proposed): our proposed method with relative attributes as described in Section 3;\n\nTo summarize, we do emotion intensity transfer to compare Emovox with the baselines (i. e., CycleGAN-EVC, StarGAN-EVC and Seq2Seq-EVC) and emotion intensity control to compare it with other emotion intensity control methods (i. e., Emovox w/ scaling factor, Emovox w/ attention weights).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Experimental Setup",
      "text": "We extract 80-dimensional Mel-spectrograms every 12.5 ms with a frame size of 50 ms for short-time Fourier transform (STFT). We then take the logarithm of the Mel-spectrograms to serve as the acoustic features. We convert text to phoneme with the Festival  [117]  G2P tool to serve as the input to the text encoder.\n\nWe use the Adam optimizer  [118]  and set the batch size to 64 and 16 for style pre-training and emotion training, respectively. We set the learning rate to 0.001 for style pre-training and halve it every seven epochs during the emotion training. We set the weight decay to 0.0001, and the weighting factors of the emotion classification loss L cls and the emotion similarity loss L sim to 1.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Recognition-Synthesis Structure",
      "text": "Emovox has a recognition-synthesis structure similar to that of  [56] ,  [119] . The Seq2Seq recognition encoder consists of an encoder which is a 2-layer 256-cell BLSTM, and a decoder which is a 1-layer 512-cell LSTM with an attention layer followed by an FC layer with an output channel of 512. Our text encoder is a 3-layer 1D CNN with a kernel size of 5 and the channel number of 512, followed by 1-layer of 256-cell BLSTM and an FC layer with an output channel number of 512. The Seq2Seq decoder has the same model architecture as that of Tacotron  [37] . The style encoder is a 2-layer of 128cell BLSTM followed by an FC layer with an output channel number of 128, which has been used in previous studies on voice conversion  [56]  and emotional voice conversion  [59] . The classifier is a 4-layer net of FC with the channel numbers of {512, 512, 512, 99}.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Relative Emotion Intensity Ranking",
      "text": "We follow an open-source implementation 2 to train the relative ranking function for emotion intensity. We extract 384-dimensional acoustic features with openSMILE  [120]  including zero-crossing rate, frame energy, pitch frequency, Mel-frequency cepstral coefficient (MFCC), and etc. These acoustic features are used in the Interspeech Emotion Challenge  [121] . We anticipate that these acoustic features can capture the subtle emotion intensity variations in speech. For each emotion category, we train a relative ranking function using neutral and emotional utterances.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Speech Emotion Recognizer",
      "text": "We train a speech emotion recognizer following a publicly available implementation  [106] . The SER includes: 1) 3 TimeDistributed two-dimensional (2-D) convolutional neural network (CNN) layers, 2) a DBLSTM layer, 3) an attention layer, and 4) a linear projection layer. The TimeDistributed 2D CNN layers and the DBLSTM layer summarize the temporal information into a fixed-length latent representation. The attention layer further preserves the effective emotional information while reducing the influence of emotion-irrelevant factors and producing discriminative utterance-level features for emotion prediction. The linear projection layer predicts the emotion class possibility from the utterance-level emotional features. We perform data augmentation by adding white Gaussian noise to improve the robustness of SER (  [122] ,  [123] ,  [124] ,  [125] ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Preparation And Emotion Training",
      "text": "We first perform style pre-training on the VCTK Corpus  [104] , where we use 99 speakers for pre-training. The total duration of pre-training speech data is about 30 hours. For SER training and emotion training, we randomly choose one male speaker from the ESD database 3 to conduct all the experiments in the same way as in  [59] .\n\nWe follow the data partition protocol given in the ESD database. For each emotion, we use 300 utterances for emotion training and 20 utterances as the evaluation set. We use In the emotion training, we first initialize all the modules with the weights learnt from style pre-training, where the style encoder and style classifier act as the emotion encoder and emotion classifier, respectively. We then randomly initialize the last projection layer of the emotion encoder and emotion classifier. The output channel numbers of the emotion encoder and the emotion classifier are set to 64 and 4, respectively. A learnt ranking function predicts a relative attribute and then is passed through an FC layer with the output channel size of 64 to obtain the intensity embedding. We then concatenate the emotion and intensity embedding to feed into the Seq2Seq decoder. The waveform is reconstructed from the converted Mel-spectrograms using Parallel WaveGAN. We use a public version of Parallel WaveGAN 4 , and train it with the ESD database.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Objective Evaluation",
      "text": "We first conduct an objective evaluation to assess the system performance using Mel-cepstral Distortion (MCD) and Differences of Duration (DDUR) as the evaluation metrics.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Mel-Cepstral Distortion (Mcd)",
      "text": "MCD  [126]  is calculated between the converted and the target Mel-cepstral coefficients (MCEPs), i. e., ŷ = {ŷ m } and y = {y m },\n\nwhere M represents the dimension of the MCEPs. A lower value of MCD indicates a smaller spectral distortion, and thus a better performance. Note that, in the Seq2Seq-EVC and Emovox models, we adopt Mel-spectrograms as the acoustic features. Therefore, we calculate MCEPs separately from the speech waveform.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Differences Of Duration (Ddur)",
      "text": "To evaluate the distortion in terms of duration, we compute the average differences between the duration of the converted and the target utterances over the voiced parts (DDUR), which is widely used in voice conversion studies  [38] ,  [56] ,  [59] ,\n\n4. https://github.com/kan-bayashi/ParallelWaveGAN where Z and Ẑ represent the duration of the reference utterance and the converted utterance, respectively. A lower value of DDUR represents a better performance in terms of duration conversion.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "We adopt two subjective metrics: 1) a mean opinion score (MOS) test for emotion similarity evaluation, and 2) a bestworst scaling (BWS) test to evaluate speech quality, emotion intensity, and emotion similarity. 18 subjects participated in all the listening tests. These 18 subjects (12 male and 6 female) are native Chinese speakers and proficient in English. Their age range is between 20-30. All the subjects are required to listen with headphones and replay each sample 2-3 times. A detailed introduction about the judging criteria is given before the tests.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Mean Opinion Score (Mos) Test",
      "text": "We conduct a mean opinion score (MOS)  [127]  test to evaluate the emotion similarity. All participants are asked to listen to the reference target speech first and then score the speech samples for emotion similarity to the reference target speech. A higher score represents a higher similarity with the target emotion, and indicates a better emotion conversion performance. We randomly select 10 utterances from the evaluation set. Each subject listens to 120 converted utterances in total (120 = 10 x 4 (# of frameworks) x 3 (# of emotion pairs)).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Best-Worst Scaling (Bws) Test",
      "text": "We also conduct a best-worst scaling (BWS)  [128]  test to evaluate:\n\n1) Speech Quality: where all the listeners are asked to choose the best and the worst sample in terms of the speech quality, which covers two aspects: a) how the linguistic and speaker identity is preserved, and b) the naturalness of the speech; 2) Emotion Intensity: where all the listeners are asked to choose the most and the least expressive one in terms of the emotion expression; 3) Emotion Similarity: where all the listeners are asked to choose the best and the worst one in terms of the emotion similarity with the reference.\n\nWe randomly select 5 utterances from the evaluation set to perform the BWS tests. We first evaluate the performance of different intensity control methods in terms of speech quality and intensity control. Each subject listens to 135 converted utterances (135 = 5 x 3 (# of frameworks) x 3 (# of intensities) x 3 (# of emotion pairs)). We further conduct an ablation study with Emovox, where each subject listens to 60 converted utterances in total to evaluate the emotion similarity with the reference (60 = 5 x 4 (# of frameworks) x 3 (# of emotion pairs)).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results",
      "text": "In this section, we report our experimental results. We first compare the performance of Emovox with that of the baselines using objective and subjective evaluations in Section 5.1. We then evaluate the proposed emotion intensity control method through the comparison with other control methods in Section 5.2. While comparing with the baselines, we use different training data settings in Section 5.3. Lastly, we study the contributions of the training strategies using ablation experiments in Section 5.4.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emovox Versus Baselines",
      "text": "In this subsection, we include CycleGAN-EVC, StarGAN-EVC, and Seq2Seq-EVC as baselines. It is noted that these baselines do not have an intensity control module. As a fair comparison, we conduct emotion intensity transfer for Emovox in both objective and subjective evaluations.  1 , all systems achieve better MCD values than that of the Zero Effort case. Zero Effort case directly compares the source and target utterances without any conversion. We also observe that Emovox completely outperforms CycleGAN-EVC and StarGAN-EVC. It also outperforms Seq2Seq-EVC for Neu-Ang and Neu-Hap (first three letters of source and target emotion, each) and achieves comparable results for Neu-Sad. This suggests that Emovox is superior to the others in terms of spectrum conversion. 'XUDWLRQ V G% G% (e) Converted Sad (Intensity = 0.9) Fig.  7 : Visualization of Mel-spectrograms from source neutral, reference sad, and converted sad utterances at three intensity values, i. e., 0.1, 0.5, 0.9, with the same speaking content (\"At the end of four\"). A greater intensity value represents a more emotional expression.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Objective Evaluation",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Mel-Cepstral Distortion (Mcd): As Shown In Table",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Differences Of",
      "text": "do not convert the speech duration. Thus, the DDUR results of these two frameworks are not reported. As shown in Table  1 , compared with Seq2Seq-EVC, Emovox achieves better results for both Neu-Ang and Neu-Hap for duration modelling, and achieves comparable results in Neu-Sad conversion. These results further confirm the effectiveness of Emovox in terms of duration conversion.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "We report Mean Opinion Score (MOS) test results for emotion similarity with the reference for our proposed Emovox and all the baselines. From Figure  6 , we observe that our proposed Emovox consistently outperforms the baselines for all the emotion pairs. This observation is consistent with  Note: Emovox w/ scaling factor, Emovox w/ attention weights, and Emovox w/ relative attributes are denoted as Scaling, Attention, and Relative respectively. \"B\" denotes \"Best\", and \"W\" denotes \"Worst\".\n\nthat in the objective evaluation. As for statistical significance, Emovox achieves the most narrow confidence interval for Neu-Ang and Neu-Hap, that suggests a high level of consistency  [129] . Furthermore, we report the p-value of t-test scores of MOS between Emovox and the others. We observe that almost all pairs achieve a p-value below 0.05, confirming the significant results  [130] . For Neu-Ang, the p-value between Emovox and Seq2Seq-EVC is about 0.0558, which is less than 0.1 and still supports our claim.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Emotion Intensity Control",
      "text": "To evaluate the emotion intensity control in Emovox, we choose three different intensity values: 0.1, 0.5, and 0.9, corresponding to weak, medium, and strong. To understand the interplay between emotion intensity and different prosodic attributes, we first visualize several related prosodic cues of the converted emotional utterances with the same speaking content but different emotion intensities.\n\nWe then compare our intensity control methods with other state-of-the-art methods.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Visual Comparisons",
      "text": "We visualize the prosodic attributes related to the emotion intensity, such as speech duration, pitch, and energy to gain an intuitive understanding of emotion intensity in vocal speech. Besides, we also would like to show that the emotion intensity control can be manifested in the changes of these prosodic features in our proposed framework.\n\n(1) Duration: Speech duration is considered as a distinct factor between active and passive emotions  [131] . To show that the emotion intensity is related to speaking rate, we compare the Mel-spectrogram of Sad as a reference emotion, which is characterized with a slower speaking rate and more resonant timbre  [132] , with that of its Neutral emotion counterpart in Figure  7 (a). We also illustrate the Melspectrograms of Emovox-converted utterances with different intensities in Figure  7 (c),(d) and (e). We observe that the converted Sad utterance with the highest intensity value has the slowest speaking rate among all three intensities (as shown in Figure  7(e) ). As the intensity value increases, the speaking rate decreases.\n\n(2) Pitch Envelope: Pitch envelope (i. e., the level, range, and shape of the pitch contour) is considered a major factor that contributes to the speech emotion, which is closely  correlated to the activity level  [132] ,  [133] . We represent pitch information with F0 contour, which is estimated with the harvest algorithm  [134]  and aligned with dynamic time warping  [135] . In Figure  8 , we visualize the pitch contour of converted Angry, Happy, and Sad utterances with three different intensities. From Figure  8 (a) and (b), we observe that the converted Angry and Happy utterances with higher intensity values tend to have higher F0 values with larger fluctuations over time. This coincides the fact that the utterances with higher intensity values are more vibrant and sharper in expressing emotions such as Angry and Happy.\n\nFor Sad, there is no big difference in F0 range for different intensities, as shown in Figure  8(c ). This observation intuitively suggests that the intensity of expressing Sad emotion may be more related to the speaking rate than the vocal pitch.\n\n(3) Speech Energy: Speech energy measures the volume or the loudness of a voice  [136] ,  [137] . Speech energy is often regarded as a prominent character of emotion intensity in the literature  [46] ,  [138] . To show the effect of intensity control, we visualize and compare the energy contour of different intensities in Figure  9 . To represent the speech energy, we use 26 Mel-filterbanks and multiply each of them with the power spectrum. Then, we can measure the speech energy by adding up the coefficients. As shown in Figure  9 (a) and (b), we observe that the converted Angry and Happy utterances with higher intensity have lager energy values, which is consistent with our observations on the F0 contour. As for Sad, we similarly observe that a higher intensity results in slightly higher energy values as shown in Figure  9 (c). These observations show that our proposed Emovox can effectively control the emotion intensity manifested in multiple prosodic factors in vocal speech.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Comparison With State-Of-The-Art Control Methods",
      "text": "As a comparative study, we implement three intensity control methods (i. e., Emovox w/ scaling factor, Emovox w/ attention weights, and Emovox w/ relative attributes) as described in Section 4.1. We evaluate the performance of these three methods in terms of speech quality and intensity control.\n\n(1) Speech Quality: We first report the BWS listening test on Emovox for speech quality evaluation in Table  2 . At each intensity value, the subjects are asked to evaluate the speech quality of the converted emotional speech with 3 different emotion intensity control methods.\n\nFrom Table  2 , we observe that Emovox w/ relative attributes always achieves the best results for Neu-Ang and Neu-Hap, and comparable results with Emovox w/ attention weights for Neu-Sad. These results show that Emovox w/ relative attributes can achieve better speech quality while controlling the output emotion intensity than other control methods.\n\n(2) Intensity Control: We then report another BWS test to evaluate the performance of emotion intensity control. For each framework, listeners are asked to assess the emotional expressiveness among three different intensities. We conjecture that the speech samples with an intensity value of 0.9 sound more expressive than others, while those with an intensity value of 0.1 sound more neutral. We report the  preference percentage scores (%) of the most and the least expressiveness for each controlling method in Figure  11  and Figure  10 , respectively. As illustrated in Figure  11 (c), Emovox w/ relative attributes achieve the best preference results on intensity control, where most listeners choose the samples with an intensity value of 0.9 as the most expressive ones. We also note that Emovox w/ scaling factor and Emovox w/ attention weights work well for converted angry and happy, as shown in Figure  11 (a) and (b). However, their performance of converted sad is not satisfactory. We further observe that Emovox w/ relative attributes also work better than the others, where most listeners choose the samples with an intensity value of 0.1 as the least expressive ones, as shown in Figure  10(c ). This observation is consistent with the previous one, which further validates the superior performance of relative attributes on emotion intensity control.\n\nAs a summary, our proposed Emovox w/ relative attributes shows better performance on emotion intensity control while achieving better speech quality than other control methods.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Impact Of Training Data Size",
      "text": "To evaluate the effect of training data on the final performance, we gradually reduce the number of utterances used at the emotion training stage. We use 300, 150, 100, and 50 training utterances for each emotion, and use 20 utterances for evaluation. In Figure  12 , we report the MCD and DDUR results of Emovox and the baseline Seq2Seq-EVC.\n\nWe observe that Emovox consistently achieves better MCD results than Seq2Seq-EVC. We further observe that the MCD scores for Emovox between 150 to 50 training utterances are comparable and not significantly poorer than that of using 300 utterances. This indicates Emovox's robustness to limited training data.\n\nFor DDUR, we first observe that both Emovox and Seq2Seq-EVC have much higher DDUR values with 50 training utterances. It suggests that both frameworks cannot predict the speech duration well if the training size is too small. Between 300 to 100 training utterances, the performance of Emovox is comparable, which again attest to Emovox's robustness to limited training data.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Ablation Studies",
      "text": "We conduct ablation studies to validate the contributions of 1) style pre-training, and 2) perceptual losses from pretrained SER in emotion training.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Style Pre-Training",
      "text": "We first compare the Mel-cepstral (MCD) results of Emovox and Emovox (w/o style pre-training), where the latter is trained directly with a limited amount of emotional speech data and without any pre-training. As shown in Table  1 , Emovox (w/o style pre-training) provides the worst results for all emotion pairs. We further compare Emovox and Emovox (w/o style pretraining) in terms of DDUR as shown in Table  1 . We observe that Emovox (w/o style pre-training) has the worst DDUR results. These results validate the effectiveness of style pretraining.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Perceptual Loss Functions",
      "text": "As discussed in Section 3.3.3, we expect that the emotion embedding similarity loss L sim and emotion classification loss L cls help generate more discriminative embeddings (see Figure  5 ). To further validate the effectiveness of these two loss functions on the final performance, we conduct a best-worse scaling listening test where we evaluate the emotion similarity with the reference emotion. To be consistent with Section 3.3.3, we only conduct an ablation study with the Emovox w/o intensity configuration. The results are reported in Table  3 .\n\nFrom Table  3 , we observe that most listeners choose \"Emovox w/o intensity, w/ L sim and L cls \" as the best in terms of emotion similarity, while most of them choose \"Emovox w/o intensity, w/o L sim or L cls \" as the worst for all the emotion pairs. This suggests that these two loss functions improve emotional expressiveness, which validates the idea of incorporating SER losses for emotion supervision.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "This contribution filled the research gap of emotion intensity control in current emotional voice conversion literature. We proposed a novel emotional voice conversion framework -Emovox -that is based on a sequence-to-sequence model. The proposed Emovox framework provides a fine-grained, effective emotion intensity control for the first time in emotional voice conversion. The key highlights are as follows:\n\n1) We formulated an emotion intensity modeling technique and proposed an emotion intensity controlling mechanism based on relative attributes. We proved that our proposed mechanism outperformed other competing controlling methods in speech quality and emotion intensity control. 2) Instead of simply correlating emotion intensity with the loudness of a voice, we presented a comprehensive analysis for the first time to understand the interplay between emotion intensity and various prosodic attributes such as speech duration, pitch envelope, and speech energy. We showed that our emotion intensity control could be manifested in various prosodic aspects. 3) We proposed style pre-training and perceptual losses from a pre-trained SER to improve the emotion intelligibility in converted emotional speech. We showed that Emovox outperformed state-of-thearts emotional voice conversion frameworks. With style pre-training and perceptual losses from a pretrained SER, Emovox could effectively perform well with a limited amount of emotional speech data.\n\nOur future directions include the study of cross-lingual emotional voice conversion and emotion style modelling with self-supervised learning. In addition, a closer coupling of conversion and speech emotion recognition is foreseen: conversion can help augment training data for recognition, while recognition can serve as objective conversion training guidance.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of a speech emotion recognizer (SER)",
      "page": 2
    },
    {
      "caption": "Figure 3: To overcome the issues such as deletion and repetition",
      "page": 4
    },
    {
      "caption": "Figure 2: Block diagram of Emovox during the conversion stage.",
      "page": 4
    },
    {
      "caption": "Figure 3: Overall training diagram of Emovox, where emotion and its intensity are separately modelled. Two utterance-level",
      "page": 5
    },
    {
      "caption": "Figure 3: , the learnt ranking function",
      "page": 5
    },
    {
      "caption": "Figure 3: , we obtain the relative attribute",
      "page": 5
    },
    {
      "caption": "Figure 4: , we ﬁrst use the emotion encoder",
      "page": 6
    },
    {
      "caption": "Figure 5: (a). We, there-",
      "page": 6
    },
    {
      "caption": "Figure 3: is not explicitly trained for characterisa-",
      "page": 6
    },
    {
      "caption": "Figure 4: An illustration of the run-time conversion phase.",
      "page": 6
    },
    {
      "caption": "Figure 5: The distributions of emotion embeddings resulting from encoders of different training schemes: (a) the pre-trained",
      "page": 7
    },
    {
      "caption": "Figure 5: (e). It is observed that emotion embeddings form",
      "page": 7
    },
    {
      "caption": "Figure 5: From Figure 5(b) - (d), it is",
      "page": 7
    },
    {
      "caption": "Figure 5: (e), we further observe a more distinct sepa-",
      "page": 7
    },
    {
      "caption": "Figure 3: Note that emotion intensity control is only available with",
      "page": 8
    },
    {
      "caption": "Figure 4: (”Emotion Intensity Transfer”) . Besides,",
      "page": 8
    },
    {
      "caption": "Figure 6: Mean Opinion Score (MOS) test with 95 % conﬁdence",
      "page": 9
    },
    {
      "caption": "Figure 7: Visualization of Mel-spectrograms from source neu-",
      "page": 10
    },
    {
      "caption": "Figure 6: , we observe that our",
      "page": 10
    },
    {
      "caption": "Figure 8: A comparison of the pitch contour from the emotional utterances converted by Emovox with three different emotion",
      "page": 11
    },
    {
      "caption": "Figure 9: A comparison of the speech energy from the emotional utterances converted by Emovox with three different emotion",
      "page": 11
    },
    {
      "caption": "Figure 7: (a). We also illustrate the Mel-",
      "page": 11
    },
    {
      "caption": "Figure 7: (c),(d) and (e). We observe that the",
      "page": 11
    },
    {
      "caption": "Figure 7: (e)). As the intensity value increases, the",
      "page": 11
    },
    {
      "caption": "Figure 10: A comparison of the preference percentage scores (%) of Emovox with 3 different emotion intensity control methods.",
      "page": 12
    },
    {
      "caption": "Figure 11: A comparison of the preference percentage scores (%) of Emovox with 3 different emotion intensity control methods,",
      "page": 12
    },
    {
      "caption": "Figure 8: , we visualize the pitch contour",
      "page": 12
    },
    {
      "caption": "Figure 8: (a) and (b), we observe",
      "page": 12
    },
    {
      "caption": "Figure 8: (c). This observation intu-",
      "page": 12
    },
    {
      "caption": "Figure 9: To represent the speech",
      "page": 12
    },
    {
      "caption": "Figure 9: (a) and (b), we observe that the converted Angry and Happy",
      "page": 12
    },
    {
      "caption": "Figure 9: (c). These observations show that our proposed Emovox",
      "page": 12
    },
    {
      "caption": "Figure 12: A comparison of MCD and DDUR results of our proposed Emovox and Seq2Seq-EVC for all three emotion pairs",
      "page": 13
    },
    {
      "caption": "Figure 10: , respectively.",
      "page": 13
    },
    {
      "caption": "Figure 11: (c), Emovox w/ relative at-",
      "page": 13
    },
    {
      "caption": "Figure 11: (a) and (b). However, their performance of",
      "page": 13
    },
    {
      "caption": "Figure 10: (c). This observation is consistent with the pre-",
      "page": 13
    },
    {
      "caption": "Figure 12: , we report the MCD and DDUR",
      "page": 13
    },
    {
      "caption": "Figure 5: ). To further validate the effectiveness of these",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Framework": "",
          "MCD [dB]": "Neu-Ang\nNeu-Hap\nNeu-Sad",
          "DDUR [s]": "Neu-Ang\nNeu-Hap\nNeu-Sad"
        },
        {
          "Framework": "Zero Effort\nCycleGAN-EVC\nStarGAN-EVC\nSeq2Seq-EVC\nEmovox (w/o style pre-training)\nEmovox",
          "MCD [dB]": "6.47\n6.64\n6.22\n4.57\n4.46\n4.32\n4.43\n4.25\n4.31\n4.23\n4.29\n4.16\n5.36\n5.32\n5.42\n4.13\n4.15\n4.25",
          "DDUR [s]": "0.36\n0.26\n0.46\n-\n-\n-\n-\n-\n-\n0.27\n0.28\n0.20\n0.79\n0.80\n0.92\n0.24\n0.17\n0.31"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: The effect of the perceptual loss function for the emotion similarity in a best-worst scaling (BWS) test for four",
      "data": [
        {
          "Emovox w/o Intensity": "",
          "Neu-Ang": "Best\nWorst",
          "Neu-Hap": "Best\nWorst",
          "Neu-Sad": "Best\nWorst"
        },
        {
          "Emovox w/o Intensity": "w/ Lsim and Lcls\nw/ Lsim\nw/ Lcls\nw/o Lsim or Lcls",
          "Neu-Ang": "56%\n5%\n33%\n22%\n9%\n24%\n2%\n49%",
          "Neu-Hap": "49%\n13%\n47%\n13%\n2%\n24%\n2%\n49%",
          "Neu-Sad": "33%\n16%\n11%\n25%\n22%\n22%\n20%\n51%"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Handling emotions in human-computer dialogues",
      "authors": [
        "J Pittermann",
        "A Pittermann",
        "W Minker"
      ],
      "year": "2010",
      "venue": "Handling emotions in human-computer dialogues"
    },
    {
      "citation_id": "3",
      "title": "A survey of using vocal prosody to convey emotion in robot speech",
      "authors": [
        "J Crumpton",
        "C Bethel"
      ],
      "year": "2016",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "4",
      "title": "Prosodic aspects of the attractive voice",
      "authors": [
        "A Rosenberg",
        "J Hirschberg"
      ],
      "year": "2021",
      "venue": "Voice Attractiveness"
    },
    {
      "citation_id": "5",
      "title": "An overview of voice conversion and its challenges: From statistical modeling to deep learning",
      "authors": [
        "B Sisman",
        "J Yamagishi",
        "S King",
        "H Li"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Speech Enhancement, Modeling and Recognition-Algorithms and Applications",
      "authors": [
        "S Ramakrishnan"
      ],
      "year": "2012",
      "venue": "Speech Enhancement, Modeling and Recognition-Algorithms and Applications"
    },
    {
      "citation_id": "7",
      "title": "Spectral voice conversion for textto-speech synthesis",
      "authors": [
        "A Kain",
        "M Macon"
      ],
      "year": "1998",
      "venue": "Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)"
    },
    {
      "citation_id": "8",
      "title": "Exploration of analyzing emotion strength in speech signal",
      "authors": [
        "J Wang",
        "R Dou",
        "Z Yan",
        "Z Wang",
        "Z Zhang"
      ],
      "year": "2009",
      "venue": "2009 Chinese Conference on Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "",
      "authors": [
        "J Averill",
        "T More"
      ],
      "year": "1993",
      "venue": ""
    },
    {
      "citation_id": "10",
      "title": "Hot or cold anger? verbal and vocal expression of anger while driving in a simulated anger-provoking scenario",
      "authors": [
        "F Biassoni",
        "S Balzarotti",
        "M Giamporcaro",
        "R Ciceri"
      ],
      "year": "2016",
      "venue": "Sage Open"
    },
    {
      "citation_id": "11",
      "title": "The intensity of emotion",
      "authors": [
        "J Brehm"
      ],
      "year": "1999",
      "venue": "Personality and social psychology review"
    },
    {
      "citation_id": "12",
      "title": "complexity of intensity: Issues concerning the structure of emotion intensity",
      "authors": [
        "N Frijda",
        "A Ortony",
        "J Sonnemans",
        "G Clore"
      ],
      "year": "1992",
      "venue": "complexity of intensity: Issues concerning the structure of emotion intensity"
    },
    {
      "citation_id": "13",
      "title": "Prosody conversion from neutral speech to emotional speech",
      "authors": [
        "J Tao",
        "Y Kang",
        "A Li"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Detecting pitch accents at the word, syllable and vowel level",
      "authors": [
        "A Rosenberg",
        "J Hirschberg"
      ],
      "year": "2009",
      "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Measuring acoustic-prosodic entrainment with respect to multiple levels and dimensions",
      "authors": [
        "R Levitan",
        "J Hirschberg"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "16",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "17",
      "title": "Controlling the Strength of Emotions in Speech-Like Emotional Sound Generated by WaveNet",
      "authors": [
        "K Matsumoto",
        "S Hara",
        "M Abe"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Improving emotional tts with an emotion intensity input from unsupervised extraction",
      "authors": [
        "B Schnell",
        "P Garner"
      ],
      "venue": "Proc. 11th ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "19",
      "title": "Emotional speech synthesis with rich and granularized control",
      "authors": [
        "S.-Y Um",
        "S Oh",
        "K Byun",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "20",
      "title": "Sequence-to-sequence emotional voice conversion with strength control",
      "authors": [
        "H Choi",
        "M Hahn"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Gmmbased emotional voice conversion using spectrum and prosody features",
      "authors": [
        "R Aihara",
        "R Takashima",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2012",
      "venue": "American Journal of Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Gmm-based voice conversion applied to emotional speech synthesis",
      "authors": [
        "H Kawanami",
        "Y Iwami",
        "T Toda",
        "H Saruwatari",
        "K Shikano"
      ],
      "year": "2003",
      "venue": "Gmm-based voice conversion applied to emotional speech synthesis"
    },
    {
      "citation_id": "23",
      "title": "Exemplar-based emotional voice conversion using non-negative matrix factorization",
      "authors": [
        "R Aihara",
        "R Ueda",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2014",
      "venue": "APSIPA ASC"
    },
    {
      "citation_id": "24",
      "title": "Data-driven emotion conversion in spoken english",
      "authors": [
        "Z Inanoglu",
        "S Young"
      ],
      "year": "2009",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "25",
      "title": "Investigating different representations for modeling and controlling multiple emotions in dnn-based speech synthesis",
      "authors": [
        "J Lorenzo-Trueba",
        "G Henter",
        "S Takaki",
        "J Yamagishi",
        "Y Morino",
        "Y Ochiai"
      ],
      "year": "2018",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "26",
      "title": "Emotional voice conversion with adaptive scales f0 based on wavelet transform using limited amount of emotional data",
      "authors": [
        "Z Luo",
        "J Chen",
        "T Takiguchi",
        "Y Ariki"
      ],
      "year": "2017",
      "venue": "Emotional voice conversion with adaptive scales f0 based on wavelet transform using limited amount of emotional data"
    },
    {
      "citation_id": "27",
      "title": "Deep bidirectional lstm modeling of timbre and prosody for emotional voice conversion",
      "authors": [
        "H Ming",
        "D Huang",
        "L Xie",
        "J Wu",
        "M Dong",
        "H Li"
      ],
      "year": "2016",
      "venue": "Deep bidirectional lstm modeling of timbre and prosody for emotional voice conversion"
    },
    {
      "citation_id": "28",
      "title": "Transforming Spectrum and Prosody for Emotional Voice Conversion with Non-Parallel Training Data",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Odyssey 2020 The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "29",
      "title": "Non-parallel emotion conversion using a deep-generative hybrid network and an adversarial pair discriminator",
      "authors": [
        "R Shankar",
        "J Sager",
        "A Venkataraman"
      ],
      "year": "2020",
      "venue": "Non-parallel emotion conversion using a deep-generative hybrid network and an adversarial pair discriminator",
      "arxiv": "arXiv:2007.12932"
    },
    {
      "citation_id": "30",
      "title": "Stargan for emotional speech conversion: Validated by data augmentation of end-to-end emotion recognition",
      "authors": [
        "G Rizos",
        "A Baird",
        "M Elliott",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Converting Anyone's Emotion: Towards Speaker-Independent Emotional Voice Conversion",
      "authors": [
        "K Zhou",
        "B Sisman",
        "M Zhang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "32",
      "title": "Vaw-gan for disentanglement and recomposition of emotional elements in speech",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "33",
      "title": "Nonparallel emotional speech conversion",
      "authors": [
        "J Gao",
        "D Chakraborty",
        "H Tembine",
        "O Olaleye"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "Multi-speaker emotion conversion via latent variable regularization and a chained encoder-decoder-predictor network",
      "authors": [
        "R Shankar",
        "H.-W Hsieh",
        "N Charon",
        "A Venkataraman"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "35",
      "title": "F0consistent many-to-many non-parallel voice conversion via conditional autoencoder",
      "authors": [
        "K Qian",
        "Z Jin",
        "M Hasegawa-Johnson",
        "G Mysore"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Char2wav: End-to-end speech synthesis",
      "authors": [
        "K Kyle",
        "K Jose",
        "S Sotelo"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations, workshop"
    },
    {
      "citation_id": "37",
      "title": "Tacotron: Towards endto-end speech synthesis",
      "authors": [
        "Y Wang",
        "R Skerry-Ryan",
        "D Stanton",
        "Y Wu",
        "R Weiss",
        "N Jaitly",
        "Z Yang",
        "Y Xiao",
        "Z Chen",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "Tacotron: Towards endto-end speech synthesis",
      "arxiv": "arXiv:1703.10135"
    },
    {
      "citation_id": "38",
      "title": "Sequence-to-sequence acoustic modeling for voice conversion",
      "authors": [
        "J.-X Zhang",
        "Z.-H Ling",
        "L.-J Liu",
        "Y Jiang",
        "L.-R Dai"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Atts2svc: Sequence-to-sequence voice conversion with attention and context preservation mechanisms",
      "authors": [
        "K Tanaka",
        "H Kameoka",
        "T Kaneko",
        "N Hojo"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Many-to-many voice transformer network",
      "authors": [
        "H Kameoka",
        "W.-C Huang",
        "K Tanaka",
        "T Kaneko",
        "N Hojo",
        "T Toda"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Textless speech emotion conversion using decomposed and discrete representations",
      "authors": [
        "F Kreuk",
        "A Polyak",
        "J Copet",
        "E Kharitonov",
        "T.-A Nguyen",
        "M Rivière",
        "W.-N Hsu",
        "A Mohamed",
        "E Dupoux",
        "Y Adi"
      ],
      "year": "2021",
      "venue": "Textless speech emotion conversion using decomposed and discrete representations",
      "arxiv": "arXiv:2111.07402"
    },
    {
      "citation_id": "42",
      "title": "Sequence-to-sequence modelling of f0 for speech emotion conversion",
      "authors": [
        "C Robinson",
        "N Obin",
        "A Roebel"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "Emotional voice conversion using multitask learning with text-to-speech",
      "authors": [
        "T.-H Kim",
        "S Cho",
        "S Choi",
        "S Park",
        "S.-Y Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "The dictionary of affect in language",
      "authors": [
        "C Whissell"
      ],
      "year": "1989",
      "venue": "The measurement of emotions"
    },
    {
      "citation_id": "45",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "46",
      "title": "Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion",
      "authors": [
        "P Juslin",
        "P Laukka"
      ],
      "year": "2001",
      "venue": "Emotion"
    },
    {
      "citation_id": "47",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "48",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Expressive tts training with frame and style reconstruction loss",
      "authors": [
        "R Liu",
        "B Sisman",
        "G Gao",
        "H Li"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "50",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "51",
      "title": "Convs2s-vc: Fully convolutional sequence-to-sequence voice conversion",
      "authors": [
        "H Kameoka",
        "K Tanaka",
        "D Kwaśny",
        "T Kaneko",
        "N Hojo"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "52",
      "title": "Non-autoregressive sequence-to-sequence voice conversion",
      "authors": [
        "T Hayashi",
        "W.-C Huang",
        "K Kobayashi",
        "T Toda"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "Fasts2s-vc: Streaming non-autoregressive sequence-to-sequence voice conversion",
      "authors": [
        "H Kameoka",
        "K Tanaka",
        "T Kaneko"
      ],
      "year": "2021",
      "venue": "Fasts2s-vc: Streaming non-autoregressive sequence-to-sequence voice conversion",
      "arxiv": "arXiv:2104.06900"
    },
    {
      "citation_id": "54",
      "title": "Pretraining techniques for sequence-to-sequence voice conversion",
      "authors": [
        "W.-C Huang",
        "T Hayashi",
        "Y.-C Wu",
        "H Kameoka",
        "T Toda"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "55",
      "title": "Anyto-many voice conversion with location-relative sequence-tosequence modeling",
      "authors": [
        "S Liu",
        "Y Cao",
        "D Wang",
        "X Liu",
        "H Meng"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "56",
      "title": "Non-parallel sequenceto-sequence voice conversion with disentangled linguistic and speaker representations",
      "authors": [
        "J.-X Zhang",
        "Z.-H Ling",
        "L.-R Dai"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "57",
      "title": "Improving sequence-to-sequence voice conversion by adding text-supervision",
      "authors": [
        "J.-X Zhang",
        "Z.-H Ling",
        "Y Jiang",
        "L.-J Liu",
        "C Liang",
        "L.-R Dai"
      ],
      "year": "2019",
      "venue": "IEEE ICASSP"
    },
    {
      "citation_id": "58",
      "title": "Acoustic feature analysis in speech emotion primitives estimation",
      "authors": [
        "D Wu",
        "T Parsons",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "59",
      "title": "Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training",
      "authors": [
        "K Zhou",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "60",
      "title": "The production and recognition of emotions in speech: features and algorithms",
      "authors": [
        "O Pierre-Yves"
      ],
      "year": "2003",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "61",
      "title": "Expressive speech: Production, perception and application to speech synthesis",
      "authors": [
        "D Erickson"
      ],
      "year": "2005",
      "venue": "Acoustical science and technology"
    },
    {
      "citation_id": "62",
      "title": "Adapting and controlling dnn-based speech synthesis using input codes",
      "authors": [
        "H.-T Luong",
        "S Takaki",
        "G Henter",
        "J Yamagishi"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "63",
      "title": "Multi-speaker modeling and speaker adaptation for dnn-based tts synthesis",
      "authors": [
        "Y Fan",
        "Y Qian",
        "F Soong",
        "L He"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "64",
      "title": "Principles for learning controllable tts from annotated and latent variation",
      "authors": [
        "G Henter",
        "J Lorenzo-Trueba",
        "X Wang",
        "J Yamagishi"
      ],
      "year": "2017",
      "venue": "Principles for learning controllable tts from annotated and latent variation"
    },
    {
      "citation_id": "65",
      "title": "Towards endto-end prosody transfer for expressive speech synthesis with tacotron",
      "authors": [
        "R Skerry-Ryan",
        "E Battenberg",
        "Y Xiao",
        "Y Wang",
        "D Stanton",
        "J Shor",
        "R Weiss",
        "R Clark",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "66",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R.-S Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "Y Jia",
        "F Ren",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "67",
      "title": "Robust and fine-grained prosody control of end-to-end speech synthesis",
      "authors": [
        "Y Lee",
        "T Kim"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "68",
      "title": "Fine-Grained Robust Prosody Transfer for Single-Speaker Neural Text-To-Speech",
      "authors": [
        "V Klimkov",
        "S Ronanki",
        "J Rohnke",
        "T Drugman"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "69",
      "title": "Towards Multi-Scale Style Control for Expressive Speech Synthesis",
      "authors": [
        "X Li",
        "C Song",
        "J Li",
        "Z Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "70",
      "title": "Fine-Grained Style Modeling, Transfer and Prediction in Text-to-Speech Synthesis via Phone-Level Content-Style Disentanglement",
      "authors": [
        "D Tan",
        "T Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "71",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "D Kingma",
        "M Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "72",
      "title": "Learning latent representations for style control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y.-J Zhang",
        "S Pan",
        "L He",
        "Z.-H Ling"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "73",
      "title": "Chive: Varying prosody in speech synthesis with a linguistically driven dynamic hierarchical conditional variational network",
      "authors": [
        "T Kenter",
        "V Wan",
        "C.-A Chan",
        "R Clark",
        "J Vit"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "74",
      "title": "Toward the simulation of emotion in synthetic speech: A review of the literature on human vocal emotion",
      "authors": [
        "I Murray",
        "J Arnott"
      ],
      "year": "1993",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "75",
      "title": "Speech prosody: A methodological review",
      "authors": [
        "Y Xu"
      ],
      "year": "2011",
      "venue": "Journal of Speech Sciences"
    },
    {
      "citation_id": "76",
      "title": "Can we generate emotional pronunciations for expressive speech synthesis?",
      "authors": [
        "M Tahon",
        "G Lecorvé",
        "D Lolive"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "77",
      "title": "Controlling emotion strength with relative attribute for end-to-end speech synthesis",
      "authors": [
        "X Zhu",
        "S Yang",
        "G Yang",
        "L Xie"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "78",
      "title": "Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis",
      "authors": [
        "Y Lei",
        "S Yang",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "79",
      "title": "Modeling emotion in complex stories: the stanford emotional narratives dataset",
      "authors": [
        "D Ong",
        "Z Wu",
        "Z.-X Tan",
        "M Reddan",
        "I Kahhale",
        "A Mattek",
        "J Zaki"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "80",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "81",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "82",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "83",
      "title": "Dnn-based emotion recognition based on bottleneck acoustic features and lexical features",
      "authors": [
        "E Kim",
        "J Shin"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "84",
      "title": "A review on five recent and near-future developments in computational processing of emotion in the human voice",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "85",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "86",
      "title": "Interactive text-to-speech via semi-supervised style transfer learning",
      "authors": [
        "Y Gao",
        "W Zheng",
        "Z Yang",
        "T Kohler",
        "C Fuegen",
        "Q He"
      ],
      "year": "2020",
      "venue": "Interactive text-to-speech via semi-supervised style transfer learning",
      "arxiv": "arXiv:2002.06758"
    },
    {
      "citation_id": "87",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "88",
      "title": "Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability",
      "authors": [
        "R Liu",
        "B Sisman",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "89",
      "title": "Controllable emotion transfer for end-to-end speech synthesis",
      "authors": [
        "T Li",
        "S Yang",
        "L Xue",
        "L Xie"
      ],
      "year": "2021",
      "venue": "2021 12th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "90",
      "title": "Emotion controllable speech synthesis using emotion-unlabeled dataset with the assistance of cross-domain speech emotion recognition",
      "authors": [
        "X Cai",
        "D Dai",
        "Z Wu",
        "X Li",
        "J Li",
        "H Meng"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "91",
      "title": "Communication and prosody: Functional aspects of prosody",
      "authors": [
        "J Hirschberg"
      ],
      "year": "2002",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "92",
      "title": "Emocat: Language-agnostic emotional voice conversion",
      "authors": [
        "B Schnell",
        "G Huybrechts",
        "B Perz",
        "T Drugman",
        "J Lorenzo-Trueba"
      ],
      "year": "2021",
      "venue": "Emocat: Language-agnostic emotional voice conversion"
    },
    {
      "citation_id": "93",
      "title": "Advances in neural information processing systems",
      "authors": [
        "V Ferrari",
        "A Zisserman"
      ],
      "year": "2007",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "94",
      "title": "Metric learning to rank",
      "authors": [
        "B Mcfee",
        "G Lanckriet"
      ],
      "year": "2010",
      "venue": "ICML"
    },
    {
      "citation_id": "95",
      "title": "Siamese neural networks for one-shot image recognition",
      "authors": [
        "G Koch",
        "R Zemel",
        "R Salakhutdinov"
      ],
      "year": "2015",
      "venue": "ICML deep learning workshop"
    },
    {
      "citation_id": "96",
      "title": "Relative attributes",
      "authors": [
        "D Parikh",
        "K Grauman"
      ],
      "year": "2011",
      "venue": "2011 International Conference on Computer Vision. IEEE"
    },
    {
      "citation_id": "97",
      "title": "Whittlesearch: Image search with relative attribute feedback",
      "authors": [
        "A Kovashka",
        "D Parikh",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "98",
      "title": "Robust relative attributes for human action recognition",
      "authors": [
        "Z Zhang",
        "C Wang",
        "B Xiao",
        "W Zhou",
        "S Liu"
      ],
      "year": "2015",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "99",
      "title": "Relative attributes for largeabandoned object detection",
      "authors": [
        "Q Fan",
        "P Gabbur",
        "S Pankanti"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "100",
      "title": "Training a support vector machine in the primal",
      "authors": [
        "O Chapelle"
      ],
      "year": "2007",
      "venue": "Neural computation"
    },
    {
      "citation_id": "101",
      "title": "Memory attention: Robust alignment using gating mechanism for end-to-end speech synthesis",
      "authors": [
        "J Lee",
        "S Cheon",
        "B Choi",
        "N Kim"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "102",
      "title": "The cmu arctic speech databases",
      "authors": [
        "J Kominek",
        "A Black"
      ],
      "year": "2004",
      "venue": "Fifth ISCA workshop on speech synthesis"
    },
    {
      "citation_id": "103",
      "title": "The lj speech dataset",
      "authors": [
        "K Ito",
        "L Johnson"
      ],
      "year": "2017",
      "venue": "The lj speech dataset"
    },
    {
      "citation_id": "104",
      "title": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit",
      "authors": [
        "C Veaux",
        "J Yamagishi",
        "K Macdonald"
      ],
      "year": "2016",
      "venue": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit"
    },
    {
      "citation_id": "105",
      "title": "Parallel wavenet: Fast high-fidelity speech synthesis",
      "authors": [
        "A Oord",
        "Y Li",
        "I Babuschkin",
        "K Simonyan",
        "O Vinyals",
        "K Kavukcuoglu",
        "G Driessche",
        "E Lockhart",
        "L Cobo",
        "F Stimberg"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "106",
      "title": "Speech-emotion-classification-with-pytorch",
      "year": "2018",
      "venue": "Speech-emotion-classification-with-pytorch"
    },
    {
      "citation_id": "107",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "108",
      "title": "Performance evaluation of some clustering algorithms and validity indices",
      "authors": [
        "U Maulik",
        "S Bandyopadhyay"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "109",
      "title": "Algorithms for clustering data",
      "authors": [
        "W Sarle"
      ],
      "year": "1990",
      "venue": "Algorithms for clustering data"
    },
    {
      "citation_id": "110",
      "title": "Evaluation of hierarchical clustering algorithms for document datasets",
      "authors": [
        "Y Zhao",
        "G Karypis"
      ],
      "year": "2002",
      "venue": "Proceedings of the eleventh international conference on Information and knowledge management"
    },
    {
      "citation_id": "111",
      "title": "Discovering knowledge in data: an introduction to data mining",
      "authors": [
        "D Larose",
        "C Larose"
      ],
      "year": "2014",
      "venue": "Discovering knowledge in data: an introduction to data mining"
    },
    {
      "citation_id": "112",
      "title": "Understanding of internal clustering validation measures",
      "authors": [
        "Y Liu",
        "Z Li",
        "H Xiong",
        "X Gao",
        "J Wu"
      ],
      "year": "2010",
      "venue": "2010 IEEE international conference on data mining"
    },
    {
      "citation_id": "113",
      "title": "V-measure: A conditional entropy-based external cluster evaluation measure",
      "authors": [
        "A Rosenberg",
        "J Hirschberg"
      ],
      "year": "2007",
      "venue": "Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning"
    },
    {
      "citation_id": "114",
      "title": "An effective style token weight control technique for end-to-end emotional speech synthesis",
      "authors": [
        "O Kwon",
        "I Jang",
        "C Ahn",
        "H.-G Kang"
      ],
      "year": "2019",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "115",
      "title": "World: a vocoder-based high-quality speech synthesis system for real-time applications",
      "authors": [
        "M Morise",
        "F Yokomori",
        "K Ozawa"
      ],
      "year": "2016",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "116",
      "title": "Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
      "authors": [
        "R Yamamoto",
        "E Song",
        "J.-M Kim"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "117",
      "title": "The festival speech synthesis system, version 1.4.2",
      "authors": [
        "A Black",
        "P Taylor",
        "R Caley",
        "R Clark",
        "K Richmond",
        "S King",
        "V Strom",
        "H Zen"
      ],
      "year": "2001",
      "venue": "The festival speech synthesis system, version 1.4.2"
    },
    {
      "citation_id": "118",
      "title": "Adam: Amethod for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "venue": "Adam: Amethod for stochastic optimization"
    },
    {
      "citation_id": "119",
      "title": "Recognition-Synthesis Based Non-Parallel Voice Conversion with Adversarial Learning",
      "authors": [
        "J.-X Zhang",
        "Z.-H Ling",
        "L.-R Dai"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "120",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "121",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "122",
      "title": "Speech emotion recognition in noisy and rever-berant environments",
      "authors": [
        "P Heracleous",
        "K Yasuda",
        "F Sugaya",
        "A Yoneyama",
        "M Hashimoto"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "123",
      "title": "Multi-conditioning and data augmentation using generative noise model for speech emotion recognition in noisy conditions",
      "authors": [
        "U Tiwari",
        "M Soni",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "124",
      "title": "Deep learning techniques for speech emotion recognition, from databases to models",
      "authors": [
        "B Abbaschian",
        "D Sierra-Sosa",
        "A Elmaghraby"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "125",
      "title": "Improved emotion recognition using gaussian mixture model and extreme learning machine in speech and glottal signals",
      "authors": [
        "H Muthusamy",
        "K Polat",
        "S Yaacob"
      ],
      "year": "2015",
      "venue": "Mathematical Problems in Engineering"
    },
    {
      "citation_id": "126",
      "title": "Mel-cepstral distance measure for objective speech quality assessment",
      "authors": [
        "R Kubichek"
      ],
      "year": "1993",
      "venue": "Proceedings of IEEE Pacific Rim Conference on Communications Computers and Signal Processing"
    },
    {
      "citation_id": "127",
      "title": "Mean opinion score (mos) revisited: methods and applications, limitations and alternatives",
      "authors": [
        "R Streijl",
        "S Winkler",
        "D Hands"
      ],
      "year": "2016",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "128",
      "title": "Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation",
      "authors": [
        "S Kiritchenko",
        "S Mohammad"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "129",
      "title": "Confidence interval estimation of interaction",
      "authors": [
        "D Hosmer",
        "S Lemeshow"
      ],
      "year": "1992",
      "venue": "Epidemiology"
    },
    {
      "citation_id": "130",
      "title": "What is a p-value",
      "authors": [
        "R Thisted"
      ],
      "year": "1998",
      "venue": "Departments of Statistics and Health Studies"
    },
    {
      "citation_id": "131",
      "title": "Personality, perceptual, and cognitive correlates of emotional sensitivity",
      "authors": [
        "J Davitz",
        "M Beldoch",
        "S Blau",
        "L Dimitrovsky",
        "E Levitt",
        "P Levy"
      ],
      "year": "1964",
      "venue": "The communication of emotional meaning"
    },
    {
      "citation_id": "132",
      "title": "Measuring emotion-related vocal acoustics",
      "authors": [
        "M Owren",
        "J.-A Bachorowski"
      ],
      "year": "2007",
      "venue": "Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "133",
      "title": "Recognition of emotion from vocal cues",
      "authors": [
        "W Johnson",
        "R Emde",
        "K Scherer",
        "M Klinnert"
      ],
      "year": "1986",
      "venue": "Archives of General Psychiatry"
    },
    {
      "citation_id": "134",
      "title": "Harvest: A high-performance fundamental frequency estimator from speech signals",
      "authors": [
        "M Morise"
      ],
      "year": "2017",
      "venue": "Harvest: A high-performance fundamental frequency estimator from speech signals"
    },
    {
      "citation_id": "135",
      "title": "Information retrieval for music and motion",
      "authors": [
        "M Üller"
      ],
      "year": "2007",
      "venue": "Information retrieval for music and motion"
    },
    {
      "citation_id": "136",
      "title": "Vocal communication of emotion: A review of research paradigms",
      "authors": [
        "K Scherer"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "137",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech communication"
    },
    {
      "citation_id": "138",
      "title": "Emotion in speech: The acoustic attributes of fear, anger, sadness, and joy",
      "authors": [
        "C Sobin",
        "M Alpert"
      ],
      "year": "1999",
      "venue": "Journal of psycholinguistic research"
    },
    {
      "citation_id": "139",
      "title": "He is currently a PhD student at the National University of Singapore. His research interests mainly focus on emotion analysis and synthesis in speech, including emotional voice conversion and emotional text-tospeech. He served as local arrangment co-chair of IEEE ASRU",
      "authors": [
        "Kun Zhou",
        "(student Member"
      ],
      "year": "2019",
      "venue": "2018, and the M. Sc. degree from the"
    }
  ]
}