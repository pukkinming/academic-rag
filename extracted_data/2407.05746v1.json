{
  "paper_id": "2407.05746v1",
  "title": "Msp-Podcast Ser Challenge 2024: L'Antenne Du Ventoux Multimodal Self-Supervised Learning For Speech Emotion Recognition",
  "published": "2024-07-08T08:52:06Z",
  "authors": [
    "Jarod Duret",
    "Mickael Rouvier",
    "Yannick Estève"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this work, we detail our submission to the 2024 edition of the MSP-Podcast Speech Emotion Recognition (SER) Challenge. This challenge is divided into two distinct tasks: Categorical Emotion Recognition and Emotional Attribute Prediction. We concentrated our efforts on Task 1, which involves the categorical classification of eight emotional states using data from the MSP-Podcast dataset. Our approach employs an ensemble of models, each trained independently and then fused at the score level using a Support Vector Machine (SVM) classifier. The models were trained using various strategies, including Self-Supervised Learning (SSL) fine-tuning across different modalities: speech alone, text alone, and a combined speech and text approach. This joint training methodology aims to enhance the system's ability to accurately classify emotional states. This joint training methodology aims to enhance the system's ability to accurately classify emotional states. Thus, the system obtained F1-macro of 0.35% on development set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) represents a challenging area of research. The complexity arises from the nuanced, subjective nature of emotional expression in speech and the challenge of extracting effective feature representations. Despite these difficulties, understanding emotions is crucial for enhancing human-computer interaction, as emotions significantly influence reasoning, decision-making, and social interactions. In the realm of speech and text, emotions, while subjective, are essential to convey meaning and intent. In recent years, advances in deep learning have contributed to notable improvements in the performance of emotion recognition systems by leveraging highly effective features extracted from deep neural networks. In the pursuit of advancing SER capabilities, the MSP-Podcast SER Challenge 2024  [1]  provides a fertile ground for exploring novel methodologies and techniques for emotion recognition from naturalistic speech data. Our focus lies primarily on Task 1: Categorical Emotion Recognition, which involves classifying speech segments into eight specified emotional states: Anger (A), Happiness (H), Sadness (S), Fear (F), Surprise (U), Contempt (C), Disgust (D), and Neutral (N).\n\nThis paper details our submission to the 2024 edition of the MSP-Podcast SER Challenge. Our approach employs an ensemble of models, each trained independently and then fused at the score level using a Support Vector Machine (SVM) classifier. The models were trained using various strategies, including Self-Supervised Learning (SSL) fine-tuning across different modalities: speech alone, text alone, and a combined speech and text approach. This joint training methodology aims to enhance the system's ability to accurately classify emotional states.\n\nThe paper is structured as follows: In Section 2, we introduce the datasets of MSP-Podcast and the task, while section 3 position our study in regards with some related works. Section 4 presents the general architecture of our system. The components of the sub-systems are described in Section 5. The subsystems themselves are presented in Section 6, as well as the fusion approach in Section 7. Finally, in Section 8, we present the results and discuss them.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Msp-Podcast Ser Challenge 2024",
      "text": "MSP-Podcast  [2]  is a large naturalistic speech emotion corpus featuring speech segments sourced from an audio-sharing website. The corpus is annotated for both categorical emotion recognition and emotional attribute prediction. The training set consists of 68,119 speaking turns and the development set contains 19,815 speaking segments. The test set comprises 2,347 unique segments from 187 speakers, with the labels not publicly disclosed. The selection of segments for the test set was carefully curated to ensure a balanced representation across primary categorical emotions.\n\nTwo tasks were proposed, we are only participating in the first task. The first task involves categorical classification within eight specified emotional states: Anger (A), Happiness (H), Sadness (S), Fear (F), Surprise (U), Contempt (C), Disgust (D), and Neutral (N). The test set for the challenge has a balanced distribution across the emotional categories. Performance evaluation and model ranking on the leaderboard are based on the Macro-F1 score. The Macro-F1 score is calculated by first computing the F1 score separately for each class, which is the harmonic mean of precision and recall for that class, and then taking the average of these F1 scores.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "SER involves a two-step process: feature extraction and emotion classification. Early SER research focused on handcrafted features like pitch, energy, and Mel-frequency cepstral coefficients (MFCCs) along with traditional machine learning methods, including Markov models, Gaussian mixture models, and support vector machines for classification  [3, 4] . More recently, neural-based models have started to replace traditional machine learning approaches  [3][4] . Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) have demonstrated improved performance in emotion recognition tasks  [5, 6] . Additionally, transfer learning, particularly arXiv:2407.05746v1 [cs.AI] 8 Jul 2024 self-supervised pretraining, has gained traction in SER. Models like Wav2Vec2 2.0  [7] , WavLM  [8]  and HuBERT  [9]  have achieved state-of-the-art results in this domain  [10, 11] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text Emotion Recognition",
      "text": "Text Emotion Detection and Recognition (TEDR) has significantly evolved over the past few years, transitioning from traditional machine learning approaches to deep learning models. Initial methods primarily utilized classifiers such as Support Vector Machine (SVM) and Maximum Entropy (MaxEnt) classifiers, later approaches increasingly relied on deep learning models in combination with different word embedding methods. For example, an emotion detection model that combines Long Short-Term Memory (LSTM) networks with Convolutional Neural Networks (CNN) was introduced  [12] . This model integrates various word embeddings, including GloVe  [13]  and FastText  [14] , to capture semantic nuances more effectively. More recently, approaches based on transformer pre-trained language models  [15]  have begun to emerge, offering remarkable breakthroughs in text emotion detection. In  [15] , the authors conducted a comprehensive comparison of models including BERT  [16] , RoBERTa  [17] , DistilBERT  [18] , and XL-Net  [19] , with RoBERTa emerging as the model achieving the best performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview Of The Approach",
      "text": "The system was developed as a two-level architecture. Given a speech segment, the first level extracts outputs from categorical emotion recognition based on various sub-systems. The outputs of theses sub-systems are fed to a SVM. Five different set of sub-systems are used.\n\nWe formulate the sub-system as a mapping from the continuous speech domain into the discrete domain of categorical labels of emotion. As depicted in Figure  1 , to achieve this, we first use an encoder (speech and/or text encoder). These encoders have been trained on unlabeled data and are capable of extracting highly robust feature representations. Following the encoder stage, we employ a pooling strategy to aggregate these features over time, ensuring a fixed-length representation regardless of the original speech duration. This representation then feeds into a classifier layer, which serves as the final component of our architecture. This layer is responsible for mapping the aggregated features to the desired categorical labels of emotion, thus completing the process of transforming continuous speech into discrete emotion predictions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sub-System Components",
      "text": "In this section, we describe the various components used in the sub-systems. In Section 5.1, we describe the various speech and text encoders used in our study. Following that, Section 5.2 is dedicated to detailing the pooling techniques employed. The discussion continues in Section 5.3, where we delve into the classifier used. Finally, Section 5.4 covers the aspects of speech audio data augmentation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Encoder",
      "text": "In the realm of speech and text processing, the choice of encoders is essential for provide powerful deep feature learning. Indeed, these encoders are trained on large unannotated datasets, enabling them to learn rich, complex patterns without the need for manually labeled data.\n\nFor speech encoder, we leverage the capabilities of WavLM 1  , a state-of-the-art Self-Supervised Learning (SSL) speech model designed to discover speech representation that encode pseudo-phonetic or phonotactic information. WavLM is distinguished by its robustness in handling a wide range of audio scenarios, including noisy environments. Additionally, we also used HuBERT 2  , another leading speech encoder model. For both model, features are extracted, at the acoustic framelevel i.e. for short speech segments of 20 ms duration.\n\nFor text encoder, we employ RoBERTa 3  , which is builds on the BERT architecture but optimizes its pre-training process for more efficient learning and better performance across a variety of Natural Language Processing (NLP) tasks. Features are extracted for every words.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pooling",
      "text": "Given that speech segments vary in length, we use pooling to aggregate the features given by the encoder across time, ensuring a fixed-length representation regardless of the original speech duration. Two different types of pooling have been used:\n\n• mean-pooling achieves this by averaging the feature values given by the encoder over the time dimension, which effectively summarizes the overall characteristics of the speech signal into a single unified representation.\n\n• attention-pooling unlike mean-pooling leverages a weighted average, where the weights are learned through the model  [20] . This allows the model to focus on more relevant parts of the speech signal, potentially capturing nuanced dynamics that mean pooling might overlook.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Classifier",
      "text": "For the classifier component, we assume that the SSL encoders have successfully captured all necessary information for predicting the targeted categorical emotion label. We choose a simple yet efficient approach by integrating a linear layer to function as the classifier. This linear classifier takes the pooled feature vectors and assigns them to the a given emotion label.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augmentation",
      "text": "The MSP-Podcast corpus includes, for each segment, the emotion label, text transcription, speaker ID, and gender. However, the test set is provided as speech only, without any annotations.\n\nTo utilize a text encoder, we needed to automatically generate transcriptions for the test set. For this purpose, we employed the Whisper  [21]  speech recognition model. Additionally, to ensure consistency and avoid any discrepancies between provided transcriptions and Whisper-generated transcriptions, we computed transcriptions for the entire dataset.\n\nWe observed a significant discrepancies in the distribution of emotion classes within both the Training and Development sets. Additionally, a considerable number of samples are labeled as \"X,\" indicating that no consensus was reached for these samples. To minimize the mismatch, we decided to automatically recompute the consensus for samples labeled as \"X\". For each sample, we have access to the labels provided by each annotator and the consensus. The consensus is determined by identifying the most frequently associated label. In scenarios where no single label predominates due to an even distribution of votes among the labels, the label \"X\" is assigned to indicate the lack of a clear consensus.\n\nOur method for recomputing the consensus is detailed as follows: First, we calculate a score for each evaluator by determining the ratio of their annotations that match the consensus to their total number of annotations. Then, we discard all annotations from evaluators whose score falls below a predefined threshold K. Finally, we recompute the consensus for the whole dataset. In scenarios where there is a tie between a specific label and the Neutral (\"N\") label, we opt to drop the label \"N\". Samples with newly attributed labels from the Training and Development sets were added to the training set.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Sub-Systems",
      "text": "In this section, we provide a detailed description of the five distinct sub-systems employed in the fusion process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sub-System A : Wavlm",
      "text": "Thi system is based on a WavLM, a mean pooling strategy and the outoput is a softmax loss function. In order to optimize this architecture, we employ the Adam optimizer. We fine-tune the pre-trained WavLM model during the training phase. This fine-tuning allows the WavLM model to adjust its parameters specifically towards recognizing emotions in speech, leveraging the rich, pre-learned representations and tailoring them to our domain of interest. We set the mini-batch size to 16 and 10 steps. And no data-augmentation is done on speech segment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sub-System B : Jeffreys Loss",
      "text": "This system is identical to System A; however, we propose in this system to replace the softmax loss function by Jeffreys loss function. The Jeffreys Loss is given in the Equation  1 . Incorporating this divergence into the cross-entropy loss function enables the maximization of the target value of the output distribution while smoothing the non-target values.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sub-System C : Joint Wav2Vec2-Wavlm",
      "text": "This system is based on a joint SSL audio strategy as depicted in Figure  2 , wherein the upstream model is performed by jointly training a WavLM and Hubert SSL model. The input speech is fed into both SSL models. Fine-tuning of the upstream model is achieved throught simultaneous training alongside a straightforward network that implements mean pooling, leading to a Linear Classifier, as illustrated in Figure  2 .\n\nFigure  2 : Illustration of the dual speech encoder emotion recognition system.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sub-System D : Wavlm And Roberta",
      "text": "The following describes the SER model depicted in Figure  3 . The WavLM encoder is divided into two parts: the CNN feature extractor and the trasnformer-based encoder. We chose to freeze the CNN feature extractor part, fixing all the parameters of these CNN layers and only fine-tune the parameters of transformer blocks. This method of partial fine-tuning acts as a strategy for domain adaptation. It is designed to maintain the integral feature extraction capabilities of the lower layers, thereby enabling the model to adjust to new tasks efficiently without compromising its overall performance. For the text encoder, we opt to fine-tune all parameters of the RoBERTa model. During the fine-tuning process, we apply three different schedulers to adjust the fine-tuning learning rates of the WavLM and RoBERTa encoders, as well as the learning rate of the classifier model. Each scheduler utilizes the Adam Optimizer in conjunction with the NewBob technique, which anneals the learning rates based on the performance during the validation stage. The fine-tuning learning rates for both the WavLM and RoBERTa encoders are set to 10 -5 , while the learning rate for the classifier model is set to 10 -4 . This model is trained using negative log-likelihood loss (NLL).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sub-System E : Data Augmentation",
      "text": "The system E is identical to the system D described in section 6.4. However, this system incorporates the data augmentation technique outlined in Section 5.4, setting K = 50%. This adjustment aims to mitigate the notable imbalances observed in the distribution of emotion classes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fusion Systems",
      "text": "The fusion process integrates the outputs from all five subsystems (A, B, C, D, E) by concatenating them to form a single feature vector. This concatenated vector serves as the input to an SVM classifier, which is trained to predict the emotion class. This fusion approach leverages the diverse strengths of each sub-system, aiming to enhance the overall performance and robustness of emotion classification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we describe our experimental setup, analyze results from individual sub-systems and their combined fusion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "To conduct our experiments, we employed the SpeechBrain toolkit  [22] , which is built on PyTorch and is specifically designed for speech-processing tasks. Additionally, we utilized the Hugging Face versions of the WavLM, HuBERT, and RoBERTa models. The source code is available on GitHub 4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Impact Of The Speech Encoder:",
      "text": "A first experiments aimed at identifying the most effective Speech encoder. Various speech encoders, including Wav2Vec, Hubert and WavLM were evaluated. These experiments were carried out on sub-system D. Table  3 , give the results of the experiments, from which we observe that the best results are obtained with the WavLM.\n\nImpact of the Text encoder: An additional experiment was conducted using text as input. This experiment, as shown in Table  3 , reveals that the performance of the RoBERTa text encoder, achieving a Macro-F1 score of 0.27, is not too far from the best speech encoder, which has a Macro-F1 score of 0.32. This result encourages the exploration of a joint speech and text emotion recognition system.   1  shows the results achieved by the different sub-systems. We observe that all the sub-systems achievd F1-Macro scores between 0.32 and 0.34. The best subsystem is sub-system E. And we observe that fusion system led to an improvement, achieving an F1-Macro score of 0.35%.\n\nConfusion matrix: Figure  4  shows the confusion matrix obtained from fusion system. Regarding the diagonal elements, we can observed that the fusion model is particularly effective at correctly predicting Neutral (N) and Happiness (H) classes. However, the fusion system struggles more with accurately predicting the Disgust (D) and Fear (F) classes. As for the offdiagonal elements, we can noted that there is a tendency for all classes to be misclassified as Neutral (N) and Disgust (D) misclassified as Contempt (C).\n\nFigure  4 : The confusion matrix provided by the fusion system.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "This paper describes LIA's participation in the MSP-Podcast SER Challenge. Our approach involves developing subsystems, each of which functions as an emotion classifier. These sub-systems model speech segments using different components to provide varied perspectives. For the final fusion step, we concatenate the outputs of the sub-systems and train a SVM for fusion. The fusion system achieved an F1-Macro score of 0.35% on the development set.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , to achieve this,",
      "page": 2
    },
    {
      "caption": "Figure 1: Illustration of our speech emotion recognition system.",
      "page": 2
    },
    {
      "caption": "Figure 2: , wherein the upstream model is performed by jointly",
      "page": 3
    },
    {
      "caption": "Figure 2: Figure 2: Illustration of the dual speech encoder emotion recog-",
      "page": 3
    },
    {
      "caption": "Figure 3: The WavLM encoder is divided into two parts: the CNN feature",
      "page": 3
    },
    {
      "caption": "Figure 3: Illustration of our joint speech and text emotion recog-",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the confusion matrix ob-",
      "page": 5
    },
    {
      "caption": "Figure 4: The confusion matrix provided by the fusion system.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Avignon Universite, France": "first.last@univ-avignon.fr"
        },
        {
          "Avignon Universite, France": "Abstract"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "In this work, we detail our submission to the 2024 edition of the"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "MSP-Podcast Speech Emotion Recognition (SER) Challenge."
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "This challenge is divided into two distinct\ntasks: Categorical"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "Emotion Recognition and Emotional Attribute Prediction. We"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "concentrated our efforts on Task 1, which involves the categor-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "ical classification of eight emotional states using data from the"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "MSP-Podcast dataset. Our approach employs an ensemble of"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "models, each trained independently and then fused at the score"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "level using a Support Vector Machine (SVM) classifier.\nThe"
        },
        {
          "Avignon Universite, France": "models were trained using various\nstrategies,\nincluding Self-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "Supervised Learning (SSL) fine-tuning across different modal-"
        },
        {
          "Avignon Universite, France": "ities: speech alone,\ntext alone, and a combined speech and text"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "approach. This joint training methodology aims to enhance the"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "system’s ability to accurately classify emotional\nstates.\nThis"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "joint training methodology aims to enhance the system’s ability"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "to accurately classify emotional states.\nThus,\nthe system ob-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "tained F1-macro of 0.35% on development set."
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "1.\nIntroduction"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "Speech Emotion Recognition (SER)\nrepresents a challenging"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "area of research. The complexity arises from the nuanced, sub-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "jective nature of emotional expression in speech and the chal-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "lenge of extracting effective feature representations.\nDespite"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "these difficulties, understanding emotions is crucial for enhanc-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "ing human-computer\ninteraction, as emotions significantly in-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "fluence reasoning, decision-making, and social\ninteractions.\nIn"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "the realm of speech and text, emotions, while subjective, are es-"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "sential to convey meaning and intent. In recent years, advances"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "in deep learning have contributed to notable improvements in"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "the performance of emotion recognition systems by leveraging"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "highly effective features extracted from deep neural networks."
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "In the pursuit of advancing SER capabilities,\nthe MSP-Podcast"
        },
        {
          "Avignon Universite, France": "SER Challenge 2024[1] provides a fertile ground for explor-"
        },
        {
          "Avignon Universite, France": "ing novel methodologies and techniques for emotion recogni-"
        },
        {
          "Avignon Universite, France": "tion from naturalistic speech data. Our focus lies primarily on"
        },
        {
          "Avignon Universite, France": ""
        },
        {
          "Avignon Universite, France": "Task 1: Categorical Emotion Recognition, which involves clas-"
        },
        {
          "Avignon Universite, France": "sifying speech segments into eight specified emotional states:"
        },
        {
          "Avignon Universite, France": "Anger (A), Happiness (H), Sadness (S), Fear (F), Surprise (U),"
        },
        {
          "Avignon Universite, France": "Contempt (C), Disgust (D), and Neutral (N)."
        },
        {
          "Avignon Universite, France": "This paper details our submission to the 2024 edition of the"
        },
        {
          "Avignon Universite, France": "MSP-Podcast SER Challenge. Our approach employs an en-"
        },
        {
          "Avignon Universite, France": "semble of models, each trained independently and then fused at"
        },
        {
          "Avignon Universite, France": "the score level using a Support Vector Machine (SVM) classi-"
        },
        {
          "Avignon Universite, France": "fier. The models were trained using various strategies,\ninclud-"
        },
        {
          "Avignon Universite, France": "ing Self-Supervised Learning (SSL) fine-tuning across different"
        },
        {
          "Avignon Universite, France": "modalities:\nspeech alone,\ntext alone, and a combined speech"
        },
        {
          "Avignon Universite, France": "and text approach.\nThis\njoint\ntraining methodology aims\nto"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "suring a fixed-length representation regardless of\nthe original"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "speech duration. Two different types of pooling have been used:"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "• mean-pooling achieves this by averaging the feature val-"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "ues given by the encoder over the time dimension, which"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "effectively summarizes the overall characteristics of the"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "speech signal into a single unified representation."
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "attention-pooling\n•\nunlike mean-pooling\nleverages\na"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "weighted average, where the weights are learned through"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "the model [20]. This allows the model to focus on more"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "relevant parts of the speech signal, potentially capturing"
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": "nuanced dynamics that mean pooling might overlook."
        },
        {
          "aggregate the features given by the encoder across\ntime, en-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "self-supervised pretraining, has gained traction in SER. Models": "like Wav2Vec2 2.0 [7], WavLM [8]\nand HuBERT [9] have"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "achieved state-of-the-art results in this domain [10, 11]."
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "3.2. Text Emotion Recognition"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "Text Emotion Detection and Recognition (TEDR) has signifi-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "cantly evolved over the past few years,\ntransitioning from tra-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "ditional machine learning approaches\nto deep learning mod-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "els.\nInitial methods primarily utilized classifiers such as Sup-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "port Vector Machine (SVM) and Maximum Entropy (MaxEnt)"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "classifiers, later approaches increasingly relied on deep learning"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "models in combination with different word embedding meth-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "ods.\nFor example, an emotion detection model\nthat combines"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "Long Short-Term Memory (LSTM) networks with Convolu-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "tional Neural Networks (CNN) was introduced [12]. This model"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "integrates various word embeddings,\nincluding GloVe [13] and"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "FastText\n[14],\nto capture semantic nuances more effectively."
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "More\nrecently,\napproaches based on transformer pre-trained"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "language models [15] have begun to emerge, offering remark-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "able breakthroughs in text emotion detection.\nIn [15],\nthe au-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "thors\nconducted a\ncomprehensive\ncomparison of models\nin-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "cluding BERT [16], RoBERTa [17], DistilBERT [18], and XL-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "Net [19], with RoBERTa emerging as the model achieving the"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "best performance."
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "4. Overview of the approach"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "The system was developed as a two-level architecture. Given a"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "speech segment, the first level extracts outputs from categorical"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "emotion recognition based on various sub-systems. The outputs"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "of\ntheses sub-systems are fed to a SVM. Five different set of"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "sub-systems are used."
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "We formulate the sub-system as a mapping from the con-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "tinuous speech domain into the discrete domain of categorical"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "labels of emotion.\nAs depicted in Figure 1,\nto achieve this,"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "we first use an encoder\n(speech and/or\ntext encoder).\nThese"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "encoders have been trained on unlabeled data and are capable"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "of extracting highly robust\nfeature representations.\nFollowing"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "the encoder stage, we employ a pooling strategy to aggregate"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "these features over time, ensuring a fixed-length representation"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "regardless of the original speech duration. This representation"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "then feeds into a classifier layer, which serves as the final com-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "ponent of our architecture. This layer is responsible for map-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "ping the aggregated features to the desired categorical labels of"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "emotion,\nthus completing the process of transforming continu-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "ous speech into discrete emotion predictions."
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "5.\nSub-System components"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "In this section, we describe the various components used in the"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "sub-systems. In Section 5.1, we describe the various speech and"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "text encoders used in our study. Following that, Section 5.2 is"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "dedicated to detailing the pooling techniques employed.\nThe"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "discussion continues in Section 5.3, where we delve into the"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "classifier used. Finally, Section 5.4 covers the aspects of speech"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "audio data augmentation."
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "5.1. Encoder"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": ""
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "In the realm of speech and text processing,\nthe choice of en-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "coders\nis\nessential\nfor provide powerful deep feature\nlearn-"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "ing.\nIndeed,\nthese encoders are trained on large unannotated"
        },
        {
          "self-supervised pretraining, has gained traction in SER. Models": "datasets, enabling them to learn rich, complex patterns without"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5.4. Data Augmentation": "The MSP-Podcast corpus includes, for each segment,\nthe emo-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "This system is based on a joint SSL audio strategy as depicted"
        },
        {
          "5.4. Data Augmentation": "tion label, text transcription, speaker ID, and gender. However,",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "in Figure 2, wherein the upstream model is performed by jointly"
        },
        {
          "5.4. Data Augmentation": "the test set is provided as speech only, without any annotations.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "training a WavLM and Hubert SSL model. The input speech is"
        },
        {
          "5.4. Data Augmentation": "To utilize a text encoder, we needed to automatically generate",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "fed into both SSL models. Fine-tuning of the upstream model"
        },
        {
          "5.4. Data Augmentation": "transcriptions for\nthe test set.\nFor\nthis purpose, we employed",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "is achieved throught simultaneous training alongside a straight-"
        },
        {
          "5.4. Data Augmentation": "the Whisper\n[21]\nspeech\nrecognition model.\nAdditionally,",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "forward network that\nimplements mean pooling,\nleading to a"
        },
        {
          "5.4. Data Augmentation": "to ensure\nconsistency and avoid any discrepancies between",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "Linear Classifier, as illustrated in Figure 2."
        },
        {
          "5.4. Data Augmentation": "provided transcriptions and Whisper-generated transcriptions,",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "we computed transcriptions for the entire dataset.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "We observed a significant discrepancies in the distribution of",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "emotion classes within both the Training and Development sets.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "Additionally, a considerable number of samples are labeled as",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "”X,” indicating that no consensus was reached for\nthese sam-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "ples. To minimize the mismatch, we decided to automatically",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "recompute the consensus for samples labeled as ”X”. For each",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "sample, we have access to the labels provided by each annotator",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "and the consensus.\nThe consensus is determined by identify-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "ing the most frequently associated label.\nIn scenarios where no",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "single label predominates due to an even distribution of votes",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "among the labels,\nthe label ”X” is assigned to indicate the lack",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "of a clear consensus.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "Our method for\nrecomputing the consensus is detailed as fol-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "lows: First, we calculate a score for each evaluator by deter-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "mining the ratio of their annotations that match the consensus",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "to their total number of annotations. Then, we discard all an-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "notations from evaluators whose score falls below a predefined",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "Figure 2: Illustration of the dual speech encoder emotion recog-"
        },
        {
          "5.4. Data Augmentation": "threshold K. Finally, we recompute the consensus for the whole",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "nition system."
        },
        {
          "5.4. Data Augmentation": "dataset.\nIn scenarios where there is a tie between a specific la-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "bel and the Neutral (”N”) label, we opt\nto drop the label ”N”.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "Samples with newly attributed labels from the Training and De-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "velopment sets were added to the training set.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "6.4.\nSub-System D : WavLM and RoBERTa"
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "The following describes the SER model depicted in Figure 3."
        },
        {
          "5.4. Data Augmentation": "6.\nSub-Systems",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "The WavLM encoder is divided into two parts:\nthe CNN feature"
        },
        {
          "5.4. Data Augmentation": "In this section, we provide a detailed description of the five dis-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "extractor and the trasnformer-based encoder. We chose to freeze"
        },
        {
          "5.4. Data Augmentation": "tinct sub-systems employed in the fusion process.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "the CNN feature extractor part, fixing all the parameters of these"
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "CNN layers and only fine-tune the parameters of\ntransformer"
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "blocks. This method of partial fine-tuning acts as a strategy for"
        },
        {
          "5.4. Data Augmentation": "6.1.\nSub-System A : WavLM",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "domain adaptation.\nIt\nis designed to maintain the integral fea-"
        },
        {
          "5.4. Data Augmentation": "Thi system is based on a WavLM, a mean pooling strategy and",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "ture extraction capabilities of the lower layers, thereby enabling"
        },
        {
          "5.4. Data Augmentation": "the outoput\nis a softmax loss\nfunction.\nIn order\nto optimize",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "the model\nto adjust\nto new tasks efficiently without compro-"
        },
        {
          "5.4. Data Augmentation": "this architecture, we employ the Adam optimizer. We fine-tune",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "mising its overall performance.\nFor\nthe text encoder, we opt"
        },
        {
          "5.4. Data Augmentation": "the pre-trained WavLM model during the training phase. This",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "to fine-tune all parameters of the RoBERTa model. During the"
        },
        {
          "5.4. Data Augmentation": "fine-tuning allows the WavLM model\nto adjust\nits parameters",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "fine-tuning process, we apply three different schedulers to ad-"
        },
        {
          "5.4. Data Augmentation": "specifically towards recognizing emotions in speech, leveraging",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "just the fine-tuning learning rates of the WavLM and RoBERTa"
        },
        {
          "5.4. Data Augmentation": "the rich, pre-learned representations and tailoring them to our",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "encoders, as well as the learning rate of\nthe classifier model."
        },
        {
          "5.4. Data Augmentation": "domain of\ninterest. We set\nthe mini-batch size to 16 and 10",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "Each scheduler utilizes the Adam Optimizer in conjunction with"
        },
        {
          "5.4. Data Augmentation": "steps. And no data-augmentation is done on speech segment.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "the NewBob technique, which anneals the learning rates based"
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "on the performance during the validation stage. The fine-tuning"
        },
        {
          "5.4. Data Augmentation": "6.2.\nSub-System B : Jeffreys Loss",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "learning rates for both the WavLM and RoBERTa encoders are"
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "set\nto 10−5, while the learning rate for\nthe classifier model\nis"
        },
        {
          "5.4. Data Augmentation": "This system is identical\nto System A; however, we propose in",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "set to 10−4. This model is trained using negative log-likelihood"
        },
        {
          "5.4. Data Augmentation": "this system to replace the softmax loss function by Jeffreys loss",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "loss (NLL)."
        },
        {
          "5.4. Data Augmentation": "function. The Jeffreys Loss is given in the Equation 1.\nIncor-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "porating this divergence into the cross-entropy loss function en-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "6.5.\nSub-System E : Data Augmentation"
        },
        {
          "5.4. Data Augmentation": "ables the maximization of the target value of the output distri-",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "bution while smoothing the non-target values.",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": ""
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "The system E is\nidentical\nto the system D described in sec-"
        },
        {
          "5.4. Data Augmentation": "",
          "6.3.\nSub-System C : Joint Wav2vec2-WavLM": "tion 6.4. However, this system incorporates the data augmenta-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Performance comparison of individual sub-systems and their fusion across all emotion classes, with overall accuracy and",
      "data": [
        {
          "Table 1: Performance comparison of": "F1-Macro scores.",
          "individual sub-systems and their": "",
          "fusion across all emotion classes, with overall accuracy and": ""
        },
        {
          "Table 1: Performance comparison of": "",
          "individual sub-systems and their": "Sub-System B",
          "fusion across all emotion classes, with overall accuracy and": "Sub-System D"
        },
        {
          "Table 1: Performance comparison of": "Anger (A)",
          "individual sub-systems and their": "0.552",
          "fusion across all emotion classes, with overall accuracy and": "0.487"
        },
        {
          "Table 1: Performance comparison of": "Contempt (C)",
          "individual sub-systems and their": "0.104",
          "fusion across all emotion classes, with overall accuracy and": "0.198"
        },
        {
          "Table 1: Performance comparison of": "Disgust (D)",
          "individual sub-systems and their": "0.166",
          "fusion across all emotion classes, with overall accuracy and": "0.191"
        },
        {
          "Table 1: Performance comparison of": "Fear (F)",
          "individual sub-systems and their": "0.026",
          "fusion across all emotion classes, with overall accuracy and": "0.064"
        },
        {
          "Table 1: Performance comparison of": "Happiness (H)",
          "individual sub-systems and their": "0.577",
          "fusion across all emotion classes, with overall accuracy and": "0.591"
        },
        {
          "Table 1: Performance comparison of": "Neutral (N)",
          "individual sub-systems and their": "0.604",
          "fusion across all emotion classes, with overall accuracy and": "0.536"
        },
        {
          "Table 1: Performance comparison of": "Sadness (S)",
          "individual sub-systems and their": "0.360",
          "fusion across all emotion classes, with overall accuracy and": "0.381"
        },
        {
          "Table 1: Performance comparison of": "Surprise (U)",
          "individual sub-systems and their": "0.186",
          "fusion across all emotion classes, with overall accuracy and": "0.211"
        },
        {
          "Table 1: Performance comparison of": "Accuracy",
          "individual sub-systems and their": "0.508",
          "fusion across all emotion classes, with overall accuracy and": "0.466"
        },
        {
          "Table 1: Performance comparison of": "F1-Macro",
          "individual sub-systems and their": "0.322",
          "fusion across all emotion classes, with overall accuracy and": "0.332"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Performance comparison of individual sub-systems and their fusion across all emotion classes, with overall accuracy and",
      "data": [
        {
          "Table 2: Distribution of emotion classes before and after data": "augmentation."
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "Emotion Class"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "N"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "H"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "S"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "A"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "U"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "C"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "D"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "F"
        },
        {
          "Table 2: Distribution of emotion classes before and after data": "Total"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Performance comparison of individual sub-systems and their fusion across all emotion classes, with overall accuracy and",
      "data": [
        {
          "Total\n53386\n60624": "8.2. Results"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "Impact of the Speech encoder: A first experiments aimed at"
        },
        {
          "Total\n53386\n60624": "identifying the most effective Speech encoder. Various speech"
        },
        {
          "Total\n53386\n60624": "encoders,\nincluding Wav2Vec, Hubert and WavLM were eval-"
        },
        {
          "Total\n53386\n60624": "uated.\nThese experiments were carried out on sub-system D."
        },
        {
          "Total\n53386\n60624": "Table 3, give the results of the experiments, from which we ob-"
        },
        {
          "Total\n53386\n60624": "serve that the best results are obtained with the WavLM."
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "Impact of\nthe Text encoder: An additional experiment was"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "conducted using text as input.\nThis experiment, as shown in"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "Table 3, reveals that\nthe performance of the RoBERTa text en-"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "coder, achieving a Macro-F1 score of 0.27,\nis not\ntoo far from"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "the best speech encoder, which has a Macro-F1 score of 0.32."
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "This result encourages the exploration of a joint speech and text"
        },
        {
          "Total\n53386\n60624": "emotion recognition system."
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "Table 3:\nPerformance Comparison of Speech and Text En-"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "coders.\nThe table shows Micro-F1 and Macro-F1 scores\nfor"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "speech encoders (Wav2Vec2, WavLM, HuBERT) and a text en-"
        },
        {
          "Total\n53386\n60624": "coder (RoBERTa)."
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "Model\nMicro-F1 ↑\nMacro-F1 ↑"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "Wav2Vec2 Large\n0.45\n0.29"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "WavLM Large\n0.51\n0.32"
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": ""
        },
        {
          "Total\n53386\n60624": "HuBERT Large\n0.50\n0.31"
        },
        {
          "Total\n53386\n60624": "RoBERTa Base\n0.44\n0.27"
        },
        {
          "Total\n53386\n60624": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Performance comparison of individual sub-systems and their fusion across all emotion classes, with overall accuracy and",
      "data": [
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "sults from individual sub-systems and their combined fusion."
        },
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "8.1. Experimental setup"
        },
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "To conduct our\nexperiments, we\nemployed the SpeechBrain"
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "toolkit\n[22], which\nis\nbuilt\non PyTorch\nand\nis\nspecifically"
        },
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "designed for\nspeech-processing tasks.\nAdditionally, we uti-"
        },
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "lized the Hugging Face versions of the WavLM, HuBERT, and"
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "RoBERTa models. The source code is available on GitHub4."
        },
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": ""
        },
        {
          "In this section, we describe our experimental setup, analyze re-": "4github.com/Chaanks/MSP-Podcast-SER-Challenge-2024"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "the different sub-systems. We observe that all\nthe sub-systems",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[1] L. Goncalves, A. N. Salman, A. Reddy Naini, L. Moro-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "achievd F1-Macro scores between 0.32 and 0.34. The best sub-",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "Velazquez, T. Thebaud, L.P. Garcia, N. Dehak, B. Sisman,"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "system is sub-system E. And we observe that fusion system led",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "and C. Busso,\n“Odyssey2024 - speech emotion recogni-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "to an improvement, achieving an F1-Macro score of 0.35%.",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "tion challenge: Dataset, baseline framework, and results,”"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "in Odyssey 2024: The Speaker and Language Recognition"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "Confusion matrix: Figure 4 shows the confusion matrix ob-",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "Workshop), Quebec, Canada, June 2024, vol. To appear."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "tained from fusion system. Regarding the diagonal elements,",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "we can observed that\nthe fusion model\nis particularly effective",
          "11. References": "[2] R. Lotfian and C. Busso,\n“Building naturalistic\nemo-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "at correctly predicting Neutral (N) and Happiness (H) classes.",
          "11. References": "tionally balanced speech corpus by retrieving emotional"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "However, the fusion system struggles more with accurately pre-",
          "11. References": "speech from existing podcast recordings,” IEEE Transac-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "dicting the Disgust\n(D) and Fear\n(F) classes. As for\nthe off-",
          "11. References": "tions on Affective Computing, vol. 10, no. 4, pp. 471–483,"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "diagonal elements, we can noted that there is a tendency for all",
          "11. References": "October-December 2019."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "classes to be misclassified as Neutral (N) and Disgust (D) mis-",
          "11. References": "[3] B. Schuller, S. Steidl,\nand A. Batliner,\n“The INTER-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "classified as Contempt (C).",
          "11. References": "SPEECH 2009 emotion challenge,”\nin Interspeech 2009."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "2009, ISCA."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[4] B. Schuller, S. Steidl, and A. et al. Batliner, “The INTER-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "SPEECH 2013 computational paralinguistics challenge:"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "social signals, conflict, emotion, autism,”\nin Interspeech"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "2013. 2013, ISCA."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[5] L. Yuanchao, Z. Tianyu,\nand K. Tatsuya,\n“Improved"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "end-to-end speech emotion recognition using self atten-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "tion mechanism and multitask learning,”\nin Interspeech"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "2019. 2019, ISCA."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[6] C. Etienne, G. Fidanza, A. Petrovskii, L. Devillers, and"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "B. Schmauch, “CNN+LSTM architecture for speech emo-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "tion recognition with data augmentation,” in Workshop on"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "Speech, Music and Mind (SMM 2018). 2018, ISCA."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[7] A. Baevski,\nY\n.\nZhou,\nA. Mohamed,\nand M. Auli,"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "“wav2vec 2.0: A framework for self-supervised learning"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "informa-\nof speech representations,” Advances in neural"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "tion processing systems, vol. 33, pp. 12449–12460, 2020."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "Figure 4: The confusion matrix provided by the fusion system.",
          "11. References": "[8]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu,"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "Yoshioka, Xiong Xiao, et al.,\n“Wavlm: Large-scale self-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "supervised pre-training for full stack speech processing,”"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "9. Conclusion",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "IEEE Journal of Selected Topics in Signal Processing, vol."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "This paper describes LIA’s participation in the MSP-Podcast",
          "11. References": "16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "SER Challenge.\nOur\napproach\ninvolves\ndeveloping\nsub-",
          "11. References": "[9] K. Lakhotia, E. Kharitonov, W. Hsu, Y. Adi, A. Polyak,"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "systems, each of which functions as an emotion classifier. These",
          "11. References": "B. Bolte, T. Nguyen, J. Copet, A. Baevski, and A. Mo-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "sub-systems model\nspeech segments using different\ncompo-",
          "11. References": "hamed,\n“On generative spoken language modeling from"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "nents to provide varied perspectives. For the final fusion step,",
          "11. References": "raw audio,” Transactions of the Association for Computa-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "we concatenate the outputs of the sub-systems and train a SVM",
          "11. References": "tional Linguistics, vol. 9, pp. 1336–1354, 2021."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "for fusion. The fusion system achieved an F1-Macro score of",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[10] M. Macary, M. Tahon, Y. Est`eve, and A. Rousseau,\n“On"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "0.35% on the development set.",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "the use of self-supervised pre-trained acoustic and linguis-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "tic features for continuous speech emotion recognition,”"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "10. Acknowledgements",
          "11. References": "in 2021 IEEE Spoken Language Technology Workshop"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "(SLT). IEEE, 2021, pp. 373–380."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "This work was granted access to the HPC resources of IDRIS",
          "11. References": ""
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "under the allocation AD011013257R2 made by GENCI.",
          "11. References": "[11] Y. Wang, A. Boumadane, and A. Heba,\n“A fine-tuned"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "wav2vec 2.0/hubert benchmark for speech emotion recog-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "nition,\nspeaker verification and spoken language under-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "standing,” arXiv preprint arXiv:2111.02735, 2021."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[12] Marco\nPolignano,\nPierpaolo Basile, Marco\nde Gem-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "mis, and Giovanni Semeraro,\n“A comparison of word-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "embeddings in emotion detection from text using bilstm,"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "cnn and self-attention,” in Adjunct publication of the 27th"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "conference on user modeling, adaptation and personaliza-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "tion, 2019, pp. 63–68."
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "[13]\nJeffrey Pennington, Richard Socher, and Christopher D"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "Manning,\n“Glove: Global vectors for word representa-"
        },
        {
          "Impact of the Fusion: Table 1 shows the results achieved by": "",
          "11. References": "tion,” in Proceedings of the 2014 conference on empirical"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "methods in natural language processing (EMNLP), 2014,": "pp. 1532–1543."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Joulin,\nEdouard\nGrave,\nPiotr\nBojanowski,"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Matthijs Douze, H´erve J´egou, and Tomas Mikolov, “Fast-"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "arXiv\ntext. zip: Compressing text classification models,”"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "preprint arXiv:1612.03651, 2016."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": ""
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "and Wenyu Chen, “Comparative analyses of bert, roberta,"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "distilbert, and xlnet\nfor\ntext-based emotion recognition,”"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "2020\n17th\nInternational Computer Conference\non\nin"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Wavelet Active Media Technology and Information Pro-"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "cessing (ICCWAMTIP). IEEE, 2020, pp. 117–121."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Toutanova,\n“Bert:\nPre-training\nof\ndeep\nbidirectional"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "transformers for language understanding,” arXiv preprint"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "arXiv:1810.04805, 2018."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": ""
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Zettlemoyer, and Veselin Stoyanov, “Roberta: A robustly"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "arXiv\npreprint\noptimized\nbert\npretraining\napproach,”"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "arXiv:1907.11692, 2019."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Lysandre Debut,\nJulien Chaumond,\nand"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Thomas Wolf,\n“Distilbert,\na distilled version of bert:"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "arXiv\npreprint\nsmaller,\nfaster,\ncheaper\nand\nlighter,”"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "arXiv:1910.01108, 2019."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": ""
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Russ R Salakhutdinov, and Quoc V Le, “Xlnet: General-"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "ized autoregressive pretraining for\nlanguage understand-"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "ing,” Advances in neural information processing systems,"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "vol. 32, 2019."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Pooyan Safari, Miquel India, and Javier Hernando, “Self-"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "attention encoding and pooling for speaker\nrecognition,”"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "2020."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": ""
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Christine McLeavey, and Ilya Sutskever,\n“Robust speech"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "recognition via large-scale weak supervision,” 2022."
        },
        {
          "methods in natural language processing (EMNLP), 2014,": ""
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Rouhe,\nSamuele Cornell,\nLoren Lugosch, Cem Sub-"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "akan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Zhong,\nJu-Chieh Chou,\nSung-Lin Yeh,\nSzu-Wei\nFu,"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "Chien-Feng Liao, Elena Rastorgueva, Franc¸ois Grondin,"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "William Aris, Hwidong Na, Yan Gao, Renato De Mori,"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "and Yoshua Bengio,\n“SpeechBrain: A general-purpose"
        },
        {
          "methods in natural language processing (EMNLP), 2014,": "speech toolkit,” 2021, arXiv:2106.04624."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Odyssey2024 -speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Reddy Naini",
        "L Moro-Velazquez",
        "T Thebaud",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Odyssey 2024: The Speaker and Language Recognition Workshop)"
    },
    {
      "citation_id": "2",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "The INTER-SPEECH 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "The INTER-SPEECH 2009 emotion challenge"
    },
    {
      "citation_id": "4",
      "title": "The INTER-SPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl"
      ],
      "venue": "The INTER-SPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism"
    },
    {
      "citation_id": "5",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "L Yuanchao",
        "Z Tianyu",
        "K Tatsuya"
      ],
      "venue": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "6",
      "title": "CNN+LSTM architecture for speech emotion recognition with data augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "7",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "On generative spoken language modeling from raw audio",
      "authors": [
        "K Lakhotia",
        "E Kharitonov",
        "W Hsu",
        "Y Adi",
        "A Polyak",
        "B Bolte",
        "T Nguyen",
        "J Copet",
        "A Baevski",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "On the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Estève",
        "A Rousseau"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "12",
      "title": "A comparison of wordembeddings in emotion detection from text using bilstm, cnn and self-attention",
      "authors": [
        "Marco Polignano",
        "Pierpaolo Basile",
        "Marco De Gemmis",
        "Giovanni Semeraro"
      ],
      "year": "2019",
      "venue": "Adjunct publication of the 27th conference on user modeling, adaptation and personalization"
    },
    {
      "citation_id": "13",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "14",
      "title": "Fasttext. zip: Compressing text classification models",
      "authors": [
        "Armand Joulin",
        "Edouard Grave",
        "Piotr Bojanowski",
        "Matthijs Douze",
        "Hérve Jégou",
        "Tomas Mikolov"
      ],
      "year": "2016",
      "venue": "Fasttext. zip: Compressing text classification models",
      "arxiv": "arXiv:1612.03651"
    },
    {
      "citation_id": "15",
      "title": "Comparative analyses of bert, roberta, distilbert, and xlnet for text-based emotion recognition",
      "authors": [
        "Francisca Acheampong",
        "Nunoo-Mensah Adoma",
        "Wenyu Henry",
        "Chen"
      ],
      "year": "2020",
      "venue": "2020 17th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)"
    },
    {
      "citation_id": "16",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "17",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "18",
      "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "authors": [
        "Victor Sanh",
        "Lysandre Debut",
        "Julien Chaumond",
        "Thomas Wolf"
      ],
      "year": "2019",
      "venue": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "19",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Selfattention encoding and pooling for speaker recognition",
      "authors": [
        "Pooyan Safari",
        "Miquel India",
        "Javier Hernando"
      ],
      "year": "2020",
      "venue": "Selfattention encoding and pooling for speaker recognition"
    },
    {
      "citation_id": "21",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "22",
      "title": "SpeechBrain: A general-purpose speech toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong",
        "Ju-Chieh Chou",
        "Sung-Lin Yeh",
        "Szu-Wei Fu",
        "Chien-Feng Liao",
        "Elena Rastorgueva",
        "William Franc ¸ois Grondin",
        "Hwidong Aris",
        "Yan Na",
        "Renato Gao",
        "Yoshua Mori",
        "Bengio"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A general-purpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    }
  ]
}