{
  "paper_id": "2403.10279v1",
  "title": "Emotion-Aware Multimodal Fusion For Meme Emotion Detection",
  "published": "2024-03-15T13:20:38Z",
  "authors": [
    "Shivam Sharma",
    "Ramaneswaran S",
    "Md. Shad Akhtar",
    "Tanmoy Chakraborty"
  ],
  "keywords": [
    "Memes",
    "multimodality",
    "emotion analysis",
    "social media",
    "information fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The ever-evolving social media discourse has witnessed an overwhelming use of memes to express opinions or dissent. Besides being misused for spreading malcontent, they are mined by corporations and political parties to glean the public's opinion. Therefore, memes predominantly offer affect-enriched insights towards ascertaining the societal psyche. However, the current approaches are yet to model the affective dimensions expressed in memes effectively. They rely extensively on large multimodal datasets for pre-training and do not generalize well due to constrained visual-linguistic grounding. In this paper, we introduce MOOD (Meme emOtiOns Dataset), which embodies six basic emotions. We then present ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection), a novel multimodal neural framework that (i) explicitly models emotion-enriched visual cues, and (ii) employs an efficient cross-modal fusion via a gating mechanism. Our investigation establishes ALFRED's superiority over existing baselines by 4.94% F1. Additionally, ALFRED competes strongly with previous best approaches on the challenging Memotion task. We then discuss ALFRED's domain-agnostic generalizability by demonstrating its dominance on two recently-released datasets -HarMeme and Dank Memes, over other baselines. Further, we analyze ALFRED's interpretability using attention maps. Finally, we highlight the inherent challenges posed by the complex interplay of disparate modality-specific cues toward meme analysis.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "M EMES on social media represent a new digital artifact genre that has become ubiquitous with time. Internet memes contain a short piece of text embedded over an image, often expressing information sarcastically or humorously, but sometimes in illicit ways. Internet memes also offer rich potential to understand or sway the sentiment and opinion of communities on social media, essentially facilitating a systematic study of their affective characteristics.\n\nThe increase in the amount of multimodal content being disseminated over the web has spurred innovation in allied areas involving multimodality in general; however, fewer efforts have been made toward analyzing memes, especially to the extent its inherent complexity solicits. Memes, having multimodal constructs, require a joint interpretation of both the embedded text and the visuals to assimilate the intended meaning comprehensively. Although existing approaches perform much better on conventional multimodal tasks, particularly the ones involving visual-linguistic grounding  [1] , they are yet to deliver for other scenarios like memes  [2] ,  [3] ,  [4] ,  [5] ,  [6] .\n\nOne of the critical challenges within the studies related to social media content analysis is the subjective perception problem  [7] , which leads to ambiguous data labeling. Consequently, several recent participatory efforts involving the detection of emotion from memes consider the categories that capture higher levels of affective abstraction like humor, sarcasm, offense, and motivation  [2] ,  [8] . Although these efforts effectively capture affective phenomena from memes that pertain to macro societal aspects like political, socio-cultural, demographics, etc., they constrain finegrained analysis of meme emotions. Efforts are needed towards studies that target more fine-grained analysis of user behavior, decisions, and perceptions  [9] , encompassing a broader spectrum of emotions that help address the multimodal affective characterization at the fundamental level. Additionally, the inherent multimodality involving image-text configurations presents an additional challenge in detecting emotions due to the complexity posed by the implicit background context abstracted by memetic visuals. For instance, memes shown in Figs.  1(a ) and 1(b) depict faces with expressions conveying anger and sadness, respectively; whereas the corresponding embedded text suggests something different, thereby vesting the required onus of meme emotion detection, primarily upon the visual cues.\n\nAlso, examples shown in Figs.  1(c ) and 1(d) carry the same background image, but their corresponding embedded texts convey contrasting emotions -anger and joy, respectively, indicating triviality of the visual cues present. These aspects pose challenges like visual-linguistic dissociation and cross-modal noise, and induce reasoning complexity by having dependencies on implicit contextual cues, effectively inhibiting the overall progress towards addressing such non-trivial tasks.\n\nIn this work, we propose a novel task of classifying the emotions expressed within memes amongst six basic Ekman  [10]  emotions. To this end, we also introduce a manually curated largescale multimodal dataset, MOOD (Meme emOtiOns Dataset). MOOD constitutes real multimodal memes (images with overlaid text) expressing various emotions that are discretely mapped to the six fundamental Ekman  [10]  emotions -fear, anger, joy, sadness, surprise, and disgust via manual annotation. We benchmark this dataset against several unimodal and multimodal systems emulating competitive baselines for meme emotion detection and report class-wise, along with macro-averaged performances. Further, we investigate the design of an effective approach to detect emotions from memes and propose ALFRED, a multimodal approach that employs systematic modality-specific interactions via gating. ALFRED constitutes (a) gated multimodal fusion (GMF) towards explicitly incorporating emotion-enriched visual features, followed by (b) gated cross-attention (GCA) to fuse emotionenriched image and text representations, conditioned upon the visual cues learned. We observe significant gains by ALFRED over other strong baselines on the MOOD dataset, along with competitive performance on Memotion tasks  [2] , followed by distinct indications of ALFRED's strong generalizability over other related tasks such as HarMeme  [11]  and Dank Memes  [12] .\n\nWe also examine the interpretability of the predictions by analyzing visual attention maps corresponding to the encoding mechanisms that ALFRED employs and highlight both affective affinity and limitations exhibited therein. Besides discussing and analyzing the overall and emotion-specific performances of various systems and ALFRED in detail, we delineate the contributory aspects of MOOD and ALFRED. Finally, we perform an extensive error analysis elucidating some imminent challenges and limitations like modality-specific obscurity and thematic overlaps rendered by the complex memetic dynamics.\n\nThrough this work, we intend to address the imperative necessity of characterizing multimodal content like memes and their fundamental affective spectrum by emphasizing their esoteric visual semiotics. In particular, we make the following contributions:\n\n• We introduce MOOD (Meme emOtiOns Dataset) that captures six basic Ekman's emotions for memes. • We propose ALFRED (emotion-Aware muLtimodal Fusion foR Emotion Detection): a multimodal neural framework that uses affect-enriched features from memes and fuses them via a gated cross-attention mechanism. • We benchmark the dataset via several unimodal and multimodal baselines and discuss their limitations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Several studies contributed to the understanding of detecting emotions from multimodal content, encompassing modalities like images and text, along with signals like audio, video, EEG, eye movement, etc. There have also been numerous recent efforts toward analyzing memes and detecting various allied harmful aspects. Since there have been limited exploratory studies on detecting emotions from memes, the field can benefit significantly from the findings of the aforementioned applications. We systematically review these areas to set the necessary background.\n\nMultimodal emotion detection: Emotion detection is a wellstudied area explored for various modalities such as text, speech, and audio  [13] ,  [14] . Significant emphasis has been laid on multimodal emotion detection as well. Unlike traditional emotion detection tasks involving single modalities, multimodal approaches require a mechanism to effectively learn features from multiple correlated modalities. An initial effort in this domain  [15]  proposed multi-kernel learning based deep-CNN towards emotion and sentiment recognition on different multimodal datasets. This was followed by proffering a pooling-based fusion mechanism in  [16]  along with introducing a multimodal social media dataset (and metadata) from Reddit towards the domain of emotion classification. Efforts have been made toward multimodal feature characterization  [17] , wherein authors introduced a Tumblr-based multimodal dataset and demonstrated the efficacy of multimodal approaches when contrasted with their unimodal counterparts. Another notable finding  [18]  explored an approach advocating coordinated representation learning for multimodal emotion recognition. The authors used a recurrent neural network to emulate correlated attention and calculate the correlation between EEG and eye movement signals.\n\nRecent efforts include a Transformer-based inter-modality attention mechanism  [19]  with self-supervision  [20] , while materializing the design for fusing features from different modalities for multimodal emotion recognition. While there has been significant progress from a computational standpoint toward emotion recognition, Mittal et al.  [21]  attempted to investigate an approach based on Frege's Context principle, which provides different interpretations of context for emotion recognition. The authors studied different interpretations using modality-specific features, semantic content, and depth-map to develop their algorithm. Although these efforts pave the way for addressing critical challenges prevalent within the affect-oriented applications for multimodal content, there is still scope for further exploring multimodal content representing dynamic cross-modal semiotics, like memes.\n\nMeme analysis: A significant influx of memes from online fringe communities, such as Gab, Reddit, and 4chan, to mainstream platforms, such as Twitter and Instagram, resulted in a massive epidemic of intended harm  [22] . This has imminently solicited addressing the prevailing challenges from the computational social science point of view. Towards this end, several datasets capturing offensiveness  [23] , hatefulness  [24] ,  [25] , and harmfulness  [26]  in memes have been curated. Besides the tasks corresponding to these resources, there are a variety of other tasks, such as detecting sexism  [27] , racism  [28] , and harmful propaganda  [29]  from memes, that have been explored from the perspective of critical discourse analysis.\n\nParticipatory events like the Facebook Hateful Meme Challenge  [24]  and shared-task on detecting hero, villain and victim from memes  [30]  have laid a strong foundation for communitylevel initiatives for detecting hate speech  [31] ,  [32] ,  [33] ,  [34] ,  [35]  and connotative role-labels in memes. As part of these challenges, several interesting approaches besides ensembling large language models, utilizing meta information, attentive interactions, and adaptive loss are attempted in the multimodal setting  [36] ,  [37] ,  [38] ,  [39] . Other notable insights from meme analyses suggest the utility of commonsense knowledge  [40] , web entities, racial aspects  [26] ,  [41] , and other external cues for detecting offense, harm, and hate speech in memes.\n\nMost of these efforts either address the detection tasks at various levels for harmfulness (see a recent survey  [42] ) or design ensemble techniques lacking cost-optimality. However, they tend to ignore the primary spectrum of emotions that plays a crucial role in cascading any adverse effect over social media. The current study aims to address this fundamental aspect of detecting the basic emotions of memes.\n\nEmotion detection from memes: Although several studies have analyzed emotions of social media content, fewer efforts have been made toward characterizing the emotions of Internet memes. French  [43]  studied the correlation between the semantics of a meme and the textual discussions in the thread related to a multimodal post. Their study indicates the effectiveness of memes as a sentiment predictor over social media. Memotion  [2] ,  [8] , a series of participatory shared tasks on meme emotion classification, initiated the task of detecting meme emotions at different levels of granularity. Their dataset was curated towards three sub-tasks, emulating various combinations of multi-class/label formulations. The three sub-tasks were -(a) sentiment classification: multiclass classification amongst positive and negative categories; (b) emotion classification: multi-class classification amongst categories sarcastic, humorous, offensive and motivational; and (c) quantification: multi-class/label classification amongst emotion intensities, represented by slightly, mildly and very and across emotion categories. Participants of this task explored several unimodal and multimodal approaches. For unimodal feature extraction, participants used a variety of models such as BERT  [44] , GloVe  [45]  for text modality and pre-trained image models such as EfficientNet  [46]  and ResNet  [47]  for image modality. Singh et al.  [48]  and Vlad et al.  [49]  used multi-task learning to jointly predict emotion and sentiment. While these solutions, as discussed in the Introduction section (c.f. Sec. 1), address the detection of affect categories at a higher level of abstraction, they do not consider the multimodal emotion characterization from a fundamental perspective. This effectively renders the investigation of basic emotions from memes obscure.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Meme Emotions Dataset (Mood)",
      "text": "Memotion dataset  [2] ,  [8]  includes affective categories like motivation, offense, sarcasm, and humor, which represent high-level emotion abstraction within memes. Although these categories are critical for studying the imminent implications of memetic discourse over social media, they are insufficient for characterizing their impact on an individual's psyche. Therefore, as part of this work, we aim to set up a framework for addressing emotion recognition from memes w.r.t. the basic emotions. Ekman and Cordaro  [10]  empirically suggested that human beings exhibit six basic emotions, namely fear (FER), anger (AGR), joy (JOY), sadness (SDN), surprise (SPR), and disgust (DGT). Besides constituting the primary spectrum for studying the human affective response, basic emotions decide the overall affective tone of memes towards analyzing their disseminative outcomes. To this end, we manually curate MOOD, a multimodal dataset for detecting basic emotions from memes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Collection And De-Duplication",
      "text": "The memes were collected primarily from two sources -Google image search 1 and imgflip 2 . We used keywords like 'happy memes', 'depression memes', 'sad cat memes', etc., to crawl a diverse set of memes, capturing the six basic Ekman emotions. Many duplicates in the collected set were removed using an offthe-shelf API called imagededup 3 , followed by manually filtering out the low-quality memes. We used a set of filtering criteria for memes to ensure that the memes collected were of high quality. A meme is discarded if any of the following criteria are met: (1) The resolution of the meme is bad, such that the meme image is unclear or the readability of the meme text is affected; (2) If the meme did not exhibit any of the 6 Ekman emotions; (3) If the meme had content that induces hate towards or is harmful to individuals or communities; (4) Contain any personal information of a user;\n\n(5) Contains text that is not in English or is code-switched.This resulted in a total of 10, 004 memes (c.f. Table  1 ), that constituted our primary proposed meme emotion dataset MOOD. The dataset constitutes generic memes corresponding to the six basic Ekman emotions, on general topics like birthdays, relationships, family, friends (to name a few), with an appropriate mix of human subjects, pop-culture references, animated characters, and animals. 4 . There is a variation in the relative proportions of the memes collected for different emotion categories. Many memes were collected from Google image search results for categories anger, joy, sadness, and disgust. Interestingly, almost half of the fear and surprise memes we collected are obtained from imgflip. This suggests the categorical diversity that the platform can provide. Also, very few memes for categories disgust and sadness could be sourced from imgflip, suggesting the positive sentiment-based genre that dominates the platform. This distribution also reflects the realistic availability of such multimodal data over different platforms.\n\nMOOD consists of a total of 2017, 2706, and 2636 memes from categories anger, joy and sadness, respectively, constituting the majority share within the dataset. These are followed by the 1208 from surprise, 871 from fear and the least from disgust, with 576 memes. The summary of MOOD can be observed from Table  1 .   Further, towards capturing the emotional signals expressed via visual modality, we leverage AffectNet  [14] , a large-scale human facial expression dataset that consists of around 400K manually annotated facial expression images capturing neutral and a total of 7 emotions, namely happy, sad, surprise, fear, disgust, anger and contempt. We randomly sample around 50K images corresponding to the Ekman emotion categories (excluding neutral and contempt from AffectNet) for our framework. AffectNet contains facial expressions only for humans, rendering the animated and graphical emotion depiction and comprehension obscure, as required for our scenario. In order to robustly identify the emotions in realistic meme images within MOOD, especially the ones containing cartoons, animated figures, and animals, we needed to emphasize our multimodal framework's capability to handle them. To this end, we collected a dataset of 1, 447 images (c.f. Table  1 ) with animated characters, cartoons, and animals and manually labeled them with Ekman emotion classes. We queried the web for using a simple formatted query string as 'reactionary templates for' + <emotion category> + <animal/cartoon>, and followed similar filtering/annotation process as for MOOD. These images represent various standard reactionary templates disseminated over social media. Fig.  4  depicts some examples of the images added to extend the AffectNet dataset, especially towards capturing the nonhuman subjects within the memes with query prefix as 'reactionary template for', and suffixes as: fearful Spongebob, joyous cat and sad puppy, depicted in Figs. 4(a), 4(b) and 4(c), respectively. We call this extended set Ext. AffectNet.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Text Length Analysis",
      "text": "The meme text length analysis indicates the complexity that could be posed within a corpus. The more diverse the text-length  distribution is for each category, the more difficult it could be to model the sequence and underlying association. It can be visualized from Fig.  3  that the distributions for anger, sadness, and joy are relatively more close to being normal, also suggesting that there is a consistent pattern for the creation of content for memes from these categories. In contrast, disgust, along with surprise and fear (with slight variability), have relatively more variation regarding the text lengths used. This suggests the challenge it poses to the language models and the diversity with which such content is created online.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotation",
      "text": "Two annotators annotated the dataset, while a consolidator oversaw the entire annotation exercise. One of the annotators is male, while the other female, and their ages range from 24-35 years. Moreover, both of them were professional lexicographers and social media savvy. On the other hand, the consolidator was an expert working in the fields of Computational Social Sciences and NLP. Before starting the annotation process, they were briefed on the task using detailed guidelines. They were requested to assess memes' textual and visual content towards the final adjudication of the meme emotion. In particular, they were asked to identify the emotion that the meme's author is trying to express via a multi-class labeling setup. A tabulated list of prescribed guidelines adopted towards the annotation is shown in Table  2 . We conducted  the annotation process in two stages -a dry run and a final annotation stage. The Cohen's Kappa  [50]  was computed to assess the inter-annotator agreement prior to the commencement of the final annotating process and was found to be nearly perfect with a score of 0.86.\n\nBased on the annotation, MOOD dataset can be exemplified via Fig.  2 , which depicts example memes for each of the six Ekman emotions from the MOOD dataset. Although most memes in MOOD are designed by their authors to disseminate some form of humor via sarcasm, satire, or benign limericks, their primary objective is to resonate with the consumer's emotional appeal. Typically, this leads to the memes exhibiting a primary emotion by design. Samples depicted in Figs. 2(a)-2(f) are specially hand-picked to ensure a clear understanding of the annotation strategy. As can be observed from the samples, the primary emotions conveyed in the form of disgust, anger, surprise, fear, sadness and joy within the respective samples are expressed by both text and visual cues within the memes. The modality-specific expressivity varies extensively across MOOD, which constitutes the key multimodal challenge posed while performing analysis over realistic memes. The demonstration via Fig.  6  depicts the independent and joint influence of both image and text modalities via memes. Fig.  6(a",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Lexical Analysis Of Mood",
      "text": "The lexical summary exhibited by the textual cues present in the memes, in the form of overlaid text (c.f. Fig.  5 ), suggests interesting characteristics. All categories except surprise, prominently exhibit affect-enriched lexicon, including nouns, adjectives and verbs, exemplified as: disgust (gross, yuck, eww), anger (punks, hate, mad), fear (afraid, screaming, therapist), sadness (depressed, anxiety, lonely) and joy (friend, happy, love). Whereas, as elucidated from Fig.  5  (e), sur- prise-based emotion characterization relies significantly upon contextual cues, instead of lexical ones. Additionally, Table  7  (c.f. Appendix A.3) shows top 10 frequent words in the meme text for each category after masking the 'category-keywords'. The corresponding TF-IDF scores are given in the parenthesis. Some category-specific relevant words can be observed at the top of the category columns, except for anger. We do not see any particular word that is usually used in the contexts conveying anger. This suggests the complex constructs people typically use while creating memes when conveying anger. Essentially, the anger might not be conveyed using explicit keywords but by using implicit and complex references instead. This indicates the complexity the affective content from social media can pose to multimodal systems. Fig.  3  shows the length distribution of meme text for each class; we observe no significant differences between the classes.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Proposed Approach",
      "text": "As shown in Fig.  7 , our proposed approach utilizes the meme image and embedded text, extracted using Google GCV OCR 5  (GOCR) as primary inputs. For encoding image features, we use Vision Transformer (ViT)  [51]  and for meme text, we use BERT  [44] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Google Cloud Vision Ocr Api",
      "text": "We model emotion-aware image representations by explicitly incorporating visually depicted emotions. We extract emotion features from the images using a ViT-based encoder network, pre-trained using the extended AffectNet dataset. We then pass the emotion and image features through a gated multimodal unit to obtain an emotion-aware image representation, as shown in Fig.  7 . Further, we feed the emotion-aware image and textual representation through a gated cross-modal attention module that facilitates selective cross-modal attuning and blocking. Finally, we concatenate the two resultant updated representations and pass the joint representation of the meme through a feed-forward network toward the classification task. We describe each of these modules in detail below.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Unimodal Feature Extraction",
      "text": "We use pre-trained unimodal encoders to obtain representations of the image (i) and text (t) for a given meme (M ), as described below.\n\n-Image Encoder: We use ViT  [51]  initialised with ImageNet weights as the image encoder and obtain an m × 768 dimensional output corresponding to m image patches, i.e.,\n\n-Text Encoder: We use pre-trained BERT  [44]  as the text encoder. Specifically, we take the token-level representations corresponding to n tokens from the last hidden layer:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Feature Extraction",
      "text": "There are well-established variants of approaches, specially tailored towards capturing semantic objects and salient features, like RCNNs  [52] ,  [53] , YOLO  [54] , and CenterNet  [55] ,  [56] , to name a few; yet, there is a dearth of solutions addressing complex emotion features from visuals, especially memes. Towards explicitly incorporating visually-depicted emotion features as part of our proposed methodology, we build an emotion feature extraction model by fine-tuning a ViT-based image patch encoder for the emotion expression classification task. We then freeze its weights toward extracting relevant emotion-enriched cues from a given meme. To this end, we leverage the AffectNet dataset, a large-scale dataset of typical human facial expressions; but we also extend it by adding non-human subjects.\n\nAfter pretraining a ViT base model using over 50K emotionenriched images from the extended AffectNet dataset for emotion classification, we freeze its weights for extracting emotionenriched image features. These features are incorporated as part of ALFRED. This is expressed as f e = ViT(i) ∈ R m×768 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Gated Multimodal Fusion (Gmf)",
      "text": "As part of effectively incorporating visual cues from memes for meme emotion recognition, it is crucial to optimally infuse emotion-enriched input signals while emphasizing other relevant visual cues. This becomes critical while fusing features from similar source modalities.\n\nTo induce selective processing of input features, we adapt a gated multimodal unit  [57]  by using low-rank bilinear pooling (LRBP)  [58]  while computing a sigmoid-based gating weight, instead of a simple concatenation based approach. The motivation for this change is the requirement to fuse the representations coming from the same input source (i.e., image), towards which a Hadamard product-based interaction is empirically observed to be preferable over a concatenation-based approach. This module performs a fusion of the emotion features (f e ) that are extracted using the emotion encoder, and the meme image features (f i ) obtained using the image encoder to finally obtain emotion-aware image features (f ei ). We do this as we have two different types of image encodings, f e and f i . Such a fusion trades off on how much novel information is required from each encoding using a sigmoid-based gated fusion mechanism.\n\nwhere W i , W e , W g ∈ R 768×768 are the weights for transforming image features (f i ∈ R m×768 ), emotion features (f e ∈ R m×768 ), and low-rank bilinear pooling based fusion\n\nrespectively. The bias terms, b i and b e ∈ R 768 , correspond to the representation learning for image and its emotion-enriched signals, respectively. σ denotes the sigmoid activation function. Also, numpy-like broadcasting is inherently applied wherever applicable via PyTorch API 6 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Gated Cross Attention (Gca)",
      "text": "Prominent conventional approaches that leverage co-attentional transformers-based layers have been observed to perform well in scenarios involving visual-linguistic grounding  [59] ,  [60] . However, they exhibit sub-optimal results while modeling memes  [26] . This could be likely due to the cross-modal noise being captured and attended to while learning dissociated cues from modalityspecific meme components. Towards regulating the inherent effect of cross-modal noise, we modify the cross-attention mechanism  [61] , by incorporating the adaptive co-attention strategy  [62] . Instead of incorporating self-attention layers for cross-modal attention, we perform gating over one modality (visual) first, followed by weighting the other modality (textual). We then perform gated attention for the first modality (visual) using the weighted textual representation to obtain its feedback-based representation. We call this Gated Cross Attention mechanism. It facilitates the extraction of useful features from emotion-aware image (f ei ∈ R m×768 ) and textual (f t ∈ R n×768 ) features. Thus, we obtain new feature representations, fei ∈ R m×768 and ft ∈ R m×768 , as follows:\n\nwhere W ei , W t ∈ R 768×768 , W α ei , and W αt ∈ R 768×1 are the weights for transforming emotion-aware image feature (f ei ∈ R m×768 ) repeated n times to account for n textual tokens), updated text feature ( ft ∈ R m×768 ) repeated m times to account for m image patches), intermediate representation of transformed emotion-aware image feature (h f ei ∈ R m×n×768 ), and intermediate representation of transformed text feature (h ft ∈ R m×m×768 ), respectively. The bias terms, b ei , b t ∈ R 768 , and b α ei , b αt ∈ R 1 , correspond to the representation learning for h f ei , h ft , α ei , and α t , respectively.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Prediction And Training Objective",
      "text": "Finally, we apply sum-pooling across the first dimension of the corresponding weight-aggregated features: fei ∈ R m×768 and ft ∈ R m×768 , followed by concatenating the sum-pooled features ( fei ∈ R 768 and ft ∈ R 768 ), to produce a joint meme representation (f z1 ∈ R 1536 ). This is given as input to a feed-forward network for the final classification.\n\nwhere\n\n, θ represents model parameters, ŷ is the predicted class index, and Y is the label-id set, with |Y| = 6. The bias terms, b z1 ∈ R 768 and b z2 ∈ R 6 , are corresponding to the last two layers. We use the cross-entropy loss for optimization. Moreover, we employ the online label smoothing  [63]  for regularization.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Models",
      "text": "Unimodal Baselines: We restrict modality-specific encoders to the following choices, as the primary objective of this work is to investigate an optimal multimodal fusion strategy.\n\n• BERT  [44] : We use the BERT-base-uncased model as our textonly baseline.\n\n• ViT  [51] : We use ViT with ImageNet weights as our image-only baseline.\n\nMultimodal Baselines: For multimodal systems, we explore the following competent approaches as comparative baselines. These systems endorse various multimodal interaction schemes, facilitating a robust assessment.\n\n• Early-fusion: In this model, the features from ViT and BERT are concatenated and passed through a feed-forward network for classification.\n\n• MMBT  [64] : It is a supervised bi-modal transformer that projects image features from unimodally pre-trained image encoders to text tokens.\n\n• CLIP  [65] : CLIP is a contrastive learning-based approach that is designed to learn visual information through natural language supervision. • VisualBERT  [60] : VisualBERT is a transformer-based model for visuo-lingual modelling. It has been trained on the MS COCO dataset employing masked language modeling and the sentence-image prediction objective functions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "Firstly, we compare ALFRED with both unimodal (image/text) and multimodal models. The comparisons are first made for meme emotion detection task using the MOOD dataset, followed by evaluation on three Memotion tasks  [2]  (c.f. Section 6.3). We further perform the ablation of our model. This is followed by examining the interpretability of ALFRED using GradCAM  [66] . We then demonstrate the generalizability of ALFRED's performance on the HarMeme  [11]  and Dank Memes datasets  [12] . Finally, we analyze the errors observed during performance evaluation. Our experimental setup's empirical examinations involve fine-tuning for the respective tasks and datasets. Since we primarily explore a multi-class classification setup, we are more interested in evaluating the system performances that factors-in the class-wise contributions equally. Therefore, we use macroaveraged formulations of accuracy, precision, recall, and F1-score as evaluation metrics. We also report class-wise F1",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details And Hyperparameter Values",
      "text": "We train all the models using Pytorch 1.10 on an Nvidia Tesla V100 GPU with 32 GB dedicated memory, CUDA-11.2 and cuDNN-8.1.1 installed. For the primary emotion classification, Memotion, HarMeme tasks, we use BERT as the text encoder and ViT as the image encoder. Specifically, we use the bert-baseuncased checkpoint for BERT and google/vit-base-patch16-224 checkpoint for ViT. However, for the Dank Memes task, we switch BERT with UmBERTo, which is a BERT-based model but pre-trained using Italian corpus. The linear layers in GCA and GMF modules are initialized using Xavier initialization, and the bias is set to zero. For the meme emotion classification task, we train all the models using the online label smoothing loss and Adam optimizer. For the Memotion, HarMeme and Dank Memes tasks, we use cross-entropy loss and Adam optimizer. We also present these details in Table  4 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation On The Mood Dataset",
      "text": "Among unimodal models, the image-only model is observed to perform better (c.f. Table  3 ) than the text-only model by 4% F1 score. Also, multimodal baselines are observed to perform either\n\nat par or better than unimodal models. One of the top-performing baselines, as shown in Table  3 , is the early-fusion model with BERT and ViT as its text and image encoders, respectively. This could be due to an optimal modeling requirement posed by the meme emotion detection task, which does not seem to favor complex co-attentive visual-linguistic grounding employed by models like MMBT, CLIP, and VisualBERT.\n\nThe early-fusion model (0.7749) yields a 7% absolute improvement in F1-score over the sophisticated VisualBERT (0.7002). Overall, both perform better than the multimodal baselines like MMBT (0.6352) and CLIP (0.6816). In comparison, ALFRED registers 4.94% F1 improvement over the early-fusion model. This improvement could be mainly attributed to the GMFbased explicit emotion modeling and GCA-based inter-modal fusion, facilitating preferential treatment for both input modalities conditioned upon emotion-enriched visual cues. Overall, ALFRED yields an improvement of 1.93%-4.94% across all four metrics.\n\nALFRED significantly increases accuracy for four classesanger (↑2.41%), joy (↑5.19%), sadness (↑10.55%) and surprise (↑6.00%). In contrast, the accuracy for disgust improves slightly (0.62%), but not as much as for the classes above. This subtle enhancement observed could be likely due to the expressiveness of the emotion disgust via either text, image, or even both (See Fig.  2 (a) ). Whereas the lower representation of the disgust class in the dataset explains the improvement that is minor compared to that of the categories mentioned above. Moreover, the performance for fear drops by 2%, wherein the discriminatory cues are predominantly image-based, the implication of which is also corroborated by the highest category-specific performance (≈ 0.91 F1-score), by ViT-based model. Besides posing challenges like object occlusion, complex pose, image quality, etc., the visual modeling is impacted by the category's under-representation in both MOOD and extended AffectNet datasets, leading to ALFRED's drop in accuracy for fear as against the enhancement observed for other categories.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Evaluation On The Memotion Dataset",
      "text": "We then compare the performance of ALFRED, other baselines, and the state-of-the-art systems on the Memotion shared task  [2] . The Memotion dataset contains approximately 8K memes. It was proposed for the three subtasks 7 sentiment analysis (positive/negative), emotion classification (humour/sarcasm/offense/motivational), and emotion class quantification (slightly/mildly/very). The original average baseline F1 for 7. We use abbreviations SENT, EMOT and EMOT-Q for sentiment analysis, emotion classification, and emotion class quantification, respectively. the three Memotion sub-tasks -SENT (0.2176), EMOT (0.5002), and EMOT-Q (0.3009) -indicate inherent non-triviality of the tasks. The previous best systems involve a word2vec  [67] ,  [68]  based feed-forward neural network for SENT  [69] , a multimodal multi-tasking based setup for EMOT  [70] , and a feature-based ensembling approach for the EMOT-Q task  [71] . The performance of the state-of-the-art systems for the three tasks (c.f. Table  5 ) are 0.3547, 0.5183, and 0.3225 F1, respectively. In comparison, ALFRED induces an increase of 0.89% and 0.28% F1-score for EMOT and EMOT-Q tasks, respectively. For SENT, however, ALFRED lags slightly behind the previous best score by 0.61% F1 score.\n\nALFRED's low score on the SENT task could be due to noise induced by the emotion-enriched feature that might complicate modeling a more straightforward task like SENT compared to simpler early-fusion-based state-of-the-art. ALFRED also performs relatively better with 1.09% -1.13% F1-score increment in the humour and sarcasm categories for the EMOT task, as against that for EMOT-Q. Since the level of abstraction for the information being modeled for EMOT (emotion classification) is relatively higher as compared to that for EMOT-Q (emotion quantification), an explicit emotion modeling could help detect emotions for EMOT, and not necessarily for fine-grained emotion intensity quantification in EMOT-Q, especially for complex categories like humor and sarcasm. On average, ALFRED's performance on Memotion tasks are comparable -it reports better scores for the EMOT and EMOT-Q tasks; however, it yields inferior performance in the SENT task. The incorporation of emotion features induces 5% improvement over ALFRED without emotion features. Further, replacing the GMF module with simple concatenation for incorporating emotion features causes a 2% performance drop. On the other hand, GCA is also observed to be pivotal as its replacement with dense co-attention (DCA)  [72]  induces a drop of 2% performance. Effectively, the exclusion of GMF and GCA from ALFRED is empirically observed to induce a performance drop of ≈ 2%, as shown in Table  6 .",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies",
      "text": "We analyze the contribution of each module of ALFRED on MOOD: emotion encoder, gated multimodal fusion (GMF), and gated cross attention (GCA) in Table.  6 . ALFRED without EMO is expected to perform worse due to the complexity of the model not being complemented by the required rich features. This corroborates the requirement of a solution that explicitly incorporates emotion-enriched feature modeling. The pre-training of the emotion encoder is discussed in detail in Section 4.2. On the other hand, the early fusion model, being efficient yet straightforward towards multimodal classification, yields impressive results, which we also consider the best baseline for comparison. Also, besides discussing the effect of ALFRED without EMO (c.f. Table  6 ), we specifically include evaluations without GMF and GCA to examine the optimal modeling of emotion features using these modules in ALFRED. For both exclusions, the performance is low. This assessment consolidates the boosting capacity of each module constituting ALFRED.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "-Novelty Aspects of ALFRED: We empirically establish the efficacy of using low-rank bilinear pooling-based non-linear gating fusion instead of simple concatenation for fusing emotion-aware image representations in GMF. This helps characterize intra-modal fusion against inter-modal fusion, for which concatenation has been the conventional fusion strategy. For the GCA module, we first perform the conditioning based upon the emotion-aware image representation to compute the textual attention and feed it back towards computing a final emotion-aware image representation. This is in contrast to the convention of performing either a textbased conditioning  [62]  or a parallel co-attention based strategy  [59] . To our understanding, this is the first attempt to emphasize the visual cues toward overall modeling.\n\nEssentially, through our proposed approach incorporating the adaptation of existing effective techniques like GMF and GCA, we present a strategy that has been observed to be empirically adequate for modeling intra-modal and inter-modal fusion. To the best of our understanding, incorporating emotion-oriented features explicitly via visual modality toward a task like a meme analysis has not been explored prior to this work.\n\n-Dataset Utility: Meme datasets mostly encapsulate affective dimensions representing higher levels of abstraction ranging from categories like humor, sarcasm, offense, and motivation in the case of Memotion, to aspects like Harmfulness and Hatespeech within memes in HarMeme and Dank Memes datasets, respectively. Additionally, since memes in such datasets usually capture realworld events involving famous personalities and phenomena, they tend to be reasonably restricted in terms of the visual subjects they embody. For instance, a significant portion of such memes does not contribute visually towards affective adjudication, limiting the characterization due to cartoons, caricatures, and expressive personifications, for meme emotion detection. Most of them typically end up projecting textual cues as their characteristic feature.\n\nIn contrast, MOOD captures the affective dimension that objectively focuses on six basic Ekman emotions via multimodal cues for generic themes, capturing the expressivity differently from other datasets and soliciting an appropriate investigative framework. Such memetic configuration aptly represents the scope of this work -detecting basic emotions from generic memes, which tend to bear emotional expressivity via both image and text modalities.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Interpretability",
      "text": "We attempt to interpret the decisions made by ALFRED using GradCam  [66] . It uses gradients flowing through a model to produce a rough attention map. This highlights the regions in the image that the model pays attention to while making a decision.\n\nIn Fig.  9 (a), the text alone is not sufficient to detect the emotion of the meme, as the excerpt 'HAPPY NEW YEARS!...' could mislead the model's decision. The key aspect here lies with the angry ('grumpy cat meme') expression of the cat. In Fig.  9 (b), we notice that the model pays attention to the cat's face to correctly predict that the emotion is anger.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Error Analysis",
      "text": "-Visual Obscurity: On analyzing the incorrect predictions from the test set, we find that most misclassifications involve complex memetic text or obscure visuals, including prominent visual occlusion. Another distinct trend for poor results is observed for the samples belonging to the least represented emotion categories fear and disgust, with 2.5 % decrease and a marginal enhancement of 0.62 % respectively, in the accuracy values compared with those from an early-fusion based model. An example that demonstrates the misclassification attempt of ALFRED due to both the aforementioned likely reasons is shown in Fig.  9(c) , depicting the facial expressions of the man to be that of fear. Still, the model incorrectly predicts it as surprise. On interpreting the visual attention-map via GradCAM-based visualization, we observe that ALFRED does not pay attention to the subtle facial expressions indicating fear, as demonstrated by the misplaced visual attention, in Fig.  9(d) .\n\n-Textual Obscurity: Data sufficiency is also observed to play pivotal role towards class-wise performances observed in Table  3  w.r.t. textual influence within memes. A critical evidence corroborating this aspect is the lexical richness of memetic text for categories disgust and fear, as against lexical obscurity for surprise, as observed from Figs. 5(f), 5(a) and 5(e), respectively. The former two, being sparsely represented within MOOD, yield subpar performances for the corresponding categories, whereas the latter being densely represented, contributes a decent accuracy (c.f. Table  3 ), despite lexical obscurity. This suggests that multimodal (as against unimodal) contextual dependency is imperative towards emotion recognition from memes since it is not just the complex cross-modal interplay that encapsulates the intended message but the modality-specific intricacies that constitute complex memetic designs.\n\n-Analyzing Thematic Overlaps: We further investigate the semantic complexity posed by the themes that various memes are based on. To this end, thematic structure and characteristics are derived via a clustering-based approach and are ascertained for semantic overlap  [73]  w.r.t. the six Ekman emotions for MOOD. Firstly, document embeddings are obtained via all-MiniLM-L6-v2 based Transformer model  [74] , followed UMAP-based dimensionality reduction  [75]  and HDBSCANbased clustering  [76] .\n\nThe hierarchical thematic sub-groupings obtained through HDBSCAN, analyzed for semantic similarity using similarity matrices (shown in Fig.  8 ), reveal distinct overlaps and proximity regarding six of Ekman's emotions. These patterns are illustrated through highlighted examples. Notably, disgust memes (topic id: 68, 75) represented by patterns #1 and #2 in Fig.  8  show variable overlaps with surprise (topic id: 37) and joy (topic id: 29, 30). Fig.  8 : Thematic overlap analysis via similarity matrix, with five example cases of inter-emotion overlap highlighted. Each x/y-axes label represents topic id, followed by a set of three corresponding representative key-words (topicid_kw1_kw2_kw3).\n\nEmbedded text misleading (a), still visual attention of ALFRED correctly interprets anger by emphasizing over cat's expression (b).\n\nThe model gets confused for fear (c) and is not able to capture suitable expressions (d). The second similarity matrix in Fig.  8  highlights patterns connecting fear (topic id: 115) with anger (topic id: 14) and joy (topic id: 16) (pattern #3), and joy (topic id: 128) with sadness (topic id: 13) and anger (topic id: 14) (pattern #4). Additionally, anger (topic id: 19) overlaps with sadness (topic id: 127) and others, along with joy (topic id: 128, 129) in pattern #5.\n\nOverall, joy and sadness consistently emerge as common factors in emotion overlaps, aligning with Ekman's [10] and Plutchik's  [77]  theories. This suggests that the proximity of these emotions in memes stems from complex linguistic content. Determining the exact valence of this content remains challenging, underlining the need for detailed emotion analysis of memes.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Generalizability",
      "text": "Here, we establish the generalizability of ALFRED for HarMeme  [11]  and Dank Memes tasks  [12] . The dataset for HarMeme constitutes ≈ 7K memes (in English) on Covid-19 and US Politics. This dataset captures annotations for harmfulness and the targeted entity types. The second dataset, Dank Memes, comprises ≈ 1K hateful memes (in Italian). The memes are about the 2019 Italian Government Crisis. There was an associated shared task involving three subtasks -a) meme detection, b) hate-speech identification, and c) event clustering. In this work, we focus on hate-speech identification to ensure evaluation consistency.\n\n-HarMeme: The best performance on this dataset was reported by MOMENTA  [26]  which strongly outperformed sophisticated multimodal baselines such as V-BERT and ViLBERT. For twoclass classification, ALFRED is observed to achieve an improvement of 3.08% and 1.8% F1 over MOMENTA, respectively, on the Harm-C and Harm-P datasets. For three-class classification, ALFRED achieves 6.43% and 23.86% F1 increment over MO-MENTA on the Harm-C and Harm-P datasets, respectively (c.f. Fig.  10 ).\n\n-Dank Memes: Dank Memes is an Italian hateful politics meme dataset. The top two submissions for the related shared-task were both early-fusion based: Unitor employs domain-specific pretraining before finetuning on Dank Memes; UPB uses VGCN-BERT for text modality  [12] . Since the task deals with memes with embedded Italian content, we replace the BERT model with UmBERTo [78] within ALFRED while keeping other components same. ALFRED achieves an absolute increment of 2.02% and 4.37% in F1 and precision, respectively, over the best baseline, while the recall lags by 1.96% behind Unitor. These results not only highlight ALFRED's generalizability, but also indicate its language-agnostic cross-lingual affinity (c.f. Fig.  10 ), especially w.r.t multimodal tasks like meme analysis.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we first introduced MOOD, a new dataset for detecting emotions in Internet memes. We then proposed ALFRED, which uses emotion-aware meme representations to detect emotions from memes. Extensive experiments indicated that ALFRED outperforms strong multimodal baseline with 4.94% F1 increment and yields robust performance on the Memotion task  [2]  dataset. Further, we investigated the interpretability of the model by establishing the correspondences between the correct emotion class being predicted and the expressive emotions being attended to within the meme image. We also highlighted the inherent limitations that explicit emotion modeling can develop. Finally, we established the generalizability of ALFRED by demonstrating its superiority over previous best baselines on the HarMeme and Dank Memes datasets. As part of the future extension to this work, we would like to explore a multi-task learning setup involving the detection of correlated fine and coarse-grained emotion features for memes. Moreover, tasks like explanation generation for various memeemotions and network structure-based investigation of meme virality, w.r.t the emotions, are also promising avenues to explore.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Appendix A Additional Details Of Mood",
      "text": "This section provides additional details on collecting and curating our proposed dataset MOOD.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.1 Filtering Criteria",
      "text": "For downloading meme images, we used the Mozilla Firefox extension tool, called Download All Images 8 , with a few downloading specifications configured. These were file size (min): 4 KB, dimensions: 200X200, and format: JPGs and PNGs. We set these specifications after carefully observing the sample quality of memes available online and the requirements of the task at hand. Since despite pre-setting the required specifications, the download process ended up collecting images that were still unsuited towards manual annotations, the annotators were asked to further manually filter out images based on filtering criteria specified in Section 3.1 in the main text. These factors involved inadequate image resolution and text readability (perceptually ambiguity), absence of any of the six Ekman emotions, harmful memes containing personal information, and memes containing non-English textual content. Our primary heuristic for keeping a meme was the perceived intelligibility w.r.t. the textual and visual cues present in it. This ensured better interpretability of the model outputs as well. We did not consider any pre-defined (or otherwise) resolution threshold after collecting the raw meme images.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.2 Data Imbalance",
      "text": "Here, we want to highlight a popular effort towards investigating hateful memes via Hateful Memes Challenge  [24] . This, although involved the curation of a balanced combination of hateful and non-hateful memes focusing on modality-specific nuances, did involve the inclusion of benign confounders towards evaluating the robustness of multimodal systems, but were created synthetically by adopting confounding strategies, essentially not reflecting the realistic data distribution. This effect has been empirically observed to exacerbate when evaluated for the content over other social media platforms like 4chan (/pol/)  [22] ,  [79] . Keeping in mind the adverse implication of the non-realistic dataset, we instead emphasized collecting and curating a dataset that not only captures the fine-grained aspects of the primary task we intended to address but also reflects the realistic distribution, offering the scope for imminent developments and hence novelties in the areas like un/self-supervised and few-shot learning. This has already demonstrated capabilities for characterizing harmful content over social media platforms 9 .\n\n8. Firefox Browser ADD-ONS -Download All Images 9. Meta AI -ML Applications",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.3 Thematic Analysis",
      "text": "Towards performing thematic analysis for MOOD, we leverage a popular topic modelling technique, called BERTopic  [73]  that uses transformers and c-TF-IDF to create dense clusters. For the thematic analysis of visual objects, the overall pipeline first converts images into embeddings, followed by the performing dimensionality reduction, followed by HDBSCAN-based dense clustering. This is followed by captioning the images while weighting the cluster representative bag-of-words using c-TF-IDF and finding the best matching images based on most representative documents. Additionally, we also assess the tf-idf ranked set of words from each categorical distribution in MOOD, as shown in Table  7 . We look for the visual diversity captured within the MOOD dataset via manual and automated assessment. The manual review suggests a pre-dominant visual representation of human subjects, pop culture references, animated characters, and animals in the memes. In addition, the memes typically consist of various artistic modifications of these basic elements -visual morphing and juxta-    utility of leveraging more contextually enriched representations towards discriminating it against the rest. Moreover, the distinct clarity of ALFRED towards discriminating a class like surprise is also corroborated by an imposing 6% lead against our reference baseline (also having the second best category-specific) score. It is also worth noting that only minor confusions for neutral class, being predicted as any other emotion category, are observed. Additionally, the general trend of distinct reduction as shown in Fig.  12 (c), in differences between the true-positive rate (TPR) for Ekman emotions and false-negative rate (FNR) w.r.t the neutral class, when the emotion-encoder in ALFRED is finetuned (over the frozen variant), clearly prescribes the effect of adapting the emotion-encoder module, towards overall emotion classification. With the subtle exception of anger class (exhibiting the enhancement of TPR-FNR difference by one sample), all the other classes project reasonable reductions in the overall confusion between Ekman emotions and neutral category, quantified by the absolute differences of 6, 35, 25, 17, and 8 samples for classes: disgust, fear, joy, sadness, and surprise, respectively.\n\nThe amount of confusion visible between Ekman emotions and the neutral category suggests further scope of improvement in terms of out-of-distribution generalizability for ALFRED.",
      "page_start": 14,
      "page_end": 17
    },
    {
      "section_name": "Appendix C Text Extraction Via Ocr",
      "text": "Text extraction via optical character recognition (OCR) is of critical importance when mining embedded text from memes. The quality of the OCR process utilized influences the overall modeling capacity of systems. Towards exploring an optimal OCR technique for our purpose, we compare the text extractions for two popular OCR-based text extraction APIs: Google Tesseract API 10 (TOCR) and Google GCV API (GOCR). We first qualitatively analyze the extraction quality for 30 random memes and find occasional mistakes by TOCR, and rare by GOCR. For TOCR, mistakes committed were mostly for difficult cases, like textimage embedded at the same location, poor quality graphics, small text, etc. Sometimes even for simple cases, we observe GOCR's text quality much better than TOCR's output. A couple of examples shown in Fig.  15  demonstrate the difference in the text-extraction quality for TOCR and GOCR. The first example shown in Fig.  15  (left) is the case consisting of a mix of simple 10. Google's Tesseract-OCR API and complex regions like black text on white background and ambiguous visual-text overlap, respectively, that cannot be correctly mined by TOCR, while GOCR, is distinctly more accurate in its extraction. On the other hand, the second, relatively simpler meme in  Fig 15 (right)  poses more obscurity to TOCR, as compared to better visibility for GOCR.\n\nWe also examine ALFRED's overall performance in terms of the macro F1-score for our primary task of emotion classification for six Ekman emotions, w.r.t the two choices of OCR techniques explored. In drastic contrast to the impressive F1-score of 0.82 observed for the GOCR-based text extraction, we find an abysmal show of performance by TOCR, with an F1-score of 0.75, which speaks volumes of its inferiority when compared with Google GCV-based OCR API. In addition to leveraging GOCR for our primary experiments, we also conclude the critical influence that the correct OCR extraction technique has over the downstream task at hand.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of memes: (a,b) text modality is misleading; (c,d)",
      "page": 1
    },
    {
      "caption": "Figure 2: Examples depicting memes for six basic Ekman emotions from our dataset, MOOD.",
      "page": 4
    },
    {
      "caption": "Figure 3: Normalized histograms of meme text length per class.",
      "page": 4
    },
    {
      "caption": "Figure 4: depicts some examples of the images added to",
      "page": 4
    },
    {
      "caption": "Figure 4: Examples of the images added to extend AffectNet dataset,",
      "page": 4
    },
    {
      "caption": "Figure 3: that the distributions for anger, sadness, and",
      "page": 4
    },
    {
      "caption": "Figure 5: Word clouds depicting the category-wise lexicon comprising the embedded texts for memes in the MOOD dataset.",
      "page": 5
    },
    {
      "caption": "Figure 6: Example memes depicting modality-specific influence for",
      "page": 5
    },
    {
      "caption": "Figure 2: , which depicts example memes for each of the six Ekman",
      "page": 5
    },
    {
      "caption": "Figure 6: depicts the independent and joint",
      "page": 5
    },
    {
      "caption": "Figure 6: (b) shows anger",
      "page": 5
    },
    {
      "caption": "Figure 6: (c) expresses surprise",
      "page": 5
    },
    {
      "caption": "Figure 5: (e), sur-",
      "page": 5
    },
    {
      "caption": "Figure 7: ALFRED’s model architecture. GCA: Gated Cross Atten-",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the length",
      "page": 5
    },
    {
      "caption": "Figure 7: , our proposed approach utilizes the meme",
      "page": 5
    },
    {
      "caption": "Figure 7: Further, we feed the emotion-aware image and textual",
      "page": 6
    },
    {
      "caption": "Figure 2: (a)). Whereas the lower representation of the disgust class in the",
      "page": 8
    },
    {
      "caption": "Figure 9: (a), the text alone is not sufficient to detect the emotion",
      "page": 9
    },
    {
      "caption": "Figure 9: (c), depicting",
      "page": 9
    },
    {
      "caption": "Figure 8: ), reveal distinct overlaps and proximity",
      "page": 9
    },
    {
      "caption": "Figure 8: show variable",
      "page": 9
    },
    {
      "caption": "Figure 8: Thematic overlap analysis via similarity matrix, with five example cases of inter-emotion overlap highlighted. Each x/y-axes",
      "page": 10
    },
    {
      "caption": "Figure 9: Depiction of: Interpretability Analysis [subfigs. (a) and",
      "page": 10
    },
    {
      "caption": "Figure 8: highlights patterns",
      "page": 10
    },
    {
      "caption": "Figure 10: ), especially",
      "page": 10
    },
    {
      "caption": "Figure 10: Performance comparison for ALFRED and previous best",
      "page": 11
    },
    {
      "caption": "Figure 11: A collection of meme examples featuring human sub-",
      "page": 14
    },
    {
      "caption": "Figure 12: Analyzing ALFRED’s performance with neutral category. (a) and (b) Confusion Matrices and F1-score for ALFRED’s two",
      "page": 15
    },
    {
      "caption": "Figure 13: Top−48 prominent topics representing themes of the visually depicted content in MOOD’s memes.",
      "page": 15
    },
    {
      "caption": "Figure 11: for third",
      "page": 15
    },
    {
      "caption": "Figure 14: Top−48 prominent topics representing themes of the textually embedded content (OCR) in MOOD’s memes.",
      "page": 16
    },
    {
      "caption": "Figure 14: APPENDIX B",
      "page": 16
    },
    {
      "caption": "Figure 12: (c). We observe that all the Ekman",
      "page": 16
    },
    {
      "caption": "Figure 3: ) hints at the",
      "page": 16
    },
    {
      "caption": "Figure 15: Comparison b/w the quality of the OCR-extracted text via",
      "page": 17
    },
    {
      "caption": "Figure 12: (c), in differences between the true-positive rate (TPR)",
      "page": 17
    },
    {
      "caption": "Figure 15: demonstrate the difference in the",
      "page": 17
    },
    {
      "caption": "Figure 15: (left) is the case consisting of a mix of simple",
      "page": 17
    },
    {
      "caption": "Figure 15: (right) poses more obscurity to TOCR, as compared to",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fear": "mom (27.2419)",
          "Anger": "face (40.5642)",
          "Joy": "love (56.7116)",
          "Sadness": "depression (45.3137)",
          "Surprise": "realize (17.5789)",
          "Disgust": "absolutely (38.0648)"
        },
        {
          "Fear": "scared (21.0282)",
          "Anger": "mad (33.5129)",
          "Joy": "day (38.6249)",
          "Sadness": "life (40.7641)",
          "Surprise": "like (16.5175)",
          "Disgust": "face (11.9207)"
        },
        {
          "Fear": "pick (15.2956)",
          "Anger": "like (29.2126)",
          "Joy": "excited (38.2704)",
          "Sadness": "like (38.8570)",
          "Surprise": "oh (14.6915)",
          "Disgust": "people (11.6430)"
        },
        {
          "Fear": "people (13.5623)",
          "Anger": "know (23.5728)",
          "Joy": "friend (36.5688)",
          "Sadness": "anxiety (33.9166)",
          "Surprise": "meme (13.0155)",
          "Disgust": "make (9.8716)"
        },
        {
          "Fear": "hear (12.0607)",
          "Anger": "make (23.2726)",
          "Joy": "mom (35.4429)",
          "Sadness": "day (31.8213)",
          "Surprise": "people (12.3859)",
          "Disgust": "food (7.7485)"
        },
        {
          "Fear": "spider (11.9407)",
          "Anger": "just (22.9047)",
          "Joy": "good (35.2838)",
          "Sadness": "depressed (29.3055)",
          "Surprise": "time (12.2300)",
          "Disgust": "meme (7.3623)"
        },
        {
          "Fear": "afraid (11.3161)",
          "Anger": "people (22.6959)",
          "Joy": "friends (33.2909)",
          "Sadness": "lonely (29.1180)",
          "Surprise": "just (10.7769)",
          "Disgust": "look (6.8671)"
        },
        {
          "Fear": "says (10.9221)",
          "Anger": "look (21.8237)",
          "Joy": "like (27.5951)",
          "Sadness": "going (28.7829)",
          "Surprise": "mom (10.2412)",
          "Disgust": "like (6.3189)"
        },
        {
          "Fear": "home (9.0328)",
          "Anger": "time (20.6286)",
          "Joy": "make (26.5385)",
          "Sadness": "friends (26.8488)",
          "Surprise": "face (9.2913)",
          "Disgust": "realize (6.1869)"
        },
        {
          "Fear": "time (8.6592)",
          "Anger": "say (20.5275)",
          "Joy": "best (25.1675)",
          "Sadness": "feel (26.7687)",
          "Surprise": "hell (8.8602)",
          "Disgust": "just (6.0652)"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Trends in integration of vision and language research: A survey of tasks, datasets, and methods",
      "authors": [
        "A Mogadala",
        "M Kalimuthu",
        "D Klakow"
      ],
      "year": "2021",
      "venue": "JAIR"
    },
    {
      "citation_id": "2",
      "title": "Semeval-2020 task 8: Memotion analysis-the visuo-lingual metaphor!",
      "authors": [
        "C Sharma",
        "D Bhageria",
        "W Scott",
        "S Pykl",
        "A Das",
        "T Chakraborty",
        "V Pulabaigari",
        "B Gamback"
      ],
      "year": "2020",
      "venue": "Semeval-2020 task 8: Memotion analysis-the visuo-lingual metaphor!",
      "arxiv": "arXiv:2008.03781"
    },
    {
      "citation_id": "3",
      "title": "What do you meme? generating explanations for visual semantic role labelling in memes",
      "authors": [
        "S Sharma",
        "S Agarwal",
        "T Suresh",
        "P Nakov",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2023",
      "venue": "Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023",
      "doi": "10.1609/aaai.v37i8.26166"
    },
    {
      "citation_id": "4",
      "title": "MEMEX: detecting explanatory evidence for memes via knowledgeenriched contextualization",
      "authors": [
        "S Sharma",
        "U Arora",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2023.acl-long.289"
    },
    {
      "citation_id": "5",
      "title": "Domain-aware self-supervised pre-training for label-efficient meme analysis",
      "authors": [
        "S Sharma",
        "M Siddiqui",
        "M Akhtar",
        "T Chakraborty",
        "Y He",
        "H Ji",
        "Y Liu",
        "S Li",
        "C Chang",
        "S Poria",
        "C Lin",
        "W Buntine",
        "M Liakata",
        "H Yan",
        "Z Yan",
        "S Ruder"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022"
    },
    {
      "citation_id": "6",
      "title": "MOMENTA: A multimodal framework for detecting harmful memes and their targets",
      "authors": [
        "S Pramanick",
        "S Sharma",
        "D Dimitrov",
        "M Akhtar",
        "P Nakov",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana",
      "doi": "10.18653/v1/2021.findings-emnlp.379"
    },
    {
      "citation_id": "7",
      "title": "Affective image content analysis: A comprehensive survey",
      "authors": [
        "S Zhao",
        "G Ding",
        "Q Huang",
        "T.-S Chua",
        "B Schuller",
        "K Keutzer"
      ],
      "year": "2018",
      "venue": "IJCAI-18"
    },
    {
      "citation_id": "8",
      "title": "Findings of memotion 2: Sentiment and emotion analysis of memes",
      "authors": [
        "P Patwa",
        "S Ramamoorthy",
        "N Gunti",
        "S Mishra",
        "A Reganti",
        "A Das",
        "T Chakraborty",
        "A Sheth",
        "A Ekbal",
        "C Ahuja"
      ],
      "venue": "Proceedings of De-Factify: Workshop on Multimodal Fact Checking and Hate Speech Detection"
    },
    {
      "citation_id": "9",
      "title": "Social media marketing: A literature review and implications: Implications of social media marketing",
      "authors": [
        "H Alves",
        "C Fernandes",
        "M Raposo"
      ],
      "year": "2016",
      "venue": "Social media marketing: A literature review and implications: Implications of social media marketing"
    },
    {
      "citation_id": "10",
      "title": "What is meant by calling emotions basic",
      "authors": [
        "P Ekman",
        "D Cordaro"
      ],
      "year": "2011",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "11",
      "title": "Detecting harmful memes and their targets",
      "authors": [
        "S Pramanick",
        "D Dimitrov",
        "R Mukherjee",
        "S Sharma",
        "M Akhtar",
        "P Nakov",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online: ACL"
    },
    {
      "citation_id": "12",
      "title": "DANKMEMES @ EVALITA 2020: The Memeing of Life: Memes, Multimodality and Politics",
      "authors": [
        "M Miliani",
        "G Giorgi",
        "I Rama",
        "G Anselmi",
        "G Lebani"
      ],
      "venue": "DANKMEMES @ EVALITA 2020: The Memeing of Life: Memes, Multimodality and Politics"
    },
    {
      "citation_id": "13",
      "title": "EmoNet: Fine-grained emotion detection with gated recurrent neural networks",
      "authors": [
        "M Abdul-Mageed",
        "L Ungar"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "14",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE TAC"
    },
    {
      "citation_id": "15",
      "title": "Convolutional mkl based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "ICDM"
    },
    {
      "citation_id": "16",
      "title": "Multimodal classification for analysing social media",
      "authors": [
        "C Duong",
        "R Lebret",
        "K Aberer"
      ],
      "year": "2017",
      "venue": "Multimodal classification for analysing social media",
      "arxiv": "arXiv:1708.02099"
    },
    {
      "citation_id": "17",
      "title": "Multimodal sentiment analysis to explore the structure of emotions",
      "authors": [
        "A Hu",
        "S Flaxman"
      ],
      "year": "2018",
      "venue": "SIGKDD"
    },
    {
      "citation_id": "18",
      "title": "Correlated attention networks for multimodal emotion recognition",
      "authors": [
        "J Qiu",
        "X Li",
        "K Hu"
      ],
      "year": "2018",
      "venue": "BIBM"
    },
    {
      "citation_id": "19",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "20",
      "title": "Multimodal emotion recognition with transformer-based self supervised feature fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Emoticon: Context-aware multimodal emotion recognition using frege's principle"
    },
    {
      "citation_id": "22",
      "title": "On the origins of memes by means of fringe web communities",
      "authors": [
        "S Zannettou",
        "T Caulfield",
        "J Blackburn",
        "E De Cristofaro",
        "M Sirivianos",
        "G Stringhini",
        "G Suarez-Tangil"
      ],
      "year": "2018",
      "venue": "IMC '18"
    },
    {
      "citation_id": "23",
      "title": "Multimodal meme dataset (MultiOFF) for identifying offensive content in image and text",
      "authors": [
        "S Suryawanshi",
        "B Chakravarthi",
        "M Arcan",
        "P Buitelaar"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Troll"
    },
    {
      "citation_id": "24",
      "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "authors": [
        "D Kiela",
        "H Firooz",
        "A Mohan",
        "V Goswami",
        "A Singh",
        "P Ringshia",
        "D Testuggine"
      ],
      "year": "2020",
      "venue": "The hateful memes challenge: Detecting hate speech in multimodal memes"
    },
    {
      "citation_id": "25",
      "title": "Exploring hate speech detection in multimodal publications",
      "authors": [
        "R Gomez",
        "J Gibert",
        "L Gomez",
        "D Karatzas"
      ],
      "venue": "Exploring hate speech detection in multimodal publications"
    },
    {
      "citation_id": "26",
      "title": "MOMENTA: A multimodal framework for detecting harmful memes and their targets",
      "authors": [
        "S Pramanick",
        "S Sharma",
        "D Dimitrov",
        "M Akhtar",
        "P Nakov",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "Findings of EMNLP 2021"
    },
    {
      "citation_id": "27",
      "title": "Old jokes, new mediaonline sexism and constructions of gender in internet memes",
      "authors": [
        "J Drakett",
        "B Rickett",
        "K Day",
        "K Milnes"
      ],
      "year": "2018",
      "venue": "Fem. & Psy"
    },
    {
      "citation_id": "28",
      "title": "Black memes matter: #livingwhileblack with Becky and Karen",
      "authors": [
        "A Williams"
      ],
      "year": "2020",
      "venue": "Social Media + Society"
    },
    {
      "citation_id": "29",
      "title": "On frogs, monkeys, and execution memes: Exploring the humor-hate nexus at the intersection of neo-Nazi and alt-right movements in Sweden",
      "authors": [
        "T Askanius"
      ],
      "year": "2021",
      "venue": "Tel. & New Media"
    },
    {
      "citation_id": "30",
      "title": "Findings of the CONSTRAINT 2022 shared task on detecting the hero, the villain, and the victim in memes",
      "authors": [
        "S Sharma",
        "T Suresh",
        "A Kulkarni",
        "H Mathur",
        "P Nakov",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2022",
      "venue": "Findings of the CONSTRAINT 2022 shared task on detecting the hero, the villain, and the victim in memes"
    },
    {
      "citation_id": "31",
      "title": "Hate is the new infodemic: A topic-aware modeling of hate speech diffusion on twitter",
      "authors": [
        "S Masud",
        "S Dutta",
        "S Makkar",
        "C Jain",
        "V Goyal",
        "A Das",
        "T Chakraborty"
      ],
      "year": "2021",
      "venue": "2021 IEEE 37th International Conference on Data Engineering (ICDE)"
    },
    {
      "citation_id": "32",
      "title": "Handling bias in toxic speech detection: A survey",
      "authors": [
        "T Garg",
        "S Masud",
        "T Suresh",
        "T Chakraborty"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "33",
      "title": "Proactively reducing the hate intensity of online posts via hate speech normalization",
      "authors": [
        "S Masud",
        "M Bedi",
        "M Khan",
        "M Akhtar",
        "T Chakraborty"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "34",
      "title": "Revisiting hate speech benchmarks: From data curation to system deployment",
      "authors": [
        "A Kulkarni",
        "S Masud",
        "V Goyal",
        "T Chakraborty"
      ],
      "year": "2023",
      "venue": "Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "35",
      "title": "Recent advances in hate speech moderation: Multimodality and the role of large models",
      "authors": [
        "M Hee",
        "S Sharma",
        "R Cao",
        "P Nandi",
        "P Nakov",
        "T Chakraborty",
        "R Lee"
      ],
      "year": "2024",
      "venue": "CoRR",
      "doi": "10.48550/arXiv.2401.16727"
    },
    {
      "citation_id": "36",
      "title": "Detecting hate speech in multi-modal memes",
      "authors": [
        "A Das",
        "J Wahi",
        "S Li"
      ],
      "year": "2020",
      "venue": "Detecting hate speech in multi-modal memes"
    },
    {
      "citation_id": "37",
      "title": "Detecting hateful memes using a multimodal deep ensemble",
      "authors": [
        "V Sandulescu"
      ],
      "year": "2020",
      "venue": "Detecting hateful memes using a multimodal deep ensemble",
      "arxiv": "arXiv:2012.13235"
    },
    {
      "citation_id": "38",
      "title": "SAFE: Similarity-aware multi-modal fake news detection",
      "authors": [
        "X Zhou",
        "J Wu",
        "R Zafarani"
      ],
      "year": "2020",
      "venue": "PAKDD"
    },
    {
      "citation_id": "39",
      "title": "A multimodal framework for the detection of hateful memes",
      "authors": [
        "P Lippe",
        "N Holla",
        "S Chandra",
        "S Rajamanickam",
        "G Antoniou",
        "E Shutova",
        "H Yannakoudakis"
      ],
      "year": "2020",
      "venue": "A multimodal framework for the detection of hateful memes",
      "arxiv": "arXiv:2012.12871"
    },
    {
      "citation_id": "40",
      "title": "KnowMeme: A knowledge-enriched graph neural network solution to offensive meme detection",
      "authors": [
        "L Shang",
        "C Youn",
        "Y Zha",
        "Y Zhang",
        "D Wang"
      ],
      "venue": "KnowMeme: A knowledge-enriched graph neural network solution to offensive meme detection"
    },
    {
      "citation_id": "41",
      "title": "Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation",
      "authors": [
        "K Karkkainen",
        "J Joo"
      ],
      "venue": "Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation"
    },
    {
      "citation_id": "42",
      "title": "Detecting and understanding harmful memes: A survey",
      "authors": [
        "S Sharma",
        "F Alam",
        "M Akhtar",
        "D Dimitrov",
        "G Da San Martino",
        "H Firooz",
        "A Halevy",
        "F Silvestri",
        "P Nakov",
        "T Chakraborty"
      ],
      "year": "2022",
      "venue": "IJCAI-ECAI '22"
    },
    {
      "citation_id": "43",
      "title": "Image-based memes as sentiment predictors",
      "authors": [
        "J French"
      ],
      "year": "2017",
      "venue": "Image-based memes as sentiment predictors"
    },
    {
      "citation_id": "44",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "ACL-HLT"
    },
    {
      "citation_id": "45",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "EMNLP '14"
    },
    {
      "citation_id": "46",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Efficientnet: Rethinking model scaling for convolutional neural networks"
    },
    {
      "citation_id": "47",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "48",
      "title": "Lt3 at semeval-2020 task 8: Multi-modal multi-task learning for memotion analysis",
      "authors": [
        "P Singh",
        "N Bauwelinck",
        "E Lefever"
      ],
      "year": "2020",
      "venue": "SEMEVAL"
    },
    {
      "citation_id": "49",
      "title": "Upb at semeval-2020 task 8: Joint textual and visual modeling in a multi-task learning architecture for memotion analysis",
      "authors": [
        "G.-A Vlad",
        "G.-E Zaharia",
        "D.-C Cercel",
        "C.-G Chiru",
        "S Trausan-Matu"
      ],
      "year": "2020",
      "venue": "Upb at semeval-2020 task 8: Joint textual and visual modeling in a multi-task learning architecture for memotion analysis"
    },
    {
      "citation_id": "50",
      "title": "Interrater reliability: the kappa statistic",
      "authors": [
        "M Mchugh"
      ],
      "year": "2012",
      "venue": "Biochemia medica"
    },
    {
      "citation_id": "51",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "52",
      "title": "R-fcn: Object detection via region-based fully convolutional networks",
      "authors": [
        "J Dai",
        "Y Li",
        "K He",
        "J Sun"
      ],
      "year": "2016",
      "venue": "NeurIPS '16"
    },
    {
      "citation_id": "53",
      "title": "Mask r-cnn",
      "authors": [
        "K He",
        "G Gkioxari",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2017",
      "venue": "ICCV '17"
    },
    {
      "citation_id": "54",
      "title": "Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
      "authors": [
        "C.-Y Wang",
        "A Bochkovskiy",
        "H.-Y Liao"
      ],
      "year": "2022",
      "venue": "Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"
    },
    {
      "citation_id": "55",
      "title": "Objects as points",
      "authors": [
        "X Zhou",
        "D Wang",
        "P Krähenbühl"
      ],
      "year": "1904",
      "venue": "ArXiv"
    },
    {
      "citation_id": "56",
      "title": "Cornernet: Detecting objects as paired keypoints",
      "authors": [
        "H Law",
        "J Deng"
      ],
      "year": "2018",
      "venue": "ECCV '18"
    },
    {
      "citation_id": "57",
      "title": "Gated multimodal units for information fusion",
      "authors": [
        "J Arevalo",
        "T Solorio",
        "M Montes-Y Gómez",
        "F González"
      ],
      "year": "2017",
      "venue": "Gated multimodal units for information fusion",
      "arxiv": "arXiv:1702.01992"
    },
    {
      "citation_id": "58",
      "title": "Hadamard product for low-rank bilinear pooling",
      "authors": [
        "J.-H Kim",
        "K.-W On",
        "W Lim",
        "J Kim",
        "J.-W Ha",
        "B.-T Zhang"
      ],
      "year": "2016",
      "venue": "Hadamard product for low-rank bilinear pooling",
      "arxiv": "arXiv:1610.04325"
    },
    {
      "citation_id": "59",
      "title": "Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "J Lu",
        "D Batra",
        "D Parikh",
        "S Lee"
      ],
      "year": "2019",
      "venue": "NeurIPS '19"
    },
    {
      "citation_id": "60",
      "title": "Visualbert: A simple and performant baseline for vision and language",
      "authors": [
        "L Li",
        "M Yatskar",
        "D Yin",
        "C.-J Hsieh",
        "K.-W Chang"
      ],
      "year": "2019",
      "venue": "Visualbert: A simple and performant baseline for vision and language",
      "arxiv": "arXiv:1908.03557"
    },
    {
      "citation_id": "61",
      "title": "Multimodal categorization of crisis events in social media",
      "authors": [
        "M Abavisani",
        "L Wu",
        "S Hu",
        "J Tetreault",
        "A Jaimes"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "62",
      "title": "Adaptive co-attention network for named entity recognition in tweets",
      "authors": [
        "Q Zhang",
        "J Fu",
        "X Liu",
        "X Huang"
      ],
      "year": "2018",
      "venue": "Adaptive co-attention network for named entity recognition in tweets"
    },
    {
      "citation_id": "63",
      "title": "Delving deep into label smoothing",
      "authors": [
        "C.-B Zhang",
        "P.-T Jiang",
        "Q Hou",
        "Y Wei",
        "Q Han",
        "Z Li",
        "M.-M Cheng"
      ],
      "year": "2021",
      "venue": "IEEE Tran. on Image Proc"
    },
    {
      "citation_id": "64",
      "title": "Supervised multimodal bitransformers for classifying images and text",
      "authors": [
        "D Kiela",
        "S Bhooshan",
        "H Firooz",
        "E Perez",
        "D Testuggine"
      ],
      "year": "2019",
      "venue": "Supervised multimodal bitransformers for classifying images and text",
      "arxiv": "arXiv:1909.02950"
    },
    {
      "citation_id": "65",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "66",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2019",
      "venue": "IJCV"
    },
    {
      "citation_id": "67",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "68",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "ICLR"
    },
    {
      "citation_id": "69",
      "title": "IITK at SemEval-2020 task 8: Unimodal and bimodal sentiment analysis of Internet memes",
      "authors": [
        "V Keswani",
        "S Singh",
        "S Agarwal",
        "A Modi"
      ],
      "year": "2020",
      "venue": "IITK at SemEval-2020 task 8: Unimodal and bimodal sentiment analysis of Internet memes"
    },
    {
      "citation_id": "70",
      "title": "UPB at SemEval-2020 task 8: Joint textual and visual modeling in a multi-task learning architecture for memotion analysis",
      "authors": [
        "G Vlad",
        "G Zaharia",
        "D Cercel",
        "C Chiru",
        "S Trausan-Matu"
      ],
      "year": "2020",
      "venue": "UPB at SemEval-2020 task 8: Joint textual and visual modeling in a multi-task learning architecture for memotion analysis"
    },
    {
      "citation_id": "71",
      "title": "Guoym at SemEval-2020 task 8: Ensemble-based classification of visuo-lingual metaphor in memes",
      "authors": [
        "Y Guo",
        "J Huang",
        "Y Dong",
        "M Xu"
      ],
      "year": "2020",
      "venue": "Guoym at SemEval-2020 task 8: Ensemble-based classification of visuo-lingual metaphor in memes"
    },
    {
      "citation_id": "72",
      "title": "Improved fusion of vis. and lang. representations by dense symmetric co-attention for vqa",
      "authors": [
        "D.-K Nguyen",
        "T Okatani"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "73",
      "title": "Bertopic: Neural topic modeling with a class-based tf-idf procedure",
      "authors": [
        "M Grootendorst"
      ],
      "year": "2022",
      "venue": "Bertopic: Neural topic modeling with a class-based tf-idf procedure",
      "arxiv": "arXiv:2203.05794"
    },
    {
      "citation_id": "74",
      "title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP '19"
    },
    {
      "citation_id": "75",
      "title": "Umap: Uniform manifold approximation and projection",
      "authors": [
        "L Mcinnes",
        "J Healy",
        "N Saul",
        "L Großberger"
      ],
      "year": "2018",
      "venue": "J. of Open Src. Soft"
    },
    {
      "citation_id": "76",
      "title": "Density-based clustering based on hierarchical density estimates",
      "authors": [
        "R Campello",
        "D Moulavi",
        "J Sander"
      ],
      "year": "2013",
      "venue": "Density-based clustering based on hierarchical density estimates"
    },
    {
      "citation_id": "77",
      "title": "The Nature of Emotions: Clinical Implications",
      "authors": [
        "R Plutchik"
      ],
      "year": "1988",
      "venue": "The Nature of Emotions: Clinical Implications"
    }
  ]
}