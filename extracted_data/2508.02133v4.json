{
  "paper_id": "2508.02133v4",
  "title": "Hierarchical Moe: Continuous Multimodal Emotion Recognition With Incomplete And Asynchronous Inputs",
  "published": "2025-08-04T07:29:17Z",
  "authors": [
    "Yitong Zhu",
    "Lei Han",
    "Guanxuan Jiang",
    "PengYuan Zhou",
    "Yuyang Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MER) is crucial for human-computer interaction, yet real-world challenges like dynamic modality incompleteness and asynchrony severely limit its robustness. Existing methods often assume consistently complete data or lack dynamic adaptability. To address these limitations, we propose a novel Hi-MoE (Hierarchical Mixture-of-Experts) framework for robust continuous emotion prediction. This framework employs a dual-layer expert structure. A Modality Expert Bank utilizes soft routing to dynamically handle missing modalities and achieve robust information fusion. A subsequent Emotion Expert Bank leverages differential-attention routing to flexibly attend to emotional prototypes, enabling fine-grained emotion representation. Additionally, a cross-modal alignment module explicitly addresses temporal shifts and semantic inconsistencies between modalities. Extensive experiments on benchmark datasets DEAP and DREAMER demonstrate our model's state-of-the-art performance in continuous emotion regression, showcasing exceptional robustness under challenging conditions such as dynamic modality absence and asynchronous sampling. This research significantly advances the development of intelligent emotion systems adaptable to complex real-world environments. The code is uploaded in the Supplementary Material.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition (MER) is critical for applications such as healthcare, social media, and personalized services  (A.V. et al. 2023; Wang 2024) , integrating modalities  (Zheng et al. 2019)  to capture complex affective states. However, a fundamental challenge in real-world MER lies in handling dynamic modality incompleteness and asynchrony, where different modalities may be arbitrarily absent or misaligned across various time steps and individual samples. This inherent heterogeneity severely compromises model robustness and generalization, especially for continuous emotion modeling which aims to provide a more nuanced and dynamic representation compared to discrete classification  (Kansizoglou et al. 2022; Romeo et al. 2022) .\n\nAdaptive fusion strategies are essential for handling dynamic and arbitrary modality incompleteness in MER.\n\nWhile methods like early fusion, late fusion, and attentionbased integration have been proposed  (Alswaidan et al. 2020; ArunaGladys et al. 2023) , they typically assume consistent modality availability. Most approaches focus on enhancing cross-modal interactions or feature alignment to improve performance with complete inputs, with limited emphasis on robustness to arbitrarily missing modalities (e.g., varying absences across time steps or samples). Simple techniques, such as zero-imputation or modality dropout, often yield suboptimal predictions and significant performance degradation. However, designing sophisticated fusion mechanisms for robust continuous emotion modeling under dynamic modality absence remains underexplored.\n\nRecent studies have explored Mixture-of-Experts (MoE) architectures in multimodal learning to address incomplete modality availability  (Wu et al. 2023; Xu et al. 2024) . By conditionally activating expert modules, MoE offers structural flexibility and has shown initial promise in handling modality uncertainty. However, existing MoE implementations  (Zhou et al. 2022; Dai et al. 2022 ) often rely on hard routing or single-path activation, which restricts inter-modal collaboration and flexible integration of partial information. Furthermore, their application has predominantly focused on shallow architectures and discrete classification, consequently leaving their efficacy for continuous emotion regression largely uninvestigated within the MoE paradigm. A critical limitation is the common assumption of strict temporal alignment; signal dynamics differences between different modalities  (Binias et al. 2020 ) often cause semantic misalignment. Collectively, these limitations reveal a significant gap: current MoE models primarily address modalitylevel variations, with limited exploration of their potential to model the complex, continuous structure of emotions for robust affective state prediction.\n\nMotivated by these limitations, we propose a novel frame work, Hi-MoE (hierarchical MoE), for multi-dimensional continuous emotion regression. Our framework features two key components: a Modality Expert Bank that uses soft routing for robust inference under missing modalities, and an Emotion Expert Bank with attention-based routing to select informative emotional prototypes within the fusion space. This dual-level architecture uniquely balances flexible modality adaptation with selective and semantically meaningful representation learning. Furthermore, it incor-porates a cross-modal alignment module to explicitly handle temporal shifts and semantic inconsistencies, enhancing the model's capacity to capture complementary cues across a wide range of modalities. We demonstrate the effectiveness of Hi-MoE on real-world datasets, including the DEAP and the DREAMER, which involves several key modalities including behavioral and physiological modalities. The results confirm the robustness and generalization of Hi-MoE in diverse scenarios of incomplete and asynchronous inputs. The main contributions of this study are summarized as follows:\n\n• We propose a Hierarchical MoE framework that effectively incorporates arbitrary modality combinations and robustly addresses the missing modality scenario, with soft-routing for modality handling and differentialattention routing for emotion prototype modeling. • We incorporate a dedicated cross-modal alignment module that explicitly handles temporal shifts and semantic inconsistencies between different modalities. • Experiments on DEAP and DREAMER showcase the consistent and robust performance of Hi-MoE in handling diverse modality combinations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work Multimodal Emotion Recognition Methods",
      "text": "Multimodal emotion recognition (MER) integrates complementary cues from modalities like physiological signals and behavioral cues to enhance accuracy via cross-modal fusion.\n\nEarly works, such as  Poria et al. (2015)  and  Williams et al. (2018) , employed early and late fusion for unified emotion representations. Subsequent studies introduced LSTMbased models for intra-modality temporal modeling  (Huang et al. 2017; Su et al. 2020 ) and self-attention mechanisms for long-range dependencies  (Sun et al. 2020 (Sun et al. , 2021)) . Hybrid approaches are usee to balanced early and late fusion benefits  (Nemati et al. 2019) . Despite these advances, existing methods often fail to capture fine-grained inter-modality interactions or rely on modality-specific pipelines, limiting their flexibility in complex scenarios. Intermediate fusion strategies, such as Self-MM  (Yu et al. 2021)  and  Han et al. (2021) , integrate modalities within network layers to improve cross-modal modeling. Transformer-based architectures further enhance dynamic inter-modal dependencies  (Zadeh et al. 2018; Mittal et al. 2020) , with some addressing cross-modal asynchrony  (Lv et al. 2021 ). However, most approaches assume complete and synchronized multimodal inputs, rendering them less effective in real-world settings with missing or degraded data. Our work aims to fill this critical gap by developing robust and adaptive fusion mechanisms that adapt to varying modality availability and explicitly manage asynchronous responses or semantic disparities.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Mixture Of Experts For Multimodal Learning",
      "text": "The Mixture-of-Experts (MoE) framework, first proposed by  Jacobs et al. (1991)  and  Jordan et al. (1994) , employs a gating mechanism to route inputs to specialized expert networks, enabling dynamic resource allocation and efficient scaling for high-capacity tasks  (Fedus, Zoph, and Shazeer 2022) . In deep learning,  Shazeer et al. (2017)  advanced MoE with sparsely-gated Transformers, followed by developments like GShard for large-scale translation  (Lepikhin et al. 2020) , Switch Transformer for simplified routing  (Fedus, Zoph, and Shazeer 2022) , and MoEfication for transforming dense models  (Riquelme et al. 2021) . Stable training strategies were also introduced  (Roller et al. 2021) . MoE has been adapted for multi-task learning, with frameworks like VmoE  (Riquelme et al. 2021)  and TaskMoE (Ye and Xu 2023) using task-aware gating to select relevant experts.\n\nRecently, MoE has been applied to multimodal learning to address cross-modal heterogeneity.  Cheng et al. (2024)  developed a dynamic fusion MoE for audio-visual speech recognition, while  Huai et al. (2025)  proposed a dual-router MoE for continual learning across modalities. In emotion recognition, MoE has been used to align modality-specific and invariant features  (Gao et al. 2024; Fang et al. 2025 ). However, these approaches often neglect temporal dynamics and modality-specific semantics, relying on shallow structures and rigid routing, which limits their expressiveness and robustness to missing or asynchronous inputs. To address these limitations, the proposed hierarchical MoE framework employs adaptive routing and conditional structure selection to effectively handle cross-modal heterogeneity and temporal dynamics for continuous emotion prediction.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "This section presents the Hierarchical Mixture-of-Experts (Hi-MoE) framework for robust continuous emotion recognition with incomplete and asynchronous multimodal inputs. As shown in Fig.  1 , the architecture features a duallayer expert structure: a Modality Expert Bank to adaptively process diverse input modalities and handle missing data, and an Emotion Expert Bank to capture fine-grained emotional prototypes. A cross-modal alignment module is integrated to address temporal and semantic inconsistencies across modalities. The framework employs adaptive routing and conditional structure selection to enhance robustness and expressiveness. The following subsections detail the design of it. While exemplified using eight modalities shown in the figure, the proposed model is generalizable to various modality combinations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Modality-Specific Encoder",
      "text": "Considering the structural and semantic differences across modalities, we introduce modality-specific encoders to project inputs into a unified feature space. To enhance sensitivity to short-term emotional dynamics, a sliding window is applied to all raw modalities, increasing temporal density and promoting local variation awareness.\n\nWe employ various encoder architectures, each specifically tailored to its respective modality's characteristics, to effectively capture its unique patterns. To preserve modalityspecific semantics, parameters are not shared across encoders. The detailed architectures of these encoders are provided in the Appendix. The resulting features {Z m } are then temporally aligned and serve as input to the subsequent expert routing and fusion modules.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Modal Alignment Module",
      "text": "Direct temporal fusion of raw or fused multimodal representations often overlooks response delays inherent in MER, leading to semantic misalignment and unreliable emotion trajectory modeling. To address this, we introduce a contrastive learning-based alignment module to mitigate temporal lags and semantic discrepancies in the latent space.\n\nFor each pair of modality representations [Z i , Z j ] ∈ R T ×d , where T is the number of time steps and d is the feature dimension, derived from the modality-specific encoders, we apply ℓ 2 normalization and compute a similarity matrix over the concatenated batch:\n\nwhere τ is a temperature hyperparameter, and diagonal entries of S are masked to avoid trivial self-similarity. The NT-Xent loss is employed to align semantically similar representations across modalities despite temporal misalignments. The training details provided in the next subsection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hierarchical Moe Structure",
      "text": "Modality Expert Bank To address modality heterogeneity, missing data, and varying signal quality, we propose a Modality Expert Bank as the first layer of the hierarchical framework. This component adaptively processes diverse input modalities while handling absence or asynchrony. At each time step t, the input from all potential modalities is denoted as\n\nt , . . . , x (M ) t\n\n}, where x t denotes the feature vector for modality m, and M is the total number of modalities. For missing modalities, m is set to a zero vector of appropriate dimension, ensuring a consistent input structure while indicating the absence of data.\n\nEach modality m is associated with an expert bank\n\nK }, comprising K learnable expert networks. A modality-specific soft gating mechanism computes routing weights as :\n\nwhere W to produce default weights, signaling the absence of valid input. The modality-specific output is computed as a weighted sum of expert responses:\n\nwhere α (m) t\n\n[k] denotes the k-th element of the gating weights. This adaptive routing mechanism enables robust processing regardless of modality availability. The modalityspecific representations are then fused into a unified embedding:\n\nwhere Fuse(•) is an attention-based aggregation mechanism that integrates modality-specific features.\n\n(5) where ϕ(•, •) is a similarity function that measures the relevance between the input z t and the output of the l-th emotion expert F emo l (z t ). This differential attention mechanism enables the DA-router to prioritize experts based on their alignment with the emotional context. The output of this module is computed as:\n\nwhere β t [l] represents the weight for the l-th expert. The DA-router's attention-based design enhances adaptability to complex emotional patterns under diverse input conditions. The resulting representation e t serves as the final emotionspecific feature for downstream prediction heads.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dimension-Specific Prediction And Loss",
      "text": "To model diverse emotion dimensions without task interference, we use independent output heads for each dimension.\n\nThe model supports two prediction formats: (1) binary classification for high/low levels and (2) ordinal regression on a 1-9 Likert scale for fine-grained intensity estimation.\n\nFor cross-modal alignment, positive pairs are defined as representations of the same sample across different modalities. The Normalized Temperature-scaled Cross Entropy (NT-Xent) loss ( Ågren 2022) is formulated as:\n\nwhere S i,j is the similarity score from Eq. 1, and y i denotes the index of the positive sample paired with i.\n\nFor emotion prediction, we use cross-entropy loss for binary classification and mean squared error (MSE) for ordinal regression. A label mask excludes missing labels from gradient updates, enhancing training stability. When the crossmodal alignment module is enabled, the alignment loss is jointly optimized with the emotion prediction loss. The total training loss is:\n\nwhere Lemo represents the emotion prediction loss, L NT-Xent is the contrastive alignment loss, and λ is a tunable coefficient balancing the two objectives. Training details are discussed in the following section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments Experimental Setup",
      "text": "Dataset To evaluate the performance of our proposed model on emotion recognition, we conduct experiments on two widely used multimodal emotion datasets: DEAP (Koelstra et al. 2012) and DREAMER  (Katsigiannis and Ramzan 2018) . Both datasets provide synchronized signals with continuous emotional annotations, offering a suitable benchmark for evaluating fine-grained affective modeling, as summarized in Table  7 . For brevity, we refer to the four affective dimensions, Valence(V), Arousal(A), Dominance(D), and Liking(L), in the following sections. The more comprehensive description of the two datasets and their pre-processing is shown in the Appendix. Experimental Settings All experiments were conducted on six NVIDIA RTX 4090 GPUs using PyTorch 1.13.1. We evaluated the model under both subject-dependent and subject-independent protocols, using 10-fold crossvalidation and Leave-One-Subject-Out (LOSO) validation, respectively. Results are reported as the mean ± standard deviation across folds or subjects. Models were trained for 75 epochs with Adam optimizer, cosine annealing, batch size 32 per GPU, and early stopping to prevent overfitting.\n\nBaseline Methods We compare Hi-MoE with a set of representative and state-of-the-art multimodal fusion baselines commonly used in emotion recognition tasks. These include late fusion  (Tang et al. 2017) , hybrid fusion Bi-LSTM (Zhao and Chen 2021), attention-based TACOFormer  (Li 2023) , and transformer fusion IANet  (Li et al. 2024) . All of which have demonstrated strong performance on benchmark datasets mentioned before. All baselines are implemented under consistent preprocessing, input formats, and training protocols to ensure fair comparison. We also adapt them to support continuous emotion regression by replacing the classification head with regression layers and using the same evaluation metrics introduced above. And * in the tables indicates that the results of this model were reproduced by ourselves, while others are from the literature.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Subject-Dependent Experiments",
      "text": "We first evaluate the proposed method in a subjectdependent setting using 10-fold cross-validation, with each subject's trials evenly split across folds. The following subsections present results on emotion regression, binary classification, robustness to missing modalities, and ablation studies. Unless noted otherwise, performance is reported using the previously introduced metrics.\n\nPerformance on Complete Multimodal Data To establish our model's baseline ability to capture fine-grained emotional variations using complete multimodal inputs and to demonstrate its generalizability across diverse datasets, we evaluate the proposed Hierarchical MoE framework on continuous emotion regression across four affective dimensions.\n\nWe conduct experiments on multiple benchmark datasets, including DEAP and DREAMER, which are shown in Tab. 2. We construct a fair and consistent comparison with representative baselines by training all methods under the same modality configurations and data preprocessing procedures for each respective dataset.\n\nTo further illustrate the model's ability to capture finegrained emotional changes, we visualize the predicted trajectories over time in comparison with the ground truth. Fig.  2  shows the V-A dimension (others are in the Appendix). Our model closely tracks the emotional trend while producing smoother and more temporally consistent predictions than baseline methods.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ground Truth Ours Tacoformer",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Robustness Under Modality Missing",
      "text": "In real-world multimodal scenarios, input modalities are often partially unavailable due to sensor failures, occlusion, or transmission noise. To evaluate Hi-MoE's robustness under such conditions, we conduct controlled experiments with varying modality missing rates during inference. We focus on the all-modality setting, as no missing input, which represents a typical multimodal configuration with complementary physiological signals. Additional results with other combinations for other combinations are provided in the Appendix. Specifically, we simulate incomplete inputs by randomly masking arbitrary combinations of modalities for each sample in both training and evaluation. The missing rate r ranges from 0.00 to 0.40 in increments of 0.05. Each configuration is repeated with five random seeds, and average performance is reported to reduce the influence of stochastic variation.\n\nTab. 3 presents the precise CCC values across different modality missing rates for Hi-MoE and various fusion-based baselines. Complementing this, Fig.  3  visually illustrates the performance trends. As expected, the performance of all models degrades as the missing rate increases. However, the proposed model consistently exhibits significantly slower performance degradation compared to the baselines mentioned before. At a 0.00 missing rate, it achieves a CCC of 0.9698, and even with a high 35% missing input, it maintains a competitive CCC of 0.8371, which is substantially higher than Hybrid Fusion (0.5744), IANet (0.6051), and TACO-Former (0.5744) at the same missing rate. This remarkable resilience to modality loss, even up to 40% missing inputs (CCC of 0.7422), strongly highlights the effectiveness of the Modality Expert Bank in dynamically re-routing computation and preserving task performance under incomplete signals. These results underscore the framework's superior robustness, making it highly suitable for real-world applications where data integrity cannot always be guaranteed. These results demonstrate that each component of the proposed framework contributes meaningfully to its overall performance and highlight the importance of hierarchical structure, adaptive expert routing, and temporal alignment in continuous emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discrete Classification With Adaptive Modality Handling",
      "text": "While the primary goal is continuous emotion regression, we also evaluate the framework's generalization ability on standard binary classification tasks. To adapt our model, we replace the regression head with a binary classification head, trained using binary cross-entropy loss, with no other architectural changes.\n\nTo ensure a fair and comprehensive comparison with existing baselines, which often report their best performance on binary classification using specific, and sometimes vary-",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Subject-Independent Evaluation",
      "text": "To evaluate the generalization capability of Hi-MoE across unseen individuals, we conduct subject-independent experiments using a LOSO cross-validation protocol. In this setup, for each fold, data from one subject is held out for testing while the model is trained on all remaining participants. This rigorous protocol simulates real-world scenarios where emotion recognition systems must operate on users not seen during training. Tab. 5 reported the average metrics across all subjects, the proposed framework also achieves strong gen-eralization performance, consistently outperforming baseline fusion methods, which demonstrate the robustness of the hierarchical soft expert design in modeling individualinvariant affective patterns.   To thoroughly evaluate the contribution of each proposed component and analyze key design choices within the posed framework, we conduct comprehensive ablation stud-ies. These experiments aim to demonstrate the necessity of each module for robust and fine-grained continuous emotion regression, and to provide insights into their specific functionalities. All ablation studies are performed on the DEAP dataset for regression, with additional results for classification provided in the Appendix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Contribution Of Core Modules",
      "text": "We first assess the individual impact of the framework's core modules by progressively removing them from the full model. Tab. 6 reports the overall average performance of the full model and its ablated variants. As observed, removing any component results in notable performance declines, confirming their essential roles in robust and fine-grained emotion regression. Specifically, ablating the Emotion Expert Bank markedly reduces performance from 0.9698 to 0.9014, underscoring its crucial role in modeling fine-grained emotional prototypes. Removing the Cross-Modal Alignment Module also leads to a significant drop of 0.0126, highlighting its importance in addressing temporal and semantic misalignments for accurate emotion tracking. Omitting Soft Routing (CCC: 0.9698 vs. 0.9477) causes substantial degradation, emphasizing its necessity for dynamic resource allocation with diverse and incomplete inputs.   4 (a) illustrates how routing weights dynamically adjust: when certain modalities are missing (indicated by '0' in the binary string on the X-axis), the model intelligently redistributes weights, often increasing reliance on available complementary modalities to compensate for the missing information. This visualization directly demonstrates how the soft routing enables flexible resource allocation and robust information integration, even when faced with arbitrary modality incompleteness. The varying weights across different experts and combinations confirm the dynamic and adaptive nature of the routing strategy, which is crucial for maintaining performance under diverse real-world conditions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of Emotion Expert Count",
      "text": "We also investigate the impact of the number of experts (L) within the Emotion Expert Bank on overall performance. As shown in Fig.  4 (b), we evaluate models with varying L values, which indicates that performance generally improves with an increasing number of experts up to a certain point, reflecting enhanced capacity to capture diverse emotional prototypes. However, beyond an optimal L, gains diminish or performance may even slightly degrade due to increased complexity or optimization challenges. These results justify the chosen L value for the Emotion Expert Bank, demonstrating a balance between model expressiveness and efficiency. Results under other missing modality rates are shown in the Appendix.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "Our hierarchical MoE framework demonstrates strong adaptability and robustness in real-world emotion recognition scenarios, achieving state-of-the-art results in continuous emotion regression on representative multimodal datasets like DEAP and DREAMER. Crucially, it maintains high prediction accuracy even under challenging conditions such as missing modalities and asynchronous sampling, directly addressing a key challenge in multimodal affective computing: reliable performance under imperfect, variable input conditions. This resilience, extending beyond controlled benchmarks to open-world settings, highlights its promise for truly deployable emotional intelligence systems and broadens the utility of MoE architectures in domains previously limited by assumptions of data reliability.\n\nAt the core of our framework is a novel dual-layer expert structure, designed for both expressiveness and adaptability. The Modality Expert Bank employs soft routing to dynamically weigh expert sub-modules, ensuring robust inference and flexible adaptation even with partial modality availability. Complementarily, the Emotion Expert Bank leverages a differential-attention routing strategy to flexibly attend to emotional prototypes, thereby capturing subtle individual or temporal affective nuances. This structural separation enhances the model's ability to dynamically adapt to varying modality quality while preserving fine-grained emotional representation. Another key contribution is our explicit consideration of temporal misalignment across modalities, a challenge often overlooked. For instance, EEG signals and facial expressions frequently exhibit response lags  (Aktürk et al. 2021; Muukkonen and Salmela 2022) . To mitigate the defect, our model incorporates a cross-modal alignment module that learns to synchronize latent features both temporally and semantically, significantly improving fusion performance.\n\nDespite the strong empirical results, the framework introduces additional computational overhead due to the use of multiple expert banks and routing operations. Moreover, the training and evaluation were conducted on datasets collected in controlled laboratory settings with limited diversity in user behavior and cultural background.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a novel hierarchical Mixtureof-Experts (MoE) framework for robust continuous emotion recognition under incomplete and asynchronous multimodal inputs, achieving state-of-the-art performance by effectively handling modality missingness and temporal inconsistencies. Future work will explore more lightweight architectures, online adaptation, user-aware learning, and validation on diverse in-the-wild datasets to enhance real-world applicability and reduce dependency on dense annotations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Appendix Modality-Specific Encoder",
      "text": "Each modality sequence X m ∈ R T ×din is processed by a dedicated encoder f m (•), producing feature representations:\n\nWe implement four types of encoders tailored to different modalities: EEG signals are encoded using EEGNet  (Lawhern et al. 2018 ) to capture spatiotemporal dependencies; facial expressions are processed via a structure-aware GCN that extracts frame-level features using EfficientNet-B0 (Tan and Le 2019), followed by AU-specific embedding heads and a graph-based GCN module; dual-channel and singlechannel physiological signals are encoded using shallow neural networks or lightweight CNN or MLP variants, depending on modality characteristics. To preserve modalityspecific semantics, parameters are not shared across encoders. The resulting features {Z m } are temporally aligned and serve as input to the expert routing and fusion modules.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Dataset Pre-Processing",
      "text": "Our study utilizes four public emotion recognition datasets: DEAP, DREAMER, SEED, and AMIGOS. Each dataset presents unique characteristics in terms of collected modalities, experimental protocols, and annotation schemes. Below in table 7, we provide a detailed description and the specific preprocessing steps applied to each dataset.\n\nFor the EEG signals in DEAP and Dreamer, we downsampled the original 512 Hz data to 128 Hz. We applied a bandpass filter from 4 Hz to 45 Hz to remove DC components and high-frequency noise. The data was segmented into 60-second trials, and a sliding window of 4 seconds with a 2-second overlap was used to create individual samples for our continuous prediction task. The PPS signals were also downsampled to 128 Hz and processed with a bandpass filter from 0.05 Hz to 2 Hz.\n\nAs for the 16-channel EEG in SEEDS, signals were downsampled from 1000 Hz to 200 Hz. We applied a bandpass filter from 1 Hz to 75 Hz. The data was segmented into 60second trials. And we converted the discrete labels into a one-hot encoding format.\n\nMoreover, for the AMIGOS datasets, we downsampled the original 128 Hz data to 64 Hz and applied a bandpass filter from 4 Hz to 45 Hz. The data was segmented into 4-second windows with a 2-second overlap. Similarly, the ECG data was downsampled and filtered (0.5-40 Hz). Video data was preprocessed to extract facial landmarks and other relevant features.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Performance On Complete Multimodal Data",
      "text": "As shown in Fig.  5 , the predicted trajectories for both Dominance and Liking dimensions demonstrate our model's ability to effectively track the ground truth trends. Similar to the V-A dimensions, our model's predictions are notably smoother and more temporally consistent compared to the baseline methods, highlighting its robustness and capability to capture nuanced emotional changes across all annotated dimensions.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ground Truth Ours Tacoformer",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Robustness Under Modality Missing",
      "text": "To provide a more comprehensive evaluation of our model's robustness to missing data, we extend the analysis from the main paper to a more constrained multimodal setting: the EEG+PPS and the EEG+EOG+EMG+EDA combination. This configuration is widely studied in emotion recognition and allows for a direct comparison with methods that rely solely on these two modalities. The experimental setup remains the same as described in the main paper, with the modality missing rate (r) ranging from 0.00 to 0.40.\n\nAs shown in Table  8  and 9, our model consistently outperforms the baseline methods across all modality missing rates. These results further underscore the effectiveness of our hierarchical Mixture-of-Experts framework. The adaptive soft routing mechanism of our Modality Expert Bank successfully re-weights the available physiological signals, enabling robust information fusion even when one of the modalities is frequently unavailable. This demonstrates that our model's robustness is not limited to the all-modality configuration but generalizes well to more specific and constrained multimodal setups, reinforcing its suitability for real-world applications.\n\nTo supplement the tables and analysis provided in the main paper, we visualize the performance trends of our model and baselines under various modality missing rates. As shown in Figure  6  and Figure  7 , our model demonstrates a more robust and graceful degradation curve compared to the baselines. This visual evidence further confirms that our framework is more resilient to modality absence and maintains superior performance even as the missing rate increases.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Discrete Classification With Adaptive Modality Handling",
      "text": "To comprehensively evaluate the model's performance, we conducted further experiments on binary classification tasks using the DEAP dataset. This section supplements the results on different physiological signal subsets, such as high-/low Arousal and high/low Valence. These findings complement the regression task results presented in the main paper, collectively demonstrating our model's effectiveness and generalization capability across various emotion recognition tasks. Detailed performance metrics, including Accuracy and F1-Score, will be presented in the following tables 10.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Features",
      "text": "",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the architecture features a dual-",
      "page": 2
    },
    {
      "caption": "Figure 1: Overall Architecture of the Proposed Hierarchical MoE Framework. (a) The hierarchical MoE framework processes",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the V-A dimension (others are in the Ap-",
      "page": 5
    },
    {
      "caption": "Figure 2: Predicted trajectories compared with ground truth.",
      "page": 5
    },
    {
      "caption": "Figure 3: visually illustrates the",
      "page": 5
    },
    {
      "caption": "Figure 3: Performance comparison under varying modality",
      "page": 5
    },
    {
      "caption": "Figure 4: (a) Average soft routing weights in the modality",
      "page": 6
    },
    {
      "caption": "Figure 5: , the predicted trajectories for both Domi-",
      "page": 10
    },
    {
      "caption": "Figure 5: Predicted trajectories compared with ground truth.",
      "page": 10
    },
    {
      "caption": "Figure 6: and Figure 7, our model demon-",
      "page": 10
    },
    {
      "caption": "Figure 6: Performance comparison under varying modality",
      "page": 11
    },
    {
      "caption": "Figure 7: Performance comparison under varying modal-",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 7: For brevity, we refer to the four affective",
      "page": 4
    },
    {
      "caption": "Table 1: Comparison of DEAP and DREAMER datasets",
      "page": 4
    },
    {
      "caption": "Table 2: Overall average performance comparison of different fusion methods on emotion regression. (CCC, PCC↑MAE↓)",
      "page": 6
    },
    {
      "caption": "Table 3: CCC under different modality missing rates on the DEAP dataset (CCC ↑).",
      "page": 6
    },
    {
      "caption": "Table 4: , Hi-MoE consistently",
      "page": 6
    },
    {
      "caption": "Table 4: Performance on binary classification (V-A).",
      "page": 6
    },
    {
      "caption": "Table 5: LOSO on DEAP dataset. Metrics: CCC (↑).",
      "page": 6
    },
    {
      "caption": "Table 6: Ablation study on DEAP dataset: Overall average",
      "page": 7
    },
    {
      "caption": "Table 7: , we provide a detailed description and the specific",
      "page": 10
    },
    {
      "caption": "Table 8: and 9, our model consistently out-",
      "page": 10
    },
    {
      "caption": "Table 7: Comparison of DEAP, DREAMER, SEED, and AMIGOS datasets",
      "page": 11
    },
    {
      "caption": "Table 8: CCC under EEG+PPS missing rates on the DEAP dataset (CCC ↑).",
      "page": 11
    },
    {
      "caption": "Table 9: CCC under EEG+EOG+EMG+EDA missing rates on the DEAP dataset (CCC ↑).",
      "page": 11
    },
    {
      "caption": "Table 10: Performance on the EEG and Facial (V-A).",
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Event-related EEG oscillatory responses elicited by dynamic facial expression",
      "authors": [
        "T Aktürk",
        "T De Graaf",
        "Y Abra",
        "¸ ¸ahoglu Göktas",
        "S Özkan",
        "D Kula",
        "A Güntekin"
      ],
      "year": "2021",
      "venue": "BioMedical Engineering OnLine"
    },
    {
      "citation_id": "2",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "N Alswaidan",
        "M Menai",
        "N Alswaidan",
        "M Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "3",
      "title": "Survey on multimodal approaches to emotion recognition",
      "authors": [
        "A Arunagladys",
        "V Vetriselvi",
        "A Arunagladys",
        "V Vetriselvi"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "4",
      "title": "Multimodal Emotion Recognition with Deep Learning: Advancements, challenges, and future directions",
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "5",
      "title": "Prediction of Pilot's Reaction Time Based on EEG Signals",
      "authors": [
        "B Binias",
        "D Myszor",
        "H Palus",
        "K Cyran"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroinformatics"
    },
    {
      "citation_id": "6",
      "title": "Mixtures of experts for audio-visual learning",
      "authors": [
        "Y Cheng",
        "Y Li",
        "J He",
        "R Feng"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "StableMoE: Stable Routing Strategy for Mixture of Experts",
      "authors": [
        "D Dai",
        "L Dong",
        "S Ma",
        "B Zheng",
        "Z Sui",
        "B Chang",
        "F Wei"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "EMOE: Modality-Specific Enhanced Dynamic Emotion Experts",
      "authors": [
        "Y Fang",
        "W Huang",
        "G Wan",
        "K Su",
        "M Ye"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "9",
      "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "authors": [
        "W Fedus",
        "B Zoph",
        "N Shazeer"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "10",
      "title": "Enhanced Experts with Uncertainty-Aware Routing for Multimodal Sentiment Analysis",
      "authors": [
        "Z Gao",
        "D Hu",
        "X Jiang",
        "H Lu",
        "H Shen",
        "X Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia, MM '24"
    },
    {
      "citation_id": "11",
      "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering",
      "authors": [
        "T Huai",
        "J Zhou",
        "X Wu",
        "Q Chen",
        "Q Bai",
        "Z Zhou",
        "L He"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "13",
      "title": "Continuous Multimodal Emotion Prediction Based on Long Short Term Memory Recurrent Neural Network. AVEC '17, 11-18",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "Z Wen",
        "M Yang",
        "J Yi"
      ],
      "year": "2017",
      "venue": "Continuous Multimodal Emotion Prediction Based on Long Short Term Memory Recurrent Neural Network. AVEC '17, 11-18"
    },
    {
      "citation_id": "14",
      "title": "Adaptive mixtures of local experts",
      "authors": [
        "R Jacobs",
        "M Jordan",
        "S Nowlan",
        "G Hinton"
      ],
      "year": "1991",
      "venue": "Neural computation"
    },
    {
      "citation_id": "15",
      "title": "Hierarchical mixtures of experts and the EM algorithm",
      "authors": [
        "M Jordan",
        "R Jacobs"
      ],
      "year": "1994",
      "venue": "Neural computation"
    },
    {
      "citation_id": "16",
      "title": "Continuous Emotion Recognition for Long-Term Behavior Modeling through Recurrent Neural Networks",
      "authors": [
        "I Kansizoglou",
        "E Misirlis",
        "K Tsintotas",
        "A Gasteratos"
      ],
      "year": "2022",
      "venue": "Continuous Emotion Recognition for Long-Term Behavior Modeling through Recurrent Neural Networks"
    },
    {
      "citation_id": "17",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "18",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "20",
      "title": "Gshard: Scaling giant models with conditional computation and automatic sharding",
      "authors": [
        "D Lepikhin",
        "H Lee",
        "Y Xu",
        "D Chen",
        "O Firat",
        "Y Huang",
        "M Krikun",
        "N Shazeer",
        "Z Chen"
      ],
      "year": "2020",
      "venue": "Gshard: Scaling giant models with conditional computation and automatic sharding",
      "arxiv": "arXiv:2006.16668"
    },
    {
      "citation_id": "21",
      "title": "Incongruity-aware multimodal physiology signals fusion for emotion recognition",
      "authors": [
        "J Li",
        "N Chen",
        "H Zhu",
        "G Li",
        "Z Xu",
        "D Chen"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "22",
      "title": "Tacoformer: Token-channel compounded cross attention for multimodal emotion recognition",
      "authors": [
        "Q Li",
        "Y Liu",
        "F Yan",
        "Q Zhang",
        "C Liu"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control",
      "arxiv": "arXiv:2306.13592"
    },
    {
      "citation_id": "23",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "F Lv",
        "X Chen",
        "Y Huang",
        "L Duan",
        "G Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "24",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "25",
      "title": "Representational structure of fMRI/EEG responses to dynamic facial expressions",
      "authors": [
        "I Muukkonen",
        "V Salmela"
      ],
      "year": "2022",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "26",
      "title": "A Hybrid Latent Space Data Fusion Method for Multimodal Emotion Recognition",
      "authors": [
        "S Nemati",
        "R Rohani",
        "M Basiri",
        "M Abdar",
        "N Yen",
        "V Makarenkov"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Towards an intelligent framework for multimodal affective data analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "A Hussain",
        "G.-B Huang"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "28",
      "title": "Scaling vision with sparse mixture of experts",
      "authors": [
        "C Riquelme",
        "J Puigcerver",
        "B Mustafa",
        "M Neumann",
        "R Jenatton",
        "A Susano Pinto",
        "D Keysers",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Recipes for Building an Open-Domain Chatbot",
      "authors": [
        "S Roller",
        "E Dinan",
        "N Goyal",
        "D Ju",
        "M Williamson",
        "Y Liu",
        "J Xu",
        "M Ott",
        "E Smith",
        "Y.-L Boureau",
        "J Weston"
      ],
      "year": "2021",
      "venue": "Recipes for Building an Open-Domain Chatbot"
    },
    {
      "citation_id": "30",
      "title": "Multiple Instance Learning for Emotion Recognition Using Physiological Signals",
      "authors": [
        "L Romeo",
        "A Cavallo",
        "L Pepa",
        "N Berthouze",
        "M Pontil"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "authors": [
        "N Shazeer",
        "A Mirhoseini",
        "K Maziarz",
        "A Davis",
        "Q Le",
        "G Hinton",
        "J Dean"
      ],
      "year": "2017",
      "venue": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "arxiv": "arXiv:1701.06538"
    },
    {
      "citation_id": "32",
      "title": "An Improved Multimodal Dimension Emotion Recognition Based on Different Fusion Methods",
      "authors": [
        "H Su",
        "B Liu",
        "J Tao",
        "Y Dong",
        "J Huang",
        "Z Lian",
        "L Song"
      ],
      "year": "2020",
      "venue": "An Improved Multimodal Dimension Emotion Recognition Based on Different Fusion Methods"
    },
    {
      "citation_id": "33",
      "title": "Multimodal Continuous Dimensional Emotion Recognition Using Recurrent Neural Network and Self-Attention Mechanism. MuSe'20",
      "authors": [
        "L Sun",
        "Z Lian",
        "J Tao",
        "B Liu",
        "M Niu"
      ],
      "year": "2020",
      "venue": "Multimodal Continuous Dimensional Emotion Recognition Using Recurrent Neural Network and Self-Attention Mechanism. MuSe'20"
    },
    {
      "citation_id": "34",
      "title": "Multimodal Emotion Recognition and Sentiment Analysis via Attention Enhanced Recurrent Model. MuSe '21",
      "authors": [
        "L Sun",
        "M Xu",
        "Z Lian",
        "B Liu",
        "J Tao",
        "M Wang",
        "Y Cheng"
      ],
      "year": "2021",
      "venue": "Multimodal Emotion Recognition and Sentiment Analysis via Attention Enhanced Recurrent Model. MuSe '21"
    },
    {
      "citation_id": "35",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "36",
      "title": "Multimodal Emotion Recognition Using Deep Neural Networks",
      "authors": [
        "H Tang",
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "Multimodal Emotion Recognition Using Deep Neural Networks"
    },
    {
      "citation_id": "37",
      "title": "Technical Research and Application Analysis of Multimodal Emotion Recognition",
      "authors": [
        "G Wang"
      ],
      "year": "2024",
      "venue": "13th International Conference of Information and Communication Technology"
    },
    {
      "citation_id": "38",
      "title": "Recognizing Emotions in Video Using Multimodal DNN Feature Fusion",
      "authors": [
        "J Williams",
        "S Kleinegesse",
        "R Comanescu",
        "O Radu",
        "A Zadeh",
        "P Liang",
        "L.-P Morency",
        "S Poria",
        "E Cambria",
        "S Scherer"
      ],
      "year": "2018",
      "venue": "Proceedings of Grand Challenge and Workshop on Human Multimodal Language"
    },
    {
      "citation_id": "39",
      "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-Rank Experts",
      "authors": [
        "J Wu",
        "X Hu",
        "Y Wang",
        "B Pang",
        "R Soricut"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Leveraging Knowledge of Modality Experts for Incomplete Multimodal Learning",
      "authors": [
        "W Xu",
        "H Jiang",
        "X Liang",
        "X Liang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Taskexpert: Dynamically assembling multi-task representations with memorial mixture-ofexperts",
      "authors": [
        "H Ye",
        "D Xu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "42",
      "title": "Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Expression EEG multimodal emotion recognition method based on the bidirectional LSTM and attention mechanism",
      "authors": [
        "Y Zhao",
        "D Chen",
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "Computational and Mathematical Methods in Medicine"
    },
    {
      "citation_id": "45",
      "title": "Mixtureof-experts with expert choice routing",
      "authors": [
        "Y Zhou",
        "T Lei",
        "H Liu",
        "N Du",
        "Y Huang",
        "V Zhao",
        "A Dai",
        "Z Chen",
        "Q Le",
        "J Laudon"
      ],
      "year": "2022",
      "venue": "Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22",
      "arxiv": "arXiv:2205.03169"
    }
  ]
}