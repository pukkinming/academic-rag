{
  "paper_id": "2005.03117v1",
  "title": "Joint Multi-Dimensional Model For Global And Time-Series Annotations",
  "published": "2020-05-06T20:08:46Z",
  "authors": [
    "Anil Ramakrishna",
    "Rahul Gupta",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Annotation fusion",
    "Emotion annotations",
    "Multi-dimensional annotations",
    "Time series annotation modeling",
    "Expectation Maximization",
    "Factor Analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Crowdsourcing is a popular approach to collect annotations for unlabeled data instances. It involves collecting a large number of annotations from several, often naive untrained annotators for each data instance which are then combined to estimate the ground truth. Further, annotations for constructs such as affect are often multi-dimensional with annotators rating multiple dimensions, such as valence and arousal, for each instance. Most annotation fusion schemes however ignore this aspect and model each dimension separately. In this work we address this by proposing a generative model for multi-dimensional annotation fusion, which models the dimensions jointly leading to more accurate ground truth estimates. The model we propose is applicable to both global and time series annotation fusion problems and treats the ground truth as a latent variable distorted by the annotators. The model parameters are estimated using the Expectation-Maximization algorithm and we evaluate its performance using synthetic data and real emotion corpora as well as on an artificial task with human annotations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Crowdsourcing is a popular tool used in collecting human judgments on subjective constructs such as emotion. Typical examples include annotations of images and video clips with categorical emotions or with continuous affective dimensions such as valence or arousal. Online platforms such as Amazon Mechanical Turk 1 (MTurk) and Crowdflower 2 have risen in popularity owing to their inexpensive annotation costs and their ability to scale efficiently.\n\nCrowdsourcing is also a popular approach in collecting labels for training supervised machine learning algorithms. Such labels are typically obtained from domain experts, which can be slow and expensive. For example, in the medical domain, it is often expensive to collect diagnosis information given laboratory tests since this requires judgments from trained professionals. On the other hand, unlabeled patient data may be easily available. Crowdsourcing has been particularly successful in such settings with easy availability of unlabeled data instances since we can collect a large number of annotations from untrained and inexpensive workers over the Internet, which when combined together may be comparable or even better than expert annotations  [1] .\n\nA typical crowdsourcing setting involves collecting annotations from a large number of workers; hence there is a need to robustly combine them to estimate the ground truth. The most common approach for this is to take simple averages for continuous annotations or perform majority voting for categorical annotations. However, this assumes uniform competency across all the workers which is not always guaranteed or justified. Several alternative approaches have been proposed to address this challenge, each assuming 1. www.mturk.com 2. www.crowdflower.com a specific function modeling the annotators' behavior. In practice, it is common to collect annotations on multiple questions for each data instance in order to reduce costs, the annotators' mental load or even to improve annotation accuracy. For example, if we're annotating valence and arousal for a given data instance (such as a single image or video segment), collecting annotations on both these dimensions in one session per instance may be preferred over collecting valence annotations for all instances followed by arousal.\n\nSuch a joint annotation task may entail task specific or annotator specific dependencies between the annotated dimensions. In the aforementioned example, task specific dependencies may occur due to inherent correlations between the valence and arousal dimensions depending on the experimental setup. Annotator specific dependencies may occur due to a given annotator's (possibly incorrect or incomplete) understanding of the annotation dimensions. Hence it is of relevance to model the dimensions jointly. However, most state of the art models in annotation fusion combine the annotations by treating the different dimensions independently.\n\nJoint modeling of the annotation dimensions may result in more accurate estimates of the ground truth as well as in giving a better picture of the annotators' behavior. In this work, we address this goal by proposing a multidimensional model which makes use of any potential relationships between the annotation dimensions while combining them. The model we propose is applicable to both the global annotation setting (such as while collecting emotion annotations on a picture, judgment about the overall tone of a conversation, etc.) as well as time series annotations (for example, time continuous annotations of audio/video clips on dimensions such as engagement or affect). Our model treats the hidden ground truth as latent variables and estimates them jointly with the annotator parameters using the Expectation Maximization (EM) algorithm  [2] . We evaluate the model in both settings with both synthetic and real emotion corpora. We also create an artificial annotation task with controlled ground truth which is used in the model evaluation for both settings.\n\nThe main contributions of this work are as follows:\n\n1) We propose a unified model to capture relationships between annotation dimensions. For ease of exposition we focus on the linear case in this paper.\n\n2) The linear model we propose results in an annotator specific matrix which captures this annotator level relationship between the annotation dimensions. 3) We create a novel multi-dimensional annotation task with controlled ground truth and use it to evaluate both the global and time series annotation settings of the model.\n\nThe rest of the paper is organized as follows. In Section 2, we review related work and motivate the problem in Section 3. In Section 4, we describe the proposed model and provide equations for parameter estimation using EM algorithm (derivations are deferred to the appendix). We evaluate the model in Section 5 and provide conclusions in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Several authors, most notably  [1] , assert the benefits of aggregating opinions from many people which is often believed to be better than those from a small number of experts, under certain conditions. Often referred to as the wisdom of crowds, this approach has been remarkably popular in recent times, specially in fields such as psychology and behavioral sciences where a ground truth may not be easily accessible or may not exist. This popularity can be largely attributed to online crowdsourcing platforms such as Mturk that connect researchers with low cost workers from around the globe. Along with cost, scalability is another major appeal with such tools leading to their frequent use in machine learning, leveraging large scale annotation of data instances such as images  [3] , audio/video clips  [4]  and text snippets  [5] .\n\nFigure  1  shows a common setting in the crowdsourcing paradigm. For each data instance m, annotator k provides a noisy annotation a m,d k which depends on the ground truth a m,d * where d is the dimension being annotated. Since we collect several annotations for each m, we need to aggregate them to estimate the unknown ground truth. The most common technique used in this aggregation is to take the average value in case of numeric annotations or perform majority voting in the case of categorical annotations as shown in Equation  1 .\n\nwhere, 1{} is the indicator function.\n\nWhile simple and easy to implement, this approach assumes consistent reliability among the different annotators which seems unreasonable, especially in online platforms such as Mturk. To address this, several approaches have been suggested that account for annotator reliability in estimating the ground truth.\n\nEarly efforts to capture reliability in annotation modeling  [6] ,  [7]  assumed specific structure to the functions modeled by each annotator. Given a set of annotations a m,d k along with the corresponding function parameters, the ground truth is estimated using the Maximum A Posteriori (MAP) estimator.\n\nwhere p(a m,d * ) is the prior probability of ground truth. In  [6] , the categorical ground truth label a m,d * = i is modified probabilistically by annotator k using a stochastic matrix Π k as shown in Equation 3 in which each row is a multinomial conditional distribution given the ground truth.\n\nGiven annotations from K different annotators, their parameters Π k and prior distribution of labels p j = P (a m,d * = j), the ground truth is estimated using MAP estimation as before.\n\nThe above expression makes a conditional independence assumption for annotations given the ground truth label. Since we do not typically have the annotator parameters Π k , these are estimated using the EM algorithm.\n\nFigure  2  shows an extension of the model in Figure  1  in which we learn a predictor (classifier/regression model) for the ground truth jointly with annotator parameters. Such a predictor may be used to estimate the ground truth for new unlabeled data instances. This strategy of jointly modeling the annotator functions as well as the ground truth predictor has been shown to have better performance when compared to predictors trained independently using the estimated ground truth  [8] . The ground truth estimate in this model is given by\n\nRecently, several additional extensions have been proposed to the model in Figure  2 ; For example, in  [9] , the authors assume varying regions of annotator expertise in the data feature space and account for this using different probabilities for label confusion for each region. The authors show that this leads to a better estimation of annotator reliability and ground truth.\n\nThe models described so far have been designed for annotation tasks in which the task is to rate some global property of the data. For example, in image based emotion annotation, the task may be to provide annotations on affective dimensions such as valence and arousal conveyed by each image. However, human interactions often involve variations of these dimensions over time  [10]  which are captured using time series annotations from audio/video clips. Various tools have been developed to collect such annotations, including Anvil  [11] , Feeltrace  [12] , EMuJoy  [13] , Gtrace  [14]  and DARMA  [15]  (for a review of available tools and their properties, see  [16]  and  [15] ). In fusing such time series annotations, the previously mentioned models are applicable only if annotations from each frame are treated independently. However, this entails several unrealistic assumptions such as independence between frames, zero lag in the annotators and synchronized response in the annotators to the underlying stimulus.\n\nSeveral works have been proposed to capture the underlying reaction lag in the annotators.  [17]  proposed a generalization of Probabilistic Canonical Correlation Analysis (PCCA)  [18]  named Dynamic PCCA which captures temporal dependencies of the shared ground truth space in a generative setting, and incorporated a latent time warping process to implicitly handle the reaction lags in annotators. They have further proposed a supervised extension of their model which jointly learns a predictor function for the latent ground truth signal similar to  [8] .  [19]  address the reaction lag by explicitly finding the time shift that maximizes the mutual information between expressive behaviors and their annotations.  [20]  generalize the work of  [19]  by using a linear time invariant (LTI) filter which can also handle any bias or scaling the annotators may introduce.\n\nMore recent works in annotation fusion include  [21]  in which the authors propose a variant of the model in Figure  1  with various annotator functions to capture four specific types of annotator behavior.  [22]  describes a mechanism named approval voting that allows annotators to provide multiple answers instead of one for instances where they are not confident.  [23]  uses repeated sampling for opinions from annotators over the same data instances to increase reliability in annotations.\n\nMost of the models described above focus on combining annotations on each dimension separately. However, the annotation dimensions are often related. For example, many studies in emotion literature have reported interrelationships between discrete emotion categories  [24] ,  [25] . The circumplex model  [26] , which attempts to capture these relationships by modeling the emotions as points on a two dimensional space, has also been noted to exhibit vshaped patterns in the joint distribution of valence and arousal  [27] . In addition, in most practical applications, the annotation tasks themselves are multi-dimensional. For example, while collecting ratings on affective dimensions it is routine to collect annotations on valence, arousal and dominance together. Further, there may be dependencies between the internal definitions the annotators hold for the annotation dimensions; for example, while annotating emotional dimensions, a particular annotator may associate certain valence values with only a certain range of arousal. Hence it may be beneficial to model the different dimensions jointly while performing annotation fusion. However, research in this direction has been limited.  [28]  proposed a model which assumes joint Gaussian noise between the annotation dimensions, but their model fails to capture structural dependencies described above between the annotation and ground truth dimensions. The model proposed in  [17]  can indeed be generalized to combine the different annotation dimensions together but they do not evaluate with joint annotated dimensions from a real dataset as that is not the focus of their work.  [29]  jointly model continuous annotations on valence and arousal using personalized basis spline functions, on which functional PCA is applied to identify the dominant spline functions. Using this model, they estimate the ground truth for each data instance using a heuristic algorithm, but their model does not include a jointly trained ground truth predictor. It is therefore of relevance to model multi-dimensional annotation fusion as part of the unified annotator function and predictor modeling paradigm.\n\nIn this work, we propose a joint multi-dimensional model to address many of the gaps mentioned above. Our model captures annotator specific linear relationships between different annotation dimensions, and is an extension of the Factor Analysis model  [30] . It incorporates an annotator specific transformation matrix parameter F k , which explicitly captures the relationship between the annotation dimensions and enables clear interpretations of the estimated relationships; the matrix F k is jointly estimated with a predictor for the ground truth signal. We further provide generalizations of our model to both global and time series annotation settings. We begin with a motivation followed by a detailed description of the model and its parameter estimation in the next sections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Motivation",
      "text": "To examine the relationships between the annotation dimensions, we created a plot of absolute values of Pearson's correlation between annotation dimensions from four commonly studied emotional corpora in Figure  3 : IEMOCAP  [31] , SEMAINE  [32] , RECOLA  [33]  and the movie emotion corpus from  [27] . Each of these corpora include annotations over affective dimensions such as valence, arousal, dominance and power. For the IEMOCAP corpus, we used global annotations while the others include time series annotations of the affective dimensions from videos. In each case, the correlations were computed from concatenated annotation values between all the dimensions.\n\nAs is evident, in almost all cases, the annotation dimensions exhibit non-zero correlations. We attribute the inconsistent correlations between the dimensions across corpora to varying underlying affective narratives as well as differences in perceptions and biases introduced by individual annotators themselves (see Section A.1). The non-zero correlations highlight the benefit of modeling the annotation dimensions jointly. The model we propose is aimed at addressing this. We explain the model in detail in the next section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Joint Multi-Dimensional Annotation Model",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Setup",
      "text": "The proposed model is shown in Figure  4 . Each data instance m has a feature vector x m and an associated multidimensional ground truth a m * , which is defined as follows,\n\nWe assume that from a pool of K annotators, a subset operates on each data instance and provides their annotation\n\nwhere index k corresponds to the k th annotator; F k is an annotator specific matrix that defines his/her linear weights for each output dimension; m and η k are noise terms defined individually in the next sections along with the functions f and g. In the global annotation setting, both a m * and a m k ∈ IR D where D is the number of items being annotated; for the time series setting a m * and a m k ∈ IR T×D , where T is the total duration of the data instance (audio/video signal). In all subsequent definitions, we use uppercase letters M, K, T, D to denote various counts and lowercase letters m, k, t, d to denote the corresponding index variables.\n\nWe make the following assumptions in our model.\n\nA1 Annotations are independent for different data instances. A2 The annotations for a given data instance are independent of each other given the ground truth. A3 The model ground truths for different annotation dimensions are assumed to be conditionally independent of each other given the features x m .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Global Annotation Model",
      "text": "In this setting, the ground truth and annotations are d dimensional vectors for each data instance. We define the ground truth a m * and annotations a m k as follows.\n\nwhere,\n\nis the annotator specific weight matrix. Each annotation dimension value a m,d k for annotator k is defined as a weighted average of the ground truth vector a m * with weights given by the vector F k (d, :).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Parameter Estimation",
      "text": "The model parameters Φ = {F k , Θ, σ 2 , τ 2 k } are estimated using Maximum Likelihood Estimation (MLE) in which they are chosen to be the values that maximize the likelihood function L.\n\nOptimizing Equation  10 directly is intractable because of the presence of the integral within the log term, hence we use the EM algorithm. Note that the model we propose assumes that only some random subset of all available annotators provide annotations on a given data instance, as shown in Figure  4 . However, for ease of exposition, we overload the variable K and use it here to indicate the number of annotators that attempt to judge the given data instance m.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Em Algorithm",
      "text": "The Expectation Maximization (EM) algorithm to estimate the model parameters is shown below. It is an iterative algorithm in which the E and M-steps are executed repeatedly until an exit condition is encountered. Complete derivation of the model can be found in Appendix B. Initialization We initialize by assigning the expected values and covariance matrices for the m ground truth vectors a m * to their sample estimates (i.e. sample mean and sample covariance) from the corresponding annotations. We then estimate the parameters as described in the maximization step using these estimates.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E-Step",
      "text": "In this step we take expectation of the log likelihood function with respect to p(a m * |a m 1 . . . a m K ) and the resulting objective is maximized with respect to the model parameters in the M-step. Equations to compute the expected value and covariance matrices for the latent variable a m * in the E-step are listed below.\n\nNote the similarity of the update equation for Θ with the familiar normal equations. We are using the soft estimate of a m * to find the expression for Θ in each iteration. Here, X is the feature matrix for all data instances; it includes individual feature vectors x m in its rows. Θ and F k are parameters from the previous iteration.\n\nTermination We run the algorithm until convergence, and stop model training when the change in log-likelihood falls below a threshold of 0.001%.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Time Series Annotation Model",
      "text": "In this setting, the ground truth and the annotations are matrices with T rows (time) and D columns (annotation dimensions). The ground truth matrix a m * is defined as follows.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vec(A",
      "text": "where a m * ∈ IR T×D , X m ∈ IR T×P and Θ ∈ IR P×D ; T represents the time dimension and is the length of the time series. X m is the feature matrix where each row corresponds to features extracted from the data instance for one particular time stamp. vec(.) is the vectorization operation which flattens the input matrix in column first order to a vector. m ∼ N (0, σ 2 I) ∈ IR TD is the additive noise vector with σ ∈ IR.\n\nIn  [20] , the authors propose a linear model where the annotation function g(a m * ; F k ) is a causal linear time invariant (LTI) filter of fixed width. The advantage of using an LTI filter is that it can capture scaling and time-delay biases introduced by the annotators.\n\nThe filter width W is chosen such that W T , where T is the number of time stamps for which we have the annotations. The annotation function for dimension d can be viewed as the left multiplication of a filter matrix B d k ∈ IR T×T as shown in Equation  12 .\n\nWe extend this model in our work to combine information from all of the annotation dimensions. Specifically, the ground truth is left multiplied by D horizontally concatenated filter matrices, each ∈ IR T×T corresponding to a different dimension as shown below.\n\nwhere,",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Parameter Estimation",
      "text": "Estimating the model parameters similar to the global model requires computing the expectations over a vector of size T D. Since T is the number of time stamps in the task and can be arbitrarily long, this may not be feasible in all tasks. For example, in the movie emotions corpus  [27] , annotations are computed at a rate of 25 frames per second with each file of duration ∼30 minutes or of ∼45k annotation frames. To avoid this we use a variant of EM named Hard EM in which instead of taking expectations over the entire conditional distribution of a m * we find its mode. This variant has been shown to be comparable in performance to the classic EM (Soft EM) despite being significantly faster and simple  [34] . This approach is similar to the parameter estimation strategy devised by  [20]  in their time series annotation model.\n\nThe likelihood function is similar to the global model in Equation 10 as shown below.\n\nHowever the integral here is with respect to the flattened vector vec(a m * ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Em Algorithm",
      "text": "The EM algorithm for the time series annotation model is listed below. Complete derivations can be found in Appendix C. Initialization Unlike the global annotation model, we initialize a m * randomly since we observed better performance when compared to initializing it with the annotation means. Given this a m * , the model parameters are estimated as described in the maximization step below.\n\nE-step In this step we assign a m * to the mode of the conditional distribution q(a m * ) = p(a m * |a m 1 , . . . , a m K ). Since this distribution is normal (see appendix B) finding the mode is equivalent to minimizing the following expression.\n\nM-step Given the estimate for a m * from the E-step, we substitute it in the likelihood function and maximize with respect to the parameters in the M-step. The estimates for the different parameters are shown below.\n\nM k is the number of files annotated by user k; A is a matrix obtained by reshaping vec(a m * ) as described in subsection C.1.2.\n\nTermination We run the algorithm until convergence, and stop model training when the change in log-likelihood falls below a threshold of 0.5%.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments & Results",
      "text": "We evaluate the models described above on three different types of data: synthetic data, an artificial task with human annotations, and finally with real data. We describe these below. We compare our joint models with their independent counterparts as baselines, in which each annotation dimension is modeled separately. This allows us to highlight the benefits of moving to a multi-dimensional annotation fusion scheme with everything else kept constant. Update equations for the independent model can be obtained by running the models described above for each dimension separately with D = 1. Note that the independent model is similar in the global setting to the regression model proposed in  [8]  (with ground truth scaled by the singleton f d k ). In the time series setting it is identical to the model proposed by  [20] .\n\nThe models are evaluated by comparing the estimated a m * with the actual ground truth. We report model performance using two metrics: the Concordance correlation coefficient (ρ c )  [35]  and the Pearson's correlation coefficient (ρ). ρ c measures any departures from the concordance line (line passing through the origin at 45 • angle). Hence it is sensitive to rotations or rescaling in the predicted ground truth. Given two samples x and y, the sample concordance coefficient ρc is defined as shown below. We also report results in Pearson's correlation to highlight the accuracy of the models in the presence of rotations.\n\nAs noted before, the models proposed in this paper are closely related to the Factor Analysis model, which is vulnerable to issues of unidentifiability  [36] , due to the matrix factorization. Different types of unidentifiability have been studied in literature, such as factor rotation, scaling and label switching. In our experiments, we handle label switching through manual judgment (by reassigning the estimated ground truth between dimensions if necessary) as is common in psychology  [37] , but defer the task of choosing an appropriate prior on the rotation matrix F k to address other unidentifiabilities for future work.\n\nWe report aggregate test set results using C-fold cross validation. To address overfitting, within each fold, we evaluate the parameters obtained after each iteration of the EM algorithm by estimating the ground truth on a disjoint validation set, and pick those with the highest performance in concordance correlation ρ c as the parameter estimates of the model. We then estimate the performance of this parameter set in predicting the ground truth from a separate held out test set for that fold. Finally, we also report statistically significant differences between the joint and independent models at 5% false-positive rate (α = 0.05) in all our experiments.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Global Annotation Model",
      "text": "The global annotation model uses the EM algorithm described in Section 4.2.2 to estimate the ground truth for discrete annotations. We evaluate the model in three different settings described below. Statistical significance tests were run by computing bootstrap confidence intervals  [38]  on the differences in model performances across the C-folds. To establish the statistical significance, we ran the joint and independent models to obtain C test set model predictions from C folds. Given these, we ran 1000 bootstrap iterations in which the test set predictions were sampled with replacement, from which ρ and ρ c were estimated for each dimension. We conclude significance if the evaluation metric being examined was higher in at least 95% of the bootstrap runs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Synthetic Data",
      "text": "We created synthetic data according to the model described in Section 4.2 with random features X ∈ IR 500 for 100 ). 10 artificial annotators, each with unique random F k matrices were used to produce annotations for all the data instances. Elements of the feature matrices were sampled from the standard normal distribution, while the elements of F k matrices were sampled from U(0, 1). Elements of ground truth a m * were sampled from U(-1, 1) and θ was estimated from a m * and X. Since its off diagonal elements are non-zero, our choice of F k represents tasks in which the annotation dimensions are related to each other.\n\nFigure  5  shows the performance of joint and independent models in predicting the ground truth a m * . For both dimensions, the proposed joint model predicts the a m * with considerably higher accuracy as shown by the higher correlations, highlighting the advantages of modeling the annotation dimensions jointly when they are expected to be related to each other.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Artificial Data",
      "text": "Since crowdsourcing experiments typically involve collecting subjective annotations, they seldom have well defined ground truth. As a result, most annotation models are evaluated on expert annotations collected by specially trained users. For example, while collecting annotations on medical data, labels estimated by fusing annotations from naive users may be evaluated against those provided by experts such as doctors. However, this poses a circular problem since the expert annotations themselves may be subjective and combining them to estimate the ground truth is not straightforward. To address this, we created an artificial task with controlled ground truth on which we collect annotations from multiple annotators and evaluate the fused annotation values with the known ground truth values, similar to  [39] . In our task, the annotators were asked to provide their best estimates on perceived saturation and brightness values for monochromatic images. The relationship between perceived saturation and brightness is well known as the Helmholtz-Kohlrausch effect  [40] , according to which, increasing the saturation of an image leads to an increase in the perceived brightness, even if the actual brightness was constant.\n\nIn our experiments, we collected annotations on images from two regimes: one with fixed saturation and varying brightness, and vice versa. This approach was chosen since it would allow us to evaluate the impact of change in either brightness or saturation while the other was held constant. The color of the images were chosen randomly (and independent of the image's saturation and brightness) between green and blue. Annotations were collected on Mturk and the annotators were asked to familiarize themselves with saturation and brightness using an online interactive tool before providing their ratings. In both experiments, a reference image with fixed brightness and saturation was inserted after every ten annotation images to prevent any bias in the annotators. The reference images were hidden from the annotators and appeared as regular annotation images. For parameter estimation, RGB values were chosen as the features for each image.\n\nWe used the joint model to estimate the ground truth for the two regimes separately since we expect the relationship between saturation and brightness to be dissimilar in the two cases. From each experiment, predicted values of the underlying dimension being varied was compared with the actual a m * values. For example, in the experiment with varying saturation and fixed brightness, the joint model was run on full annotations, but only the estimated values of saturation were compared with ground truth saturation. For the independent model, we use annotation values of the underlying dimension being varied from each regime, and compare the estimated values with ground truth.\n\nFigure  6  shows the performance of the joint and independent models for this experiment. The joint model leads to better estimates of saturation when compared to the independent model by making use of the annotations on brightness. This agrees with the Helmholtz-Kohlrausch phenomenon described above, since the annotators can perceive the changing saturation as a change in brightness, leading to correlated annotations for the two dimensions. On the other hand, the independent model leads to better estimates of brightness, which seems to have no effect on perceived saturation annotations. This experiment highlights the benefits of jointly modeling annotations in cases where the annotation dimensions may be correlated or dependent on one another.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Real Data",
      "text": "Our final experiment for the global model was on the task of annotating news headlines in which the annotators provide numeric ratings for various emotions. This dataset was first described in the 2007 SemEval task on affective text  [41] . Numeric ratings from the original task were labeled by trained annotators and we treat these as expert annotations. We use Mturk annotations from  [5]  as the actual input to our model. Sentence level annotations are provided on seven emotions (D=7): anger, disgust, fear, joy, sadness, surprise and valence (positive/negative polarity). We use sentence level embeddings computed using the pre-trained sentence embedding model sent2vec 3  [42]  as feature vectors x for the model.\n\nFigure  7  shows the performance of the joint and independent models on this task. The joint model shows better performance in predicting the reference emotion labels for anger, disgust, fear, joy and sadness, but performs worse than the independent model in predicting surprise and valence.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Time Series Annotation Model",
      "text": "In this setting, the annotations are collected on data with a temporal dimension, such as time series data, video or audio signals. Similar to the global model, we evaluate this model in 3 settings: synthetic, artificial and on real data. The evaluation metrics ρ c and ρ are computed over estimated and actual ground truth vectors a m * by concatenating the data instances into a single vector. The time series models have the window size W as an additional hyperparameter, which is selected using a validation set. In each fold of the dataset, we train model parameters for different window sizes from the set {5, 10, 20, 50}, and pick W and related parameters with the highest concordance correlation ρ c on the validation set. These are then evaluated on a disjoint test set, and we repeat the process for each fold. In each experiment, the parameters were initialized randomly, and the process was repeated 20 times at different random initializations, selecting the best starting point using the validation set. To identify significant differences, we compute the test set performance of the two models for each fold, and run the paired t-test between the C sized samples of ρ and ρ c corresponding to the joint and independent models. We do not bootstrap confidence intervals due to smaller test set sizes.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Synthetic Data",
      "text": "The synthetic dataset was created using the model described in Section 4.3. Elements of the feature matrix were sampled from the standard normal distribution while elements of F k and ground truth were sampled from U(0, 1). In this setting each data instance includes T feature vectors, one for each time stamp. The time dependent feature matrices were created using a random walk model without drift but with lag to mimic a real world task. In other words, while creating the P dimensional time series, the features vectors were held fixed for a time period arbitrarily chosen to be between 2 to 4 time stamps. This was done because in most tasks the underlying dimension (such as emotion) is expected to remain constant at least for a few seconds.\n\nIn addition, the transition between changes in the feature vectors were linear and not abrupt. In our experiments, we chose P = 500, T = 350, D = 2, M = 18 and the number of annotators K = 6.\n\nFigure  8  shows the aggregate results across C-folds (C = 5) for the joint and independent models in the 3 settings. In the synthetic dataset, the joint model achieves higher values for Pearson's correlation ρ for both the dimensions and higher value for ρ c for dimension 1. For dimension 2 however, the independent model achieves better ρ c .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Artificial Data",
      "text": "We collected annotations on videos with the artificial task of identifying saturation and brightness, described in the previous section. The videos consisted of monochromatic images with the underlying saturation and brightness varied independent of each other. The dimensions were created using a random walk model with lag as described in Section 5.2.1. The annotations were collected in house using an annotation system developed using the Robot Operating System  [43] . 10 graduate students gave their ratings on the two dimensions. Each dimension was annotated independently using a mouse controlled slider. For parameter estimation, the feature vectors for each time stamp were RGB values.\n\nAs seen in Figure  8 , both models achieve similar performance in predicting the ground truth for saturation and brightness in terms of ρ, as well as in predicting saturation in terms of ρ c . The independent model achieves slightly better performance in predicting brightness in terms of concordance correlation (though not statistically significant); however, their performance in terms of ρ suggests that the joint model output differs only in terms of a linear scaling. The joint model appears to be at par with the independent model for the most part, suggesting that the transformation matrix F k connecting the two dimensions for each annotator, is unable to accurately capture the dependencies between the dimensions, likely due to the fact that, unlike the global annotation model, the underlying brightness and saturation were varied simultaneously and independent of each other (leading to non-linear dependencies between them), and that we limit F k to only capture linear relationships.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Real Data",
      "text": "We finally evaluate our model on a real world task with time series annotations. We chose the task of predicting the affective dimensions of valence and arousal from movie clips, first described in  [27] . The associated corpus includes time series annotations of valence and arousal on contiguous 30 minute video segments from 12 Academy Award winning movies. This task was chosen because the data set includes both expert annotations as well as annotations from naive users. We treat the expert annotations as reference and evaluate the estimated dimensions against them; however, we note that the expert labels were provided by just one annotator, which may itself be noisy.\n\nFor each movie clip, 6 annotators provide annotations on their perceived valence and arousal using the Feeltrace  [12]  annotation tool. The features used in our parameter estimation include combined audio and video features extracted separately. The audio features were estimated using the emotion recognition baseline features from Opensmile  [44]  at 25 fps (same frame rate as the video clips) and aggregated at a window size of 5 seconds using the following statistical functionals: mean, max, min, std, range, kurtosis, skewness and inter-quartile range. The video features were extracted using OpenCV  [45]  and included frame level luminance, intensity, Hue-Saturation-Value (HSV) color histograms and optical flow  [46] , which were also aggregated to 5 seconds using simple averaging. The combined features were of size P = 1225 for each frame.\n\nFigure  8  shows the performance of the two models in estimating the affective dimensions for the dataset. The joint model seems to considerably outperform the independent model while estimating arousal while the independent models seem to produce better estimates of valence from the annotations. The independent model seems to perform poorly in arousal prediction, but the joint model shows a balanced performance, with the joint modeling constraint likely acting as a regularizer.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Effect Of Dependency Among Dimensions",
      "text": "To evaluate the impact of the magnitude of dependency between the annotation dimensions on the performance of the models, we created a set of synthetic annotations for the global model similar to Section 5.1.1. We created 10 synthetic datasets, each with constant F k matrices across all annotators. The principal diagonal elements were fixed to 1 while the off diagonal elements were increased between 0.1 to 1 with a step size of 0.1. Similar to the previous setting, we created 100 annotators, each operating on 10 files. Note that despite the annotators having identical F k matrices, their annotations on a given file were different because of the noise term η k in Equation  7 .\n\nFigure  9  shows the 5-fold cross validated performance of the joint and independent models on this task. As seen in the figure, the joint model consistently outperforms the independent model in both metrics. Both the models start with similar performance when the off diagonal elements are close to zero since this implies no dependency between 0.1 0.4 0. Fig.  9 : Effect of varying dependency between annotation dimensions for the synthetic model the annotation dimensions, and the performance of both models continues to degrade as the off diagonal elements increase. However, the joint model is able to make better predictions of the ground truth by making use of the dependency between the dimensions, highlighting the benefits of modeling the annotation dimensions jointly. Visualizations for averaged estimates of the F k matrices from this experiment can be found in Section A.2.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "We presented a model to combine multi-dimensional annotations from crowdsourcing platforms such as Mturk. The model assumes the ground truth to be latent and distorted by the annotators. The latent ground truth and the model parameters are estimated using the EM algorithm. EM updates are derived for both global and time series annotation settings. We evaluate the model on synthetic and real data. We also propose an artificial task with controlled ground truth and evaluate the model. Weaknesses of the model include vulnerability to unidentifiability issues like most variants of factor analysis  [36] . Typical strategies to address this issue involve adapting a suitable prior constraint on the factor matrix. For example, in PCA, the factors are ordered such that they are orthogonal to each other and arranged in decreasing order of variance.\n\nIn our experiments, the model was found to be vulnerable to unidenfiability due to label switching, which was addressed through manual judgements. We defer the task of choosing an appropriate prior constraint on F k for future work.\n\nFuture work includes generalizing the model with Bayesian extensions, in which case the parameters can be estimated using variational inference, in addition to adding model constraints to ensure identifiability of all model parameters. Though we limit our analysis here to linear relationships between the transformation matrix F k and the ground truth vector a m * , we note that extending the model to capture non-linear relationships is straightforward. For example, the vector a m * in Equation  7 can be replaced by one that includes a non-linear dependence on a m * . Providing theoretical bounds to the model performance, specially with respect to the sample complexity may also be possible since we have assumed normal distributions throughout the model.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Appendix A Supplementary Analyses A.1 Annotator Specific Correlations",
      "text": "Figure  10  highlights the correlations between annotation dimensions for 4 annotators from the movie emotions  [27]  and RECOLA  [33]  corpora. As noted earlier, different annotators may exhibit different degree of associations between the annotation dimensions, leading to the observed differences in correlations both within and between the two corpora. This difference in annotator behavior also leads to the different inter-dimension correlations observed among the corpora in Figure  3 .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.2 Effect Of Dependency Among Dimensions",
      "text": "The model we present includes the annotator specific parameter F k which measures the relationships between the annotation dimensions. To highlight the ability of the model to recover this parameter, in Figure  11 , we show a plot of averages of all predicted F k matrices for different step sizes from the synthetic experiment described in Section 5.3. In each case, the predicted F k matrices closely resemble the actual matrices for the annotators highlighting the accuracy of the joint model. However, as we get closer to step size 1, the estimated F k matrices appear to be washed out (despite being accurate to a scaling term), with all terms of the estimated F k close to 0.5 instead of 1 (Figure  11f ), due to model unidentifiability. To help with the model formulation, we first derive parameters of the joint distribution p(a m 1 . . . a m K , a m * ). Since the product of two normal distributions is also normal  [47] , this joint distribution is also normal and is given by,",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Appendix B Em Derivation For Global",
      "text": "The different components of the covariance matrix from Equation 15 are derived below.\n\nIn the derivation of Σ kikj , the first equation is a direct application of the law of total covariance and the second equation is because of the conditional independence assumption of annotation values a m ki given the ground truth a m * Finally, owing to the jointly normal distributions,",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "B.1 Em Formulation",
      "text": "We begin by introducing a new distribution q(a m * ) in Equation 10. We drop the parameters Φ from the likelihood function expansion for convenience.\n\nUsing Jensen's inequality over log of expectation, we can write the above as follows,\n\nThe bound above becomes tight when the expectation is taken over a constant value, i.e. The E-step involves simply assuming q(a m * ) to follow the conditional distribution p(a m * |a m 1 . . . a m K ). To help with future computations, we also compute the following expectations, where the first two are a result of equations 16 and 17; third equation is by definition of covariance and the last one is a standard result (see the matrix cookbook eq. 327).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "P(A",
      "text": "concatenating the K annotation vectors a m 1 , . . . a m K and their corresponding expected values F 1 Θ T x m . . . F K Θ T x m .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B.1.2 M-Step",
      "text": "In the M-step, we find the parameters of the model by maximizing Equation  19 . We first write this equation as an expectation and an equality. The expectation below is with respect to q(a m * ) = p(a m * |a m 1 . . . a m K ); we drop the subscript for ease of exposition\n\nwhere p(a m * ) and p(a m k |a m * ) are given by equations 8 and 9 respectively. The last equation above uses that fact that we assume independence among annotators given the ground truth. Also expectation commutes with the linear sum over the K terms.\n\nHere, H is the entropy of p(a m * |a m 1 . . . a m K ). We maximize Equation 20 with respect to each of the parameters to obtain the M-step updates.\n\nEstimating F k Differentiating Equation  (20)  with respect to F k and equating the derivative to 0\n\nwhere, M k is the number of points annotated by user k.\n\nWe used the following facts in the above derivation: trace(x) = x for scalar x; trace(AB) = trace(BA); ∆ A trace(A T x) = x and ∆ A trace(A T AB) = AB + AB T for matrix A. We also make use of the fact that expectation and trace of a matrix are commutative since trace is a linear sum.\n\nEstimating Θ Similarly, to find Θ, we differentiate Equa-tion  (20)  with respect to Θ and equate it to 0.\n\nSince we assume that each annotator is independent of the others given the ground truth, we have For convenience, we reshape a m * into a vector and optimize with respect to the flattened vector. If we choose vec(a m * ) = v and vec(X m Θ) = y, the objective becomes,\n\nDifferentiating Q with respect to v and equating the gradient to 0, we get In the last step we make use of the fact that a m k depends on a m * through Gaussian noise. We also discard all other dimensions d = d since these do not depend on f d k . To estimate f d k , we can rearrange F d k vec(a m * ) such that f d k is now the parameter vector of a linear regression problem with the independent variables represented by matrix A which is obtained by creating a filtering matrix out of vec(a m * ). Hence, the optimization problem becomes\n\nA T a m,d k Estimating τ k Differentiating Equation (  22 ) with respect to τ k and equating the gradient to 0, we have. Estimating Θ Differentiating Equation  (22)  with respect to Θ and equating the gradient to 0, we have.\n\nBy definition, each column of Θ is independent of each other. Hence we can estimate each θ d separately (taking derivatives with respect to above equation would cancel all terms except those in θ d ).",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows a common setting in the crowdsourcing",
      "page": 2
    },
    {
      "caption": "Figure 1: Plate notation for a basic annotation model. am,d",
      "page": 2
    },
    {
      "caption": "Figure 2: shows an extension of the model in Figure 1 in",
      "page": 2
    },
    {
      "caption": "Figure 2: Annotation model proposed by [8] with a jointly",
      "page": 3
    },
    {
      "caption": "Figure 2: ; For example, in [9], the",
      "page": 3
    },
    {
      "caption": "Figure 1: with various annotator functions to capture four speciﬁc",
      "page": 3
    },
    {
      "caption": "Figure 4: Each data in-",
      "page": 4
    },
    {
      "caption": "Figure 4: However, for ease of exposition, we overload",
      "page": 4
    },
    {
      "caption": "Figure 3: Correlation heatmaps for annotations from a representative sample of emotion annotated datasets; v - valence, a -",
      "page": 5
    },
    {
      "caption": "Figure 4: Proposed model. xm is the set of features for the mth",
      "page": 5
    },
    {
      "caption": "Figure 5: Performance of global annotation model on synthetic",
      "page": 7
    },
    {
      "caption": "Figure 6: Performance of global annotation model on artiﬁcial",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the performance of joint and inde-",
      "page": 7
    },
    {
      "caption": "Figure 6: shows the performance of the joint and in-",
      "page": 8
    },
    {
      "caption": "Figure 7: shows the performance of the joint and inde-",
      "page": 8
    },
    {
      "caption": "Figure 7: Performance of global annotation model on the text",
      "page": 8
    },
    {
      "caption": "Figure 8: Concordance and Pearson correlation coefﬁcients between ground truth/reference and model predictions for the",
      "page": 9
    },
    {
      "caption": "Figure 8: shows the aggregate results across C-folds",
      "page": 9
    },
    {
      "caption": "Figure 8: , both models achieve similar per-",
      "page": 9
    },
    {
      "caption": "Figure 8: shows the performance of the two models in",
      "page": 10
    },
    {
      "caption": "Figure 9: shows the 5-fold cross validated performance",
      "page": 10
    },
    {
      "caption": "Figure 9: Effect of varying dependency between annotation",
      "page": 10
    },
    {
      "caption": "Figure 10: highlights the correlations between annotation di-",
      "page": 13
    },
    {
      "caption": "Figure 11: , we show a plot of",
      "page": 13
    },
    {
      "caption": "Figure 11: f), due to",
      "page": 13
    },
    {
      "caption": "Figure 10: Annotator speciﬁc correlations between annotation dimensions for the Movie emotions and RECOLA corpora;",
      "page": 14
    },
    {
      "caption": "Figure 11: Average Fk plots estimated from the joint model at different step sizes for off diagonal elements of the annotator’s",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "m,d\na*\nM": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "*m\n,d\nx\na\nm\nM": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "*m\n*m\n*m\n. .\n,1a\n,2a\nx\nm\n,da\nM": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The wisdom of crowds",
      "authors": [
        "J Surowiecki"
      ],
      "year": "2005",
      "venue": "The wisdom of crowds"
    },
    {
      "citation_id": "2",
      "title": "Maximum likelihood from incomplete data via the EM algorithm",
      "authors": [
        "A Dempster",
        "N Laird",
        "D Rubin"
      ],
      "year": "1977",
      "venue": "Journal of the royal statistical society. Series B"
    },
    {
      "citation_id": "3",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Efficiently scaling up crowdsourced video annotation",
      "authors": [
        "C Vondrick",
        "D Patterson",
        "D Ramanan"
      ],
      "year": "2013",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "5",
      "title": "Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks",
      "authors": [
        "R Snow",
        "B O'connor",
        "D Jurafsky",
        "A Ng"
      ],
      "year": "2008",
      "venue": "Proceedings of the conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "6",
      "title": "Maximum likelihood estimation of observer error-rates using the EM algorithm",
      "authors": [
        "A Dawid",
        "A Skene"
      ],
      "year": "1979",
      "venue": "Applied statistics"
    },
    {
      "citation_id": "7",
      "title": "Inferring ground truth from subjective labelling of venus images",
      "authors": [
        "P Smyth",
        "U Fayyad",
        "M Burl",
        "P Perona",
        "P Baldi"
      ],
      "year": "1995",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Learning from crowds",
      "authors": [
        "V Raykar",
        "S Yu",
        "L Zhao",
        "G Valadez",
        "C Florin",
        "L Bogoni",
        "L Moy"
      ],
      "year": "2010",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "9",
      "title": "A globally-variant locallyconstant model for fusion of labels from multiple diverse experts without using reference labels",
      "authors": [
        "K Audhkhasi",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "2nd International Workshop on Emotion Representation, Analysis and Synthesis in Continuous Time and Space"
    },
    {
      "citation_id": "11",
      "title": "Anvil-a generic annotation tool for multimodal dialogue",
      "authors": [
        "M Kipp"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "12",
      "title": "FEELtrace: An instrument for recording perceived emotion in real time",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "S Savvidou",
        "E Mcmahon",
        "M Sawey",
        "M Schr Öder"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "13",
      "title": "EMuJoy: Software for continuous measurement of perceived emotions in music",
      "authors": [
        "F Nagel",
        "R Kopiez",
        "O Grewe",
        "E Altenm Üller"
      ],
      "year": "2007",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "14",
      "title": "Gtrace: General trace program compatible with emotionml",
      "authors": [
        "R Cowie",
        "M Sawey",
        "C Doherty",
        "J Jaimovich",
        "C Fyans",
        "P Stapleton"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "15",
      "title": "DARMA: Software for dual axis rating and media annotation",
      "authors": [
        "J Girard",
        "A Wright"
      ],
      "year": "2018",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "16",
      "title": "Oudjat: A configurable and usable annotation tool for the study of facial expressions of emotion",
      "authors": [
        "D Dupre",
        "D Akpan",
        "E Elias",
        "J.-M Adam",
        "B Meillon",
        "N Bonnefond",
        "M Dubois",
        "A Tcherkassof"
      ],
      "year": "2015",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "17",
      "title": "Dynamic probabilistic CCA for analysis of affective behavior and fusion of continuous annotations",
      "authors": [
        "M Nicolaou",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2014",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "A probabilistic interpretation of canonical correlation analysis",
      "authors": [
        "F Bach",
        "M Jordan"
      ],
      "year": "2005",
      "venue": "A probabilistic interpretation of canonical correlation analysis"
    },
    {
      "citation_id": "19",
      "title": "Correcting time-continuous emotional labels by modeling the reaction lag of evaluators",
      "authors": [
        "S Mariooryad",
        "C Busso"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Modeling multiple time series annotations as noisy distortions of the ground truth: An expectation-maximization approach",
      "authors": [
        "R Gupta",
        "K Audhkhasi",
        "Z Jacokes",
        "A Rozga",
        "S Narayanan"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "21",
      "title": "Modeling annotator behaviors for crowd labeling",
      "authors": [
        "Y Kara",
        "G Genc",
        "O Aran",
        "L Akarun"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "22",
      "title": "Approval voting and incentives in crowdsourcing",
      "authors": [
        "N Shah",
        "D Zhou",
        "Y Peres"
      ],
      "year": "2015",
      "venue": "Approval voting and incentives in crowdsourcing",
      "arxiv": "arXiv:1502.05696"
    },
    {
      "citation_id": "23",
      "title": "Get another label? improving data quality and data mining using multiple, noisy labelers",
      "authors": [
        "V Sheng",
        "F Provost",
        "P Ipeirotis"
      ],
      "year": "2008",
      "venue": "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining"
    },
    {
      "citation_id": "24",
      "title": "On traits and temperament: General and specific factors of emotional experience and their relation to the five-factor model",
      "authors": [
        "D Watson",
        "L Clark"
      ],
      "year": "1992",
      "venue": "Journal of personality"
    },
    {
      "citation_id": "25",
      "title": "On the bipolarity of positive and negative affect",
      "authors": [
        "J Russell",
        "J Carroll"
      ],
      "year": "1999",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "26",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "27",
      "title": "A supervised approach to movie emotion tracking",
      "authors": [
        "N Malandrakis",
        "A Potamianos",
        "G Evangelopoulos",
        "A Zlatintsi"
      ],
      "year": "2011",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "An expectation maximization approach to joint modeling of multidimensional ratings derived from multiple annotators",
      "authors": [
        "A Ramakrishna",
        "R Gupta",
        "R Grossman",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "29",
      "title": "A functional data analysis approach for continuous 2-d emotion annotations",
      "authors": [
        "K Sharma",
        "M Wagner",
        "C Castellini",
        "E Van Den Broek",
        "F Stulp",
        "F Schwenker"
      ],
      "year": "2019",
      "venue": "Web Intelligence"
    },
    {
      "citation_id": "30",
      "title": "Modern factor analysis",
      "authors": [
        "H Harman"
      ],
      "year": "1976",
      "venue": "Modern factor analysis"
    },
    {
      "citation_id": "31",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "32",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schroder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "34",
      "title": "Viterbi training improves unsupervised dependency parsing",
      "authors": [
        "V Spitkovsky",
        "H Alshawi",
        "D Jurafsky",
        "C Manning"
      ],
      "year": "2010",
      "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning"
    },
    {
      "citation_id": "35",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "36",
      "title": "Evaluating the use of exploratory factor analysis in psychological research",
      "authors": [
        "L Fabrigar",
        "D Wegener",
        "R Maccallum",
        "E Strahan"
      ],
      "year": "1999",
      "venue": "Psychological methods"
    },
    {
      "citation_id": "37",
      "title": "The varimax criterion for analytic rotation in factor analysis",
      "authors": [
        "H Kaiser"
      ],
      "year": "1958",
      "venue": "Psychometrika"
    },
    {
      "citation_id": "38",
      "title": "An introduction to the bootstrap",
      "authors": [
        "B Efron",
        "R Tibshirani"
      ],
      "year": "1994",
      "venue": "An introduction to the bootstrap"
    },
    {
      "citation_id": "39",
      "title": "A novel method for human bias correction of continuous-time annotations",
      "authors": [
        "B Booth",
        "K Mundnich",
        "S Narayanan"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "The brightness of colour",
      "authors": [
        "D Corney",
        "J.-D Haynes",
        "G Rees",
        "R Lotto"
      ],
      "year": "2009",
      "venue": "PloS one"
    },
    {
      "citation_id": "41",
      "title": "Semeval-2007 task 14: Affective text",
      "authors": [
        "C Strapparava",
        "R Mihalcea"
      ],
      "year": "2007",
      "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, ser. SemEval '07"
    },
    {
      "citation_id": "42",
      "title": "Unsupervised learning of sentence embeddings using compositional n-gram features",
      "authors": [
        "M Pagliardini",
        "P Gupta",
        "M Jaggi"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "43",
      "title": "Ros: an open-source robot operating system",
      "authors": [
        "M Quigley",
        "B Gerkey",
        "K Conley",
        "J Faust",
        "T Foote",
        "J Leibs",
        "E Berger",
        "R Wheeler",
        "A Ng"
      ],
      "year": "2009",
      "venue": "Proc. of the IEEE Intl. Conf. on Robotics and Automation (ICRA) Workshop on Open Source Robotics"
    },
    {
      "citation_id": "44",
      "title": "Opensmile: The Munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, ser. MM '10"
    },
    {
      "citation_id": "45",
      "title": "The OpenCV Library",
      "authors": [
        "G Bradski"
      ],
      "year": "2000",
      "venue": "Dr. Dobb's Journal of Software Tools"
    },
    {
      "citation_id": "46",
      "title": "Multimodal human emotion/expression recognition",
      "authors": [
        "L Chen",
        "T Huang",
        "T Miyasato",
        "R Nakatsu"
      ],
      "year": "1998",
      "venue": "Proceedings of Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "47",
      "title": "Pattern Recognition and Machine Learning",
      "authors": [
        "C Bishop"
      ],
      "year": "2006",
      "venue": "Pattern Recognition and Machine Learning"
    }
  ]
}