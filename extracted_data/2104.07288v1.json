{
  "paper_id": "2104.07288v1",
  "title": "Speaker Attentive Speech Emotion Recognition",
  "published": "2021-04-15T07:59:37Z",
  "authors": [
    "Cl√©ment Le Moine",
    "Nicolas Obin",
    "Axel Roebel"
  ],
  "keywords": [
    "speaker attentive emotion recognition",
    "convolutional recurrent neural networks",
    "attention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) task has known significant improvements over the last years with the advent of Deep Neural Networks (DNNs). However, even the most successful methods are still rather failing when adaptation to specific speakers and scenarios is needed, inevitably leading to poorer performances when compared to humans. In this paper, we present novel work based on the idea of teaching the emotion recognition network about speaker identity. Our system is a combination of two ACRNN classifiers respectively dedicated to speaker and emotion recognition. The first informs the latter through a Self Speaker Attention (SSA) mechanism that is shown to considerably help to focus on emotional information of the speech signal. Experiments on social attitudes database Att-HACK and IEMOCAP corpus demonstrate the effectiveness of the proposed method and achieve the state-of-the-art performance in terms of unweighted average recall.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction 1.Context",
      "text": "Speech is one of the most important medium for human communication, not only conveying linguistic information but also rich and subtle para-linguistic information. In particular the affective prosody of speech conveys the emotions which play an important role in the perception of an utterance meaning. Therefore, speech emotion recognition (SER), which aims to recognize the actual emotional state of a speaker from the utterance he produced, has raised a great interest among researchers.\n\nAs pointed out by Scherer in  [1] , there is an undeniable contradiction between the apparent ease with which listeners judge emotions from speech and the intricacy of finding discriminative features in speech signal for emotion recognition. This contradiction, in part, lies in the role that individual difference variables play in the production of emotional speech  [2] . In particular, the acoustic characteristics of the speaker voice as well as its speaking style, for a significant part, determine its way of expressing the emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Formerly adressed using statistical methods and traditional learning techniques such as Hidden Markov Models (HMMs), Gaussian Mixture Models (GMMs) and Support Vector Machines (SVMs), the SER task has known significant improvements over the past years with the advent of deep neural networks (DNNs). Indeed such deep networks have shown excellent abilities to model more complex patterns within speech utterances by extracting high-level features from speech signal for better recognition of the emotional state of the speakers.\n\nMao et al.  [3]  firstly introduced Convolutional Neural Networks (CNNs) for the SER task and obtained remarkable results on various datasets by learning affective-salient features. Recurrent neural networks (RNNs) has also been introduced for SER purpose with a deep Bidirectional Long Short-Term Memory (BLTSM) network proposed by Lee et al.  [4] . Several papers have then presented CNNs in combination with LSTM cells to improve speech emotion recognition, based on log Mel filterbanks (logMel)  [5]  or raw signal in an end-to-end manner  [6] .\n\nRecently, attention mechanisms have raised great interest in the SER research area for their ability to focus on specific parts of an utterance that characterize emotions. Mirasmadi et al.  [7]  approached the problem with a reccurent neural network and a local attention model used to learn weighted time-pooling strategies. Neumann et al.  [8]  used an attentive CNN (ACNN) and showed the importance of the model architecture choice against the features choice. Ramet et al.  [9]  presented a review of attention models on top of BLSTMs and proposed a new attention computed from the outputs of an added BLSTM layer. Chen et al.  [10]  proposed a 3-D Attention-based Convolutional Recurrent Neural Networks (ACRNN) for SER with 3-D log-Mel spectrograms (static, deltas and delta-deltas) as input features. They showed 3-D convolution can better capture more effective information for SER compared with 2-D convolution. Recently, Meng et al. outperformed this method by using dilated convolutions in place of a pooling layer and skip connection.\n\nAttempts to inform emotion classification networks with extra-information involved in the description of emotions were proposed in the past years. Based on previous works  [11, 12, 13] , Li et al.  [14]  proposed a multitask learning framework that involves gender classification as an auxiliary task to provide emotion-relevant information leading to significant improvements in SER. Analogously, speaker identity has been used to inform emotion classification networks. The problem was approached by Sidorov et al.  [15]  with speaker dependent models for emotion recognition. Recently, a method for speaker aware SER was introduced by Assunc ¬∏√£o et al.  [16] , a CNN model VGGVox  [17]  is trained for speaker identification but is instead used as a front-end for extracting robust features from emotional speech. These first attempts have shown that teaching SER systems with additional signal-based information can greatly improve performances.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Contributions Of The Paper",
      "text": "In this paper, we assume that individuals may use different means to express emotions, and then that SER should be conditionned on the speaker identity information. Following this hypothesis, we propose a novel method based on previous work  [10] . The major contributions of this paper are summarized as:\n\n1. Conditioning emotion classification to speaker identity by using a key-query-value attention we called Self Speaker Attention (SSA) that allows to compute both self and cross-attribute (relation between speaker identity and emotions) attention scores to focus on emotion relevant parts of an utterance.\n\n2. Proposing a novel regularization by constraining the weights of the last fully connected layer of the network so as to avoid class collapse.\n\n3. Achieving an absolute increase of UAR by 13.10 % for IEMOCAP compared with the best existing result.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methodology",
      "text": "In this section, we introduce our SSA-CRNN proposal for SER and a regularization answering class collapse issue. First, the computation of log-Mel spectrograms is described 2.1, then the basic ACRNN architecture from which our proposal is derived is presented in 2.2. Finally our contributions are detailed : the SSA-CRNN architecture is presented in 1 and our novel regularization in 2.4.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "3-D Log-Mel Spectrograms",
      "text": "3-D Log-Mel spectrograms (with delta and delat-deltas), already used as input features for various tasks, were introduced for the SER task by Chen et al.  [10]  as input of their ACRNN model and later used in  [18] . In this paper, the Log-Mel spectrograms are computed as presented in  [18] . The 3-D Log-Mel spectrogram consists of a three channel input. The first channel is the static of the Log-Mel spectrogram from 40 filterbanks, the second and third channels are respectively deltas and deltadeltas which can be considered as approximations of the first and second derivatives of the first channel. Once obtained, each 3-D input sample is normalized to have zero mean and unit variance for a given speaker.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Architecture Of (Self) Attentive Convolutional Recurrent Neural Net (Acrnn)",
      "text": "Given 3-D log-Mel spectrograms, CRNN is used in  [10]  to extract high-level features for speech emotion recognition. The same architecture is used in our proposal to extract high-level features in both cases of SER and speaker recognition (SR).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Architecture Of Crnn",
      "text": "The CRNN architecture consists of several 3-D convolution layers, one 3-D maxpooling layers, one linear layer and one LSTM layer. Specifically, the first convolutional layer has 128 feature maps, while the remaining convolutional layers have 256 feature maps, and the filter size of each convolutional layer is 5 √ó 3 , where 5 corresponds to the time axis, and 3 corresponds to the frequency axis. A max-pooling is performed after the first convolutional with pooling size is 2 √ó 2. The 3D features are reshaped to 2D, keeping time dimension unchanged and passed to a linear layer for dimension reduction before reaching the recurrent layer. As precised in  [10] , a linear layer of 768 output units is shown to be appropriate. These features are then processed through a bi-directional recurrent neural network with long short term memory cells (BLSTM)  [19] , with 128 cells in each direction, for temporal summarization, which allows to get 256-dimensional high-level feature representations.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self Attention (Sa) Mechansim",
      "text": "With a sequence of high-level representations, an attention layer is employed to focus on relevant features and produce discriminative utterance-level representations for classification, since not all frame-level CRNN features contribute equally to the representation of the attributes to recognize, in both cases of SER and speaker recognition. Specifically, with the classifier's BLSTM output hatt = [h 1 att , ..., h T att ] ‚àà R T √ód , a temporal vector Œ±att ‚àà R T , representing the contribution per frame to the attribute to recognize, is computed depending on learnt weights vector Watt ‚àà R d . Then Œ±att is used to obtain an utterance-level representation by computing the weighted sum of temporal BLSTM internal states catt often called context vector.\n\nThe attention layer is followed by a fully connected layer that determines the embedding size.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Architecture Of The Proposed Self Speaker Attentive Convolutional Neural Net (Ssa-Crnn)",
      "text": "Our system shown in Figure  1  is formed by two classifiers Csp and Cem. The first is an ACRNN and trained for SR. The second has the same architecture apart from its Self Speaker Attention (SSA) layer and is trained (once Csp has been trained) for SER, conditionally to the speakers embedding produced by Csp and through the SSA mechanism described further. This conditioning of SER to speaker identity is expected to help Cem to better capture speakers individual strategies for emotion expression leading to better global SER performances.\n\nFigure  2 : Self Speaker Attention (SSA) Mechanism\n\nInspired by  [19]  that used multi-modal attention technique for SER, we employed the query-key-value representation to compute the attention from two inputs : hem and hsp respectively related to \"self\" and \"speaker\" aspects. We first compute the query of speech emotion qem through learnable weights   The keys K and values V are computed using learnable weights W k em , W k sp , W v em , W v sp ‚àà R T as follows.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Self Speaker Attention",
      "text": "The cross-attribute and self attention scores are computed by the product of the query qem and transposed keys K T . It is then normalized, passed to a softmax activation layer and finally multiplied with values V to obtain Œ±em ‚àà R T which represents the interaction of speaker identity and speech emotion answering to speech emotion query. This temporal vector Œ±em is then used to compute the weighted sum of temporal BLSTM internal states cem.\n\n3. Experiments",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "To evaluate the performance of our proposed model, we perform speaker-independent SER experiments on the speech database for speech social attitudes Att-HACK  [20]  and the Interactive Emotional Dyadic Motion Capture database (IEMOCAP)  [21] .\n\nAtt-HACK comprises 20 speakers interpreting 100 utterances in 4 social attitudes : friendly, distant, dominant and seductive. With 3 to 5 repetitions each per attitude for a total of around 30 hours of speech, the database offers a wide variety of prosodic strategies in the expression of attitudes. IEMOCAP consists of 5 sessions and each session is displayed by a pair of speakers (female and male) in scripted and improvised scenarios. For this experiment, only improvised data and 4 emotions, happy, angry, sad and neutral were considered.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setup",
      "text": "As for the feature extraction, we split the speech signal into equal-length segments of 3 seconds for better parallel acceleration and adopt zero-padding for utterances not reaching the 3 seconds.\n\n‚Ä¢ ACRNN : Re-implemented strictly following  [10]  apart from the embedding size that we increased from 64 to 128. Our proposed systems are derived from this baseline SER system Cem.\n\n‚Ä¢ ACRNN-r : Addition of a regularization (described in 2.4) to Cem model to avoid class collapse.\n\n‚Ä¢ SSA-CRNN-r : A first ACRNN model Csp (embedding size 64) is trained for speaker recognition to learn highlevel features that define the speaker identity, and from which our SSA mechanism will extract emotion salient information to inform the SER task. Once best accuracy is reached, the model is saved and fixed. Then a second ACRNN model Cem (embedding size 128) is trained conditionally to the BLSTM's outputs of Csp by mean of the SSA mechanism for SER task.\n\nFor evaluations, 10-fold cross validation technique is performed. Respectively for IEMOCAP and Att-HACK, 9 and 18 speakers are considered for training while the remaining ones are kept for validation. In the case of Att-HACK validation sets are balanced in gender (male-female couples) apart from 2 folds, as the database includes 12 females and only 8 males. As we aspire to perform emotion recognition independently of the speaker, we must consider two approaches for training the SSA-CRNN-r Csp module.\n\n1. speaker-dependant approach : a train/valid split have been performed linguistically on the whole database, all speakers have been seen by Csp. Consequently, the SER task is not performed independently of the speaker and there is no insurance the model will be able to generalize to speakers it has never seen.\n\n2. speaker-independant approach : a LOSO (Leave-One-Speaker-Out) protocol is followed so as to ensure that the speaker (or speakers for Att-HACK) on which the model is validated has never been seen by the model. For each fold k ‚àà  [1, 10]  the speaker s v k (or speakers\n\n) chosen to be in the validation set are left out of the database for the training and validation of Csp .\n\nEach module is optimized with respect to the cross-entropy objective function. Trainings are done with emotionally balanced mini-batches of 40 samples, using the Adam optimizer with Nestorov momentum. The initial learning rate is set to 0.0001 and the momentum is set to 0.9. The final model parameters are selected by maximizing the Unweighted Average Recall (UAR) on the validation set.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiment Results",
      "text": "Table  1  is divided in two parts, the first part shows the results in UAR on IEMOCAP of state-of-the-art methods for SER, including the ACRNN system by  [10]  and the gender-informed system by  [13] . The second part shows the UAR results of our proposed methods on both Att-HACK and IEMOCAP databases. First, we compare our ACRNN re-implementation with the state-of-the-art method described in  [10] . Our reimplementation achieves slightly better performance for IEMO-CAP with an absolute improvement of 1.64% and a rather low standard deviation possibly due to the increased embedding size.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ser System",
      "text": "Att-HACK IEMOCAP Chen et al.  [    1 : SER results with 95% confidence interval for different methods on Att-HACK and IEMOCAP in terms of UAR Next, we investigate the effectiveness of our proposed regularization. Compared with ACRNN, ACRNN-r obtains an absolute improvement of 38.47% on Att-HACK and 19.97% for IEMOCAP. This shows our regularization brings stability during training that allows the model to further recognize more subtle emotions. The confusion matrices depicted in Figure  3  also show the model is no longer over-focusing on one class, all the other classes are confused, with which was the case for dominant attitude in Att-HACK with the ACRNN.\n\nTo evaluate the addition of our proposed SSA mechanism, it is legitimate to chose the approach with LOSO protocol described in 3.2. Compared with ACRNN-r, this approach achieves an absolute improvement of 21.95 % for Att-HACK and 9.55 % for IEMOCAP. This shows combining emotion classification and speaker recognition through our SSA mechanism is relevant and lead to better capture speaker means of emotional expression. This approach achieves a global absolute improvement of 60.82 % for Att-HACK and 29.52 % for IEMOCAP compared with ACRNN. Compared with the gender-informed SER system by Zhang et al.  [13] , it achieves 13.10 % of UAR improvement on IEMOCAP. As expected, the speakerdependant approach achieves slightly better performance than the LOSO-based one although it does not seem significant. It appears that Csp is able to generalize enough for new speakers so as the SSA can capture the speaker high-level features that are relevant in the representation of speech emotions.  Finally, the t-SNE analysis in Figure  4  tend to show both attitudes and emotions are well separated in their respective embeddings. Although this point would need to be deeper investigated, we can further expect to use these embeddings in a voice conversion context as proposed by Zhou et al. in  [22] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Att-Hack Iemocap",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we propose a Self Speaker Attentive Convolutional Neural Network (SSA-CRNN) model to train the SER model operating on 3D Log-Mel spectrograms and a novel training regularization that allows to avoid class collapse. The evaluation on Att-HACK and IEMOCAP databases demonstrates that the proposed method outperforms the state-of-theart methods. Notably, the evaluation on IEMOCAP achieves 13.10 % absolute improvement of UAR compared with the gender-informed SER system by Zhang et al.  [13] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "This research is supported by the MoVe project: \"MOdelling of speech attitudes and application to an expressive conversationnal agent\", and funded by the Paris Region Ph2D grant.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: is formed by two classiÔ¨Åers Csp",
      "page": 2
    },
    {
      "caption": "Figure 2: Self Speaker Attention (SSA) Mechanism",
      "page": 2
    },
    {
      "caption": "Figure 1: Self Speaker Attentive Convolutional Recurrent Neural Network (SSA-CRNN) for speaker attentive speech emotion recogni-",
      "page": 3
    },
    {
      "caption": "Figure 3: also show the model is no longer over-focusing on one class,",
      "page": 4
    },
    {
      "caption": "Figure 3: SER normalized confusion matrices where each row",
      "page": 4
    },
    {
      "caption": "Figure 4: tend to show both",
      "page": 4
    },
    {
      "caption": "Figure 4: T-SNE representations obtained from emotion embed-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Paris, France": "better recognition of the emotional state of the speakers."
        },
        {
          "Paris, France": "Mao et al.\n[3] Ô¨Årstly introduced Convolutional Neural Net-"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "works (CNNs) for the SER task and obtained remarkable results"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "on various datasets by learning affective-salient features. Recur-"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "rent neural networks (RNNs) has also been introduced for SER"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "purpose with a deep Bidirectional Long Short-Term Memory"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "(BLTSM) network proposed by Lee et al.\n[4]. Several papers"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "have then presented CNNs in combination with LSTM cells to"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "improve speech emotion recognition, based on log Mel Ô¨Ålter-"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "banks (logMel) [5] or raw signal in an end-to-end manner [6]."
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "Recently, attention mechanisms have raised great\ninterest"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "in the SER research area for\ntheir ability to focus on speciÔ¨Åc"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "parts of an utterance that characterize emotions. Mirasmadi et"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "al. [7] approached the problem with a reccurent neural network"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "and a local attention model used to learn weighted time-pooling"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "strategies. Neumann et al.\n[8] used an attentive CNN (ACNN)"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "and showed the importance of\nthe model architecture choice"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "against\nthe features choice.\nRamet et al.\n[9] presented a re-"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "view of attention models on top of BLSTMs and proposed a"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "new attention computed from the outputs of an added BLSTM"
        },
        {
          "Paris, France": "layer. Chen et al.\n[10] proposed a 3-D Attention-based Con-"
        },
        {
          "Paris, France": "volutional Recurrent Neural Networks (ACRNN) for SER with"
        },
        {
          "Paris, France": "3-D log-Mel spectrograms (static, deltas and delta-deltas) as in-"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "put features. They showed 3-D convolution can better capture"
        },
        {
          "Paris, France": "more effective information for SER compared with 2-D con-"
        },
        {
          "Paris, France": "volution. Recently, Meng et al.\noutperformed this method by"
        },
        {
          "Paris, France": "using dilated convolutions in place of a pooling layer and skip"
        },
        {
          "Paris, France": "connection."
        },
        {
          "Paris, France": "Attempts\nto inform emotion classiÔ¨Åcation networks with"
        },
        {
          "Paris, France": "extra-information involved in the description of emotions were"
        },
        {
          "Paris, France": "proposed in the past years. Based on previous works [11, 12,"
        },
        {
          "Paris, France": "13], Li et al. [14] proposed a multitask learning framework that"
        },
        {
          "Paris, France": "involves gender classiÔ¨Åcation as an auxiliary task to provide"
        },
        {
          "Paris, France": "emotion-relevant\ninformation leading to signiÔ¨Åcant\nimprove-"
        },
        {
          "Paris, France": "ments in SER. Analogously, speaker identity has been used to"
        },
        {
          "Paris, France": "inform emotion classiÔ¨Åcation networks. The problem was ap-"
        },
        {
          "Paris, France": "proached by Sidorov et al.\n[15] with speaker dependent mod-"
        },
        {
          "Paris, France": "els\nfor emotion recognition.\nRecently, a method for\nspeaker"
        },
        {
          "Paris, France": "aware SER was introduced by Assunc¬∏ Àúao et al.\n[16], a CNN"
        },
        {
          "Paris, France": "model VGGVox [17] is trained for speaker identiÔ¨Åcation but\nis"
        },
        {
          "Paris, France": "instead used as a front-end for extracting robust features from"
        },
        {
          "Paris, France": "emotional speech. These Ô¨Årst attempts have shown that\nteach-"
        },
        {
          "Paris, France": "ing SER systems with additional signal-based information can"
        },
        {
          "Paris, France": "greatly improve performances."
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "1.3. Contributions of the paper"
        },
        {
          "Paris, France": ""
        },
        {
          "Paris, France": "In this paper, we\nassume\nthat\nindividuals may use different"
        },
        {
          "Paris, France": "means to express emotions, and then that SER should be con-"
        },
        {
          "Paris, France": "ditionned on the speaker\nidentity information.\nFollowing this"
        },
        {
          "Paris, France": "hypothesis, we propose a novel method based on previous work"
        },
        {
          "Paris, France": "[10]. The major contributions of this paper are summarized as:"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "by\nusing\na\nkey-query-value\nattention we\ncalled Self",
          "2.2.2.\nSelf Attention (SA) mechansim": ""
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "With a sequence of high-level representations, an attention layer"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "Speaker Attention (SSA)\nthat allows\nto compute both",
          "2.2.2.\nSelf Attention (SA) mechansim": ""
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "is employed to focus on relevant features and produce discrim-"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "self and cross-attribute (relation between speaker\niden-",
          "2.2.2.\nSelf Attention (SA) mechansim": ""
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "inative utterance-level\nrepresentations for classiÔ¨Åcation,\nsince"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "tity and emotions) attention scores to focus on emotion",
          "2.2.2.\nSelf Attention (SA) mechansim": ""
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "not all frame-level CRNN features contribute equally to the rep-"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "relevant parts of an utterance.",
          "2.2.2.\nSelf Attention (SA) mechansim": ""
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "resentation of the attributes to recognize,\nin both cases of SER"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "2.\nProposing\na\nnovel\nregularization\nby\nconstraining\nthe",
          "2.2.2.\nSelf Attention (SA) mechansim": "and speaker recognition."
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "weights of the last fully connected layer of the network",
          "2.2.2.\nSelf Attention (SA) mechansim": "SpeciÔ¨Åcally, with the classiÔ¨Åer‚Äôs BLSTM output hatt ="
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "so as to avoid class collapse.",
          "2.2.2.\nSelf Attention (SA) mechansim": "[h1\natt, ..., hT\natt] ‚àà RT √ód, a temporal vector Œ±att ‚àà RT , repre-"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "senting the contribution per frame to the attribute to recognize,"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "3. Achieving an absolute increase of UAR by 13.10 % for",
          "2.2.2.\nSelf Attention (SA) mechansim": "is computed depending on learnt weights vector Watt ‚àà Rd."
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "IEMOCAP compared with the best existing result.",
          "2.2.2.\nSelf Attention (SA) mechansim": "is used to obtain an utterance-level\nrepresentation\nThen Œ±att"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "by computing the weighted sum of\ntemporal BLSTM internal"
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "",
          "2.2.2.\nSelf Attention (SA) mechansim": "states catt often called context vector."
        },
        {
          "1. Conditioning emotion classiÔ¨Åcation to speaker\nidentity": "2. Proposed Methodology",
          "2.2.2.\nSelf Attention (SA) mechansim": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "by computing the weighted sum of\ntemporal BLSTM internal"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "states catt often called context vector."
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "2. Proposed Methodology"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "exp(hattWatt)"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "In this section, we introduce our SSA-CRNN proposal for SER\n‚àà RT\n(1)\nŒ±att ="
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "(cid:80)T"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "and a regularization answering class collapse issue.\nFirst,\nthe\natt)\nt=1 exp(Wattht"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "computation of log-Mel spectrograms is described 2.1, then the"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "speaker \nConvolutional reccurrent self attentive encoder"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "embedding\nbasic ACRNN architecture from which our proposal\nis derived\nŒ±t"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "T(cid:88) t\n(2)\ncatt =\nattht\natt\nget features from 3D spectrogram"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "temporal \nis presented in 2.2. Finally our contributions are detailed :\nthe"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "=1\nCsp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "summerization\nspeciÔ¨Åc focus"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "SSA-CRNN architecture is presented in 1 and our novel regu-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "The attention layer is followed by a fully connected layer"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Conv3D \nConv3D \nConv3D \nConv3D \nConv3D \nBatch-Norm\nsoftmax\nlarization in 2.4.\nFC"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Maxpooling 3D\nAttention\nFC\nBLSTM"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "that determines the embedding size."
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "T]\n1, ‚Ä¶, hsp\nhsp = [hsp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "2.1.\n3-D Log-Mel spectrograms"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": ""
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "ClassiÔ¨Åer\n2.3. Architecture of\nthe proposed Self Speaker Attentive"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "zsp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "3-D Log-Mel\nspectrograms\n(with delta and delat-deltas),\nal-\nConvolutional Neural Net (SSA-CRNN)"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "emotion \nConvolutional reccurrent  self speaker attentive encoder"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "ready used as input features for various tasks, were introduced"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Our system shown in Figure 1 is formed by two classiÔ¨Åers Csp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "get features from 3D spectrogram\nfor the SER task by Chen et al.\n[10] as input of their ACRNN"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "temporal \nis an ACRNN and trained for SR. The sec-\nCem\nand Cem. The Ô¨Årst"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "model and later used in [18].\nIn this paper,\nthe Log-Mel spec-\nsummerization"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "ond has the same architecture apart from its Self Speaker Atten-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "trograms are computed as presented in [18]. The 3-D Log-Mel"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": ""
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Conv3D \nMaxpooling 3D\nConv3D \nConv3D \nConv3D \nConv3D \nBatch-Norm\nsoftmax\nspectrogram, delta \ntion (SSA) layer and is trained (once Csp has been trained) for\nFC"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Attention\nFC\nBLSTM"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "spectrogram consists of a three channel input. The Ô¨Årst channel"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "SER, conditionally to the speakers embedding produced by Csp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "is the static of\nthe Log-Mel spectrogram from 40 Ô¨Ålterbanks,"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "and through the SSA mechanism described further. This condi-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "the second and third channels are respectively deltas and delta-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "ClassiÔ¨Åer"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "tioning of SER to speaker identity is expected to help Cem to"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "zem\ndeltas which can be considered as approximations of\nthe Ô¨Årst"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "better capture speakers individual strategies for emotion expres-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "and second derivatives of the Ô¨Årst channel. Once obtained, each"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "sion leading to better global SER performances."
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "3-D input sample is normalized to have zero mean and unit vari-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "ance for a given speaker."
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "2.3.1.\nSelf Speaker Attention (SSA) mechanism"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "2.2. Architecture of\n(self) Attentive Convolutional Recur-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "rent Neural Net (ACRNN)"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Self Speaker Attention"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "v\nV\nWsp\nGiven 3-D log-Mel spectrograms, CRNN is used in [10] to ex-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "ùõÇem = softmax("
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "from speaker"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "qemK/T^1/2)V\ntract high-level\nfeatures for speech emotion recognition.\nThe\nclassiÔ¨Åer's"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "k"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "BLSTM layer"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Wsp\nsame architecture is used in our proposal\nto extract high-level\nvem vsp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "features in both cases of SER and speaker recognition (SR)."
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "v"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Wem"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "T]\n1, ‚Ä¶, hsp\nhsp = [hsp\nK"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "to emotion  \n2.2.1. Architecture of CRNN"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "classiÔ¨Åer's"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "FC layer"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "k\nThe CRNN architecture consists of several 3-D convolution lay-\nWem\nkem\nksp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "from emotion"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "cem = ùõÇemhem\nclassiÔ¨Åer's  \ners, one 3-D maxpooling layers, one linear layer and one LSTM"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "BLSTM layer"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "layer. SpeciÔ¨Åcally,\nthe Ô¨Årst convolutional\nlayer has 128 feature\nq"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Wem\nqemK"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "maps, while the remaining convolutional\nlayers have 256 fea-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "ture maps, and the Ô¨Ålter size of each convolutional layer is 5 √ó 3"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "qem\nT]"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "hem = [hem"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": ", where 5 corresponds to the time axis, and 3 corresponds to"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "the frequency axis. A max-pooling is performed after the Ô¨Årst"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Figure 2: Self Speaker Attention (SSA) Mechanism"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "convolutional with pooling size is 2 √ó 2. The 3D features are"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "reshaped to 2D, keeping time dimension unchanged and passed"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "Inspired by [19] that used multi-modal attention technique for\nto a linear layer for dimension reduction before reaching the re-\nBLSTM"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "SER, we employed the query-key-value representation to com-\ncurrent\nlayer. As precised in [10], a linear layer of 768 output"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "BLSTM\npute\nthe\nattention from two inputs\nrespec-\nhem and hsp"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "units is shown to be appropriate. These features are then pro-"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "tively related to ‚Äùself‚Äù and ‚Äùspeaker‚Äù aspects. We Ô¨Årst com-\ncessed through a bi-directional\nrecurrent neural network with"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "pute the query of speech emotion qem through learnable weights\nlong short term memory cells (BLSTM) [19], with 128 cells in"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "W q\nem ‚àà RT .\neach direction, for temporal summarization, which allows to get"
        },
        {
          "IEMOCAP compared with the best existing result.\nis used to obtain an utterance-level\nrepresentation\nThen Œ±att": "256-dimensional high-level feature representations.\n(3)\nqem = hemW q\nem"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "& delta-delta\ntemporal": "summerization"
        },
        {
          "& delta-delta\ntemporal": "speciÔ¨Åc focus\nClassiÔ¨Åer"
        },
        {
          "& delta-delta\ntemporal": "get features from 3D spectrogram"
        },
        {
          "& delta-delta\ntemporal": "emotion"
        },
        {
          "& delta-delta\ntemporal": "Emotion ACRNN Cem\nembedding"
        },
        {
          "& delta-delta\ntemporal": "Figure 1: Self Speaker Attentive Convolutional Recurrent Neural Network (SSA-CRNN) for speaker attentive speech emotion recogni-"
        },
        {
          "& delta-delta\ntemporal": "tion composed with two classiÔ¨Åers Csp (up) and Cem (bottom) connected through the Self Speaker Attention (SSA) mechanism."
        },
        {
          "& delta-delta\ntemporal": "The keys K and values V\nare computed using learnable\nfor speech social attitudes Att-HACK [20] and the Interactive"
        },
        {
          "& delta-delta\ntemporal": "weights W k\nEmotional Dyadic Motion Capture database (IEMOCAP) [21].\nem, W k\nsp, W v\nem, W v\nsp ‚àà RT as follows."
        },
        {
          "& delta-delta\ntemporal": "v"
        },
        {
          "& delta-delta\ntemporal": "V\nWsp\nAtt-HACK comprises 20 speakers interpreting 100 utterances\nfrom speaker"
        },
        {
          "& delta-delta\ntemporal": "(4)\nK = concat{hemW k\nem, hspW k\nsp}\nin 4 social attitudes :\nfriendly, distant, dominant and seductive."
        },
        {
          "& delta-delta\ntemporal": "k"
        },
        {
          "& delta-delta\ntemporal": "BLSTM layer\nWsp"
        },
        {
          "& delta-delta\ntemporal": "vem vsp\nWith 3 to 5 repetitions each per attitude for a total of around 30\n(5)\nV = concat{hemW v\nem, hspW v\nsp}"
        },
        {
          "& delta-delta\ntemporal": "hours of speech,\nthe database offers a wide variety of prosodic"
        },
        {
          "& delta-delta\ntemporal": "v"
        },
        {
          "& delta-delta\ntemporal": "strategies in the expression of attitudes.\nIEMOCAP consists of\nThe cross-attribute and self attention scores are computed\nK"
        },
        {
          "& delta-delta\ntemporal": "5 sessions and each session is displayed by a pair of speakers\nIt is\nby the product of the query qem and transposed keys K T ."
        },
        {
          "& delta-delta\ntemporal": "classiÔ¨Åer's"
        },
        {
          "& delta-delta\ntemporal": "FC layer\n(female and male)\nin scripted and improvised scenarios.\nFor\nthen normalized, passed to a softmax activation layer and Ô¨Ånally"
        },
        {
          "& delta-delta\ntemporal": "Wem\nkem\nksp"
        },
        {
          "& delta-delta\ntemporal": "this experiment, only improvised data and 4 emotions, happy,\nmultiplied with values V to obtain Œ±em ‚àà RT which represents\nclassiÔ¨Åer's"
        },
        {
          "& delta-delta\ntemporal": "angry, sad and neutral were considered.\nthe interaction of speaker identity and speech emotion answer-"
        },
        {
          "& delta-delta\ntemporal": "q"
        },
        {
          "& delta-delta\ntemporal": "qemK"
        },
        {
          "& delta-delta\ntemporal": "ing to speech emotion query. This temporal vector Œ±em is then"
        },
        {
          "& delta-delta\ntemporal": "used to compute the weighted sum of temporal BLSTM internal\n3.2. Experiment Setup"
        },
        {
          "& delta-delta\ntemporal": "qem\nT]"
        },
        {
          "& delta-delta\ntemporal": "hem = [hem"
        },
        {
          "& delta-delta\ntemporal": "states cem."
        },
        {
          "& delta-delta\ntemporal": "As\nfor\nthe feature extraction, we split\nthe speech signal\ninto"
        },
        {
          "& delta-delta\ntemporal": "(cid:19)\nequal-length segments of 3 seconds for better parallel acceler-\n(cid:18) qemK T"
        },
        {
          "& delta-delta\ntemporal": "‚àö\nV\n(6)\nŒ±em = sof tmax"
        },
        {
          "& delta-delta\ntemporal": "ation and adopt zero-padding for utterances not reaching the 3"
        },
        {
          "& delta-delta\ntemporal": "T"
        },
        {
          "& delta-delta\ntemporal": "seconds."
        },
        {
          "& delta-delta\ntemporal": "BLSTM"
        },
        {
          "& delta-delta\ntemporal": "T(cid:88) t\nŒ±t\n(7)\ncem =\nemht"
        },
        {
          "& delta-delta\ntemporal": "em\n‚Ä¢ ACRNN : Re-implemented strictly following [10] apart\nBLSTM"
        },
        {
          "& delta-delta\ntemporal": "=1"
        },
        {
          "& delta-delta\ntemporal": "from the embedding size that we increased from 64 to"
        },
        {
          "& delta-delta\ntemporal": "128. Our proposed systems are derived from this base-"
        },
        {
          "& delta-delta\ntemporal": "2.4. Weights regularization"
        },
        {
          "& delta-delta\ntemporal": "line SER system Cem."
        },
        {
          "& delta-delta\ntemporal": "During preliminary experiments with ACRNN on the\nlarge"
        },
        {
          "& delta-delta\ntemporal": "‚Ä¢ ACRNN-r : Addition of a regularization (described in"
        },
        {
          "& delta-delta\ntemporal": "database Att-HACK presented in 3.1, we faced a class collapse"
        },
        {
          "& delta-delta\ntemporal": "2.4) to Cem model to avoid class collapse."
        },
        {
          "& delta-delta\ntemporal": "issue :\nthe model\ntended to focus on one or\ntwo of\nthe four"
        },
        {
          "& delta-delta\ntemporal": "classes at\nthe expense of\nthe others.\nThat\nled to have one or\n‚Ä¢ SSA-CRNN-r : A Ô¨Årst ACRNN model Csp (embedding"
        },
        {
          "& delta-delta\ntemporal": "several categories very badly recognized.\nIn order to avoid this\nsize 64) is trained for speaker recognition to learn high-"
        },
        {
          "& delta-delta\ntemporal": "pitfall, we elaborated a training regularization by constraining\nlevel features that deÔ¨Åne the speaker identity, and from"
        },
        {
          "& delta-delta\ntemporal": "the weights W ‚àà Rnem√ó4 of\nthe last\nfully connected layer\nwhich our SSA mechanism will extract emotion salient"
        },
        {
          "& delta-delta\ntemporal": "F C c, also called classiÔ¨Åcation layer, just before the softmax ac-\ninformation to inform the SER task. Once best accuracy"
        },
        {
          "& delta-delta\ntemporal": "tivation. Denoting W c ‚àà Rnem the c-ieth column of W , N\nis reached,\nthe model is saved and Ô¨Åxed. Then a second"
        },
        {
          "& delta-delta\ntemporal": "the batch size,\nthen for all\ninput batch x = [x1, ..., xN ], F C c\nis\ntrained\nACRNN model Cem (embedding size 128)"
        },
        {
          "& delta-delta\ntemporal": "is being fed with,\nthe constraint ensuring all classes are equi-\nconditionally to the BLSTM‚Äôs outputs of Csp by mean"
        },
        {
          "& delta-delta\ntemporal": "outputed can be expressed as follows.\nof the SSA mechanism for SER task."
        },
        {
          "& delta-delta\ntemporal": "(cid:32)\n(cid:33)‚àí1\nFor evaluations, 10-fold cross validation technique is per-"
        },
        {
          "& delta-delta\ntemporal": "N(cid:88) i\n4(cid:107)\n(8)\nformed. Respectively for IEMOCAP and Att-HACK, 9 and 18\n(cid:107)W c(cid:107)1 = N\nxi(cid:107)1"
        },
        {
          "& delta-delta\ntemporal": "=1\nspeakers are considered for\ntraining while the remaining ones"
        },
        {
          "& delta-delta\ntemporal": "are kept\nfor validation.\nIn the case of Att-HACK validation"
        },
        {
          "& delta-delta\ntemporal": "sets are balanced in gender\n(male-female couples) apart\nfrom\n3. Experiments"
        },
        {
          "& delta-delta\ntemporal": "2 folds, as the database includes 12 females and only 8 males."
        },
        {
          "& delta-delta\ntemporal": "3.1. Datasets"
        },
        {
          "& delta-delta\ntemporal": "As we aspire to perform emotion recognition independently of"
        },
        {
          "& delta-delta\ntemporal": "the speaker, we must consider two approaches for training the\nTo evaluate the performance of our proposed model, we perform"
        },
        {
          "& delta-delta\ntemporal": "speaker-independent SER experiments on the speech database\nSSA-CRNN-r Csp module."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "been performed linguistically on the whole database, all",
          "expression. This approach achieves a global absolute improve-": "ment of 60.82 % for Att-HACK and 29.52 % for\nIEMOCAP"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "speakers have been seen by Csp. Consequently, the SER",
          "expression. This approach achieves a global absolute improve-": "compared with ACRNN. Compared with the gender-informed"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "task is not performed independently of the speaker and",
          "expression. This approach achieves a global absolute improve-": "SER system by Zhang et\nal.\n[13],\nit\nachieves 13.10 % of"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "there is no insurance the model will be able to generalize",
          "expression. This approach achieves a global absolute improve-": "UAR improvement on IEMOCAP. As expected,\nthe speaker-"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "to speakers it has never seen.",
          "expression. This approach achieves a global absolute improve-": "dependant approach achieves slightly better performance than"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "the LOSO-based one although it does not seem signiÔ¨Åcant.\nIt"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "2.\nspeaker-independant approach : a LOSO (Leave-One-",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "appears that Csp is able to generalize enough for new speakers"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "Speaker-Out) protocol\nis followed so as to ensure that",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "so as the SSA can capture the speaker high-level features that"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "the speaker\n(or speakers for Att-HACK) on which the",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "are relevant in the representation of speech emotions."
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "model\nis validated has never been seen by the model.",
          "expression. This approach achieves a global absolute improve-": "FR\nDIST\nDOM\nSED\nFR\nDIST\nDOM\nSED"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "For each fold k ‚àà [1, 10]\nthe speaker sv\n(or speakers",
          "expression. This approach achieves a global absolute improve-": "FR\nFR\n0.34\n0.19\n0.35\n0.12\n0.94\n0.01\n0.02\n0.03"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "k",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "[sv1\n, sv2\n]) chosen to be in the validation set are left out\nk\nk",
          "expression. This approach achieves a global absolute improve-": "DIST\nDIST\n0.21\n0.27\n0.38\n0.14\n0.07\n0.88\n0.03\n0.02"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "of the database for the training and validation of Csp .",
          "expression. This approach achieves a global absolute improve-": "Att-HACK \nACRNN\nAtt-HACK \nSSA-CRNN (LOSO)"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "DOM\nDOM\n0.22\n0.17\n0.53\n0.07\n0.09\n0.04\n0.85\n0.02"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "Each module is optimized with respect to the cross-entropy",
          "expression. This approach achieves a global absolute improve-": "SED\nSED\n0.19\n0.18\n0.40\n0.23\n0.10\n0.03\n0.02\n0.85"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "objective function.\nTrainings are done with emotionally bal-",
          "expression. This approach achieves a global absolute improve-": "HAP\nANG\nSAD\nNEU\nHAP\nANG\nSAD\nNEU"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "HAP\nHAP\n0.96\n0.01\n0.02\n0.02\n0.61\n0.16\n0.05\n0.19"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "anced mini-batches of 40 samples, using the Adam optimizer",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "with Nestorov momentum.\nThe initial\nlearning rate is\nset\nto",
          "expression. This approach achieves a global absolute improve-": "IEMOCAP \nACRNN\nANG\nIEMOCAP \nSSA-CRNN (LOSO)\nANG\n0.01\n0.96\n0.01\n0.02\n0.09\n0.78\n0.04\n0.08"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "0.0001 and the momentum is set\nto 0.9.\nThe Ô¨Ånal model pa-",
          "expression. This approach achieves a global absolute improve-": "SAD\nSAD\n0.05\n0.02\n0.78\n0.15\n0.00\n0.00\n0.99\n0.01"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "rameters are selected by maximizing the Unweighted Average",
          "expression. This approach achieves a global absolute improve-": "NEU\nNEU\n0.21\n0.11\n0.19\n0.49\n0.01\n0.00\n0.04\n0.95"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "Recall (UAR) on the validation set.",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "Figure 3: SER normalized confusion matrices where each row"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "3.3. Experiment Results",
          "expression. This approach achieves a global absolute improve-": "presents the class dependant prediction performance."
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "Table 1 is divided in two parts,\nthe Ô¨Årst part shows the results",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "Finally,\nthe t-SNE analysis in Figure 4 tend to show both"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "in UAR on IEMOCAP of state-of-the-art methods for SER, in-",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "attitudes and emotions are well separated in their respective em-"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "cluding the ACRNN system by [10] and the gender-informed",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "beddings. Although this point would need to be deeper investi-"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "system by\n[13].\nThe\nsecond\npart\nshows\nthe UAR results",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "gated, we can further expect to use these embeddings in a voice"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "of our proposed methods on both Att-HACK and IEMOCAP",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "",
          "expression. This approach achieves a global absolute improve-": "conversion context as proposed by Zhou et al.\nin [22]."
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "databases.\nFirst, we compare our ACRNN re-implementation",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "with the\nstate-of-the-art method described in [10].\nOur\nre-",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "implementation achieves slightly better performance for IEMO-",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "CAP with an absolute improvement of 1.64% and a rather low",
          "expression. This approach achieves a global absolute improve-": "Att-HACK"
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "standard deviation possibly due\nto the\nincreased embedding",
          "expression. This approach achieves a global absolute improve-": ""
        },
        {
          "1.\nspeaker-dependant approach : a train/valid split have": "size.",
          "expression. This approach achieves a global absolute improve-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "standard deviation possibly due\nto the\nincreased embedding"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "size."
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "SER system\nAtt-HACK\nIEMOCAP"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "‚àí\n64.74 ¬± 5.44\nChen et al. [10]"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "‚àí\n69.32 ¬± 3.76\nMeng et al. [18]"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "‚àí\n82.80 ¬± ‚àí\nZhang et al. [13]"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "27.49 ¬± 3.26\n66.38 ¬± 2.72\nACRNN"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "66.36 ¬± 5.68\n86.35 ¬± 3.21\nACRNN-r"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "88.93 ¬± 4.36\n96.12 ¬± 4.27\nSSA-CRNN-r"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "88.31 ¬± 5.60\n95.90 ¬± 4.31\nSSA-CRNN-r (LOSO)"
        },
        {
          "CAP with an absolute improvement of 1.64% and a rather low": "Table 1: SER results with 95% conÔ¨Ådence interval for different"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "‚ÄúContext-aware attention mechanism for speech emotion recog-"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "nition,‚Äù\nin 2018 IEEE Spoken Language Technology Workshop"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "(SLT), 2018, pp. 126‚Äì131."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[10] M. Chen, X. He, J. Yang, and H. Zhang, ‚Äú3-d convolutional\nre-"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "current neural networks with attention model for speech emotion"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "recognition,‚Äù IEEE Signal Processing Letters, vol. 25, no. 10, pp."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "1440‚Äì1444, 2018."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[11] D. Ververidis, ‚ÄúAutomatic speech classiÔ¨Åcation to Ô¨Åve emotional"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "states based on gender information,‚Äù 01 2004."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[12]\nT. Vogt and E. Andr¬¥e, ‚ÄúImproving automatic emotion recognition"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "from speech via gender differentiaion,‚Äù in LREC, 2006."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[13]\nL. Zhang, L. Wang, J. Dang, L. Guo, and Q. Yu, Gender-Aware"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "CNN-BLSTM for Speech Emotion Recognition:\n27th Interna-"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "tional Conference on ArtiÔ¨Åcial Neural Networks, Rhodes, Greece,"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "October 4-7, 2018, Proceedings, Part I, 09 2018, pp. 782‚Äì790."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[14] Y. Li, T. Zhao, and T. Kawahara, ‚ÄúImproved end-to-end speech"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "emotion recognition using self attention mechanism and multitask"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "learning,‚Äù 09 2019, pp. 2803‚Äì2807."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[15] M. Sidorov, S. Ultes, and A. Schmitt, ‚ÄúEmotions are a personal"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "thing: Towards speaker-adaptive emotion recognition,‚Äù in 2014"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "Processing (ICASSP), 2014, pp. 4803‚Äì4807."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[16] G. Assunc¬∏ Àúao, P. Menezes, and F. PerdigÀúao, ‚ÄúSpeaker awareness"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "for speech emotion recognition,‚Äù International Journal of Online"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "and Biomedical Engineering (iJOE), vol. 16, p. 15, 04 2020."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[17] A. Nagrani,\nJ. S. Chung,\nand A. Zisserman,\n‚ÄúVoxceleb:\nA"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "Interspeech\nlarge-scale\nspeaker\nidentiÔ¨Åcation\ndataset,‚Äù\n2017,"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "Aug\n2017.\n[Online].\nAvailable:\nhttp://dx.doi.org/10.21437/"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "Interspeech.2017-950"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[18] H. Meng, T. Yan, F. Yuan, and H. Wei, ‚ÄúSpeech emotion recogni-"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "tion from 3d log-mel spectrograms with deep learning network,‚Äù"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "IEEE Access, vol. 7, pp. 125 868‚Äì125 881, 2019."
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "[19]\nZ. Pan, Z. Luo,\nJ. Yang, and H. Li, ‚ÄúMulti-modal attention for"
        },
        {
          "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,": "speech emotion recognition,‚Äù 2020."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "Speech Database with Social Attitudes,‚Äù",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "in\nSpeech Prosody,"
        },
        {
          "6. References": "[1] K. Scherer, ‚ÄúVocal affect expression:\na review and a model\nfor",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "Tokyo,\nJapan, May",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "2020.\n[Online]. Available:\nhttps://hal."
        },
        {
          "6. References": "future research.‚Äù Psychological bulletin, vol. 99 2, pp. 143‚Äì65,",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "archives-ouvertes.fr/hal-02508362",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "1986.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "[21] C.\nBusso,\nM.",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "Bulut,\nC.-C.\nLee,\nA.\nKazemzadeh,"
        },
        {
          "6. References": "[2]\nJ.-A. Bachorowski, ‚ÄúVocal expression and perception of emotion,‚Äù",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "E. Mower Provost, S. Kim, J. Chang, S. Lee, and S. Narayanan,"
        },
        {
          "6. References": "Current Directions\nin Psychological Science, vol. 8, no. 2, pp.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "‚ÄúIemocap: Interactive emotional dyadic motion capture database,‚Äù"
        },
        {
          "6. References": "53‚Äì57,\n1999.\n[Online].\nAvailable:\nhttps://doi.org/10.1111/",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "Language Resources and Evaluation, vol. 42, pp. 335‚Äì359, 12"
        },
        {
          "6. References": "1467-8721.00013",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "2008.",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[3] Q. Mao, M. Dong, Z. Huang, and Y. Zhan, ‚ÄúLearning salient fea-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "[22] K. Zhou, B. Sisman, R. Liu, and H. Li, ‚ÄúSeen and unseen emo-"
        },
        {
          "6. References": "tures for speech emotion recognition using convolutional neural",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "tional style transfer",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": "for voice conversion with a new emotional"
        },
        {
          "6. References": "networks,‚Äù IEEE Transactions on Multimedia, vol. 16, no. 8, pp.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "",
          "[20] C.\nLe Moine\nand": "speech dataset,‚Äù 2021.",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "2203‚Äì2213, 2014.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[4]\nJ. Lee and I. Tashev, ‚ÄúHigh-level feature representation using re-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "current neural network for speech emotion recognition,‚Äù 09 2015.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[5] G. Keren and B. Schuller,\n‚ÄúConvolutional\nrnn:\nAn enhanced",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "model\nfor extracting features from sequential data,‚Äù in 2016 In-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "ternational Joint Conference on Neural Networks (IJCNN), 2016,",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "pp. 3412‚Äì3419.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[6] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nico-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "laou, B. Schuller, and S. Zafeiriou, ‚ÄúAdieu features?\nend-to-end",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "speech emotion recognition using a deep convolutional recurrent",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "network,‚Äù in 2016 IEEE International Conference on Acoustics,",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "Speech and Signal Processing (ICASSP), 2016, pp. 5200‚Äì5204.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[7]\nS. Mirsamadi, E. Barsoum,\nand C. Zhang,\n‚ÄúAutomatic speech",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "emotion recognition using recurrent neural networks with local",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "attention,‚Äù 03 2017.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[8] M. Neumann and N. T. Vu, ‚ÄúAttentive convolutional neural net-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "work based speech emotion recognition: A study on the impact of",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "input features, signal length, and acted speech,‚Äù 2017.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[9] G. Ramet,\nP. N. Garner, M. Baeriswyl,\nand A.\nLazaridis,",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "‚ÄúContext-aware attention mechanism for speech emotion recog-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "nition,‚Äù\nin 2018 IEEE Spoken Language Technology Workshop",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "(SLT), 2018, pp. 126‚Äì131.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[10] M. Chen, X. He, J. Yang, and H. Zhang, ‚Äú3-d convolutional\nre-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "current neural networks with attention model for speech emotion",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "recognition,‚Äù IEEE Signal Processing Letters, vol. 25, no. 10, pp.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "1440‚Äì1444, 2018.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[11] D. Ververidis, ‚ÄúAutomatic speech classiÔ¨Åcation to Ô¨Åve emotional",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "states based on gender information,‚Äù 01 2004.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[12]\nT. Vogt and E. Andr¬¥e, ‚ÄúImproving automatic emotion recognition",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "from speech via gender differentiaion,‚Äù in LREC, 2006.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[13]\nL. Zhang, L. Wang, J. Dang, L. Guo, and Q. Yu, Gender-Aware",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "CNN-BLSTM for Speech Emotion Recognition:\n27th Interna-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "tional Conference on ArtiÔ¨Åcial Neural Networks, Rhodes, Greece,",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "October 4-7, 2018, Proceedings, Part I, 09 2018, pp. 782‚Äì790.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[14] Y. Li, T. Zhao, and T. Kawahara, ‚ÄúImproved end-to-end speech",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "emotion recognition using self attention mechanism and multitask",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "learning,‚Äù 09 2019, pp. 2803‚Äì2807.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[15] M. Sidorov, S. Ultes, and A. Schmitt, ‚ÄúEmotions are a personal",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "thing: Towards speaker-adaptive emotion recognition,‚Äù in 2014",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "IEEE International Conference on Acoustics, Speech and Signal",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "Processing (ICASSP), 2014, pp. 4803‚Äì4807.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[16] G. Assunc¬∏ Àúao, P. Menezes, and F. PerdigÀúao, ‚ÄúSpeaker awareness",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "for speech emotion recognition,‚Äù International Journal of Online",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "and Biomedical Engineering (iJOE), vol. 16, p. 15, 04 2020.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[17] A. Nagrani,\nJ. S. Chung,\nand A. Zisserman,\n‚ÄúVoxceleb:\nA",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "Interspeech\nlarge-scale\nspeaker\nidentiÔ¨Åcation\ndataset,‚Äù\n2017,",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "Aug\n2017.\n[Online].\nAvailable:\nhttp://dx.doi.org/10.21437/",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "Interspeech.2017-950",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[18] H. Meng, T. Yan, F. Yuan, and H. Wei, ‚ÄúSpeech emotion recogni-",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "tion from 3d log-mel spectrograms with deep learning network,‚Äù",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "IEEE Access, vol. 7, pp. 125 868‚Äì125 881, 2019.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "[19]\nZ. Pan, Z. Luo,\nJ. Yang, and H. Li, ‚ÄúMulti-modal attention for",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        },
        {
          "6. References": "speech emotion recognition,‚Äù 2020.",
          "[20] C.\nLe Moine\nand": "",
          "N.\nObin,\n‚ÄúAtt-HACK:\nAn\nExpressive": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Vocal affect expression: a review and a model for future research",
      "authors": [
        "K Scherer"
      ],
      "year": "1986",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "3",
      "title": "Vocal expression and perception of emotion",
      "authors": [
        "J.-A Bachorowski"
      ],
      "year": "1999",
      "venue": "Current Directions in Psychological Science",
      "doi": "10.1111/1467-8721.00013"
    },
    {
      "citation_id": "4",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Convolutional rnn: An enhanced model for extracting features from sequential data",
      "authors": [
        "G Keren",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "ternational Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "7",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "Automatic speech emotion recognition using recurrent neural networks with local attention"
    },
    {
      "citation_id": "9",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech"
    },
    {
      "citation_id": "10",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "G Ramet",
        "P Garner",
        "M Baeriswyl",
        "A Lazaridis"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "12",
      "title": "Automatic speech classification to five emotional states based on gender information",
      "authors": [
        "D Ververidis"
      ],
      "year": "2004",
      "venue": "Automatic speech classification to five emotional states based on gender information"
    },
    {
      "citation_id": "13",
      "title": "Improving automatic emotion recognition from speech via gender differentiaion",
      "authors": [
        "T Vogt",
        "E Andr√©"
      ],
      "year": "2006",
      "venue": "LREC"
    },
    {
      "citation_id": "14",
      "title": "Gender-Aware CNN-BLSTM for Speech Emotion Recognition: 27th International Conference on Artificial Neural Networks",
      "authors": [
        "L Zhang",
        "L Wang",
        "J Dang",
        "L Guo",
        "Q Yu"
      ],
      "year": "2018",
      "venue": "Gender-Aware CNN-BLSTM for Speech Emotion Recognition: 27th International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "15",
      "title": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Improved end-to-end speech emotion recognition using self attention mechanism and multitask learning"
    },
    {
      "citation_id": "16",
      "title": "Emotions are a personal thing: Towards speaker-adaptive emotion recognition",
      "authors": [
        "M Sidorov",
        "S Ultes",
        "A Schmitt"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Speaker awareness for speech emotion recognition",
      "authors": [
        "G Assunc ¬∏√£o",
        "P Menezes",
        "F Perdig√£o"
      ],
      "venue": "International Journal of Online and Biomedical Engineering (iJOE)"
    },
    {
      "citation_id": "18",
      "title": "Voxceleb: A large-scale speaker identification dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: A large-scale speaker identification dataset",
      "doi": "10.21437/Interspeech.2017-950"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal attention for speech emotion recognition",
      "authors": [
        "Z Pan",
        "Z Luo",
        "J Yang",
        "H Li"
      ],
      "year": "2020",
      "venue": "Multi-modal attention for speech emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "Att-HACK: An Expressive Speech Database with Social Attitudes",
      "authors": [
        "C Le Moine",
        "N Obin"
      ],
      "year": "2020",
      "venue": "Speech Prosody"
    },
    {
      "citation_id": "22",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Provost",
        "J Kim",
        "S Chang",
        "S Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "23",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset"
    }
  ]
}