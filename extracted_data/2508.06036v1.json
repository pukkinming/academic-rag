{
  "paper_id": "2508.06036v1",
  "title": "More Is Better: A Moe-Based Emotion Recognition Framework With Human Preference Alignment",
  "published": "2025-08-08T05:44:26Z",
  "authors": [
    "Jun Xie",
    "Yingjian Zhu",
    "Feng Chen",
    "Zhenghao Zhang",
    "Xiaohui Fan",
    "Hongzhu Yi",
    "Xinming Wang",
    "Chen Yu",
    "Yue Bi",
    "Zhaoran Zhao",
    "Xiongjun Guan",
    "Zhepeng Wang"
  ],
  "keywords": [
    "‚Ä¢ Human-centered computing ‚Üí Human computer interaction (HCI)",
    "‚Ä¢ Computing methodologies ‚Üí Artificial intelligence",
    "Natural language processing",
    "Computer vision MER 2025, multimodal emotion recognition, mixture of experts, semi-supervised learning, human preference alignment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present our solution for the semi-supervised learning track (MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the principle that \"more is better, \" to construct a robust Mixture of Experts (MoE) emotion recognition system. Our approach integrates a diverse range of input modalities as independent experts, including novel signals such as knowledge from large Vision-Language Models (VLMs) and temporal Action Unit (AU) information. To effectively utilize unlabeled data, we introduce a consensus-based pseudo-labeling strategy, generating high-quality labels from the agreement between a baseline model and Gemini, which are then used in a two-stage training paradigm. Finally, we employ a multi-expert voting ensemble combined with a rule-based re-ranking process to correct prediction bias and better align the outputs with human preferences. Evaluated on the MER2025-SEMI challenge dataset, our method achieves an F1-score of 0.8772 on the test set, ranking 2nd in the track. Our code is available at https://github.com/zhuyjan/MER2025-MRAC25.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is fundamental to human interaction and the development of empathetic human-computer interaction (HCI) systems  [13, 21] . Multimodal Emotion Recognition (MER)  [15] [16] [17] , a core area within affective computing, aims to interpret human emotional states by integrating multiple data channels, such as facial expressions, vocal intonation, and textual content. This research offers significant academic value and has substantial application potential in fields like smart driving  [7] , healthcare  [12] , and education  [19] . By enabling machines to understand and adapt to user emotions, MER facilitates more harmonious and effective human-machine collaboration.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Mixture Of Experts",
      "text": "Self-Attn &FFN Self-Attn &FFN",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self-Attn &Ffn",
      "text": "Reliance Voting",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Alignment With Human Preference",
      "text": "Gemini Modification The MER2025-SEMI Challenge  [14]  provides a limited amount of labeled training data and encourages participants to leverage large volumes of unlabeled data to improve model performance. Most previous studies have focused on selecting more effective encoders  [3, 22, 31, 32]  and designing complex fusion mechanisms  [8, 9, 25] . For instance, Qi et al.  [22]  adopted a prompt-learning approach to fine-tune CLIP, resulting in a more robust video encoder. Shi et al.  [25]  proposed an Audio-Guided Fusion strategy to integrate features from multiple modalities. However, excessive model complexity often leads to overfitting. Therefore, instead of designing overly complex fusion mechanisms or fine-tuning encoders, we adopt a simple multi-expert voting approach. This strategy enables multiple expert models to collaborate and make collective decisions, compensating for the limitations of individual models by leveraging the diversity and breadth of knowledge across the ensemble.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Post-Processing",
      "text": "Existing pseudo-labeling strategies for utilizing unlabeled data exhibit significant limitations. These methods  [8, 22]  typically depend on the predictions of pre-trained models to generate supplementary training data, thereby forming a self-reinforcing loop. This often results in \"confirmation bias, \" where the model keeps reinforcing its existing beliefs and struggles to learn new patterns. To break this cycle of knowledge closure, we introduce Gemini  [26] , a large vision-language model (VLM) with stronger generalization capabilities, to validate the model's predictions, thereby significantly improving the quality of pseudo-labels.\n\nFurthermore, annotator bias is a prevalent issue in emotion recognition. Due to individual differences and the inherent subjectivity of emotional perception, the GT labels provided by different annotators often contain bias. Consequently, models trained on such data inevitably exhibit systematic deviations from the ideal judgment. To address this issue, we acknowledge that relying solely on the model's learning capabilities is insufficient. Therefore, we propose a post-processing alignment step following the model's prediction.\n\nOur main contribution can be summarized as follows:\n\n‚Ä¢ Rather than fixating on the choice of input modality, or striving to identify the most optimal encoder, we adopt the principle of \"more is better. \" We advocate for integrating as many input branches as possible, treating each branch as an independent expert, ultimately forming a powerful Mixture of Experts (MoE) system. ‚Ä¢ To address the limitations of existing pseudo-labeling strategies, we propose a novel consensus-based pseudo-label generation method, along with a complementary two-stage training paradigm. This approach allows for more effective utilization of large-scale unlabeled data. ‚Ä¢ We introduce a post-processing strategy aimed at aligning predictions with human preferences. By calibrating the model's output distribution with the class distribution of the test dataset, we significantly enhance the final prediction accuracy.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method 2.1 More Signals, Broader Perspective",
      "text": "As illustrated in Fig.  1 , the core design of our method involves integrating as many sentiment analysis experts as possible to form a Mixture of Experts (MoE) system, which leverages their diverse knowledge perspectives to collaboratively produce the final prediction. Specifically, our model can integrate up to 18 different experts, and the \"more branches\" represent additional signals such as caption text, facial features, action unit analysis  [6, 20] , and others. The knowledge of each expert is extracted through their corresponding encoders  [11, 23, 30] , followed by self-attention and FFN. The Router then assigns weight scores to each expert by concatenating the features from all preceding branches and passing the combined representation through a linear layer. The Fused Expert is generated by concatenating all the features from the previous branches into a unified representation. The outputs of all experts are aggregated through a weighted sum to produce the emotion label. We experimented with various combinations of modalities and expert quantities. For the combinations achieving higher scores, their predictions are aligned with human judgment preferences through the post-processing.\n\nOur final architecture comprises six distinct input branches: fullframe images, cropped face images, caption text, audio stream, Gemini knowledge, and temporal AU (Action Unit) analysis. In contrast to the MER2025 baseline  [14] , which uses cropped facial images as visual input, the full-frame input includes entire video frames along with background information. Full-frame images provide richer contextual cues that potentially convey more comprehensive information about the individual than isolated facial features extracted through OpenFace  [1] . Additionally, our experiments reveal that face images of varying resolutions exhibit significant performance differences in unimodal evaluations. Therefore, we treat each resolution as a distinct feature branch. In addition to the modalities commonly used in previous works, we also propose two novel feature branches, described as follows.\n\n2.1.1 Gemini Knowledge. As illustrated in the blue part of Fig.  2 , we use a carefully designed prompt (refer to our code) to input raw videos with audio tracks into Gemini-2.5-Flash for emotion analysis. The model outputs its response in JSON format, which contains the following fields: (1) Reasoning Process: A detailed analysis of the main character's emotions based on facial expressions (action units), body posture, head pose, inter-frame relationships, audio features (pitch, speech rate, volume, and tone quality), and other relevant indicators. (2) Confidence Scores: Confidence levels for six emotions-neutral, angry, happy, sad, worried, and surprised. (3) Label: For ease of data processing, we instruct Gemini to output a final predicted emotion label. (4) Modality Contribution: The predictive influence of each modality is dynamic. For instance, audio cues might dominate in one clip, while visual information is more critical in another. To leverage this, we prompt Gemini to output its degree of reliance on each modality, enabling it to better weigh their specific insights for the final prediction. As shown in Fig.  1 , the resulting JSON file for each video clip is treated as a new signal branch and is fed into the Baichuan2-7B text encoder to extract textual features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Temporal Au Analysis.",
      "text": "Variations in action units (AUs) are crucial for analyzing human emotions. In previous studies, facial information has primarily been utilized by extracting frame-level visual features through visual encoders, which are then aggregated using global average pooling to obtain an utterance-level facial representation. While this approach has achieved reasonable performance, it overlooks important temporal dynamics that are critical in emotion recognition. To address this limitation, we propose temporal AU analysis as a novel signal branch.\n\nAs illustrated in the red region of Fig.  2 , the Action Unit (AU) information for each frame is encoded as a 35-dimensional vector, corresponding to the 35 defined facial AU components. This yields an initial input tensor with a shape of ùêµ√óùëá √ó35, where ùêµ denotes the batch size, ùëá is the number of frames. Subsequently, a linear layer projects these 35-dimensional vectors into a d-dimensional feature space, transforming the tensor's shape to ùêµ √ó ùëá √ó ùëë. To aggregate temporal features, a learnable [CLS] token with dimensions ùêµ √ó 1 √ó ùëë is prepended to the sequence, modifying the input shape to ùêµ √ó (ùëá + 1) √ó ùëë. The sequence is then processed using self-attention and a feed-forward network (FFN), which yields an output of the same dimensions. Finally, the representation corresponding to the [CLS] token is extracted to serve as the temporal AU feature.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "More Samples, Fuller Distribution",
      "text": "A primary challenge in MER2025-SEMI is the scarcity of labeled training data, which often leads to overfitting, especially with complex model architectures. A common strategy to address this is pseudo-labeling, where a model's own predictions on unlabeled data are used as new training targets. However, this approach has a fundamental limitation: it essentially confines the model to its learned data distribution and can amplify its inherent biases. To overcome this limitation and effectively leverage the unlabeled dataset, we introduce a novel two-stage training paradigm centered on a consensus-based pseudo-labeling strategy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pseudo",
      "text": "Labeling. First, we train an efficient baseline model purely on the provided labeled set. This model, which represents an optimal trade-off between performance and complexity, is then used to generate predictions for the entire unlabeled test set. Concurrently, we leverage the powerful capabilities of the Gemini to obtain predictions for the same data. The core of our strategy lies in the subsequent filtering step: we create a high-confidence pseudo-labeled dataset by selecting only those samples where the predictions from our baseline model and Gemini are identical. This consensus between a task-specific model and a large VLM acts as a powerful filter, significantly increasing the quality and reliability of the labels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Two-Stage Training.",
      "text": "Although the generated pseudo-labeled data are of high quality, they may still exhibit distributional bias relative to the given labeled data. To mitigate the potential impact of this bias on the labeled data, we adopt a two-stage training paradigm. First, we pretrain the model on the pseudo-labeled data, enabling it to learn more robust and generalizable feature representations from a larger and more diverse dataset. Then, we fine-tune the model on the labeled training data using a learning rate that is 10 times smaller than that used during pretraining.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "More Deliberation, Less Bias",
      "text": "To overcome the limitations inherent in a single model, we introduced a post-processing strategy aimed at achieving more robust outcomes. This approach effectively reduces the influence of random biases introduced by individual models. By averaging or weighting their predictions, the system achieves more robust and consistent outputs, even in the presence of noisy or ambiguous data  [29] . Moreover, this strategy enhances the generalization ability of the overall framework, as the diverse decision patterns of the experts can better adapt to heterogeneous or unseen samples. Specifically, we first determined the individual prediction accuracy of each expert and utilized this as a confidence measure to perform weighted voting on the aggregated results. Let ≈∑ùëñ and ùë¶ ùëñ represent the prediction results and the actual emotions, respectively, the reliability ùëü of a certain expert ùëí is calculated as\n\nwhere I is the indicator function, ùëõ is total number of samples evaluated. The final result is classified based on\n\nwhere ùëÄ is the total number of experts involved in decision-making, ùëù ùëò is the probability of the ùëò-th emotion. Notably, Equation 2 yielded significantly improved results without training, yet we observed a strong bias towards predicting 'neutral'. A plausible explanation is that neutral resides at the 'center' of the emotional spectrum, making it inherently more susceptible to confusion and misjudgment compared to other, more distinct emotional states. On the other hand, we observed that the closed-source VLM demonstrated better performance on emotions with relatively clearer decision boundaries, such as 'angry', 'happy' and 'surprise'. However, it struggled to differentiate between 'neutral' and other emotions, as well as between 'worried' and 'sad'. Therefore, we further introduced rule-based adjustments to re-rank the results. Based on the confusion matrix of emotion classification and the category ratio of predicted results and ground truth, the final rule adopted is as follows:\n\n‚Ä¢ When the highest number of votes is 'neutral' and the second highest is 'angry', modify it to 'angry'.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "All training processes are performed with an initial learning rate of 1e-3 (end of 1e-4), cosine annealing scheduler, default AdamW optimizer and batch size of 256 until convergence (about 10 epochs).\n\nDuring the preparation phase of multi-stage training, we first train an initial model using the Train & Val dataset. Next, we generate pseudo labels for 20,000 samples and train a new model with these pseudo labels to equip it with foundational emotion recognition capabilities. Finally, we fine-tune the model using high-quality labels from the Train & Val dataset to correct the bias and enhance its performance. By selecting different branch features and dividing the training sets in various ways, we ultimately developed 15 distinct models. Their performance was further enhanced through the ensemble strategy and reordering rules described in Section 2.3.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "In experiments, the impact of single-modal encoder was first evaluated and then fixed. Table  2  provides a relative comparison of leading models for reference. On this basis, we make incremental  adjustments to the model structure, loss function, and voting strategy. The key ablation experiment results are presented in Table  3 .\n\nThe second group shows that adjusting the model structure and loss function improved the baseline by 1%. This improvement is attributed to two factors: (1) the finer-grained attention interaction mechanism of the transformer, and (2) the loss function's role in balancing emotion categories with varying proportions and difficulty levels. Furthermore, incorporating new input signals has led to remarkable improvements. One possible explanation is that using frozen parameters in the encoder may result in the loss of emotion related representations. The additional input features effectively supplement valuable missing information. Finally, with the support of pre-training and model integration, the accuracy of our solution has been significantly enhanced once again. This strongly validates our analysis in Sections 2.2 and 2.3.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a Mixture of Experts (MoE) system for multimodal emotion recognition that integrates a wide array of input modalities, including novel signals like VLM-generated knowledge and temporal AU analysis. To effectively leverage the vast amount of unlabeled data, we introduced a consensus-based pseudo-labeling strategy coupled with a two-stage training paradigm, significantly enhancing the model's robustness and generalization. Finally, we addressed inherent model and annotator biases through a multi-stage post-processing pipeline. Extensive experiments validate the effectiveness of each component, culminating in a final F-score of 0.8772, ranking 2nd in the MER-SEMI 2025 track.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of our proposed multimodal sentiment analysis framework. The model consists of three",
      "page": 2
    },
    {
      "caption": "Figure 1: , the core design of our method involves",
      "page": 2
    },
    {
      "caption": "Figure 2: More signal examples in our method.",
      "page": 3
    },
    {
      "caption": "Figure 1: , the resulting JSON file for each video clip is treated as a",
      "page": 3
    },
    {
      "caption": "Figure 2: , the Action Unit (AU)",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Temporal AU Analysis": "AU01r"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Face at Different Resolution"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE"
    },
    {
      "citation_id": "2",
      "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "authors": [
        "Zhe Chen",
        "Weiyun Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Zhangwei Gao",
        "Erfei Cui",
        "Jinguo Zhu",
        "Shenglong Ye",
        "Zhaoyang Hao Tian",
        "Liu"
      ],
      "year": "2024",
      "venue": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "arxiv": "arXiv:2412.05271"
    },
    {
      "citation_id": "3",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "5",
      "title": "Pretraining with whole word masking for chinese bert",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Ting Liu",
        "Bing Qin",
        "Ziqing Yang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "7",
      "title": "Multimodal driver emotion recognition using motor activity and facial expressions",
      "authors": [
        "Carlos H Espino-Salinas",
        "Huizilopoztli Luna-Garc√≠a",
        "M Jos√©",
        "Cristian Celaya-Padilla",
        "Nadia Barr√≠a-Huidobro",
        "Karina Rosales",
        "David Rondon",
        "Klinge Orlando Villalba-Condori"
      ],
      "year": "2024",
      "venue": "Frontiers in Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Leveraging contrastive learning and self-training for multimodal emotion recognition with limited labeled samples",
      "authors": [
        "Qi Fan",
        "Yutong Li",
        "Yi Xin",
        "Xinyu Cheng",
        "Guanglai Gao",
        "Miao Ma"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Early joint learning of emotion information makes multimodal model understand you better",
      "authors": [
        "Mengying Ge",
        "Mingyang Li",
        "Dongkai Tang",
        "Pengbo Li",
        "Kuo Liu",
        "Shuhao Deng",
        "Songbai Pu",
        "Long Liu",
        "Yang Song",
        "Tao Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Pengcheng Guo",
        "Shixing Liu"
      ],
      "year": "2022",
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "12",
      "title": "Multimodal Interpretable Depression Analysis Using Visual, Physiological, Audio and Textual Data",
      "authors": [
        "Puneet Kumar",
        "Shreshtha Misra",
        "Zhuhong Shao",
        "Bin Zhu",
        "Balasubramanian Raman",
        "Xiaobai Li"
      ],
      "year": "2025",
      "venue": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "13",
      "title": "MIE-Net: Motion information enhancement network for fine-grained action recognition using RGB sensors",
      "authors": [
        "Yutong Li",
        "Miao Ma",
        "Jie Wu",
        "Kaifang Yang",
        "Zhao Pei",
        "Jie Ren"
      ],
      "year": "2024",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "14",
      "title": "Mer 2025: When affective computing meets large language models",
      "authors": [
        "Zheng Lian",
        "Rui Liu",
        "Kele Xu",
        "Bin Liu",
        "Xuefei Liu",
        "Yazhou Zhang",
        "Xin Liu",
        "Yong Li",
        "Zebang Cheng",
        "Haolin Zuo"
      ],
      "year": "2025",
      "venue": "Mer 2025: When affective computing meets large language models",
      "arxiv": "arXiv:2504.19423"
    },
    {
      "citation_id": "15",
      "title": "Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM international conference on multimedia"
    },
    {
      "citation_id": "16",
      "title": "Mer 2024: Semisupervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "18",
      "title": "Focal loss for dense object detection",
      "authors": [
        "Tsung-Yi Lin",
        "Priya Goyal",
        "Ross Girshick",
        "Kaiming He",
        "Piotr Doll√°r"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "19",
      "title": "Advancing education through tutoring systems: A systematic literature review",
      "authors": [
        "Vincent Liu",
        "Ehsan Latif",
        "Xiaoming Zhai"
      ],
      "year": "2025",
      "venue": "Advancing education through tutoring systems: A systematic literature review",
      "arxiv": "arXiv:2503.09748"
    },
    {
      "citation_id": "20",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Thibaud Senechal",
        "May Amr",
        "Jeffrey Cohn",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition framework using multiple modalities for an effective humancomputer interaction",
      "authors": [
        "Anam Moin",
        "Farhan Aadil",
        "Zeeshan Ali",
        "Dongwann Kang"
      ],
      "year": "2023",
      "venue": "The Journal of Supercomputing"
    },
    {
      "citation_id": "22",
      "title": "Multimodal emotion recognition with vision-language prompting and modality dropout",
      "authors": [
        "Anbin Qi",
        "Zhongliang Liu",
        "Xinyong Zhou",
        "Jinba Xiao",
        "Fengrun Zhang",
        "Qi Gan",
        "Ming Tao",
        "Gaozheng Zhang",
        "Lu Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning (ICML)"
    },
    {
      "citation_id": "24",
      "title": "LMFLOSS: A hybrid loss for imbalanced medical image classification",
      "authors": [
        "Adnan Abu",
        "Labib Sadi",
        "Nusrat Chowdhury",
        "Jahan"
      ],
      "year": "2022",
      "venue": "LMFLOSS: A hybrid loss for imbalanced medical image classification",
      "arxiv": "arXiv:2212.12741"
    },
    {
      "citation_id": "25",
      "title": "Audio-guided fusion techniques for multimodal emotion analysis",
      "authors": [
        "Pujin Shi",
        "Fei Gao"
      ],
      "year": "2024",
      "venue": "Audio-guided fusion techniques for multimodal emotion analysis",
      "arxiv": "arXiv:2409.05007"
    },
    {
      "citation_id": "26",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew Dai",
        "Anja Hauth",
        "Katie Millican"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "27",
      "title": "Bloom: A 176b-parameter open-access multilingual language model",
      "authors": [
        "Bigscience Workshop",
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Iliƒá",
        "Roman Hesslow",
        "Alexandra Castagn√©",
        "Fran√ßois Sasha Luccioni",
        "Yvon"
      ],
      "year": "2022",
      "venue": "Bloom: A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "28",
      "title": "FG-CLIP: Fine-Grained Visual and Textual Alignment",
      "authors": [
        "Chunyu Xie",
        "Bin Wang",
        "Fanjing Kong",
        "Jincheng Li",
        "Dawei Liang",
        "Gengshen Zhang",
        "Dawei Leng",
        "Yuhui Yin"
      ],
      "venue": "Forty-second International Conference on Machine Learning"
    },
    {
      "citation_id": "29",
      "title": "Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of Large Models via Differentiated Thinking and Complementary Ensembles",
      "authors": [
        "Jun Xie",
        "Xiongjun Guan",
        "Yingjian Zhu",
        "Zhaoran Zhao",
        "Xinming Wang",
        "Hongzhu Yi",
        "Feng Chen",
        "Zhepeng Wang"
      ],
      "year": "2025",
      "venue": "Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of Large Models via Differentiated Thinking and Complementary Ensembles",
      "arxiv": "arXiv:2505.16784"
    },
    {
      "citation_id": "30",
      "title": "Open largescale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open largescale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "31",
      "title": "Emotion-anchored contrastive learning framework for emotion recognition in conversation",
      "authors": [
        "Fangxu Yu",
        "Junjie Guo",
        "Zhen Wu",
        "Xinyu Dai"
      ],
      "year": "2024",
      "venue": "Emotion-anchored contrastive learning framework for emotion recognition in conversation",
      "arxiv": "arXiv:2403.20289"
    },
    {
      "citation_id": "32",
      "title": "Improving multimodal emotion recognition by leveraging acoustic adaptation and visual alignment",
      "authors": [
        "Zhixian Zhao",
        "Haifeng Chen",
        "Xi Li",
        "Dongmei Jiang",
        "Lei Xie"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    }
  ]
}