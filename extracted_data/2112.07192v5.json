{
  "paper_id": "2112.07192v5",
  "title": "Date Of Publication Xxxx 00, 0000, Date Of Current Version Xxxx 00, 0000",
  "published": "2021-12-14T06:54:08Z",
  "authors": [
    "Naoki Takashima",
    "Frédéric Li",
    "Marcin Grzegorzek",
    "Kimiaki Shirahama"
  ],
  "keywords": [
    "Music emotion recognition",
    "Embeddings",
    "Canonical correlation analysis",
    "Kullback-Leibler divergence",
    "Bidirectional retrieval"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Most music emotion recognition approaches perform classification or regression that estimates a general emotional category from a distribution of music samples, but without considering emotional variations (e.g., happiness can be further categorised into much, moderate or little happiness). We propose an embedding-based music emotion recognition approach that associates music samples with emotions in a common embedding space by considering both general emotional categories and fine-grained discrimination within each category. Since the association of music samples with emotions is uncertain due to subjective human perceptions, we compute composite loss-based embeddings obtained to maximise two statistical characteristics, one being the correlation between music samples and emotions based on canonical correlation analysis, and the other being a probabilistic similarity between a music sample and an emotion with KL-divergence. The experiments on two benchmark datasets demonstrate the effectiveness of our embedding-based approach, the composite loss and learned acoustic features. In addition, detailed analysis shows that our approach can accomplish robust bidirectional music emotion recognition that not only identifies music samples matching with a specific emotion but also detects emotions expressed in a certain music sample.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "M USIC is a powerful means to evoke human emotions.\n\nAnalysing the interactions between them is thus important in affective computing, and is one of main focuses of Music Emotion Recognition (MER) which attempts to automatically identify the emotion matching a specific music  [1] . MER is useful for many potential applications such as music recommendation and playlist generation for streaming services, and even music therapy in biomedicine  [2] .\n\nMER is performed differently in the literature depending on several factors. The first one is how emotions are modelised. Two main frameworks to define emotions currently co-exist: the categorical and the continuous ones. The former defines emotions as explicit categories, either directly using the six 'basic' emotions highlighted in Ekman's theory (i.e., happiness, sadness, anger, fear, disgust and surprise)  [3]  or derivatives from them. The latter decomposes emotions along several axes, among which the most popular axes are arousal (level of energy) and valence (level of pleasantness) based on Russell's Circumplex model  [4] . Both of the categorical and continuous frameworks have their pros and cons. While the former can clearly identify general emotions in music, it is not the most appropriate to take into account the richness and variations of human emotions. For example, there are several degrees of happiness ranging from little to intense happiness, that cannot be distinguished from each other with the cate-gorical models. On the other hand, the continuous approach can express fine-grained human emotions in a vector space defined by the arousal and valence axes. However, it is difficult to identify general emotions because dissimilar emotions such as 'fear' and 'anger' are located close to each other in the arousal-valence space  [5] . Therefore, neither categorical nor continuous approach has become predominant over the other in the literature, despite the benefits of each approach being essential for MER.\n\nThe second main difference among MER work lies in how MER is translated into a machine learning problem. The most popular approaches so far have consisted in considering MER either as a classification or a regression problem, depending on whether the categorical or continuous modelisation of emotions is used  [6] . Classification and regression methods are however unidirectional, and mostly investigate Music to Emotion (M2E). In this framework, a classification or regression model is trained to respectively output categorical emotion estimations or arousal-valence intensity scores given some music-related input data (e.g., audio records, lyrics transcripts, playlist information, etc.). On the other hand, Emotion to Music (E2M) aims to retrieve some relevant music extract given some emotion-related input remains more marginal. More recently, embedding-based retrieval approaches have emerged to address this issue. In a retrieval problem, a model is trained to return a list of examples ordered in a descending order of similarities to an input example, also referred to as a query  [7] . For M2E, the query and retrieved examples are respectively music-related data and emotion, while the reverse is true for E2M. Embeddingbased methods on the other hand aim to project examples from various modalities into a common space referred to as an embedding space, so that the embeddings (i.e. vectorial projections) of two data examples associated to the same concept are close to each other  [8] . The combination of embedding and retrieval approaches enables bidirectional retrieval, allowing retrieval methods to perform either E2M or M2E in the case of MER, and consequently has led to an increased interest from the research community over the past years  [9] -  [14] . But while audio, image and text modalities are the most popular in the literature to the best of our knowledge, no other work has so far attempted to investigate bidirectional retrieval between audio and emotion ratings, except for our previous study  [15] .\n\nIn this paper, we propose an Embedding-based Music Emotion Recognition (EMER) approach that performs bidirectional retrieval based on the continuous model of emotions. Our approach can directly analyse the similarity between music and emotion in the embedding space, where an embedding designates here the vector representation of a music sample or an emotion in the embedding space. Fig.  1  shows a standard EMER approach that projects music samples and emotions into an embedding space, in such a way that associated music and emotions are close to each other in the embedding space. This allows it to identify general emotions because similar emotions are gathered close\n\nPrinciple of an embedding-based retrieval approach for MER. A music encoder and an emotion encoder are first jointly trained to project associated music samples and emotions close to each other in an embedding space. The trained encoders are then used to obtain embeddings of the query and test samples. The test samples are finally ordered by decreasing similarity to the query in the embedding space.\n\nto each other in terms of their embeddings. It can be noted that by projecting highly associated music samples and specific emotions in proximity, their fine-grained relations are preserved in the embedding space. This way, EMER can treat both of general and specific emotions. Once both music and emotion encoders are trained, they can be used to obtain the embeddings of a query and test samples, and rank the latter by decreasing similarities to the query in the embedding space.\n\nOne challenge when working directly with emotional ratings is that they are inherently uncertain, because they are subjectively annotated according to human perceptions which are highly influenced by many factors such as age, personality, cultural background and surrounding conditions  [1] ,  [16] . We refer to this phenomenon as emotional uncertainty. On the one hand, some existing MER approaches bypass this problem by extracting emotion-related information from sources that are more reliable than individual subjective reports, such as music tags  [14] ,  [17]  or lyrics  [12] ,  [13] . On the other hand, many other past work directly use subjective emotion ratings without considering emotional uncertainty, and assume that the provided annotations are completely correct. This uncertainty could however cause the trained models to be either inaccurate or biased, and thus negatively impact the recognition performances.\n\nTo mitigate the impact of emotional uncertainty, we develop an approach, called EMER using Composite Loss (EMER-CL) that trains music and emotion embeddings with a compound loss examining two statistical characteristics that are affected in a limited way by possible inaccuracies in the emotional annotations. Firstly, we assume that even if emotional intensities differ from user to user in terms of intensity values, they remain nevertheless correlated when listening to the same music sample. Thus, Canonical Correlation Analysis (CCA) is used to devise a correlationbased loss  [18] . This loss enables us to deal with intersubject variations in emotional intensities by maximising the correlation between music samples and their associated emotions in an embedding space, so as to find their 'relative' connection. That is, the embedding space characterises how acoustic features change according to an increase/decrease in arousal/valence intensities and vice versa. Secondly, music samples yielding very different acoustic features can evoke similar emotions. For instance, happiness can be expressed in different genres like rock, blues and jazz. This kind of large intra-class variation in acoustic features for one emotional category makes projecting a music sample or an emotion into a single point (as shown in Fig.  1 ) suboptimal. Thus, we additionally project each of music samples and emotions as a probability distribution in another embedding space  [19] ,  [20] . This idea is implemented by defining a distributionbased loss that measures the Kullback-Leibler (KL) divergence between the probability distribution for a music sample and the one for an emotion in the embedding space.\n\nOur composite loss consisting of the correlation-and distribution-based losses is necessary for managing the aforementioned inter-subject and intra-class variations resulting from the emotional uncertainty. Only using the correlation-based loss cannot cover the large intra-class variation of acoustic features, while the inter-subject variation of emotional intensities cannot be handled only with the distribution-based loss. The experimental results in Section IV-D validate the necessity of combining the correlationand distribution-based losses.\n\nTo sum up, this paper contains the following three main contributions: Firstly, we propose EMER-CL that can work with both general and specific emotions since it uses the continuous model of emotions to obtain embeddings of emotions, and the embedding space maintains not only associated music samples and emotions close to each other but also non-associated ones far away. The embedding space serves as a bridge between music samples and emotions and offers bidirectional MER as a by-product of EMER-CL. Secondly, we propose a new composite loss combining the CCA and KL-divergence losses to take into account the emotional uncertainty. Finally, we perform extensive experiments on two benchmark datasets, MediaEval Database for Emotional Analysis in Music (DEAM)  [21]  and PMEmo  [22] . We demonstrate the effectiveness of EMER-CL over regression baselines not relying on embeddings, of the composite loss over other alternatives and of the features learned by EMER-CL relatively to the state-of-the-art MER methods. In addition, detailed analysis of EMER-CL results reveals that reasonable recognition is robustly attained even in cases of mis-recognition.\n\nThis paper is organised as follows: Section II reviews the literature of exsting MER approaches grouped into several categories. Section III details our EMER-CL and Section IV reports the experimental results demonstrating its effectiveness. Detailed analysis of recognition results by EMER-CL is conducted in Section V. Finally, Section VI presents the conclusion and our future work. In addition to these main contents, several appendices are provided to show experimental details, such as hyper-parameter tuning for EMER-CL and the comparison approaches involved in the comparative studies in Section IV, additional insights and results for the detailed analysis in Section 5, and the computational cost of EMER-CL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "This section provides a short review of existing MER approaches by dividing them into M2E and E2M. We first review existing M2E approaches by classifying them into three categories: \"feature engineering\" that hand-crafts emotionrelated acoustic features, \"feature learning\" based on deep learning that automatically learns emotion-related features, and \"relation modelling\" to extract the relationship between emotional intensities and acoustic features obtained by feature engineering or learning. We then discuss past work dealing with E2M. Through this review, we clarify the novelties of the proposed EMER-CL. Feature engineering for M2E: Several libraries like MIRtoolbox  [23]  and openSMILE  [24]  are currently available to extract fundamental acoustic features such as Zero-Crossing Rate (ZCR), Root-Mean-Square (RMS) energy, Mel-Frequency Cepstral Coefficients (MFCCs), Short-Time Fourier Transform (STFT), etc. However, acoustic signal analysis alone might not be enough to account for all required acoustic characteristics  [5] . As a result, a large focus of M2E approaches has been put on feature engineering in recent years. Panda et al.  [25]  distinguished several types of emotion-related acoustic features including spectral features (low-level feature), rhythm clarity (perceptual feature) and genre (high-level semantic feature). Mo and Niu  [26]  presented an acoustic feature extraction technique that combines three signal processing algorithms, the orthogonal matching pursuit, Gabor functions, and the Wigner distribution function, to provide an adaptive time-varying description of music signals with a higher spatial and temporal resolution. Panda et al.  [27]  proposed algorithms to extract acoustic features related to musical texture and expressive performance techniques (e.g., vibrato, tremolo and glissando).\n\nThe aforementioned work mainly focuses on designing acoustic features and feature selection to effectively estimate an emotion from a music sample, but it can be claimed that feature engineering does not inherently take into account the emotional uncertainty unlike our EMER-CL approach. Feature Learning for M2E: The advantage of feature learning is the ability to capture high-level features from raw data or hand-crafted (low-level) features. Feature learning for M2E has become a fast moving research topic due to the increasing interest in deep learning over the past decade. For this reason, we report only the most recent work (i.e. less than five-year-old) that we found related to this topic. Malik et al.  [28]  demonstrated the effectiveness of stacking a Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) to predict arousal/valence from acoustic features exclusively based on log mel-band energy. Dong et al.  [29]  developed a Bidirectional Convolutional Recurrent Sparse Network (BCRSN) that uses the spectrogram of audio signals and reduces computational complexity by converting the continuous arousal/valence prediction process to multiple binary classification problems. Sarkar et al.  [30]  applied a CNN taking log-mel spectrogram as input to the four-class classification problem defined by Russell's model quadrants  [4] . Hizlisoy et al.  [2]  proposed a Convolutional Long short term memory Deep Neural Network (CLDNN) for the classification of three quadrants excluding low arousal -high valence from Russell's model quadrants  [4] . Choi et al.  [31]  presented a transfer learning approach where a CNN taking mel-spectrograms as input is firstly trained for a music tagging task, and then transferred and fine-tuned for six other tasks such as music genre classification, speech/music classification, emotion prediction etc. Koh et al.  [32]  presented a comparison of state-of-the-art deep feature learning architectures including VGGish and L 3 -Net that take audio spectrograms as inputs. The two models outperformed MFCCs features on various M2E datasets for either classification or regression tasks. Orjesek et al.  [33]  proposed two deep-learning-based architectures to learn features for M2E as a regression problem. The first one is based on a CNN stacked with a bidirectional Gated Recurrent Unit (GRU) and Multi-Layer Perceptron (MLP). The other has the same architecture, except that an autoencoder is inserted between the CNN and bidrectional GRU. The ensemble is trained by adding a reconstruction term to the loss function. He et al.  [34]  proposed a two-stage approach for the classification of low/high arousal and valence. Log-mel spectrograms obtained from the raw audio signals are first used to trained a convolutional autoencoder. The encoder is then used to train two bidirectional Long Short-term Memory (LSTM), one for arousal and the other for valence classification.\n\nThe aforementioned approaches do not take into account the emotional uncertainty. On the other hand, our EMER-CL approach uses high-level acoustic features learned by a pre-trained VGGish model  [35] , and takes advantage of the composite loss to deal with the emotional uncertainty. Relation modelling for M2E: Past work has also investigated relationships between music samples and emotions, although the lower popularity of this topic in MER research compared to feature engineering or feature learning means that this field is moving at a slower pace. Yang et al.  [36]  built a group-wise MER scheme (GWMER) which divides users into various groups based on user information such as generation, gender, occupation and personality, and trains a Support Vector Regression (SVR) for the prediction of arousal/valence for each group. GWMER can this way partially address the problem that continuous emotions are more affected by subjective issue than discrete emotions when annotating. Yang and Chen  [37]  presented a ranking-based neural network model that ranks a collection of music samples by emotion and determines the emotional intensity of each music sample. Yang and Chen  [38]  and Chin et al.  [39]  developed probabilistic approaches to deal with the emotional uncertainty by estimating the distribution of emotional intensities from hand-crafted acoustic features. Markov and Matsui  [40]  showed that modelling with Gaussian Processes (GP) was more powerful than SVR for arousal/valence regression with hand-crafted acoustic features. Fukayama and Goto  [41]  evaluated the effectiveness of aggregating multiple GP regressions, each trained with different acoustic features. Wang et al.  [42]  presented Acoustic Emotion Gaussians (AEGs) that treat the emotional uncertainty by modelling hand-crafted acoustic features as a parametric probability distribution (soft assignment) instead of a single point (hard assignment). Wang et al.  [43]  proposed a Histogram Density Mixture (HDM) model that quantises the arousal/valence space into cells and extracts latent histograms representing characteristic emotion distributions over cells based on handcrafted acoustic features. Wang et al.  [44]  developed a MER system for 34 emotional categories based on Hierarchical Dirichlet Process Mixture Model (HDPMM) that links emotion classes using the property of sharing components in the HDPMM.\n\nTo the best of our knowledge, relation modelling approaches have so far exclusively relied on feature engineering, and not yet been used with high-level features obtained by feature learning. On the other hand, our EMER-CL approach uses high-level acoustic features based on VGGish, and projects emotional intensities into an embedding space, which enables us to extract high-level feature representations for emotions. E2M: E2M has not been explored as extensively as M2E, especially not in the recent MER literature. One possible reason for this scarcity is the fact that the existing acoustic features associated with emotions are high dimensional, and thus not easy to predict directly from emotions using traditional machine learning methods like regression. Except for studies older than a decade  [45] -  [48] , we could find only a single recent method incorporating E2M elements proposed by Deng et al.  [47] . In this work, a music recommendation method taking into account the emotions of the user is proposed. An M2E model is first trained using a classification framework to predict the emotional state associated with a music sample. The model is then used to predict an emotional state sequence containing the emotions associated with the last songs listened by the user. A model based on Conditional Random Fields is used to predict the user's current emotion based on this emotional state sequence. Finally, the similarities between the predicted current emotion and the ones associated to songs of the dataset are computed to retrieve relevant songs to be suggested.\n\nIn this paper, we propose a new bidirectional MER approach able to perform either M2E and E2M based on projecting music samples and associated emotions in proximity in two embeddings spaces. Unlike Deng et al., our approach can directly perform E2M without relying on a M2E system. Embedding-based recognition: Approaches in this category have attracted much attention as techniques that can perform effective bidirectional recognition between different modalities (e.g., image, text and audio). When it comes to approaches involving audio modality, the most common investigations include embedding-based recognition between audio and image  [9] ,  [10] , between audio and text (lyrics)  [11] ,  [12] , or even between all three of image, audio and text  [13] . Some attempts have also focused on extracting meaningful embeddings from music meta-data (e.g. genre, instrument, mood/theme) and playlist information  [14] . However to the best of our knowledge, no existing work addresses embedding-based recognition between audio and emotion except our previous study  [15] , where MLPs trained with the CCA loss are used to compute embeddings of music samples and emotions. This paper is an extension of our previous study by adopting RNNs in addition to MLPs, devising a composite loss that combines the CCA and KL-divergence losses, and conducing significantly deeper analysis of experimental results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Emer-Cl Approach",
      "text": "Fig.  2  shows an overview of our EMER-CL approach. First, a music sample is converted into a sequence of acoustic features\n\nTm of length T m . Here,\n\nx -dimensional feature vector at time t (1 ≤ t ≤ T m ). In our implementation, VGGish produced by Google  [35]  is used to segment every music sample recorded with a sampling rate of 44.1kHz into segments of 0.96 seconds, and then x (m) t is extracted as a 128-dimensional vector (i.e., D (m) x = 128) from each segment. The emotion associated with the music sample is represented as a sequence\n\nTe of length T e where x\n\nx -dimensional vector containing characteristics of the emotion at time t (1 ≤ t ≤ T e ). In our setting, x (e) t is defined as a two-dimensional vector (i.e., D (e) x = 2) indicating arousal and valence intensities recorded with a sampling rate of 2Hz. Unlike X (m) , we do not perform feature extraction on the raw arousal/valence intensities and use the latter directly as X (e) . This is because we consider that only two types of intensities recorded with a low sampling rate have relatively simple characteristics. Note that for simplicity X (e) is called an arousal/valence sequence in the following discussions.\n\nIt is cumbersome to directly project X (m) and X (e) into an embedding space because they are sequences of different lengths T m and T e . Thus, as shown in Fig.  2    An overview of our EMER-CL approach. X (m) and X (e) respectively designate the music and emotion sequences input to the music and emotion encoders. v (m) and v (e) are the vectorial outputs of the music and emotion encoders, respectively. Branches of fully connected (FC) layers project v ( * ) into a point-based embedding φ ( * ) in the space C (cca) , and a probabilistic-based embedding following a multivariate Gaussian distribution N (µ ( * ) , σ ( * ) ) in the space C (kl) , for * ∈ {m, e}. Both music and emotion models are jointly trained to minimise a composite loss computing the CCA loss between φ (m) and φ (e) , and a KL-divergence loss between N (µ (m) , σ (m) ) and N (µ (e) , σ (e) ).\n\nmusic and emotion encoders are used to transform X (m)  and\n\nv , respectively. Each of v (m) and v (e) is a high-level feature that effectively summarises the features in X (m) or X (e) and their temporal relations. We use either an MLP or RNN based on bidirectional Gated Recurrent Unit (GRU)  [49] ,  [50]  as music and emotion encoders as described in Section III-A.\n\nThen, v (m) and v (e) are projected into an embedding space C (cca) of dimensionality D (cca) using different Fully Connected (FC) layers with linear activation. The embeddings for v (m) and v (e) in C (cca) are denoted by\n\n, respectively. In addition, two branches of FC layers are used to transform v (m) into a mean vector µ\n\nand a covariance matrix\n\n. This defines an additional embedding for v (m) as a multivariate Gaussian distribution N (µ (m) , Σ (m) ) in another embedding space C (kl) of dimensionality D (kl) . Considering the expensive computational cost to process multivariate Gaussian distributions, we assume that each dimension is independent based on the standard practice of the literature  [51] ,  [52] . Thus,\n\nis converted into N (µ (e) , σ (e) ) in C (kl) using two branches of FC layers. Under the above-mentioned setting, EMER-CL trains the music and emotion encoders and the six branches of FC layers by jointly minimising the CCA loss between φ (m) and φ (e) and the KL-divergence loss between N (µ (m) , σ (m) ) and N (µ (e) , σ (e) ), as illustrated in Fig.  2  (c ).\n\nThe following sections describe encoding of music samples and emotions, and more details of the training process. The dimensionalities\n\nv , D (cca) and D (kl) are hyper-parameters of EMER-CL whose specific values are provided in Section IV-C.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Music And Emotion Encoders",
      "text": "For the music encoder, two types of neural networks were tested: an MLP and an RNN using bidirectional GRU  [49] ,  [50] . When the former is used, a mean feature vector x(m) is computed by averaging x\n\nTm in X (m) , and fed into the MLP which performs several non-linear transformations on x(m) to output v (m) . The RNN using bidirectional GRU computes two types of\n\nh that represent temporal characteristics of X (m) in the forward and backward directions, respectively. Roughly speaking,\n\nTm and\n\nis a highlevel feature expressing bidirectional temporal characteristics in X (m) , and is fed into the FC layers to produce a higherlevel feature as v (m) .\n\nOur preliminary experiments showed the effectiveness of an RNN using bidirectional GRU as the emotion encoder regardless of datasets. We hypothesise that this is due the fact that bidirectional GRUs can capture the best the variations in arousal and valence levels during the listening of a music sample, and therefore take this information into account to produce meaningful embeddings. Therefore, a feature vector v (e) for an arousal/valence sequence X (e) is extracted the same way as v (m) when an RNN is used as the music encoder. Finally, the specific values of hyper-parameters like\n\nh for the RNN-based emotion encoder, and the configuration of FC layers are provided in Section IV-C.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Training With The Composite Loss",
      "text": "Let X = {(X (m)  n , X (e) n )} N n=1 be a batch consisting of N pairs of an acoustic feature sequence and an arousal/valence sequence for associated music samples and emotions. And,\n\nn=1 is a set of feature pairs obtained by feeding (X (m)  n , X (e) n ) ∈ X to the music and emotion encoders. The CCA loss CCA(F)  [18]  is computed by con-verting V into a set of embeddings F = {(φ (m)  n , φ (e) n )} N n=1 . In addition, the KL-divergence loss KL(G)  [19]  is calculated by transforming V into a set of pairs of multivariate Gaussian distributions G = {(N (µ\n\nOur composite loss CL(F, G) combines CCA(F) and KL(G) as follows:\n\nwhere λ ∈ [0, 1] is a weight parameter to balance CCA(F) and KL(G). Details of CCA(F) and KL(G) are described in the following Sections III-B1 and III-B2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "1) Correlation-Based Embedding With Cca",
      "text": "The CCA loss is employed to construct an embedding space C (cca) where {φ (m) n } N n=1 and {φ (e) n } N n=1 are strongly correlated. More specifically, for each dimension of C (cca) , music and emotion embeddings are linearly correlated with each other regardless of their actual values. This linear correlation indicates what change in acoustic features (or emotions) would be associated to the corresponding change in emotions (or acoustic features) for each of these dimensions. This allows us to characterise the relationship between music samples and emotions in a way that is independent from the actual emotion intensities attributed by individuals, which is useful for addressing the inter-subject variations described in Section I. To extract embeddings capturing complex correlations between music samples and emotions, FC layers are firstly used to refine v Let z (m) be a random vector that is sampled from the probability distribution estimated using a set of N samples {z (m) n } N n=1 , and z (e) be a random vector from the probability distribution estimated using {z z -dimensional weight vectors to project z (m) and z (e) into scalars, respectively. CCA optimises w (m) and w (e) so as to maximise the following correlation between w (m)T z (m) and w (e)T z (e)  [18] :\n\nwhere n } N n=1 , respectively. In Eq. (  2 ) the quantity to maximise is invariant in scaling of w (m) and w (e) , so it is possible to focus on the problem where the denominator is equal to 1. In other words, the objective of CCA is to maximise the numerator in Eq. (  2 ) subject to the constraints w (m)T Σ (mm) w (m) = 1 and w (e)T Σ (ee) w (e) = 1.\n\nThe CCA approach described above can be re-applied independently on each dimension of C (cca) . For this, D (cca) pairs of weight vectors {(w be a matrix where each column is w\n\nz ×D (cca)   where each column is w (e) d is defined to create φ (e) = W (e)T z (e) . From this perspective, the general CCA maximises the sum of correlations each computed for one dimension of φ (m) and φ (e) . Past work has shown that the batch optimisation of {(w\n\nd=1 can be done by solving the following constrained optimisation problem  [18] :\n\nsubject to :\n\nwhere the trace operation (tr) is used to sum up the correlations on each dimension of φ (m) and φ (e) . After obtaining the optimal W (m) * and W (e) * , the correlation-based embedding for z n , respectively.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "2) Distribution-Based Embedding With Kl-Divergence",
      "text": "The CCA loss analyses only the relation (correlation) between music samples and their associated emotions, but neither the relation of a music sample to non-associated emotions nor the relation of an emotion to non-associated music samples. In addition, φ (m) n and φ (e) n are points in C (cca) , which is unsuitable for managing the large intra-class variation of acoustic features, as discussed in Section I. To address these issues, the KL-divergence loss KL(G) is used to build an embedding space C (kl) that attempts to fulfil the following conditions: 1) a music sample and its associated emotion are projected as multivariate Gaussian distributions N (µ\n\nn ) which are similar to each other; 2) a music sample and its non-associated emotion are projected as dissimilar distributions N (µ\n\nSimilarly, an emotion and its non-associated music sample are transformed into dissimilar distributions N (µ n , consisting of nonassociated music sample and emotion. Note that by following the standard of embedding-based retrieval  [19] ,  [53] , we consider that a music sample and an emotion whose indices are the same form a positive pair, and any other pair is a negative pair.\n\nThe aforementioned conditions for C (kl) can be formulated using a triplet (N\n\nwhere ψ(•, •) represents a distance between two distributions. Additionally, α > 0 is a margin hyper-parameter which determines how far the difference between the distance for the positive pair (N\n\nn ) and the one for the negative pair (N n as an anchor:\n\nFor each positive pair (N\n\nn ) in G, KL(G) examines the distance conditions in Eqs. 4 and 5. Specifically, the following ranking loss r(N\n\nn ) is defined by combining two hinge losses as follows:\n\n=\n\n, where the first term becomes zero if all the negative pairs defined using N (m) n as an anchor lead to distances that are greater than the distance between the positive pair by more than α. The second term also checks a similar distance condition using N (e) n as an anchor. This way r(N\n\nn ) defined for all the negative pairs.\n\nTo compute r(N\n\nn ), the KL-divergence is employed as a distance ψ(•, •) between two multivariate Gaussian distributions N (µ 1 , σ 1 ) and N (µ 2 , σ 2 ) in C (kl) of dimensionality D (kl) , and is computed as follows\n\n1 2\n\nwhere µ 1 and σ 1 are expanded as\n\n) T , respectively. Similarly, µ 2 and σ 2 are also expanded. Finally, KL(G) is defined as the sum of r(N\n\nn ) for all the positive pairs in G. The minimisation of KL(G) can therefore lead both music and emotion encoders and FC layers to learn parameters so that the KL-divergence between each positive pair is minimised, while maximising the KLdivergence between each negative pair.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Testing Emer-Cl In M2E And E2M",
      "text": "We evaluate EMER-CL in the framework of M2E and E2M that are formulated as a retrieval task. In the following paragraphs, q designates the index of a query, with 1 ≤ q ≤ Q where Q is the number of examples in the test set. In M2E, the query is X (m) q which represents a music sample that is associated with an emotion rating X (e)  q . The trained music model (consisting in the music encoder and three branches of FC layers) is used to encode the query music sample X (m) q into its correlation-based embedding φ (m) q and distribution-based embedding N (m) q . Similarly, the trained emotion model (consisting in the emotion encoder and three branches of FC layers) is used to convert the jth test emotion X q , X (e) j ) between the query music sample and the jth test emotion is computed as follows:\n\nwhere λ ∈ [0, 1] is the same weighting parameter used for the loss in Eq. (  1 ), ψ designates the negative KL-divergence, and Γ is a correlation-based similarity between φ (m) q = (φ\n\nTo determine a suitable correlation-based similarity Γ, we proceeded as follows. As a reminder, the CCA loss is designed to maximise the correlation on each of D (cca) dimensions independently. for the test emotion is defined as its similarity to the query music sample on the dth dimension. By summing up such similarities on all the D (cca) dimensions, Γ(φ (m)  q , φ (e) j ) is computed as follows:\n\nwhere each dimension d is filtered out or weighted by the correlation ρ d . If ρ d is lower than the threshold P , the approximation on the d th dimension is regarded as inaccurate and the similarity on this dimension is not counted. In contrast, as ρ d becomes higher, the approximation is regarded as more accurate and the similarity is more prioritised by weighting it with ρ d .\n\nThe similarities between the query music and all test emotions s(X (m)  q , X (e) j ) are computed for all 1 ≤ j ≤ Q using Eq. (  8 ), and then sorted by decreasing similarities. The performance of M2E is evaluated by examining whether the test emotion associated with the query music sample X (e)  q is ranked at a high position in the sorted output list.\n\nSimilarly to M2E, E2M is performed by encoding a query emotion X (e)  q and test music samples X (m) j\n\nwith the trained emotion and music models, respectively. Then, the Q test music samples are sorted by computing their similarities to the query emotion s(X (e)  q , X (m) j\n\n) for 1 ≤ j ≤ Q according to Eq. (  8 ). The rank of the music sample associated with the query emotion is checked to measure the performance of E2M.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Iv. Experiments",
      "text": "In this section, we evaluate EMER-CL on two datasets: MediaEval Database for Emotional Analysis in Music (DEAM)  [21]  and PMEmo  [22] . We first present an overview and pre-processing on each dataset, the evaluation metrics, and the implementation details. Then, we present the results of three experiments. The first is an ablation study to validate the composite loss, the second compares EMER-CL to regression baselines not relying on embeddings, and the last examines the generality of EMER-CL by comparing it to the state-of-the-art MER methods.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Datasets",
      "text": "DEAM  [21]   2  provides 1802 music samples that are free audio source records, and their corresponding arousal/valence sequences where arousal and valence intensities lie in  [-10, 10] . Each music sample was annotated with arousal and valence intensities every 0.5 seconds by at least 5 subjects recruited using Amazon's crowdsourcing platform Mechanical Turk. These intensities were projected into the range [-1, 1] for each subject. DEAM contains 1744 45second-long music samples and 58 samples that have durations longer than 45 seconds. The authors of DEAM decided to discard the first 15 seconds of annotations after observing high instabilities due to a high variance in how music samples start. Because of this and the fact that most music samples last only 45 seconds, each music sample is normalised to have a length of 30 seconds by taking the segment starting at 15 seconds and ending at 45 seconds. In order to make our system robust for the average music listener, an \"average sequence\" is created for each of arousal and valence by computing the average value over all subjects at each timestamp. The average sequences for arousal and valence are then concatenated into an arousal/valence sequence X (e) . Finally, the 30-second segment corresponding to the paired music sample is extracted.\n\nPMEmo  [22]  (and more specifically the updated dataset PMEmo2019 3 ) contains 794 music samples which are the chorus parts of high quality popular pop-songs gathered from the Billboard Hot 100, the iTunes Top 100 Songs (USA) and the UK Top 40 Singles Chart. 457 subjects including 366 Chinese university students, 44 Chinese music students and 47 English speaking individuals were recruited for the annotation process. Each music sample is annotated with arousal and valence intensities between 1 (low) and 9 (high) every 0.5 seconds, and then projected into the range [0, 1]. Similarly to DEAM, the first 15 seconds of annotations were discarded by taking into account a large variance in beginnings of music samples. Unlike DEAM, music samples and associated arousal and valence sequences in PMEmo have variable lengths ranging from 0.08 to 73.24 seconds. We decided to select music samples with a total length of at least 7.0 seconds to evaluate in total 701 samples. Similarly to DEAM, an arousal/valence sequence X (e) was associated to each music sample by averaging arousal and valence intensities over all subjects who annotated the sample at each timestamp.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "Each dataset is split into training and test partitions with a proportion of 8 : 2. Specifically, DEAM is split into training and test partitions, respectively containing 1441 and Q = 361 pairs of a music sample and an emotion. The training and test partitions of PMEmo include 560 and Q = 141 pairs, respectively. A model trained on a training partition is evaluated on the corresponding test partition in the framework of M2E and E2M. On both datasets, M2E is run Q times using each of the Q music samples X (m) j as a query. For each query, the Q test emotions X (e) j are sorted in decreasing order of their similarities to the query s(X (m)  q , X (e) j ), and the performance is evaluated by checking the rank of the emotion associated with the query music sample. Similarly, E2M is executed Q times by adopting each of the Q emotions X (e) j as a query and examining the rank of its associated music sample X (m) j . In this framework, only one sample (i.e, emotion or music sample) is associated with a query (i.e., query music sample or query emotion). We use the Mean Reciprocal Rank (MRR) as the main evaluation metric as it is commonly employed in retrieval studies  [11] ,  [12] ,  [54] . The MRR is calculated based on r q that is the rank of the sample associated with a query q in the sorted list of samples (1 ≤ r q ≤ Q). The MRR is defined as the average of reciprocals of all r q over Q queries, that is, MRR = 1 |Q| |Q| q=1 1 rq . However, the MRR is biased in the sense that it puts much higher priorities on samples ranked at high positions than those at low positions. This can lead to low MRR values even if the rank of the sample associated to the query is low. For example, r q = 1 leads to a reciprocal of 1 while it is close to 0 (0.05) for r q = 20, even though r q = 20 might still be a very good retrieval result. In our case, samples associated with queries are rarely ranked at the very top 3 https://github.com/HuiZhangDB/PMEmo positions because each query in either the DEAM or PMEmo dataset has many 'close neighbours' (i.e., samples annotated with similar emotions, or from the same music style) that may easily be ranked above the sample associated with the query. The MRR values might therefore be non-intuitive and not trivially interpretable when checking the performances of our system. Thus, we additionally compute the Average Rank (AR) that is the average of all r q over Q queries, namely AR = 1 |Q| |Q| q=1 r q . Using the AR, samples can be equally evaluated regardless of their ranks. Although the median of all r q is one popular evaluation measure for embedding-based retrieval  [53] , we use their average to be consistent with the calculation of the MRR. To sum up, our evaluation is based on the MRR and AR that respectively become higher and lower as a better performance is obtained. Finally, all models in each configuration are run 10 times. In each of them, all the parameters in EMER-CL (i.e., parameters of music and emotion encoders and six branches of FC layers in Fig.  2 ) are randomly initialised, and a dataset is randomly split into training and test partitions with a proportion of 8 : 2. The mean and standard-deviation of MRRs and ARs obtained in 10 runs are reported.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Implementation Details",
      "text": "We tested various combinations of an MLP and RNN for music and emotion encoders on DEAM and PMEmo. To simplify the selection of encoders, MLPs (or RNNs) with the same architecture were used regardless of encoder types. We found that for the music encoder, an MLP and RNN performed the best on DEAM and PMEmo, respectively. For the emotion encoder, RNNs are the best on both datasets.\n\nThe numbers of layers and units per layer of the MLP and RNNs were chosen by grid search. The MLP consists of five FC layers, each of which performs a non-linear transformation based on units using softplus σ(x) = log(1 + e x ) as their activation function. The number of units in each layer is 256 for the first layer, 512 for the second and third layers, and 1024 units for the fourth and fifth layers. That is, D (m) v = 1024 when the encoder is an MLP. A dropout layer with a dropout rate of 0.5 is inserted between two consecutive layers.\n\nEach RNN based on bidirectional GRU has a single layer with a 512-dimensional hidden state (i.e., D\n\n), and finally outputs a 1024-dimensional vector by concatenating the hidden states obtained in the forward and backward directions. This vector is subsequently passed to an MLP consisting of five FC layers where units use softplus as their activation function. The number of units per FC layer was chosen as 512 for the first three ones, and 1024 for the two last ones (i.e., D",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "1024).",
      "text": "A dropout layer with a dropout rate of 0.5 is also added behind all layers except the RNN layer and the output layer.\n\nRegarding the embedding spaces based on the CCA and KL-divergence losses, their dimensionalities are set to D (cca) = D (kl) = 1024. Based on our experiments, we  VOLUME 4, 2016  recommend to set the threshold for the correlation-based similarity P , the margin in the KL-divergence loss α and the combination weight for the composite loss λ to default values of (P, α, λ) = (0.4, 1.0, 0.5) when testing our approach on a new dataset. It is nevertheless possible to optimise these values on a specific dataset by following the optimisation strategy detailed in Appendix A. The results in the next subsections were obtained using the optimal hyper-parameters (P, α, λ) = (0.5, 1.0, 0.6) on DEAM and (P, α, λ) = (0.7, 2.0, 0.5) on PMEmo.\n\nOur EMER-CL model is trained using Adam  [55]  as the optimiser with an initial learning rate of 1e -5 . The model was trained for 5001 and 10001 epochs on DEAM and PMEmo, respectively. We implemented all the codes using TensorFlow library (version 1.15) on a machine equipped with Intel i9-9900K CPU, 64GB RAM, NVIDIA RTX 2080Ti GPU and CUDA version 10.0.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Evaluation Of The Composite Loss",
      "text": "To evaluate the effectiveness of our proposed composite loss (Composite), we compare its performance to the ones individually obtained only using the CCA loss (CCA-Loss) or KL-divergence loss (KL-Loss). In addition, to examine the effectiveness of projecting music samples and emotions as probability distributions, we implement the most popular embedding approach that projects them as points based on their cosine similarities in an embedding space  [53] . For this approach, all the configurations of our EMER-CL model are the same except that v (m) and v (e) from the music and emotion encoders are projected into vectors instead of multivariate Gaussian distributions, and ψ(•, •) in Eq. (  6 ) is replaced with the negative of their cosine similarity. We report both the performance only using the loss based on cosine similarities (Cos-Loss) and the one obtained by the composite loss combining CCA-Loss and Cos-Loss (Composite-C). Following the optimisation strategy described in Appendix B, the hyper-parameters of Cos-Loss and Composite-C were also optimised to (P, α, λ) = (0.9, 0.3, 0.7) on DEAM and (P, α, λ) = (0.9, 0.1, 0.1) on PMEmo Table  1  shows the results obtained with the five losses described previously. For both DEAM and PMEmo, the MRRs and ARs using KL-Loss are better than those using CCA-Loss. This may be due to the fact that CCA-Loss alone only considers the correlation between music samples and emotions, and does not necessarily make sure that music samples and emotions in positive pairs are placed close to each other in the embedding space. On the other hand, KL-Loss assigns music samples and emotions in positive pairs to similar multivariate Gaussian distributions while distinguishing the ones in negative pairs by dissimilar distributions. Furthermore, the performances are significantly improved when using Composite compared to using only KL-Loss or CCA-Loss. This verifies the effectiveness of Composite that simultaneously considers CCA-Loss and KL-Loss. Also, the fact that KL-Loss outperforms Cos-Loss and Composite outperforms Composite-C on both DEAM and PMEmo datasets indicates the effectiveness of distribution-based embeddings compared to point-based ones. Finally, it can be noted that the performances on PMEmo are significantly better than those on DEAM. This could be attributed to the fact that music samples in PMEmo are more standardised, for instance by including only chorus parts of pop songs, which leads the music encoder to find more specialised feature. In other words, the higher diversity in music samples on DEAM makes M2E and E2M on this dataset likely to be more difficult.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Comparison With The Baseline Models",
      "text": "To the best of our knowledge, no existing EMER method that can be directly compared to EMER-CL has been proposed yet. In addition, all the existing approaches using continuous emotion modelling only perform M2E based on a regression approach to predict real-valued characteristics of the arousal/valence sequence (e.g., the average arousal or valence) for a given music sample  [28] ,  [29] ,  [36] -  [38] ,  [40] -  [43] ,  [56] -  [58] . Moreover, no existing method can handle E2M to predict acoustic features of the music sample for a given arousal/valence sequence. Considering the aforementioned state of the current MER research, we define the following regression-based baselines to show the effectiveness of EMER-CL. M2E baselines: Two M2E baselines RegBiGRU-M2E and RegMLP-M2E train a regression model that analyses a query music sample and outputs a two-dimensional emotion vector x (e) representing the arousal and valence averaged over time for this query music sample. In particular, RegBiGRU-M2E predicts x (e) by applying an RNN based on bidirectional GRU to a sequence of acoustic features X (m) , while RegMLP-M2E employs an MLP that uses the mean x(m) of features in X (m) over time to compute x (e) . Both baselines are trained to minimise the Mean Absolute Error (MAE) between x (e) and the ground-truth mean emotion x(e) computed from the actual arousal/valence sequence X (e) .\n\nIn the evaluation, given the qth test music sample as a query, we evaluate the trained model by predicting its mean emotion x (e) q and checking whether x (e) q is similar to the ground-truth mean emotion\n\nx(e) q . To this end, we compute the similarities of x (e) q to the ground-truth mean emotions {x (e) t } Q t=1 for all the Q test music samples. The Absolute Error (AE) between x (e) q and\n\nx(e) t is used as their dissimilarity. Then, the ground-truth mean emotions are sorted in ascending order of their AEs to get the rank r q of\n\nx(e) q . Finally, r q is used to calculate an MRR and AR. E2M baselines: Similarly to the M2E baselines, an RNN based on bidirectional GRU (RegBiGRU-E2M) and an MLP model (RegMLP-E2M) are used as E2M baselines to predict a mean acoustic feature x (m) . RegBiGRU-E2M and RegMLP-E2M take as input an arousal/valence sequence X (e) and the mean vector x(e) of X (e) , respectively. RegBiGRU-E2M and RegMLP-E2M are trained to minimise the MAE between x (m) and the ground-truth mean acoustic feature x(m) computed from a sequence of acoustic features X (m) . Like M2E, an MRR and AR is computed by predicting x (m) q for the qth test emotion, measuring the AEs of x (m) q to the ground-truth mean acoustic features {x\n\nt=1 for all the Q music samples, and check the rank r q of x(m) q . The baseline models were tuned by grid search to find the hyper-parameters leading to the best performances on each dataset. Details about this hyper-parameter tuning can be found in Appendix C. Table  2  shows the comparison between the above-mentioned baselines and EMER-CL (referred to as Composite in Table  1 ). EMER-CL significantly outperforms the baselines based on one-way regression of emotion or acoustic features. This highlights the superiority of our embedding-based approach over traditional regression methods not using embeddings.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "F. Comparison To The Mer State-Of-The-Art",
      "text": "To evaluate the features learnt by EMER-CL, we performed a comparison of the latter against the state-of-the-art on DEAM and PMEmo. To the best of our knowledge, the EMER problem remains still unexplored in the literature, which makes it difficult to find a past study with which our results can be directly compared. Therefore, we carry out the evaluation on the significantly more popular task of MER.\n\nThe MER literature is fairly scattered, with each study carrying out experiments on different datasets, choosing different evaluation metrics and strategies. Our experiments on both DEAM and PMEmo were carried out based on the most commonly used setting, that is, a K-fold cross validation evaluated either using the Root Mean Squared Error (RMSE), the Pearson correlation coefficient R or the coefficient of determination R 2 .\n\nOur EMER-CL model is trained for K = 10 folds using the default values of α = 1 and λ = 0.5 (P is only required for a retrieval problem, not a regression one). After training the whole of our EMER-CL model, three vectors obtained from the music model (i.e., φ (m) , µ (m) and σ (m) in Fig.  2 ) are used to train a soft-margin Support Vector Regression model (C-SVR) with Radial Basis Function (RBF) kernel that attempts to predict the arousal and valence intensities associated with a music sample that is input to the music model. Three different configurations for the input to C-SVR are tested: φ (m) used alone (also referred to as EMER-CL (cca)), µ (m) and σ (m) concatenated together (EMER-CL (kl)), and all the three vectors concatenated together (EMER-CL (all)). On DEAM, the target to predict was chosen as x(e) , the average of an arousal/valence sequence X (e) over time. On PMEmo, the last arousal and valence values of the emotion sequence X (e) were predicted instead. The hyperparameters of the C-SVR (soft-margin and kernel parameters) were optimised by maximising the average R 2 after grid search.\n\nTable  3  shows the results obtained for arousal and valence prediction on DEAM and PMEmo. The learnt features φ (m) , µ (m) and σ (m) can compete with the state-of-the-art for MER. This indicates that our EMER-CL model can still yield proper MER performances. Curiously, φ (m) , µ (m) and σ (m) yield average results for arousal prediction, and notably good ones for valence prediction which is commonly considered as the most difficult of the two problems. All the three tested combinations of φ (m) , µ (m) and σ (m) return fairly similar performances.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "V. Detailed Analysis",
      "text": "MRRs and ARs are global metrics that only depend on the rank r q of the music sample or emotion associated with a query. It is also desirable to check the relevance of the top-ranked music samples (or emotions) to the query. For this, we compute for M2E an average cosine similarity that averages the cosine similarities between the mean acoustic  and the ones associated with the top 5% emotions retrieved by EMER-CL, i.e., {x (m) j |1 ≤ j ≤ Q, r j ≤ 0.05 × Q}. For E2M, this average cosine similarity is computed in a likewise way by taking the mean of the cosine similarities computed between the query emotion averaged over time\n\nx(e) q and the ones associated with the top 5% retrieved music samples in E2M, i.e., {x In what follows, we present the analysis for E2M since the mean emotion for each music sample is two-dimensional and can be interpreted easily. It should be noted that arousal/valence intensities are in [-1, 1] and [0, 1] for DEAM and PMEmo respectively, meaning that average cosine similarities range between -1 and 1 on DEAM, and 0 and 1 on PMEmo. Fig.  3  shows the average cosine similarities for DEAM and PMEmo. In the bar graphs in the left side of Fig.  3 (a)  and (b) , each query emotion on the horizontal axis is sorted in increasing order of the rank r q of its associated music sample. That is, the more to the left a query emotion is, the higher its ground truth music sample was ranked for E2M, meaning that the music sample associated with the query emotion was well recognised.\n\nAs shown in the bar graphs of Fig.  3    PMEmo is provided in Appendix E). In addition, the box plots in the right side of Fig.  3  (a) and (b) show the variations in the average cosine similarities. Here, at least 75 percent of all the average cosine similarities are higher than the 25th percentile (first quartile). The fact that the 25th percentile for DEAM and PMEmo are 0.692 and 0.989 respectively, indicates that EMER-CL is able to robustly recognise music samples associated to highly similar emotions to a query emotion. In other words, even if the music sample associated to a query emotion was ranked at a low position, the top 5% music samples recognised by EMER-CL still exhibit emotions close to the query emotion. Nevertheless, average cosine similarities for some query emotions in DEAM are low, indicating room for improvement in the future.\n\nThe same experiment for M2E that computes the average cosine similarity between the acoustic feature of a query music sample and those of music samples associated with the top 5% emotions in M2E showed similarly good performances. Figures showing such average cosine similarities can be found in Appendix D, and the medians of average cosine similarities on DEAM and PMEmo are 0.753 and 0.837, respectively.\n\nFinally as a last check of the validity of our approach, we also visualise the embeddings learnt by our music and emotion models on the training and testing sets of both DEAM and PMEmo using t-SNE  [63] . We plotted the correlation-based and probabilistic-based embeddings produced by the music model (φ (m) , µ (m) and σ (m) ) and the emotion model (φ (e) , µ (e) and σ (e) ). The t-SNE projections were labelled with the emotion-related information available on both DEAM and PMEmo datasets as follows: each emotion sequence X (e) j in either the training or testing set was first averaged over time to obtain the two-dimensional emotion vector\n\nx(e) j , and then associated to one of the four quadrants of the arousal/valence space: high arousal/high valence (HA/HV), high arousal/low valence (HA/LV), low arousal/low valence (LA/LV) and low arousal/high valence (LA/HV). The t-SNE projections of the embeddings obtained either from X (e) j or its associated music sample X (m) j were then annotated with one of these four labels. The t-SNE plots for the DEAM dataset are provided in Figs.  4  and 5  for the music and emotion embeddings, respectively. Since DEAM emotion annotations lie in the range [-1, 1], the cut-off value for the definition of the quadrants was set to 0 for both arousal and valence.\n\nIt can be seen from both Figs. 4 and 5 that the DEAM music and emotion embeddings associated with opposite emotional quadrants (e.g. HA/HV and LA/LV, or HA/LV and LA/HV) are very well separated in their respective embedding spaces for both the training and testing examples. This indicates that our EMER-CL approach was successful in learning to project music samples associated to similar emotions close to each other in the embedding space, while simultaneously maximising the distance between embeddings of dissimilar emotions. Similar plots can be obtained for the PMEmo dataset and are provided in Appendix G.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "In this paper, we introduced an Embedding-based Music Emotion Recognition using Composite Loss (EMER-CL) approach that projects music samples and emotions into embedding spaces, in order to consider both general emotional categories and fine-grained discrimination within each category. In particular, to deal with the emotional uncertainty, EMER-CL uses the composite loss consisting of the CCA loss to maximise the correlation between music samples and their associated emotions in an embedding space, and the KL-divergence loss to project them as similar multivariate Gaussian distributions in another embedding space. The experimental results on DEAM and PMEmo validate the effectiveness of the composite loss, embedding-based approach and features learned by EMER-CL. In addition, a detailed analysis of EMER-CL's results demonstrates that it can robustly recognise reasonable music samples (or emotions) even when failing to identify the ground-truth ones. 4 https://mu-lab.info/naoki_takashima/emer-cl To further improve the performance of EMER-CL, we aim to extend the music and emotion encoders by pre-training them with self-supervised learning  [64]  which can learn underlying feature representations using unlabelled data. We also plan to adopt a self-attention layer  [65]  which can capture long-term dependencies of features, and have led to promising performances when jointly used with bidirectional LSTM and GRU, in particular for sentiment analysis  [66]  or sensor-based emotion recognition  [67] . Because emotions are also strongly dependent on cultural background, we also plan to use MER datasets that provide detailed background information about their raters in future work. The generality of our approach across cultures could be then be demonstrated.\n\nFinally, the codes (and the instruction of data usage) used in this paper are available on our GitLab repository 4 , in order for other researchers to reproduce the results and extend the current EMER-CL more easily.\n\n.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Appendix A Hyper-Parameter Tuning For Emer-Cl",
      "text": "This section presents how to tune EMER-CL's hyperparameters, especially P ∈ [0, 1] used in the correlationbased similarity to filter out useless dimensions characterised by weak correlations between music samples and their associated emotions, α ∈ R + used in the KL-divergence loss to handle the margin between associated music-emotion pairs and non-associated ones, and λ ∈ [0, 1] to control the combination weights of the CCA and KL-divergence losses.\n\nOnly using either of these losses, an embedding space can be constructed to perform M2E and E2M. Thus, P is firstly tuned based on the performances of M2E and E2M only using the CCA loss. Similarly, α is tuned by carrying out M2E and E2M only with the KL-divergence loss. Finally, λ is tuned based on M2E and E2M by combining the CCA and KL-divergence losses that are configured by the separately optimised P and α, respectively.\n\nA. TUNING P Fig.  6  shows the various performances obtained only using the CCA loss configured by different values of P . As previously described in Section IV, a performance is measured by an MRR and AR. A good performance is indicated by a high MRR and a low AR. Fig.  6  shows the box plots of the MRR and AR performances obtained for each value of P over 10 runs. They were computed as follows: Since building an embedding space based on the CCA loss is independent of the choice of P , 10 spaces are firstly constructed by randomly initialising all parameters in EMER-CL (i.e., the parameters of both music and emotion encoders and the ones of six branches of FC layers in Fig.  2  1 ), and randomly splitting a dataset into training and test partitions with a proportion fixed to 8 : 2. Then, every value of P is used to filter out useless dimensions in each of these 10 embedding spaces to get 10 performances.\n\nTo determine a range of values to be tested for P , we check the maximum correlations among the 1024 dimensions of each embedding space. In particular, among the 10 embedding spaces constructed for each dataset, the maximum correlation is 0.783-0.833 and 0.719-0.774 for DEAM and PMEmo, respectively 2 . We test values between 0 and the maximum correlation with increments of 0.1 for P .\n\nIn each graph of Fig.  6 , the larger P is, the smaller the number of dimensions used in the correlation-based similarity is. In other words, if P is large, only a small number 1 Since the KL-divergence loss is always zero in this setting, the four branches of FC layers to produce mean and variance vectors of multivariate Gaussian distributions are not trained. 2 When the composite loss is used, the maximum correlations for 10 CCAbased embedding spaces increase to 0.856-0.857 and 0.9995-0.9997 for DEAM and PMEmo, respectively. This is possibly due to more generalised music and emotion encoders being trained by exploiting both the CCA and KL-divergence losses, which leads to higher-quality embedding spaces. Thus, better EMER-CL's performances than those reported in this paper might be obtained by carrying out grid search on P , α and λ. But, due to its expensive computational cost, we opt to separately tune these hyperparameters in this paper. of dimensions characterised by correlations higher than it are used to compute correlation-based similarities. For each dataset, the optimal value of P is selected as the one that yields the best 'overall' performance by considering both M2E and E2M performances. We provide an example of how to select the optimal P value on DEAM by referring to Fig.  6 . As seen from this figure, although P = 0.2 yields the highest median of MRRs in M2E, P = 0.5 and 0.6 both lead to the highest median of MRRs in E2M and the lowest medians of ARs in both M2E and E2M. Thus, P = 0.5 or 0.6 can be considered as optimal on DEAM. In this case in particular, P = 0.5 is selected after observing that its neighbouring value P = 0.4 yields higher performances than P = 0.7 neighbouring P = 0.6. For PMEmo, P = 0.7 is chosen because of its significantly higher performances on E2M compared to the other P values. It can be noted that the performances of CCA-Loss in the comparative study in Section IV-D are nothing but the ones that are obtained only using the CCA loss based on the aforementioned optimal P values.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B. Tuning Α",
      "text": "Unlike P that is bounded, the margin α can theoretically take any positive value. We decided to test values between 0 and 1.5 with increments of 0.1, and powers of 2 between 2 and 128. Using the same box plot format as Fig.  6 , Fig.  7  illustrates the performances obtained using only the KLdivergence loss, which is configured by different values of α. Using a similar strategy as for P , we select the optimal α value in Fig.  7  as the one that leads to the best overall performance. As it can be seen from Fig.  7 , the performances are similar for all tested values, with the exception of small values 0.0, 0.1 and 0.2 for which the performances are significantly worse. We decide to select α = 1.0 and 2.0 as the optimal values for DEAM and PMEmo respectively, on the basis that the overall performances with these P values are slightly higher than those with the others. It can be noted that the performances acquired by the aforementioned optimal α values are reported as the ones of KL-Loss in the comparative study in Section IV-D.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "C. Tuning Λ",
      "text": "We tested values of λ between 0 and 1 with increments of 0.1. In the same manner as Figs.  6  and 7 , Fig.  8  illustrates EMER-CL's performances obtained for different values of λ. For each of them, the CCA and KL-divergence losses that are configured by the optimal P and α values (found from Figs. 6 and 7 respectively) are used to compute the composite loss. The higher λ is, the higher the weight of the CCA loss is. In particular, λ = 0 means only using the KL-divergence loss while only the CCA loss is used with λ = 1. Following the same strategy as the ones employed for choosing the optimal P and λ values, λ = 0.6 is chosen as the optimal value yielding the best overall performance on DEAM, and similarly λ = 0.5 is regarded as optimal for PMEmo. To summarise the whole EMER-CL hyper-parameter selection process, the optimal values found are P = 0.5, α = 1.0 and λ = 0.6 on DEAM, and P = 0.7, α = 2.0 and λ = 0.5 on PMEmo. The performances of EMER-CL using these optimal values are reported as the ones of Composite in Section IV-D. In addition, it should be noted that these optimal hyper-parameter values only marginally differ from the default ones (i.e., P = 0.4, α = 1.0 and λ = 0.5). Table  4  shows the performances of EMER-CL with optimised parameters -referred to as Composite -and the ones obtained with the default parameters -referred to as Composite-d. As it can be seen from this table, the performances of the latter are relatively similar to the ones of the former. The marginal difference in hyper-parameter values between Composite and Composite-d and their similar performances validate the relevance of the default values.",
      "page_start": 15,
      "page_end": 19
    },
    {
      "section_name": "Appendix B Hyper-Parameter Tuning For Composite-C",
      "text": "In this section, we tune hyper-parameters of the two methods Cos-loss and Composite-C used in the comparative study in Section IV-D. Cos-Loss produces point-based embeddings using the loss based on cosine similarities between music samples and emotions  [53] , and is used to examine the effectiveness of our proposed KL-Loss implementing distributionbased embeddings. Both of Cos-Loss and KL-Loss construct an embedding space in the same ranking loss framework that involves α to control the margin between associated musicemotion pairs and non-associated ones. Thus, α for Cos-Loss is tuned in the same manner as α for KL-Loss in Section A-B.\n\nSimilarly to Composite, Composite-C is characterised by λ that handles the combination weights of CCA-Los and Cos-Loss. However, while Cos-Loss is based on normalised cosine similarities ranging from -1 to 1, the KL-divergences used in KL-Loss are contained in a much wider range, like on average about 5.1 for DEAM and 423.7 for PMEmo. It should be noted that P impacts the range of correlation-based similarities because it determines the number of dimension-wise similarities counted to compute an overall similarity, as seen from Eq. (  9 ). Thus, both P and λ can be tuned to balance the combination of CCA-Loss and Cos-Loss, and the P values found in Fig.  6  for Composite may not be suitable for Composite-C. Consequently, after selecting the optimal α for Cos-Loss, grid search on P and λ is carried out to avoid missing an effective combination of CCA-Loss and Cos-Loss for Composite-C. This grid search for Composite-C is more exhaustive and favourable than the separate optimisation of P and λ employed for Composite.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "A. Tuning Α Of Cos-Loss",
      "text": "The same values as in Section A-B were tested to tune α for Cos-Loss, i.e., values between 0 and 1.5 with increments of 0.1, and powers of 2 between 2 and 128. Using the same box plot format as Figs. 6, 7 and 8, Fig.  9  displays various performances on DEAM and PMEmo only using Cos-Loss configured by different values of α. By following the aforementioned criteria addressing the best overall performance on M2E and E2M, α = 0.3 for DEAM and α = 0.1 for PMEmo are chosen as the optimal values. The performances of Cos-Loss configured with these α values are used in the comparative study in Section IV-D.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "B. Tuning P And Λ Of Composite-C By Grid Search",
      "text": "Fig.  10  presents grid search results for different pairs of P and λ values, where P ranges between 0 and the maximum correlation on the considered dataset with an increment of 0.1 and λ ∈ {0.1, • • • 0.9}. Each bar in a three-dimensional bar graph indicates the mean of 10 performances (i.e., MRRs or ARs) obtained using a pair of P and λ values. Like for the hyper-parameter tuning procedure previously described, these 10 performances are acquired by randomly initialising all parameters in Composite-C and randomly splitting a dataset into training and test partitions with a ratio of 8 : 2. In addition, the computational cost of grid search can be reduced by considering that P is only related to the test process as described in Section A-A. More specifically, after training 10 Composite-C models (i.e., music and emotion encoders, and embedding spaces based on CCA-Loss and Cos-Loss) using the α value optimised in the previous section and a specific λ value, their test processes are repeatedly run to obtain 10 performances for each of the different P values. Note that to make visual interpretation of the results easier, the axes of λ and P are depicted in the horizontal and depth directions for the MRR histograms, while the directions of these axes are swapped for the AR histograms in Fig.  10 .\n\nAs indicated by the red arrows in Fig.  10 , the best overall performance is attained using P = 0.9 and λ = 0.7 for DEAM and P = 0.9 and λ = 0.1 for PMEmo. The performances obtained using these optimal P and λ are reported as the ones of   to the query are still recognised because they are associated with acoustically similar music samples to the query.\n\nJust like Fig.  3 , the bar graphs in the left part of Fig.  11  (a ) and (b) are drawn by sorting query music samples in ascending order of the ranks r q of their associated groundtruth emotions. The more to the left a query music sample is located, the higher its associated emotion is ranked by EMER-CL. It should be noted that the median of pairwise cosine similarities among acoustic features of music samples is 0.674 and 0.800 on DEAM and PMEmo, respectively. Each of these numbers can be interpreted as the cosine similarity between the acoustic features of two randomly selected music samples. The bar graphs and box plots in Fig.  11  show that the median of average cosine similarities is 0.753 and 0.837 on DEAM and PMEmo, respectively. These medians are significantly higher than the median of pairwise cosine similarities, which validates the meaningfulness of EMER-CL's M2E results. In addition, as shown by the box plots in Fig.  11 , the 25th percentile (first quartile) of average cosine similarities on DEAM and PMEmo are 0.709 and 0.810, respectively. The fact that even these 25th percentiles are higher than the medians of pairwise cosine similarities, indicates that in most cases EMER-CL's M2E works better than randomly selecting an emotion for a query music sample. Here, arousal and valence intensities in PMEmo are in [0, 1], so all the emotions are distributed only in the first quadrant. Furthermore, the variance of this distribution is small in particular, as illustrated in Fig.  12 . As a result, the average of pairwise cosine similarities among these emotions is 0.9884 ± 0.0196, and even the smallest pairwise cosine similarity characterised by the emotions marked by the crosses in Fig.  12  is 0.7846.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Appendix E About The Detailed Analysis For E2M On Pmemo",
      "text": "It should be noted that if the emotion ratings in Fig.  12  could be translated so that their origin is (0, 0), the range of average cosine similarities would be much wider and a detailed analysis based on them would be much clearer. However, it is difficult to precisely locate the origin of the arousal/valence space in Fig.  12 . For example, znormalisation can be carried out so that emotions have zero mean and unit variance, but there is no guarantee that the resulting zero vector corresponds to the origin. Cosine similarities significantly rely on the location of the origin because they measure similarities between the angles of two vectors. In addition, considering Russell's circumplex model where emotions are circularly located in the arousal/valence space  [4] , an angle-based similarity measure like cosine similarity is preferred to other types of measures like Euclidean distance. Thus, reliable analysis is impossible when the exact location of the origin is unknown. For this reason, our detailed analysis on PMEmo is performed using the original arousal and valence intensities without any modification.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Appendix F Computational Costs Of Emer-Cl",
      "text": "We discuss about the computational costs of EMER-CL by referring to Table  6 . Each number in this table indicates the average runtime of 10 runs to train or test an EMER-CL model with a random initialisation of all parameters and a random 8 : 2 split of a dataset into training and test partitions. The runtime for testing the EMER-CL model is the average elapsed time to get an M2E (or E2M) result given a query music sample (or query emotion). The hyper-parameters of the EMER-CL model are set to the optimal values found in Section A. Furthermore, the runtimes in Table  6    As it can be seen from the first column in Table  6 , training an EMER-CL model on PMEmo takes significantly longer time than training it on DEAM, even though the training partition of PMEmo only contains 561 music-emotion pairs. One main reason is that both of the music and emotion encoders for PMEmo are defined as RNNs that need to process the acoustic feature or emotion vector sequentially in time. To improve the scalability of EMER-CL, we plan to define the music and emotion encoders as self-attention models that can perform batch processing of acoustic features or emotion vectors at all times  [65] .\n\nThe second and last columns in Table  6  show the very short runtimes of EMER-CL's test process. This is due to the fact that the test partitions of DEAM and PMEmo only contain 361 and 141 samples (i.e., music samples for E2M or emotions for M2E), respectively. Nevertheless, since the runtime of EMER-CL's test process scales linearly with the number of samples, the test process would still be expected to finish within seconds even with a number of samples three orders of magnitude higher than the currently tested numbers.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Appendix G T-Sne Plots Of Embeddings On The Pmemo Dataset",
      "text": "In a similar way as for the DEAM dataset, we plotted the t-SNE projections of the correlation-based and probabilistic- 3 The codes are available at https://mu-lab.info/naoki_takashima/emer-cl based embeddings produced by both the music (φ (m) , µ (m) and σ (m) ) and emotion models (φ (e) , µ (e) and σ (e) ) trained on the PMEmo dataset. Plots were obtained for examples in both the training and testing sets. For each example of either dataset, we first computed the average emotion vector over time\n\nx(e) j and used it to label the t-SNE projections in terms of which quadrant in the arousal/valence space (HA/HV, HA/LV, LA/LV or LA/HV) each music sample X (m) j or mean emotion sequence x (e) j was associated with. Unlike for the DEAM dataset, the definition of the emotional quadrants on PMEmo is not trivial. This is due to the fact that both arousal and valence ratings are projected into the [0, 1] interval, and that the dataset includes exclusively pop songs which skew it strongly towards the quadrant high arousal/high valence (HA/HV). Taking the centre of the emotional space (0.5, 0.5) as cut-off point for the definition of the quadrants leads to a very imbalanced data repartition with 476, 53, 106 and 66 songs out of 701 associated to the HA/HV, HA/LV, LA/LV and LA/HV quadrants, respectively. To mitigate the effects of this imbalance, we instead take the barycentre of all 701 PMEmo music samples in the arousal/valence space (0.61, 0.63) as mid-point to define the quadrants. This leads to a more balanced repartition of 306, 77, 84 and 234 songs associated to the HA/HV, HA/LV, LA/LV and LA/HV quadrants, respectively. The t-SNE plots of embeddings obtained with these emotion annotations are provided in Figs. 13 and 14 for the music and emotion embeddings, respectively.\n\nThese figures show that embeddings tend to be grouped by their associated emotional quadrants on both the training and testing sets, although this trend on the PMEmo dataset is less clear than the one for the DEAM dataset, as some groups mixing embeddings associated with different quadrants can also be seen. We hypothesise that this is due to the fact that the PMEmo dataset aggregates music samples belong to a single genre (pop songs) that are quite similar to each other in terms of audio features and elicited emotions. Despite this, music samples or emotions associated to the same quadrants still tend to be located in the same neighbourhoods of the embedding space.",
      "page_start": 24,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows a standard EMER approach that projects music",
      "page": 2
    },
    {
      "caption": "Figure 1: Principle of an embedding-based retrieval approach for MER. A",
      "page": 2
    },
    {
      "caption": "Figure 1: ) suboptimal. Thus, we",
      "page": 3
    },
    {
      "caption": "Figure 2: shows an overview of our EMER-CL approach. First,",
      "page": 5
    },
    {
      "caption": "Figure 2: (a) and (b),",
      "page": 5
    },
    {
      "caption": "Figure 2: An overview of our EMER-CL approach. X(m) and X(e)",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the average cosine similarities for",
      "page": 12
    },
    {
      "caption": "Figure 3: (a) and (b), each query emotion on the horizontal axis",
      "page": 12
    },
    {
      "caption": "Figure 3: (a) and (b), the",
      "page": 12
    },
    {
      "caption": "Figure 3: Bar graphs showing average cosine similarities and their box plots",
      "page": 12
    },
    {
      "caption": "Figure 3: (a) and (b) show the variations",
      "page": 12
    },
    {
      "caption": "Figure 4: t-SNE projections of the music embeddings φ(m), µ(m) and σ(m) (respectively ﬁrst, second and third rows) on the DEAM training (left column) and",
      "page": 14
    },
    {
      "caption": "Figure 5: t-SNE projections of the emotion embeddings φ(e), µ(e) and σ(e) (respectively ﬁrst, second and third rows) on the DEAM training (left column) and",
      "page": 14
    },
    {
      "caption": "Figure 6: shows the various performances obtained only using",
      "page": 15
    },
    {
      "caption": "Figure 6: shows the box plots of the",
      "page": 15
    },
    {
      "caption": "Figure 21: ), and randomly splitting a",
      "page": 15
    },
    {
      "caption": "Figure 6: , the larger P is, the smaller the",
      "page": 15
    },
    {
      "caption": "Figure 6: As seen from this ﬁgure, although P = 0.2 yields",
      "page": 15
    },
    {
      "caption": "Figure 7: illustrates the performances obtained using only the KL-",
      "page": 15
    },
    {
      "caption": "Figure 7: as the one that leads to the best overall",
      "page": 15
    },
    {
      "caption": "Figure 7: , the performances",
      "page": 15
    },
    {
      "caption": "Figure 8: illustrates",
      "page": 15
    },
    {
      "caption": "Figure 6: Transitions of EMER-CL performances (MRRs and ARs) obtained only using the CCA loss that is conﬁgured by different values of P . The optimal P",
      "page": 16
    },
    {
      "caption": "Figure 7: Transitions of EMER-CL performances (MRRs and ARs) obtained only using the KL-divergence loss that is conﬁgured by different values of α. The",
      "page": 17
    },
    {
      "caption": "Figure 8: Transitions of EMER-CL performances (MRRs and ARs) obtained by different λ values, each of which is used to combine the CCA and KL-divergence",
      "page": 18
    },
    {
      "caption": "Figure 6: for Composite may not be suitable",
      "page": 19
    },
    {
      "caption": "Figure 9: displays various",
      "page": 19
    },
    {
      "caption": "Figure 10: presents grid search results for different pairs of P",
      "page": 19
    },
    {
      "caption": "Figure 10: As indicated by the red arrows in Fig. 10, the best overall",
      "page": 19
    },
    {
      "caption": "Figure 9: Transition of Composite-C performances (MRRs and ARs) obtained using Cos-Loss conﬁgured by different values of α. The optimal α value is selected",
      "page": 20
    },
    {
      "caption": "Figure 10: Transitions of Composite-C performances (MRRs and ARs) obtained by different pairs of P and λ values. Here, α of Cos-Loss is set to the optimal",
      "page": 21
    },
    {
      "caption": "Figure 9: As indicated by the red arrows, the optimal pair of P and λ values is selected as (P = 0.9, λ = 0.7) and (P = 0.9, λ = 0.1) for DEAM",
      "page": 21
    },
    {
      "caption": "Figure 11: shows the average cosine similarities computed for",
      "page": 22
    },
    {
      "caption": "Figure 11: Bar graphs showing average cosine similarities and their box",
      "page": 23
    },
    {
      "caption": "Figure 3: , the bar graphs in the left part of Fig. 11",
      "page": 23
    },
    {
      "caption": "Figure 11: show that the median of average cosine similarities",
      "page": 23
    },
    {
      "caption": "Figure 11: , the 25th percentile (ﬁrst quartile)",
      "page": 23
    },
    {
      "caption": "Figure 12: Distribution of PMEmo test emotions in the valence/arousal",
      "page": 23
    },
    {
      "caption": "Figure 3: (b) shows that the average cosine similarities in E2M",
      "page": 23
    },
    {
      "caption": "Figure 12: depicts the distribution of 141 test emotions,",
      "page": 23
    },
    {
      "caption": "Figure 12: As a result, the aver-",
      "page": 23
    },
    {
      "caption": "Figure 12: is 0.7846.",
      "page": 23
    },
    {
      "caption": "Figure 12: could be translated so that their origin is (0, 0), the range",
      "page": 23
    },
    {
      "caption": "Figure 12: For example, z-",
      "page": 23
    },
    {
      "caption": "Figure 13: t-SNE projections of the music embeddings φ(m), µ(m) and σ(m) (respectively ﬁrst, second and third rows) on the PMEmo training (left column) and",
      "page": 25
    },
    {
      "caption": "Figure 14: t-SNE projections of the emotion embeddings φ(e), µ(e) and σ(e) (respectively ﬁrst, second and third rows) on the PMEmo training (left column) and",
      "page": 25
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the results obtained with the ﬁve losses",
      "page": 10
    },
    {
      "caption": "Table 1: Comparison of MRRs and ARs using ﬁve different losses. The",
      "page": 10
    },
    {
      "caption": "Table 2: shows the comparison",
      "page": 11
    },
    {
      "caption": "Table 1: ). EMER-CL signiﬁcantly",
      "page": 11
    },
    {
      "caption": "Table 2: Comparison between the baselines and EMER-CL (Composite).",
      "page": 11
    },
    {
      "caption": "Table 3: shows the results obtained for arousal and valence",
      "page": 11
    },
    {
      "caption": "Table 3: Comparison between EMER-CL and the state-of-the-art for M2E.",
      "page": 12
    },
    {
      "caption": "Table 4: shows the performances of EMER-CL",
      "page": 19
    },
    {
      "caption": "Table 4: Performance comparison between EMER-CL using the optimal",
      "page": 19
    },
    {
      "caption": "Table 2: , these baselines are signiﬁcantly outperformed by",
      "page": 22
    },
    {
      "caption": "Table 5: Hyper-parameters of the baselines after grid search.",
      "page": 22
    },
    {
      "caption": "Table 6: Each number in this table indicates the",
      "page": 24
    },
    {
      "caption": "Table 6: are measured",
      "page": 24
    },
    {
      "caption": "Table 6: Runtimes of EMER-CL.",
      "page": 24
    },
    {
      "caption": "Table 6: , training",
      "page": 24
    },
    {
      "caption": "Table 6: show the very",
      "page": 24
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of music emotion recognition",
      "authors": [
        "Donghong Han",
        "Yanru Kong",
        "Jiayi Han",
        "Guoren Wang"
      ],
      "venue": "Frontiers of Computer Science"
    },
    {
      "citation_id": "2",
      "title": "Music emotion recognition using convolutional long short term memory deep neural networks",
      "authors": [
        "Serhat Hizlisoy",
        "Serdar Yildirim",
        "Zekeriya Tufekci"
      ],
      "year": "2021",
      "venue": "Engineering Science and Technology, an International Journal"
    },
    {
      "citation_id": "3",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "4",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "5",
      "title": "Using circular models to improve music emotion recognition",
      "authors": [
        "Isabelle Dufour",
        "George Tzanetakis"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Music emotion recognition: toward new, robust standards in personalized and context-sensitive applications",
      "authors": [
        "Juan Sebastian",
        "Estefania Gomez",
        "Tuomas Cano",
        "Perfecto Eerola",
        "Xiao Herrera",
        "Yi-Hsuan Hu",
        "Emilia Yang",
        "Gomez"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "7",
      "title": "Deep learning for instance retrieval: a survey",
      "authors": [
        "Wei Chen",
        "Yu Liu",
        "Weiping Wang",
        "Erwin Bakker",
        "Theodorous Georgiou",
        "Paul Fieguth",
        "Li Liu",
        "Micheal Lew"
      ],
      "year": "2022",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "8",
      "title": "Probabilistic embeddings for cross-modal retrieval",
      "authors": [
        "Sanghyuk Chun",
        "Joon Seong",
        "Rafael Oh",
        "Yannis Sampaio De Rezende",
        "Diane Kalantidis",
        "Larlus"
      ],
      "year": "2021",
      "venue": "Proc. of CVPR 2021"
    },
    {
      "citation_id": "9",
      "title": "Audio-visual embedding for cross-modal music video retrieval through supervised deep CCA",
      "authors": [
        "Donghuo Zeng",
        "Yi Yu",
        "Keizo Oyama"
      ],
      "year": "2018",
      "venue": "Proc. of ISM 2018"
    },
    {
      "citation_id": "10",
      "title": "Deep triplet neural networks with cluster-cca for audio-visual cross-modal retrieval",
      "authors": [
        "Donghuo Zeng",
        "Yi Yu",
        "Keizo Oyama"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "11",
      "title": "Deep cross-modal correlation learning for audio and lyrics in music retrieval",
      "authors": [
        "Yi Yu",
        "Suhua Tang",
        "Francisco Raposo",
        "Lei Chen"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "12",
      "title": "Emotion embedding spaces for matrhcing music to stories",
      "authors": [
        "Minz Won",
        "Justin Salamon",
        "Nicholas Bryan",
        "J Gautham",
        "Xavier Mysore",
        "Serra"
      ],
      "year": "2021",
      "venue": "Emotion embedding spaces for matrhcing music to stories",
      "arxiv": "arXiv:2111.13468"
    },
    {
      "citation_id": "13",
      "title": "CCMR: a clasic-enriched connotation-aware music retrieval system on social media with visual inputs",
      "authors": [
        "Lanyu Shang",
        "Daniel Zhang",
        "Jialie Shen",
        "Eamon Lopez Marmion",
        "Dong Wang"
      ],
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "14",
      "title": "Enriched music representations with multiple crossmodal contrastive learning",
      "authors": [
        "Andres Ferraro",
        "Xavier Favory",
        "Konstantinos Drossos",
        "Yuntae Kim",
        "Dmitry Bogdanov"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "15",
      "title": "Marcin Grzegorzek, and Kimiaki Shirahama. Cross-modal music-emotion retrieval using deepcca",
      "authors": [
        "Naoki Takashima",
        "Frédéric Li"
      ],
      "year": "2021",
      "venue": "Proc. of ITIB"
    },
    {
      "citation_id": "16",
      "title": "Music emotion classification: A fuzzy approach",
      "authors": [
        "Yi-Hsuan Yang",
        "Chia-Chu Liu",
        "Homer H Chen"
      ],
      "year": "2006",
      "venue": "Proc. of ACM Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Multimodal metric learning for tag-based music retrieval",
      "authors": [
        "Minz Won",
        "Sergio Oramas",
        "Orial Nieto",
        "Fabien Gouyon",
        "Xavier Serra"
      ],
      "year": "2021",
      "venue": "Proc. of IEEE ICASSP 2021"
    },
    {
      "citation_id": "18",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "Galen Andrew",
        "Raman Arora",
        "Jeff Bilmes",
        "Karen Livescu"
      ],
      "year": "2013",
      "venue": "Proc. of ICML 2013"
    },
    {
      "citation_id": "19",
      "title": "Image-caption retrieval by embedding to gaussian distribution",
      "authors": [
        "Kenta Hama",
        "Takashi Matsubara",
        "Kuniaki Uehara"
      ],
      "year": "2018",
      "venue": "IEICE Technical Report"
    },
    {
      "citation_id": "20",
      "title": "Joint imagetext representation by gaussian visual-semantic embedding",
      "authors": [
        "Hailin Zhou Ren",
        "Zhe Jin",
        "Chen Lin",
        "Alan Fang",
        "Yuille"
      ],
      "year": "2016",
      "venue": "Proc. of ACM Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Developing a benchmark for emotional analysis of music",
      "authors": [
        "Anna Aljanaki",
        "Yi-Hsuan Yang",
        "Mohammad Soleymani"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "22",
      "title": "The pmemo dataset for music emotion recognition",
      "authors": [
        "Kejun Zhang",
        "Hui Zhang",
        "Simeng Li",
        "Changyuan Yang",
        "Lingyun Sun"
      ],
      "year": "2018",
      "venue": "Proc. of ICMR 2018"
    },
    {
      "citation_id": "23",
      "title": "A matlab toolbox for musical feature extraction from audio",
      "authors": [
        "Olivier Lartillot",
        "Petri Toiviainen"
      ],
      "year": "2007",
      "venue": "Proc. of DAFx-07"
    },
    {
      "citation_id": "24",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proc. of ACM Multimedia 2010"
    },
    {
      "citation_id": "25",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "Renato Panda",
        "Ricardo Manuel Malheiro",
        "Rui Pedro"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "A novel method based on ompgw method for feature extraction in automatic music mood classification",
      "authors": [
        "Shasha Mo",
        "Jianwei Niu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Novel audio features for music emotion recognition",
      "authors": [
        "Renato Panda",
        "Ricardo Malheiro",
        "Rui Pedro"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "authors": [
        "Miroslav Malik",
        "Sharath Adavanne",
        "Konstantinos Drossos",
        "Tuomas Virtanen",
        "Dasa Ticha",
        "Roman Jarina"
      ],
      "year": "2017",
      "venue": "Stacked convolutional and recurrent neural networks for music emotion recognition",
      "arxiv": "arXiv:1706.02292"
    },
    {
      "citation_id": "29",
      "title": "Bidirectional convolutional recurrent sparse network (bcrsn): An efficient model for music emotion recognition",
      "authors": [
        "Yizhuo Dong",
        "Xinyu Yang",
        "Xi Zhao",
        "Juan Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Recognition of emotion in music based on deep convolutional neural network",
      "authors": [
        "Rajib Sarkar",
        "Sombuddha Choudhury",
        "Saikat Dutta",
        "Aneek Roy",
        "Sanjoy Saha"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "31",
      "title": "Transfer learning for music classification and regression tasks",
      "authors": [
        "Keunwoo Choi",
        "György Fazekas",
        "Mark Sandler",
        "Kyunghyun Cho"
      ],
      "year": "2017",
      "venue": "Transfer learning for music classification and regression tasks",
      "arxiv": "arXiv:1703.09179"
    },
    {
      "citation_id": "32",
      "title": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "authors": [
        "Eunjeong Koh",
        "Shlomo Dubnov"
      ],
      "year": "2021",
      "venue": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "arxiv": "arXiv:2104.06517"
    },
    {
      "citation_id": "33",
      "title": "End-to-end music emotion variation detection using iteratively reconstructed deep features",
      "authors": [
        "Richard Orjesek",
        "Roman Jarina",
        "Michal Chmulik"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "34",
      "title": "Music emotion recognition based on segmentlevel two-stage learning",
      "authors": [
        "Na He",
        "Sam Ferguson"
      ],
      "year": "2022",
      "venue": "International Journal of Multimedia Information Retrieval"
    },
    {
      "citation_id": "35",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey"
      ],
      "year": "2017",
      "venue": "Proc. of ICASSP 2017"
    },
    {
      "citation_id": "36",
      "title": "A regression approach to music emotion recognition",
      "authors": [
        "Yi-Hsuan Yang",
        "Yu-Ching Lin",
        "Ya-Fan Su",
        "Homer H Chen"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "37",
      "title": "Ranking-based emotion recognition for music organization and retrieval",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Prediction of the distribution of perceived music emotions using discrete samples",
      "authors": [
        "Yi-Hsuan Yang",
        "Homer Chen"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio Speech and Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Predicting the probability density function of music emotion using emotion space mapping",
      "authors": [
        "Yu-Hao Chin",
        "Jia-Ching Wang",
        "Ju-Chiang Wang",
        "Yi-Hsuan Yang"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Music genre and emotion recognition using gaussian processes",
      "authors": [
        "Konstantin Markov",
        "Tomoko Matsui"
      ],
      "year": "2014",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "41",
      "title": "Music emotion recognition with adaptive aggregation of gaussian process regressors",
      "authors": [
        "Satoru Fukayama",
        "Masataka Goto"
      ],
      "year": "2016",
      "venue": "Proc. of ICASSP 2016"
    },
    {
      "citation_id": "42",
      "title": "Modeling the affective content of music with a gaussian mixture model",
      "authors": [
        "Ju-Chiang Wang",
        "Yi-Hsuan Yang",
        "Hsin-Min Wang",
        "Shyh-Kang Jeng"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "A histogram density modeling approach to music emotion recognition",
      "authors": [
        "Ju-Chiang Wang",
        "Hsin-Min Wang",
        "Gert Lanckriet"
      ],
      "year": "2015",
      "venue": "Proc. of ICASSP 2015"
    },
    {
      "citation_id": "44",
      "title": "Hierarchical dirichlet process mixture model for music emotion recognition",
      "authors": [
        "Jia-Ching Wang",
        "Yuan-Shan Lee",
        "Yu-Hao Chin",
        "Ying-Ren Chen",
        "Wen-Chi Hsieh"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Emotion-based music recommendation by association discovery from film music",
      "authors": [
        "Fang-Fei Kuo",
        "Meng-Fen Chiang",
        "Man-Kwan Shan",
        "Suh-Yin Lee"
      ],
      "year": "2005",
      "venue": "Proc. of ACM Multimedia 2005"
    },
    {
      "citation_id": "46",
      "title": "Emotion-based music retrieval on a well-reduced audio feature space",
      "authors": [
        "Bee Maria M Ruxanda",
        "Alexandros Yong Chua",
        "Christian Nanopoulos",
        "Jensen"
      ],
      "year": "2009",
      "venue": "Proc. of ICASSP 2009"
    },
    {
      "citation_id": "47",
      "title": "Emotional states associated with music: Classification, prediction of changes, and consideration in recommendation",
      "authors": [
        "J James",
        "Clement Hc Deng",
        "Alfredo Leung",
        "Li Milani",
        "Chen"
      ],
      "year": "2015",
      "venue": "ACM Transactions on Interactive Intelligent Systems"
    },
    {
      "citation_id": "48",
      "title": "Music retrieval in the emotion plane",
      "authors": [
        "Yang Yi Hsuan",
        "Yu Ching Lin",
        "Heng Tze Cheng",
        "Homer H Chen",
        "Mr"
      ],
      "year": "2008",
      "venue": "Proc. of ACM Multimedia"
    },
    {
      "citation_id": "49",
      "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "authors": [
        "Junyoung Chung",
        "Caglar Gulcehre",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
      "arxiv": "arXiv:1412.3555"
    },
    {
      "citation_id": "50",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "52",
      "title": "Image classification with the fisher vector: Theory and practice",
      "authors": [
        "Jorge Sánchez",
        "Florent Perronnin",
        "Thomas Mensink",
        "Jakob Verbeek"
      ],
      "year": "2013",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "53",
      "title": "Unifying visualsemantic embeddings with multimodal neural language models",
      "authors": [
        "Ryan Kiros",
        "Ruslan Salakhutdinov",
        "Richard Zemel"
      ],
      "year": "2014",
      "venue": "Unifying visualsemantic embeddings with multimodal neural language models",
      "arxiv": "arXiv:1411.2539"
    },
    {
      "citation_id": "54",
      "title": "Dense text retrieval based on pretrained language models: a survey",
      "authors": [
        "Jing Wayne Xin Zhao",
        "Ruiyang Liu",
        "Ji-Rong Ren",
        "Wen"
      ],
      "year": "2022",
      "venue": "Dense text retrieval based on pretrained language models: a survey",
      "arxiv": "arXiv:2211.14876"
    },
    {
      "citation_id": "55",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "56",
      "title": "Feature learning in dynamic environments: Modeling the acoustic structure of musical emotion",
      "authors": [
        "Erik Schmidt",
        "Jeffrey Scott",
        "Youngmoo Kim"
      ],
      "year": "2012",
      "venue": "Proc. of ISMIR 2012"
    },
    {
      "citation_id": "57",
      "title": "On-line continuoustime music mood regression with deep recurrent neural networks",
      "authors": [
        "Felix Weninger",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2014",
      "venue": "Proc. of ICASSP 2014"
    },
    {
      "citation_id": "58",
      "title": "Dblstm-based multi-scale fusion for dynamic emotion prediction in music",
      "authors": [
        "Xinxing Li",
        "Jiashen Tian",
        "Mingxing Xu",
        "Yishuang Ning",
        "Lianhong Cai"
      ],
      "year": "2016",
      "venue": "Proc. of ICME 2016"
    },
    {
      "citation_id": "59",
      "title": "Analysis of the effect of dataset construction methodology on transferability of music emotion recognition models",
      "authors": [
        "Sabina Hult",
        "Bay Line",
        "Sami Kreiberg",
        "Sebastian Brandt",
        "Björn Þór"
      ],
      "year": "2020",
      "venue": "Proc. of ICMR 2020"
    },
    {
      "citation_id": "60",
      "title": "Regression-based music emotion prediction using triplet neural networks",
      "authors": [
        "Kin Cheuk",
        "Yin-Jyun Luo",
        "B T Balamurali",
        "Gemma Roig",
        "Dorien Herremans"
      ],
      "year": "2020",
      "venue": "Proc. of IJCNN 2020"
    },
    {
      "citation_id": "61",
      "title": "Deep gaussian processes for music mood estimation and retrieval with locally aggregated acoustic fisher vector",
      "authors": [
        "Santosh Chapaneri",
        "Deepak Jayaswal"
      ],
      "year": "2020",
      "venue": "Sādhanā"
    },
    {
      "citation_id": "62",
      "title": "The multiple voices of musical emotions: source separation for improving music emotion recognition models and their interpretability",
      "authors": [
        "Angelo Jacopo De Berardinis",
        "Eduardo Cangelosi",
        "Coutinho"
      ],
      "year": "2020",
      "venue": "Proc. of ISMIR 2020"
    },
    {
      "citation_id": "63",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "64",
      "title": "A survey on contrastive selfsupervised learning",
      "authors": [
        "Ashish Jaiswal",
        "Ramesh Ashwin",
        "Mohammad Babu",
        "Debapriya Zadeh",
        "Fillia Banerjee",
        "Makedon"
      ],
      "year": "2021",
      "venue": "Technologies"
    },
    {
      "citation_id": "65",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. of NIPS 2017"
    },
    {
      "citation_id": "66",
      "title": "ABCDM: An attention-based bidirectional CNN-RNN deep model for sentiment analysis",
      "authors": [
        "Shahla Mohammad Ehsan Basiri",
        "Moloud Nemati",
        "Erik Abdar",
        "U Cambria",
        "Acharya"
      ],
      "year": "2021",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "67",
      "title": "EEG-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "Wei Tao",
        "Cheng Li",
        "Rencheng Song",
        "Juan Cheng",
        "Yu Liu",
        "Feng Wan",
        "Xun Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "68",
      "title": "FRÉDÉRIC LI received his engineering degree (equivalent of a M. Sc. degree) in the French Grande Ecole ENSTA Paristech with a specialisation in Robotics and Embedded Systems in 2015. He worked as a research assistant under the supervision of Prof. Dr.-Ing. habil. M. Grzegorzek in the Pattern Recognition Group of the University of Siegen (Germany) between 2016 and 2019, and since",
      "authors": [
        "Naoki Takashima"
      ],
      "year": "2019",
      "venue": "FRÉDÉRIC LI received his engineering degree (equivalent of a M. Sc. degree) in the French Grande Ecole ENSTA Paristech with a specialisation in Robotics and Embedded Systems in 2015. He worked as a research assistant under the supervision of Prof. Dr.-Ing. habil. M. Grzegorzek in the Pattern Recognition Group of the University of Siegen (Germany) between 2016 and 2019, and since"
    }
  ]
}