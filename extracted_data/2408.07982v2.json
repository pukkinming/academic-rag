{
  "paper_id": "2408.07982v2",
  "title": "Toward A Dialogue System Using A Large Language Model To Recognize User Emotions With A Camera",
  "published": "2024-08-15T07:03:00Z",
  "authors": [
    "Hiroki Tanioka",
    "Tetsushi Ueta",
    "Masahiko Sano"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The performance of ChatGPT© and other LLMs has improved tremendously, and in online environments, they are increasingly likely to be used in a wide variety of situations, such as ChatBot on web pages, call center operations using voice interaction, and dialogue functions using agents. In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots. In this multimodal dialogue, mutual emotion recognition between the AI and the user will become important. So far, there have been methods for expressing emotions on the part of the AI agent or for recognizing them using textual or voice information of the user's utterances, but methods for AI agents to recognize emotions from the user's facial expressions have not been studied. In this study, we examined whether or not LLM-based AI agents can interact with users according to their emotional states by capturing the user in dialogue with a camera, recognizing emotions from facial expressions, and adding such emotion information to prompts. The results confirmed that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "In August 2024, some teams of Center for Administration of Information Technology (abbreviated: AIT center) in Tokushima University will be moving out. Since the move will be to a different building, it was discussed to put up a sign or other information in the original room. Ideas for the signs included noting the location of the new rooms and a QR code for a Web page that would provide detailed directions. The idea is that it is effective to use both analog as well as digital methods. However, we are the AIT center that should develop the most digital services within the university. The initial motivation for this study is to take this opportunity one step further and open an online contact point.\n\nIn order to set up an online window, digital devices (terminals) such as tablets are essential. Some suggested that a university character (Tokupon) could serve as a receptionist on standby. If it goes that far, it would be like a receptionist robot. To realize this, a tablet terminal or similar device should be set up in front of the room before the move, ready for online meetings. Online meetings can be conducted remotely by a representative waiting in the new room or by a pseudo-Artificial Intelligence agent (AI agent). Therefore, we considered using ChatGPT© one of the representatives of Large Language Models(LLMs), as the AI agent. 1 Center for Administration of Information Technology, Tokushima University, 770-8506 Tokushima, Japan † tanioka.hiroki@tokushima-u.ac.jp\n\nIn this case, the person in charge of interacting remotely can see the other person's facial expression while conversing if the conversation is conducted in an online conference. However, LLMs usually recognize only textual information; some services, such as ChatGPT-4o  [1]  and Gemini  [2] , can recognize images and text simultaneously, but special prompts must be developed to capture them as the facial expressions of the interlocutor. In this study, we propose a method to recognize emotional information from the user's facial expression captured by a camera and hand it over to the LLM as a prompt together with the dialogue content.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Research",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotion Recognition Technologies",
      "text": "Recently, a technique called EmotionNet  [3] , which applies convolutional neural networks to the recognition of emotions in human facial photos, has been proposed. Li-breFace  [4]  uses deep learning technology to recognize emotional expressions with higher accuracy than conventional methods. In addition, techniques for recognizing emotions from images, such as those using the Vision Transformer, have been proposed by Fuyan Ma et al  [5] . Since ChatGPT-4o is a multimodal AI, it can be recognize emotions from facial expressions. However, in this study, we employed FER instead of ChatGPT-4o because of the need for image recognition technology that can work locally (or embedded), taking personal information into consideration. FER is a Python library trained using Dataset  [6] . The model is based on the Convolutional Neural Network proposed in Octavio Arriaga et al.  [7] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Multimodal Interactive Robot",
      "text": "A survey article  [8] , which conducted a systematic review using the PRISMA protocol, found that task-based conversations are becoming more systematic, that there are improvements in the recognition rate of speech recognition, and that the appearance and expressiveness of the robot must be improved. The results of this study show that there is a need to improve the appearance and expressiveness of the robot. It is also reported that multimodal cues such as gestures, eye gaze, or facial expressions can be used. Miyama et al.  [9]  attempted to enrich task-based conversations with multimodal interaction by an android robot, with good results. DFER-CLIP  [3]  attempts to use LLM to recognize emotions for multimodal information, while ChatGPT-4o  [1]  and Gemini  [2]  are also being used to combine voice and image with text information. Multimodal conversations combining voice, images, and textual information are becoming possible.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Approach Our Proposed Llm-Based Facial Expression Recognition",
      "text": "ChatBot is called FacingBot (FBot). We employ a python library FER. The FBot system configuration is shown in Figure  1 . First, the user's face to be interacted with was generated by ChatGPT-4o using the face of a Japanese woman with four different facial expressions: normal, smiling, angry, and sad, as shown in Figure  2 . The generated face images were printed out on paper and recognized in the form of a photograph taken with the built-in camera of a laptop computer. Next, gpt-3.5-Turbo  [10]  was used as the LLM for dialogue in natural language. In addition to simply entering dialogue sentences, emotional information obtained using FER was added to the prompts in JSON(JavaScript Object Notation) format.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotional Expressions In Json Format",
      "text": "Figure  3  shows an example of recognition results for a single smiling image. \"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", and \"neutral\" each take a value between 0 and 1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "{",
      "text": "' a n g r y ' : 0 . 0 3 , ' d i s g u s t ' : 0 . 0 , ' f e a r ' : 0 . 1 2 , ' happy ' : 0 . 4 8 , ' s a d ' : 0 . 2 2 , ' s u r p r i s e ' : 0 . 0 , ' n e u t r a l ' : 0 . 1 4 } Fig.  3 . Example of emotional expressions for a smile face in JSON format.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Query Prompt With Json",
      "text": "The prompt simply concatenated the message by string and the emotional information in JSON format, as expressed in the formula (1).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "[Prompt] = [Message] + [Emotion(Json)]",
      "text": "(1) Figure  4  is an example of a prompt when the message is \"Hello.\" and the query is made with a smile.  First, prepare multiple face images of one user interacting with gpt-3.5-turbo. Next, prepare several scenarios of interaction with gpt-3.5-turbo. Then, we will import and implement the FER and OpenAI libraries on Google Colaboratory using Python 3.10.12. In the experiment, the user interacts with gpt-3.5-turbo according to the scenarios, and we will investigate how different facial expressions affect gpt-3.5turbo's response.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Preparation Of Face Images",
      "text": "Using ChatGPT-4o, four facial expressions were generated for a Japanese woman who came for a consultation. In this case, four types of facial expressions are used: Normal, which has no emotional expression, a smiling face, an angry face, and a sad face, simply because she is asking for advice. They were captured using the built-in camera of a laptop computer, and the results of emotion recognition by FER are shown in Figure  2 . From the images, we found that women's facial expressions are either Happy or Neutral when normal, but almost Happy when smiling, Angry when angry, and Sad when sad, with higher score values for sad faces.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Dialogue Scenario",
      "text": "Two dialogue scenarios were used: Case A was simply saying \"Hello.\" and by looking at the differences in FBot's responses when spoken to with a normal expression, a smile, an angry face, or a sad face, we can see if the emotional expressions added in JSON format were correctly recognized. In the other case B, the user first asks \"How can I comfort a friend with a broken heart?\" and responds to FBot's answer with \"Thank you.\" followed by \"Bye.\" At this point, we see what kind of response we get for the four different facial expressions.\n\n• Case A User \"Hello.\" (A1) FBot (Answer for A1) • Case B User \"How can I comfort a friend with a broken heart?\" (B1) FBot (Answer for B1) User \"Thank you.\" (B2) FBot (Answer for B2) User \"Bye.\" (B3) FBot (Answer for B3)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Results",
      "text": "Figure  5  shows how the user says \"Bye.\" with a smile in response to FBot. As shown in Table  I , when spoken to with a smile, the response is that the speaker is pleased to be smiling. For instance, if the user inputed \"Hello.\" with smile face, FBot responded \"I'm glad you are happy! How can I help you today?\" On the other hand, if the face is angry or sad, there is a caring remark. For instance, if the user inputed \"Hello.\" with angry face, FBot responded \"I'm here to listen and support you. If you'd like to talk about what's on your mind or if there's anything I can do to help, feel free to share. Remember, it's okay to feel a range of emotions, and I'm here to offer assistance in any way I can. Take a deep breath, and know that you're not alone.\" These results confirm that it is possible to create a dialogue based on the user's emotional state by adding the emotion information recognized by FER in JSON format to gpt-3.5-turbo and sending it to a conversation using text strings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Discussion",
      "text": "The following is a discussion of the experimental results. First, we confirmed that gpt-3.5-turbo can recognize emotional states from users' facial expressions using FER. Next, we confirmed that gpt-3.5-turbo can respond to multimodal queries with emotion information according to the emotional state. We also confirmed that gpt-3.5-turbo does not respond according to the emotional state when the emotional state is ambiguous.\n\nOther issues that were found through the experiments are that emotion recognition using FER fluctuates the recognition results depending on the proximity, brightness, and angle of the face. Furthermore, there are still issues to be considered, such as whether to acquire the average or the maximum value of the emotion information within a certain period of time before the chat program speaks to the user. The average facial expression within a certain time period needs to be calculated, or the facial expression immediately after the chat string is sent may be important. This area needs to be investigated and studied, and the system design needs to be reviewed.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, for multimodal interaction by AI agents, we designed and developed a system that, when a user makes an inquiry offline, sends visual information, especially emotional information obtained from facial expressions, along with the inquiry to the LLM to ensure that the system can respond according to the facial expressions of the user. Experimental results showed that LLM can respond differently to smiling, angry, and sad faces respectively. However, further research is needed on how to summarize the emotional information added to the text of the query by the user, since the facial expressions of the user are not always constant and the recognition results by FER are also variable. Based on the results of this research, the system could be applied to offline receptionist operations using AI agents, online receptionist operations using AI agents, information sharing systems in university laboratories, and university operations such as medical institutions. It could also be used to understand the psychological state of athletes and coaches from their facial expressions, to understand the state of athletes and teams, and to improve the training efficiency of athletes. In the field of information security, it may be possible to determine whether a person is suspicious or not from his/her facial expression by combining visual information and query information in reception work that requires identification, such as password re-issuance at universities and companies.\n\nIn the future, we would like to develop an algorithm to more accurately grasp emotions from facial expressions, develop a learning model to recognize the emotions of users wearing masks, and conduct research and development of a multimodal dialogue system that combines speech recognition.",
      "page_start": 3,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of FacingBot(FBot) system.",
      "page": 1
    },
    {
      "caption": "Figure 1: First, the user’s face to be interacted with was gen-",
      "page": 2
    },
    {
      "caption": "Figure 2: The generated face images",
      "page": 2
    },
    {
      "caption": "Figure 3: shows an example of recognition results for a",
      "page": 2
    },
    {
      "caption": "Figure 3: Example of emotional expressions for a smile face in JSON format.",
      "page": 2
    },
    {
      "caption": "Figure 4: is an example of a prompt when the message is",
      "page": 2
    },
    {
      "caption": "Figure 4: Example of user prompt with JSON (for a smile face).",
      "page": 2
    },
    {
      "caption": "Figure 5: Screenshot of the proposed system execution screen",
      "page": 2
    },
    {
      "caption": "Figure 2: From the images, we found that women’s",
      "page": 2
    },
    {
      "caption": "Figure 2: Faces (upper left: normal, upper right: smiling, lower left: angry, and lower right: sad) of Japanese woman who visiting for consultation.",
      "page": 3
    },
    {
      "caption": "Figure 5: shows how the user says “Bye.” with a smile",
      "page": 3
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Chatgpt-4o",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "Chatgpt-4o"
    },
    {
      "citation_id": "2",
      "title": "Gemini",
      "authors": [
        "Google"
      ],
      "year": "2024",
      "venue": "Gemini"
    },
    {
      "citation_id": "3",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Libreface: An open-source toolkit for deep facial expression analysis",
      "authors": [
        "D Chang",
        "Y Yin",
        "Z Li",
        "M Tran",
        "M Soleymani"
      ],
      "year": "2024",
      "venue": "2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV57701.2024.00802"
    },
    {
      "citation_id": "5",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "Z Chuang",
        "Y Bengio"
      ],
      "year": "2013",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "7",
      "title": "Realtime convolutional neural networks for emotion and gender classification",
      "authors": [
        "O Arriaga",
        "M Valdenegro-Toro",
        "P.-G Plöger"
      ],
      "year": "2017",
      "venue": "ArXiv"
    },
    {
      "citation_id": "8",
      "title": "A survey on dialogue management in human-robot interaction",
      "authors": [
        "M Reimann",
        "F Kunneman",
        "C Oertel",
        "K Hindriks"
      ],
      "year": "2024",
      "venue": "J. Hum.-Robot Interact",
      "doi": "10.1145/3648605"
    },
    {
      "citation_id": "9",
      "title": "A multimodal dialogue system for customer service based on user personality adaptation and dialogue strategies",
      "authors": [
        "T Miyama",
        "S Okada"
      ],
      "year": "2024",
      "venue": "Advanced Robotics",
      "doi": "10.1080/01691864.2024.2319137"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": ""
    }
  ]
}