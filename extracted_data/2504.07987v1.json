{
  "paper_id": "2504.07987v1",
  "title": "Mixeeg: Enhancing Eeg Federated Learning For Cross-Subject Eeg Classification With Tailored Mixup",
  "published": "2025-04-07T06:24:23Z",
  "authors": [
    "Xuan-Hao Liu",
    "Bao-Liang Lu",
    "Wei-Long Zheng"
  ],
  "keywords": [
    "EEG",
    "Federated Learning",
    "mixup",
    "Domain Generalization",
    "Domain Adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The cross-subject electroencephalography (EEG) classification exhibits great challenges due to the diversity of cognitive processes and physiological structures between different subjects. Modern EEG models are based on neural networks, demanding a large amount of data to achieve high performance and generalizability. However, privacy concerns associated with EEG pose significant limitations to data sharing between different hospitals and institutions, resulting in the lack of large dataset for most EEG tasks. Federated learning (FL) enables multiple decentralized clients to collaboratively train a global model without direct communication of raw data, thus preserving privacy. For the first time, we investigate the crosssubject EEG classification in the FL setting. In this paper, we propose a simple yet effective framework termed mixEEG. Specifically, we tailor the vanilla mixup considering the unique properties of the EEG modality. mixEEG shares the unlabeled averaged data of the unseen subject rather than simply sharing raw data under the domain adaptation setting, thus better preserving privacy and offering an averaged label as pseudo-label. Extensive experiments are conducted on an epilepsy detection and an emotion recognition dataset. The experimental result demonstrates that our mixEEG enhances the transferability of global model for cross-subject EEG classification consistently across different datasets and model architectures. Code is published at: https://github.com/XuanhaoLiu/mixEEG.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Brain-Computer Interfaces (BCIs) based on electroencephalography (EEG) signals have been significantly facilitating the health care of people in a wide range of areas, encompassing cognitive research  (Tan et al., 2024; Finley et al., 2024; X.-H. Liu, Liu, et al., 2024) , epilepsy detection  (Shoeb & Guttag, 2010; Tasci et al., 2023) , and emotion recognition  (Aggarwal & Chugh, 2022; Jiang, Liu, Zheng, & Lu, 2023 , 2024) . A common but challenging practical requirement of EEG BCI is training an EEG BCI that achieves good performance on unseen subjects, which is also known as cross-subject EEG classification. Due to the diversity of cognitive processes and physiological structures, electroencephalogram (EEG) data present significant individual differences. Modern EEG BCIs are largely based on neural networks (NN) and it is widely acknowledged that NN follow the Scaling Law  (Aghajanyan et al., 2023; Cherti et al., 2023) , i.e., the performance of NN improves when scaling up the amount and quality of training data. However, due to the expensive collection costs and privacy concerns (X.-H.  Liu, Song, He, Lu, & Zheng, 2024) , there is no public EEG dataset that is large enough to train well generalizable EEG BCIs. The majority of EEG data around the world is stored in various medical hospitals or research institutes in the format of small datasets. How can we leverage these dispersed data resources to train more generalizable EEG BCIs? Federated learning (FL) allows multiple decentralized clients to collaboratively train a global model without direct communication of raw data (R.  Liu et al., 2024) . For instance, during each communication round of the FedAvg  (McMahan, Moore, Ramage, Hampson, & y Arcas, 2017) , the clients update its local model exclusively with their own data, and upload the parameters of their local model to the global server, where the global model is then updated through the aggregation of parameters from these local models. Although there are many studies adopted FL in EEG-based BCI  (Ju et al., 2020; Baghersalimi, Teijeiro, Atienza, & Aminifar, 2021; X. Chen, Niu, Zhao, & Qin, 2024; Chan, Zheng, Xu, Wang, & Heng, 2024) , these studies predominantly focused on improving the performance of the global model on existing subjects of each client, overlooking the practical requirement of cross-subject EEG classification on unseen subjects.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Global Server",
      "text": "Basically, there are two cross-subject settings: domain generalization (DG) and domain adaptation (DA). While DG requires no data from unseen subjects, DA allows the trainer to access the unlabeled data from unseen subjects. For the first time, we investigate these two settings in FL, i.e., the DG FL setting and DA FL setting. Illustrated in Figure  1 , these two settings require training a global model on multisource domains while preserving the data privacy of each source domain. With the strict prohibition of data communication among source domains, the DG FL and DA FL problems present heightened challenges compared to traditional DG and DA settings, in which the global server can directly access to all source domains data.\n\nInspired by mixup (H. Zhang, Cisse, Dauphin, & Lopez-Paz, 2018), a simple yet surprisingly effective data augmentation technique, we propose a novel FL framework called mixEEG to tackle the two proposed problem settings. Specifically, we tailor the vanilla mixup for EEG modality to unleash the potential of mixup for developing better crosssubject EEG BCI. Solely through mixing up the local EEG data of each client, our mixEEG already enhances the generalizability of global models under the DG FL setting. By adopting the unlabeled data from the target domain, our mixEEG achieves even better performance under the DA FL setting.\n\nIn summary, the contributions of this paper are as follows:\n\n• New problem: We for the first time investigate two new problem settings for cross-subject EEG FL: the DG FL setting and the DA FL setting.\n\n• New framework: We introduce a new FL framework called mixEEG to improve the transferability of global models trained by FL methods.\n\n• New mixup: Beyond the vanilla mixup, which linearly interpolates raw data, we specially design and investigate two new mixup approaches for EEG data: the Channel Mixup and the Frequency Mixup.\n\n• Extensive Experiments: We conduct extensive experiments on two public datasets and two common network architectures to comprehensively investigate different mixEEG with diverse tailored mixup.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Cross-subject EEG Classification\n\nThe inter-subject variability of EEG signals has hindered the promotion of EEG BCIs for a long period of time. Previous DG methods predominantly concentrated on traditional supervised learning or unsupervised learning. The mixtureof-experts are adopted to learn different brain regions representations (X.-H.  Liu, Jiang, Zheng, & Lu, 2024) . Contrastive learning enables networks to learn the difference among features, which is applied in both a graph-based multitask self-supervised learning (GMSS)  (Li et al., 2022 ) and a prototype contrastive domain generalization (PCDG)  (Cai & Pan, 2023) . However, these methods cannot be directly adopted for training a global model with multiple decentralized clients, limiting its access to large amount of data. Our mixEEG framework enables BCI developer to train a global model with the collaboration of multiple decentralized clients while preserving privacy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Eeg Federated Learning",
      "text": "Considering the privacy preservation of EEG, many previous works introduced FL in EEG classification tasks. R.  Liu et al.  proposed FLEEG to to surmount the challengings of the heterogeneity of EEG devices among different clients (hospitals and institutions) (R.  Liu et al., 2024) . FedEEG  (Hang et al., 2023)  used an inter-subject structure matching-based FL framework to extract the discriminative features for improving the performance of each clients. Mongardi and Pinoli explored the potential of federated learning for the task of emotion recognition from BCI data, focusing on performance with respect to centralized approaches  (Mongardi & Pinoli, 2024) . Despite there are lots of research on the EEG FL, the cross-subject issues are largely unexplored. Hence, we propose mixEEG to enhance the EEG FL for cross-subject EEG classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mixup",
      "text": "Mixup (H.  Zhang et al., 2018)  is a simple yet surprisingly effective data augmentation technique, which linearly interpolates two data's inputs and labels to generate a mixup data. By adopting mixup strategy, centralized learning is able to train a more robust model with the same training dataset. Recently, lots of paper utilized mixup for fedreated learning in many research fields, like medical image segmentation  (Wicaksana et al., 2022) , vertical FL  (Cheng et al., 2024) , and image classification  (Yoon, Shin, Hwang, & Yang, 2021) . Previous EEG classification work simply employed the vanilla mixup  (Zhou et al., 2024; Yao, Liu, Lu, Wu, & Li, 2024) . To tailor the vanilla mixup for diverse data modality, many works like mixing up data in the latent space  (Verma et al., 2019) , detecting the saliency feature for images  (Uddin, Monira, Shin, Chung, & Bae, 2021) , and mixup for natural language (H.  Chen, Han, Yang, & Poria, 2022)  were proposed. Taking the properties of EEG modality with multiple channels and frequency bands into consideration, we naturally propose Channel Mixup and Frequency Mixup for EEG modality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method Problem Statement",
      "text": "Let D s \" tX s ,Y s u and D t \" tX t ,Y t u denote the source domain dataset and the target domain dataset. There are two normal cross-subject settings, domain generalization (DG) and domain adaptation (DA). Different from DG setting which requires training DL models only using the source dataset D s , DA setting allows trainers to have additional access to the unlabeled target data X t .\n\nUnder the federated learning setting, there are K clients whose datasets multiple sub-dataset D k s , where k P 1, ..., K. For the purpose of privacy preservation, the direct data communication between clients is strictly forbidden, i.e., each client train their local model only using their own sub-dataset D k s . The goal of DG FL is to train a global model that can generalize to D t .\n\nIn contrast to DG FL, DA FL setting allows clients to access the unlabeled data X t from target domain, making the cross-subject isssue easier but damaging the privacy of target domain D t . Hence, it is necessary to consider the trade-off of the privacy preservation and the data sharing. We define a sharing ratio of r P r0, 1s, denoting the ratio of the number of the shared data to the target domain data. When r \" 0, the DA FL setting is equal to the DG FL setting. Larger r represents more data communication but less privacy preservation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline: Fedavg",
      "text": "Federated Averaging (FedAvg) is the most commonly used algorithm framework in FL  (McMahan et al., 2017) . For every communication round t \" 0, ..., T ´1, a part of clients train their own local models with their own sub-dataset D k s . We denote the fraction of the number of clients participating in the update to the total number of clients as φ. Under the DA FL setting, each client also has the access to the unlabeled shared data from target domain dataset X t . Afterwards, client k sends the model weight w k t to the global server, which update the parameters of global model by simply averaging the client parameters received by:\n\nThe updated global model w t is sent back to clients for the next round local training. The process will undergoes T times or until the global model converges.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Tailored Mixup",
      "text": "Mixup (H.  Zhang et al., 2018)  is a simple data augmentation technique using a linear interpolation between two data-label pairs px i , y i q and px j , y j q to produce a mixup data-label pair r x \" λx i `p1 ´λqx j , r y \" λy i `p1 ´λqy j ,\n\nwhere λ P r0, 1s is a hyperparameter. Through this elegant augmentation, model gains better robustness and generalization (L.  Zhang & Deng, 2021; Carratino, Cissé, Jenatton, & Vert, 2022) . In this paper, we explore and validate three mixup methods on DE features as shown in Figure  2 . It is worth noting that these tailored mixup might not always useful as we are the first to explore them.\n\n1) Linear Mixup: The same as the original mixup.\n\n2) Channel Mixup: Channel mixup could help the model learn localized spatial representations robust to inter-subject variability in channel activations. There are C channels for EEG data, we separate the whole C channels set into two non-overlapping subsets C 1 and C 2 . The mixup data is:\n\nwhere M r pq P t0, 1u CˆF denotes a binary mask. The rows of M r pC 1 q corresponding to the channels in C 1 are 1, and other rows are 0. 3) Frequency Mixup: Frequency mixup may enable capturing discriminative features across multiple spectral bands, which is known to be important for EEG analysis tasks.\n\nThere are F frequency bands for EEG data, we separate the whole F frequency bands set into two non-overlapping subsets F 1 and F 2 . The mixup data is:\n\nwhere M c pq P t0, 1u CˆF denotes a binary mask. The columns of M c pF 1 q corresponding to the frequency bands in F 1 are 1, and other columns are 0.\n\nmixEEG For FedAvg algorithm framework, the local clients updates the client model via stochastic gradient descent (SGD) for E epochs:\n\nHere, η is learning rate, L is loss function, and f px, wq is the model output of the input x given model weight w.\n\nAlgorithm 1 mixEEG under the DA FL setting\n\ns for k \" 1, ..., N, D t , sharing ratio r P r0, 1s, when r \" 0, this algorithm is FedAvg. 2: Initialize w 0 for global model. 3: X g Ð GetSharingDatapD t , rq 4: for t \" 0, ..., T ´1 do 5:\n\nfor k \" 1, ..., N do 6:\n\nend for 8: for batchpX, Yq do 5:\n\nSelect a sharing data x g from X g 6:\n\nX m Ð mixup X with x g 7:\n\nL 1 Ð Lp f pX m ; wq, Yq w Ð w ´ηt`1 ∇pλL 1 `p1 ´λqL 2 q 10: end for 11: end for 12: return w\n\nThe SGD algorithm for updating local models usually leads these models to overfit their own subset, damaging the global model's generalizability and making the global model hard to do cross-subject EEG classification. To this end, we propose mixEEG, a simple but effective framework for enhancing the generalizability of the global EEG model learned under the FL setting.\n\nFor the DG FL setting, mixEEG simply replacing the local updating strategy of local clients from SGD with mixup, i.e. using mixup technique to update the local model w k t . We validate several different mixup strategies to fully investigate which type of mixup is suitable for EEG modality.\n\nFor the DA FL setting, local clients are able to see the r ¨|D t | unlabeled data. Here are two questions: Q1: what is the shared data? Q2: how do local clients leverage these unlabeled data?\n\nFor question Q1, we argue that directly sharing the subset of X t definitely harm the privacy security of the target domain. So instead, we propose to generate shared data by averaging the target domain data X t . Specifcally, we define a hyperparameter s P Z `, indicating the number of data used for aggregating a single averaged data. We randomly shuffle the X t , and linearly interpolating a number of s data from X t to generate a single shared data by Eq 6. This process is repeated r ¨|D t | times to get enough shared data.\n\nFor question Q2, by assuming that the appearing probability of each emotion in the target dataset is equal, we can generate a pseudo label for each shared data: ỹt \" 1 c r1, 1, ..., 1s P R c , where c is the number of emotion classes. After generating the shared data and correpsonding labels, mixEEG can leverage these data from target dataset to improve the transfer ability. Unlike the DG FL issue, where we mixup data within a single batch, we mixup each data-label pairs px i s , y i s q from D k s with the shared data. Decoding Onepq as the onehot encoding, the corresponding mixup label is: r y \" λOnepy i s q `p1 ´λq",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments Eeg Datasets",
      "text": "We exploit two datasets: an epilepsy detection dataset  (Shoeb & Guttag, 2010)  and an emotion recognition dataset  (Zheng & Lu, 2015) . Differential Entropy (DE) features  (Duan, Zhu, & Lu, 2013)  are extracted for classification.\n\nEpilepsy Detection (ED): CHB-MIT dataset is an epilepsy dataset required from 23 patients. We cropped and resampled the CHB-MIT dataset to build an ED dataset with four types of EEG: ictal, preictal, postictal, and interictal phase EEG.\n\nEmotion Recognition (ER): SEED dataset records the 62channels EEG data corresponding to 3 types of emotions from 15 subjects: positive, neutral, and negative.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "Each subject is regarded as a decentralized source domain, and the results under leave-one-subject-out (LOSO) settings are reported. Specifically, we select n ´1 subjects as source domains, train a global model using FL methods on the source domains (the direct data communication among source domains) and test the performance on the last remaining subject, which is regarded as target domain.\n\nThe number of update round of the global server T \" 50, for each client, the number of local update round E \" 5. We use SGD as the optimizer, with its learning rate η \" 0.01. The fraction φ is set to 0.2 for ER dataset, and 0.3 for ED dataset. The sharing ratio r is 0.1 and aggregating hyperparameter s \" 10 for DA FL setting.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eeg Models",
      "text": "There are many methods for the classification of EEG data. We choose the following two type of models: 1) MLP: Multi-Layer Perception, here we use an MLP with 2 hidden layers of 128 and 64 neurons. 2) CNN: Convolutional Neural Network, here we use a 3 layer CNN with kernal sizes of {(3,3), (3,3), and (7,1)}. The activate function is the GELU function and the dropout probability is 0.5 for all models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eeg Mixup Appoaches",
      "text": "Basically, we evaluate three mixup ways, but there are more implementation details to evaluate: 1) Linear Mixup: In the origin paper, λ \" Betapα, αq, α P p0, `8q, we test two cases: α \" 0.2 and α \" 5. 2) Channel Mixup: We test two cases:\n\n(1) dividing the EEG channels into the left scalp and the right scalp; (2) dividing the EEG channels equally into two sets randomly. We set λ \" 0.5 for both cases. 3) Frequency Mixup: For the DE features with five frequency bands (δ, θ, α, β, and γ), we test two cases: (1) dividing the EEG frequency bands into the tα, β, γu and tδ, θu; (2) dividing the EEG frequency bands into the tδ, α, γu and tθ, βu. We set λ \" 0.6 for both cases.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dg Fl Setting",
      "text": "Under DG FL settings, all clients can only access to their own data. We compare our mixEEG with the baseline FedAvg It can be seen that for MLP architecture, Linear Mixup is the best for ED task, while Channel Mixup with Binary Split is the best for ER task. However, for CNN architecture, Linear Mixup is always better across two datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Da Fl Setting",
      "text": "Under DA FL settings, there are r ¨|D t | unlabeled data from target domain sharing to local clients. The results are presented in Table  2 , it can been observed that mixEEG enhances the performance from 46% to 49% for the ED task and from 90% to 93% for the ER task.\n\nIn this setting, the Channel Mixup method with Binary Split outperforms other mixup strategies for MLP architecture across two datasets. While for CNN architecture, the Linear Mixup exhibits the best improvement on these two tasks. However, we notice that the performance of Frequency Mixup doesn't work well under the DA FL setting, except for MLP architecture on the ED dataset, Frequency Mixup always harms the generalizabitliy for other cases. Taking the performance of the DG FL setting into consideration, we can conclude that Frequency Mixup might not be suitable for EEG modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "Linear Mixup Hyperparameter α Through the experiments of the DG/DA FL settings, it can be found that the performance of Linear Mixup are influenced by the hyperparameter α. Thus, we conduct an extra experiment to evaluate the impact of α. We choose 14 values for α and exhibits the performance of CNN model on the ED dataset, as shown in Fig 3 . It can be seen that in this case, the best performance is achieved when α is around 0.3. When α is larger than 1.0, the performances are similar.    Sharing ratio and Aggregating number We conduct additional experiments to validate the effectiveness of the sharing data by adjusting the sharing ratio r and the aggregating hyperparameter s. The range of r is from 0.01 to 0.1 and the range of s is from 5 to 50 (step is 5). Since this experiment is quite large (consists of 100 sets of hyperparameters), we only run the global model for 10 epochs, i.e., T \" 10 in this case. Displayed in Fig 5, it can be observed that the performances increase when sharing ratio r increases, indicating that more sharing data helps cross-subject EEG classification. We can also see that though the performances fluctuates, larger aggregation s achieves better results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Representation Visualization",
      "text": "To better present the difference between each mixup strategies and explore the reason why some mixup strategies are better on learning transferable features, we adopt UMAP technique  (McInnes, Healy, & Melville, 2018)   that FedAvg's feature has some chaotic distribution on the left up side. Linear Mixup tends to learn more dispersed features, thus may increase the generalizability in some cases. In this case (MLP on ER task), the Channel Mixup with Binary is the best, it can be seen that the learned feature are more distinguishable since the features of the same class are aggregated more tightly. Frequency Mixup also enables the global model to learn more tightly aggregated features, but the learning on the boundaries between classes is relatively vague, leading to suboptimal generalizabitliy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The problem setting of domain generalization fed-",
      "page": 1
    },
    {
      "caption": "Figure 2: Three EEG mixup methods: (a) Linear Mixup, (b)",
      "page": 3
    },
    {
      "caption": "Figure 3: It can be seen that in this case, the best performance",
      "page": 5
    },
    {
      "caption": "Figure 3: The accuracy, F1 score and Cohen’s Kappa score",
      "page": 5
    },
    {
      "caption": "Figure 4: The accuracy of the global model trained with dif-",
      "page": 5
    },
    {
      "caption": "Figure 5: The 3D heatmap of the LOSO results on the SEED",
      "page": 6
    },
    {
      "caption": "Figure 5: , it can be observed that the performances",
      "page": 6
    },
    {
      "caption": "Figure 6: , where we plot the learned feature",
      "page": 6
    },
    {
      "caption": "Figure 6: The UMAP visualization of learned feature on the",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The DA FL setting: The accuracy, F1 score and Cohen’s Kappa score (%) under the DA FL setting. L, C, and F",
      "data": [
        {
          "9 0 %) 9 0 %)\n8 0 accuracy ( 8 8 0 5 accuracy (\n7 0 7 5\n7 0\n6 0 6 5\n60\n50\n50 50\n3 0 4 0 tion s 3 0 4 0 tion s\n0.0 s 2 ha 0 r . in 0 g 4 r 0 a . t 0 io 6 r 0.08 0.10 10 2 a 0 g gr eg a 0.0 s 2 ha 0 r . in 0 g 4 r 0 a . t 0 io 6 r 0.08 0.10 10 2 a 0 g gr eg a\n(a) Linear Mixup =0.2 (b) Channel Mixup Binary": "",
          "Column_2": "%)\naccuracy ("
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Review of machine learning techniques for EEG based brain computer interface",
      "authors": [
        "S Aggarwal",
        "N Chugh"
      ],
      "year": "2022",
      "venue": "Archives of Computational Methods in Engineering"
    },
    {
      "citation_id": "2",
      "title": "Scaling laws for generative mixed-modal language models",
      "authors": [
        "A Aghajanyan",
        "L Yu",
        "A Conneau",
        "W.-N Hsu",
        "K Hambardzumyan",
        "S Zhang",
        ". Zettlemoyer"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "3",
      "title": "Personalized real-time federated learning for epileptic seizure detection",
      "authors": [
        "S Baghersalimi",
        "T Teijeiro",
        "D Atienza",
        "A Aminifar"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "4",
      "title": "Two-phase prototypical contrastive domain generalization for cross-subject EEG-based emotion recognition",
      "authors": [
        "H Cai",
        "J Pan"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "On mixup regularization",
      "authors": [
        "L Carratino",
        "M Cissé",
        "R Jenatton",
        "J.-P Vert"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "6",
      "title": "Adaptive federated learning for EEG emotion recognition",
      "authors": [
        "C Chan",
        "Q Zheng",
        "C Xu",
        "Q Wang",
        "P Heng"
      ],
      "year": "2024",
      "venue": "International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "7",
      "title": "Doublemix: Simple interpolation-based data augmentation for text classification",
      "authors": [
        "H Chen",
        "W Han",
        "D Yang",
        "S Poria"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "An efficient group federated learning framework for large-scale EEGbased driver drowsiness detection",
      "authors": [
        "X Chen",
        "Y Niu",
        "Y Zhao",
        "X Qin"
      ],
      "year": "2024",
      "venue": "International Journal of Neural Systems"
    },
    {
      "citation_id": "9",
      "title": "Fedmix: Boosting with data mixture for vertical federated learning",
      "authors": [
        "Y Cheng",
        "L Zhang",
        "J Wang",
        "X Chu",
        "D Huang",
        "L Xu"
      ],
      "year": "2024",
      "venue": "IEEE 40th International Conference on Data Engineering"
    },
    {
      "citation_id": "10",
      "title": "Reproducible scaling laws for contrastive language-image learning",
      "authors": [
        "M Cherti",
        "R Beaumont",
        "R Wightman",
        "M Wortsman",
        "G Ilharco",
        "C Gordon",
        ". Jitsev"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "6th international IEEE/EMBS Conference on Neural Engineering"
    },
    {
      "citation_id": "12",
      "title": "Resting EEG periodic and aperiodic components predict cognitive decline over 10 years",
      "authors": [
        "A Finley",
        "D Angus",
        "E Knight",
        "C Van Reekum",
        "M Lachman",
        "R Davidson",
        "S Schaefer"
      ],
      "year": "2024",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "13",
      "title": "Multimodal adaptive emotion transformer with flexible modality inputs on a novel dataset with continuous labels",
      "authors": [
        "W Hang",
        "J Li",
        "S Liang",
        "Y Wu",
        "B Lei",
        "J Qin",
        ". Choi",
        "K.-S Jiang",
        "W.-B Liu",
        "X.-H Zheng",
        "W.-L Lu"
      ],
      "year": "2023",
      "venue": "IEEE International Confer-ence on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Seed-vii: A multimodal dataset of six basic emotions with continuous labels for emotion recognition",
      "authors": [
        "W.-B Jiang",
        "X.-H Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Federated transfer learning for EEG signal classification",
      "authors": [
        "C Ju",
        "D Gao",
        "R Mane",
        "B Tan",
        "Y Liu",
        "C Guan"
      ],
      "year": "2020",
      "venue": "42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society"
    },
    {
      "citation_id": "16",
      "title": "Gmss: Graph-based multi-task self-supervised learning for EEG emotion recognition",
      "authors": [
        "Y Li",
        "J Chen",
        "F Li",
        "B Fu",
        "H Wu",
        "Y Ji",
        ". Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Aggregating intrinsic information to enhance bci performance through federated learning",
      "authors": [
        "R Liu",
        "Y Chen",
        "A Li",
        "Y Ding",
        "H Yu",
        "C Guan"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "18",
      "title": "Moge: Mixture of graph experts for cross-subject emotion recognition via decomposing eeg",
      "authors": [
        "X.-H Liu",
        "W.-B Jiang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "2024 ieee international conference on bioinformatics and biomedicine"
    },
    {
      "citation_id": "19",
      "title": "EEG2video: Towards decoding dynamic visual perception from EEG signals",
      "authors": [
        "X.-H Liu",
        "Y.-K Liu",
        "Y Wang",
        "K Ren",
        "H Shi",
        "Z Wang",
        ". Zheng"
      ],
      "year": "2024",
      "venue": "The thirty-eighth annual conference on neural information processing systems (neurips)"
    },
    {
      "citation_id": "20",
      "title": "Professor x: Manipulating eeg bci with invisible and robust backdoor attack",
      "authors": [
        "X.-H Liu",
        "X Song",
        "D He",
        "B.-L Lu",
        "W.-L Zheng"
      ],
      "year": "2024",
      "venue": "Professor x: Manipulating eeg bci with invisible and robust backdoor attack",
      "arxiv": "arXiv:2409.20158"
    },
    {
      "citation_id": "21",
      "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "authors": [
        "L Mcinnes",
        "J Healy",
        "J Melville"
      ],
      "year": "2018",
      "venue": "Umap: Uniform manifold approximation and projection for dimension reduction",
      "arxiv": "arXiv:1802.03426"
    },
    {
      "citation_id": "22",
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "B Mcmahan",
        "E Moore",
        "D Ramage",
        "S Hampson",
        "B Arcas"
      ],
      "year": "2017",
      "venue": "Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "23",
      "title": "Exploring federated learning for emotion recognition on brain-computer interfaces",
      "authors": [
        "S Mongardi",
        "P Pinoli"
      ],
      "year": "2024",
      "venue": "Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization"
    },
    {
      "citation_id": "24",
      "title": "Application of machine learning to epileptic seizure detection",
      "authors": [
        "A Shoeb",
        "J Guttag"
      ],
      "year": "2010",
      "venue": "Proceedings of the 27th International Conference on Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "Theta activity and cognitive functioning: Integrating evidence from resting-state and task-related developmental electroencephalography (EEG) research",
      "authors": [
        "E Tan",
        "S Troller-Renfree",
        "S Morales",
        "G Buzzell",
        "M Mcsweeney",
        "M Antúnez",
        "N Fox"
      ],
      "year": "2024",
      "venue": "Developmental Cognitive Neuroscience"
    },
    {
      "citation_id": "26",
      "title": "Epilepsy detection in 121 patient populations using hypercube pattern from EEG signals",
      "authors": [
        "I Tasci",
        "B Tasci",
        "P Barua",
        "S Dogan",
        "T Tuncer",
        "E Palmer",
        ". Acharya"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "27",
      "title": "Saliencymix: A saliency guided data augmentation strategy for better regularization",
      "authors": [
        "A Uddin",
        "M Monira",
        "W Shin",
        "T Chung",
        "S.-H Bae"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Manifold mixup: Better representations by interpolating hidden states",
      "authors": [
        "V Verma",
        "A Lamb",
        "C Beckham",
        "A Najafi",
        "I Mitliagkas",
        "D Lopez-Paz",
        "Y Bengio"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "29",
      "title": "Fedmix: Mixed supervised federated learning for medical image segmentation",
      "authors": [
        "J Wicaksana",
        "Z Yan",
        "D Zhang",
        "X Huang",
        "H Wu",
        "X Yang",
        "K.-T Cheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Medical Imaging"
    },
    {
      "citation_id": "30",
      "title": "Advancing semi-supervised EEG emotion recognition through feature extraction with mixup and large language models",
      "authors": [
        "S Yao",
        "L Liu",
        "J Lu",
        "D Wu",
        "Y Li"
      ],
      "year": "2024",
      "venue": "2024 ieee international conference on bioinformatics and biomedicine"
    },
    {
      "citation_id": "31",
      "title": "Fedmix: Approximation of mixup under mean augmented federated learning",
      "authors": [
        "T Yoon",
        "S Shin",
        "S Hwang",
        "E Yang"
      ],
      "year": "2021",
      "venue": "9th International Conference on Learning Representations"
    },
    {
      "citation_id": "32",
      "title": "Mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International conference on learning representations"
    },
    {
      "citation_id": "33",
      "title": "How does mixup help with robustness and generalization?",
      "authors": [
        "L Zhang",
        "Z Deng"
      ],
      "year": "2021",
      "venue": "The Ninth International Conference on Learning Representations"
    },
    {
      "citation_id": "34",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "35",
      "title": "EEGmatch: Learning with incomplete labels for semisupervised eeg-based cross-subject emotion recognition",
      "authors": [
        "R Zhou",
        "W Ye",
        "Z Zhang",
        "Y Luo",
        "L Zhang",
        "L Li",
        ". Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    }
  ]
}