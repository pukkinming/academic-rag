{
  "paper_id": "2310.11830v2",
  "title": "Clara: Multilingual Contrastive Learning For Audio Representation Acquisition",
  "published": "2023-10-18T09:31:56Z",
  "authors": [
    "Kari A Noriy",
    "Xiaosong Yang",
    "Marcin Budka",
    "Jian Jun Zhang"
  ],
  "keywords": [
    "Contrastive Learning",
    "multilingual speech",
    "computational para-linguistics",
    "text-to-audio retrieval",
    "audio-to-text retrieval",
    "audio classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In multilingual speech processing, accurately understanding and interpreting emotions is pivotal yet challenging due to the scarcity of labelled datasets. The recent strides in contrastive learning have catalysed self-supervised methodologies, enabling the harnessing of unlabelled data. In light of these advancements, we introduce CLARA, a groundbreaking multilingual framework, as our solution to diminish labelled data dependence and escalate generalisation across a diverse array of languages and conditions. CLARA excels in guiding models to embody shared representations across languages, seamlessly facilitating the cross-lingual transfer of speech and emotional understanding, even amidst scenarios with limited target language data. A cornerstone of our approach is proficiently capturing emotional subtleties within speech, transcending the challenges posed by the subjective nature of perceptual assessments. Embarking into self-supervised learning with a rich multilingual data corpus, we aim to delineate speech representations imbued with emotive dimensions, unlocking new potentials in emotion-aware multilingual speech processing. Employing a substantial corpus of multilingual audio data, our methodology leverages data augmentation techniques to broaden the dataset spectrum, incorporating visual understanding via textual embedding, augmentation of language data from highresource data sources to low-resource languages and model CLARA to learn these representations across domains. Rigorous experimentation demonstrates our model's superior performance across various tasks, such as emotion recognition, multilingual language comprehension, audio classification, and retrieval benchmarks, especially in zero-shot and few-shot scenarios. Our model presents a compelling approach to obtaining shared and adaptable speech representations across languages and acoustic conditions while encoding latent emotional aspects. Additionally, we showcase the model's capability to adapt to lowresource languages, marking a significant stride in multilingual speech representation learning.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "The domain of multilingual speech processing, particularly in natural and emotive speech representations, grapples with substantial challenges stemming from the requirement of extensively annotated datasets across a vast linguistic spectrum. Supervised learning methods that rely on large labelled corpora have achieved high performance in high-resource languages like English. Still, their effectiveness does not transfer well to low-resource languages where transcribed speech and emotionally labelled data in a myriad of environmental condi-tions are scarce. This data dependence limits the development of speech technologies for global users.\n\nRecent research has explored semi-supervised and selfsupervised approaches to learning from unlabelled data by leveraging data abundance to compensate for the lack of annotations. Contrastive learning has emerged as a promising technique for self-supervised representation learning in domains like computer vision and natural language processing. By training models to distinguish between similar and dissimilar sample pairs. The contrastive objective enables models to extract meaningful representations from unlabelled data.\n\nSeveral studies have applied speech representations learning from audio. For instance, AudioCLIP  [1]  demonstrates that contrastive losses can align environmental sounds and visual representations in a shared space. Still, this approach learns a shared representation of labelled data. COLA  [2]  introduces a framework tailored for self-supervised speech representation learning through sample pairs drawn from diverse unlabelled audio, but this approach does not consider the lexical content and solely focuses on learning representations from a single modality (waveform). Furthermore, existing contrastive speech models are trained on single languages and need to fully leverage multilingual data.\n\nLearning generalised multilingual natural language representations is crucial for advancing speech processing across low-resource languages. Hence, we advocate a multifaceted paradigm that encapsulates natural language text descriptors and the emotive states of speakers, aspiring to delve beyond mere lexical interpretation to a comprehensive understanding of the speakers. We incorporate image-based embedding into our natural language text descriptions to further enhance this aspect of our work. This approach allows us to continue building on high-quality data such as EmoV-DB  [3] , where a video recording of the speaker is not available, reducing model complexity and aiding in faster learning.\n\nMultilingual training exposes models to greater diversity in speakers, accents, acoustics, phonetics, and languages. This facilitates learning robust and transferable representations that perform well even for limited target language data. While supervised multilingual models require abundant labelled data, our semi-supervised approach provides an avenue to exploit the abundance of unlabelled multilingual speech.\n\nMotivated by these challenges, we introduce an innovative multilingual framework geared towards assimilating universal speech and emotional representations, employing a rich arXiv:2310.11830v2 [cs.SD] 1 Nov 2023 tapestry of speech data across myriad languages. Our approach learns a shared representation space aligned across languages to enable positive transfer to low-resource languages. We train on an ensemble of unlabelled multilingual speech and video data. We also introduce novel data augmentation techniques that leverage visual cues without dependence on visual information and multilingual natural language captions, unlike similar models, further expanding the diversity of training samples.\n\nThis paper makes the following contributions: 1) A multilingual approach trained to extract a joint representation space from speech across languages while incorporating vision, textual descriptions and transcriptions. 2) Innovative data augmentation techniques that fuse visual and multilingual textual data, significantly amplifying the richness of both labelled and unlabelled datasets. 3) Through rigorous experiments, which unveil a state-of-the-art performance across various downstream speech processing tasks within zero-shot and fewshot frameworks, underlining the substantial advancements our framework brings forth in tackling the identified challenges. These contributions provide an effective approach to learning joint speech and emotion representations that generalise across languages, accents, and acoustic conditions.\n\nThe subsequent sections will elaborate on our proposed approach, model training methodology, and present experimental results. We conclude by discussing key takeaways, limitations, and promising future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Contrastive learning has become an effective technique for unsupervised and weakly supervised representation learning across modalities, including vision, language, and audio. Models trained with contrastive objectives learn robust features from unlabelled data by distinguishing between similar and dissimilar sample pairs. Several pioneering works have successfully applied contrastive learning in computer vision. SimCLR  [4] ,  [5]  introduced a simple framework for image representation learning by maximising agreement between differently augmented views of the same image and minimising agreement between views from different images. MoCo  [6]  proposed a momentum contrast approach using a dynamic dictionary to achieve stateof-the-art image classification.\n\nCLIP  [7]  extends SimCLR by introducing a hybrid loss function that combines cross-entropy and contrastive loss. This approach enables CLIP to learn representations that are capable of language understanding and image recognition tasks, and it has been applied successfully in various applications such as image generation  [8] -  [10]  and machine translation  [11] .\n\nMore recently, contrastive learning has been adapted to the audio domain. AudioCLIP  [1]  builds upon the success of CLIP  [7]  by extending its domain to audio and learning cross-modal representations of audio and visual data through a contrastive loss function. Similarly, COLA  [12]  introduced a self-supervised objective tailored to learn from diverse unlabelled audio data. However, existing contrastive models have focused on single languages.\n\nOur work builds on these prior works by proposing a novel multilingual contrastive learning approach to acquire shared speech representations across languages. We leverage the abundance of diverse multilingual speech and automatic transcriptions, in contrast to supervised methods reliant on scarce labelled corpora. Our data augmentation techniques incorporating visual and textual information further expand training samples for robust representation learning.\n\nIn summary, our framework aims to capitalise on the successes of contrastive learning in other domains by tailoring it to the multilingual speech setting through innovations like cross-lingual pretraining strategies and multimodal data augmentations. This helps overcome the limitations of supervisedonly multilingual models and single-language contrastive approaches for speech and enables several downstream tasks, such as emotion detection, speech-to-text, and expressive speech synthesis and allows users of such model to guide the model via natural language text prompts such as \"A woman speaking is a joyful mood\".",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Corpus",
      "text": "Contrastive models benefit from large datasets. Due to the semi-supervised nature of these models, a considerable amount of analogous and heterogeneous sample pairs is essential to achieve meaningful representation. In contrast, when data is limited, the model may require a more in-depth comprehension of the underlying data structure to ensure accurate predictions. Furthermore, contrastive learning often involves training the model with a vast number of negative samples (i.e., pairs of dissimilar data points), which can be a resource-intensive process. Therefore, a larger dataset can expedite the training process by providing additional negative samples for the model to learn from.\n\nWith this objective in mind, we compiled a collection of openly available speech, music, and sound effects datasets to establish an ensemble dataset. Furthermore, we have employed various augmentation techniques to expand the dataset's magnitude and incorporate speech metadata in a natural manner, including emotional attributes, which we explore in detail in this section.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets",
      "text": "In collaboration with Laion, we compile publicly accessible data sets, incorporating a fusion of meticulously gathered webscraped data and openly available resources. This amalgamated data reservoir consists of pairs of audio and text, encompassing both naturally occurring linguistic interactions and artificially generated language constructs. The latter category encompasses supplementary metadata attributes, including linguistic origin, titular designations, transcriptions, and audio-visual content manifestations such as animal vocalisations or news presenter narratives.\n\nThe collection surpasses 16,000 hours, with approximately 12 million samples attributed to male speakers. In comparison, approximately 3 million samples pertain to female speakers, and approximately 5 million samples remain unclassified regarding gender categorisation. Moreover, around 6,000 hours within this compilation are comprised of environmental sounds. Refer to Appendix A for a comprehensive elaboration on these constituent datasets.\n\nRecent models such as CLIP  [7]  and SimCLR v1/v2  [4] ,  [5]  have demonstrated the potential of large-scale models to comprehend contextual representation. Learning from a massive corpus offers several advantages over alternative techniques. For instance, loosely supervised natural language data obtained from the web is more accessible to scale than conventionally labelled data. Models can passively acquire knowledge from large amounts of loosely labelled data from the web. As demonstrated by CLIP  [7] , these models do not solely learn a representation but also establish connections between the representation and language, permitting the flexible zero-shot transfer.\n\n1) Data Preparation: A standardised format is employed to systematically organize audio-text pairs. Audio samples undergo FLAC conversion and resampling to 48kHz for lossless compression and high-quality distribution, aligning with the EMNS dataset guidelines  [13] . Metadata, containing descriptions like \"A gender saying transcription in a emotion voice,\" is stored in JSON format, where gender, transcription and emotion are datapoint from a given dataset used to form a natural language description. .tar files are used for scalable streaming. For extended audio exceeding 10 seconds, two truncation strategies are implemented. The first employs Montreal Forces Aligner (MFA)  [14]  for precise alignment of speech recordings and transcripts, while the second uses Whisper  [15] , a transformer-based ASR system, to transcribe and truncate lengthy samples. The data pipeline segmentation is visually depicted in Figure  1a .\n\n2) Multimodal Fusion and Text Generation: Chen et al.  [4]  demonstrated the significant impact of data augmentation on improving the performance of contrastive learning within the computer vision field. Other studies have also indicated that data augmentation can enhance the generalisation of supervised neural networks and combat overfitting  [16] ,  [17] . Augmentation techniques increase the diversity of acoustic environments and recording conditions, acquiring robust and invariant features and ultimately regularising the model. Accordingly, we employ various augmentation techniques to diversify speech data for our model.\n\nUnlike previous methods where class labels or simple templates are used. Our augmentation strategy encompasses a comprehensive set of techniques to broaden the diversity of our audio data. These techniques include both traditional Fig.  2 . The proposed framework provides a straightforward approach for augmenting natural language descriptions and incorporating environmental noise layering. Let an, tn be given audio and text pairs, let p t be a prefix string to prepend, and mn and pn n are additional meta information, such as audio quality, gender and emotion. Let f (.) be a schema combining labels to natural language description. Let pn a (pub noise) be a series of environmental sounds and pn t labels. ân, tn be the augmented audio and natural language description. audio augmentation methods and neural approaches (Figure  2 ). In traditional audio augmentation, we apply various transformations such as adding reverb, clipping, masking, pitch modulation, and introducing environmental sounds to simulate different acoustic environments or distorted audio. These transformations are layered on top of the original audio signals to create augmented versions.\n\nFor example, we generate augmented audio samples by combining two raw audio signals, a i and a j , using a scale factor λ that ranges between 0.01 and 0.8 used to control the contribution of environmental sounds (pub noise), where a i is a speech segment and a j the background sounds to be added. The resulting augmented audio â is a linear combination of the two signals. Similarly, the corresponding text labels t i and t j are combined using a function f (.) that employs concatenative and neural-based approaches to generate natural language descriptions. This augmentation pipeline is visually depicted in Figure  2 .\n\nTo overcome the lack of large multilingual datasets for sound event classification. We made use of pre-trained multilingual language models to translate labels into different languages and overlay them on mono-lingual data, synthesising multilingual datasets. This efficient data augmentation approach leverages language models to avoid costly manual collection, rapidly creating diverse training corpora covering many languages to enable multilingual fusion for low-resource language.\n\nTo provide a concrete example, consider the following concatenated text: \"A person saying It's raining outside, background wind noise\", where t i and t j are 'It's raining outside' and 'wind noise' respectively and p t is a given template 'A person saying {}, background {}'. To obtain meaningful text descriptions from this concatenated sentence, we leverage the Language Models such as Open Assistant  [18]  to generate natural language descriptions by processing the concatenated text, incorporating context, and producing coherent and human-like outputs. In our example, the language model generates a description like \"Amidst wind noise in the background, an individual declares, 'It's raining outside'\". We further use CoCa to generate visual captions for samples with both audio and visual elements, employing Structural Similarity Index (SSIM) to identify significant changes in visual frames and generate visual captions accordingly. These visual captions are combined with audio descriptions using a language model to create a unified representation of multimedia content, enhancing caption generation.\n\nIn Appendix B, we elaborate on our augmentation strategy, detailing additional techniques and their specific implementation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Method",
      "text": "We train two encoder networks (depicted in Figure  3a ) to learn mixed input representations. Our training approach employs a contrastive loss function to optimise the similarity between matching pairs and minimise the similarity between dissimilar pairs. z a = g(f a (A))\n\nThe method employed is demonstrated through equation 2 and visualised in figure  3a . The Audio Encoder f a processes log mel spectrogram A to generate corresponding hidden representation z a ; our audio encoder utilises latent vectors, providing an efficient acoustic encoder with a fixed number of latent variables. This approach allows CLARA to process inputs like audio without linearly scaling the number of parameters with the input size. The Text Encoder f t produces the hidden representation z t by processing text samples T using a sinusoidal position embedding. Finally, both encoders are connected to a residual projection head g(.) that maps the encoder representation to the contrastive loss space.\n\nThe encoders incorporate a transformer architecture with a 1024 embedding. The text encoder is a transformer utilising Flash Attention  [20] , our audio encoder utilises the Perceiver head  [19]  for handling large input data points, and we modify the encoder by adding a convolution head and GELU activation to the key and values. The latent vector provides an efficient acoustic encoder with a fixed number of latent variables, allowing it to process inputs like audio without scaling the number of parameters linearly with the input size, while flash attention increases modelling capacity and training efficiency. Both encoders employ Cosine positional encoding to integrate positional information of tokens within the input sequence effectively.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Loss",
      "text": "We adopt a refined CLIP loss function inspired by  [4] ,  [7]  to map audio and text inputs towards a shared latent space. This alteration offers distinct advantages over the conventional contrastive loss framework, enabling the concurrent attainment of multi-modal alignments and the extraction of semantic nuances from both modalities. The evolved representation thus orchestrates the cohesive clustering of audio and text based on semantic similarity within the latent space, accounting for both magnitude and direction.\n\nIn this adapted context, the loss function is expressed as:\n\nThis equation introduces the learnable temperature parameters τ a and τ t for audio and text, respectively. The terms z k a and z k t denote the k-th pair of audio and text features, while N represents the minibatch size. Through this augmented loss formulation, we seek to enhance the discriminative potential of the shared latent space, leveraging individualised temperature controls for each modality to capture intricate relationships more effectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Experiments",
      "text": "We conducted extensive experiments to evaluate our proposed multilingual contrastive learning framework. Models were assessed on a diverse set of speech processing benchmarks under zero-shot and few-shot conditions.\n\nTo evaluate the multilingual performance, we use Keyword spotting (KWS). KWS demonstrates language-agnostic representation learning, capturing the underlying acoustic patterns and demonstrating the ability to adapt to low-resource languages. In addition, we perform zero-shot (ZS) classification and retrieval, which evaluates our model on tasks not directly trained on, such as emotion detection and sound event classification. Zero-shot allows us to gauge our model's ability to generalise its learned knowledge to tasks it hasn't been exposed to during training and its ability to transfer knowledge to downstream tasks. To further evaluate the representations learned by our model, we perform a Linear Probe (LP). Liner probe is the process of training a simple linear classifier on top of our pre-trained network, which allows us to evaluate the quality of the learned representations by assessing how well they can be used for a specific downstream task without any further finetuning or adaptation of our model. Finally, we look into information retrieval; retrieval demonstrates understanding and incorporation of contextual information from the input queries.\n\nWe juxtapose our model against state-of-the-art contrastive models documented in the existing literature, encompassing\n\nFig.  3 . The above figures present an overview of our proposed CLARA approach, highlighting its structural architecture and key role in enhancing audio-text feature learning and related tasks. 3a describes our contrastive framework designed for learning audio and text feature representations. This framework employs Speech-Aware segmentation combined with scene change detection, multimodal fusion, and text description refinement. 3b illustrates the pretrained encoders within the model that map both audio inputs and corresponding text data into a shared representation space. A Perceiver  [19]  head is used to handle a vast number of input data points, such as those from long-form audio. This unified space enables accurate classification and retrieval of class labels, transcriptions, or text descriptions corresponding to the input audio. 90.00  [26]  69.70  [27]  zero shot (ZS) and supervised/finetuned approaches (Best), to determine its performance under various scenarios. Furthermore, we compare with contemporary models trained for a single task (SOTA).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "A. Preprocessing",
      "text": "The audio data preprocessing pipeline converts raw audio waveforms to log Mel spectrogram representations. The spectrograms are computed using a sampling rate of 48 kHz, 80 Mel filterbanks spanning the frequency range 0-8 kHz, a hop length of 512 samples between successive frames, and a Hann window size of 1024 samples. Randomly selected audio clips are truncated or zero-padded to match the duration of the longest sample within each training batch. This standardised spectrogram representation allows the model to learn robust audio features for the downstream task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Multilingual Keyword Spotting",
      "text": "An analysis of the empirical results attained by our proposed CLARA model on the Multilingual Spoken Words Corpus (See figure  4 ), as detailed further in Appendix D, demonstrates that while sufficient amounts of labelled training data are indispensable for achieving superior keyword spotting performance in a given target language, synergistic transfer of linguistic knowledge from closely related or cognate languages with abundant annotated audio can also confer substantial accuracy benefits. This is evidenced by CLARA's ability to generalise well even when trained on reasonably small sample sizes for low-resource languages like Odia, Lithuanian, and Guarani, attaining 61.5%, 93.7%, and 88.8% accuracy, respectively, with just 1-6 minutes of audio data or between 100-300 samples. Our model capitalises on overlapping linguistic typologies and shared roots with other languages, transferring representations of phonology, morphology, and lexicon. Cross-lingual transferability appears less pronounced for isolated or linguistically distinct languages, underscoring the importance of language family relations and genealogical proximity for enabling efficient transfer learning in low-resource multilingual keyword spotting tasks. Nonetheless, these results highlight CLARA's promising capabilities for generalised multilingual keyword spotting across diverse languages.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Zero-Shot",
      "text": "Zero-Shot explores the generalisation of CLARA to unseen samples. By training a model using contrastive learning, we facilitate mapping these attribute representations into a shared semantic space, enabling the model to reason about unseen classes based on their relationships with seen classes. This experiment investigates the effectiveness of zero-shot learning in recognising and classifying previously unseen classes, showcasing the potential for knowledge transfer and generalisation. Our findings shed light on the capabilities and limitations of zero-shot learning in real-world scenarios, offering insights into its applicability and performance.\n\n1) Setup: We construct a natural language description from the given class label. Different templates were used for different tasks. For instance, we used 'The sound of {label}' for sound classification and 'A person talking in a {label} voice' for emotion classification.\n\n2) Results: In zero-shot classification tasks (Table  I ), our model achieves superior performance compared to existing contrastive models on emotion recognition benchmarks CREMA-D and RAVDESS, with improvements of up to 300% on CREMA-D. Specifically, our zero-shot accuracy of 54.87% on CREMA-D significantly exceeds the 17.84% achieved by CLAP, representing a 3x increase. Similarly, on RAVDESS, our 26.33% zero-shot accuracy exceeds CLAP's performance by a factor of 1.6x.\n\nAdditionally, our model establishes new state-of-the-art zero-shot results on the sound event classification datasets Audioset, US8K, and FSD50K. On Audioset, CLARA attains 13.49% zero-shot accuracy, surpassing CLAP (5.80%). On US8K, we achieves 87.84% accuracy, outperforming CLAP (73.24%), AudioCLIP (65.30%), and Wav2CLIP (40.44%). Finally, on FSD50K, CLARA's Mean Average Precision (mAP) of 32.90% exceeds CLAP (30.24%) and Wav2CLIP (3.02%), representing a 2.66% increase over previous methods.\n\n3) Zero-Shot Templates: Natural language templates are crucial for zero-shot learning, as their wording and framing fundamentally affect generalisation. More unambiguous templates improve accuracy on unseen classes, while conceptual prompts degrade performance. We use simple descriptive prompts for each target class. For instance, 'A person talking in a {label} voice' for emotion detection. Table  II  shows templates for different tasks. The choice of template impacts metrics like accuracy, as abstract prompts require more reasoning and cause declines as shown in table III on sound event classification. Careful design is needed to maximise inclusion and avoid biases. Template engineering is vital for zero-shot learning, balancing clarity, specificity and inclusivity.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Linear Probe (Lp)",
      "text": "Following established best practices, we conducted a linear probing analysis to evaluate the pre-trained model's ability to learn representations that transfer to novel downstream tasks. Specifically, we appended a randomly initialised linear evaluation head on top of our pre-trained CLARA model. This linear probe comprises three fully connected layers with ReLU activations and dropout regularisation. LP allows us to analyse the transferability of the base model's learned representations by training the linear probe model to minimise cross-entropy loss datasets for downstream tasks. 1) Setup: We froze the parameters of the pre-trained base model and attached a simple classifier to the tail of our model. The classification tail is optimised end-to-end using the AdamW optimiser with a small learning rate of 1e -4.\n\nThe training (Table  IV ) was limited to a maximum of 20 epochs except for Best to prevent overfitting, all models utilise validation loss for early stopping. This framework allows assessing the generalisable feature learning abilities of the base model without degrading performance on its original pretraining task.\n\n2) Results: Our linear probe (table  IV ) experiments demonstrate the efficacy of our model's learned representations for downstream emotion and gender classification tasks without explicit training on these targets. The linear probe (LP) consistently outperforms the base zero-shot (ZS) model on both EmoDB and CREMA-D benchmarks. For emotion classification, the LP achieves 90.93% top-1 recall on EmovDB versus just 42.13% for the ZS baseline and 68.46% versus 42.00% on CREMA-D. Despite our model not being specifically trained to classify the emotions, we observe significant gains in recall scores, demonstrating that our base model has acquired generalisable internal representations containing substantial information about emotional content. Furthermore, the LP reaches performance comparable to state-of-the-art models trained solely for emotion classification, substantiating the versatility of our general-purpose learned features. on RAVDESS we outperform VQ-MAE-S-12  [24]  by 2.8% absolute and achieve comparable results on CREMA-D to LeRaC  [28]  (70.95%). Our model exhibits an aptitude for intrinsic emotional modelling, as evidenced by the linear probe's strong exploitability of these internal representations for emotion and gender predictions without task-specific finetuning.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Retrieval",
      "text": "The retrieval process of both audio and text is accomplished through a series of steps involving the use of the corresponding audio encoder (f a (.)) and text encoder (f t (.)) for capturing and representing the audio and text samples, respectively. These encoders transform the audio and text data into high-dimensional feature vectors (embeddings) that encode the relevant information needed for effective retrieval. The embeddings are projected into a shared contrastive space via the projection function g(.), which enables the comparison of the audio and text embeddings. Cosine similarity is then calculated between the projected audio and text embeddings to determine the degree of semantic alignment. This pipeline allows retrieving the most relevant text passage for a query audio clip and vice versa, as illustrated in Figure  3a .\n\n1) Results: Table  V  shows that our proposed model achieves state-of-the-art performance on both audio-to-text and text-to-audio retrieval benchmarks. In audio-to-text retrieval on the AudioCaps dataset, our model outperforms prior work, increasing R@1 by 11.27% absolute over the current CLAP model and 2.68% over ML-ATCMR. More gains are observed at R@5 and R@10 ranks, with our model demonstrating 39.66% and 19.82% improvements over CLAP, and 9.57% and 9.10% over ML-ATCMR, respectively.\n\nFor text-to-audio retrieval, our model exhibits strong R@5 and R@10 results, surpassing existing models by 3.32% and 8.26% absolute. The considerable gains in lower-rank performance highlight our model's capabilities in recall-oriented retrieval, leveraging versatile relevance matching and broad conceptual knowledge. While precision at top-1 is lower, our model nonetheless produces nuanced and useful retrievals beyond the most obvious result. These comprehensive gains underscore the efficacy of our approach for both precisionand recall-oriented multi-modal retrieval.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "This paper introduced CLARA, a novel framework for multilingual learning of speech and emotion representations. CLARA adeptly assimilates shared representations harmonised across diverse linguistic landscapes, fostering a transference of learned attributes to novel languages and tasks.\n\nIn our exploration of data augmentation, we capitalised on visual cues derived from video content to glean emotional and contextual insights. These insights were transformed into text format and injected into the natural language descriptors affiliated with audio data. This approach eliminates the need for creating new datasets where both audio and video must be present, allowing us also to use existing audio datasets where video data is unavailable. This further ensures flexibility and enables this work to expand into areas where video data cannot be captured. Utilising advanced neural methodologies, we amplified textual meta-information and cues, aiming to enrich the training sample spectrum across a plethora of languages.  This endeavour facilitated the generation of coherent natural language descriptions that encapsulate the contextual essence of the audio content. Our carefully designed experimental framework bore testimony to the robust performance of the model. This achievement indicates a significant stride towards enhancing state-of-the-art benchmarks in the domains of emotion recognition, audio classification, and retrieval tasks. The model showcased efficacy under zero-shot and few-shot scenarios, thereby underscoring its potential in navigating the complexities inherent in multilingual speech processing tasks.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Table V Performance Of Audio-To-Text And Text-To-Audio Retrieval On The Audiocaps Dataset. Our Proposed Model Achieves State-Of-The-Art Results, Outperforming Prior Models Clap And Ml-Atcmr On Metrics",
      "text": "The empirical outcomes accentuate the merits of our framework in procuring universally generalisable speech representations that thrive even amidst the scarcity of target language data. The versatility and adaptability of the model were further corroborated by the competitive performance of simple linear classifiers, which, when employed on frozen base features, required no task-specific fine-tuning. Some limitations were observed in extending representations to isolated languages. Future work should explore approaches tailored to the specific characteristics of individual languages, especially those that are isolated and consult with linguistic experts to better understand the nuances and intricacies of isolated languages to address these weaknesses. Nonetheless, our results highlight CLARA's promising capabilities for generalised learning of emotion and speech across diverse languages.\n\nThis study marks notable milestones, unveiling a model for speech and emotion understanding, data augmentation techniques incorporating visual, meta and audio data, and robust empirical validation of superior performance across various speech-processing tasks. It underscores the potential of this methodology in harnessing universal speech representations that transcend linguistic, accentual, and acoustic diversities, thus paving the way for groundbreaking advancements in the domain of multilingual speech and emotion processing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vii. Acknowledgements",
      "text": "This research was supported by The Engineering and Physical Sciences Research Council (EPSRC) by means of a grant awarded to Kari A Noriy. CDE2: EP/L016540/1 (for 2014/15 cohort and subsequent intakes), and we would like to thank the support of Stability AI for providing compute resources that were instrumental in conducting the research presented in this publication and the Laion community member who has helped collect and process the dataset used in training this model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Appendix",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Training Dataset",
      "text": "The training dataset consists of diverse publicly available audio-text pairs and sound event datasets from various sources. In total, the ensemble dataset consists of over 16,000 hours of audio matched with textural description, equating to approximately 21 Million audio text pairs. An essential data constituent comprises naturally occurring speech recordings extracted from datasets including Common Voice, Multilingual Spoken Words (MSWC) and LJSpeech. These datasets collectively provide thousands of hours of read speech originating from audiobook narrations, public addresses, and crowdsourced contributions in a multitude of languages. The inherent diversity in speakers, accents, ambient noise conditions, and discourse topics induces a rich representation of real-world speech.\n\nAdditionally, synthetic descriptions are obtained from audio captioning datasets such as Clotho, AudioCaps, and Audiocaps, comprising human-authored captions summarising the essence of brief audio snippets. These descriptive sentences provide semantic textual portrayals of sounds and acoustic events.\n\nTo further augment the training data, unlabeled audio segments are paired with generated captions and metadata by models such as CoCa and Whisper. CoCa is an image-text foundation model that produces natural language descriptions for visual scenes. Whisper is an automatic speech recognition system capable of transcribing speech. These models facilitate the synthesis of captions that expand the diversity of linguistic styles and speech contexts displayed in the data.\n\nThe extensive and multifaceted dataset is indispensable for training an efficacious contrastive speech representation model per our proposed approach. The corpus construction methodology aims to engender expansive coverage of linguistic phenomena through diverse speech recordings paired with descriptive captions.\n\n1) Demographic: Table  VII  provides an overview of the key details for each dataset utilised for training, including the type of audio content (such as speech, emotion, age, gender), total hours of audio, number of languages represented, and speaker gender metadata. The composite multilingual dataset enables robust representation learning by the model across 181 languages, various accents, age groups, genders, and acoustic environments. The diversity of speech, spoken words, emotions, ages, genders, and environmental sounds enhances the model's semantic parsing capabilities for both human vocals and ambient sounds.\n\nThe top 5 languages in terms of audio sample size including train, validation and test splits are English (4,240 hours), Catalan (2,038 hours), Kinyarwanda (1,864 hours), German (1,835 hours) and French (1,434 hours). For English, the gender distribution is imbalanced, with 18,925 male and 27,828 female samples, along with 75,045 samples with unknown gender. Understanding such distributions across languages and demographics will inform active learning to improve model generalisation. Overall, the tabular overviews offer essential insights into sample diversity, gaps, and opportunities from a multilingual and gender representation perspective to guide the ongoing enhancement of the composite dataset.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "B. Visual Cues Fusion",
      "text": "In addition to the data augmentation strategy discussed in Section III-A2, we employ an additional technique aimed at improving the generation of captions for samples that possess both audio and visual representations. This supplementary approach involves utilising visual cues extracted from static images to assist in caption generation. To accomplish this, we utilise the CoCa model  [2] , which is an image-to-text foundation model that has demonstrated state-of-the-art performance on vision and vision-language tasks. Designed explicitly for generating captions for images, CoCa leverages both visual and textual information to produce accurate and descriptive captions.\n\nLet V F represent a set of frames extracted from a given video. We apply the image-to-caption model, denoted as I2T, to each frame in order to generate visual captions. The process is guided by the Structural Similarity Index Measure (SSIM) between consecutive frames, denoted as V F i and V F i+1 , respectively. Visual captions C v are obtained for frames that exhibit a significant change in scene content, determined by comparing the SSIM value to a threshold θ. Mathematically, the set of visual captions C v can be expressed as:\n\nWhere I2T represents the image-to-caption model, which is a specialised model trained on a dataset comprising imagetext pairs. Its primary objective is to generate captions that accurately describe the content of images. The SSIM metric serves as a measure of structural similarity, allowing the identification of changes in scene content. The threshold θ determines the level of change required for a frame to be considered distinct.\n\nTo integrate the audio descriptions C t with the visual captions C v , we employ a language model (LLM), as described in Equation 1 and illustrated in Figure  2 . The LLM converts the raw captions into meaningful text descriptions, unifying the audio and visual modalities. This step ensures a cohesive and comprehensive representation of the combined audio-visual content.\n\nIn summary, by incorporating visual cues from static images using the CoCa model and employing an LLM to unify audio and visual captions, our approach enhances the generation of captions for samples with audio and visual representations. This methodology promotes a more comprehensive and cohesive understanding of multimedia content.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "C. Multilingual Augmentation",
      "text": "To expand the diversity of languages represented in the training data, we employ neural augmentation techniques leveraging pre-trained multilingual models. The process involves first translating the class labels or captions in the original mono-lingual dataset into target languages using a language model (LM). For instance, an English label like \"dog barking\" can be translated into French, Hindi, Mandarin etc. We then overlay these translated labels on the original audio samples to create synthetic multilingual samples. The same audio recording is duplicated with different language overlays. This methodology scales efficiently by relying on LMs' pre-trained multilingual capabilities. It avoids the need for manual annotation or collection of speech data in multiple languages. We apply this technique to augment several monolingual datasets, synthesising parallel corpora covering the 181 languages.\n\nThis multilingual data augmentation enriches the variability of languages and accents within our training set. The model is exposed to far greater linguistic diversity, which enhances its generalisation capabilities. We efficiently compensate for the lack of real multilingual speech/sound-event data.\n\nIn summary, neural multilingual augmentation proves highly effective in expanding the corpus to encompass a wider range of languages and linguistic phenomena. This promotes more universal speech representation learning and improves zeroshot transferability.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Keyword Spotting",
      "text": "The Multilingual Spoken Words Corpus used for evaluation consists of short spoken word utterances covering 50 languages. For each language, there are thousands of speakers in various accents and conditions. This allows benchmarking keyword spotting across a variety of languages and speech variations. To evaluate performance, we calculate accuracy on a per-word basis -the percentage of test word utterances correctly recognised by the model for each language. The accuracy results showcase CLARA's ability to learn effective representations for high-resource languages like English, French, and German with abundant training examples. For instance, English achieves 97% accuracy with over 1,400 hours of training data.\n\nMore interestingly, CLARA demonstrates an aptitude for cross-lingual transfer and generalisation, even with minimal data for low-resource languages. For example, Lithuanian attains 93.7% accuracy with just 0.03 hours of audio (100 samples). Similarly, Guarani reaches 88.8% accuracy with only 0.1 hours (285 samples). The model appears to leverage similarities with other trained languages.\n\nIn contrast, isolated languages like Georgian achieve lower accuracy of 86.3% despite more training data of 0.9 hours. This indicates language proximity is critical for effective transfer. Georgians experiences more errors due to its complex consonant clusters. Integrating phonetic and linguistic knowledge could help address such issues. However, there remain challenges in improving robustness in isolated and complex languages.\n\nOverall, these results demonstrate CLARA's promising capability for generalised multilingual keyword spotting. The model effectively transfers learned representations between related languages, even with minimal target language data. Our augmentation strategies also help improve learning.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The above outlines the two frameworks for generating and augmenting existing data for speech. 1a outlines Speech-Aware Segmentation: Let MFA be",
      "page": 3
    },
    {
      "caption": "Figure 2: The proposed framework provides a straightforward approach for",
      "page": 3
    },
    {
      "caption": "Figure 2: To overcome the lack of large multilingual datasets for",
      "page": 3
    },
    {
      "caption": "Figure 3: a. The Audio Encoder fa processes",
      "page": 4
    },
    {
      "caption": "Figure 3: The above figures present an overview of our proposed CLARA approach, highlighting its structural architecture and key role in enhancing audio-text",
      "page": 5
    },
    {
      "caption": "Figure 4: ), as detailed further in Appendix D, demonstrates that",
      "page": 5
    },
    {
      "caption": "Figure 4: The relationship between the quantity of training data and keyword",
      "page": 6
    },
    {
      "caption": "Figure 2: The LLM converts the",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Email: {knoriy, xyang, mbudka,": "Abstract—In multilingual speech processing, accurately under-",
          "jzhang}@bournemouth.ac.uk": "tions are scarce. This data dependence limits the development"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "standing\nand\ninterpreting\nemotions\nis\npivotal\nyet\nchallenging",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "of speech technologies for global users."
        },
        {
          "Email: {knoriy, xyang, mbudka,": "due\nto\nthe\nscarcity\nof\nlabelled datasets. The\nrecent\nstrides\nin",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "Recent\nresearch\nhas\nexplored\nsemi-supervised\nand\nself-"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "contrastive learning have catalysed self-supervised methodologies,",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "supervised\napproaches\nto\nlearning\nfrom unlabelled\ndata\nby"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "enabling\nthe\nharnessing\nof\nunlabelled\ndata.\nIn\nlight\nof\nthese",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "advancements, we\nintroduce CLARA,\na\ngroundbreaking mul-",
          "jzhang}@bournemouth.ac.uk": "leveraging\ndata\nabundance\nto\ncompensate\nfor\nthe\nlack\nof"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "tilingual\nframework, as our\nsolution to diminish labelled data",
          "jzhang}@bournemouth.ac.uk": "annotations. Contrastive\nlearning has\nemerged as\na promis-"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "dependence and escalate generalisation across a diverse array",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "ing\ntechnique\nfor\nself-supervised\nrepresentation\nlearning\nin"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "of\nlanguages and conditions. CLARA excels\nin guiding models",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "domains like computer vision and natural language processing."
        },
        {
          "Email: {knoriy, xyang, mbudka,": "to embody shared representations across\nlanguages,\nseamlessly",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "By training models to distinguish between similar and dissim-"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "facilitating the cross-lingual transfer of speech and emotional un-",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "derstanding, even amidst scenarios with limited target\nlanguage",
          "jzhang}@bournemouth.ac.uk": "ilar sample pairs. The contrastive objective enables models to"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "data. A cornerstone\nof\nour\napproach is proficiently\ncapturing",
          "jzhang}@bournemouth.ac.uk": "extract meaningful\nrepresentations from unlabelled data."
        },
        {
          "Email: {knoriy, xyang, mbudka,": "emotional\nsubtleties within speech,\ntranscending the\nchallenges",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "Several studies have applied speech representations learning"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "posed by the\nsubjective nature of perceptual assessments. Em-",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "from audio. For\ninstance, AudioCLIP [1] demonstrates\nthat"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "barking into self-supervised learning with a rich multilingual data",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "contrastive losses can align environmental\nsounds and visual"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "corpus, we aim to delineate speech representations imbued with",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "emotive dimensions, unlocking new potentials\nin emotion-aware",
          "jzhang}@bournemouth.ac.uk": "representations in a shared space. Still,\nthis approach learns a"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "multilingual speech processing.",
          "jzhang}@bournemouth.ac.uk": "shared representation of\nlabelled data. COLA [2]\nintroduces"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "Employing a substantial corpus of multilingual audio data, our",
          "jzhang}@bournemouth.ac.uk": "a framework tailored for self-supervised speech representation"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "methodology leverages data augmentation techniques to broaden",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "learning through sample pairs drawn from diverse unlabelled"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "the\ndataset\nspectrum,\nincorporating\nvisual\nunderstanding\nvia",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "audio, but\nthis approach does not consider\nthe lexical content"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "textual\nembedding, augmentation of\nlanguage data from high-",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "and solely focuses on learning representations\nfrom a single"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "resource\ndata\nsources\nto\nlow-resource\nlanguages\nand model",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "CLARA to learn these representations across domains.",
          "jzhang}@bournemouth.ac.uk": "modality (waveform). Furthermore, existing contrastive speech"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "Rigorous experimentation demonstrates our model’s superior",
          "jzhang}@bournemouth.ac.uk": "models\nare\ntrained\non\nsingle\nlanguages\nand\nneed\nto\nfully"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "performance across various\ntasks,\nsuch as emotion recognition,",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "leverage multilingual data."
        },
        {
          "Email: {knoriy, xyang, mbudka,": "multilingual\nlanguage\ncomprehension,\naudio\nclassification,\nand",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "Learning generalised multilingual natural\nlanguage\nrepre-"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "retrieval benchmarks,\nespecially in zero-shot and few-shot\nsce-",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "sentations\nis crucial\nfor advancing speech processing across"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "narios. Our model presents a compelling approach to obtaining",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "shared and adaptable\nspeech representations\nacross\nlanguages",
          "jzhang}@bournemouth.ac.uk": "low-resource\nlanguages. Hence, we\nadvocate\na multifaceted"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "and acoustic conditions while encoding latent emotional aspects.",
          "jzhang}@bournemouth.ac.uk": "paradigm that encapsulates natural\nlanguage text descriptors"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "Additionally, we showcase the model’s capability to adapt to low-",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "and the emotive states of\nspeakers, aspiring to delve beyond"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "resource languages, marking a significant\nstride in multilingual",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "mere lexical\ninterpretation to a comprehensive understanding"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "speech representation learning.",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "of\nthe speakers. We incorporate image-based embedding into"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "Index Terms: Contrastive Learning, multilingual speech, com-",
          "jzhang}@bournemouth.ac.uk": "our natural\nlanguage text descriptions to further enhance this"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "putational para-linguistics, text-to-audio retrieval, audio-to-text",
          "jzhang}@bournemouth.ac.uk": "aspect\nof\nour work. This\napproach\nallows\nus\nto\ncontinue"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "retrieval, audio classification.",
          "jzhang}@bournemouth.ac.uk": "building on high-quality data\nsuch as EmoV-DB [3], where"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "a video recording of\nthe\nspeaker\nis not\navailable,\nreducing"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "model complexity and aiding in faster\nlearning."
        },
        {
          "Email: {knoriy, xyang, mbudka,": "I.\nINTRODUCTION",
          "jzhang}@bournemouth.ac.uk": ""
        },
        {
          "Email: {knoriy, xyang, mbudka,": "",
          "jzhang}@bournemouth.ac.uk": "Multilingual\ntraining exposes models to greater diversity in"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "The domain of multilingual speech processing, particularly",
          "jzhang}@bournemouth.ac.uk": "speakers,\naccents,\nacoustics, phonetics,\nand languages. This"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "in natural and emotive speech representations, grapples with",
          "jzhang}@bournemouth.ac.uk": "facilitates learning robust and transferable representations that"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "substantial challenges stemming from the requirement of ex-",
          "jzhang}@bournemouth.ac.uk": "perform well\neven\nfor\nlimited\ntarget\nlanguage\ndata. While"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "tensively annotated datasets across a vast\nlinguistic spectrum.",
          "jzhang}@bournemouth.ac.uk": "supervised multilingual models require abundant labelled data,"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "Supervised learning methods\nthat\nrely on large labelled cor-",
          "jzhang}@bournemouth.ac.uk": "our\nsemi-supervised approach provides an avenue to exploit"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "pora have\nachieved high performance\nin high-resource\nlan-",
          "jzhang}@bournemouth.ac.uk": "the abundance of unlabelled multilingual speech."
        },
        {
          "Email: {knoriy, xyang, mbudka,": "guages like English. Still,\ntheir effectiveness does not\ntransfer",
          "jzhang}@bournemouth.ac.uk": "Motivated by these challenges, we introduce an innovative"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "well\nto low-resource languages where transcribed speech and",
          "jzhang}@bournemouth.ac.uk": "multilingual\nframework\ngeared\ntowards\nassimilating\nuniver-"
        },
        {
          "Email: {knoriy, xyang, mbudka,": "emotionally labelled data in a myriad of environmental condi-",
          "jzhang}@bournemouth.ac.uk": "sal\nspeech and emotional\nrepresentations,\nemploying a\nrich"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "Our work\nbuilds\non\nthese\nprior works\nby\nproposing\na"
        },
        {
          "2": "novel multilingual\ncontrastive\nlearning\napproach\nto\nacquire"
        },
        {
          "2": "shared speech representations across\nlanguages. We leverage"
        },
        {
          "2": "the abundance of diverse multilingual\nspeech and automatic"
        },
        {
          "2": "transcriptions,\nin\ncontrast\nto\nsupervised methods\nreliant\non"
        },
        {
          "2": "scarce\nlabelled\ncorpora. Our\ndata\naugmentation\ntechniques"
        },
        {
          "2": "incorporating visual\nand textual\ninformation further\nexpand"
        },
        {
          "2": "training samples for\nrobust\nrepresentation learning."
        },
        {
          "2": "In\nsummary,\nour\nframework\naims\nto\ncapitalise\non\nthe"
        },
        {
          "2": "successes of contrastive learning in other domains by tailoring"
        },
        {
          "2": "it\nto the multilingual\nspeech setting through innovations\nlike"
        },
        {
          "2": "cross-lingual pretraining strategies and multimodal data aug-"
        },
        {
          "2": "mentations. This helps overcome the limitations of supervised-"
        },
        {
          "2": "only multilingual models and single-language contrastive ap-"
        },
        {
          "2": "proaches\nfor\nspeech and enables\nseveral downstream tasks,"
        },
        {
          "2": "such\nas\nemotion\ndetection,\nspeech-to-text,\nand\nexpressive"
        },
        {
          "2": "speech synthesis and allows users of such model\nto guide the"
        },
        {
          "2": "model via natural\nlanguage text prompts\nsuch as ”A woman"
        },
        {
          "2": "speaking is a joyful mood”."
        },
        {
          "2": ""
        },
        {
          "2": "III. CORPUS"
        },
        {
          "2": ""
        },
        {
          "2": "Contrastive models benefit\nfrom large datasets. Due to the"
        },
        {
          "2": ""
        },
        {
          "2": "semi-supervised nature of these models, a considerable amount"
        },
        {
          "2": ""
        },
        {
          "2": "of analogous and heterogeneous\nsample pairs\nis essential\nto"
        },
        {
          "2": ""
        },
        {
          "2": "achieve meaningful\nrepresentation.\nIn contrast, when data is"
        },
        {
          "2": ""
        },
        {
          "2": "limited, the model may require a more in-depth comprehension"
        },
        {
          "2": ""
        },
        {
          "2": "of the underlying data structure to ensure accurate predictions."
        },
        {
          "2": "Furthermore, contrastive learning often involves\ntraining the"
        },
        {
          "2": ""
        },
        {
          "2": "model with\na\nvast\nnumber\nof\nnegative\nsamples\n(i.e.,\npairs"
        },
        {
          "2": ""
        },
        {
          "2": "of dissimilar data points), which can be a resource-intensive"
        },
        {
          "2": ""
        },
        {
          "2": "process. Therefore, a larger dataset can expedite the training"
        },
        {
          "2": ""
        },
        {
          "2": "process by providing additional negative samples for the model"
        },
        {
          "2": ""
        },
        {
          "2": "to learn from."
        },
        {
          "2": ""
        },
        {
          "2": "With this objective in mind, we compiled a collection of"
        },
        {
          "2": ""
        },
        {
          "2": "openly available speech, music, and sound effects datasets to"
        },
        {
          "2": ""
        },
        {
          "2": "establish an ensemble dataset. Furthermore, we have employed"
        },
        {
          "2": ""
        },
        {
          "2": "various augmentation techniques to expand the dataset’s mag-"
        },
        {
          "2": ""
        },
        {
          "2": "nitude and incorporate speech metadata in a natural manner,"
        },
        {
          "2": ""
        },
        {
          "2": "including emotional attributes, which we explore in detail\nin"
        },
        {
          "2": ""
        },
        {
          "2": "this section."
        },
        {
          "2": ""
        },
        {
          "2": ""
        },
        {
          "2": "A. Datasets"
        },
        {
          "2": ""
        },
        {
          "2": "In collaboration with Laion, we compile publicly accessible"
        },
        {
          "2": "data sets, incorporating a fusion of meticulously gathered web-"
        },
        {
          "2": "scraped data and openly available resources. This amalgamated"
        },
        {
          "2": "data reservoir consists of pairs of audio and text, encompassing"
        },
        {
          "2": "both naturally occurring linguistic interactions and artificially"
        },
        {
          "2": "generated\nlanguage\nconstructs. The\nlatter\ncategory\nencom-"
        },
        {
          "2": "passes supplementary metadata attributes,\nincluding linguistic"
        },
        {
          "2": "origin,\ntitular\ndesignations,\ntranscriptions,\nand\naudio-visual"
        },
        {
          "2": "content manifestations\nsuch as animal vocalisations or news"
        },
        {
          "2": "presenter narratives."
        },
        {
          "2": "The collection surpasses 16,000 hours, with approximately"
        },
        {
          "2": "12 million samples attributed to male speakers. In comparison,"
        },
        {
          "2": "approximately\n3 million\nsamples\npertain\nto\nfemale\nspeak-"
        },
        {
          "2": "ers,\nand\napproximately\n5 million\nsamples\nremain\nunclassi-"
        },
        {
          "2": "fied regarding gender categorisation. Moreover, around 6,000"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)": "Fig. 1. The above outlines the two frameworks for generating and augmenting existing data for speech. 1a outlines Speech-Aware Segmentation: Let M F A be",
          "(b)": ""
        },
        {
          "(a)": "a forced aligner generating a time-aligned transcription and ASR be an automatic speech recognition model. Let an, tn be a meaningful",
          "(b)": "truncated waveform"
        },
        {
          "(a)": "and transcription.\nIf no ground truth transcript",
          "(b)": "is provided, ASR is used. 1b outlines a string tokeniser and a Text-To-Speech model\nfor generating speech,"
        },
        {
          "(a)": "Let segmentation be a rule-based string tokeniser and T T S(.) be a neural speech synthesiser generating an audio and it corresponding transcript tn.",
          "(b)": ""
        },
        {
          "(a)": "hours within this compilation are comprised of environmental",
          "(b)": ""
        },
        {
          "(a)": "sounds. Refer to Appendix A for a comprehensive elaboration",
          "(b)": ""
        },
        {
          "(a)": "on these constituent datasets.",
          "(b)": ""
        },
        {
          "(a)": "Recent models such as CLIP [7] and SimCLR v1/v2 [4], [5]",
          "(b)": ""
        },
        {
          "(a)": "have demonstrated the potential of large-scale models to com-",
          "(b)": ""
        },
        {
          "(a)": "prehend contextual\nrepresentation. Learning from a massive",
          "(b)": ""
        },
        {
          "(a)": "corpus offers\nseveral advantages over alternative techniques.",
          "(b)": ""
        },
        {
          "(a)": "For instance, loosely supervised natural language data obtained",
          "(b)": ""
        },
        {
          "(a)": "from the web is more accessible to scale than conventionally",
          "(b)": ""
        },
        {
          "(a)": "labelled data. Models can passively acquire knowledge from",
          "(b)": ""
        },
        {
          "(a)": "large\namounts\nof\nloosely\nlabelled\ndata\nfrom the web. As",
          "(b)": "Fig. 2.\nThe proposed framework provides\na\nstraightforward approach for"
        },
        {
          "(a)": "",
          "(b)": "augmenting natural\nlanguage descriptions\nand incorporating environmental"
        },
        {
          "(a)": "demonstrated by CLIP [7],\nthese models do not\nsolely learn",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "let pt be a prefix\nnoise layering. Let an, tn be given audio and text pairs,"
        },
        {
          "(a)": "a\nrepresentation but\nalso establish connections between the",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "string to prepend, and mn and pnn are additional meta information, such as"
        },
        {
          "(a)": "representation and language, permitting the flexible zero-shot",
          "(b)": "audio quality, gender and emotion. Let f (.) be a schema combining labels to"
        },
        {
          "(a)": "",
          "(b)": "natural language description. Let pna (pub noise) be a series of environmental"
        },
        {
          "(a)": "transfer.",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "sounds and pnt\nlanguage\nlabels. ˆan, ˆtn be the augmented audio and natural"
        },
        {
          "(a)": "1) Data Preparation: A standardised format\nis employed",
          "(b)": "description."
        },
        {
          "(a)": "to\nsystematically\norganize\naudio-text\npairs. Audio\nsamples",
          "(b)": ""
        },
        {
          "(a)": "undergo\nFLAC conversion\nand\nresampling\nto\n48kHz\nfor",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "audio\naugmentation methods\nand\nneural\napproaches\n(Fig-"
        },
        {
          "(a)": "lossless\ncompression\nand\nhigh-quality\ndistribution,\naligning",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "ure 2).\nIn traditional\naudio augmentation, we\napply various"
        },
        {
          "(a)": "with the EMNS dataset guidelines\n[13]. Metadata,\ncontain-",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "transformations\nsuch\nas\nadding\nreverb,\nclipping, masking,"
        },
        {
          "(a)": "ing descriptions\nlike\n”A gender\nsaying transcription in a",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "pitch modulation,\nand\nintroducing\nenvironmental\nsounds\nto"
        },
        {
          "(a)": "emotion voice,”\nis\nstored in JSON format, where\ngender,",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "simulate different\nacoustic\nenvironments or distorted audio."
        },
        {
          "(a)": "transcription and emotion are datapoint from a given dataset",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "These transformations are layered on top of the original audio"
        },
        {
          "(a)": "used\nto\nform a\nnatural\nlanguage\ndescription.\n.tar files\nare",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "signals to create augmented versions."
        },
        {
          "(a)": "used\nfor\nscalable\nstreaming. For\nextended\naudio\nexceeding",
          "(b)": ""
        },
        {
          "(a)": "10 seconds,\ntwo truncation strategies\nare\nimplemented. The",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "a = λai + (1 − λ)aj"
        },
        {
          "(a)": "first employs Montreal Forces Aligner (MFA) [14] for precise",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "(1)"
        },
        {
          "(a)": "alignment\nof\nspeech\nrecordings\nand\ntranscripts, while\nthe",
          "(b)": "t = f (pt + ti + tj)"
        },
        {
          "(a)": "second uses Whisper\n[15], a transformer-based ASR system,",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "For\nexample, we\ngenerate\naugmented\naudio\nsamples\nby"
        },
        {
          "(a)": "to transcribe and truncate lengthy samples. The data pipeline",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "scale\ncombining two raw audio signals, ai\nand aj, using a"
        },
        {
          "(a)": "segmentation is visually depicted in Figure 1a.",
          "(b)": ""
        },
        {
          "(a)": "",
          "(b)": "factor λ that\nranges between 0.01 and 0.8 used to control\nthe"
        },
        {
          "(a)": "2) Multimodal Fusion and Text Generation: Chen et\nal.",
          "(b)": "is\ncontribution of environmental sounds (pub noise), where ai"
        },
        {
          "(a)": "[4] demonstrated the significant\nimpact of data augmentation",
          "(b)": "the background sounds to be added.\na speech segment and aj"
        },
        {
          "(a)": "on improving the performance of contrastive learning within",
          "(b)": "a\nThe\nresulting\naugmented\naudio\nis\na\nlinear\ncombination"
        },
        {
          "(a)": "the computer vision field. Other\nstudies have also indicated",
          "(b)": "of\nthe\ntwo signals. Similarly,\nthe\ncorresponding text\nlabels"
        },
        {
          "(a)": "that\ndata\naugmentation\ncan\nenhance\nthe\ngeneralisation\nof",
          "(b)": "are\ncombined using a\nfunction f (.)\nthat\nemploys\nti\nand tj"
        },
        {
          "(a)": "supervised neural networks and combat overfitting [16],\n[17].",
          "(b)": "concatenative and neural-based approaches to generate natural"
        },
        {
          "(a)": "Augmentation\ntechniques\nincrease\nthe\ndiversity\nof\nacoustic",
          "(b)": "language descriptions. This augmentation pipeline is visually"
        },
        {
          "(a)": "environments and recording conditions, acquiring robust and",
          "(b)": "depicted in Figure 2."
        },
        {
          "(a)": "invariant\nfeatures and ultimately regularising the model. Ac-",
          "(b)": "To\novercome\nthe\nlack\nof\nlarge multilingual\ndatasets\nfor"
        },
        {
          "(a)": "cordingly, we\nemploy\nvarious\naugmentation\ntechniques\nto",
          "(b)": "sound event classification. We made use of pre-trained mul-"
        },
        {
          "(a)": "diversify speech data for our model.",
          "(b)": "tilingual\nlanguage models\nto\ntranslate\nlabels\ninto\ndifferent"
        },
        {
          "(a)": "Unlike\nprevious methods where\nclass\nlabels\nor\nsimple",
          "(b)": "languages\nand\noverlay\nthem on mono-lingual\ndata,\nsynthe-"
        },
        {
          "(a)": "templates\nare used. Our\naugmentation strategy encompasses",
          "(b)": "sising multilingual datasets. This efficient data augmentation"
        },
        {
          "(a)": "a\ncomprehensive\nset of\ntechniques\nto broaden the diversity",
          "(b)": "approach leverages\nlanguage models\nto avoid costly manual"
        },
        {
          "(a)": "of our audio data. These techniques\ninclude both traditional",
          "(b)": "collection,\nrapidly creating diverse training corpora covering"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4": "to integrate positional\ninformation of\ntokens within the input"
        },
        {
          "4": "sequence effectively."
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": "A. Loss"
        },
        {
          "4": ""
        },
        {
          "4": "We adopt a refined CLIP loss function inspired by [4],\n[7]"
        },
        {
          "4": ""
        },
        {
          "4": "to map audio and text\ninputs\ntowards a shared latent\nspace."
        },
        {
          "4": ""
        },
        {
          "4": "This alteration offers distinct advantages over the conventional"
        },
        {
          "4": ""
        },
        {
          "4": "contrastive loss framework, enabling the concurrent attainment"
        },
        {
          "4": ""
        },
        {
          "4": "of multi-modal\nalignments\nand\nthe\nextraction\nof\nsemantic"
        },
        {
          "4": ""
        },
        {
          "4": "nuances from both modalities. The evolved representation thus"
        },
        {
          "4": ""
        },
        {
          "4": "orchestrates the cohesive clustering of audio and text based on"
        },
        {
          "4": ""
        },
        {
          "4": "semantic similarity within the latent space, accounting for both"
        },
        {
          "4": ""
        },
        {
          "4": "magnitude and direction."
        },
        {
          "4": ""
        },
        {
          "4": "In this adapted context,\nthe loss function is expressed as:"
        },
        {
          "4": ""
        },
        {
          "4": ""
        },
        {
          "4": "(cid:34)"
        },
        {
          "4": ""
        },
        {
          "4": "exp((zk\na · zk\nt )τa)"
        },
        {
          "4": "1 N\nN(cid:88) k\nlog\n+\nL = −"
        },
        {
          "4": ""
        },
        {
          "4": "(cid:80)N"
        },
        {
          "4": "t )τa)\na · zn\nn=1 exp((zk\n=1"
        },
        {
          "4": ""
        },
        {
          "4": "(cid:35)"
        },
        {
          "4": "exp((zk\n· zk\nt\na )τt)"
        },
        {
          "4": "log\n(3)"
        },
        {
          "4": "(cid:80)N"
        },
        {
          "4": "· zn"
        },
        {
          "4": "a )τt)\nn=1 exp((zk"
        },
        {
          "4": ""
        },
        {
          "4": "This equation introduces the learnable temperature param-"
        },
        {
          "4": ""
        },
        {
          "4": "for audio and text,\nrespectively. The terms zk\neters τa and τt"
        },
        {
          "4": "a"
        },
        {
          "4": "denote the k-th pair of audio and text\nfeatures, while\nand zk\nt"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a)\n(b)": "Fig. 3.\nThe above figures present an overview of our proposed CLARA approach, highlighting its structural architecture and key role in enhancing audio-text"
        },
        {
          "(a)\n(b)": "feature learning and related tasks. 3a describes our contrastive framework designed for learning audio and text feature representations. This framework employs"
        },
        {
          "(a)\n(b)": "Speech-Aware segmentation combined with scene change detection, multimodal\nfusion, and text description refinement. 3b illustrates the pretrained encoders"
        },
        {
          "(a)\n(b)": "within the model\nthat map both audio inputs and corresponding text data into a shared representation space. A Perceiver\n[19] head is used to handle a vast"
        },
        {
          "(a)\n(b)": "number of input data points, such as those from long-form audio. This unified space enables accurate classification and retrieval of class labels,\ntranscriptions,"
        },
        {
          "(a)\n(b)": "or\ntext descriptions corresponding to the input audio."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": ""
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": ""
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "Model"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "CLAP (ZS)\n[21]"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "CLAP (Best)"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "AudioCLIP (ZS)"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "AudioCLIP [1]"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "Wav2CLIP (ZS)"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "Wav2CLIP [22]"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "ours (ZS)"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "ours (Best)"
        },
        {
          "FINE-TUNED EVALUATIONS. SOTA ARE MODEL TRAINED SOLELY FOR A SINGLE TASK.": "SOTA"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "zero shot (ZS) and supervised/finetuned approaches (Best),\nto",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "B. Multilingual Keyword Spotting"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "determine\nits performance under various\nscenarios. Further-",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": ""
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "more, we\ncompare with contemporary models\ntrained for\na",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "An analysis of the empirical results attained by our proposed"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "single task (SOTA).",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "CLARA model on the Multilingual Spoken Words Corpus (See"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "figure 4), as detailed further in Appendix D, demonstrates that"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "A. PreProcessing",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": ""
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "while\nsufficient\namounts of\nlabelled training data\nare\nindis-"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "The audio data preprocessing pipeline converts\nraw audio",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "pensable for achieving superior keyword spotting performance"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "waveforms to log Mel spectrogram representations. The spec-",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "in a given target\nlanguage,\nsynergistic\ntransfer of\nlinguistic"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "trograms are computed using a sampling rate of 48 kHz, 80",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "knowledge\nfrom closely\nrelated\nor\ncognate\nlanguages with"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "Mel filterbanks spanning the frequency range 0-8 kHz, a hop",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "abundant annotated audio can also confer substantial accuracy"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "length of 512 samples between successive frames, and a Hann",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "benefits. This is evidenced by CLARA’s ability to generalise"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "window size of 1024 samples. Randomly selected audio clips",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "well even when trained on reasonably small sample sizes for"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "are\ntruncated\nor\nzero-padded\nto match\nthe\nduration\nof\nthe",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "low-resource languages like Odia, Lithuanian, and Guarani, at-"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "longest\nsample within each training batch. This\nstandardised",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "taining 61.5%, 93.7%, and 88.8% accuracy, respectively, with"
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "spectrogram representation allows\nthe model\nto learn robust",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "just 1-6 minutes of audio data or between 100-300 samples."
        },
        {
          "SOTA\n70.95 [23]\n84.10 [24]": "audio features for\nthe downstream task.",
          "47.10 [25]\n90.00 [26]\n69.70 [27]": "Our model capitalises on overlapping linguistic typologies and"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "NATURAL LANGUAGE TEMPLATES USED TO GENERATE DESCRIPTIVE"
        },
        {
          "TABLE II": "PHRASES FOR VARIOUS ZERO-SHOT CLASSIFICATION TASKS. {LABEL} IS"
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Template"
        },
        {
          "TABLE II": "’A {label} year old’"
        },
        {
          "TABLE II": "’A {label}’"
        },
        {
          "TABLE II": "’A person talking in a {label} voice’"
        },
        {
          "TABLE II": "’The sound of {label}’"
        },
        {
          "TABLE II": "’{label} speaker’"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": "ACCURACY ON ZERO-SHOT SOUND CLASSIFICATION WITH DIFFERENT"
        },
        {
          "TABLE III": "NATURAL LANGUAGE PROMPT TEMPLATES. MORE CONCRETE TEMPLATES"
        },
        {
          "TABLE III": "LIKE ”LABEL” AND ”THE SOUND OF A LABEL” ACHIEVE HIGHER"
        },
        {
          "TABLE III": "ACCURACY. MORE CONCEPTUAL PROMPTS LIKE ”THE LABEL EMITS A"
        },
        {
          "TABLE III": "DISTINCTIVE SOUND” REDUCE ACCURACY, REQUIRING INCREASED"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "REASONING. THE PHRASING AND SPECIFICITY OF TEMPLATES"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "SUBSTANTIALLY IMPACTS ZERO-SHOT GENERALIZATION, UNDERSCORING"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "THE IMPORTANCE OF CAREFUL TEMPLATE DESIGN"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Template\nAcc"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "’{label}’\n89.09"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "’A {label}’\n89.09"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "’{label} sounds’\n89.20"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "’A {label} can be heard’\n66.25"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "’The sound of a {label}’\n89.32"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "’The {label} emits a distinctive sound’\n55.00"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "on CREMA-D. Specifically, our zero-shot accuracy of 54.87%"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "on CREMA-D significantly exceeds the 17.84% achieved by"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "CLAP,\nrepresenting a 3x increase. Similarly, on RAVDESS,"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "our 26.33% zero-shot accuracy exceeds CLAP’s performance"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "by a factor of 1.6x."
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Additionally,\nour model\nestablishes\nnew state-of-the-art"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "zero-shot\nresults\non\nthe\nsound\nevent\nclassification\ndatasets"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Audioset, US8K, and FSD50K. On Audioset, CLARA attains"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "13.49% zero-shot\naccuracy,\nsurpassing CLAP (5.80%). On"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "US8K, we\nachieves 87.84% accuracy, outperforming CLAP"
        },
        {
          "TABLE III": "(73.24%), AudioCLIP (65.30%), and Wav2CLIP (40.44%). Fi-"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "nally, on FSD50K, CLARA’s Mean Average Precision (mAP)"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "of 32.90% exceeds CLAP (30.24%) and Wav2CLIP (3.02%),"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "representing a 2.66% increase over previous methods."
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "3) Zero-Shot Templates: Natural\nlanguage\ntemplates\nare"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "crucial\nfor zero-shot\nlearning, as\ntheir wording and framing"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "fundamentally affect generalisation. More unambiguous tem-"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "plates improve accuracy on unseen classes, while conceptual"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "prompts\ndegrade\nperformance. We\nuse\nsimple\ndescriptive"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "prompts for each target class. For\ninstance,\n’A person talking"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "in\na\n{label}\nvoice’\nfor\nemotion\ndetection. Table\nII\nshows"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "templates for different\ntasks. The choice of\ntemplate impacts"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "metrics\nlike accuracy, as abstract prompts\nrequire more rea-"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "soning and cause declines as shown in table III on sound event"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "classification. Careful design is needed to maximise inclusion"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "and avoid biases. Template engineering is vital\nfor zero-shot"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "learning, balancing clarity, specificity and inclusivity."
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "D. Linear probe (LP)"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Following established best practices, we conducted a linear"
        },
        {
          "TABLE III": "probing analysis\nto evaluate\nthe pre-trained model’s\nability"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmovDB\n42.13\n83.22\n90.93": "CREMA-D\n42.00\n79.63\n68.46",
          "99.59\n90.12\n58.35\n99.05": "85.20\n69.53\n61.94\n91.61"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "RAVDESS\n58.12\n90.54\n88.44",
          "99.59\n90.12\n58.35\n99.05": "99.24\n86.92\n94.62\n94.65"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "to\nlearn\nrepresentations\nthat\ntransfer\nto\nnovel\ndownstream",
          "99.59\n90.12\n58.35\n99.05": "data into high-dimensional\nfeature vectors\n(embeddings)\nthat"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "tasks. Specifically, we appended a randomly initialised linear",
          "99.59\n90.12\n58.35\n99.05": "encode the relevant\ninformation needed for effective retrieval."
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "evaluation head on top of our pre-trained CLARA model. This",
          "99.59\n90.12\n58.35\n99.05": "The embeddings are projected into a shared contrastive space"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "linear probe comprises three fully connected layers with ReLU",
          "99.59\n90.12\n58.35\n99.05": "via the projection function g(.), which enables the comparison"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "activations and dropout regularisation. LP allows us to analyse",
          "99.59\n90.12\n58.35\n99.05": "of\nthe audio and text embeddings. Cosine similarity is\nthen"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "the transferability of\nthe base model’s learned representations",
          "99.59\n90.12\n58.35\n99.05": "calculated between the projected audio and text embeddings"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "by training the linear probe model\nto minimise cross-entropy",
          "99.59\n90.12\n58.35\n99.05": "to determine the degree of semantic alignment. This pipeline"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "loss datasets for downstream tasks.",
          "99.59\n90.12\n58.35\n99.05": "allows\nretrieving the most\nrelevant\ntext passage for a query"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "1)\nSetup: We froze the parameters of\nthe pre-trained base",
          "99.59\n90.12\n58.35\n99.05": "audio clip and vice versa, as illustrated in Figure 3a."
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "model\nand\nattached\na\nsimple\nclassifier\nto\nthe\ntail\nof\nour",
          "99.59\n90.12\n58.35\n99.05": "1) Results:\nTable\nV\nshows\nthat\nour\nproposed model"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "model. The\nclassification tail\nis optimised end-to-end using",
          "99.59\n90.12\n58.35\n99.05": "achieves state-of-the-art performance on both audio-to-text and"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "the AdamW optimiser with a small\nlearning rate of 1e − 4.",
          "99.59\n90.12\n58.35\n99.05": "text-to-audio retrieval benchmarks.\nIn audio-to-text\nretrieval"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "The\ntraining\n(Table\nIV) was\nlimited\nto\na maximum of\n20",
          "99.59\n90.12\n58.35\n99.05": "on the AudioCaps dataset, our model outperforms prior work,"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "epochs except for Best\nto prevent overfitting, all models utilise",
          "99.59\n90.12\n58.35\n99.05": "increasing R@1 by 11.27% absolute over\nthe current CLAP"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "validation\nloss\nfor\nearly\nstopping. This\nframework\nallows",
          "99.59\n90.12\n58.35\n99.05": "model and 2.68% over ML-ATCMR. More gains are observed"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "assessing\nthe\ngeneralisable\nfeature\nlearning\nabilities\nof\nthe",
          "99.59\n90.12\n58.35\n99.05": "at R@5\nand R@10\nranks, with\nour model\ndemonstrating"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "base model without\ndegrading\nperformance\non\nits\noriginal",
          "99.59\n90.12\n58.35\n99.05": "39.66% and 19.82% improvements over CLAP,\nand 9.57%"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "pretraining task.",
          "99.59\n90.12\n58.35\n99.05": "and 9.10% over ML-ATCMR,\nrespectively."
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "2) Results: Our linear probe (table IV) experiments demon-",
          "99.59\n90.12\n58.35\n99.05": "For\ntext-to-audio retrieval, our model exhibits strong R@5"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "strate the efficacy of our model’s\nlearned representations\nfor",
          "99.59\n90.12\n58.35\n99.05": "and R@10 results, surpassing existing models by 3.32% and"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "downstream emotion and gender classification tasks without",
          "99.59\n90.12\n58.35\n99.05": "8.26% absolute. The\nconsiderable\ngains\nin\nlower-rank\nper-"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "explicit\ntraining\non\nthese\ntargets.\nThe\nlinear\nprobe\n(LP)",
          "99.59\n90.12\n58.35\n99.05": "formance highlight our model’s capabilities in recall-oriented"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "consistently outperforms\nthe base\nzero-shot\n(ZS) model on",
          "99.59\n90.12\n58.35\n99.05": "retrieval,\nleveraging versatile\nrelevance matching and broad"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "both EmoDB and CREMA-D benchmarks. For emotion clas-",
          "99.59\n90.12\n58.35\n99.05": "conceptual knowledge. While precision at\ntop-1 is lower, our"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "sification,\nthe LP achieves 90.93% top-1 recall on EmovDB",
          "99.59\n90.12\n58.35\n99.05": "model\nnonetheless\nproduces\nnuanced\nand\nuseful\nretrievals"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "versus\njust 42.13% for\nthe ZS baseline\nand 68.46% versus",
          "99.59\n90.12\n58.35\n99.05": "beyond the most obvious\nresult. These comprehensive gains"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "42.00% on CREMA-D. Despite our model not being specifi-",
          "99.59\n90.12\n58.35\n99.05": "underscore\nthe\nefficacy of our\napproach for both precision-"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "cally trained to classify the emotions, we observe significant",
          "99.59\n90.12\n58.35\n99.05": "and recall-oriented multi-modal\nretrieval."
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "gains\nin\nrecall\nscores,\ndemonstrating\nthat\nour\nbase model",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "has acquired generalisable internal\nrepresentations containing",
          "99.59\n90.12\n58.35\n99.05": "VI. CONCLUSIONS"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "substantial\ninformation about emotional content. Furthermore,",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "This\npaper\nintroduced CLARA,\na\nnovel\nframework\nfor"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "the LP\nreaches\nperformance\ncomparable\nto\nstate-of-the-art",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "multilingual\nlearning\nof\nspeech\nand\nemotion\nrepresenta-"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "models trained solely for emotion classification, substantiating",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "tions. CLARA adeptly assimilates shared representations har-"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "the\nversatility\nof\nour\ngeneral-purpose\nlearned\nfeatures.\non",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "monised across diverse linguistic landscapes, fostering a trans-"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "RAVDESS we outperform VQ-MAE-S-12 [24] by 2.8% abso-",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "ference of\nlearned attributes to novel\nlanguages and tasks."
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "lute and achieve comparable results on CREMA-D to LeRaC",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "In our exploration of data augmentation, we capitalised on"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "[28]\n(70.95%). Our model\nexhibits\nan aptitude\nfor\nintrinsic",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "visual\ncues\nderived\nfrom video\ncontent\nto\nglean\nemotional"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "emotional modelling, as evidenced by the linear probe’s strong",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "and contextual\ninsights. These insights were transformed into"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "exploitability of these internal representations for emotion and",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "text\nformat and injected into the natural\nlanguage descriptors"
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "gender predictions without\ntask-specific finetuning.",
          "99.59\n90.12\n58.35\n99.05": ""
        },
        {
          "EmovDB\n42.13\n83.22\n90.93": "",
          "99.59\n90.12\n58.35\n99.05": "affiliated with audio data. This approach eliminates the need"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "RETRIEVAL DIRECTIONS.": ""
        },
        {
          "RETRIEVAL DIRECTIONS.": ""
        },
        {
          "RETRIEVAL DIRECTIONS.": "R@1"
        },
        {
          "RETRIEVAL DIRECTIONS.": "30.81"
        },
        {
          "RETRIEVAL DIRECTIONS.": "39.40"
        },
        {
          "RETRIEVAL DIRECTIONS.": "-"
        },
        {
          "RETRIEVAL DIRECTIONS.": "42.08"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "complexities inherent\nin multilingual speech processing tasks."
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "The empirical outcomes accentuate the merits of our frame-"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "work in procuring universally generalisable speech represen-"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "tations that\nthrive even amidst\nthe scarcity of\ntarget\nlanguage"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "data. The versatility and adaptability of the model were further"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "corroborated by the competitive performance of simple linear"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "classifiers, which, when employed on frozen base\nfeatures,"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "required no task-specific fine-tuning. Some\nlimitations were"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "observed in extending representations\nto isolated languages."
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "Future work should explore approaches tailored to the specific"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "characteristics of\nindividual\nlanguages,\nespecially those\nthat"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "are\nisolated\nand\nconsult with\nlinguistic\nexperts\nto\nbetter"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "understand the nuances and intricacies of isolated languages to"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "address\nthese weaknesses. Nonetheless, our\nresults highlight"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "CLARA’s promising capabilities\nfor generalised learning of"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "emotion and speech across diverse languages."
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "This study marks notable milestones, unveiling a model for"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "speech and emotion understanding, data\naugmentation tech-"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "niques\nincorporating visual, meta and audio data, and robust"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "empirical validation of\nsuperior performance\nacross various"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "speech-processing tasks.\nIt underscores\nthe potential of\nthis"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "methodology in harnessing universal\nspeech representations"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "that\ntranscend linguistic,\naccentual,\nand acoustic diversities,"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "thus paving the way for groundbreaking advancements in the"
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": ""
        },
        {
          "scenarios,\nthereby underscoring its potential\nin navigating the": "domain of multilingual speech and emotion processing."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "92.71\n96.24\n96.24\nCREMA-D": "EDB + CD + RAV\n71.74\n83.29\n84.05",
          "90.97\n87.85\n93.46\n93.37\n86.11": "71.49\n69.65\n82.05\n82.80\n69.44"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "This endeavour\nfacilitated the generation of coherent natural",
          "90.97\n87.85\n93.46\n93.37\n86.11": "VII. ACKNOWLEDGEMENTS"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "language descriptions that encapsulate the contextual essence",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "This research was supported by The Engineering and Phys-"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "of\nthe\naudio\ncontent. Our\ncarefully\ndesigned\nexperimental",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "ical Sciences Research Council (EPSRC) by means of a grant"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "framework bore\ntestimony to the\nrobust performance of\nthe",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "awarded to Kari A Noriy. CDE2: EP/L016540/1 (for 2014/15"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "model. This\nachievement\nindicates\na\nsignificant\nstride\nto-",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "cohort and subsequent intakes), and we would like to thank the"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "wards enhancing state-of-the-art benchmarks in the domains of",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "support of Stability AI\nfor providing compute resources\nthat"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "emotion recognition, audio classification, and retrieval\ntasks.",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "were instrumental\nin conducting the research presented in this"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "The model\nshowcased efficacy under zero-shot and few-shot",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "publication and the Laion community member who has helped"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "scenarios,\nthereby underscoring its potential\nin navigating the",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "collect and process the dataset used in training this model."
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "complexities inherent\nin multilingual speech processing tasks.",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "REFERENCES"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "The empirical outcomes accentuate the merits of our frame-",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[1] A. Guzhov, F. Raue,\nJ. Hees, and A. Dengel, “AudioCLIP: Extending"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "work in procuring universally generalisable speech represen-",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "CLIP to\nImage, Text\nand Audio,”\nJun.\n2021,\narXiv:2106.13043\n[cs,"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "tations that\nthrive even amidst\nthe scarcity of\ntarget\nlanguage",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "eess].\n[Online]. Available: http://arxiv.org/abs/2106.13043"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "data. The versatility and adaptability of the model were further",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[2]\nJ. Yu,\nZ. Wang, V. Vasudevan,\nL. Yeung, M.\nSeyedhosseini,\nand"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Y\n. Wu,\n“CoCa: Contrastive Captioners\nare\nImage-Text\nFoundation"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "corroborated by the competitive performance of simple linear",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Models,”\nJun.\n2022,\narXiv:2205.01917\n[cs].\n[Online].\nAvailable:"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "classifiers, which, when employed on frozen base\nfeatures,",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "http://arxiv.org/abs/2205.01917"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "required no task-specific fine-tuning. Some\nlimitations were",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[3] A. Adigwe, N. Tits, K. E. Haddad,\nS. Ostadabbas,\nand T. Dutoit,"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "“The\nemotional\nvoices\ndatabase:\nTowards\ncontrolling\nthe\nemotion"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "observed in extending representations\nto isolated languages.",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "dimension in voice generation systems,” CoRR, vol.\nabs/1806.09514,"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "Future work should explore approaches tailored to the specific",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "2018.\n[Online]. Available: http://arxiv.org/abs/1806.09514"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "characteristics of\nindividual\nlanguages,\nespecially those\nthat",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[4]\nT.\nChen,\nS. Kornblith, M. Norouzi,\nand G. Hinton,\n“A Simple"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Framework\nfor\nContrastive\nLearning\nof\nVisual\nRepresentations,”"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "are\nisolated\nand\nconsult with\nlinguistic\nexperts\nto\nbetter",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Jun.\n2020,\narXiv:2002.05709\n[cs,\nstat].\n[Online].\nAvailable:\nhttp:"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "understand the nuances and intricacies of isolated languages to",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "//arxiv.org/abs/2002.05709"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "address\nthese weaknesses. Nonetheless, our\nresults highlight",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[5]\nT. Chen,\nS. Kornblith, K.\nSwersky, M. Norouzi,\nand G. Hinton,"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "“Big Self-Supervised Models\nare Strong Semi-Supervised Learners,”"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "CLARA’s promising capabilities\nfor generalised learning of",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Oct.\n2020,\narXiv:2006.10029\n[cs,\nstat].\n[Online].\nAvailable:\nhttp:"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "emotion and speech across diverse languages.",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "//arxiv.org/abs/2006.10029"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[6] K.\nHe,\nH.\nFan,\nY\n. Wu,\nS.\nXie,\nand\nR.\nGirshick,\n“Momentum"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Contrast for Unsupervised Visual Representation Learning,” Mar. 2020,"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "This study marks notable milestones, unveiling a model for",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "arXiv:1911.05722\n[cs].\n[Online]. Available:\nhttp://arxiv.org/abs/1911."
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "speech and emotion understanding, data\naugmentation tech-",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "05722"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "niques\nincorporating visual, meta and audio data, and robust",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[7] A. Radford,\nJ. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "G.\nSastry,\nA.\nAskell,\nP. Mishkin,\nJ.\nClark,\nG.\nKrueger,\nand"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "empirical validation of\nsuperior performance\nacross various",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "I.\nSutskever,\n“Learning\nTransferable Visual Models\nFrom Natural"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "speech-processing tasks.\nIt underscores\nthe potential of\nthis",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Language Supervision,” Feb.\n2021,\narXiv:2103.00020\n[cs].\n[Online]."
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "methodology in harnessing universal\nspeech representations",
          "90.97\n87.85\n93.46\n93.37\n86.11": "Available: http://arxiv.org/abs/2103.00020"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "[8] A. Ramesh, M.\nPavlov, G. Goh,\nS. Gray, C. Voss, A. Radford,"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "that\ntranscend linguistic,\naccentual,\nand acoustic diversities,",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "M. Chen, and I. Sutskever, “Zero-Shot Text-to-Image Generation,” Feb."
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "thus paving the way for groundbreaking advancements in the",
          "90.97\n87.85\n93.46\n93.37\n86.11": ""
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "",
          "90.97\n87.85\n93.46\n93.37\n86.11": "2021,\narXiv:2102.12092 [cs].\n[Online]. Available: http://arxiv.org/abs/"
        },
        {
          "92.71\n96.24\n96.24\nCREMA-D": "domain of multilingual speech and emotion processing.",
          "90.97\n87.85\n93.46\n93.37\n86.11": "2102.12092"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "[31]\nS. Chen, H. Li, Q. Wang, Z. Zhao, M. Sun, X. Zhu, and J. Liu, “VAST:"
        },
        {
          "9": "A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model\nand"
        },
        {
          "9": "Dataset,” May 2023, arXiv:2305.18500 [cs, eess] version: 1.\n[Online]."
        },
        {
          "9": "Available: http://arxiv.org/abs/2305.18500"
        },
        {
          "9": "[32]\nS. R. Livingstone and F. A. Russo, “The ryerson audio-visual database"
        },
        {
          "9": "of\nemotional\nspeech and song (RAVDESS): A dynamic, multimodal"
        },
        {
          "9": "set of\nfacial and vocal expressions\nin north american english,” PLOS"
        },
        {
          "9": "ONE,\nvol.\n13,\nno.\n5,\np.\ne0196391, May\n2018.\n[Online]. Available:"
        },
        {
          "9": "https://doi.org/10.1371/journal.pone.0196391"
        },
        {
          "9": "[33] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler,\nJ. Meyer,"
        },
        {
          "9": "R. Morais, L. Saunders, F. M. Tyers, and G. Weber, “Common voice:"
        },
        {
          "9": "A massively-multilingual\nspeech corpus,” CoRR, vol. abs/1912.06670,"
        },
        {
          "9": "2019.\n[Online]. Available: http://arxiv.org/abs/1912.06670"
        },
        {
          "9": "[34] M. Mazumder,\nS.\nChitlangia,\nC.\nBanbury, Y. Kang,\nJ. M.\nCiro,"
        },
        {
          "9": "K. Achorn, D. Galvez, M. Sabini, P. Mattson, D. Kanter, G. Diamos,"
        },
        {
          "9": "P. Warden,\nJ. Meyer,\nand V.\nJ. Reddi,\n“Multilingual Spoken Words"
        },
        {
          "9": "Corpus,” Aug. 2021.\n[Online]. Available: https://openreview.net/forum?"
        },
        {
          "9": "id=c20jiJ5K2H"
        },
        {
          "9": "[35] V. Panayotov, G. Chen, D. Povey,\nand S. Khudanpur,\n“Librispeech:"
        },
        {
          "9": "An asr\ncorpus based on public domain audio books,”\nin 2015 IEEE"
        },
        {
          "9": "International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "9": "(ICASSP), 2015, pp. 5206–5210."
        },
        {
          "9": "[36] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova,"
        },
        {
          "9": "and R. Verma, “Crema-d: Crowd-sourced emotional multimodal actors"
        },
        {
          "9": "dataset,” IEEE Transactions on Affective Computing, vol. 5, no. 4, pp."
        },
        {
          "9": "377–390, 2014."
        },
        {
          "9": "[37]\nJ.\nKominek\nand\nA. W.\nBlack,\n“THE\nCMU\nARCTIC\nSPEECH"
        },
        {
          "9": "DATABASES.” [Online]. Available: http://festvox.org/cmu arctic/"
        },
        {
          "9": "[38]\n“Cambridge Dictionary | English Dictionary, Translations & Thesaurus,”"
        },
        {
          "9": "Aug. 2023.\n[Online]. Available: https://dictionary.cambridge.org/"
        },
        {
          "9": "[39] C. D. Kim, B. Kim, H. Lee,\nand G. Kim,\n“AudioCaps: Generating"
        },
        {
          "9": "Proceedings\nof\nthe\n2019\nCaptions\nfor Audios\nin\nThe Wild,”\nin"
        },
        {
          "9": "Conference\nof\nthe North.\nMinneapolis, Minnesota: Association\nfor"
        },
        {
          "9": "Computational Linguistics,\n2019,\npp.\n119–132.\n[Online]. Available:"
        },
        {
          "9": "http://aclweb.org/anthology/N19-1011"
        },
        {
          "9": "[40]\n“Tunebot,”\nJul.\n2023.\n[Online]. Available:\nhttps://interactiveaudiolab."
        },
        {
          "9": "github.io/resources/datasets/tunebot.html"
        },
        {
          "9": "[41] M. Cartwright\nand B. Pardo,\n“VocalSketch: Vocally Imitating Audio"
        },
        {
          "9": "of\nthe\n33rd Annual ACM Conference\non\nConcepts,”\nin Proceedings"
        },
        {
          "9": "Human Factors in Computing Systems, ser. CHI\n’15.\nNew York, NY,"
        },
        {
          "9": "USA: Association\nfor Computing Machinery, Apr.\n2015,\npp.\n43–46."
        },
        {
          "9": "[Online]. Available: https://doi.org/10.1145/2702123.2702387"
        },
        {
          "9": "[42] B. Kim and\nB.\nPardo,\n“Fine-grained Vocal\nImitation\nSet,” Nov."
        },
        {
          "9": "2019.\n[Online].\nAvailable:\nhttps://zenodo.org/record/3538534/export/"
        },
        {
          "9": "schemaorg jsonld"
        },
        {
          "9": "[43]\nJ. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W. Lawrence,"
        },
        {
          "9": "R. C. Moore, M. Plakal, and M. Ritter, “Audio Set: An ontology and"
        },
        {
          "9": "human-labeled dataset\nfor\naudio events,”\nin 2017 IEEE International"
        },
        {
          "9": "Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar."
        },
        {
          "9": "2017, pp. 776–780,\niSSN: 2379-190X."
        },
        {
          "9": "[44] K.\nDrossos,\nS.\nLipping,\nand\nT.\nVirtanen,\n“Clotho:\nan\nAudio"
        },
        {
          "9": "ICASSP\n2020\n-\n2020\nIEEE\nInternational\nCaptioning Dataset,”\nin"
        },
        {
          "9": "Conference\non Acoustics,\nSpeech\nand\nSignal Processing\n(ICASSP)."
        },
        {
          "9": "Barcelona, Spain:\nIEEE, May 2020, pp. 736–740.\n[Online]. Available:"
        },
        {
          "9": "https://ieeexplore.ieee.org/document/9052990/"
        },
        {
          "9": "[45] H. Chen, W. Xie, A. Vedaldi,\nand A. Zisserman,\n“VGGSound: A"
        },
        {
          "9": "Large-scale Audio-Visual Dataset,” Sep. 2020,\narXiv:2004.14368 [cs,"
        },
        {
          "9": "eess].\n[Online]. Available: http://arxiv.org/abs/2004.14368"
        },
        {
          "9": "[46] K.\nJ. Piczak, “ESC: Dataset\nfor Environmental Sound Classification,”"
        },
        {
          "9": "Proceedings\nof\nthe\n23rd\nACM\ninternational\nconference\non\nin"
        },
        {
          "9": "Multimedia,\nser. MM ’15.\nNew York, NY, USA: Association\nfor"
        },
        {
          "9": "Computing Machinery, Oct. 2015, pp. 1015–1018.\n[Online]. Available:"
        },
        {
          "9": "https://doi.org/10.1145/2733373.2806390"
        },
        {
          "9": "[47]\nJ. Salamon, C.\nJacoby,\nand\nJ. P. Bello,\n“A Dataset\nand Taxonomy"
        },
        {
          "9": "Proceedings\nof\nthe\n22nd\nACM\nfor\nUrban\nSound\nResearch,”\nin"
        },
        {
          "9": "international\nconference\non Multimedia.\nOrlando\nFlorida\nUSA:"
        },
        {
          "9": "ACM,\nNov.\n2014,\npp.\n1041–1044.\n[Online].\nAvailable:\nhttps:"
        },
        {
          "9": "//dl.acm.org/doi/10.1145/2647868.2655045"
        },
        {
          "9": "[48]\nE.\nFonseca, X.\nFavory,\nJ.\nPons,\nF.\nFont,\nand X.\nSerra,\n“FSD50K:"
        },
        {
          "9": "An Open Dataset\nof Human-Labeled\nSound\nEvents,” Apr.\n2022,"
        },
        {
          "9": "arXiv:2010.00475\n[cs,\neess,\nstat]\nversion:\n2.\n[Online].\nAvailable:"
        },
        {
          "9": "http://arxiv.org/abs/2010.00475"
        },
        {
          "9": "[49]\nI. M. Morato and A. Mesaros,\n“MACS - Multi-Annotator Captioned"
        },
        {
          "9": "Soundscapes,” Jul. 2021.\n[Online]. Available: https://zenodo.org/record/"
        },
        {
          "9": "5114771"
        },
        {
          "9": ""
        },
        {
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "a multilingual and gender\nrepresentation perspective to guide"
        },
        {
          "10": "the ongoing enhancement of\nthe composite dataset."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "B. Visual Cues Fusion"
        },
        {
          "10": ""
        },
        {
          "10": "In addition to the data augmentation strategy discussed in"
        },
        {
          "10": ""
        },
        {
          "10": "Section III-A2, we employ an additional\ntechnique aimed at"
        },
        {
          "10": ""
        },
        {
          "10": "improving the generation of captions for samples that possess"
        },
        {
          "10": ""
        },
        {
          "10": "both\naudio\nand\nvisual\nrepresentations. This\nsupplementary"
        },
        {
          "10": ""
        },
        {
          "10": "approach involves utilising visual cues extracted from static"
        },
        {
          "10": ""
        },
        {
          "10": "images to assist\nin caption generation. To accomplish this, we"
        },
        {
          "10": ""
        },
        {
          "10": "utilise the CoCa model [2], which is an image-to-text founda-"
        },
        {
          "10": ""
        },
        {
          "10": "tion model\nthat has demonstrated state-of-the-art performance"
        },
        {
          "10": ""
        },
        {
          "10": "on vision and vision-language tasks. Designed explicitly for"
        },
        {
          "10": ""
        },
        {
          "10": "generating captions\nfor\nimages, CoCa\nleverages both visual"
        },
        {
          "10": ""
        },
        {
          "10": "and textual\ninformation to produce\naccurate\nand descriptive"
        },
        {
          "10": ""
        },
        {
          "10": "captions."
        },
        {
          "10": ""
        },
        {
          "10": "Let V F\nrepresent a set of\nframes extracted from a given"
        },
        {
          "10": ""
        },
        {
          "10": "video. We\napply\nthe\nimage-to-caption model,\ndenoted\nas"
        },
        {
          "10": ""
        },
        {
          "10": "I2T,\nto each frame in order\nto generate visual captions. The"
        },
        {
          "10": ""
        },
        {
          "10": "process is guided by the Structural Similarity Index Measure"
        },
        {
          "10": ""
        },
        {
          "10": "(SSIM) between consecutive frames, denoted as V F\nand V F\ni+1,"
        },
        {
          "10": "i"
        },
        {
          "10": "respectively. Visual captions C v\nare obtained for\nframes that"
        },
        {
          "10": ""
        },
        {
          "10": "exhibit a significant change in scene content, determined by"
        },
        {
          "10": ""
        },
        {
          "10": "comparing the SSIM value to a threshold θ. Mathematically,"
        },
        {
          "10": ""
        },
        {
          "10": "the set of visual captions C v can be expressed as:"
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ", V F\nC v = (cid:8)I2T(V F\n) | SSIM(V F\ni\ni\ni+1)"
        },
        {
          "10": ""
        },
        {
          "10": "> θ,\ni = 1, 2, . . . , n − 1}\n(4)"
        },
        {
          "10": ""
        },
        {
          "10": "Where I2T represents the image-to-caption model, which is"
        },
        {
          "10": "a\nspecialised model\ntrained on a dataset\ncomprising image-"
        },
        {
          "10": "text pairs.\nIts primary objective\nis\nto generate\ncaptions\nthat"
        },
        {
          "10": "accurately describe the content of\nimages. The SSIM metric"
        },
        {
          "10": "serves\nas\na measure\nof\nstructural\nsimilarity,\nallowing\nthe"
        },
        {
          "10": "identification of\nchanges\nin scene\ncontent. The\nthreshold θ"
        },
        {
          "10": "determines\nthe\nlevel\nof\nchange\nrequired\nfor\na\nframe\nto\nbe"
        },
        {
          "10": "considered distinct."
        },
        {
          "10": "To integrate the audio descriptions C t with the visual cap-"
        },
        {
          "10": "tions C v, we employ a language model (LLM), as described in"
        },
        {
          "10": "Equation 1 and illustrated in Figure 2. The LLM converts the"
        },
        {
          "10": "raw captions\ninto meaningful\ntext descriptions, unifying the"
        },
        {
          "10": "audio and visual modalities. This step ensures a cohesive and"
        },
        {
          "10": "comprehensive\nrepresentation\nof\nthe\ncombined\naudio-visual"
        },
        {
          "10": "content."
        },
        {
          "10": ""
        },
        {
          "10": "C = f (C v, C t)\n(5)"
        },
        {
          "10": ""
        },
        {
          "10": "In summary, by incorporating visual cues from static images"
        },
        {
          "10": ""
        },
        {
          "10": "using the CoCa model and employing an LLM to unify audio"
        },
        {
          "10": ""
        },
        {
          "10": "and visual captions, our approach enhances the generation of"
        },
        {
          "10": ""
        },
        {
          "10": "captions\nfor\nsamples with audio and visual\nrepresentations."
        },
        {
          "10": ""
        },
        {
          "10": "This methodology promotes a more comprehensive and cohe-"
        },
        {
          "10": ""
        },
        {
          "10": "sive understanding of multimedia content."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "C. Multilingual Augmentation"
        },
        {
          "10": ""
        },
        {
          "10": "To\nexpand\nthe\ndiversity\nof\nlanguages\nrepresented\nin\nthe"
        },
        {
          "10": "training\ndata, we\nemploy\nneural\naugmentation\ntechniques"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": "ENVIRONMENTAL SOUNDS (S). DETAILS PROVIDED INCLUDE TOTAL AUDIO HOURS, LANGUAGES PER DATASET, AND SPEAKER GENDER INFORMATION,"
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": "EMPOWER CLARA TO ACQUIRE ROBUST AUDIO REPRESENTATIONS SPANNING LANGUAGES, ACCENTS, AGES, GENDERS, AND ENVIRONMENTS."
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": "Imitation Set\n[42]"
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        },
        {
          "CLARA TRAINING USES VARIOUS AUDIO DATASETS ENCOMPASSING SPEECH (SP), EMOTION (E), AGE (A), GENDER (G), SPOKEN WORDS (W), AND": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MACS [49]\nS\n11": "Total\n-\n≈ 20848.7",
          "-\n-\n-\n-": "181\n12,881,426\n3,274,461\n5,847,175"
        },
        {
          "MACS [49]\nS\n11": "leveraging pre-trained multilingual models. The process\nin-",
          "-\n-\n-\n-": "variations. To\nevaluate\nperformance, we\ncalculate\naccuracy"
        },
        {
          "MACS [49]\nS\n11": "volves\nfirst\ntranslating\nthe\nclass\nlabels\nor\ncaptions\nin\nthe",
          "-\n-\n-\n-": "on a per-word basis\n-\nthe percentage of\ntest word utterances"
        },
        {
          "MACS [49]\nS\n11": "original mono-lingual dataset\ninto\ntarget\nlanguages using\na",
          "-\n-\n-\n-": "correctly\nrecognised\nby\nthe model\nfor\neach\nlanguage. The"
        },
        {
          "MACS [49]\nS\n11": "language model (LM). For instance, an English label like ”dog",
          "-\n-\n-\n-": "accuracy\nresults\nshowcase CLARA’s\nability\nto\nlearn\neffec-"
        },
        {
          "MACS [49]\nS\n11": "barking” can be translated into French, Hindi, Mandarin etc.",
          "-\n-\n-\n-": "tive representations for high-resource languages like English,"
        },
        {
          "MACS [49]\nS\n11": "We then overlay these translated labels on the original audio",
          "-\n-\n-\n-": "French,\nand German with\nabundant\ntraining\nexamples. For"
        },
        {
          "MACS [49]\nS\n11": "samples\nto create\nsynthetic multilingual\nsamples. The\nsame",
          "-\n-\n-\n-": "instance, English achieves 97% accuracy with over 1,400 hours"
        },
        {
          "MACS [49]\nS\n11": "audio recording is duplicated with different language overlays.",
          "-\n-\n-\n-": "of\ntraining data."
        },
        {
          "MACS [49]\nS\n11": "This methodology\nscales\nefficiently\nby\nrelying\non LMs’",
          "-\n-\n-\n-": "More\ninterestingly, CLARA demonstrates\nan aptitude\nfor"
        },
        {
          "MACS [49]\nS\n11": "pre-trained multilingual\ncapabilities.\nIt\navoids\nthe\nneed\nfor",
          "-\n-\n-\n-": "cross-lingual\ntransfer\nand generalisation,\neven with minimal"
        },
        {
          "MACS [49]\nS\n11": "manual\nannotation or\ncollection of\nspeech data\nin multiple",
          "-\n-\n-\n-": "data\nfor\nlow-resource\nlanguages.\nFor\nexample, Lithuanian"
        },
        {
          "MACS [49]\nS\n11": "languages. We apply this technique to augment several mono-",
          "-\n-\n-\n-": "attains 93.7% accuracy with just 0.03 hours of\naudio (100"
        },
        {
          "MACS [49]\nS\n11": "lingual datasets, synthesising parallel corpora covering the 181",
          "-\n-\n-\n-": "samples).\nSimilarly, Guarani\nreaches\n88.8% accuracy with"
        },
        {
          "MACS [49]\nS\n11": "languages.",
          "-\n-\n-\n-": "only 0.1 hours (285 samples). The model appears to leverage"
        },
        {
          "MACS [49]\nS\n11": "This multilingual data augmentation enriches the variability",
          "-\n-\n-\n-": "similarities with other\ntrained languages."
        },
        {
          "MACS [49]\nS\n11": "of languages and accents within our training set. The model\nis",
          "-\n-\n-\n-": "In contrast,\nisolated languages like Georgian achieve lower"
        },
        {
          "MACS [49]\nS\n11": "exposed to far greater\nlinguistic diversity, which enhances its",
          "-\n-\n-\n-": "accuracy of 86.3% despite more\ntraining data of 0.9 hours."
        },
        {
          "MACS [49]\nS\n11": "generalisation capabilities. We efficiently compensate for\nthe",
          "-\n-\n-\n-": "This\nindicates\nlanguage\nproximity\nis\ncritical\nfor\neffective"
        },
        {
          "MACS [49]\nS\n11": "lack of\nreal multilingual speech/sound-event data.",
          "-\n-\n-\n-": "transfer. Georgians experiences more errors due to its complex"
        },
        {
          "MACS [49]\nS\n11": "In summary, neural multilingual augmentation proves highly",
          "-\n-\n-\n-": "consonant clusters.\nIntegrating phonetic and linguistic knowl-"
        },
        {
          "MACS [49]\nS\n11": "effective in expanding the corpus to encompass a wider range",
          "-\n-\n-\n-": "edge could help address\nsuch issues. However,\nthere remain"
        },
        {
          "MACS [49]\nS\n11": "of\nlanguages and linguistic phenomena. This promotes more",
          "-\n-\n-\n-": "challenges\nin improving robustness\nin isolated and complex"
        },
        {
          "MACS [49]\nS\n11": "universal\nspeech representation learning and improves\nzero-",
          "-\n-\n-\n-": "languages."
        },
        {
          "MACS [49]\nS\n11": "shot\ntransferability.",
          "-\n-\n-\n-": "Overall,\nthese results demonstrate CLARA’s promising ca-"
        },
        {
          "MACS [49]\nS\n11": "",
          "-\n-\n-\n-": "pability\nfor\ngeneralised multilingual\nkeyword\nspotting. The"
        },
        {
          "MACS [49]\nS\n11": "",
          "-\n-\n-\n-": "model\neffectively\ntransfers\nlearned\nrepresentations\nbetween"
        },
        {
          "MACS [49]\nS\n11": "D. Keyword Spotting",
          "-\n-\n-\n-": ""
        },
        {
          "MACS [49]\nS\n11": "",
          "-\n-\n-\n-": "related languages,\neven with minimal\ntarget\nlanguage data."
        },
        {
          "MACS [49]\nS\n11": "The Multilingual Spoken Words Corpus used for evaluation",
          "-\n-\n-\n-": "Our augmentation strategies also help improve learning."
        },
        {
          "MACS [49]\nS\n11": "consists\nof\nshort\nspoken word\nutterances\ncovering\n50\nlan-",
          "-\n-\n-\n-": ""
        },
        {
          "MACS [49]\nS\n11": "guages. For\neach language,\nthere\nare\nthousands of\nspeakers",
          "-\n-\n-\n-": ""
        },
        {
          "MACS [49]\nS\n11": "in various accents and conditions. This allows benchmarking",
          "-\n-\n-\n-": ""
        },
        {
          "MACS [49]\nS\n11": "keyword spotting across\na variety of\nlanguages\nand speech",
          "-\n-\n-\n-": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "",
          "INDICATING CLARA’S ABILITY TO TRANSFER": ""
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "",
          "INDICATING CLARA’S ABILITY TO TRANSFER": ""
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Language",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "Accuracy"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Arabic",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "94.1"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Breton",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "93.2"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Catalan; Valencian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "98.2"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Welsh",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "97.5"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "German",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "98.0"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Divehi",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "71.4"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Greek",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "92.8"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "English",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "97.0"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Esperanto",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "97.8"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Estonian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "95.8"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Basque",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "97.9"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Persian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "98.2"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "French",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "97.4"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Indonesian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "93.8"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Italian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "96.7"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Georgian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "86.3"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Kyrgyz",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "95.1"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Mongolian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "94.1"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Polish",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "96.7"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Romanian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "94.7"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Russian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "93.9"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Kinyarwanda",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "94.7"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Tamil",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "85.5"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Turkish",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "94.4"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Ukrainian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "93.8"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Odia",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "61.5"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Lithuanian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "93.7"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Slovak",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "94.0"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Guarani",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "88.8"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Assamese",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "67.1"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Latvian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "95.6"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Vietnamese",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "73.6"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Hausa",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "89.5"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Slovenian",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "95.0"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Chuvash",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "94.2"
        },
        {
          "HIGHER ACCURACY WITH MINIMAL TRAINING DATA THAN ISOLATED LANGUAGES LIKE VIETNAMESE,": "Maltese",
          "INDICATING CLARA’S ABILITY TO TRANSFER": "96.0"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "AudioCLIP: Extending CLIP to Image, Text and Audio",
      "authors": [
        "A Guzhov",
        "F Raue",
        "J Hees",
        "A Dengel"
      ],
      "year": "2021",
      "venue": "AudioCLIP: Extending CLIP to Image, Text and Audio",
      "arxiv": "arXiv:2106.13043"
    },
    {
      "citation_id": "2",
      "title": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
      "authors": [
        "J Yu",
        "Z Wang",
        "V Vasudevan",
        "L Yeung",
        "M Seyedhosseini",
        "Y Wu"
      ],
      "year": "2022",
      "venue": "CoCa: Contrastive Captioners are Image-Text Foundation Models",
      "arxiv": "arXiv:2205.01917"
    },
    {
      "citation_id": "3",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "A Adigwe",
        "N Tits",
        "K Haddad",
        "S Ostadabbas",
        "T Dutoit"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "4",
      "title": "A Simple Framework for Contrastive Learning of Visual Representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "A Simple Framework for Contrastive Learning of Visual Representations",
      "arxiv": "arXiv:2002.05709"
    },
    {
      "citation_id": "5",
      "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
      "authors": [
        "T Chen",
        "S Kornblith",
        "K Swersky",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
      "arxiv": "arXiv:2006.10029"
    },
    {
      "citation_id": "6",
      "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Momentum Contrast for Unsupervised Visual Representation Learning",
      "arxiv": "arXiv:1911.05722"
    },
    {
      "citation_id": "7",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Learning Transferable Visual Models From Natural Language Supervision",
      "arxiv": "arXiv:2103.00020"
    },
    {
      "citation_id": "8",
      "title": "Zero-Shot Text-to-Image Generation",
      "authors": [
        "A Ramesh",
        "M Pavlov",
        "G Goh",
        "S Gray",
        "C Voss",
        "A Radford",
        "M Chen",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Zero-Shot Text-to-Image Generation",
      "arxiv": "arXiv:2102.12092"
    },
    {
      "citation_id": "9",
      "title": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "authors": [
        "A Ramesh",
        "P Dhariwal",
        "A Nichol",
        "C Chu",
        "M Chen"
      ],
      "year": "2022",
      "venue": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
      "arxiv": "arXiv:2204.06125"
    },
    {
      "citation_id": "10",
      "title": "Highresolution image synthesis with latent diffusion models",
      "authors": [
        "R Rombach",
        "A Blattmann",
        "D Lorenz",
        "P Esser",
        "B Ommer"
      ],
      "year": "2021",
      "venue": "Highresolution image synthesis with latent diffusion models"
    },
    {
      "citation_id": "11",
      "title": "ClipCap: CLIP Prefix for Image Captioning",
      "authors": [
        "R Mokady",
        "A Hertz",
        "A Bermano"
      ],
      "year": "2021",
      "venue": "ClipCap: CLIP Prefix for Image Captioning",
      "arxiv": "arXiv:2111.09734"
    },
    {
      "citation_id": "12",
      "title": "Contrastive Learning of General-Purpose Audio Representations",
      "authors": [
        "A Saeed",
        "D Grangier",
        "N Zeghidour"
      ],
      "year": "2020",
      "venue": "Contrastive Learning of General-Purpose Audio Representations",
      "arxiv": "arXiv:2010.10915"
    },
    {
      "citation_id": "13",
      "title": "EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels",
      "authors": [
        "K Noriy",
        "X Yang",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels",
      "arxiv": "arXiv:2305.13137"
    },
    {
      "citation_id": "14",
      "title": "",
      "authors": [
        "Online"
      ],
      "venue": ""
    },
    {
      "citation_id": "15",
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "authors": [
        "M Mcauliffe",
        "M Socolof",
        "S Mihuc",
        "M Wagner",
        "M Sonderegger"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2022",
      "venue": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "17",
      "title": "A Comparison on Data Augmentation Methods Based on Deep Learning for Audio Classification",
      "authors": [
        "S Wei",
        "S Zou",
        "F Liao",
        "W Lang"
      ],
      "year": "2020",
      "venue": "Journal of Physics: Conference Series",
      "doi": "10.1088/1742-6596/1453/1/012085"
    },
    {
      "citation_id": "18",
      "title": "mixup: Beyond Empirical Risk Minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "mixup: Beyond Empirical Risk Minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "19",
      "title": "Open Assistant",
      "venue": "Open Assistant"
    },
    {
      "citation_id": "20",
      "title": "Perceiver: General Perception with Iterative Attention",
      "authors": [
        "A Jaegle",
        "F Gimeno",
        "A Brock",
        "A Zisserman",
        "O Vinyals",
        "J Carreira"
      ],
      "year": "2021",
      "venue": "Perceiver: General Perception with Iterative Attention",
      "arxiv": "arXiv:2103.03206"
    },
    {
      "citation_id": "21",
      "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "authors": [
        "T Dao",
        "D Fu",
        "S Ermon",
        "A Rudra",
        "C Ré"
      ],
      "year": "2022",
      "venue": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "arxiv": "arXiv:2205.14135"
    },
    {
      "citation_id": "22",
      "title": "CLAP: Learning Audio Concepts From Natural Language Supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2022",
      "venue": "CLAP: Learning Audio Concepts From Natural Language Supervision",
      "arxiv": "arXiv:2206.04769"
    },
    {
      "citation_id": "23",
      "title": "Wav2CLIP: Learning Robust Audio Representations From CLIP",
      "authors": [
        "H.-H Wu",
        "P Seetharaman",
        "K Kumar",
        "J Bello"
      ],
      "year": "2022",
      "venue": "Wav2CLIP: Learning Robust Audio Representations From CLIP",
      "arxiv": "arXiv:2110.11499"
    },
    {
      "citation_id": "24",
      "title": "LeRaC: Learning Rate Curriculum",
      "authors": [
        "F.-A Croitoru",
        "N.-C Ristea",
        "R Ionescu",
        "N Sebe"
      ],
      "year": "2022",
      "venue": "LeRaC: Learning Rate Curriculum",
      "arxiv": "arXiv:2205.09180"
    },
    {
      "citation_id": "25",
      "title": "A vector quantized masked autoencoder for speech emotion recognition",
      "authors": [
        "S Sadok",
        "S Leglaive",
        "R Séguier"
      ],
      "year": "2023",
      "venue": "A vector quantized masked autoencoder for speech emotion recognition",
      "arxiv": "arXiv:2304.11117"
    },
    {
      "citation_id": "26",
      "title": "Efficient Training of Audio Transformers with Patchout",
      "authors": [
        "K Koutini",
        "J Schlüter",
        "G Widmer"
      ],
      "year": "2022",
      "venue": "Interspeech 2022",
      "arxiv": "arXiv:2110.05069"
    },
    {
      "citation_id": "27",
      "title": "End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network",
      "authors": [
        "A Gazneli",
        "G Zimerman",
        "T Ridnik",
        "G Sharir",
        "A Noy"
      ],
      "year": "2022",
      "venue": "End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network",
      "arxiv": "arXiv:2204.11479"
    },
    {
      "citation_id": "28",
      "title": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
      "authors": [
        "P Wang",
        "S Wang",
        "J Lin",
        "S Bai",
        "X Zhou",
        "J Zhou",
        "X Wang",
        "C Zhou"
      ],
      "year": "2023",
      "venue": "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities",
      "arxiv": "arXiv:2305.11172[cs,eess]version:1"
    },
    {
      "citation_id": "29",
      "title": "LeRaC: Learning Rate Curriculum",
      "authors": [
        "F.-A Croitoru",
        "N.-C Ristea",
        "R Ionescu",
        "N Sebe"
      ],
      "year": "2022",
      "venue": "LeRaC: Learning Rate Curriculum",
      "arxiv": "arXiv:2205.09180[cs]version:2"
    },
    {
      "citation_id": "30",
      "title": "Audio Retrieval with WavText5K and CLAP Training",
      "authors": [
        "S Deshmukh",
        "B Elizalde",
        "H Wang"
      ],
      "year": "2022",
      "venue": "Audio Retrieval with WavText5K and CLAP Training",
      "arxiv": "arXiv:2209.14275"
    },
    {
      "citation_id": "31",
      "title": "On Metric Learning for Audio-Text Cross-Modal Retrieval",
      "authors": [
        "X Mei",
        "X Liu",
        "J Sun",
        "M Plumbley",
        "W Wang"
      ],
      "year": "2022",
      "venue": "On Metric Learning for Audio-Text Cross-Modal Retrieval",
      "arxiv": "arXiv:2203.15537"
    },
    {
      "citation_id": "32",
      "title": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
      "authors": [
        "S Chen",
        "H Li",
        "Q Wang",
        "Z Zhao",
        "M Sun",
        "X Zhu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset",
      "arxiv": "arXiv:2305.18500[cs,eess]version:1"
    },
    {
      "citation_id": "33",
      "title": "The ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "34",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "35",
      "title": "Multilingual Spoken Words Corpus",
      "authors": [
        "M Mazumder",
        "S Chitlangia",
        "C Banbury",
        "Y Kang",
        "J Ciro",
        "K Achorn",
        "D Galvez",
        "M Sabini",
        "P Mattson",
        "D Kanter",
        "G Diamos",
        "P Warden",
        "J Meyer",
        "V Reddi"
      ],
      "year": "2021",
      "venue": "Multilingual Spoken Words Corpus"
    },
    {
      "citation_id": "36",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "THE CMU ARCTIC SPEECH DATABASES",
      "authors": [
        "J Kominek",
        "A Black"
      ],
      "venue": "THE CMU ARCTIC SPEECH DATABASES"
    },
    {
      "citation_id": "39",
      "title": "Cambridge Dictionary | English Dictionary, Translations & Thesaurus",
      "year": "2023",
      "venue": "Cambridge Dictionary | English Dictionary, Translations & Thesaurus"
    },
    {
      "citation_id": "40",
      "title": "AudioCaps: Generating Captions for Audios in The Wild",
      "authors": [
        "C Kim",
        "B Kim",
        "H Lee",
        "G Kim"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North"
    },
    {
      "citation_id": "41",
      "title": "Tunebot",
      "year": "2023",
      "venue": "Tunebot"
    },
    {
      "citation_id": "42",
      "title": "VocalSketch: Vocally Imitating Audio Concepts",
      "authors": [
        "M Cartwright",
        "B Pardo"
      ],
      "year": "2015",
      "venue": "Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, ser. CHI '15",
      "doi": "10.1145/2702123.2702387"
    },
    {
      "citation_id": "43",
      "title": "Fine-grained Vocal Imitation Set",
      "authors": [
        "B Kim",
        "B Pardo"
      ],
      "year": "2019",
      "venue": "Fine-grained Vocal Imitation Set"
    },
    {
      "citation_id": "44",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "45",
      "title": "Clotho: an Audio Captioning Dataset",
      "authors": [
        "K Drossos",
        "S Lipping",
        "T Virtanen"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "46",
      "title": "VGGSound: A Large-scale Audio-Visual Dataset",
      "authors": [
        "H Chen",
        "W Xie",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "VGGSound: A Large-scale Audio-Visual Dataset",
      "arxiv": "arXiv:2004.14368"
    },
    {
      "citation_id": "47",
      "title": "ESC: Dataset for Environmental Sound Classification",
      "authors": [
        "K Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM international conference on Multimedia, ser. MM '15",
      "doi": "10.1145/2733373.2806390"
    },
    {
      "citation_id": "48",
      "title": "A Dataset and Taxonomy for Urban Sound Research",
      "authors": [
        "J Salamon",
        "C Jacoby",
        "J Bello"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia",
      "doi": "10.1145/2647868.2655045"
    },
    {
      "citation_id": "49",
      "title": "FSD50K: An Open Dataset of Human-Labeled Sound Events",
      "authors": [
        "E Fonseca",
        "X Favory",
        "J Pons",
        "F Font",
        "X Serra"
      ],
      "year": "2022",
      "venue": "FSD50K: An Open Dataset of Human-Labeled Sound Events",
      "arxiv": "arXiv:2010.00475"
    },
    {
      "citation_id": "50",
      "title": "MACS -Multi-Annotator Captioned Soundscapes",
      "authors": [
        "I Morato",
        "A Mesaros"
      ],
      "year": "2021",
      "venue": "MACS -Multi-Annotator Captioned Soundscapes"
    }
  ]
}