{
  "paper_id": "2310.18363v1",
  "title": "A Contextualized Real-Time Multimodal Emotion Recognition For Conversational Agents Using Graph Convolutional Networks In Reinforcement Learning",
  "published": "2023-10-24T14:31:17Z",
  "authors": [
    "Fathima Abdul Rahman",
    "Guang Lu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Owing to the recent developments in Generative Artificial Intelligence (GenAI) and Large Language Models (LLM), conversational agents are becoming increasingly popular and accepted. They provide a human touch by interacting in ways familiar to us and by providing support as virtual companions. Therefore, it is important to understand the user's emotions in order to respond considerately. Compared to the standard problem of emotion recognition, conversational agents face an additional constraint in that recognition must be realtime. Studies on model architectures using audio, visual, and textual modalities have mainly focused on emotion classification using full video sequences that do not provide online features. In this work, we present a novel paradigm for contextualized Emotion Recognition using Graph Convolutional Network with Reinforcement Learning (conER-GRL). Conversations are partitioned into smaller groups of utterances for effective extraction of contextual information. The system uses Gated Recurrent Units (GRU) to extract multimodal features from these groups of utterances. More importantly, Graph Convolutional Networks (GCN) and Reinforcement Learning (RL) agents are cascade trained to capture the complex dependencies of emotion features in interactive scenarios. Comparing the results of the conER-GRL model with other state-of-the-art models on the benchmark dataset IEMOCAP demonstrates the advantageous capabilities of the conER-GRL architecture in recognizing emotions in real-time from multimodal conversational signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Artificial Intelligence (AI)-driven conversational agents have already proven their worth in areas such as healthcare, customer service, and education by providing effective and personalized advice  [3] . These conversational agents have tremendous potential if they are aware of the emotions associated with utterances, which are an essential part of human interaction. Most models developed so far have been trained to capture information about conversational partner's emotion states from video datasets  [12] . Multimodal models allow us to capture the emotions expressed by conversational partners more effectively  [15] , i.e, visually by capturing facial features or features of the environment, aurally by capturing features of speech and sounds, and textually by capturing the information in the content of the conversation.\n\nAn important component of conversation is contextual information, which represents the effect of an earlier time window of utterances on the target utterance. Emotional states generally correlate well with contextual information  [19] . Consequently, the use of contextual information increases the accuracy of an emotion recognition model. Another feature to capture is the flow of emotions between conversational partners. The emotions corresponding to an utterance show dependence on the utterances of other conversational partners in addition to one's own  [9] . An emotion recognition model should be able to capture this inter-and intra-dependency of emotions between conversational partners to improve its performance. Finally, multimodal emotion recognition models should be able to capture the above features in an online setting to perform real-time emotion recognition. Many existing models operate in an offline environment i.e., they take the entire conversation upfront as input and use both past and future utterances to detect the current emotion  [9] . This limits their usefulness in conversational agents which need to perform real-time predictions for efficient and responsive dialogues.\n\nTo address these challenges we propose a novel architecture for contextualized Emotion Recognition using Graph Convolutional Networks with Reinforcement Learning (conER-GRL), which effectively captures the contextual information in a conversation as well as the dependency between utterances and achieves real-time performance in emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "We present some prominent approaches and techniques of emotion recognition in conversations that are relevant to our study.\n\nContextual Information: DialogueGCN  [8]  is a Graph Convolutional Network (GCN)-based approach that constructs a graph of the conversations and uses GCN to propagate information from these graphs to learn the contextual representation of the utterances. The ConSK-GCN architecture  [7]  leverages the inter-and intra-speaker dependency captured by GCN to model contextual information. Recently, COGMEN  [9]  used a transformer encoder for context extraction in conversations which also captures the global information that represents the impact of the underlying information on emotion development. ERLDK  [19]  is an architecture that supports real-time features and introduces the concept of Domain Knowledge (DK) to capture contextual information in conversations.\n\nDependency of Emotions on Conversational Partners: Representative models include DialogueGCN  [8] , ConSK-GCN  [7]  and COGMEN  [9] , all of which are based on graph architectures. These models use dependency between and within speakers to model conversational context by arXiv:2310.18363v1 [cs.CL] 24 Oct 2023 Fig.  1 . conER-GRL model architecture constructing a graph of the conversation in which nodes represent utterances and edges represent dependency between speakers in the conversation. The models then use GCN to propagate information over the graph to learn the dependency between utterances. For example, COGMEN  [9]  constructed graphs with directed edges and used Relational-GCN  [14]  to learn the direction of dependencies between speakers. These models show that GCNs are well suited for modeling conversational data.\n\nReal-Time Performance: Most of the above approaches cannot provide real-time emotion recognition in conversations. Reinforcement Learning (RL) is used in real-time models because it allows agents to dynamically learn from their trials and adapt to changes in the environment with or without human intervention.  [10]  used the REINFORCE  [17]  algorithm for its RL agent on audio-visual modalities to develop an online emotion recognition model. In the ERLDK architecture  [19] , the RL agent is based on the Dueling-DQN  [16]  algorithm. An attention model is used by  [1] , which use multimodal information for an attentional descriptor to capture the correlation between each word and image component in targeted tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Probability Module",
      "text": "The starting point of our proposed conER-GRL model is to calculate the emotion probability based on the given conversations. The model uses the concept of DK introduced by  [19]  for effectively extracting context in conversations. More specifically, emotion-pairs are defined, which represent the smallest unit of conversation comprising of utterances from both conversational partners. It is a fixed window size of past utterances used to identify the emotion of the target utterance. The DK for a given emotion-pair is the summarized information about the correlation between the emotion-pair and the emotion of the target utterance.\n\nIn this work, the size of the emotion-pair was experimented with different window sizes of utterances (2, 3, 4, and 5) to find the optimal size. The DK is extracted for each emotion-pair size. The emotion labels for an emotion-pair in original order is its corresponding label-pair. An example of a label-pair for emotion-pair size 3 might be: happy-excitedhappy, where the three labels represent the emotion labels of the utterances before the target utterance. The probability of occurrence of these label pairs is calculated to understand the occurrence of the emotion categories. This information is recorded as DK. Note that the emotion-pair size affects the DK and its impact on model performance. Short emotionpairs would not provide sufficient DK while long emotionpairs would introduce noise and not allow fair generalization of DK.\n\nIn mathematical form, if L denotes the label-pair, e denotes an emotion (e.g., happy, sad, frustrated, excited, angry, and neutral)  [4] , and N um(e|L) is the number of occurrences of the emotion e with the label-pair L preceding it, then P (e|L), which indicates the probability of occurrence of e with label-pair L preceding e, is calculated as follows:\n\nThe correlation between the emotion e and the label-pair L is denoted by C(e|L), which is calculated by applying the softmax function to P (e|L):",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Reinforcement Learning Module",
      "text": "The extracted DK is fed into the RL module. The sequential occurrence of utterances in a conversation is similar to the state transitions in RL. The reward function is created based on the selected action by the agent and the emotion state of the target utterance. The RL module uses the Dueling-DQN as a learning algorithm and consists of the following parts:\n\n1) Three bidirectional Gated Recurrent Units (GRU) take each modality (audio, video, and text) as input to capture contextual information in the unimodals. A fourth bidirectional multilayer GRU is used to capture the cross-modal contextual relationship after fusing all modalities. The state of the RL agent s(t) is formed by merging the features of the emotion-pair E pair and the target utterance T :\n\n2) The extracted contextual features are then passed as input for graph formation. The graph of the conversation is constructed with utterances as nodes and the directed relations between the speakers as edges, capturing the dependency of emotions on utterances between and within speakers. The direction represents the affect of speakers on emotions, which include the inter-relations (affect of the utterance of one speaker on another) and the intra-relations (affect of one's own utterance).\n\n3) The graph is then fed into a vanilla Relational-GCN  [14] , which is typically used to capture relationshipspecific transformations of neighboring nodes depending on the type and direction of edges in the graph.\n\nHere, it captures the dependency between and within speakers in neighboring utterances. 4) A Graph Transformer  [18]  is then used to extract the features of the nodes. 5) The output of the graph networks is fed into dense layers of neural networks for the dueling mechanism of the Dueling-DQN. The emotion labels that make the action space a(t), of the RL agent is given by:\n\nwhere the numbers in A represent happy, sad, neutral, angry, excited and frustrated, respectively. The Qnetwork for the Dueling-DQN is formed during model training. The output, q eval , is the correct probability for the chosen action (emotion):\n\nIn the dueling mechanism, the previous output q eval is updated using the reward function, R: R = r, q action = label(t) -r, otherwise\n\nwhere r is the reward value and label(t) is the correct emotion label for the target utterance T (t). The features of the next state s(t + 1) is denoted by Q ′ . The loss Loss(t) is computed as follows:\n\nq expect (s(t + 1), a(t + 1)) = R + γ max(q eval (s(t + 1), a(t + 1))\n\nLoss(t) = E[q expect (s(t + 1), a(t + 1)) -q eval (s(t + 1), a(t + 1))]",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Test Module",
      "text": "This module is applied during the testing phase. To initialize the RL environment in this phase, a conversation video is randomly sampled from the test dataset. Based on the selected emotion-pair size, the first label-pair of this conversation is set as the initialization state for the RL agent. This state is used to revise the predicted output by the RL module. The output corresponding to each utterance is recorded. Once we have the required number (emotionpair size) of predictions, the predicted label-pair is used as an index to search for corresponding probability in the DK to get the target emotion state. For this, the probabilities in equations 1 and 2 are simply added to revise the prediction outputs. This process is continued till the end of the conversation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Dataset And Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset: Iemocap",
      "text": "The IEMOCAP dataset  [2]  is one of the oldest datasets and widely used in emotion recognition models for conversations. This enacted dataset is annotated using 6 emotions: happy, sad, excited, frustration, anger and neutral. It has 151 videos in total, 2 actors enacting in each. When the videos are split into individual utterances, there are 7442 utterances. The dataset is available as split sets of training (5146 utterances), validation (664 utterances), and testing (1632 utterances) data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "The text, audio and video features are of size 100, 100 and 512 respectively. The bidirectional GRU has 512 layers and a drop rate of 0.3. The γ for the Dueling-DQN is set to 0.9 and the updated frequency of target Q net is 100. The learning rate is 0.00015 and the decay weight of the ADAM optimizer is set to 10e -5.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Test Setup And Hardware",
      "text": "All coding concerning this study are done in Python 3.7. Programming is done on a virtual machine with a NVIDIA Titan Xp GPU. PyTorch  [11]  is the machine learning framework used for training. PyTorch Geometric  [6]  is the library used to train the GCN. OpenSMILE  [5]  is used for audio feature extraction. SBERT  [13]  is used to extract text features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "Table  I  shows the results of the conER-GRL model on the IEMOCAP dataset using audio, visual and text modalities. It shows the overall F1-score and the F1-scores for each emotion class. The best performance is for emotion-pair size 3. Table I also compares the performance of the conER-GRL model with other baseline models. The proposed model outperforms the baseline models in all emotion classes and in overall performance. The proposed architecture achieves 4.42% points higher in F1-score over previous SOTA model. The F1-scores of the proposed model have a standard deviation of 0.623. As stated in  [19] , the emotion-pair size shows significant impact on the DK. The significance of the contextual information captured in the DK is ascertained by the improvement in the model's performance. The performance improves for sizes 2, 3, and considerably for 4 as well, but drops as size increases. This could be caused by noise in the contextual information in longer emotion-pairs. The affect of emotionpair size is insignificant on the emotion classes as the change in F1-scores stays consistent across the classes.\n\nThe impact of constructing graphs of conversations and using GCNs to capture the dependency relationships between conversational partners is also significantly, as previous baseline models were not able to do this. The improvement in performance can be easily correlated to the use of this approach in the model architecture.\n\nError Analysis: Fig.  2  shows the confusion matrix for the 6 emotion classes. Similar emotions such as happy and excited or angry and frustrated, have been misclassified due to similarity in expressions. This is also seen in previous models such as  [8]  and  [9] . But compared to their errors, conER-GRL shows improved performance in classifying similar emotion classes. Similarly, the neutral class is the most mislabeled emotion. This could be related to the higher proportion of utterances with the neutral label. When compared to baseline models, conER-GRL performs better at avoiding the data bias.\n\nDiscussion In this paper, we tried to address three broad Fig.  2 . Confusion Matrix challenges that characterize the research problem of multimodal emotion recognition, which are essentially contextual information, dependency of emotions between and within conversational partners, and real-time performance. Popular baseline approaches have addressed these challenges using different methods. In our approach, we adapted the concept of DK introduced in  [19]  for extracting contextual information more efficiently. Since the size of the emotionpairs affects the performance of the model, it is possible to dynamically choose the size of the emotion-pair to optimize the performance of the model, which needs further investigation. Our approach introduced graph-based neural networks to increase the performance by leveraging the graphical structure in conversations and used it with an RL agent to improve the performance of real-time emotion recognition model. This novel approach has not only improved real-time performance, but also rectified misclassification of similar emotion classes and misclassification caused by biases in the data.\n\nWhen applied to a conversational agent, the model can be further optimized to predict emotions of the user alone as the user is an invariant conversation partner. The model only needs to analyze the effect of the conversational agent's utterances on the user. In the graph construction, the relation from the user to the conversational agent can thus be ignored. This increases computational efficiency.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "A novel approach has been presented, where GCN is used with an RL agent for real-time emotion recognition using multimodal data. The proposed model conER-GRL has been tested on the IEMOCAP dataset. The conER-GRL results on the IEMOCAP dataset outperforms other state-of-the-art methods by 4.42% points in the F1-score. The analysis of the results show the relevance of each module in the conER-GRL architecture. It also points at possible improvements in the model to enhance performance. The selection of emotionpair size can be dynamic and the RL agent and the graph networks can be optimized further to capture minor shifts in emotions.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: conER-GRL model architecture",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the confusion matrix for",
      "page": 4
    },
    {
      "caption": "Figure 2: Confusion Matrix",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal attentive learning for real-time explainable emotion recognition in conversa-tions",
      "authors": [
        "B Arumugam",
        "S Bhattacharjee",
        "J Yuan"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Symposium on Circuits and Systems (ISCAS)"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Ai-enabled persuasive personal health assistant",
      "authors": [
        "I Donadello",
        "M Dragoni"
      ],
      "year": "2022",
      "venue": "Social Network Analysis and Mining"
    },
    {
      "citation_id": "4",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "5",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Fast graph representation learning with pytorch geometric",
      "authors": [
        "M Fey",
        "J Lenssen"
      ],
      "year": "2019",
      "venue": "Fast graph representation learning with pytorch geometric",
      "arxiv": "arXiv:1903.02428"
    },
    {
      "citation_id": "7",
      "title": "Context-and knowledge-aware graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Y Fu",
        "S Okada",
        "L Wang",
        "L Guo",
        "Y Song",
        "J Liu",
        "J Dang"
      ],
      "year": "2022",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "8",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "9",
      "title": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "Cogmen: Contextualized gnn based multimodal emotion recognition",
      "arxiv": "arXiv:2205.02455"
    },
    {
      "citation_id": "10",
      "title": "An active learning paradigm for online audio-visual emotion recognition",
      "authors": [
        "I Kansizoglou",
        "L Bampis",
        "A Gasteratos"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "13",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "arxiv": "arXiv:1908.10084"
    },
    {
      "citation_id": "14",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "M Schlichtkrull",
        "T Kipf",
        "P Bloem",
        "R Van Den",
        "I Berg",
        "M Titov",
        "Welling"
      ],
      "year": "2018",
      "venue": "The Semantic Web: 15th International Conference"
    },
    {
      "citation_id": "15",
      "title": "Multimodal conversational ai: A survey of datasets and approaches",
      "authors": [
        "A Sundar",
        "L Heck"
      ],
      "year": "2022",
      "venue": "Multimodal conversational ai: A survey of datasets and approaches",
      "arxiv": "arXiv:2205.06907"
    },
    {
      "citation_id": "16",
      "title": "Dueling network architectures for deep reinforcement learning",
      "authors": [
        "Z Wang",
        "T Schaul",
        "M Hessel",
        "H Hasselt",
        "M Lanctot",
        "N Freitas"
      ],
      "year": "2016",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "17",
      "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "authors": [
        "R Williams"
      ],
      "year": "1992",
      "venue": "Machine learning"
    },
    {
      "citation_id": "18",
      "title": "Graph transformer networks",
      "authors": [
        "S Yun",
        "M Jeong",
        "R Kim",
        "J Kang",
        "H Kim"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "19",
      "title": "Real-time video emotion recognition based on reinforcement learning and domain knowledge",
      "authors": [
        "K Zhang",
        "Y Li",
        "J Wang",
        "E Cambria",
        "X Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    }
  ]
}