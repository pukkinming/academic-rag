{
  "paper_id": "2411.10018v1",
  "title": "Once More, With Feeling: Measuring Emotion Of Acting Performances In Contemporary American Film",
  "published": "2024-11-15T07:53:02Z",
  "authors": [
    "Naitian Zhou",
    "David Bamman"
  ],
  "keywords": [
    "film",
    "performance",
    "computational film analysis",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Narrative film is a composition of writing, cinematography, editing, and performance. While much computational work has focused on the writing or visual style in film, we conduct in this paper a computational exploration of acting performance. Applying speech emotion recognition models and a variationist sociolinguistic analytical framework to a corpus of popular, contemporary American film, we find narrative structure, diachronic shifts, and genre-and dialogue-based constraints located in spoken performances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Film is rich in its supply of semiotic resources, communicating meaning from the interaction of language (encoded in a script), visuals (choices of composition, blocking, cinematography), sound and more. Much computational work has arisen to examine slices of this semiotic field, including measuring how gender stereotypes or plot arcs are reflected in dialogue  [1, 2, 3]  or how visual features like color variance and shot length constitute genre  [4, 5] . One critical area, however, that has been neglected in this study is the role of performance in creating meaning.\n\nAs Naremore  [6]  notes, film is a medium in which meaning is acted out; an acting performance provides a semiotic frame through which we can understand the events that unfold. Given the fixed text of a script, the rendering of the final performance is an interpretive process in which the actor, director and editor jointly imbue the words with additional meaning. In this view, the same line of dialogue exhibits variation in meaning when performed in distinct diegetic contexts. As one example, consider the following line in Knives Out (2019): \"I'm warning you. \"\n\nMuch of the film revolves around these three words, overheard in a conversation between the wealthy Harlan Thrombey and his grandson, Ransom. The line is uttered by multiple characters as the film unfolds: angrily shouted by Ransom, somberly recalled by the eavesdropper, and gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located within a single diegetic event, has great capacity for meaning-making in performance.\n\nWhen viewed in this light, we can apply the analytical framework from variationist sociolinguistics to better understand this space of performance. Given a fixed line of dialogue (equivalent to a linguistic variable), a performance entails a choice -a selection from the set of possible variants. It is this choice, and the meaning contained within, which we study.\n\nIn this work, we design computational models to explore this form of variation by considering the emotional range of performances in contemporary American film, exploring in particular the tension between what characters say and how they say it. As distinct from prior work in the computational humanities that has measured emotion from text alone  [3, 7] , we measure acted emotion from speech, allowing us to disentangle the emotion present in the script from the choices made in creating the performance.\n\nUsing a speech emotion recognition model, we construct a parallel dataset of spoken performances (utterances) aligned with the text of the words being spoken (dialogue phrases). This dataset allows us to isolate and examine how performances vary in meaning from their paralinguistic features in addition to the textual meaning of the screenplay. We use this dataset to carry out several case studies exploring variation in performance in American film:\n\n1. First, we carry out a structural analysis of emotion as performed over narrative time.\n\nDoing so allows us to characterize film as performance text, relating emotional performance to larger narrative structure. 2. Second, we study diachronic variation by comparing emotionality of films across release years, testing the degree to which performances have intensified over time (following Bordwell's theories of visual style  [8] ). 3. Finally, we examine the capacity for performance by constructing a novel measure of emotional range for an utterance-the space of possible emotions that can be performed.\n\nIn doing so, we demonstrate how both contextual (genre) and textual (dialogue) aspects of film can carry constraints and affordances on acting performance.\n\nIn this work, we use computational methods to survey how both textual and contextual variables inform and reflect the performances rendered on screen.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "In order to perform our analysis, we need to construct an aligned dataset of actor performances (utterances), the text of the words they speak (phrases), and the emotions in each utterance. We create a pipeline that takes as input a set of full-length movies, and outputs time-aligned transcriptions for utterances, their emotion labels, and groups of semantically similar phrases.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing Pipeline",
      "text": "We first construct a data pipeline to segment and transcribe utterances from movie dialogues. The pipeline takes as input a set of MP4 files, where each file is one digitized film. Our analysis takes place in the speech and text modalities, so we use ffmpeg to extract the audio track.\n\nWe use the pyannote 1  segmentation model to detect continuous, single-speaker speech segments. Then, we use faster-whisper 2  to transcribe each speech segment. Because pyannote speech segments are based on voice activity detection and silences, it can label extended, multisentence speech as a single segment. For our analysis, however, we are interested in utterances as a discursive unit. If a character makes an assessment, then poses a question, we would like to split these into two distinct utterances. As a middle ground between raw voice activity detection and segmenting discursive units, which requires complex conversational understanding, we perform a post-processing step where we further split speech segments by sentence boundaries derived from the transcriptions. Because whisper is an end-to-end model that does not produce fine-grained time alignments, we then use a speech-to-text fine-tuned wav2vec2  3  to perform word-level time alignment between the transcription and the audio, then split the audio based on sentence boundaries generated by syntok, 4  a fast, rule-based sentence segmenter.\n\nTo prevent the end credit sequences from interfering with the results, we detect when the end credits begin by performing optical character recognition (OCR) on the shots in a movie and identifying long continuous sequences of shots that contain large amounts of text. We trim the movie to the beginning of the end credits.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "To perform speech emotion recognition, we use a wav2vec2 large model without any taskspecific fine-tuning to extract audio features. Then, we train a classification head to perform seven-way emotion classification, based on the Ekman model  [9]  of six basic emotions (anger, disgust, happiness, sadness, fear, and surprise) and a neutral label. To train these models, we use the MELD dataset, which contains 1,000 sampled dialogues from the TV series Friends  [10] .\n\nWe experiment with two classification settings: an utterance-level model which makes predictions based on only the speech features of the input utterance and a conversation-level model which includes the speech features of both the input utterance and its surrounding utterances.\n\nIn both cases, we use a pretrained wav2vec2 model as a backbone model for generating vector representations of each utterance. Because wav2vec2 creates an embedding for each audio frame (roughly 20ms of speech), we follow prior work in computing utterance embeddings by averaging across all timestamps within an utterance  [11] . We compute embeddings from the attention activations at each layer of the wav2vec2 model instead of just taking the last-layer activations; prior work has shown that, for paralinguistic tasks such as emotion recognition, early-and intermediate-layer activations are more useful than later layers  [11, 12] . At the end of the embedding step, each utterance is represented by a set of 25 768-dimensional vectors.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Utterance-Level Emotion Recognition",
      "text": "We implement the utterance-level model from Pepino et al.  [11]  and match the reported performance. We first take a weighted average of layer activations for a given utterance; these weights are learned during training. Then, we apply a fully-connected classification head to produce a probability distribution over the seven emotion labels. Unlike the original paper, we do not use features from the initial convolutional layer of the pre-trained model; we use only the attention head activations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contextual Emotion Recognition",
      "text": "We also train an contextual model which uses a bidirectional LSTM to predict the emotion of utterances within the context of a conversation. To do so, we define conversations as groups of utterances where each occurs within 3 seconds of the next. In the MELD dataset, there are 1,478 conversations in the training split according to this criterion.\n\nFor each conversation, we predict the emotion labels of all utterances in the conversation by first passing weighted activations through the biLSTM before applying the classification head to each hidden state. As before, the weights of activations are learned during training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation",
      "text": "We expect the movie data to be similar in nature to the MELD dataset, since both consist of professionally produced and acted clips. However, to ensure that our models do not experience domain shift despite the greater range in release year and setting of the film corpus, we evaluate these models on the test split of the MELD dataset as well as a manually collected dataset consisting of 333 clips from a subset of 35 contemporary American films. Each clip was a conversation with at least 2 utterances, where conversations were identified with the same heuristic used to construct training data for the contextual model. This resulted in a final evaluation dataset of 2,157 utterances with emotion labels. The clips were labeled by two annotators: 51 clips were labeled by both annotators and 250 clips were labeled by a single annotator. The Krippendorff's ùõº between the two annotators was 0.334, and the Fleiss' ùúÖ was 0.333, which matches the agreement of the MELD dataset.\n\nTable  1  shows the evaluation results on the MELD and Movies datasets. The models perform comparably to each other, and comparably across evaluation datasets. This performance also approaches the state of the art on MELD for audio-only models. Because the performance of the contextual model is slightly higher, we use its inference outputs for our analysis.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Identifying Dialogue Phrase Groups",
      "text": "One powerful aspect of this dataset is that we align actor performances to the words that they speak. To account for variation in how highly semantically similar phrases can be realized, we cluster together phrases with high semantic similarity. We use the sentence-transformers library to compute sentence embeddings of utterances and cluster them with the Leiden community detection algorithm  [13] . Table  2  shows some examples of phrases that are grouped together. We expect the phrases in each group to have similar prior distributions of emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Analysis",
      "text": "The above pipeline measures the emotions performed in an utterance and ties each utterance to the text being spoken. We apply this methodology to a large corpus of contemporary, popular American films  [14]  in order to study the variation of emotion within and between them.\n\nOur corpus consists of the top-50 live-action, narrative films by U.S. box office from 1980-2022. We supplement these with films nominated for \"Best Picture\"-equivalent awards by one of six organizations in those years: Academy Awards, Golden Globes, British Academy of Film and Television Arts, Los Angeles Film Critics Association, National Board of Review, and National Society of Film Critics. We only include English-language films in this analysis, resulting in a total of 2,283 films.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Film As Performance Text",
      "text": "Plantinga  [15]  describes how emotionality can reflect narrative structure -emotionally-charged events can serve as catalyst to disrupt the expository \"stable state\" and set the narrative in motion. Much attention has been paid to characterizing narratives in literature and film in terms of emotionality using trajectories of sentiment  [16]  and emotion  [17, 3, 18, 19] ; these have focused on the emotion encoded in text. Audiences of movies, however, are not directly exposed to that text; their experience is mediated by the performance. In order to study this more directly, we turn our attention to characterizing narratives with emotion as performed.\n\nWe study the distribution of emotions in utterances over the course of a movie. How do the prevalence of emotions shift over narrative time? Similar to previous work on dialogue in screenplays, we ask if there are emotional regularities across films  [18] . We examine first the emotionality of utterances -the average probability that an utterance is not neutral -  before looking more closely at how specific emotions are distributed temporally. We plot the average probability of an emotion label for an utterance in intervals of 5 percent, expressed as a percentage of the full run-time of the film. Specific emotions are measured as proportions of the emotional labels, excluding the neutral label.\n\nWe find that the emotional trajectories of performances are, in fact, structured over narrative time. Figure  1a  shows that emotionality increases over narrative time. We examine also the trajectory of specific emotions across films (figs. 1b,1c,1d). We find that joyful performances follow a U-shaped curve, with a steep increase towards the end, as movies resolve. Like Hipson and Mohammad  [18] , we find that negative-valence emotions like sadness and anger decrease at the end. Further, anger peaks 85% into the film, reminiscent of a climax-resolution structure.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evolving Emotionality",
      "text": "Subscribing to a particular categorization of emotions can be restrictive; in the remainder of the paper, we explore emotional performance, but depart from analyzing specific emotional labels. First, we study performance at a less granular level, focusing on the concept of emotionality as the proportion of utterances with any emotion label.  5  We measure how emotionality has changed historically over the decades spanned by our corpus. Emotional shifts have been identified in English fiction books: Morin and Acerbi  [20]  find that the content of those stories have experienced a decline in emotional expression. Within cinema, David Bordwell has written about how shorter shot lengths and tighter framing serve to intensify the visual style in more recent films compared to earlier ones  [8] . We ask whether there is a similar shift in performance: is there an intensification of emotion that matches the visual intensification of film, or perhaps an emotional cooling in performance that matches the findings in English fiction? When we split the data by release year, we find a mild effect that earlier films have a higher proportion of emotional utterances compared to later ones, with emotionality hitting a minimum around 2010 (see Fig.  2 ). However, the question remains whether the emotional content is changing (as Morin and Acerbi find in literature) or if the style with which words are being uttered is changing.\n\nTo disentangle the effects of shifting content and shifting style, we consider the change in emotionality over the years within the semantically equivalent phrase groups. If it is indeed the writing, and not the performance, that drives this shift in emotionality, we should see little change within a phrase group. However, when we look at the 511 phrases that are used in all 43 years of the dataset, we find that a fixed-effects regression shows a slightly negative, statistically significant correlation between the year and emotionality even within phrase groups (ùëÖ 2 = 0.048, ùêπ (1, 21461), ùëù < 0.001).\n\nThough this result is seemingly at odds with Bordwell's finding that visual style intensifies, it is also possible that they are harmonious. In Hollywood film, the close-up shot has always been associated with emotional expression  [21] . Panovsky  [22]  writes that close-ups provide a rich \"field of action\" that affords nuanced acting performances. These visual performances, which are almost imperceptible if viewed from a natural distance, provide an alternative to the spoken word as a channel of expression. Comparing to the stage, Panovsky writes the spoken word makes a stronger impression \"if we are not permitted to count the hairs in Romeo's mustache. \" As cinema further grows into its medium, Bordwell finds that close-up shots have indeed grown tighter on the subject. With an increase in the capacity for more nuanced performance in the visual channel, the emotionality of the spoken word need not bear so strong a burden.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Measuring Emotional Range",
      "text": "Range is often said to be the mark of a great actor. Naremore writes about the importance of an actor splitting their character \"visibly into different aspects\", showing off emotional range  [6] . Kuleshov similarly stressed the actors must be able to create a full range of gestures to create complex meaning  [23] . Wilson  [24]  takes this a step further and argues that the hallmark of great acting is projecting a character into complex situations. In this section, we explore the limits of range through the constraints that genre and script impose on emotional performance.\n\nFor this analysis, we construct a general measure of emotional range across a set of utterances ùë¢ 1...ùëõ . We characterize each utterance ùë¢ ùëñ with a performance vector ùë£ ‚Éó ùëñ , which is a distribution over emotions, given by the predicted probability distribution from the speech emotion recognition model. This allows us to take a more nuanced view of performances as a mixture of emotions. We model the distribution from which the vectors ùë£ ‚Éó 1...ùëõ are drawn as a Dirichlet, and find the parameters which maximize the likelihood of the observed vectors. We define emotional range as the entropy of this distribution: a higher entropy means there is greater variance in the distribution of performances, and a lower entropy signals lower emotional range.\n\nOne criticism of the Ekman emotional model lies in its construct validity: seven discrete emotion labels may be insufficient to characterize the space of emotions. Ideally, we would model a continuous space of \"performance\". In our previous analyses, we use these emotion labels as an intermediate between that ideal on one end, and sentiment analysis on the other. Here, our measure of emotional range is agnostic to the meaning of specific emotion labels, and serves to demonstrate how emotion classification can be a useful proxy task through which we can analyze performance in a more continuous space.\n\nThrillers have the least range; family-friendly films have the most.  Wilson [24]  speculates that some genres, like some types of comedy, have less capacity for emotional range than others. Previous work has shown that emotional arcs are correlated with genre  [25] . We ask whether different genres are associated with different capacities for emotional performance.\n\nWe calculate the emotional range for each movie, and find the average score for each genre. Genre information comes from IMDB, and we exclude genres with fewer than 30 films in our dataset.  6  Figure  3  shows the average entropy across genres. Thrillers, biographies, and mysteries have the least emotional range; fantasy, musicals and family films rank highest. While it is 11.  4    difficult to attribute these results to a particular property of specific genres, these findings show that some genres have more constrained or consistent emotional registers than others.\n\nFunctional phrases have less capacity for emotional range. Naremore  [6]  references Goffman when theorizing about performance: actors draw on and play against the interactional norms with which we as audience are already familiar. We ask if this bears out in our data. Does the emotional range of dialogue phrases reflect their discursive properties?\n\nTo study this, we measure the emotional range in dialogue. Because we tie specific performances to the words that are spoken, we can identify instances across the corpus when a given phrase was uttered. We isolate the 2,656 phrase groups that are uttered at least 50 times across our dataset. For each phrase group, we calculate the emotional range of its utterances.\n\nTable  3  shows phrases with the highest and lowest entropies. By inspecting the phrases at either end of the spectrum, we find qualitative differences in the kinds of phrases that have higher and lower emotional range: the capacity for emotional variance reflects the discursive flexibility of the words being spoken. Phrases with low range are functional and generally part of highly directed interactions: most phrases are either yes-or-no questions or answers to them. Phrases with high emotional range, on the other hand, mostly have more open-ended, evaluative discursive functions. In these cases, the prosody or intonation of speech can easily lend color to the statement being made. \"You're alive\" can be said with joy or relief to a loved one, as Marty McFly to his mentor Doc in Back to the Future (1985), or with anger at the sight of an enemy, as Lord Norinaga greets Walker in Teenage Mutant Ninja Turtles III (1993).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Discussion And Limitations",
      "text": "With this work, we demonstrate that films can, and should, be studied as performance texts. We tie our findings to both film theory and other computational work on narratives. Here, we discuss some limitations of the current study.\n\nMeasuring emotions. We follow a vast body of previous work within natural language processing  [26, 27] , affective computing  [28, 29]  and computational literary studies  [25, 30]  in using Ekman's basic emotions. However, the validity of this model has been questioned  [31] . First, there are doubts about the ecological validity of emotion recognition, especially as most speech emotion recognition datasets contain acted emotion as opposed to natural emotion. We note that, unlike much affective computing work, we use emotion recognition models trained on acted speech to make inference on acted speech. The professionally-produced, acted speech in the MELD dataset is well-suited to our data, which is also professionally-produced and acted. Indeed, we find that performance is similar between MELD and our in-domain evaluation data.\n\nAnother criticism lies in the cultural relativity of emotion. Though Ekman argues that the basic emotions are universal, he acknowledges there may be cultural differences in the emotions elicited in a given context. It is reasonable to suppose that viewers' normative knowledge also influences the interpretation of these emotions. We focus on contemporary American film in both our analysis and training data, holding at least the intended cultural audience constant. Cultural variation in performance is a ripe area for future work, as cultural differences exist in not only the production and interpretation of emotion, but also in theories of acting.\n\nAside from these specific criticisms of the Ekman model, the low interannotator agreement in both our evaluation set as well as other datasets, including MELD, suggest that this model for emotion may remain too coarse to precisely describe the data. Work in both affective psychology and NLP have attempted to address this by using more fine-grained classes  [32, 33]  or a continuous spaces of emotion  [34, 33] . While we used the Ekman model due to the availability of training data as well as to provide comparison with previous studies of emotion narratives, alternative emotion models may prove useful in future work.\n\nA question of authorship. In cinema, the performance that audiences see on screen is co-created by the actor, the director, and the editor. Baron and Carnicke  [35]  describe the conventional wisdom within film analysis to be that cinematic performances are made in the cutting room. \"True\" acting happens on the stage. Though our work studies film as performance text, it does not disentangle the processes through which the performance is constructed. It is about the performance as viewed, but not about the choices made by actors as separate from the director or editor. Our work makes the point that performance carries meaning worth studying, and opens the door for future computational work that explores its authorial roots.\n\nEmbodied erformance. Finally, we examine only performance as enacted through speech. This is perhaps the modality that lies closest to the script, and allows us to apply a variationist approach to studying the relationship between performance and text, but of course performance includes not just speech but also gesture, posture, facial expression, and more. Visual description has been found to be more useful for aligning narrative events than dialogue  [36] , and quantitative analysis of theater performance has found narratively meaningful patterns in movement  [37] . Film is a multimodal medium that deserves analysis in all its modalities. We hope that our work examining film across the speech and text can serve as a basis for more work that examines performance as embodied visually.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we explore the relation between film as narrative text and as performance text. Using a novel parallel dataset of speech and text from popular contemporary American film, we develop computational methods to measure how emotional prevalence and emotional range vary by both textual factors of narrative time and dialogue, as well as contextual factors of release year and genre. We hope this work inspires further multimodal studies of performance in computational film analysis.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotionality increases, but specific emotions show non-linear trajectories over narrative time",
      "page": 6
    },
    {
      "caption": "Figure 1: a shows that emotionality increases over narrative time. We examine also the",
      "page": 6
    },
    {
      "caption": "Figure 2: Emotionality is higher in older films (95% bootstrap confidence interval).",
      "page": 7
    },
    {
      "caption": "Figure 2: ). However, the question remains whether the emotional",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the average entropy across genres. Thrillers, biographies, and mysteries",
      "page": 8
    },
    {
      "caption": "Figure 3: Relative emotional range for different film genres (95% bootstrap confidence intervals).",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Naitian Zhou*, David Bamman": "School of Information, UC Berkeley, USA"
        },
        {
          "Naitian Zhou*, David Bamman": "Abstract"
        },
        {
          "Naitian Zhou*, David Bamman": ""
        },
        {
          "Naitian Zhou*, David Bamman": "computational work has focused on the writing or visual style in film, we conduct"
        },
        {
          "Naitian Zhou*, David Bamman": ""
        },
        {
          "Naitian Zhou*, David Bamman": ""
        },
        {
          "Naitian Zhou*, David Bamman": ""
        },
        {
          "Naitian Zhou*, David Bamman": "performances."
        },
        {
          "Naitian Zhou*, David Bamman": "Keywords"
        },
        {
          "Naitian Zhou*, David Bamman": "film, performance, computational film analysis, speech emotion recognition"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "within a single diegetic event, has great capacity for meaning-making in performance."
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": ""
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "olinguistics to better understand this space of performance. Given a fixed line of dialogue"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "(equivalent to a linguistic variable), a performance entails a choice ‚Äî a selection from the set of"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "possible variants. It is this choice, and the meaning contained within, which we study."
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": ""
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "the emotional range of performances in contemporary American film, exploring in particular"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "the tension between what characters say and how they say it. As distinct from prior work in"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "the computational humanities that has measured emotion from text alone [3, 7], we measure"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "acted emotion from speech, allowing us to disentangle the emotion present in the script from"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "the choices made in creating the performance."
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": ""
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "formances (utterances) aligned with the text of"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "This dataset allows us to isolate and examine how performances vary in meaning from their"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "paralinguistic features in addition to the textual meaning of the screenplay. We use this dataset"
        },
        {
          "gleefully recounted by inspector Benoit Blanc upon solving the crime. Even a single line, located": "to carry out several case studies exploring variation in performance in American film:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "This dataset allows us to isolate and examine how performances vary in meaning from their"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "paralinguistic features in addition to the textual meaning of the screenplay. We use this dataset"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "to carry out several case studies exploring variation in performance in American film:"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "1.\nFirst, we carry out a structural analysis of emotion as performed over narrative time."
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "Doing so allows us to characterize film as performance text, relating emotional performance"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "to larger narrative structure."
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "2.\nSecond, we study diachronic variation by comparing emotionality of films across release"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "years, testing the degree to which performances have intensified over time (following"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "Bordwell‚Äôs theories of visual style [8])."
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "Finally, we examine the capacity for performance by constructing a novel measure of"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "emotional range for an utterance‚Äîthe space of possible emotions that can be performed."
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "In doing so, we demonstrate how both contextual (genre) and textual (dialogue) aspects"
        },
        {
          "formances (utterances) aligned with the text of\nthe words being spoken (dialogue phrases).": "of film can carry constraints and affordances on acting performance."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "We use the pyannote1 segmentation model": "segments. Then, we use faster-whisper2 to transcribe each speech segment. Because pyannote",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "",
          "to detect continuous, single-speaker speech": "speech segments are based on voice activity detection and silences, it can label extended, multi-"
        },
        {
          "We use the pyannote1 segmentation model": "sentence speech as a single segment. For our analysis, however, we are interested in utterances",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "as a discursive unit. If a character makes an assessment, then poses a question, we would like to",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "split these into two distinct utterances. As a middle ground between raw voice activity detection",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "and segmenting discursive units, which requires complex conversational understanding, we",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "perform a post-processing step where we further split speech segments by sentence boundaries",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "derived from the transcriptions. Because whisper is an end-to-end model that does not produce",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "fine-grained time alignments, we then use a speech-to-text fine-tuned wav2vec23 to perform",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "word-level time alignment between the transcription and the audio, then split the audio based",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "on sentence boundaries generated by syntok,4 a fast, rule-based sentence segmenter.",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "",
          "to detect continuous, single-speaker speech": "To prevent the end credit sequences from interfering with the results, we detect when the"
        },
        {
          "We use the pyannote1 segmentation model": "end credits begin by performing optical character recognition (OCR) on the shots in a movie",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "and identifying long continuous sequences of shots that contain large amounts of text. We trim",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "the movie to the beginning of the end credits.",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "2.2. Speech emotion recognition",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "",
          "to detect continuous, single-speaker speech": "To perform speech emotion recognition, we use a wav2vec2 large model without any task-"
        },
        {
          "We use the pyannote1 segmentation model": "specific fine-tuning to extract audio features. Then, we train a classification head to perform",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "seven-way emotion classification, based on the Ekman model [9] of six basic emotions (anger,",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "disgust, happiness, sadness, fear, and surprise) and a neutral label. To train these models, we",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "",
          "to detect continuous, single-speaker speech": "use the MELD dataset, which contains 1,000 sampled dialogues from the TV series Friends [10]."
        },
        {
          "We use the pyannote1 segmentation model": "",
          "to detect continuous, single-speaker speech": "We experiment with two classification settings: an utterance-level model which makes pre-"
        },
        {
          "We use the pyannote1 segmentation model": "dictions based on only the speech features of the input utterance and a conversation-level model",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "which includes the speech features of both the input utterance and its surrounding utterances.",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "",
          "to detect continuous, single-speaker speech": "In both cases, we use a pretrained wav2vec2 model as a backbone model for generating vector"
        },
        {
          "We use the pyannote1 segmentation model": "representations of each utterance. Because wav2vec2 creates an embedding for each audio",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "frame (roughly 20ms of speech), we follow prior work in computing utterance embeddings by",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "averaging across all timestamps within an utterance [11]. We compute embeddings from the",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "attention activations at each layer of the wav2vec2 model instead of just taking the last-layer",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "activations; prior work has shown that, for paralinguistic tasks such as emotion recognition,",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "early- and intermediate-layer activations are more useful than later layers [11, 12]. At the end",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "of the embedding step, each utterance is represented by a set of 25 768-dimensional vectors.",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "2.2.1. Utterance-level emotion recognition",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "We implement the utterance-level model from Pepino et al. [11] and match the reported per-",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "formance. We first take a weighted average of layer activations for a given utterance; these",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "1https://huggingface.co/pyannote/speaker-segmentation",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "2https://github.com/SYSTRAN/faster-whisper",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "3https://huggingface.co/facebook/wav2vec2-base-960h",
          "to detect continuous, single-speaker speech": ""
        },
        {
          "We use the pyannote1 segmentation model": "4https://github.com/fnl/syntok",
          "to detect continuous, single-speaker speech": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "the attention head activations."
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "2.2.2. Contextual emotion recognition"
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "We also train an contextual model which uses a bidirectional LSTM to predict the emotion of"
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "1,478 conversations in the training split according to this criterion."
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "to each hidden state. As before, the weights of activations are learned during training."
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "2.2.3. Evaluation"
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "We expect the movie data to be similar in nature to the MELD dataset, since both consist of"
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "heuristic used to construct"
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "0.333, which matches the agreement of the MELD dataset."
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "the contextual model is slightly higher, we use its inference outputs for our analysis."
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "Table 1"
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": ""
        },
        {
          "weights are learned during training. Then, we apply a fully-connected classification head to": "interval bounds."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.3.": "",
          "Identifying dialogue phrase groups": "One powerful aspect of this dataset is that we align actor performances to the words that they"
        },
        {
          "2.3.": "",
          "Identifying dialogue phrase groups": "speak. To account for variation in how highly semantically similar phrases can be realized, we"
        },
        {
          "2.3.": "",
          "Identifying dialogue phrase groups": "cluster together phrases with high semantic similarity. We use the sentence-transformers"
        },
        {
          "2.3.": "",
          "Identifying dialogue phrase groups": "library to compute sentence embeddings of utterances and cluster them with the Leiden com-"
        },
        {
          "2.3.": "",
          "Identifying dialogue phrase groups": "munity detection algorithm [13]. Table 2 shows some examples of phrases that are grouped"
        },
        {
          "2.3.": "together. We expect the phrases in each group to have similar prior distributions of emotion.",
          "Identifying dialogue phrase groups": ""
        },
        {
          "2.3.": "Table 2",
          "Identifying dialogue phrase groups": ""
        },
        {
          "2.3.": "Examples of utterances which are clustered into dialogue phrase groups.",
          "Identifying dialogue phrase groups": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Examples of utterances which are clustered into dialogue phrase groups.": ""
        },
        {
          "Examples of utterances which are clustered into dialogue phrase groups.": "‚ÄúLet‚Äôs go,"
        },
        {
          "Examples of utterances which are clustered into dialogue phrase groups.": ""
        },
        {
          "Examples of utterances which are clustered into dialogue phrase groups.": ""
        },
        {
          "Examples of utterances which are clustered into dialogue phrase groups.": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.50": ""
        },
        {
          "0.50": "0.48"
        },
        {
          "0.50": ""
        },
        {
          "0.50": "0.46"
        },
        {
          "0.50": ""
        },
        {
          "0.50": "0.44"
        },
        {
          "0.50": ""
        },
        {
          "0.50": "0.42"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": ""
        },
        {
          "(a) Emotionality": "40%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "Narrative Time",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "(d) Anger",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        },
        {
          "10%": "",
          "20%": "",
          "30%": "",
          "40%\n50%\n60%": "",
          "70%": "",
          "80%": "",
          "90% 100%": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.50": "0.48"
        },
        {
          "0.50": "0.46"
        },
        {
          "0.50": "0.44"
        },
        {
          "0.50": "0.42"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1980\n1990\n2000\n2010\n2020": "Release Year"
        },
        {
          "1980\n1990\n2000\n2010\n2020": ""
        },
        {
          "1980\n1990\n2000\n2010\n2020": "as the proportion of utterances with any emotion label.5 We measure how emotionality has"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "changed historically over the decades spanned by our corpus. Emotional shifts have been"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "identified in English fiction books: Morin and Acerbi [20] find that the content of those stories"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "have experienced a decline in emotional expression. Within cinema, David Bordwell has written"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "about how shorter shot lengths and tighter framing serve to intensify the visual style in more"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "recent films compared to earlier ones [8]. We ask whether there is a similar shift in performance:"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "is there an intensification of emotion that matches the visual intensification of film, or perhaps"
        },
        {
          "1980\n1990\n2000\n2010\n2020": ""
        },
        {
          "1980\n1990\n2000\n2010\n2020": "the data by release year, we find a mild effect\nthat earlier films have a"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "higher proportion of emotional utterances compared to later ones, with emotionality hitting"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "a minimum around 2010 (see Fig. 2). However, the question remains whether the emotional"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "is changing (as Morin and Acerbi find in literature) or if the style with which words are"
        },
        {
          "1980\n1990\n2000\n2010\n2020": ""
        },
        {
          "1980\n1990\n2000\n2010\n2020": "To disentangle the effects of shifting content and shifting style, we consider the change in"
        },
        {
          "1980\n1990\n2000\n2010\n2020": ""
        },
        {
          "1980\n1990\n2000\n2010\n2020": "the writing, and not the performance, that drives this shift in emotionality, we should see little"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "change within a phrase group. However, when we look at the 511 phrases that are used in"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "all 43 years of the dataset, we find that a fixed-effects regression shows a slightly negative,"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "statistically significant correlation between the year and emotionality even within phrase groups"
        },
        {
          "1980\n1990\n2000\n2010\n2020": ""
        },
        {
          "1980\n1990\n2000\n2010\n2020": "Though this result is seemingly at odds with Bordwell‚Äôs finding that visual style intensifies, it"
        },
        {
          "1980\n1990\n2000\n2010\n2020": "is also possible that they are harmonious. In Hollywood film, the close-up shot has always been"
        },
        {
          "1980\n1990\n2000\n2010\n2020": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "‚Äúfield of action‚Äù that affords nuanced acting performances. These visual performances, which"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "are almost imperceptible if viewed from a natural distance, provide an alternative to the spoken"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "word as a channel of expression. Comparing to the stage, Panovsky writes the spoken word"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "makes a stronger impression ‚Äúif we are not permitted to count the hairs in Romeo‚Äôs mustache.‚Äù"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "As cinema further grows into its medium, Bordwell finds that close-up shots have indeed grown"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "tighter on the subject. With an increase in the capacity for more nuanced performance in the"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "visual channel, the emotionality of the spoken word need not bear so strong a burden."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "3.3. Measuring emotional range"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "Range is often said to be the mark of a great actor. Naremore writes about the importance of an"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "actor splitting their character ‚Äúvisibly into different aspects‚Äù, showing off emotional range [6]."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "Kuleshov similarly stressed the actors must be able to create a full range of gestures to create"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "complex meaning [23]. Wilson [24] takes this a step further and argues that the hallmark of"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "great acting is projecting a character into complex situations. In this section, we explore the"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "limits of range through the constraints that genre and script impose on emotional performance."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "For this analysis, we construct a general measure of emotional range across a set of utterances"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "ùë¢1...ùëõ. We characterize each utterance ùë¢ùëñ with a performance vector ùë£‚Éó ùëñ, which is a distribution"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "over emotions, given by the predicted probability distribution from the speech emotion recog-"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "nition model. This allows us to take a more nuanced view of performances as a mixture of"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "emotions. We model the distribution from which the vectors ùë£‚Éó 1...ùëõ are drawn as a Dirichlet,"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "and find the parameters which maximize the likelihood of the observed vectors. We define"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "emotional range as the entropy of this distribution: a higher entropy means there is greater"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "variance in the distribution of performances, and a lower entropy signals lower emotional range."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "One criticism of the Ekman emotional model\nlies in its construct validity: seven discrete"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "emotion labels may be insufficient to characterize the space of emotions.\nIdeally, we would"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "model a continuous space of ‚Äúperformance‚Äù. In our previous analyses, we use these emotion"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "labels as an intermediate between that ideal on one end, and sentiment analysis on the other."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "Here, our measure of emotional range is agnostic to the meaning of specific emotion labels, and"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "serves to demonstrate how emotion classification can be a useful proxy task through which we"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "can analyze performance in a more continuous space."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "Wilson [24] specu-"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "Thrillers have the least range; family-friendly films have the most."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "lates that some genres, like some types of comedy, have less capacity for emotional range than"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "others. Previous work has shown that emotional arcs are correlated with genre [25]. We ask"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "whether different genres are associated with different capacities for emotional performance."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "We calculate the emotional range for each movie, and find the average score for each genre."
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "Genre information comes from IMDB, and we exclude genres with fewer than 30 films in our"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "dataset.6 Figure 3 shows the average entropy across genres. Thrillers, biographies, and mysteries"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "have the least emotional range; fantasy, musicals and family films rank highest. While it is"
        },
        {
          "associated with emotional expression [21]. Panovsky [22] writes that close-ups provide a rich": "6Three genres were excluded: Western (19 films), Documentary (3), and Animation (2)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Family": ""
        },
        {
          "Family": ""
        },
        {
          "Family": "Figure 3: Relative emotional range for different film genres (95% bootstrap confidence intervals)."
        },
        {
          "Family": "difficult to attribute these results to a particular property of specific genres, these findings show"
        },
        {
          "Family": "that some genres have more constrained or consistent emotional registers than others."
        },
        {
          "Family": ""
        },
        {
          "Family": "Functional phrases have less capacity for emotional range."
        },
        {
          "Family": "Goffman when theorizing about performance: actors draw on and play against the interactional"
        },
        {
          "Family": "norms with which we as audience are already familiar. We ask if this bears out in our data."
        },
        {
          "Family": "Does the emotional range of dialogue phrases reflect their discursive properties?"
        },
        {
          "Family": ""
        },
        {
          "Family": "mances to the words that are spoken, we can identify instances across the corpus when a given"
        },
        {
          "Family": "phrase was uttered. We isolate the 2,656 phrase groups that are uttered at least 50 times across"
        },
        {
          "Family": "our dataset. For each phrase group, we calculate the emotional range of its utterances."
        },
        {
          "Family": ""
        },
        {
          "Family": "either end of the spectrum, we find qualitative differences in the kinds of phrases that have"
        },
        {
          "Family": "higher and lower emotional range: the capacity for emotional variance reflects the discursive"
        },
        {
          "Family": "flexibility of the words being spoken. Phrases with low range are functional and generally"
        },
        {
          "Family": "part of highly directed interactions: most phrases are either yes-or-no questions or answers to"
        },
        {
          "Family": "them. Phrases with high emotional range, on the other hand, mostly have more open-ended,"
        },
        {
          "Family": "evaluative discursive functions. In these cases, the prosody or intonation of speech can easily"
        },
        {
          "Family": "lend color to the statement being made. ‚ÄúYou‚Äôre alive‚Äù can be said with joy or relief to a loved"
        },
        {
          "Family": "one, as Marty McFly to his mentor Doc in Back to the Future (1985), or with anger at the sight of"
        },
        {
          "Family": "an enemy, as Lord Norinaga greets Walker in Teenage Mutant Ninja Turtles III"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "sentative phrase from each group."
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "Low Emotional Range"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "Phrase"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúCould I ask you something?‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúThis is your captain speaking.‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúIs that okay?‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúCan I get something for you?‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúCan I get something to drink?‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúHey, what can I get you?‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúYou wanna come?‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúYeah, that‚Äôs good.‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúAny questions?‚Äù"
        },
        {
          "Dialogue phrase groups with the highest and lowest emotional range scores. The table shows a repre-": "‚ÄúThat‚Äôs correct.‚Äù"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "or a continuous spaces of emotion [34, 33]. While we used the Ekman model due to the"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "availability of training data as well as to provide comparison with previous studies of emotion"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "narratives, alternative emotion models may prove useful in future work."
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": ""
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "A question of authorship."
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "co-created by the actor,"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "conventional wisdom within film analysis to be that cinematic performances are made in the"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "cutting room. ‚ÄúTrue‚Äù acting happens on the stage. Though our work studies film as performance"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "text, it does not disentangle the processes through which the performance is constructed. It is"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "about the performance as viewed, but not about the choices made by actors as separate from the"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": ""
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "and opens the door for future computational work that explores its authorial roots."
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": ""
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "Embodied erformance."
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "This is perhaps the modality that lies closest to the script, and allows us to apply a variationist"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": ""
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "mance includes not just speech but also gesture, posture, facial expression, and more. Visual"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": ""
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "and quantitative analysis of theater performance has found narratively meaningful patterns in"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "movement [37]. Film is a multimodal medium that deserves analysis in all its modalities. We"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "hope that our work examining film across the speech and text can serve as a basis for more"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "work that examines performance as embodied visually."
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "5. Conclusion"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": ""
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": ""
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "we develop computational methods to measure how emotional prevalence and emotional range"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "vary by both textual factors of narrative time and dialogue, as well as contextual factors of"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "release year and genre. We hope this work inspires further multimodal studies of performance"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "in computational film analysis."
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "Acknowledgments"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "The research reported in this article was supported by funding from Mellon Foundation and the"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "National Science Foundation (IIS-1942591 and DGE-2146752). We thank Jacob Lusk and Lucy Li"
        },
        {
          "psychology and NLP have attempted to address this by using more fine-grained classes [32, 33]": "for insightful discussion and feedback."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] Y. Yu, Y. Hao, P. Dhillon, Unpacking Gender Stereotypes in Film Dialogue,\nin: F. Hopfgart-"
        },
        {
          "References": "ner, K. Jaidka, P. Mayr, J. Jose, J. Breitsohl (Eds.), Social Informatics, Springer International"
        },
        {
          "References": "Publishing, Cham, 2022, pp. 398‚Äì405. doi:10.1007/978-3-031-19097-1_26."
        },
        {
          "References": "[2] B. M. Schmidt,\nPlot arceology: A vector-space model of narrative structure,\nin: 2015"
        },
        {
          "References": "IEEE International Conference on Big Data (Big Data), 2015, pp. 1667‚Äì1672. doi:10.1109/"
        },
        {
          "References": "BigData.2015.7363937."
        },
        {
          "References": "[3] A. K. Kayhani, F. Meziane, R. Chiky, Movies Emotional Analysis Using Textual Contents,"
        },
        {
          "References": "in: E. M√©tais, F. Meziane, H. Horacek, P. Cimiano (Eds.), Natural Language Processing and"
        },
        {
          "References": "Information Systems, volume 12089, Springer International Publishing, Cham, 2020, pp."
        },
        {
          "References": "205‚Äì212. doi:10.1007/978-3-030-51310-8_19."
        },
        {
          "References": "[4] Z. Rasheed, Y. Sheikh, M. Shah, On the use of computable features for film classification,"
        },
        {
          "References": "IEEE Transactions on Circuits and Systems for Video Technology 15 (2005) 52‚Äì64. doi:10."
        },
        {
          "References": "1109/TCSVT.2004.839993."
        },
        {
          "References": "[5] T. Guha, N. Kumar, S. S. Narayanan, S. L. Smith, Computationally deconstructing movie"
        },
        {
          "References": "narratives: An informatics approach,\nin: 2015 IEEE International Conference on Acous-"
        },
        {
          "References": "tics, Speech and Signal Processing (ICASSP), 2015, pp. 2264‚Äì2268. doi:10.1109/ICASSP."
        },
        {
          "References": "2015.7178374."
        },
        {
          "References": "[6]\nJ. Naremore, Acting in the Cinema, University of California Press, 1988."
        },
        {
          "References": "[7] E. Kim, S. Pad√≥, R. Klinger,\nInvestigating the Relationship between Literary Genres"
        },
        {
          "References": "and Emotional Plot Development,\nin: Proceedings of the Joint SIGHUM Workshop on"
        },
        {
          "References": "Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Liter-"
        },
        {
          "References": "ature, Association for Computational Linguistics, Vancouver, Canada, 2017, pp. 17‚Äì26."
        },
        {
          "References": "doi:10.18653/v1/W17-2203."
        },
        {
          "References": "[8] D. Bordwell,\nIntensified Continuity Visual Style in Contemporary American Film, Film"
        },
        {
          "References": "Quarterly 55 (2002) 16‚Äì28. doi:10.1525/fq.2002.55.3.16."
        },
        {
          "References": "[9] P. Ekman, An argument for basic emotions, Cognition and Emotion 6 (1992) 169‚Äì200."
        },
        {
          "References": "doi:10.1080/02699939208411068."
        },
        {
          "References": "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, R. Mihalcea, MELD: A\n[10]"
        },
        {
          "References": "Multimodal Multi-Party Dataset\nfor Emotion Recognition in Conversations,\n2019."
        },
        {
          "References": "arXiv:1810.02508."
        },
        {
          "References": "[11] L. Pepino, P. Riera, L. Ferrer, Emotion Recognition from Speech Using Wav2vec 2.0 Em-"
        },
        {
          "References": "beddings, 2021. arXiv:2104.03502."
        },
        {
          "References": "[12]\nJ. Shor, S. Venugopalan, TRILLsson: Distilled Universal Paralinguistic Speech Representa-"
        },
        {
          "References": "Interspeech 2022, 2022, pp. 356‚Äì360. doi:10.21437/Interspeech.2022-118."
        },
        {
          "References": "arXiv:2203.00236."
        },
        {
          "References": "L. Waltman, N.\nJ.\nvan Eck,\nFrom Louvain to Leiden:\nGuarantee-\n[13] V. A. Traag,"
        },
        {
          "References": "5233. doi:10.1038/"
        },
        {
          "References": "s41598-019-41695-z."
        },
        {
          "References": "[14] D. Bamman, R. Samberg, R. J. So, N. Zhou, Measuring diversity in hollywood through"
        },
        {
          "References": "the large-scale computational analysis of film, Proceedings of the National Academy of"
        },
        {
          "References": "Sciences (2024)."
        },
        {
          "References": "[15] C. R. Plantinga, Moving Viewers: American Film and the Spectator‚Äôs Experience, University"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of California Press, Berkeley, 2009.": "[16] M. L. Jockers, Syuzhet: Extract sentiment and plot arcs from text (2015)."
        },
        {
          "of California Press, Berkeley, 2009.": "[17] A. J. Reagan, L. Mitchell, D. Kiley, C. M. Danforth, P. S. Dodds, The emotional arcs of"
        },
        {
          "of California Press, Berkeley, 2009.": "stories are dominated by six basic shapes, EPJ Data Science 5 (2016) 1‚Äì12. doi:10.1140/"
        },
        {
          "of California Press, Berkeley, 2009.": "epjds/s13688-016-0093-1."
        },
        {
          "of California Press, Berkeley, 2009.": "[18] W. E. Hipson, S. M. Mohammad, Emotion dynamics in movie dialogues, PLOS ONE 16"
        },
        {
          "of California Press, Berkeley, 2009.": "(2021) e0256153. doi:10.1371/journal.pone.0256153."
        },
        {
          "of California Press, Berkeley, 2009.": "[19] K. Vishnubhotla, A. Hammond, G. Hirst, S. M. Mohammad, The Emotion Dynamics of"
        },
        {
          "of California Press, Berkeley, 2009.": "Literary Novels, 2024. arXiv:2403.02474."
        },
        {
          "of California Press, Berkeley, 2009.": "[20] O. Morin, A. Acerbi, Birth of the cool: A two-centuries decline in emotional expression in"
        },
        {
          "of California Press, Berkeley, 2009.": "Anglophone fiction, Cognition & Emotion 31 (2017) 1663‚Äì1675. doi:10.1080/02699931."
        },
        {
          "of California Press, Berkeley, 2009.": "2016.1260528."
        },
        {
          "of California Press, Berkeley, 2009.": "[21] K. Pendlebury,\nCutting across the century: An investigation of\nthe close up and the"
        },
        {
          "of California Press, Berkeley, 2009.": "long-shot in ‚Äúcine choreography‚Äù since the invention of the camera, The International"
        },
        {
          "of California Press, Berkeley, 2009.": "Journal of Screendance 4 (2014). doi:10.18061/ijsd.v4i0.4527."
        },
        {
          "of California Press, Berkeley, 2009.": "[22] E. Panovsky, Style and medium in the moving pictures,\nin: Film, an Anthology / Compiled"
        },
        {
          "of California Press, Berkeley, 2009.": "and Edited by Daniel Talbot., Simon and Schuster, New York, 1959."
        },
        {
          "of California Press, Berkeley, 2009.": "[23] L. V. Kuleshov, Kuleshov on Film: Writings, Berkeley : University of California Press, 1974."
        },
        {
          "of California Press, Berkeley, 2009.": "[24] G. B. Wilson,\nLevels of Achievement in Acting,\nEducational Theatre Journal 3 (1951)"
        },
        {
          "of California Press, Berkeley, 2009.": "230‚Äì236. doi:10.2307/3204063. arXiv:3204063."
        },
        {
          "of California Press, Berkeley, 2009.": "[25]\nS. Samothrakis, M. Fasli, Emotional Sentence Annotation Helps Predict Fiction Genre,"
        },
        {
          "of California Press, Berkeley, 2009.": "PLOS ONE 10 (2015) e0141922. doi:10.1371/journal.pone.0141922."
        },
        {
          "of California Press, Berkeley, 2009.": "[26]\nJ. Zhao, T. Zhang, J. Hu, Y. Liu, Q. Jin, X. Wang, H. Li, M3ED: Multi-modal Multi-scene"
        },
        {
          "of California Press, Berkeley, 2009.": "Multi-label Emotional Dialogue Database,\nin: S. Muresan, P. Nakov, A. Villavicencio (Eds.),"
        },
        {
          "of California Press, Berkeley, 2009.": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
        },
        {
          "of California Press, Berkeley, 2009.": "(Volume 1: Long Papers), Association for Computational Linguistics, Dublin, Ireland, 2022,"
        },
        {
          "of California Press, Berkeley, 2009.": "pp. 5699‚Äì5710. doi:10.18653/v1/2022.acl-long.391."
        },
        {
          "of California Press, Berkeley, 2009.": "[27]\nS. M. Zahiri,\nJ. D. Choi, Emotion Detection on TV Show Transcripts with Sequence-"
        },
        {
          "of California Press, Berkeley, 2009.": "doi:10.48550/arXiv.1708.04299."
        },
        {
          "of California Press, Berkeley, 2009.": "arXiv:1708.04299."
        },
        {
          "of California Press, Berkeley, 2009.": "[28] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, S. S."
        },
        {
          "of California Press, Berkeley, 2009.": "Narayanan,\nIEMOCAP: Interactive emotional dyadic motion capture database, Language"
        },
        {
          "of California Press, Berkeley, 2009.": "Resources and Evaluation 42 (2008) 335‚Äì359. doi:10.1007/s10579-008-9076-6."
        },
        {
          "of California Press, Berkeley, 2009.": "[29] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, R. Verma, CREMA-D:"
        },
        {
          "of California Press, Berkeley, 2009.": "Crowd-sourced Emotional Multimodal Actors Dataset,\nIEEE transactions on affective"
        },
        {
          "of California Press, Berkeley, 2009.": "computing 5 (2014) 377‚Äì390. doi:10.1109/TAFFC.2014.2336244."
        },
        {
          "of California Press, Berkeley, 2009.": "[30] L. Barros, P. Rodriguez, A. Ortigosa,\nAutomatic Classification of Literature Pieces"
        },
        {
          "of California Press, Berkeley, 2009.": "by Emotion Detection: A Study on Quevedo‚Äôs Poetry,\nin:\n2013 Humaine Associa-"
        },
        {
          "of California Press, Berkeley, 2009.": "tion Conference on Affective Computing and Intelligent Interaction, 2013, pp. 141‚Äì146."
        },
        {
          "of California Press, Berkeley, 2009.": "doi:10.1109/ACII.2013.30."
        },
        {
          "of California Press, Berkeley, 2009.": "[31]\nF. M. Plaza-del-Arco, A. Curry, A. C. Curry, D. Hovy, Emotion Analysis in NLP: Trends,"
        },
        {
          "of California Press, Berkeley, 2009.": "Gaps and Roadmap for Future Directions, 2024. arXiv:2403.01222."
        },
        {
          "of California Press, Berkeley, 2009.": "[32] D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade, S. Ravi, GoEmotions: A"
        },
        {
          "of California Press, Berkeley, 2009.": "Dataset of Fine-Grained Emotions, 2020. arXiv:2005.00547."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "Dimensional Taxonomy of Emotional Experience and Expression, Psychological Science"
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "in the Public Interest 20 (2019) 69‚Äì90. doi:10.1177/1529100619850176."
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "[34]\nJ. A. Russell, A circumplex model of affect.,\nJournal of Personality and Social Psychology"
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "39 (1980) 1161‚Äì1178. doi:10.1037/h0077714."
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "[35] C. Baron, S. Carnicke, Reframing Screen Performance, University of Michigan Press, Ann"
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "Arbor, MI, 2008. doi:10.3998/mpub.104480."
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "[36]\nF. Zhou, F. Pianzola, Evaluation and alignment of movie events extracted via machine"
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "learning from a narratological perspective,\nin: 2023 Computational Humanities Research"
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "Conference, CHR 2023, CEUR Workshop Proceedings (CEUR-WS. org), 2023, pp. 49‚Äì62."
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "[37] M. Escobar Varela, G. O. F. Parikesit, A quantitative close analysis of a theatre video"
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "recording, Digital Scholarship in the Humanities 32 (2017) 276‚Äì283. doi:10.1093/llc/"
        },
        {
          "[33] A. Cowen, D. Sauter,\nJ. L. Tracy, D. Keltner, Mapping the Passions: Toward a High-": "fqv069."
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Unpacking Gender Stereotypes in Film Dialogue",
      "authors": [
        "Y Yu",
        "Y Hao",
        "P Dhillon"
      ],
      "year": "2022",
      "venue": "Social Informatics",
      "doi": "10.1007/978-3-031-19097-1_26"
    },
    {
      "citation_id": "2",
      "title": "Plot arceology: A vector-space model of narrative structure",
      "authors": [
        "B Schmidt"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Big Data (Big Data)",
      "doi": "10.1109/BigData.2015.7363937"
    },
    {
      "citation_id": "3",
      "title": "Natural Language Processing and Information Systems",
      "authors": [
        "A Kayhani",
        "F Meziane",
        "R Chiky"
      ],
      "year": "2020",
      "venue": "Natural Language Processing and Information Systems",
      "doi": "10.1007/978-3-030-51310-8_19"
    },
    {
      "citation_id": "4",
      "title": "On the use of computable features for film classification",
      "authors": [
        "Z Rasheed",
        "Y Sheikh",
        "M Shah"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology",
      "doi": "10.1109/TCSVT.2004.839993"
    },
    {
      "citation_id": "5",
      "title": "Computationally deconstructing movie narratives: An informatics approach",
      "authors": [
        "T Guha",
        "N Kumar",
        "S Narayanan",
        "S Smith"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2015.7178374"
    },
    {
      "citation_id": "6",
      "title": "Acting in the Cinema",
      "authors": [
        "J Naremore"
      ],
      "year": "1988",
      "venue": "Acting in the Cinema"
    },
    {
      "citation_id": "7",
      "title": "Investigating the Relationship between Literary Genres and Emotional Plot Development",
      "authors": [
        "E Kim",
        "S Pad√≥",
        "R Klinger"
      ],
      "year": "2017",
      "venue": "Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage",
      "doi": "10.18653/v1/W17-2203"
    },
    {
      "citation_id": "8",
      "title": "Intensified Continuity Visual Style in Contemporary American Film",
      "authors": [
        "D Bordwell"
      ],
      "year": "2002",
      "venue": "Intensified Continuity Visual Style in Contemporary American Film",
      "doi": "10.1525/fq.2002.55.3.16"
    },
    {
      "citation_id": "9",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "10",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "11",
      "title": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "12",
      "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "authors": [
        "J Shor",
        "S Venugopalan"
      ],
      "year": "2022",
      "venue": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "doi": "10.21437/Interspeech.2022-118",
      "arxiv": "arXiv:2203.00236"
    },
    {
      "citation_id": "13",
      "title": "From Louvain to Leiden: Guaranteeing well-connected communities",
      "authors": [
        "V Traag",
        "L Waltman",
        "N Van Eck"
      ],
      "year": "2019",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-019-41695-z"
    },
    {
      "citation_id": "14",
      "title": "Measuring diversity in hollywood through the large-scale computational analysis of film",
      "authors": [
        "D Bamman",
        "R Samberg",
        "R So",
        "N Zhou"
      ],
      "year": "2024",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "15",
      "title": "Moving Viewers: American Film and the Spectator's Experience",
      "authors": [
        "C Plantinga"
      ],
      "year": "2009",
      "venue": "Moving Viewers: American Film and the Spectator's Experience"
    },
    {
      "citation_id": "16",
      "title": "Syuzhet: Extract sentiment and plot arcs from text",
      "authors": [
        "M Jockers"
      ],
      "year": "2015",
      "venue": "Syuzhet: Extract sentiment and plot arcs from text"
    },
    {
      "citation_id": "17",
      "title": "The emotional arcs of stories are dominated by six basic shapes",
      "authors": [
        "A Reagan",
        "L Mitchell",
        "D Kiley",
        "C Danforth",
        "P Dodds"
      ],
      "year": "2016",
      "venue": "EPJ Data Science",
      "doi": "10.1140/epjds/s13688-016-0093-1"
    },
    {
      "citation_id": "18",
      "title": "Emotion dynamics in movie dialogues",
      "authors": [
        "W Hipson",
        "S Mohammad"
      ],
      "year": "2021",
      "venue": "Emotion dynamics in movie dialogues",
      "doi": "10.1371/journal.pone.0256153"
    },
    {
      "citation_id": "19",
      "title": "The Emotion Dynamics of Literary Novels",
      "authors": [
        "K Vishnubhotla",
        "A Hammond",
        "G Hirst",
        "S Mohammad"
      ],
      "year": "2024",
      "venue": "The Emotion Dynamics of Literary Novels",
      "arxiv": "arXiv:2403.02474"
    },
    {
      "citation_id": "20",
      "title": "Birth of the cool: A two-centuries decline in emotional expression in Anglophone fiction",
      "authors": [
        "O Morin",
        "A Acerbi"
      ],
      "year": "2017",
      "venue": "Cognition & Emotion",
      "doi": "10.1080/02699931.2016.1260528"
    },
    {
      "citation_id": "21",
      "title": "Cutting across the century: An investigation of the close up and the long-shot in \"cine choreography\" since the invention of the camera",
      "authors": [
        "K Pendlebury"
      ],
      "year": "2014",
      "venue": "The International Journal of Screendance",
      "doi": "10.18061/ijsd.v4i0.4527"
    },
    {
      "citation_id": "22",
      "title": "Style and medium in the moving pictures",
      "authors": [
        "E Panovsky"
      ],
      "year": "1959",
      "venue": "Film, an Anthology / Compiled"
    },
    {
      "citation_id": "23",
      "title": "Kuleshov on Film: Writings",
      "authors": [
        "L Kuleshov"
      ],
      "year": "1974",
      "venue": "Kuleshov on Film: Writings"
    },
    {
      "citation_id": "24",
      "title": "Levels of Achievement in Acting",
      "authors": [
        "G Wilson"
      ],
      "year": "1951",
      "venue": "Educational Theatre Journal",
      "doi": "10.2307/3204063"
    },
    {
      "citation_id": "25",
      "title": "Emotional Sentence Annotation Helps Predict Fiction Genre",
      "authors": [
        "S Samothrakis",
        "M Fasli"
      ],
      "year": "2015",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0141922"
    },
    {
      "citation_id": "26",
      "title": "M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2022.acl-long.391"
    },
    {
      "citation_id": "27",
      "title": "Emotion Detection on TV Show Transcripts with Sequencebased Convolutional Neural Networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2017",
      "venue": "Emotion Detection on TV Show Transcripts with Sequencebased Convolutional Neural Networks",
      "doi": "10.48550/arXiv.1708.04299",
      "arxiv": "arXiv:1708.04299"
    },
    {
      "citation_id": "28",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "29",
      "title": "CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "30",
      "title": "Automatic Classification of Literature Pieces by Emotion Detection: A Study on Quevedo's Poetry",
      "authors": [
        "L Barros",
        "P Rodriguez",
        "A Ortigosa"
      ],
      "year": "2013",
      "venue": "Humaine Association Conference on Affective Computing and Intelligent Interaction",
      "doi": "10.1109/ACII.2013.30"
    },
    {
      "citation_id": "31",
      "title": "Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions",
      "authors": [
        "F Plaza-Del-Arco",
        "A Curry",
        "A Curry",
        "D Hovy"
      ],
      "year": "2024",
      "venue": "Emotion Analysis in NLP: Trends, Gaps and Roadmap for Future Directions",
      "arxiv": "arXiv:2403.01222"
    },
    {
      "citation_id": "32",
      "title": "GoEmotions: A Dataset of Fine-Grained Emotions",
      "authors": [
        "D Demszky",
        "D Movshovitz-Attias",
        "J Ko",
        "A Cowen",
        "G Nemade",
        "S Ravi"
      ],
      "year": "2020",
      "venue": "GoEmotions: A Dataset of Fine-Grained Emotions",
      "arxiv": "arXiv:2005.00547"
    },
    {
      "citation_id": "33",
      "title": "Mapping the Passions: Toward a High-Dimensional Taxonomy of Emotional Experience and Expression",
      "authors": [
        "A Cowen",
        "D Sauter",
        "J Tracy",
        "D Keltner"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest",
      "doi": "10.1177/1529100619850176"
    },
    {
      "citation_id": "34",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "35",
      "title": "Reframing Screen Performance",
      "authors": [
        "C Baron",
        "S Carnicke"
      ],
      "year": "2008",
      "venue": "Reframing Screen Performance",
      "doi": "10.3998/mpub.104480"
    },
    {
      "citation_id": "36",
      "title": "Evaluation and alignment of movie events extracted via machine learning from a narratological perspective",
      "authors": [
        "F Zhou",
        "F Pianzola"
      ],
      "year": "2023",
      "venue": "2023 Computational Humanities Research Conference, CHR 2023, CEUR Workshop Proceedings (CEUR-WS. org)"
    },
    {
      "citation_id": "37",
      "title": "A quantitative close analysis of a theatre video recording",
      "authors": [
        "M Varela",
        "G Parikesit"
      ],
      "year": "2017",
      "venue": "Digital Scholarship in the Humanities",
      "doi": "10.1093/llc/fqv069"
    }
  ]
}