{
  "paper_id": "2503.06405v3",
  "title": "Heterogeneous Bimodal Attention Fusion For Speech Emotion Recognition",
  "published": "2025-03-09T02:50:49Z",
  "authors": [
    "Jiachen Luo",
    "Huy Phan",
    "Lin Wang",
    "Joshua Reiss"
  ],
  "keywords": [
    "attention",
    "multi-modal sentiment analysis",
    "conversational emotion recognition",
    "intra-modal interaction",
    "inter-modal interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-modal emotion recognition in conversations is a challenging problem due to the complex and complementary interactions between different modalities. Audio and textual cues are particularly important for understanding emotions from a human perspective. Most existing studies focus on exploring interactions between audio and text modalities at the same representation level. However, a critical issue is often overlooked: the heterogeneous modality gap between low-level audio representations and highlevel text representations. To address this problem, we propose a novel framework called Heterogeneous Bimodal Attention Fusion (HBAF) for multi-level multi-modal interaction in conversational emotion recognition. The proposed method comprises three key modules: the uni-modal representation module, the multi-modal fusion module, and the inter-modal contrastive learning module. The uni-modal representation module incorporates contextual content into low-level audio representations to bridge the heterogeneous multi-modal gap, enabling more effective fusion. The multi-modal fusion module uses dynamic bimodal attention and a dynamic gating mechanism to filter incorrect cross-modal relationships and fully exploit both intra-modal and inter-modal interactions. Finally, the inter-modal contrastive learning module captures complex absolute and relative interactions between audio and text modalities. Experiments on the MELD and IEMOCAP datasets demonstrate that the proposed HBAF method outperforms existing state-of-the-art baselines.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Conversational emotion recognition aims to understand human emotions and sentiment when interacting with one another during conversations and classify each utterance into its associated emotional state. Emotion recognition in conversations has found widespread applications in many tasks  (Singh and Goel, 2022; Cai et al., 2023; Ezzameli and Mahersia, 2023; Geetha et al., 2024) . However, automatic recognition of emotions is a challenging problem in real-world scenarios as human emotional expressions are often diverse in nature across individuals and cultures  (Singh and Mehr, 2023; Gong et al., 2023) .\n\nHumans convey emotions through various modalities, including speech, facial expressions, and body postures  (Ezzameli and Mahersia, 2023; Zhang et al., 2023) . Among these, speech stands out as a predominant, contact-free channel through which voice characteristics and linguistic content can be effectively communicated  (Sebe et al., 2005; Kessous et al., 2010) . It has been emphasized that an ideal model for conversational emotion recognition should integrate multi-modal information, as this approach better reflects human sensory systems  (Calvert and Thesen, 2004; Turk, 2014) . For example, as shown in Figure  1 , the short sentence 'You really like it?' is ambiguous and can convey positive, neutral, or negative emotions depending on the context. It is challenging to determine the associated emotion based solely textual utterance. However, when accompanied by low voices or sobbing sounds, the sentiment behind the short sentence becomes easier to interpret as negative. Dif-ferent modalities carry complementary information, and relying on a single modality is insufficient for accurate emotion recognition in real-world scenarios  (Soleymani et al., 2011; Liu et al., 2021; Zhang et al., 2024) . Therefore, combining information from multiple modalities provides richer, emotion-relevant insights by allowing modalities to complement or augment one another.\n\nThe heterogeneous gap and heterogeneous conflicts are key challenges in multi-modal emotion recognition  (Cai et al., 2023; Geetha et al., 2024) . The heterogeneous gap refers to differences in representation levels between modalities, such as the abstract and contextual nature of text features compared to the detailed, low-level patterns in audio features. These differences make aligning and integrating information from multiple modalities difficult, as they require careful consideration of how each modality's characteristics complement or contrast with others. Heterogeneous conflicts arise when features from different modalities are directly fused into a shared representation space without proper alignment, leading to noise and misinterpretation. Addressing these issues is critical for building effective models that can leverage the complementary strengths of multiple modalities to reliably recognize emotions.\n\nWhile previous methods have shown promise  (Cai et al., 2023; Geetha et al., 2024) , significant challenges remain, particularly in learning from multi-modal data for conversational emotion recognition  (Ezzameli and Mahersia, 2023) . Addressing the heterogeneous gap and heterogeneous conflicts requires (1) the presence of a modality gap between audio and text modalities, (2) the differing contributions of audio and text in bimodal speech emotion recognition, and (3) the existence of both unique and shared information between audio and text modalities. These observations lead to the identified problem of the heterogeneous modality gap and the hypothesis of leveraging intra-and inter-modal interactions. To address this, we propose the HBAF method for bimodal speech emotion recognition. The 'similarity' in the figure represents the alignment or similarity between audio and text modalities.\n\ndeveloping effective representations for each modality that can adapt to the variability of emotional expression across individuals. Moreover, capturing complex intra-and inter-modal interactions is critical for accurately recognizing emotional content. Contextual cues, especially in multi-speaker conversations, are essential and demand learning interactions across both low-level and high-level representations. However, most existing approaches focus on interactions at the same representation level  (Tsai et al., 2019; Liu et al., 2024) , overlooking the benefits of cross-level fusion. To overcome these limitations, we propose a novel audio context module that enriches lowlevel audio features with contextual information, enabling better alignment and interaction across modalities. By addressing the heterogeneous gap and conflicts, this approach provides a robust foundation for more effective and accurate multi-modal emotion recognition.\n\nBuilding on efforts to address the heterogeneous gap and conflicts, a key challenge is the effective modeling of dynamic intra-and inter-modal interactions across modalities with varying contributions  (Hazarika et al., 2018b; Majumder et al., 2019; Ezzameli and Mahersia, 2023) . These interactions are crucial for bridging representation gaps and ensuring seamless integration of multi-modal data. However, many previous methods simply concatenate data at the input level, neglecting intra-modal relationships and amplifying noise. This approach often assumes equal contributions from all modalities, which rarely reflects reality. Advancements such as HGraph-CL have attempted to address these issues through hierarchical graph contrastive learning  (Lin et al., 2022) , but they struggle to disentangle modality-specific features and capture shared patterns necessary for effective inter-modal integration. We propose that inter-modal contrastive learning offers significant potential to overcome these limitations, enabling better alignment, integration, and utilization of complementary features to improve the robustness and accuracy of multi-modal emotion recognition.\n\nMotivated by the above observations, we propose a Heterogeneous Bimodal Attention Fusion (HBAF) method that incorporates low-level audio representations to assist high-level text representations for bimodal emotion recognition. As shown in Figure  2 , the proposed method mainly consists of three modules, uni-modal representation, multi-modal fusion, and intermodal contrastive learning.\n\nIn daily communication, a person might say, \"I'm fine,\" with a sharp and irritated tone. While the text alone suggests a neutral or positive sentiment, the audio reveals frustration through tone, pitch, and speech rate. The HBAF method addresses this mismatch by enriching audio features with contextual information and dynamically adjusting attention to emphasize emotional cues from the audio while integrating the semantics of the text. This approach allows the model to accurately identify the underlying frustration, effectively resolving the heterogeneous gap in real-world scenarios. The main novelty of the proposed method is summarized as follows.\n\nFirst, we incorporate contextual information into low-level audio representations to project all modalities into a common shared subspace, aligning their contextual distribution to facilitate better fusion of heterogeneous modalities. Second, we design a multi-modal fusion module to discover comprehensive intra-and inter-modal interactions among different utterances across audio and text. Specifically, we rely on bimodal attention network, dynamic filter gate, and residual connection to train a multi-modal fusion network, enabling dynamic self-and cross-modal attentive weights to filter incorrect cross-modal interactions and exploit intra-and inter-modal relationships. Finally, we employ inter-modal contrastive learning module to model absolute and relative inter-modal content, encouraging the model to detect more complementary interactions between audio and text pairs.\n\nWe validate the proposed method experimentally and demonstrate that the paradigm for multi-modal emotion recognition primarily relies on supervised learning as the main method, with self-supervised learning serving as a complementary approach to enhance effectiveness and achieve a more comprehensive understanding of emotions.\n\nThe rest of this paper is organized as follows. Section 2 presents a brief literature review. Section 3 describes our method in detail. Section 4 illustrates experiments. Section 5 demonstrates results and discussion. Finally, Section 6 gives conclusions based on this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Representation",
      "text": "The quality of different modality information plays a decisive role in emotion recognition in conversations. To date, many studies have explored effective and efficient features in audio, textual, and visual modalities for multi-modal emotion recognition  (Calvo and D'Mello, 2010; Singh and Goel, 2022; Cai et al., 2023; Ezzameli and Mahersia, 2023) . Generally, audio feature engineering methods are roughly divided into two categories: prosodic and spectral features  (Koolagudi and Rao, 2012; Wani et al., 2021; Hashem et al., 2023) . Most of multimodal approaches use prosodic or hybrid features combining of prosodic and spectral features with traditional classifiers such as Hidden Markov Model  (Schuller et al., 2003) , and Support Vector Machines  (Jain et al., 2020) .\n\nBy contrast, due to the powerful representation capabilities of deep learning, some pre-trained audio models can automatically explore more informative features  (Liu et al., 2022; Zaman et al., 2023) . Pre-trained audio models, such as, VG-Gish  (Gemmeke et al., 2017) , openL3  (Cramer et al., 2019) , wav2vec  (Baevski et al., 2020) , data2vec  (Baevski et al., 2022) , etc., have accelerated the development of automatic speech recognition, voice conversation, and emotion recognition. For example,  Hung et al. employed  OpenL3 feature-informed embedding space regulrarization for audio classification  (Hung et al., 2019) .  Eunjeong et al. investigated  the effectiveness of L3-Net and VGGish deep audio embedding methods for music emotion inference over four music datasets  (Eunjeong and Jin Ha, 2021) .  Pepino et al.  have also explored the use of the wav2vec 2.0 model as a feature extractor for emotion recognition  (Pepino et al., 2021) .\n\nFor textual representation, much process has been made in learning embedding of individual words such as word2vec  (Mikolov et al., 2013) , GloVe  (Pennington et al., 2014)  and of phrases and sentences such as doc2vec  (Le and Mikolov, 2014) . More recently, there exists growing attention over the development of large unsupervised pre-trained language models  (Wang et al., 2023) . Pre-trained language models such as ELMo  (Peters et al., 2017) , GPT  (Radford et al., 2018)  and BERT  (Devlin et al., 2019) , have achieved high performance in various tasks by constructing contextual representation  (Deng and Ren, 2021; Al Maruf et al., 2024) . By pretraining on largescale unsupervised texts, these models enable learning linguistic representation related to the emotional states. In  (Acheampong et al., 2021) ,  Acheampong et al.  analyzed the efficacy of BERT, RoBERTa, DistilBERT, and XLNet pre-trained transformer models in recognizing emotions from text.\n\nCommonsense knowledge is essential for modeling the structure and flow of the dialogue, as well as the emotional dynamics of the participants  (Zhou et al., 2018a; Lin et al., 2020) . Commonsense knowledge is beneficial to emotion understanding. The commonsense transformer model COMET is commonly used to extract the commonsense features  (Bosselut et al., 2019) . A typical example in this line of work is Deepanway et al., who incorporated different elements of commonsense such as intra-and inter-speaker information and causal relations, and builds upon them to learn interactions between interlocutors participating for emotion identification in conversations  (Ghosal et al., 2020) .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "After feature extraction, there are two main multi-modal fusion strategies: feature fusion  (Majumder et al., 2019; Chudasama et al., 2022a)  and decision fusion  (Poria et al., 2017; Zhao et al., 2021) .  Poria et al. (Poria et al., 2018)  and Zhou et al.  (Zhou et al., 2018b ) leverage multi-modal information through concatenating features from different modalities without modeling the interaction between modalities. Lee et al. proposed weighted sum and weighted product rules for audio, text and video decision-level fusion, avoiding the difficulty of fusing heterogeneous information  (Lee et al., 2021) . But it is unable to capture interactions of different modalities.\n\nTo get better emotion recognition information, conversational emotion recognition requires deep understanding of human interactions in conversations  (Singh and Goel, 2022; Cai et al., 2023; Geetha et al., 2024) . However, there is no provision to model interactive influences. Additionally, conversational emotion recognition primarily focuses on employing deep-learning based algorithms to carry out contextual modeling within either a textual or multi-modal setting  (Mittal et al., 2020; Deng and Ren, 2021) . Numerous studies have emphasized that emotional dynamics are interactive in nature, rather than confined to individuals and following a unidirectional pattern  (Calvo and D'Mello, 2010; Singh and Goel, 2022; Cai et al., 2023) . These studies have attempted to capture such dynamics by examining transition properties.\n\nRecently, several studies employ memory networks to capture the intra-and inter-modal interactions in two-speaker conversations  (Hazarika et al., 2018a; Wang et al., 2024) . Majumder et al. proposed a RNN-based neural architecture that kept track of the intra-modal interaction throughout the conversation and used this information for emotion classification  (Majumder et al., 2019) . Hazarika et al. took the multimodal approach comprising audio, visual and textual features with greater recurrent units to model past utterances of each speaker into memories. Such memories were then merged using attention-based hops to capture inter-speaker dependencies  (Hazarika et al., 2018b) . Hazarika et al. designed interactive conversational memory network, a multi-modal emotion detection framework that extracted multi-modal features from conversational videos and hierarchically modeled the intra-and inter-speaker emotion influences into global memories. Such memories generated contextual summaries which aided in predicting the emotional orientation of utterance-videos  (Hazarika et al., 2018b) .\n\nBesides, Ren et al. introduced a cross-modal attention fusion module to capture cross-modal interactions of multi-modal information, and employed a conversational modeling module to explore the context information and speaker dependency of the whole conversation. Concretely, the cross-modal attention fusion module captured the cross-modal interactions and complementary information among the pre-extracted uni-modal features from acoustic, textual and visual modalities based on the cross-modal attention block. Afterward, the updated features from each modality were fused to concentrate more on the informative modality and achieve a refined feature for each constituent utterance. The conversational modeling module defined three different gated recurrent units with respect to the context information, the speaker dependency, and the emotional state of utterances  (Ren et al., 2021) . However, previous work did not pay significant attention to long-term contextual information, speaker-sensitive content and intent information  (Hazarika et al., 2018b; Majumder et al., 2019) . Due to its effectiveness, the transformer attracted some in the field for its ability to combine multi-modal features.\n\nXie et al. proposed a Transformer-based cross-modality fusion with the EmbraceNet architecture to combine all the representation vectors from each modality  (Xie et al., 2021) .  Li et al.  designed a new structure named Emoformer to extract multimodal emotion vectors from different modalities and fuse them with sentence vector to be an emotion capsule. Furthermore, they designed an end-to-end conversational emotion recognition model which extracted emotion vectors through the Emoformer structure and obtain the emotion classification results from a context analysis model  (Li et al., 2022) . Chudasama et al. designed a multi-modal fusion network that extracted emotion-relevant features from visual, audio and text modality  (Chudasama et al., 2022a) . It employed a multi-head attentionbased fusion mechanism to combine emotion-rich latent representations of the input data. But it ignored the mismatch between different modalities.\n\nExisting works on bimodal speech emotion recognition share some common limitations. First, most approaches do not adequately account for low-level audio representations, which can serve as important indicators of emotion  (Wani et al., 2021; Singh and Goel, 2022) . Second, since text tends to carry more information than audio  (Calvo and D'Mello, 2010; Cai et al., 2023) , many approaches emphasize the text while neglecting the relationship between audio and text. Third, previous works fail to fully leverage the associations across different levels of multiple modalities, limiting their ability to model mutual correlations and capture long-term contextual dependencies  (Ren et al., 2023) . Unfortunately, the existing approaches can not guarantee the cross-modality interaction at different levels, and often the learning of intra-and inter-modal will lose some semantic content. To address this challenge, we propose HBAF method to dynamically capture intra-and inter-modal interactions between low-level audio representation and high-level text representation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "Our goal is to infer the emotion of utterances presented in multi-turn and multi-speaker conversations. Emotion recognition in conversations task includes C emotion categories, whose set is\n\n, where a, l represent audio and textual modality, respectively. For each utterance\n\nAs shown in Fig.  2 , the proposed method HBAF is composed of three key modules: uni-modal representation module, multi-modal fusion module, and inter-modal contrastive learning module. We provide a detailed explanation of each component below.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Uni-Modal Representation Module",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Features Extraction And Context Network",
      "text": "Audio Features Extraction: For the audio modality, we use the openSMILE toolkit to extract 6373 dimensional features constituting several low-level descriptors and various statistical functionals of varied vocal and prosodic features (a o )  (Eyben et al., 2010) . Following the baseline work  (Poria et al., 2018) , we address the high dimensionality of the audio representation by employing SVMs for feature selection, resulting in a dense representation of the overall audio segment with a dimension of 512.\n\nAudio Context Network: To bridge the heterogeneous modality gap, we employ an audio contextual network to enhance low-level audio representations (H a ) with a dimension of 512. These low-level descriptors (LLDs), which include statistical features such as pitch, energy, and prosodic functionals, are lack temporal or sequential information. This makes them insufficient for capturing the contextual emotional patterns present in audio data. To address this limitation, the audio contextual network introduces contextualized audio information by modeling relationships across the entire audio sequence. This transformation enriches the LLDs with temporal and sequential context, making them more effective for emotion understanding.\n\nThe audio contextual network consists of a convolutional layer, two bidirectional long short-term memory (LSTM) layers, and three Transformer encoder layers (see Figure  3a ). Specifically, the convolutional layer has a kernel size of 3, a stride of 1, and 64 filters, designed to extract localized audio patterns. Each bidirectional LSTM layer comprises 256 units per direction, capturing sequential dependencies in both forward and backward directions. The Transformer encoder layers each have 8 attention heads and a hidden dimension of 512, enabling long-range contextual understanding through self-attention mechanisms. The convolutional layer captures localized audio patterns, such as changes in pitch or energy. The bidirectional LSTMs model sequential dependencies in both forward and backward directions, enabling the network to understand the temporal flow of audio signals. Finally, the Transformer layers provide long-range contextual understanding through self-attention, allowing the network to focus on key parts of the sequence relevant to emotion recognition. Together, these components transform the low-level LLDs into contextrich audio representations that significantly improve the contribution of the audio modality in multi-modal fusion tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Textual Features Extraction And Context Network",
      "text": "Textual Features Extraction: Previous studies have established that text is the primary modality in conversational emotion recognition  (Calvo and D'Mello, 2010; Singh and Goel, 2022; Cai et al., 2023) . However, incorporating a secondary modality alongside text is often challenging due to the inherent differences between modalities (see Figure  1 ). We aim to evaluate the ability of our proposed HBAF model to bridge and integrate information across different levels of abstraction, achieving our primary goal of exploring interactions between diverse modalities.\n\nSpecifically, the textual representation includes two types of features for each utterance: contextual features and commonsense features. Contextual features, extracted using RoBERTa, capture the semantic relationships and linguistic context within the text, enabling the model to understand nuances based on surrounding words and phrases  (Liu et al., 2019)   text representation with implicit understanding beyond the literal text  (Sap et al., 2019) . Together, these features form a comprehensive high-level text representation. RoBERTa excels at extracting contextual features due to its pretraining on large corpora using masked language modeling, which enables it to model word dependencies and contextual meanings. Similarly, COMET specializes in extracting commonsense features through its training on structured knowledge graphs, allowing it to infer hidden relationships and provide reasoning capabilities.\n\nFor contextual features, we use the pre-trained RoBERTa Large Model as our language model to extract contextual representations  (Liu et al., 2019) . Each utterance is first passed through the pre-trained RoBERTa model to obtain the outputs from the final four layers, which are then averaged to generate a independent context utterance feature vector (l r ) with a dimension of 1024. For commonsense features, we use the COMET model to extract commonsense knowledge for emotion prediction  (Sap et al., 2019) . Each utterance is processed by the COMET model to produce three distinct commonsense relations: external state (l e ), internal state (l i ), and purpose state (l p )  (Sap et al., 2019) . These three relations capture the external, internal, and purposeful states, each with a dimension of 1024, in dynamic conversations. The RoBERTa and COMET features are then passed through a linear layer to generate highlevel text features with a dimension of 512.\n\nTextual Context Network: To enhance the contribution of contextual information to the utterance at the time index t , we take into account the independent context utterance feature vectors of the previous utterances using soft attention. The resulting attentive contextual representation l c t is computed as follows:\n\n(2)\n\nwhere the parameters W s and b s are learnable weights and biases.\n\nWe combine the attentive contextual representation l c t with three different commonsense relations using gated recurrent units (GRUs) to model the external state (GRU e ), internal state (GRU i ), and purpose state (GRU p ) as follows (see Figure  3b ). Each GRUs comprises 512 units. External state GRU e aims to capture the participants' external state. At time step t, the external state is updated based on the previous external state l e t-1 and the attentive contextual representation l c t . The external state at time step t, l e t , can be computed as follows:\n\nInternal state GRU i models the internal state of the participants. The previous internal state of the person l i t-1 , and the attentive contextual representation l c t are the inputs to internal state GRU i at timestamp t:\n\nPurpose state GRU p carries out the purpose state of the participants. For time step t, the purpose state l p t is updated by taking into account the previous purpose state l p t-1 , and the attentive contextual representation l c t .\n\nFinally, we concatenate the external state l e , internal state l i and purpose state l p to form the high-level text representation (H l ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Modal Fusion Module",
      "text": "To effectively learn intra-and inter-modal interactions between low-level audio representation and high-level text representation, we design a multi-modal fusion module which mainly consists of bimodal attention network, dynamic filter gate and residual connection (see Figure  4 ).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Bimodal Attention Network",
      "text": "The bimodal attention network introduces parallel selfattention and cross-attention mechanisms to learn intra-and inter-modal interactions between low-level audio representation and high-level text representation. Specifically, self-attention operates within the same modality to capture intra-modal information. In this process, the target utterance acts as the query, while the contextual utterances serve as the key, enabling each utterance to determine how much information should be activated from the surrounding contextual utterances.\n\nWe estimate the associations within the same modality in a self-attentive manner using scaled dot-attention  (Vaswani et al., 2017) . The representations of modality m, denoted as H m where m represents either audio (a) or text (l), is projected into the query matrix (Q m ), key matrix (K m ), and value matrix (V m ) through linear projections without bias:\n\nwhere the query (Q m ), key (K m ), and value (V m ) can be interpreted as representations of modality m in different projection spaces. ζH m is the propagated information within modality m, m ∈ {a, l}.\n\nParallel to self-attention module, the cross-attention module is used to learn the inter-modal interactions between audio and text modalities. The cross-attention mechanism shares a similar principle with the self-attention mechanism. The only difference is that the query, key and value are from the different modality. The cross-attention module can be described as:\n\nwhere ζH a-l and ζH l-a represent the propagated information from audio to text and text to audio, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dynamic Filter Gate",
      "text": "Each modality contributes differently to the emotional expression of an utterance, necessitating a mechanism to balance and regulate their interactions during the multi-modal fusion process. To address this, we employ the dynamic filter gate to selectively filter out incorrect or irrelevant cross-modal relations, ensuring that the multi-modal fusion focuses on meaningful and complementary information. The dynamic filter gate operates as follows:\n\n• Gate Calculation: For each modality, a gating mechanism is defined to compute the importance of the modality's own information and the cross-modal information from the other modality:\n\nwhere the parameters W sa , W ca , and b a are learnable weights and biases used to model the contribution of selfmodal and cross-modal features. The sigmoid function ensures that the gate values are scaled between 0 and 1, controlling the contribution of each component.\n\n• Modality Refinement: Once the gate values (G a ) are computed, the representation of the modality H a * is refined as a weighted combination of its own features and cross-modal features:\n\nHere, the Hadamard product (⊙) is used to apply the gate values to the respective components, ensuring that the fusion process is dynamically weighted based on the gate outputs.\n\n• Cross-Modal Integration: The same process is applied to the complementary modality l:\n\nwhere the parameters W sa , W ca , W sl , W cl , b a , and b l are trainable, enabling the model to learn optimal weights for the contributions of self-modal and cross-modal features.\n\nThe dimensions of h a * and h l * are both 512.\n\nUnlike static fusion methods  (Hazarika et al., 2018b; Wang et al., 2022; Shou et al., 2024a) , the dynamic filter gate enables the model to adaptively weigh and combine modalityspecific and cross-modal features, ensuring robustness against noise and irrelevant information from any modality. The two parallel flows in Fig.  4  represent the processing of two distinct modalities (e.g., audio and text), with each flow preserving its modality-specific features while interacting with the other through cross-modal attention. This design allows the dynamic filter gate to selectively balance the contributions of self-modal features and cross-modal interactions for each modality.\n\nThe dynamic filter gate computes gating values for each modality using a sigmoid function, which evaluates the relative importance of self-modal and cross-modal features. It is described as \"dynamic\" because it adjusts these weights for each input, effectively filtering out noisy or irrelevant information and retaining only meaningful and complementary features during the fusion process. By explicitly modeling the dependencies between modalities, this mechanism refines their interactions, resulting in a more accurate, robust, and context-aware multimodal representation.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Residual Connection",
      "text": "To further enhance the representation capacity, we feed the dynamic filter gate features (H a * , H l * ) into a residual layer to obtain the final interactive information between the audio and text modalities. Specifically, the residual connection consists of a feed forward layer and a normalization layer.\n\nWe denote the final interactive information between the audio and text modality as h a and h l , respectively, where ⊕ represents the concatenation operation. The dimensions of h a and h l are both 512.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Inter-Modal Contrastive Learning Module",
      "text": "The traditional approach to contrastive learning relies on self-supervised contrastive learning  (Gutmann and Hyvärinen, 2010; Radford et al., 2021) . This method centers on the principle of pulling an anchor and its positive sample closer together in the feature space while pushing the anchor away from negative samples. By doing so, the model learns robust feature representations that effectively capture meaningful patterns, enhancing its ability to distinguish between similar and dissimilar data points.\n\nIn this work, we extend the concept of contrastive learning by introducing inter-modality contrastive learning, designed to capture the complex interactions between audio and text modalities (see Figure  5 ). Unlike traditional approaches that focus on a single modality, our method leverages both the individual characteristics of each modality and their combined representation. To achieve this, we employ two types of contrastive losses that operate on the encoded uni-modal representations (h a for audio and h l for text) and the multi-modal fusion representation (h m ) during training. These losses ensure that the fused multimodal representation effectively integrates information while preserving inter-modal relationships.\n\nThe first loss, Absolute Inter-Modal Loss, ensures alignment between the uni-modal and multi-modal representations, maintaining consistency across modalities. The second loss, Relative Inter-Modal Loss, captures the subtle dynamics within and between modalities, respecting their inherent differences while reinforcing their complementary nature. Together, these losses enable the model to learn both inter-modal and intra-modal relationships, enhancing its ability to process and interpret multimodal data. This novel framework represents a significant advancement, particularly in multi-modal emotion detection, by achieving a deeper understanding of the relationships between audio and text modalities. The proposed contrastive learning strategies are as follows:\n\nThe absolute inter-modal loss focuses on capturing the interaction between the encoded uni-modal representations (h a and h l ) and the encoded multi-modal fusion representation (h m ). In this setup, h a and h l are concatenated to form the multimodal representation h m , which serves as the anchor. The audio and text representations from the same sample act as its augmented versions. Each mini-batch is constructed with K samples, where each sample consists of an audio representation, a text representation, and a multi-modal fusion representation. For each anchor, a batch of randomly sampled pairs includes two positive pairs and 2K negative pairs. Positive pairs are formed by pairing the multi-modal fusion representation with its corresponding audio and text representations from the same sample. Negative pairs are generated by pairing the multi-modal fusion representation with the audio and text representations from different samples. In this case, the absolute inter-modality loss can be formulated as:\n\nwhere ⟨, ⟩ is cosine similarity. L a , m , and L l , m represent the absolute inter-modal loss of bimodal-audio and bimodal-text, respectively. τ refers to the temperature hyperparameters for scaling.\n\nThe relative inter-modal loss captures the interactions between the encoded audio (h a ) and text (h l )modalities. We consider the audio modality representation as the anchor and the text representation from the same sample as its augmented version. For each anchor, a batch of randomly sampled pairs includes one positive pair and K -1 negative pairs. Here, positive pairs include the audio representation paired with its corresponding text representation from the same sample. Negative pairs are formed by pairing the audio representation with text representation from different samples. In this case, the relative inter-modal loss can be formulated as: Finally, the inter-modal loss function is a weighted sum of absolute and relative inter-modal loss, which can be formulated as:\n\nwhere λ 1 , λ 2 , and λ 3 are hyperparameters to constrain the contributions of the different inter-modal contrastive loss.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Emotion Classification",
      "text": "The multi-modal representation h m is passed through a fully connected layer to predict the confidence score p c for the final emotional state. For the classification task, the standard crossentropy loss is defined as follows:\n\nwhere c represents the emotion categories, and y c indicates the presence based on the ground-truth label.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Training",
      "text": "To train the model, we combine the cross-entropy loss and inter-modal contrastive loss as the overall loss function:\n\nwhere µ is hyperparameter to constrain the contribution of the inter-modal contrastive loss.\n\nOur experiments indicated that the proposed HBAF model performed best when µ was set to 0.2. We demonstrate that the paradigm for multi-modal emotion recognition relies on supervised learning, supplemented by self-supervised learning, to effectively and comprehensively achieve emotion understanding. This approach aligns with trends in classification tasks, where supervised learning benefits from labeled data for precise predictions. Self-supervised learning complements this by extracting rich features from unlabeled data, which not only enhances model robustness but also mitigates potential errors introduced by noisy or inconsistent labels in supervised datasets. 4. Experiments",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dataset And Metrics",
      "text": "We evaluated our proposed HBAF method on the two benchmark datasets: MELD  (Poria et al., 2018)  and IEMOCAP  (Busso et al., 2008) . Both these two commonly used public datasets are multi-modal, containing audio, text and video modality for every utterance. Table  1  shows the distribution of training and test samples for both two datasets.\n\n• MELD is a multi-modal and multi-party dataset for conversational emotion recognition  (Poria et al., 2018) . It consists of 13,708 utterances in 1,433 dialogues collecting from the Friends TV shows. Each utterance is labeled with one in seven emotions: anger, joy, sadness, neutral, disgust, fear and surprise.\n\n• IEMOCAP contains videos of dyadic conversations of ten speakers, spanning 12.46 hours  (Busso et al., 2008) . Each utterance is annotated using the following discrete categories: happy, sad, neutral, angry, excited, and frustrated.\n\nDue to the natural imbalance across various emotions in the dataset, we evaluated the performance of multi-modal emotion recognition tasks using the weighted F1-score, which weights the F1-score for each class based on the proportion of its samples.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Baselines And State-Of-The-Art",
      "text": "In order to comprehensively evaluate the performance of proposed method, we compared the obtained performance with that of the Bidirectional Contextual LSTM (bc-LSTM) baseline  (Poria et al., 2018) . This baseline system leveraged an utterance-level LSTM to model context-aware utterance representations from the surrounding utterances. Furthermore, we also compared the proposed framework to various existing state-of-the-art methods. We ran their publicly available models on the two benchmark datasets. The experimental settings, model weights, and datasets were kept consistent across all methods.\n\nOn the MELD dataset, the following systems were used for comparison:\n\n• CMN  (Hazarika et al., 2018b)  leveraged two speakersensitive GRUs to learn contextual utterance combining dialogue history information. These contexts were served as memories to help the prediction of an incoming utterance.\n\n• DialogueRNN  (Majumder et al., 2019)  deployed three different GRUs to track speaker's states and sequential information in dialogues, and learn inter-modal interactions for conversational emotion recognition.\n\n• HGraph-CL  (Majumder et al., 2019)  used hierarchical graph contrastive learning to model intra-and inter-modal relations for precisely learning sentiment content.\n\n• AGF-IB  (Shou et al., 2024a)  introduced graph contrastive representation learning to capture intra-modal and intermodal complementary semantic information and learn intra-class and inter-class boundary information of emotion categories.\n\n• TelME1  (Yun et al., 2024)  incorporated cross-modal knowledge to transfer information from a language model (acting as the teacher) to non-verbal modalities (the students), thereby optimizing the performance of the weaker modalities.\n\nOn the IEMOCAP dataset, the following systems were used for comparison:\n\n• MMGCN  (Hu et al., 2021)  modeled multi-modal dependencies and learned inter-modal interaction for predicting emotions.\n\n• M2FNet  (Chudasama et al., 2022b ) employed a multihead attention-based fusion mechanism to combine emotion-rich latent representations of emotion-relevant features from visual, audio, and text modality and learn intra-and inter-modal relationships.\n\n• M2R2  (Wang et al., 2022)  utilized RNN to handle incomplete utterances by spreading dependency on all conversation parties.\n\n• Mamba  (Shou et al., 2024b ) designed a multi-modal fusion strategy based on probability guidance to maximize the consistency of information between modalities and learned intra-and inter-modal interactions for conversational emotion recognition.\n\n• TelME1  (Yun et al., 2024)  incorporated cross-modal knowledge transfer, where a language model (acting as the teacher) transfers information to non-verbal modalities (the students), thereby optimizing the performance of the weaker modalities.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Model Configuration",
      "text": "Following  (Luo et al., 2023) , we implemented our proposed HBAF method using the Pytorch 1.11.0 framework. The model was trained with Adam optimizer with an initial learning rate of 1e-4 and a batch size of 32. To mitigate overfitting, the network was regularized by L2 norm of the model's parameters with a weight of 3e-4. The model training was stopped if the validation loss did not decrease for 15 consecutive epochs.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results And Discussion",
      "text": "To verify the effectiveness of our proposed HBAF method, we conduct comparative experiments against previous state-ofthe-art baselines. Next, we present uni-modal and bimodal results, analyzing the contribution of each modality, particularly different level audio representation. Following that, we emphasize the importance of uni-modal representation module, multimodal fusion module and inter-modal contrastive learning module. Finally, we conduct case studies and error analysis.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Comparison With State-Of-The-Art Baselines",
      "text": "To verify the effectiveness of our proposed HBAF method, we present our comparative studies against state-of-the-art baseline systems (see Figure  6 ). HBAF demonstrates superior performance over state-of-the-art baseline systems in terms of weighted F1-score, achieving a 2.2% improvement over TelME1 on the MELD dataset and a 6.9% improvement over Mamba on the IEMOCAP dataset. These results demonstrate the superior expressive power and efficacy of incorporating low-level audio representation into the high-level text representation for bimodal speech emotion recognition. Notably, bimodal speech emotion recognition models consistently outperform most single-modality models. This improvement can be attributed to three factors: the consolidation of contextual information, the interaction within multi-modal fusion, and the quality of inter-modal contrastive learning. HBAF achieves an F1-score improvement of 2.3% and 5.1% over text-based unimodal models on the MELD and IEMOCAP datasets, respectively, when compared to our proposed HBAF model relying solely on textual input. This highlights the significant advantage of integrating multi-modal features for enhanced performance.\n\nIt is worth noting that the performance gains on the MELD dataset are more subtle compared to the IEMOCAP dataset. HBAF shows the strongest ability to infer emotions like joy, anger, and sadness in the MELD dataset, likely because these emotions are explicitly expressed through both audio and text. However, performance on fear is lower across most models, possibly due to its implicit expression and the limited number of samples.  Upon further analysis, we observed that the dialogues in the MELD dataset are relatively shorter (mostly 5 to 9 utterances) compared to those in the IEMOCAP dataset, which average around 70 utterances per dialogue. Additionally, the MELD dataset is based on real-world scenarios, which contain substantial background noise (e.g., honking, barking). This noise may be introduced into the model, particularly for emotions like fear and frustration, which can be easily masked by noise. Consequently, Our proposed HBAF method achieves better performance on the IEMOCAP dataset.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Importance Of Modalities",
      "text": "We examined the importance of each modality in this section. We chose audio, text, or bi-modality as the input for conversational emotion recognition. Specifically, for the audio modality, we fed different low-level and high-level audio representation into HBAF framework for emotion prediction. We show all uni-modal and bimodal results in Figure  6  and Table  2 .\n\nFor uni-modal average weighted F1-score results (see Figure  6 ), it should be highlighted that text modality significantly outperforms audio modality on MELD by 23.6% (HBAF) to 30.9% (HGraph-CL) and IEMOCAP by 6.2% (HBAF) to 11.2% (bc-LSTM). This is plausible for at least two reasons: 1) acoustic information may sometimes confuse emotion recognition task. For instance: \"angry\" and \"joy\" may have similar acoustic performances (high volume and pitch) even though they belong to opposite sentiments; and 2) audio signal has a lot of background noise. Thus, the essential problem is how to find the best audio representation due to the complex and variation of audio features. What's more, the word-level lexical features contains a wealth of information, such as topic, intent, instance, and so on. The content is important to release the nature and flow of the emotional dynamics of participants in conversations.\n\nWe hypothesize that our proposed HBAF method effectively bridges the heterogeneous modality gap between different levels of modality representation by leveraging its key components, particularly the Audio Context Network (ACN). The ACN plays a vital role in aligning audio features with other modalities, ensuring smoother integration and improving the overall fusion process. To verify the effectiveness of HBAF, we maintained the same high-level text representation and introduced different levels of audio representation (see Table  2 ). Specifically, we explored various audio representations, including handcrafted feature sets (e.g., LLDs  (Eyben et al., 2010) ), spectral features (e.g., MFCCs  (Davis and Mermelstein, 1980) ), and pre-trained audio representations (e.g., VGGish, openL3, wav2vec, data2vec)  (Liu et al., 2022) . We find that the audio context module is effective for low-level audio representation such as LLDs and MFCC, but not for high-level representation like VGGish, openL3, wav2vec, data2vec (see Table2). We suspect this is because the low-level audio representation lack contextual content, whereas high-level audio representation already contain contextual content obtained through deep pre-trained models.\n\nHowever, emotion recognition in conversations sometimes is contextual dependent to its audio, rather than text, and relies on features like voice tone, pitch, speed, and other audio information to infer what the emotion of a participant in conversations. For example, a single word \"okay\" is uncertainty and ambiguous, and it can express a variety of emotions in different situations. After fusing corresponding audio features, it is not difficult to infer the sentiment of this short utterance is negative. In general, integrating information from multiple modalities tends to achieve better performance than those under single-modal collection.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Ablation Study",
      "text": "To validate the effectiveness and reasonableness of each component of our proposed HBAF method, we carefully ablated the model's audio context network, multi-modal fusion module, and inter-modal contrastive learning module (see Figure  6 ). We observe the full version of HBAF achieves the best performance on the MELD and IEMOCAP datasets. We place particular emphasis on the removal of either the audio contextual network, multi-modal fusion module, or inter-modal contrastive learning module, each of which adversely affected the model's results. This indicates that the multi-modal fusion module comprising of bimodal attention network, dynamic filter gate and residual connection is crucial for fully learning intra-and inter-modal interaction for bimodal speech emotion recognition.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Effect Of The Audio Context Network",
      "text": "We observe that the absence of the audio contextual module (ACN) for low-level audio representations causes a noticeable performance decrease (see Table  2 ). Specifically, on the MELD dataset, the performance drops by 1.4% for LLDs and 1.1% for MFCCs, while on the IEMOCAP dataset, the performance decreases by 1.6% for LLDs and 1.3% for MFCCs. This highlights the critical role of the ACN in enriching lowlevel audio features by incorporating contextual information, which substantially enhances the model's performance across both datasets. However, the effect of the audio context network on high-level audio representation is very weak and may even have negative side effects. This result can be explained by the fact that the audio contextual network introduces contextual information to low-level audio representation learning, promoting semantic alignment for bridging heterogeneous modality gap.\n\nBy leveraging different level audio and textual representations, recent fusion models are geared toward directly modeling intra-and inter-modal interactions  (Hazarika et al., 2018b; Majumder et al., 2019; Shou et al., 2024b; Yun et al., 2024) . However, such approaches suffer from a heterogeneous modality gap in multi-modal fusion interactions, particularly concerning the information discrepancy between low-level and high-level unimodal representations. This is because audio and text modalities may inevitably map into different semantic spaces when they are input into different pre-trained models to generate unimodal representations. Specifically, a sequence of words is processed by a transformer-based model to produce high-level text representation. In contrast, raw audio data is conducted simple statistical analysis to generate low-level audio representation.\n\nWe contend that directly fusing audio and text representations at different levels into a common multi-modal embedding space is suboptimal, as it may lead to heterogeneous conflicts during the fusion stage. In other words, the transformer-based model imparts contextual information to the text modality using both linear and nonlinear structures, but the low-level audio representation do not undergo these operations. However, contextual information is a crucial factor influencing emotion, and when context is incorporated into uni-modal representations, we can infer more emotional states. To bridge this heterogeneous gap, a audio context module comprising of a onedimensional convolutional layer, two bidirectional long shortterm memory layers, and three transformer encoder layers unleashes the full contextual power of complex feature intercorrelations by aligning multi-modal interactions. These observations underscore the necessity and effectiveness of exploring context semantic alignment before fusing multi-modal representations.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Effect Of The Multi-Modal Fusion Module",
      "text": "The multi-modal fusion module is designed to leverage the dynamic attention weight to learn intra-and inter-modal interactions between the low-level audio representation and the high-level text representation. As shown in Figure  5 , the multimodal fusion module contributes to a performance improvement of 3.6% on the MELD dataset and 6.6% on the IEMOCAP dataset, compared to our proposed HBAF model without the multi-modal fusion module. Specifically, the performance decline follows a consistent trend when different components are removed from the model: bimodal attention network > intermodal contrastive learning module > dynamic filter network > residual network (see Figure  5 ). Combining information from self-and cross-attention is intuitively beneficial, as they complement and enhance each other, thus providing richer emotionrelevant information for bimodal speech emotion recognition.\n\nIn the ablation study of the multi-modal fusion module, we attribute the performance improvement to the multi-modal fusion module's explicit modeling of intra-and inter-modal interactions within and between audio and text, as well as its ability to capture long-term contextual information. By introducing dynamic self-and cross-modal attentive weights to uni-modal features, we enhance the uni-modal representations to build robust and effective multi-modal representation. We speculate that the multi-modal fusion module can leverage heterogeneous knowledge in a new high-dimensional space to capture more detailed information embedded in each modality, while adaptively fusing implicit complementary content to enhance interactions and correlations. This multi-modal fusion module improves performance by encouraging the proposed model to explore complementary information and interactions between the audio and text modalities during the dynamic interaction process.\n\nWe also observe improved performance from the proposed model when utilizing the dynamic filter gate. The dynamic filter gate filters information generated by incorrect cross-modal interactions, selectively removing or adding data to refine the complete emotion-relevant representation of multi-modal intercorrelations. Additionally, the residual connection offers an ef-   ficient method to explore heterogeneous spaces, learn modalityspecific information, and facilitate synchronization.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Effect Of The Inter-Modal Contrastive Learning Module",
      "text": "Inter-modal contrastive learning aims to fully explore intermodal interactions.When the inter-modal contrastive learning module is removed, the performance of HBAF drops by 2.9% on the MELD dataset and 5.4% on the IEMOCAP dataset, compared to the full model with the module included. This suggests that the inter-modal contrastive learning module facilitates emotional interactions between modalities, leading to improved emotion recognition. Multi-modal interaction based on contrastive learning enables uni-modal interactions to sufficiently capture multi-modal complementary. This is because supervised models may inevitably lose some distinct characteristics of each modality during multi-modal fusion. Under the guidance of self-supervised signals, inter-modal contrastive learning module leverages both absolute and relative inter-modal interactions to fully exploit inter-modal characteristics and enhance different level modality interaction.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Case Studies",
      "text": "To qualitatively validate the effectiveness of our proposed HBAF, we visualize several typical examples of MELD and IEMOCAP in Figure  7 , respectively. Specially, we compare the difference between audio, text and bimodal for conversational emotion recognition. HBAF provides closer scores to the ground truths than uni-modal audio and text, owing to fully capture intra-and inter-modal interactions between audio and text modality. The cases indicate that HBAF can effectively integrate non-verbal modalities with verbal modality. Combining information from audio and text modalities provides richer, emotion-relevant insights by allowing modalities to complement or augment one another. In addition, we also analyze the wrong examples to indicate the shortcomings of single modal for emotion prediction. As shown in Figure  6 , HBAF achieves incorrect classification results on cases 5. We analyze the content of these cases. For example, the single word 'Um' is ambiguous and can convey positive, neutral, or negative emotions depending on the context. It is challenging to determine the associated emotion based solely textual utterance. The text modality of the case 6 misclassify anger as neutral. Therefore, the bias of the text modality leads to wrong classification results. However, audio modality contains high pitch and tense, shaky voice, the sentiment behind the single word becomes easier to interpret as negative.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Error Analysis",
      "text": "We visualize the confusion matrices of the test set in Figure  8 . For the MELD dataset, our proposed HBAF method performs well across most emotion categories, including neutral, joy, sadness, and surprise. However, emotions like disgust and fear are under-represented in MELD. For the IEMOCAP dataset, HBAF excels in recognizing sadness, excitement, and frustration. We also note that the neutral emotion can be confused with excitement.\n\nBy inferring pre-defined emotions in conversations, we demonstrates that the errors made by the HBAF are mainly caused by the following aspects. First, emotion is a subjective concept with involved uncertainty, resulting in human bias introduced during the labeling of the utterances. The emotion felt by the speaker and perceived by human annotators may have different stances. HBAF sometimes got confused and missclassified similar or close emotions such as disgust and sad. Secondly, the MELD dataset is highly imbalanced. In particular, the system could not deal with minority classes very well, such as fear and disgust). Many emotions are overwhelmingly predicted as neutral for MELD dataset. These phenomena also appear in previous works  (Cai et al., 2023; Singh and Goel, 2022; Ezzameli and Mahersia, 2023) . What's more, human express emotion by various modalities such as facial expressions, voice characteristics, linguistic content of verbal communications, and body postures, while HBAF method has only focused on audio and text modality for predicting emotions in conversations.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a bimodal framework for bimodal speech emotion recognition, \"Heterogeneous Bimodal Attention Fusion (HBAF)\", as an early attempt to explore the interactions between audio and text modalities at the different representation level. HBAF was built on three key modules: unimodal representation module, multi-modal fusion module and inter-modal contrastive learning module. Specifically, we incorporated contextual content into low-level audio representations to bridge the heterogeneous multi-modal gap and improve multi-modal fusion. At the heart of HBAF is the introduction of multi-modal fusion module comprising of bimodal attention network, dynamic filter gate and residual connection, which enables effective and efficient exploration of filtering out incorrect cross-modal relationships and learning both intra-and intermodal interactions between audio and text modality. Furthermore, the inter-modal contrastive learning module enables fully capturing absolute and relative inter-modal interactions towards the understanding of emotions. Experiments demonstrated that our HBAF outperforms state-of-the-art methods.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Acknowledgment",
      "text": "We would like to thank the anonymous reviewers for their valuable comments and feedback. Special acknowledgment is given to the Centre for Digital Music at Queen Mary University of London for its support. This research was funded by the China Scholarship Council and Queen Mary University of London.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the short sentence ‘You really like it?’ is ambiguous",
      "page": 1
    },
    {
      "caption": "Figure 1: Illustration of the proposed Heterogeneous Bimodal Attention Fusion (HBAF) method. Based on experimental observations, this figure illustrates key",
      "page": 2
    },
    {
      "caption": "Figure 2: , the proposed method mainly consists of three mod-",
      "page": 3
    },
    {
      "caption": "Figure 2: Block diagrams of the proposed heterogeneous bimodal speech emotion recognition method (HBAF). The method consists of three modules, uni-modal",
      "page": 5
    },
    {
      "caption": "Figure 2: , the proposed method HBAF is com-",
      "page": 5
    },
    {
      "caption": "Figure 1: ). We aim to evalu-",
      "page": 5
    },
    {
      "caption": "Figure 3: Update scheme of the audio context network (a) and text context",
      "page": 6
    },
    {
      "caption": "Figure 4: Update scheme of the multi-modal fusion module.",
      "page": 7
    },
    {
      "caption": "Figure 4: represent the processing of two dis-",
      "page": 7
    },
    {
      "caption": "Figure 5: ). Unlike traditional approaches that focus",
      "page": 8
    },
    {
      "caption": "Figure 5: Update scheme of the inter-modal contrastive learning module:the absolute inter-modal loss: Lm , a and La , l and the relative-modal loss: Lm , a.",
      "page": 9
    },
    {
      "caption": "Figure 6: Overall performance comparison among the proposed HBAF method, state-of-the-art methods, and baseline approaches on the MELD",
      "page": 11
    },
    {
      "caption": "Figure 6: and Table 2.",
      "page": 11
    },
    {
      "caption": "Figure 6: ), it should be highlighted that text modality significantly out-",
      "page": 11
    },
    {
      "caption": "Figure 5: , the multi-",
      "page": 12
    },
    {
      "caption": "Figure 5: ). Combining information from",
      "page": 12
    },
    {
      "caption": "Figure 7: Input of audio, text and bimodality and prediction with HBAF method, bc-LSTM and TEIMEI on MELD and IEMOCAP datasets in our case study.",
      "page": 13
    },
    {
      "caption": "Figure 8: Visualization confusion matrices on the test set of MELD (a) and IEMOCAP (b)",
      "page": 13
    },
    {
      "caption": "Figure 7: , respectively. Specially, we compare",
      "page": 13
    },
    {
      "caption": "Figure 6: , HBAF achieves",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LLDs": "MFCCs",
          "0.412↑": "0.346↑",
          "0.398": "0.335",
          "0.583↑": "0.528↑",
          "0.567": "0.515"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "84.66": "14.59",
          "2.77": "62.28",
          "0.87": "1.07",
          "2.06": "1.78",
          "5.45": "4.11",
          "0.79": "2.14",
          "3.40": "11.74"
        },
        {
          "84.66": "18.00",
          "2.77": "10.60",
          "0.87": "26.00",
          "2.06": "12.00",
          "5.45": "10.00",
          "0.79": "4.00",
          "3.40": "14.00"
        },
        {
          "84.66": "32.69",
          "2.77": "8.17",
          "0.87": "3.37",
          "2.06": "36.54",
          "5.45": "2.88",
          "0.79": "1.44",
          "3.40": "9.40"
        },
        {
          "84.66": "18.07",
          "2.77": "7.67",
          "0.87": "1.24",
          "2.06": "1.73",
          "5.45": "66.58",
          "0.79": "3.96",
          "3.40": "0.76"
        },
        {
          "84.66": "23.53",
          "2.77": "2.94",
          "0.87": "2.94",
          "2.06": "2.94",
          "5.45": "7.35",
          "0.79": "41.18",
          "3.40": "19.12"
        },
        {
          "84.66": "18.26",
          "2.77": "7.83",
          "0.87": "1.45",
          "2.06": "1.74",
          "5.45": "10.72",
          "0.79": "3.19",
          "3.40": "56.81"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "63.89": "2.04",
          "8.33": "85.31",
          "4.17": "3.27",
          "6.25": "1.22",
          "15.97": "2.86",
          "1.39": "5.31"
        },
        {
          "63.89": "4.95",
          "8.33": "3.39",
          "4.17": "77.08",
          "6.25": "3.41",
          "15.97": "2.34",
          "1.39": "8.33"
        },
        {
          "63.89": "1.76",
          "8.33": "1.18",
          "4.17": "2.94",
          "6.25": "76.47",
          "15.97": "1.18",
          "1.39": "16.47"
        },
        {
          "63.89": "10.37",
          "8.33": "3.01",
          "4.17": "5.35",
          "6.25": "3.68",
          "15.97": "75.92",
          "1.39": "1.67"
        },
        {
          "63.89": "3.41",
          "8.33": "1.67",
          "4.17": "8.14",
          "6.25": "3.97",
          "15.97": "3.15",
          "1.39": "69.03"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Transformer models for text-based emotion detection: A review of bert-based approaches",
      "authors": [
        "F Acheampong",
        "H Nunoo-Mensah",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "2",
      "title": "Challenges and opportunities of text-based emotion detection: A survey",
      "authors": [
        "A Al Maruf",
        "F Khanam",
        "M Haque",
        "Z Jiyad",
        "F Mridha",
        "Z Aung"
      ],
      "year": "2024",
      "venue": "Challenges and opportunities of text-based emotion detection: A survey"
    },
    {
      "citation_id": "3",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "A Bosselut",
        "H Rashkin",
        "M Sap",
        "C Malaviya",
        "A Celikyilmaz",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition using different sensors, emotion models, methods and datasets: A comprehensive review",
      "authors": [
        "Y Cai",
        "X Li",
        "J Li"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "Multisensory integration: methodological approaches and emerging principles in the human brain",
      "authors": [
        "G Calvert",
        "T Thesen"
      ],
      "year": "2004",
      "venue": "Journal of Physiology-Paris"
    },
    {
      "citation_id": "9",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S D'mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "10",
      "title": "2022a. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "2022b. M2fnet: Multi-modal fusion network for emotion recognition in conversation",
      "authors": [
        "V Chudasama",
        "P Kar",
        "A Gudmalwar",
        "N Shah",
        "P Wasnik",
        "N Onoe"
      ],
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Look, listen, and learn more: Design choices for deep audio embeddings",
      "authors": [
        "A Cramer",
        "H Wu",
        "J Salamon",
        "J Bello"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences",
      "authors": [
        "S Davis",
        "P Mermelstein"
      ],
      "year": "1980",
      "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "16",
      "title": "A study on music emotion recognition using deep audio embeddings: Comparison of l3-net and vggish",
      "authors": [
        "C Eunjeong",
        "Jin Ha"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "17",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "19",
      "title": "Multimodal emotion recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Audio set: An ontology and humanlabeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "21",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "R Mihalcea"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Cross-cultural emotion recognition with eeg and eye movement signals based on multiple stacked broad learning system",
      "authors": [
        "X Gong",
        "C Chen",
        "T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "23",
      "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
      "authors": [
        "M Gutmann",
        "A Hyvärinen"
      ],
      "year": "2010",
      "venue": "Proceedings of the thirteenth international conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition approaches: A systematic review",
      "authors": [
        "A Hashem",
        "M Arif",
        "M Alghamdi"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "25",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "26",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "27",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "28",
      "title": "Multitask learning for frame-level instrument recognition",
      "authors": [
        "C Hung",
        "J Salamon",
        "C Jacoby",
        "J Bello"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "M Jain",
        "S Narayan",
        "P Balaji",
        "A Bhowmick",
        "R Muthu"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition using support vector machine",
      "arxiv": "arXiv:2002.07590"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis",
      "authors": [
        "L Kessous",
        "G Castellano",
        "G Caridakis"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "S Koolagudi",
        "K Rao"
      ],
      "year": "2012",
      "venue": "International journal of speech technology"
    },
    {
      "citation_id": "32",
      "title": "Distributed representations of sentences and documents",
      "authors": [
        "Q Le",
        "T Mikolov"
      ],
      "year": "2014",
      "venue": "International conference on machine learning, PMLR"
    },
    {
      "citation_id": "33",
      "title": "Multimodal emotion recognition fusion analysis adapting bert with heterogeneous feature unification",
      "authors": [
        "S Lee",
        "D Han",
        "H Ko"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "34",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li",
        "F Tang",
        "M Zhao",
        "Y Zhu"
      ],
      "year": "2022",
      "venue": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "35",
      "title": "Modeling intra-and inter-modal relations: Hierarchical graph contrastive learning for multimodal sentiment analysis",
      "authors": [
        "Z Lin",
        "B Liang",
        "Y Long",
        "Y Dang",
        "M Yang",
        "M Zhang",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "36",
      "title": "Ket: Knowledge enriched transformer for emotion detection",
      "authors": [
        "Z Lin",
        "H Zhou",
        "T Ge",
        "G Li",
        "M Sun"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "37",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Com-puting"
    },
    {
      "citation_id": "38",
      "title": "Audio self-supervised learning: A survey",
      "authors": [
        "S Liu",
        "A Mallol-Ragolta",
        "E Parada-Cabaleiro",
        "K Qian",
        "X Jing",
        "A Kathan",
        "B Hu",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Audio self-supervised learning: A survey"
    },
    {
      "citation_id": "39",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J Qiu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "40",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "41",
      "title": "Cross-modal fusion techniques for utterancelevel emotion recognition from text and speech",
      "authors": [
        "J Luo",
        "H Phan",
        "J Reiss"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "43",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Emoticon: Context-aware multimodal emotion recognition using frege's principle",
      "authors": [
        "T Mittal",
        "P Guhan",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "46",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "D Raj",
        "A Pyrkosz"
      ],
      "year": "2021",
      "venue": "Proceedings of Interspeech 2021, ISCA"
    },
    {
      "citation_id": "47",
      "title": "Semi-supervised sequence tagging with bidirectional language models",
      "authors": [
        "M Peters",
        "W Ammar",
        "C Bhagavatula",
        "R Power"
      ],
      "year": "2017",
      "venue": "Semi-supervised sequence tagging with bidirectional language models",
      "arxiv": "arXiv:1705.00108"
    },
    {
      "citation_id": "48",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "49",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "50",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "Learning transferable visual models from natural language supervision"
    },
    {
      "citation_id": "51",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "52",
      "title": "Maln: Multimodal adversarial learning network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "J Liu",
        "M Liu",
        "X Li",
        "A Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "53",
      "title": "Interactive multimodal attention network for emotion recognition in conversation",
      "authors": [
        "M Ren",
        "X Huang",
        "X Shi",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "54",
      "title": "Atomic: An atlas of machine commonsense for if-then reasoning",
      "authors": [
        "M Sap",
        "R Le Bras",
        "E Allaway",
        "C Bhagavatula",
        "N Lourie",
        "H Rashkin",
        "B Roof",
        "N Smith",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "55",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "Multimodal approaches for emotion recognition: a survey",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Internet Imaging VI, SPIE"
    },
    {
      "citation_id": "57",
      "title": "2024a. Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "venue": "Information Fusion"
    },
    {
      "citation_id": "58",
      "title": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "authors": [
        "Y Shou",
        "T Meng",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multi-modal emotion learning with broad state space models and probability-guidance fusion",
      "arxiv": "arXiv:2404.17858"
    },
    {
      "citation_id": "59",
      "title": "Universality, domain-specificity and development of psychological responses to music",
      "authors": [
        "M Singh",
        "S Mehr",
        "Y Singh",
        "S Goel"
      ],
      "year": "2022",
      "venue": "Nature reviews psychology"
    },
    {
      "citation_id": "60",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "61",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "62",
      "title": "Multimodal interaction: A review",
      "authors": [
        "M Turk"
      ],
      "year": "2014",
      "venue": "Pattern recognition letters"
    },
    {
      "citation_id": "63",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "64",
      "title": "Pretrained language models in biomedical domain: A systematic survey",
      "authors": [
        "B Wang",
        "Q Xie",
        "J Pei",
        "Z Chen",
        "P Tiwari",
        "Z Li",
        "J Fu"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "65",
      "title": "M2r2: Missing-modality robust emotion recognition framework with iterative data augmentation",
      "authors": [
        "N Wang",
        "H Cao",
        "J Zhao",
        "R Chen",
        "D Yan",
        "J Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "66",
      "title": "Dynamic emotion-dependent network with relational subgraph interaction for multimodal emotion recognition",
      "authors": [
        "Y Wang",
        "W Zhang",
        "K Liu",
        "W Wu",
        "F Hu",
        "H Yu",
        "G Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "67",
      "title": "A comprehensive review of speech emotion recognition systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE access"
    },
    {
      "citation_id": "68",
      "title": "Robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion",
      "authors": [
        "B Xie",
        "M Sidulova",
        "C Park"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "69",
      "title": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "T Yun",
        "H Lim",
        "J Lee",
        "M Song"
      ],
      "year": "2024",
      "venue": "Telme: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "arxiv": "arXiv:2401.12987"
    },
    {
      "citation_id": "70",
      "title": "A survey of audio classification using deep learning",
      "authors": [
        "K Zaman",
        "M Sah",
        "C Direkoglu",
        "M Unoki"
      ],
      "year": "2023",
      "venue": "A survey of audio classification using deep learning"
    },
    {
      "citation_id": "71",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "S Zhang",
        "Y Pan",
        "J Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "72",
      "title": "Deep learning-based multimodal emotion recognition from audio, visual, and text modalities: A systematic review of recent advancements and future prospects",
      "authors": [
        "S Zhang",
        "Y Yang",
        "C Chen",
        "X Zhang",
        "Q Leng",
        "X Zhao"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "73",
      "title": "Multimodal affective states recognition based on multiscale cnns and biologically inspired decision fusion model",
      "authors": [
        "Y Zhao",
        "X Cao",
        "J Lin",
        "D Yu",
        "X Cao"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Commonsense knowledge aware conversation generation with graph attention",
      "authors": [
        "H Zhou",
        "T Young",
        "M Huang",
        "H Zhao",
        "J Xu",
        "X Zhu"
      ],
      "year": "2018",
      "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "75",
      "title": "Inferring emotion from conversational voice data: A semi-supervised multi-path generative neural network approach",
      "authors": [
        "S Zhou",
        "J Jia",
        "Q Wang",
        "Y Dong",
        "Y Yin",
        "K Lei"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    }
  ]
}