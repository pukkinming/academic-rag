{
  "paper_id": "2202.02522v2",
  "title": "Leapmood: Light And Efficient Architecture To Predict Mood With Genetic Algorithm Driven Hyperparameter Tuning",
  "published": "2022-02-05T09:44:37Z",
  "authors": [
    "Harichandana B S S",
    "Sumit Kumar"
  ],
  "keywords": [
    "Emotion Recognition in Conversation",
    "Mood Prediction",
    "Affective Computing",
    "Hyperparameter Optimization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Accurate and automatic detection of mood serves as a building block for use cases like user profiling which in turn power applications such as advertising, recommendation systems, and many more. One primary source indicative of an individual's mood is textual data. While there has been extensive research on emotion recognition, the field of mood prediction has been barely explored. In addition, very little work is done in the area of on-device inferencing, which is highly important from the user privacy point of view. In this paper, we propose for the first time, an on-device deep learning approach for mood prediction from textual data, LEAPMood. We use a novel on-device deployment-focused objective function for hyperparameter tuning based on the Genetic Algorithm (GA) and optimize the parameters concerning both performance and size. LEAPMood consists of Emotion Recognition in Conversion (ERC) as the first building block followed by mood prediction using K-means clustering. We show that using a combination of character embedding, phonetic hashing, and attention along with Conditional Random Fields (CRF), results in a performance closely comparable to that of the current State-Of-the-Art with a significant reduction in model size (> 90%) for the task of ERC. We achieve a Micro F1 score of 62.05% with a memory footprint of a mere 1.67MB on the DailyDialog dataset. Furthermore, we curate a dataset for the task of mood prediction achieving a Macro F1-score of 72.12% with LEAPMood.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Mood changes are experienced by everybody. Recognizing and keeping track of these moods, can improve the overall experience for the user during any human-computer interactions and additionally help improve their mental wellbeing. Though the benefits of accurately predicting the user's mood state are numerous, doing so is a very complex task  [1] . This is because moods can be affected by various factors being psychological state, environmental changes, etc.\n\nTechnology has a huge impact on the emotional state of a person nowadays, due to growing user engagement in social media. The latest data show that 57% of the total global population use social media around the world in July 2021. These figures suggest that more than 9 in 10 internet users now use social media each month  [2] . As a direct outcome, Detection of mood becomes more imperative in post covid world. At large, all of the studies on the psychological disorders during the COVID-19 pandemic have reported that the affected individuals show several symptoms of mental trauma, such as emotional distress, depression, mood swings, insomnia, anger, etc.  [3] . In this paper, we focus on automatically predicting the mood of an individual from textual data. This can serve many use cases like a mood calendar to help track one's state of mood as shown in Figure  1 .\n\nThe first requirement for this problem statement is to ensure the privacy of user data. Thus, we aim to create an effective and lightweight solution that is deployed completely on-device to protect the user's privacy. Today there are over six billion smartphone subscribers worldwide and is forecasted to further grow by several hundred million in the next few years  [4] . A prediction by Gartner shows that 80% of smartphones will have on-device AI, and this is up from just 10% in 2017  [5] . This proves that on-device solutions for such problem statements are very crucial.\n\nWhile there has been extensive work done in the field of Emotion recognition, mood prediction is relatively untouched. The concepts of mood and emotion may seem similar but they consist of major differences in reality. Mood can last longer than emotions  [6]  and are very subtle compared to emotions which can be intense like fear, joy, etc. and this may result in unawareness of one's mood i.e. good or bad until self-reflection. Mood tracking can be done at set time intervals, to identify patterns in how their mood varies. It is found that Emotions come first, then moods develop from a combination of feelings  [7] . Hence, we can use emotions detected and predict mood for a specific time duration.\n\nTo this end, we propose a novel Light and Efficient Architecture to Predict Mood (LEAPMood). To summarize our major contributions in this paper:\n\n• We propose a novel pipeline LEAPMood, for efficient on-device mood prediction from textual data. • We propose a novel lightweight model for on-device ERC as a subcomponent in our pipeline and introduce phonetic hashing as a pre-processing step to capture Out of Vocabulary (OOV) and exaggerated emotions. • We introduce a new Fitness function for Hyperparameter tuning using GA focused on on-device deployment. • We benchmark our solution for on-device ERC on the DailyDialog dataset and analyze the overall performance of LEAPMood on a custom dataset for mood prediction.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Psychologists have attempted to understand and explain human emotion since the 19th-century  [8] . The concept of Affective computing was proposed and studied as early as the 1990s where R.W. Picard talks about this concept in his book  [9] . Soon, there has been numerous and extensive work done in this field attracting many researchers to detect an individual's emotional state from various modalities as sources such as eye gaze, audio, gestures, etc.  [10] [11] [12]  [13]  [14] . One such popular modality to recognize human emotions is text. Emotion Recognition in conversation (ERC) is challenging in the sense that the same set of words or phrases can be used to depict different emotions.\n\nEarly research in the field of ERC involved keyword-based techniques which are one of the most intuitive and naive approaches.  [15]  use syntactic features and identify opinions in deeply nested clauses to classify their strengths. Following this, some semi-automatic methods for creating emotion dictionaries utilizing methods like WordNet were explored  [16] . In the following years, limitations of keyword-based techniques were uncovered by researchers like  [17] .\n\nLearning-based approaches have been explored to overcome the limitations of keyword-based methods.  [18]  propose a set theory and Support Vector Machine (SVM) based approach. This was followed by the use of Conditional Random Fields (CRF) which showed improved performance for emotion classification  [19] .\n\nIn the coming years, RNN based architectures for multimodal emotion architectures  [20] . DialogueRNN introduced in  [21]  used GRUs to model global and speaker states. With the advance of Transformer based models, many works have been done using different variants of these  [22]    [23] . The current SOA CESTa uses a transformer-based encoder along with CRF  [24] . One major drawback is that these models are very huge in size making them not suitable for on-device deployment.\n\nIn contrast to emotion recognition, mood prediction has been explored mainly from the biological sensor-based approach  [25]    [26] .  [27] [28]  attempt to predict mood from textual and other mobile data but do not consider finegrained information like emotional states experienced by user to do so. We attempt to do this for the first time.\n\nSince we require a solution completely on-device, it is crucial to get the most out of small model architecture. Hyperparameter tuning has proved to be effective to do so. Research in this field include  [29]  [30] and many more. We observe that there is no work done in this field specifically for on-device deployment. Thus, we explore this area.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Pipeline",
      "text": "The proposed pipeline consists of first identifying the emotion triggered by a specific text and then, based on the duration on which mood is to be determined, processing the emotions recognized to predict the mood. The first challenge encountered for modeling this solution is the lack of a dataset that supports mood prediction.\n\nOn the other hand, there is extensive work done in the field of sentiment analysis and emotion recognition using textual input. We use DailyDialog dataset  [31]  to train and test our model on the emotion recognition task. We notice one disadvantage of such open datasets which is that they do not give complete insight on the real-world performance due to lack of coverage on spell mistakes, short forms, exaggerated words, etc. which may lead to many unknown tokens after pre-processing which in turn results in loss of important information to give correct results.\n\nTo tackle such scenarios, we use our custom data as well which consists of group chat data extracted and annotated  Fig.  3 . Custom Dataset Distribution on both emotions and mood categories. This helps further to train and test our complete pipeline for this proposed solution which is shown in Fig.  2 . It is worth noting that we use both character and word embedding along with phonetic hashing to reduce OOVs. The detailed explanations for each module are covered in the following subsections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Dataset",
      "text": "We use the DailyDialog dataset  [31]  to train and benchmark our model for the task of emotion recognition. The dataset consists of 7 emotion categories, details of which can be seen in Table  I . It can be observed that the data is highly imbalanced. To solve this issue, we use class weights while training the model which is explained in section III-C.\n\nWe create our custom dataset to test the real-time performance of the model. We curated the data by extracting multiple group chats from WhatsApp and removing the private content of the involved individuals. We then annotate the data according to the emotion from the textual content collecting over 1600 samples. This is done by 10 annotators from diverse age and gender groups. The details of our dataset are shown in Fig.  3 . The data is further pre-processed before model training as explained in Section III-B.\n\nTo further train our solution for mood prediction, we utilize the output logits from our ERC module. Since mood is an aggregation of various emotions over time, we group our samples according to timestamps and annotate these groups using binary labels indicating the mood i.e Good Mood or Bad Mood. We get a total train set of 394 and a test set of 76 mood groups as shown in Fig.  3 . One sample of such grouping is shown in Table  II . To ensure the correctness of these labels, we cross-verify the labels with help of the above-mentioned annotators. The details of mood prediction are explained in the following Section III-C.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Pre-Processing",
      "text": "The pre-processing of the data involves the following steps: Cleaning, Replace acronyms, Tokenization, Phonetic hashing, Create Sequences, Padding. We begin by cleaning the input text sample which includes converting to lowercase, removing non-alphanumeric symbols using regex, removing emoji(s) if present (in the case of the custom dataset) and finally, replacing numbers with a number tag. After this cleaning process, we replace the acronyms. For this, we create a map of the 100 most frequently used acronyms curated from our custom dataset and replace them with their respective full forms. The next step in the pre-processing pipeline is tokenization. During this process, we also create a word vocabulary of size 30,000 tokens. This is done by first keeping track of the frequencies of occurrence of each token and choosing the top 30,000 most frequently occurring tokens. Along with this, we create a character vocabulary as well to support character embeddings. We create a character vocabulary of size 30 characters which includes all english alphabets and a handful of other frequently used characters/symbols. Following this step, we use phonetic hashing of tokens. This is done using the Soundex algorithm  [32] . This results in a four-character code for any token with the characteristic of having the same codes for phonetically similar words as shown in Fig.  4 . Due to this property, this is a possible solution for spell errors and exaggerated words, as shown in Table  III . For this, we map the phonetic codes for tokens in vocabulary, and during tokenization, in the case of an OOV, we match its phonetic code with that of the ones in the vocabulary. If a match is found, we replace the token with the one in the vocabulary. Thus, we avoid tagging these as unknown and risk loss of important information.\n\nNext, we create input sequences by mapping the tokens and characters with their respective IDs in the word and character vocabularies created in the previous steps. This is then followed by padding the sequences to ensure uniform input size across all samples for model training.\n\nThe complete pipeline is shown in Fig.  4 . The final preprocessed samples are then fed to our model for training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Leapmood",
      "text": "We propose a novel architecture LEAPMood, for efficient on-device mood prediction from textual data. As mood can be determined by the set of emotions experienced during the required period, rather than training directly for mood prediction, we model the solution as two components: This consists of two main components:\n\n• Emotion Recognition • Mood Prediction This also enables the use of ERC as an individual module for other use cases. The complete architecture of LEAPMood is shown in Fig.  5 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Emotion Recognition In Conversation:",
      "text": "The first component of ERC mainly consists of five prominent layers: Word Embedding, Char Embedding, Long Short-Term Memory (LSTM) encoder for char embeddings, Bidirectional LSTM (BILSTM), Attention, and CRF as shown in Fig.  5 .\n\nThe pre-processed dataset having word sequences after phonetic hashing (Input-1) and character sequences (Input-2) along with one-hot encoded ground-truth labels are fed to the model. The input shapes are set to (,100) and (,100,10) respectively where the maximum sequence length is set to 100, and the maximum character sequence length to 10. The first layer is the word embedding layer which uses Input-1 and the embedding size is set to 56. Following this, we define a TimeDistributed Layer with Embedding layer which is applied on Input-2, resulting in char embeddings as output. For this, we set the character embedding dimension to be 16. We then use an LSTM layer with 20 units to encode these character embeddings and make them compatible to concatenate with the word embeddings. After concatenating both word and character embeddings, we use a Bidirectional LSTM with 57 units followed by a Dense Layer with softmax activation to get the logits for the input sample. This is followed by a CRF Layer to model the contextual information of previous and future labels. While compiling the model, we use Adam optimizer and cross-categorical entropy loss with class weights calculated to balance the loss on different imbalanced class labels. We train for 25 epochs with EarlyStopping, batch size of 90, and a learning rate of 0.0001.\n\nThese parameters along with other model hyperparameters are determined using a unique and novel Genetic Algorithm based hyperparameter optimization which will be discussed in detail in Section III-D.\n\n2) Mood Prediction: Following the emotion recognition module, we use the K-means clustering algorithm to predict the mood. The mood of a group of textual data can be predicted from the individual emotions recognized from the previous component as explained in Section III-C. We use clustering because we observe that for any positive emotion, the logits for all the positive emotion categories are high and this applies similarly for negative emotions. Thus, clustering will be able to group these and also does not require much memory space.\n\nWe use the prepared dataset where we group batches of textual data of similar timestamps. We create such batches of text grouping data within the window of a 60-minute threshold. This configuration enables to predict mood for an interval of 1 hour for the user. This is configurable according to the user's needs. Following the creation of these batches of text, we take the aggregate of the logits corresponding to each class label according to (1).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ag G R Eg At E_L Og I T S =",
      "text": "Where N is the number of textual data samples in the group and M is the number of labels. x i M is the value of the i t h sample for the M t h l abel . Finally, we use these logits to perform K-means clustering. We choose the value of K to be 2, giving two clusters one representing a good mood and the other as a bad mood. We choose to first experiment with a small number of clusters to avoid wrong predictions.\n\nGoing forward, we will explore using higher values of K and thus predict fine-level mood other than just good or bad.  We use the Genetic Algorithm for Hyperparameter tuning. Since we focus on the on-device deployment of our model, we need to optimize both model performance and size. To do this, we use a novel fitness function that accounts for these two factors. The steps for on-device focused Hyperparameter tuning (as shown in Fig.  6 ) are:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Hyperparameter Tuning",
      "text": "• Fitness Function and Stopping criteria definition",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fitness Function And Stopping Criteria Defination:",
      "text": "Fitness function is defined to judge how good the individual solution is to solve the problem. In our case, it shows how good the choice of parameters is for the model performance. Additionally, we use the model size information correspond-ing to the chosen parameters to make sure we get the most optimal set of parameters that give the best performance with the least model size possible. We calculate the model size using the number of model weights and parameters required for the individual solution using  (2) .\n\nHere, H P i is the i t h solution for Hyper parameters required to be tuned. Each solution H P i contains set of hyper parameters p i j k representing j t h parameter of component k, C k . The total model parameters are calculated by summing up the number of weights required for each Component using parameter p i j k represented by #C k p i j k . The final Fitness function used for our Model tuning is as represented by (3).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "F I T Ness I = Mod El _Accur Ac Y I H P I",
      "text": "Tot al _Mod el _P ar ams i (3)\n\nWhere Mod el _Accur ac y i H P i is the Model accuracy after substituting parameters with those in H P i .\n\n2) Population Initialization: Population in genetic algorithm consists of chromosomes which in turn consist of genes. In this case, each chromosome represents one possible solution (H P i in (  2 )) and each gene represents a hyperparameter to be optimized i.e p i j k . Each gene belongs to one of the components of the model architecture defined in Section III-C1 Ex: BiLSTM (represented by C k ). We optimize 10 hyperparameters namely: batch size, epochs, word embedding dimension, char embedding dimension, char LSTM hidden size, spacial dropout, LSTM dropout, LSTM recurrent dropout, BiLSTM hidden size, and BiLSTM recurrent dropout. These constitute the genes in each chromosome. We keep track of the type of gene i.e. continuous or discrete and set a range of [minimum, maximum] for each gene. We initialize these values randomly depending on the type and range. The population size is set to 7. This means we explore 7 different optimizations at a time.\n\nThe Stopping criteria determine when to stop executing the steps and return the most optimal solution at the time of termination. We set this to be the count of process loops reaching 250. That is, as soon as the process repeats 250 times, it terminates.\n\n3) Fitness Evaluation: Following population initialization, we evaluate the fitness of each chromosome using our proposed fitness function. We train the model with the parameter values in the corresponding chromosome to get its fitness score.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "4) Selection:",
      "text": "We use Roulette wheel method for selection. In this, we divide the wheel into N divisions where N is equal to the population size which here is 7. Each division occupies an area proportionate to the fitness score of the corresponding chromosome according to  (4) .\n\nThis ensures that the best fit chromosomes have a high probability to get selected as parent chromosomes.\n\n5) Cross-over: Cross over is used to recombine genes from two different parents to get a new offspring with its genes a mixture of the two selected parents. We use one point crossover technique where a point in the range of (0, Chromosome Size) is randomly picked called 'crossover point'. After this, the genes to the right of the crossover point are swapped resulting in two new offspring. This operation is repeated till a particular limit which is determined by 'Crossover_Rate'. We set the crossover rate as 0.5. This indicates that this process is repeated for Popul at i on_Si ze * C r ossover _Rat e times. Here chromosome size is 10. We randomly select two parents from the selected population and perform this operation.\n\n6) Mutation: Mutation is used to ensure genetic diversity in the offspring. While crossover interchanges the genes in parents, the individual values of the genes themselves are not changed. This is taken care of by using mutation. To do this we randomly choose a gene from a chromosome in the population after crossover, choose a value within its range defined earlier, and replace it with the new value. This is repeated for each chromosome and the number of genes chosen for mutation is equal to the (C hr omosome_Si ze * Mut at i on_Rat e). We set the mutation rate to be 0.25.\n\n7) Repeat till Stopping Criteria: On execution of the above steps, we re-evaluate the fitness scores for all the new offspring in the population. We then check if the stopping criteria defined earlier is met. In our case, it is a count of process loops that is set to 250. If this is not yet met, we repeat the process.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. On-Device Deployment",
      "text": "For deploying on-device, We also performed quantization using TensorFlow tflite as well on this model to further reduce the size which results in a model size of ∼1.9MB. We implement pre-processing techniques explained in section III-B on Android. We then create a test application to run the model for on-device inferencing. This application takes input a set of textual input and outputs their respective emotion categories and also the overall mood.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Results",
      "text": "We experiment with our proposed pipeline on-device using the Samsung A50 smartphone model (6GB RAM, 64GB ROM, Samsung Exynos 7 Octa 9610).\n\nWe analyse the performance of the ERC module explained in Section III-C1 on the DailyDialog dataset  [31]  as shown in Table  IV . To do this, we train our model on the DailyDialog dataset and measure the Precision, Recall, and F1 scores for each category following which we measure the Macro and Micro Averages. Since this dataset has a large number of samples (>80%) belonging to the 'other' class, we do not consider this category to calculate the Micro and Macro Averages as this trend has been followed by all the previous State-of-Art (SOA) works  [23]    [24] . The results show that our model performs exceptionally, achieving a Micro F1 Score of 62.05%. This is more impressive considering that it has a memory footprint of a mere 1.678MB.\n\nWe compare our model with some of the recent baseline models and on the current SOA CESTa  [24]  as shown in Table  V . We observe that our performance is very closely comparable to the SOA and stands second, behind by just 1.07% in the Micro F1 Score of CESTa. This is with a significant reduction in model size (∼90%), as CESTa uses a 12-layer 2048 inner-layer dimension and 100 hidden dimensions along with an embedding size of 300, resulting in a model size greater than 200MB, while our model is of just 1.678MB. This is a significant contribution towards the task of ERC.  Further, we test our model on our custom data explained in Section III-A to evaluate its performance on real-world chat scenarios and observe that our model performs well on the basis that these contain acronyms, spelling mistakes, etc. and has more categories of emotions as shown in Table  VI . This is possible due to the phonetic hashing component in the model the effect of which is explained in Section IV-B.\n\nWe study the effect of using Hyperparameter tuning fine-tuned for on-device deployment as explained in Section III-D. Fig.  7  shows the variation of some of the Hyperparameters as a function of Fitness Score as observed during the optimization process using GA. We utilize the most optimal values of hyperparameters obtained after We observe that GA contributes to an increase in Accuracy of 1.08% with a decrease in model size of 0.46MB.\n\nWe evaluate the mood prediction performance of LEAP-Mood on the curated and labeled data as shown in Table  VII . We observe good performance with a Macro F1-Score of 72.12%. More details into the model performance are observed after Error Analysis which is explained in Section IV-A.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Error Analysis",
      "text": "We conduct an error analysis to observe any trends among the false predictions by our model. On analyzing ERC results, we observe that most of the errors are due to predictions of similar emotion categories. For Ex: Happy and Surprise (Wow that's great), Sadness and Fear (I cannot do this). This is because the same sentence can be part of two different labels depending on the tone and we lack tone features in the textual data. We also observe the majority of the false positives are due to the 'other' class in DailyDialog.\n\nOn examining the mood prediction module results, we can conclude that most of the errors are due to samples where the emotion in the early stages of the grouped data is lost slowly with time. Moods are more affected by the We aim to address these as future work of this proposal.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Ablation Study",
      "text": "We perform an ablation study to analyze the effect of each layer on the model performance. We eliminate each layer and study the impact on overall model results as shown in Table  VIII . The results prove that each component in the proposed architecture is crucial to obtain better performance.\n\nThe character embedding layer helps learn fine character level features and improve performance by 1.33% in DailyDialog and 1.75% on our custom data. Although Phonetic hashing shows a minute change on the DailyDialog dataset, it shows a significant improvement on the custom data showing that for real-world use cases, it is a good addition to improve performance. The addition of attention shows good improvement on both datasets, aligning with our expectations. Finally, the CRF layer proves to be a major contributor towards the model performance, especially on the DailyDialog dataset where the context of previous and neighboring emotions play a major role to determine the emotion category.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we propose for the first time a novel approach to predict users' moods from textual data. Since mood is not just determined by one single message, we create a pipeline to detect the overall mood resulting from various sets of inputs. This is done by first recognizing the emotion category that the individual set of text represents and further modeling these for the final output. We create our custom dataset to improve the real-time performance of the model and analyze the overall performance of the model on our custom dataset. We introduce a novel fitness function for Hyperparameter tuning using GA and show its effectiveness. We further benchmark our emotion recognition pipeline with the DailyDialog dataset and observe a performance of 62.05% Micro F1 score which is just 1.07% less than the SOA and with a mere 1.67MB memory footprint.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Mood Calendar: Automatically keep track of user’s daily mood states",
      "page": 1
    },
    {
      "caption": "Figure 1: The ﬁrst requirement for this problem statement is to",
      "page": 1
    },
    {
      "caption": "Figure 2: Overall Pipeline of LEAPMood",
      "page": 2
    },
    {
      "caption": "Figure 3: Custom Dataset Distribution",
      "page": 3
    },
    {
      "caption": "Figure 2: It is worth noting that we",
      "page": 3
    },
    {
      "caption": "Figure 3: The data is further pre-processed",
      "page": 3
    },
    {
      "caption": "Figure 3: One sample of",
      "page": 3
    },
    {
      "caption": "Figure 4: Due to this property, this is a possible",
      "page": 3
    },
    {
      "caption": "Figure 4: The ﬁnal pre-",
      "page": 3
    },
    {
      "caption": "Figure 4: Preprocessing Pipeline.",
      "page": 4
    },
    {
      "caption": "Figure 5: 1) Emotion Recognition in Conversation: The ﬁrst compo-",
      "page": 4
    },
    {
      "caption": "Figure 5: The pre-processed dataset having word sequences after",
      "page": 4
    },
    {
      "caption": "Figure 5: LEAPMood Architecture.",
      "page": 5
    },
    {
      "caption": "Figure 6: Genetic Algorithm",
      "page": 5
    },
    {
      "caption": "Figure 7: shows the variation of some of the",
      "page": 7
    },
    {
      "caption": "Figure 7: Graph showing Different Hyper parameter values vs Fitness Function",
      "page": 7
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Predicting short term mood developments among depressed patients using adherence and ecological momentary assessment data",
      "authors": [
        "A Mikus",
        "M Hoogendoorn",
        "A Rocha",
        "J Gama",
        "J Ruwaard",
        "H Riper"
      ],
      "year": "2018",
      "venue": "Internet interventions"
    },
    {
      "citation_id": "2",
      "title": "Global social media stats",
      "venue": "Global social media stats"
    },
    {
      "citation_id": "3",
      "title": "Prevalence of stress, anxiety, depression among the general population during the covid-19 pandemic: a systematic review and meta-analysis",
      "authors": [
        "N Salari",
        "A Hosseinian-Far",
        "R Jalali",
        "A Vaisi-Raygani",
        "S Rasoulpoor",
        "M Mohammadi",
        "S Rasoulpoor",
        "B Khaledi-Paveh"
      ],
      "year": "2020",
      "venue": "Globalization and health"
    },
    {
      "citation_id": "4",
      "title": "Statista",
      "year": "2016",
      "venue": "Statista"
    },
    {
      "citation_id": "5",
      "title": "Gartner highlights 10 uses for ai-powered smartphones",
      "venue": "Gartner"
    },
    {
      "citation_id": "6",
      "title": "Mood vs emotion: Differences and traits",
      "venue": "Mood vs emotion: Differences and traits"
    },
    {
      "citation_id": "7",
      "title": "Emotions, feelings and moods: What's the difference?",
      "venue": "Sixseconds"
    },
    {
      "citation_id": "8",
      "title": "Emotion in a century: A review of emotion recognition",
      "authors": [
        "T Thanapattheerakul",
        "K Mao",
        "J Amoranto",
        "J Chan"
      ],
      "year": "2018",
      "venue": "Proceedings of the 10th International Conference on Advances in Information Technology"
    },
    {
      "citation_id": "9",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "10",
      "title": "Bimodal expression of emotion by face and voice",
      "authors": [
        "J Cohn",
        "G Katz"
      ],
      "year": "1998",
      "venue": "Proceedings of the Sixth ACM International Conference on Multimedia: Face/Gesture Recognition and Their Applications, ser. MULTIMEDIA '98",
      "doi": "10.1145/306668.306683"
    },
    {
      "citation_id": "11",
      "title": "Bimodal emotion recognition",
      "authors": [
        "L Silva",
        "P Ng"
      ],
      "year": "2000",
      "venue": "Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion detection in taskoriented spoken dialogues",
      "authors": [
        "L Devillers",
        "L Lamel",
        "I Vasilescu"
      ],
      "year": "2003",
      "venue": "Emotion detection in taskoriented spoken dialogues"
    },
    {
      "citation_id": "13",
      "title": "Investigation of combining svm and decision tree for emotion classification",
      "authors": [
        "T Nguyen",
        "M Li",
        "I Bass",
        "I Sethi"
      ],
      "year": "2005",
      "venue": "Investigation of combining svm and decision tree for emotion classification"
    },
    {
      "citation_id": "14",
      "title": "An emotion processing system based on fuzzy inference and subjective observations",
      "authors": [
        "T Yanaru"
      ],
      "year": "1995",
      "venue": "Proceedings 1995 Second New Zealand International Two-Stream Conference on Artificial Neural Networks and Expert Systems"
    },
    {
      "citation_id": "15",
      "title": "Just how mad are you? finding strong and weak opinion clauses",
      "authors": [
        "T Wilson",
        "J Wiebe",
        "R Hwa"
      ],
      "year": "2004",
      "venue": "Just how mad are you? finding strong and weak opinion clauses"
    },
    {
      "citation_id": "16",
      "title": "Semi-automatic creation of an emotion dictionary using wordnet and its evaluation",
      "authors": [
        "D Bracewell"
      ],
      "year": "2008",
      "venue": "2008 IEEE Conference on Cybernetics and Intelligent Systems"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition from text using semantic labels and separable mixture models",
      "authors": [
        "C.-H Wu",
        "Z.-J Chuang",
        "Y.-C Lin"
      ],
      "year": "2006",
      "venue": "ACM transactions on Asian language information processing (TALIP)"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition from text based on the rough set theory and the support vector machines",
      "authors": [
        "Z Teng",
        "F Ren",
        "S Kuroiwa"
      ],
      "year": "2007",
      "venue": "2007 International Conference on Natural Language Processing and Knowledge Engineering"
    },
    {
      "citation_id": "19",
      "title": "Emotion classification using web blog corpora",
      "authors": [
        "C Yang",
        "K -Y. Lin",
        "H.-H Chen"
      ],
      "year": "2007",
      "venue": "IEEE/WIC/ACM International Conference on Web Intelligence (WI'07"
    },
    {
      "citation_id": "20",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "21",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "arxiv": "arXiv:2012.08695"
    },
    {
      "citation_id": "23",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "B Lee",
        "Y Choi"
      ],
      "year": "2021",
      "venue": "Graph based network with contextualized representations of turns in dialogue",
      "arxiv": "arXiv:2109.04008"
    },
    {
      "citation_id": "24",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Y Wang",
        "J Zhang",
        "J Ma",
        "S Wang",
        "J Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "25",
      "title": "Multimodal autoencoder: A deep learning approach to filling in missing sensor data and enabling better mood prediction",
      "authors": [
        "N Jaques",
        "S Taylor",
        "A Sano",
        "R Picard"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "26",
      "title": "Mood prediction of patients with mood disorders by machine learning using passive digital phenotypes based on the circadian rhythm: prospective observational cohort study",
      "authors": [
        "C.-H Cho",
        "T Lee",
        "M.-G Kim",
        "H In",
        "L Kim",
        "H.-J Lee"
      ],
      "year": "2019",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "27",
      "title": "Multimodal privacy-preserving mood prediction from mobile data: A preliminary study",
      "authors": [
        "T Liu",
        "P Liang",
        "M Muszynski",
        "R Ishii",
        "D Brent",
        "R Auerbach",
        "N Allen",
        "L.-P Morency"
      ],
      "year": "2020",
      "venue": "Multimodal privacy-preserving mood prediction from mobile data: A preliminary study",
      "arxiv": "arXiv:2012.02359"
    },
    {
      "citation_id": "28",
      "title": "Features for mood prediction in social media",
      "authors": [
        "M Roshanaei",
        "R Han",
        "S Mishra"
      ],
      "year": "2015",
      "venue": "2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining"
    },
    {
      "citation_id": "29",
      "title": "Bohb: Robust and efficient hyperparameter optimization at scale",
      "authors": [
        "S Falkner",
        "A Klein",
        "F Hutter"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Optuna: A nextgeneration hyperparameter optimization framework",
      "authors": [
        "T Akiba",
        "S Sano",
        "T Yanase",
        "T Ohta",
        "M Koyama"
      ],
      "year": "2019",
      "venue": "Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "31",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "32",
      "title": "Phonetic matching: A better soundex",
      "authors": [
        "A Beider",
        "S Morse"
      ],
      "year": "2010",
      "venue": "Association of Professional Genealogists Quarterly"
    }
  ]
}