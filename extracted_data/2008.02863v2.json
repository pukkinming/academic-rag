{
  "paper_id": "2008.02863v2",
  "title": "A Transfer Learning Method For Speech Emotion Recognition From Automatic Speech Recognition",
  "published": "2020-08-06T20:37:22Z",
  "authors": [
    "Sitong Zhou",
    "Homayoon Beigi"
  ],
  "keywords": [
    "transfer learning",
    "emotion recognition",
    "IEMO-CAP",
    "time-delay neural network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents a transfer learning method in speech emotion recognition based on a Time-Delay Neural Network (TDNN) architecture. A major challenge in the current speechbased emotion detection research is data scarcity. The proposed method resolves this problem by applying transfer learning techniques in order to leverage data from the automatic speech recognition (ASR) task for which ample data is available. Our experiments also show the advantage of speaker-class adaptation modeling techniques by adopting identity-vector (ivector) based features in addition to standard Mel-Frequency Cepstral Coefficient (MFCC) features.  [1]  We show the transfer learning models significantly outperform the other methods without pretraining on ASR. The experiments performed on the publicly available IEMOCAP dataset which provides 12 hours of emotional speech data. The transfer learning was initialized by using the Ted-Lium v.2 speech dataset providing 207 hours of audio with the corresponding transcripts. We achieve the highest significantly higher accuracy when compared to stateof-the-art, using five-fold cross validation. Using only speech, we obtain an accuracy 71.7% for anger, excitement, sadness, and neutrality emotion content.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Detecting emotions from speech has attracted attention for its usage in enhancing natural human-computer interaction. The ability of understanding human emotion status is helpful for machines to bring empathy in various applications.\n\nSpeech emotion recognition is suffering from insufficiency of labeled data. Though several emotion datasets have been released  [2] [3]  [4] , the size of emotion datasets are relatively small due to the expensive collection costs compared with plentiful data for tasks like Automatic Speech Recognition(ASR)  [5]  and speaker recognition  [6] . Most speech emotion detection models are trained from scratch within a single dataset  [7] [8]  [9] [10], therefore cannot successfully adapt to novel scenarios which has not been encountered during training. One possible solution is to leverage knowledge acquired from large-scale datasets of relevant speech tasks to emotion recognition domain. Although some efforts were spent on transfer learning method for categorical emotion detection from other paralinguistic tasks, such as speaker, and gender recognition and effective emotional attributes prediction  [11]   [12] , however they dont choose source domain with large-scale datasets, not shown significant improvement over non-transfer learning methods.\n\nPrevious research has shown that speech emotion detection can be improved after combined with textual data  [13] . Multi-modal methods can significantly improve the emotion detection performance by incorporating lexical features from given transcripts with the acoustic features from audios  [14] [15]  [16] .However, in real application scenario, transcripts are often absent. Although ASR can provide transcripts in real time to emotional speech data  [16] , it requires large language models loaded, and is computational costly when decoding sequence. Therefore, transfer learning using ASR as the source domain might be an efficient solution in emotion detection to incorporate textual features through high level features extracted in ASR models.\n\nAnother challenge for emotion recognition is that speakers express emotions in different ways, in addition, environments can affect acoustic features. Speaker adaptation is useful to capsulate speaker and environment specific information into acoustic features. iVector  [17]  based adaptation has been shown fast and efficient in speech recognition  [18] . We employ i-vector based speaker adaptation in emotion detection.\n\nThis paper proposes a transfer learning method to adapt ASR models in emotion recognition domain. The model is pre-trained on Tedlium2 dataset  [5] , with over 207 hours data, and fine tuned on 12 hours of emotional speech. The model architecture is TDNN-based  [19] [1]  [18]  with input as speaker adapted MFCC  [1]  features. Our experiments show the improvements in emotion detection using transfer learning from ASR to speech emotion recognition combined with speaker adaptation. The performance is evaluated on the benchmark dataset Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [2] , with 71.7% unweighted accuracy among angry, happy, sad and neutral under the 5-fold cross validation strategy. Our method significantly outperforms the state-of-art strategy  [9] .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Most efficient models in prior work are based on deep learning models, which can learn high-level features from low-level acoustic features. Lees work  [7]  showed the importance of longrange context effect, and significantly improve the RNN results over DNN model. This work had been staying state-of-art for years with 63.89% unweighted accuracy(UA), until surpassed by a hybrid approach of convolution layers and LSTM convolutional LSTM  [9]  with 68.8% accuracy. Previous literature has seldomly discussed about TDNN architecture in emotion recognition. TDNN can efficiency capture temporal information as RNN and LSTM do, but is faster for its parallelization ability and lower computation costs during training  [18] , which is a desired property when training on large-scale ASR data.\n\nMany approaches  [7]  [10]are speaker independent where features are normalized for individuals resulting in information loss, while our work is conducted in speaker dependent context using full MFCC raw features combined with iVectors contain-ing speaker characteristics  [17] . Peddinti  [18]  has proposed an efficient TDNN-based architecture for ASR with features as the combination of MFCC and iVectors, efficiently learned robust representations among various speakers and environments. Our study uses bottleneck layers of this TDNN architecture to use high-level feature representations that reflect insights from ASR tasks, and fine tunes on emotion datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Problem Definition",
      "text": "The emotion recognition problem is a classification problem when we represent emotion as categories rather than dimensional representations,\n\nwhere where X are the acoustic features input and z is dimensional output corresponding to the emotion prediction. We want to find a function\n\nto map features to categories. This model is trained on framelevel labels, and predicts the utterance labels by aggregating frame-level predictions through max likelihood by summing up the results of frames.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature",
      "text": "Full MFCC features with all 40 coefficients are computed at each time index is used as input to neural network. Instead of mean normalization on MFCC, an 100-dimension iVector is appended to MFCC features at each frame to encode meanoffset information. The iVector extraction model is trained as described in  [18] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Tdnn For Asr",
      "text": "TDNN is designed for capturing long term temporal dependencies in lower computational costs compared to RNN. It operates similar to a feed forward DNN architecture where lower layers focus input content in narrow windows, and higher layers connects windows of selected previous layer nodes to process the information from a wider context. Therefore its deeper layers can learn effective long term temporal dependencies without recurrent connections which hurdle parallel computation. The pretraining on ASR follows the Kaldi recipe for the TED-Lium tasksi  [20] , where uses 13 TDNN layers and each layer consists of 1024 activation nodes. The time stride of each layer, which defines the window at which calculating over nodes at neighbor time steps in the past layer, is assigned as 0 for the 1 st and 5 th TDNN layer, as 1 from the 2 nd and 4 th layer, and as 3 for layers after since the 6 th . A fully connected prefinal layer of 1024 dimension follows the 13 th TDNN layer before decoding output sequences. The model is trained with a sequencelevel objective function named lattice-free version of the maximum mutual information (LF-MMI)  [21] , for maximising the log-likelihood of the correct sequences.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training On Emotion Labels",
      "text": "Emotion labels are given for each utterance in the dataset. We label all the frames using the utterance label where the frames lie in. To train for emotion detection, the 12 th and 13 th TDNN layers as well as the ASR prefinal layer are selected to produce bottleneck embeddings as high level features learnt from ASR, a new fully connected dense layer is appended after the embedding layer for predicting the frame-level emotion labels, and a softmax layer with four dimension outputs is used to predict frame-level emotion. The model uses cross-entropy as the objective function for frame-level classification.\n\nThe output of this model is for frame unit rather than for utterance unit, we aggregate frame-level predictions, using maximum likelihood by adding the output vectors over frames, corresponding to the highest valued dimension of the sum of the softmax layer output over all frames within the utterance 4. Experiment",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "This work uses IEMOCAP, which contains 12 hours of audio data with scripted and improvised speech, performed by ten actors, one male and one female as a pair in five sessions. In training and testing, four categories angry, happy, sad and neutral are selected out of ten categories, for a more balanced and efficient dataset, resulting in a final collection of 4936 utterances, each utterance has unique emotion label. This dataset consists of five sessions, and the category distribution is as in Table  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pretraining On Asr",
      "text": "For pretraining on ASR, we use the feed-forward TDNN to capture long term temporal dependencies from short term feature representations. Hidden activations are sub-sampled in order to speed up the training  [18] . The model is pre-trained on ASR data, with 13 TDNN layers and output layers for decoding sequence based on acoustic models. As neighboring activations shares largely overlapped input contexts, sub-sampling on activations can reduce computational costs without sacrificing the coverage range over input frames. The hyper parameters for model architecture and training are chosen according to Kaldi Tedlium2 TDNN recipe  [20] , which has been tuned properly on Tedlium2 dataset, achieving 7.6 word error rate(WER) on test dataset after six epochs training. The parameters are optimized through preconditioned stochastic gradient descent (SGD) updates, following the training recipe detailed in  [22] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training For Emotion Classification",
      "text": "After obtaining the pretraining model, we use 12 th , 13 th and the prefinal layer as the bottleneck features for the appended fully connected layer and the softmax layer for predicting frame emotions. We test on session 5 after training on session 1-4, and find the 12 th TDNN layer the best performance, therefore we use the 12 th layer output as the bottleneck embeddings for later experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Method",
      "text": "For parallel comparison with other methods  [7][9] [10], we train under a 5-fold cross validation strategy where each time we choose a session from IEMOCAP for testing, and the other four for training. The results are evaluated by the average of unweighted accuracy over five cross validation experiment runs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Bottleneck Layer Selection",
      "text": "Compare the prefinal, 12 th and 13 th layer as in Table  2 , we found that the 12 th layer has the best performance and prefinal have the worst. We hypothesis that is because in ASR, prefinal and 13 th layer are more specialized in speech recognition as they are closer to the final output layer, while the 12 th learns the general high-level acoustic features that helps emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Performance",
      "text": "Our model using the 12 th TDNN layer outperforms other current state-of-art methods. Table  3  to our best knowledge. The 5-fold cross validation unweighted accuracy is improved from 68.8%  [9]  to 71.7%.   [7]  63.9 Conv-LSTM  [9]  68.8 CTC-BLSTM  [10]  54",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Error Analysis",
      "text": "To study the model performance within each emotion category, we present a confusion matrix, shown in Figure  1 , by calculating the average confusion matrix of five cross validation experiments for our best model architecture, using TDNN 12 th layer as the bottleneck layer. The excitement category has a lower accuracy compared to other categories, in which 28% samples are confused with neutral utterances. We observed that the netral emotion is the most likely wrong prediction from all non-neutral categories. It might due to the fact that non-neutral utterances usually consist of a large proportion of frames carrying no emotional content. We also found the model confuses neutrality as",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "Our study shows transfer learning from ASR is a good strategy for emotion classification, and indicates potential feature overlap between speech-to-text and emotion recognition. Our method is limited in frame-level prediction, where frames are predicted first then aggregated into utterance level labels. The frame-level structure results in the ignorance of sequential information in emotion labels decoding. In future, we expect sequence models can predict at utterance-level and bring further performance improvements by considering sequential information for sequence decoding.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , by calculat-",
      "page": 3
    },
    {
      "caption": "Figure 1: Confusion Matrix for Transfer Learning Method Pre-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Recognition Technologies, Inc. and Columbia University": "2beigi@recotechnologies.com"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "[13]. Multi-modal methods can signiﬁcantly improve the emo-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "tion detection performance by incorporating lexical\nfeatures"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "from given transcripts with the acoustic features from audios"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "[14][15][16].However,\nin real application scenario,\ntranscripts"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "are often absent. Although ASR can provide transcripts in real"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "time to emotional speech data[16],\nit\nrequires large language"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "models loaded, and is computational costly when decoding se-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "quence. Therefore,\ntransfer\nlearning using ASR as the source"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "domain might be an efﬁcient solution in emotion detection to in-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "corporate textual features through high level features extracted"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "in ASR models."
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "Another challenge for emotion recognition is\nthat\nspeak-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "ers express emotions\nin different ways,\nin addition,\nenviron-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "ments can affect acoustic features. Speaker adaptation is useful"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "to capsulate speaker and environment speciﬁc information into"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "acoustic features.\niVector[17] based adaptation has been shown"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "fast and efﬁcient in speech recognition[18]. We employ i-vector"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "based speaker adaptation in emotion detection."
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "This paper proposes a transfer\nlearning method to adapt"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "ASR models\nin emotion recognition domain.\nThe model\nis"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "pre-trained on Tedlium2 dataset[5], with over 207 hours data,"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "and ﬁne tuned on 12 hours of emotional speech.\nThe model"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "architecture is TDNN-based[19][1][18] with input as speaker"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "adapted MFCC[1] features. Our experiments show the improve-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "ments in emotion detection using transfer\nlearning from ASR"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "to speech emotion recognition combined with speaker adapta-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "tion. The performance is evaluated on the benchmark dataset"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "Interactive Emotional Dyadic Motion Capture (IEMOCAP)[2],"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "with 71.7% unweighted accuracy among angry, happy, sad and"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "neutral under the 5-fold cross validation strategy. Our method"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "signiﬁcantly outperforms the state-of-art strategy[9]."
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "2. Related Work"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": ""
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "Most efﬁcient models in prior work are based on deep learn-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "ing models, which can learn high-level features from low-level"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "acoustic features. Lees work[7] showed the importance of long-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "range context effect, and signiﬁcantly improve the RNN results"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "over DNN model. This work had been staying state-of-art for"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "years with 63.89% unweighted accuracy(UA), until surpassed"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "by a hybrid approach of convolution layers and LSTM convo-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "lutional LSTM[9] with 68.8% accuracy. Previous literature has"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "seldomly discussed about TDNN architecture in emotion recog-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "nition. TDNN can efﬁciency capture temporal\ninformation as"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "RNN and LSTM do, but\nis faster for its parallelization ability"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "and lower computation costs during training[18], which is a de-"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "sired property when training on large-scale ASR data."
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "Many approaches[7]\n[10]are\nspeaker\nindependent where"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "features are normalized for individuals resulting in information"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "loss, while our work is conducted in speaker dependent context"
        },
        {
          "2Recognition Technologies, Inc. and Columbia University": "using full MFCC raw features combined with iVectors contain-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to map features to categories. This model": "",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "level\nlabels, and predicts",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "frame-level predictions through max likelihood by summing up",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "the results of frames.",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "3.2. Feature",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "Full MFCC features with all 40 coefﬁcients are computed at",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "each time index is used as",
          "is trained on frame-": "to neural network."
        },
        {
          "to map features to categories. This model": "",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "of mean normalization on MFCC, an 100-dimension iVector",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "is appended to MFCC features at each frame to encode mean-",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "offset",
          "is trained on frame-": ""
        },
        {
          "to map features to categories. This model": "described in [18].",
          "is trained on frame-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "efﬁcient TDNN-based architecture for ASR with features as the",
          "bottleneck embeddings as high level features learnt from ASR,": "a new fully connected dense layer is appended after the embed-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "combination of MFCC and iVectors, efﬁciently learned robust",
          "bottleneck embeddings as high level features learnt from ASR,": "ding layer for predicting the frame-level emotion labels, and a"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "representations among various speakers and environments. Our",
          "bottleneck embeddings as high level features learnt from ASR,": "softmax layer with four dimension outputs is used to predict"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "study uses bottleneck layers of this TDNN architecture to use",
          "bottleneck embeddings as high level features learnt from ASR,": "frame-level emotion. The model uses cross-entropy as the ob-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "high-level feature representations that reﬂect insights from ASR",
          "bottleneck embeddings as high level features learnt from ASR,": "jective function for frame-level classiﬁcation."
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "tasks, and ﬁne tunes on emotion datasets.",
          "bottleneck embeddings as high level features learnt from ASR,": "The output of this model is for frame unit rather than for ut-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "terance unit, we aggregate frame-level predictions, using maxi-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "3. Method",
          "bottleneck embeddings as high level features learnt from ASR,": "mum likelihood by adding the output vectors over frames, cor-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "responding to the highest valued dimension of\nthe sum of\nthe"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "3.1. Problem Deﬁnition",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "softmax layer output over all frames within the utterance"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "The emotion recognition problem is a classiﬁcation problem",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "when we represent emotion as categories\nrather\nthan dimen-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "4. Experiment"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "sional representations,",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "4.1. Dataset"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "D = {(X, z)}\n(1)",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "This work uses IEMOCAP, which contains 12 hours of audio"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "where where X are the acoustic features input and z is dimen-",
          "bottleneck embeddings as high level features learnt from ASR,": "data with scripted and improvised speech, performed by ten ac-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "sional output corresponding to the emotion prediction. We want",
          "bottleneck embeddings as high level features learnt from ASR,": "tors, one male and one female as a pair in ﬁve sessions. In train-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "to ﬁnd a function",
          "bottleneck embeddings as high level features learnt from ASR,": "ing and testing,\nfour categories angry, happy, sad and neutral"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "are selected out of ten categories, for a more balanced and efﬁ-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "D = {f : X → z}\n(2)",
          "bottleneck embeddings as high level features learnt from ASR,": "cient dataset, resulting in a ﬁnal collection of 4936 utterances,"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "each utterance has unique emotion label. This dataset consists"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "to map features to categories. This model\nis trained on frame-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "of ﬁve sessions, and the category distribution is as in Table 1."
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "level\nlabels, and predicts\nthe utterance labels by aggregating",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "frame-level predictions through max likelihood by summing up",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "Table 1: Emotion category distribution in IEMOCAP"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "the results of frames.",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "3.2. Feature",
          "bottleneck embeddings as high level features learnt from ASR,": "session\nang\nexc\nneu\nsad\ntotal"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "229\n143\n384\n194\n950\nses1"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "Full MFCC features with all 40 coefﬁcients are computed at",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "137\n210\n362\n197\n906\nses2"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "each time index is used as\ninput\nto neural network.\nInstead",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "240\n151\n320\n305\n1016\nses3"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "of mean normalization on MFCC, an 100-dimension iVector",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "327\n238\n258\n143\n966\nses4"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "is appended to MFCC features at each frame to encode mean-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "170\n299\n384\n245\n1098\nses5"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "offset\ninformation. The iVector extraction model\nis trained as",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "described in [18].",
          "bottleneck embeddings as high level features learnt from ASR,": "1103\n1041\n1708\n1084\n4936\ntotal"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "3.3. TDNN for ASR",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "TDNN is designed for capturing long term temporal dependen-",
          "bottleneck embeddings as high level features learnt from ASR,": "4.2. Pretraining on ASR"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "cies in lower computational costs compared to RNN. It operates",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "For pretraining on ASR, we use the feed-forward TDNN to cap-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "similar to a feed forward DNN architecture where lower layers",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "ture long term temporal dependencies from short\nterm feature"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "focus input content\nin narrow windows, and higher layers con-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "representations. Hidden activations are sub-sampled in order to"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "nects windows of selected previous layer nodes to process the",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "speed up the training[18].\nThe model\nis pre-trained on ASR"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "information from a wider context. Therefore its deeper layers",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "data, with 13 TDNN layers and output\nlayers for decoding se-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "can learn effective long term temporal dependencies without re-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "quence based on acoustic models. As neighboring activations"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "current connections which hurdle parallel computation.",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "shares largely overlapped input contexts, sub-sampling on acti-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "The pretraining on ASR follows the Kaldi\nrecipe for\nthe",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "vations can reduce computational costs without sacriﬁcing the"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "TED-Lium tasksi[20], where uses 13 TDNN layers and each",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "coverage range over\ninput\nframes.\nThe hyper parameters for"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "layer consists of 1024 activation nodes. The time stride of each",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "model architecture and training are chosen according to Kaldi"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "layer, which deﬁnes the window at which calculating over nodes",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "Tedlium2 TDNN recipe[20], which has been tuned properly on"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "at neighbor time steps in the past layer,\nis assigned as 0 for the",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "Tedlium2 dataset, achieving 7.6 word error rate(WER) on test"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "1st and 5th TDNN layer, as 1 from the 2nd and 4th\nlayer, and as",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "dataset after six epochs training. The parameters are optimized"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "3 for layers after since the 6th. A fully connected preﬁnal layer",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "through preconditioned stochastic gradient descent (SGD) up-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "of 1024 dimension follows the 13th TDNN layer before decod-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "dates, following the training recipe detailed in [22]."
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "ing output sequences.\nThe model\nis trained with a sequence-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "level objective function named lattice-free version of the max-",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "imum mutual\ninformation (LF-MMI)[21],\nfor maximising the",
          "bottleneck embeddings as high level features learnt from ASR,": "4.3. Training for Emotion Classiﬁcation"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "log-likelihood of the correct sequences.",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "After obtaining the pretraining model, we use 12th, 13th and the"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "preﬁnal\nlayer as the bottleneck features for the appended fully"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "3.4. Training on Emotion Labels",
          "bottleneck embeddings as high level features learnt from ASR,": ""
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "",
          "bottleneck embeddings as high level features learnt from ASR,": "connected layer and the softmax layer for predicting frame emo-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "Emotion labels are given for each utterance in the dataset. We",
          "bottleneck embeddings as high level features learnt from ASR,": "tions. We test on session 5 after training on session 1-4, and ﬁnd"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "label all\nthe frames using the utterance label where the frames",
          "bottleneck embeddings as high level features learnt from ASR,": "the 12th TDNN layer the best performance, therefore we use the"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "lie in. To train for emotion detection,\nthe 12th and 13th TDNN",
          "bottleneck embeddings as high level features learnt from ASR,": "12thlayer output as the bottleneck embeddings for later experi-"
        },
        {
          "ing speaker characteristics[17].\nPeddinti[18] has proposed an": "layers as well as the ASR preﬁnal layer are selected to produce",
          "bottleneck embeddings as high level features learnt from ASR,": "ments."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: Test Accuracy on Session 5 with different bottleneck",
      "data": [
        {
          "4.4. Evaluation Method": "For parallel comparison with other methods[7][9][10], we train"
        },
        {
          "4.4. Evaluation Method": "under a 5-fold cross validation strategy where each time we"
        },
        {
          "4.4. Evaluation Method": "choose a session from IEMOCAP for testing, and the other four"
        },
        {
          "4.4. Evaluation Method": "for\ntraining.\nThe results are evaluated by the average of un-"
        },
        {
          "4.4. Evaluation Method": "weighted accuracy over ﬁve cross validation experiment runs."
        },
        {
          "4.4. Evaluation Method": "5. Results"
        },
        {
          "4.4. Evaluation Method": "5.1. Bottleneck Layer Selection"
        },
        {
          "4.4. Evaluation Method": "Compare the preﬁnal, 12th and 13th layer as in Table 2, we found"
        },
        {
          "4.4. Evaluation Method": "that\nthe 12th\nlayer has the best performance and preﬁnal have"
        },
        {
          "4.4. Evaluation Method": "the worst. We hypothesis that\nis because in ASR, preﬁnal and"
        },
        {
          "4.4. Evaluation Method": "13th layer are more specialized in speech recognition as they are"
        },
        {
          "4.4. Evaluation Method": "closer to the ﬁnal output layer, while the 12th learns the general"
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": "high-level acoustic features that helps emotion recognition."
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": "Table 2: Test Accuracy on Session 5 with different bottleneck"
        },
        {
          "4.4. Evaluation Method": "layers"
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": "Bottleneck Layer\nTest Accuracy on Ses 5 (%)"
        },
        {
          "4.4. Evaluation Method": "63.4\nPreﬁnal"
        },
        {
          "4.4. Evaluation Method": "66.7\n13th TDNN"
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": "69.3\n12th TDNN"
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": "5.2. Model performance"
        },
        {
          "4.4. Evaluation Method": ""
        },
        {
          "4.4. Evaluation Method": "Our model using the 12th TDNN layer outperforms other cur-"
        },
        {
          "4.4. Evaluation Method": "rent state-of-art methods. Table 3 to our best knowledge. The"
        },
        {
          "4.4. Evaluation Method": "5-fold cross validation unweighted accuracy is improved from"
        },
        {
          "4.4. Evaluation Method": "68.8%[9] to 71.7%."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Test Accuracy on Session 5 with different bottleneck",
      "data": [
        {
          "performance improvements by considering sequential informa-": "tion for sequence decoding."
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "7. References"
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "Fundamentals\nof\nSpeaker\n[1] H.\nBeigi,\nRecognition.\nNew"
        },
        {
          "performance improvements by considering sequential informa-": "York:\nSpringer, 2011,\nISBN: 978-0-387-77591-3, http://www."
        },
        {
          "performance improvements by considering sequential informa-": "fundamentalsofspeakerrecognition.org."
        },
        {
          "performance improvements by considering sequential informa-": "[2] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "performance improvements by considering sequential informa-": "S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nIn-"
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "teractive emotional dyadic motion capture database,” Language"
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "Resources and Evaluation, vol. 42, no. 4, pp. 335–359, 2008,"
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "springerlink Online: DOI 10.1007/s10579-008-9076."
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "[3]\nF. Burkhardt, A.\nPaeschke, M. Rolfes, W.\nSendlmeier,\nand"
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "B. Weiss, “A database of german emotional speech,” in Proceed-"
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "ings of\nthe 9th European Conference on Speech Communication"
        },
        {
          "performance improvements by considering sequential informa-": "and Technology, vol. 5, Lisbon, Portugal, 2006."
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "[4]\nP. Jackson and S. ul Haq, “Surrey audio-visual expressed emotion"
        },
        {
          "performance improvements by considering sequential informa-": "(SAVEE) database,” 04 2011."
        },
        {
          "performance improvements by considering sequential informa-": "[5] A. Rousseau, P. Delglise,\nand Y. Estve,\n“Enhancing the TED-"
        },
        {
          "performance improvements by considering sequential informa-": "LIUM corpus with selected data for language modeling and more"
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "TED talks,” 05 2014."
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "[6] A. Nagrani, S. Albanie, and A. Zisserman, “Seeing voices and"
        },
        {
          "performance improvements by considering sequential informa-": "hearing\nfaces:\nCross-modal\nbiometric matching,”\nJun.\n18-22"
        },
        {
          "performance improvements by considering sequential informa-": "2018."
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "[7]\nJ. Lee and I. Tashev, “High-level feature representation using re-"
        },
        {
          "performance improvements by considering sequential informa-": "current neural network for speech emotion recognition,” Sep."
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "[8] Y. Kim, H. Lee, and E. M. Provost, “Deep learning for\nrobust"
        },
        {
          "performance improvements by considering sequential informa-": "feature generation in audiovisual emotion recognition,” pp. 3687–"
        },
        {
          "performance improvements by considering sequential informa-": "3691, May 26-31 2013."
        },
        {
          "performance improvements by considering sequential informa-": ""
        },
        {
          "performance improvements by considering sequential informa-": "[9] A. Satt, S. Rozenberg, and R. Hoory, “Eifﬁcient emotion recog-"
        },
        {
          "performance improvements by considering sequential informa-": "nition from speech using deep learning on spectrograms,” Aug."
        },
        {
          "performance improvements by considering sequential informa-": "20-24."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "tion from speech with recurrent neural networks,” arXiv preprint"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "arXiv:1701.08071, Jul. 2018."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[11]",
          "and P. Prihodko,\n“Emotion recogni-": "S. Latif, R. Rana, S. Younis, J. Qadir, and J. Epps, “Transfer learn-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "ing for\nimproving speech emotion classiﬁcation accuracy,” Sep."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "2-6."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[12]",
          "and P. Prihodko,\n“Emotion recogni-": "S. Ghosh, E. Laksana, L.-P. Morency, and S. Scherer, “Represen-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "tation learning for speech emotion recognition,” in Interspeech,"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "Sep. 2016."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[13]",
          "and P. Prihodko,\n“Emotion recogni-": "S. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "affective computing: From unimodal analysis to multimodal fu-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "sion,” Information Fusion, vol. 37, pp. 98–126, Sep. 2017."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[14]",
          "and P. Prihodko,\n“Emotion recogni-": "S. Tripathi\nand H. Beigi,\n“Multi-modal\nemotion\nrecognition"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "arXiv:1804.05788v1\non iemocap dataset using deep learning,”"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "[cs.AI], Apr 2018."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[15]",
          "and P. Prihodko,\n“Emotion recogni-": "P. Tzirakis, G. Trigeorgis, M. A. Nicolaou, B. W. Schuller, and"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "S. Zafeiriou, “End-to-End multimodal emotion recognition using"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "deep neural networks,” IEEE Journal of Selected Topics in Signal"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "Processing, vol. 11, pp. 1301–1309, Sep. 2017."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[16]",
          "and P. Prihodko,\n“Emotion recogni-": "J. Cho, R. Pappagari, P. Kulkarni,\nJ. Villalba, Y. Carmiel, and"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "N. Dehak, “Deep neural networks for emotion recognition com-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "bining audio and transcripts,” Sep. 2-6."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[17] N. Dehak, P. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet,",
          "and P. Prihodko,\n“Emotion recogni-": ""
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "“Front-end factor analysis for speaker veriﬁcation,” vol. 19, no. 4,"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "pp. 788–798, May 2011."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[18] V. Peddinti, D. Povey, and S. Khundanpur, “A time delay neural",
          "and P. Prihodko,\n“Emotion recogni-": ""
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "network architecture for efﬁcient modeling of long temporal con-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "texts,” in Interspeech, Sep. 2015."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[19] M. Sugiyama, H. Sawai, and A. Waibel, “Review of tdnn (time de-",
          "and P. Prihodko,\n“Emotion recogni-": ""
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "lay neural network) architectures for speech recognition,” vol. 1,"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "Jun 1991, pp. 582–585."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[20]",
          "and P. Prihodko,\n“Emotion recogni-": "I. GitHub,\n“A TDNN recipe\nfor\nautomatic\nspeech\nrecog-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "nition\ntraining\non\nted-lium,”\nWebsite.\n[Online].\nAvail-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "able:\nhttps://github.com/kaldi-asr/kaldi/blob/master/egs/tedlium/"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "s5 r2/local/chain/tuning/run tdnn 1g.sh"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[21] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar,",
          "and P. Prihodko,\n“Emotion recogni-": ""
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "X. Na, Y. Wang, and S. Khudanpur, “Purely sequence-trained neu-"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "ral networks for asr based on lattice-free mmi,” in Interspeech,"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "Sep. 2016."
        },
        {
          "[10] V. Chernykh, G. Sterling,": "[22] X. Zhang,",
          "and P. Prihodko,\n“Emotion recogni-": "J. Trmal, D. Povey,\nand S. Khudanpur,\n“Improving"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "deep neural network acoustic models using generalized maxout"
        },
        {
          "[10] V. Chernykh, G. Sterling,": "",
          "and P. Prihodko,\n“Emotion recogni-": "networks,” May 4–9."
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Fundamentals of Speaker Recognition",
      "authors": [
        "H Beigi"
      ],
      "year": "2011",
      "venue": "Fundamentals of Speaker Recognition"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076"
    },
    {
      "citation_id": "4",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2006",
      "venue": "Proceedings of the 9 th European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "5",
      "title": "Surrey audio-visual expressed emotion (SAVEE) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2011",
      "venue": "Surrey audio-visual expressed emotion (SAVEE) database"
    },
    {
      "citation_id": "6",
      "title": "Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks",
      "authors": [
        "A Rousseau",
        "P Delglise",
        "Y Estve"
      ],
      "year": "2014",
      "venue": "Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks"
    },
    {
      "citation_id": "7",
      "title": "Seeing voices and hearing faces: Cross-modal biometric matching",
      "authors": [
        "A Nagrani",
        "S Albanie",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Seeing voices and hearing faces: Cross-modal biometric matching"
    },
    {
      "citation_id": "8",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "9",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "Deep learning for robust feature generation in audiovisual emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "Eifficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "venue": "Eifficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition from speech with recurrent neural networks",
      "authors": [
        "V Chernykh",
        "G Sterling",
        "P Prihodko"
      ],
      "year": "2018",
      "venue": "Emotion recognition from speech with recurrent neural networks",
      "arxiv": "arXiv:1701.08071"
    },
    {
      "citation_id": "12",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "venue": "Transfer learning for improving speech emotion classification accuracy"
    },
    {
      "citation_id": "13",
      "title": "Representation learning for speech emotion recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Representation learning for speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "15",
      "title": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788v1[cs.AI]"
    },
    {
      "citation_id": "16",
      "title": "End-to-End multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts"
    },
    {
      "citation_id": "18",
      "title": "Front-end factor analysis for speaker verification",
      "authors": [
        "N Dehak",
        "P Kenny",
        "R Dehak",
        "P Dumouchel",
        "P Ouellet"
      ],
      "year": "2011",
      "venue": "Front-end factor analysis for speaker verification"
    },
    {
      "citation_id": "19",
      "title": "A time delay neural network architecture for efficient modeling of long temporal contexts",
      "authors": [
        "V Peddinti",
        "D Povey",
        "S Khundanpur"
      ],
      "year": "2015",
      "venue": "A time delay neural network architecture for efficient modeling of long temporal contexts"
    },
    {
      "citation_id": "20",
      "title": "Review of tdnn (time delay neural network) architectures for speech recognition",
      "authors": [
        "M Sugiyama",
        "H Sawai",
        "A Waibel"
      ],
      "year": "1991",
      "venue": "Review of tdnn (time delay neural network) architectures for speech recognition"
    },
    {
      "citation_id": "21",
      "title": "A TDNN recipe for automatic speech recognition training on ted-lium",
      "authors": [
        "I Github"
      ],
      "venue": "A TDNN recipe for automatic speech recognition training on ted-lium"
    },
    {
      "citation_id": "22",
      "title": "Purely sequence-trained neural networks for asr based on lattice-free mmi",
      "authors": [
        "D Povey",
        "V Peddinti",
        "D Galvez",
        "P Ghahrmani",
        "V Manohar",
        "X Na",
        "Y Wang",
        "S Khudanpur"
      ],
      "year": "2016",
      "venue": "Purely sequence-trained neural networks for asr based on lattice-free mmi"
    },
    {
      "citation_id": "23",
      "title": "Improving deep neural network acoustic models using generalized maxout networks",
      "authors": [
        "X Zhang",
        "J Trmal",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2009",
      "venue": "Improving deep neural network acoustic models using generalized maxout networks"
    }
  ]
}