{
  "paper_id": "2010.11985v2",
  "title": "Mtag: Modal-Temporal Attention Graph For Unaligned Human Multimodal Language Sequences",
  "published": "2020-10-22T18:58:50Z",
  "authors": [
    "Jianing Yang",
    "Yongxin Wang",
    "Ruitao Yi",
    "Yuying Zhu",
    "Azaan Rehman",
    "Amir Zadeh",
    "Soujanya Poria",
    "Louis-Philippe Morency"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-ofthe-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With recent advances in machine learning research, analysis of multimodal sequential data has become increasingly prominent. At the core of modeling this form of data, there are the fundamental research challenges of fusion and alignment. Fusion is the process of blending information from multiple modalities. It is usually preceded by alignment, which is the process of finding temporal relations between the modalities. An important research area that exhibits this form of data is multimodal language analysis, where sequential modalities of * Equal contribution 1 Code is available at https://github.com/ jedyang97/MTAG. Each circle represents a node from video/text/audio modalities, and the blue lines denote the learned attention weights (i.e. the thicker and darker a blue line is, the larger the attention weight). We observe high intensities between semantically correlated graph entities, such as \"Really Enjoy\" and the raise in eyebrow, which indicate positive sentiment. Note that our graphbased model learns multimodal interactions without prior alignment, and captures diverse types of interactions across multiple modalities all the same time. Edge types are not shown for visual clarity. language, vision, and acoustic are present. These three modalities carry the communicative information and interact with each other through time; e.g. positive word at the beginning of an utterance may be the cause of a smile at the end. When analyzing such multimodal sequential data, it is crucial to build models that perform both fusion and alignment accurately and efficiently by a) aligning arbitrarily distributed asynchronous modalities in an interpretable manner, b) efficiently accounting for short and long-range dependencies, c) explicitly modeling the inter-modal interactions between it was a very good movie the modalities while simultaneously accounting for intra-modal dynamics.\n\nIn this paper, we propose MTAG (Modal-Temporal Attention Graph). MTAG is capable of both fusion and alignment of asynchronously distributed multimodal sequential data. Modalities do not need to be pre-aligned, nor do they need to follow similar sampling rate. MTAG can capture interactions of various types across any number of modalities all at once, comparing to previous methods that model bi-modal interactions at a time  (Tsai et al., 2019a) . At its core, MTAG utilizes an efficient trimodal-temporal graph fusion operation. Coupled with our proposed dynamic pruning technique, MTAG learns a parameter-efficient and interpretable graph. In our experiments, we use two unaligned multimodal emotion recognition and sentiment analysis benchmarks: IEMOCAP  (Busso et al., 2008)  and CMU-MOSI  (Zadeh et al., 2016) . The proposed MTAG model achieves stateof-the-art performance with far fewer parameters. Subsequently, we visualize the learned relations between modalities and explore the underlying dynamics of multimodal language data. Our model incorporates all three modalities in both alignment and fusion, a fact that is also substantiated in our ablation studies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "Human Multimodal Language Analysis Analyzing human multimodal language involves learning from data across multiple heterogeneous sources that are often asynchronous, i.e. language, visual, and acoustic modalities that each uses a different sampling rate. Earlier works assumed multimodal sequences are aligned based on word boundaries  (Lazaridou et al., 2015; Ngiam et al., 2011; Gu et al., 2018; Dumpala et al., 2019; Pham et al., 2019)  and applied fusion methods for aligned sequences. To date, modeling unaligned multimodal language sequences remains understudied, except for  (Tsai et al., 2019a; Khare et al., 2020; Zheng et al., 2020) , which used cross-modal Transformers to model unaligned multimodal language sequences. However, the cross-modal Transformer module is a bi-modal operation that only account for two modalities' input at a time. In  Tsai et al. (2019a) , the authors used multiple cross-modal Transformers and applies late fusion to obtain trimodal features, resulting in a large amount of parameters needed to retain original modality information. Other works that also used cross-modal Transformer architecture for include  Yang et al. (2020) ;  Siriwardhana et al. (2020) . In contrast to the existing works, our proposed graph method, with very small amount of model parameters, can aggregate information from multiple (more than 2) modalities at early stage by building edges between the corresponding modalities, allowing richer and more complex representation of the interactions to be learned.\n\nGraph Neural Networks Graph Neural Network (GNN) was introduced in  (Gori et al., 2005; Scarselli et al., 2008)  with an attempt to extend deep neural networks to handle graph-structured data. Since then, there has been an increasing research interest on generalizing deep neural network's operations such as convolution  (Kipf and Welling, 2016; Schlichtkrull et al., 2017; Hamilton et al., 2017) , recurrence  (Nicolicioiu et al., 2019) , and attention  (Veličković et al., 2018)  to graph.\n\nRecently, several heterogeneous GNN methods  (Wang et al., 2019a; Wei et al., 2019; Shi et al., 2016)  have been proposed. The heterogeneous nodes referred in these works consist of uni-modal views of multiple data generating sources (such as movie metadata node, audience metadata node, etc.), whereas in our case the graph nodes represent multimodal views of a single data generating source (visual, acoustic, textual nodes from a single speaking person). In the NLP domain, multimodal GNN methods  (Khademi, 2020; Yin et al., 2020)  on tasks such as Visual Question Answering and Machine Translation. However, these settings still differ from ours because they focused on static images and short text which, unlike the multimodal video data in our case, do not exhibit long-term temporal dependencies across modalities.\n\nBased on these findings, we discovered there has been little research using graph-based methods for modeling unaligned, multimodal language sequences, which includes video, audio and text. In this paper, we demonstrate our proposed MTAG method can effectively model such unaligned, multimodal sequential data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mtag",
      "text": "In this section, we describe our proposed framework: Modal Temporal Attention Graph (MTAG) for unaligned multimodal language sequences. We describe how we formulate the multimodal data into a graph G(V, E), and the MTAG fusion operation that operates on G. In essence, our graph formulation by design alleviates the need for any hard alignments, and combined with MTAG fusion, allows nodes from one modality to interact freely with nodes from all other modalities at the same time, breaking the limitation of only modeling pairwise modality interactions in previous works. Figure  2  gives a high-level overview of the framework.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Node Construction",
      "text": "As illustrated in Figure  2 , each modality's input feature vectors are first passed through a modalityspecific Feed-Forward-Network. This allows feature embeddings from different modalities to be transformed into the same dimension. A positional embedding (details in Appendix A) is then added (separately for each modality) to each embedding to encode temporal information. The output of this operation becomes a node v i in the graph. Each node is marked with a modality identifier π i , where π i ∈ {Audio, Video, Text} in our case.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Edge Construction",
      "text": "In this section, we describe our design of modality edges and temporal edges. For a given node of a particular modality, its interactions with nodes from different modalities should be considered differently. For example, given a Video node, its interaction with an Audio node should be different from that with a Text node. In addition, the temporal order of the nodes also plays a key role in multimodal analysis  (Poria et al., 2017) . For example, a transition from a frown to a smile ( → → ) may imply a positive sentiment, whereas a transition from a smile to a frown ( → → ) may imply a negative sentiment. Therefore, interactions between nodes that appear in different temporal orders should also be considered differently. In GNNs, the edges define how node features are aggregated within a graph. In order to encapsulate the diverse types of node interactions, we assign edge types to each edge so that information can be aggregated differently on different types of edges. By indexing edges with edge types, different modal and temporal interactions between nodes can be addressed separately.\n\nMultimodal Edges. As we make no assumption about prior alignment of the modalities, the graph is initialized to be a fully connected graph. We use e ij to represent an edge from v i to v j . We assign e ij with a modality type identifier\n\nFor example, an edge pointing from a Video node to a Text node will be marked with type\n\nTemporal Edges. In addition to φ ij , we also assign a temporal label τ ij to each e ij . Depending on the temporal order of v i and v j connected by e ij , we determine the value of τ ij to be either of {past, present, future}. For nodes from the same modality, the temporal orders can be easily determined by comparing their order of occurrences. To determine the temporal orders for nodes across different modalities, we first roughly align the two modalities with our pseudo-alignment. Then the temporal order can be simply read out.\n\nPseudo-Alignment. As mentioned above, it is simple to determine the temporal edge types for nodes in a single modality. However, there is no clear definition of \"earlier\" or \"later\" across two modalities, due to the unaligned nature of our input sequences.\n\nTo this end, we introduce the pseudo-alignment heuristic that coarsely defines the past, present and future connections between nodes across two modalities. Given a node v i from one modality π i , our pseudo-alignment first determines a set of nodes V i,present in the other modality that can be aligned to v i and considered as \"present\". All nodes in the other modality that exists after V i,present are considered \"future\" V i,f uture , and all those before are considered V i,past . Once the coarse temporal order is established, the cross-modal temporal edge types can be easily determined. Figure  3  shows an example of such pseudo-alignment, and more details regarding the calculations can be found in Appendix A.2.  We first align the longer sequence to the shorter one as uniformly as possible. Then the aligned nodes from the longer sequence becomes the V i,present for node v i in the shorter sequence. V i,past and V i,f uture can then be determined accordingly.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Fusion And Pruning",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mtag Fusion",
      "text": "With our formulation of the graph, we design the MTAG fusion operation that can digest our graph data with various node and edge types, and thus model the modal-temporal interactions. An algorithm of our method is shown in Algorithm 1 and a visual illustration is given in Figure  4 . Specifically, for each neighbor node v j that has an edge incident into a center node v i , we compute a raw attention score β [h],i,j based on that edge's modality and temporal type:\n\nwhere [•||•] denotes the concatenation of two column vectors into one long column vector. The [h] index is used to distinguish which multi-head attention head is being used. Note that a\n\ndepends on both the modality and temporal edge types of e ji . This results in 27 edge types (9 types of modality interaction × 3 types of temporal interaction).\n\nWe normalize the raw attention scores over all neighbor nodes v j with Softmax so that the normalized attention weight sums to 1 to preserve the scale of the node features in the graph.\n\nThen, we perform node feature aggregation for each node v i following:\n\nAlgorithm 1: MTAG with edge pruning\n\ncalculate raw attention score using modality-and temporal-edge-type specific parameters:\n\n6 calculate node output feature\n\n7 calculate average attention weight across all heads α i,j = 1 H H h=1 (α [h],i,j ) 8 sort α i,j and delete the edges with the smallest k% average attention weight from\n\nwhere N i defines the neighbors of v i and hyperparameter H is the number of total attention heads. z i now becomes the new node embedding for node v i . After aggregation, v i transformed from a node with unimodal information into a node encoding the diverse modal-temporal interactions between v i and its neighbors (illustrated by the mixing of colors of the nodes in Figure  2 ).\n\nWe desgined the operation to have H multi-head attention heads because the heterogeneous input data of the multimodal graph could be of different scales, making the variance of the data high. Adding multi-head attention could help stabilize the behavior of the operation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dynamic Edge Pruning",
      "text": "Our graph formulation models interactions for all 27 edge types. This design results in a very large number of edges in the graph, making the computation graph difficult to fit into GPU memories. More importantly, when there are so many edges, it is hard to avoid some of these edges from inducing spurious correlations and distracting the model from focusing on the truly important interactions  (Lee et al., 2019; Knyazev et al., 2019) . To address these challenges, we propose to dynamically prune edges as the model learns the graph. Specifically, after each layer of MTAG, we have the attention weight α [h],i,j for each attention head h and for each edge e ij . We take the average of the attention weights over the attention heads:\n\nThen, we sort α ij and delete k% edges with the smallest attention weights, where k is a hyperparameter. These deleted edges will no longer be calculated in the next MTAG fusion layer. Our ablation study in Section 5.2 empirically verifies the effectiveness of this approach by comparing to no pruning and random pruning.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Graph Readout",
      "text": "At the end of the MTAG fusion process, we need to read out information scattered in the nodes into a single vector so that we can pass it through a classification head. Recall that the pruning process drops edges in the graph. If all edges incident into a node have been dropped, then it means that node was not updated based on its neighbors. In that case, we simply ignore that node in the readout process.\n\n(5) We readout the graph by averaging all the surviving nodes' output features into one vector. This vector is then passed to a 3-layer Multi-Layer-Perceptron (MLP) to make the final prediction.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "We empirically evaluate MTAG model on two datasets: IEMOCAP  (Busso et al., 2008)  and CMU-MOSI  (Zadeh et al., 2016) ; both are well-known datasets used by prior works  (Liang et al., 2018; Pham et al., 2019; Tsai et al., 2019b,a)  to benchmark multimodal emotion recognition and sentiment analysis.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset And",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines",
      "text": "For basleine evaluations, we use Early Fusion LSTM (EF-LSTM) and Late Fusion LSTM (LF-LSTM)  (Tsai et al., 2019a)  as baselines. In addition, we compare our model against similar methods as in previous works  (Tsai et al., 2019a) , which combine a Connectionist Temporal Classification (CTC) loss  (Graves et al., 2006)  with the preexisting methods such as EF-LSTM, MCTN  (Pham et al., 2019) , RAVEN  (Wang et al., 2019b) .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "Shown in Table  2  and Table 3  Qualitative Analysis The attention weights on the graph edges forms a natural way to interpret our model. We visualize the edges to probe what MTAG has learned. The following case study is a randomly selected video clip from the CMU-MOSI validation set. We observe the phenomena shown below is a general trend.\n\nIn Figure  5 , we show an example of the asymmetric bi-modal relations between vision and text. We observe that our model picks on meaningful relations between words such as \"I really enjoyed\" and facial expressions such as raising eyebrow, highlighted in the red dashed boxes in Figure  5a . Our model can also learn long-range correlation between \"I really enjoyed\" and head nodding. Interestingly, we discover that strong relations that are not detected by vision-to-text edges can be recovered by the text-to-vision edges. This advocates the design of the multi-type edges, which allows the model to learn different relations independently that can complement one another.\n\nFigure  1  gives a holistic view of the attention weights among all three modalities. We observe a pattern where almost all edges involve the text modality. A possible explanation for this observation is that the text is the dominant modality with respect to the sentiment analysis task. This hypothesis is verified by the ablation study in Sec. 5.3. Meanwhile, there appears to be very small amount of edges connecting directly between vision and audio, indicating that there might be little meaningful correlation between them. This resonates with our ablation studies in Table  5 , where vision and audio combined produce the lowest bi-modal performance. Under such circumstance, our MTAG learns to kill direct audio-vision relations and instead fuse their information indirectly using the text modality as a proxy, whereas previous methods such as MulT keeps audio-vision attentions alive along the way, introducing possible spurious relations that could distract model learning.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation study using unalgined CMU-MOSI dataset. MTAG Full Model implements multimodal temporal edge types, adopts TopK edge pruning that keeps edges with top 80% edge weights, and includes all three modalities as its input. Table  5  shows the performance. We present research questions (RQs) as follows and discuss how ablation studies address them.\n\n5.1 RQ1: Does using 27 edge types help?\n\nWe first study the effect of edge types on our model performance. As we incrementally add in multimodal and temporal edge types, our model's performance continues to increase. The model with 27 edge types performs the best under all metrics. By dedicating one attention vector a φ ji ,τ ji to each edge, MTAG can model each complex relation individually, without having one relation interfering another. As shown in Figure  5  and Table  5 , such design enhances multimodal fusion and alignment, helps maintain long-range dependencies in multimodal sequences, and yields better results.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Rq2: Does Our Pruning Method Help?",
      "text": "We compare our TopK edge pruning to no pruning and random pruning to demonstrates it effectiveness. We find that TopK pruning exceeds both no pruning and random pruning models in every aspect. It is clear that, by selectively keeping the top 80% most important edges, our model learns more   meaningful representations than randomly keeping 80%. Our model also beats the one where no pruning is applied, which attests to our assumption and observation from previous work  (Lee et al., 2019; Knyazev et al., 2019)  that spurious correlations do exist and can distract model from focusing on important interactions. Therefore, by pruning away the spurious relations, the model learned a better representation of the interactions, while using significantly fewer computation resources.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Rq3: Are All Modalities Helpful?",
      "text": "Lastly, we study the impact of different modality combinations used in our model. As shown in Table 5, we find that adding a modality consistently brings performance gains to our model. Through the addition of individual modalities, we find that adding the text modality gives the most significant performance gain, indicating that text may be the most dominant modality for our task. This can also be qualitative confirmed by seeing the concentrated edge weights around text modality in Figure  1 . This observation also conforms with the observations seen in prior works  (Tsai et al., 2019a; Pham et al., 2019) . On the contrary, adding audio only brings marginal performance gain. Overall, this ablation study demonstrates that all modalities are beneficial for our model to learn better multimodal representations.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented the Modal-Temporal Attention Graph (MTAG). We showed that MTAG is an interpretable model that is capable of both fusion and alignment. It achieves similar to SOTA performance on two publicly available datasets for emotion recognition and sentiment analysis while utilizing substantially lower number of parameters than a transformer-based model such as MulT. Figure  6 : Examples of pseudo-align heuristic to coarsely define past, present and future relationships between two unaligned modalities. We try to spread and match the two modalities as much as uniformly possible (the top figure). When the shorter modality contains more and more nodes, we align as many nodes from the shorter sequence as possible with a minimum alignment window size of 2 to the longer sequence, and the rest nodes from the shorter sequence are aligned with window size of 1 (the bottom figure).",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A Appendix",
      "text": "For node v i in π i , in order to determine the \"present\" nodes V present in a different modality, we draw an analogy from 1D convolution operation. We are given two sequences of different lengths, and we can treat the longer sequence as input and shorter sequence as output to a Conv1D operation. Our goal is to find a feasible stride and kernel size that aligns the input and output. The kernel size defines how many nodes from the longer sequence to be aligned as \"present\" to each node from the shorter sequence. The stride size defines how far away such alignments should spread in time. We do not consider any padding and have the following equation in Conv1D operation:\n\nwhere M and N are the sequence lengths of the output and input to a Conv1D operator, respectively. W is the kernel size and S the stride size. From Eq. 8, we can further write the relationship between W and S as W = M -(N -1) * S. It is clear that the minimum stride size is 1 to a Conv1D operation, and the maximum is M N -1 in order to keep W positive. We take the average of the minimum and maximum possible values of S as our stride size. In case that N > M 2 , we set window size as 2 and stride as 2. We then find the maximal number of nodes from N that can have kernel size of 2, and the rest of the nodes will have kernel size of 1. Eq. 9 shows our kernel size and stride size calculation and Figure . 6 illustrates our pseudoalignment heuristic.\n\nA.3 Model Efficiency Number of Parameters. We compare the parameter efficiency of our model against the SOTA model, the Multimodal Transformer (MulT)  (Tsai et al., 2019a) . We first look at the total number of parameters used by the two models.   uses -MTAG has a much smaller parameter search space for the optimizer, resulting in faster training and earlier convergence.\n\nTraining time. We also compare how fast our model runs against MulT. Specifically, under the same condition, we calculate the time it takes for each model to run training for 1 epoch. Table  7  shows the details. We can see that our model runs significantly faster than MulT on both benchmarks, which can be attributed to our light-weight model design (as shown in Table  8 ). Meanwhile, our edge pruning also reduces the number of computation by throwing away edges that are deemed less important by the model, thus improving the run-time of our model.\n\nOverall Efficiency. From the perspective of training time, number of parameters used, and convergence analysis, it is clear that our model is capable of achieving better results while using much smaller amount of computational resources than the previous state of the art.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.4 Hyperparameters",
      "text": "We elaborate on the technical details including hyperparameter settings in Table  6 . We conduct a basic grid search to find good hyperparameters such as initial learning rate, number of MTAG layers etc. We use Adam as our optimizer and decays the learning rate by half whenever the validation loss plateaus. Notice that we are using a design that roughly yields a model with a similar structure as in previous works such as MulT. Nevertheless, we still manage to use far less number of parameters during optimization. We use one NVIDIA GTX 1080 Ti for training and evaluation. In addition, the model and hyperparameters we use for ablation study are the same as the ones used for the main experiment, both of which are conducted on CMU-MOSI.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.5 Number Of Parameters Comparison",
      "text": "For a fair comparison on number of parameters between MTAG and MulT, we use the same number of layers and attention heads for both models (i.e. 6 layers of MulT with 4 attention heads). A detailed comparison is shown in Table  8 .",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example visualization of tri-modal Modal-",
      "page": 1
    },
    {
      "caption": "Figure 2: The 3-stage MTAG framework: Node Construction, Edge Construction and Fusion+Pruning. [Node",
      "page": 2
    },
    {
      "caption": "Figure 2: gives a high-level overview of the",
      "page": 3
    },
    {
      "caption": "Figure 2: , each modality’s input",
      "page": 3
    },
    {
      "caption": "Figure 3: An example of the pseudo-alignment be-",
      "page": 4
    },
    {
      "caption": "Figure 4: Visualization of the MTAG operation around",
      "page": 5
    },
    {
      "caption": "Figure 5: , we show an example of the asymmet-",
      "page": 7
    },
    {
      "caption": "Figure 1: gives a holistic view of the attention",
      "page": 7
    },
    {
      "caption": "Figure 5: and Table 5, such",
      "page": 7
    },
    {
      "caption": "Figure 5: We visualized the asymmetric attention weights of the text-to-vision and vision-to-text edges for one of",
      "page": 8
    },
    {
      "caption": "Figure 1: This observation also conforms with the observa-",
      "page": 8
    },
    {
      "caption": "Figure 6: Examples of pseudo-align heuristic to",
      "page": 11
    },
    {
      "caption": "Figure 7: Convergence comparison between MTAG",
      "page": 11
    },
    {
      "caption": "Figure 7: gives a comparison be-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MulT": "MTAG"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "2",
      "title": "Covarep-a collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "3",
      "title": "Audio-visual fusion for sentiment classification using cross-modal autoencoder",
      "authors": [
        "Sri Harsha Dumpala",
        "Imran Sheikh",
        "Rupayan Chakraborty",
        "Sunil Kumar"
      ],
      "year": "2019",
      "venue": "Proc. Neural Inf. Process. Syst.(NIPS)"
    },
    {
      "citation_id": "4",
      "title": "A new model for learning in graph domains",
      "authors": [
        "Marco Gori",
        "Gabriele Monfardini",
        "Franco Scarselli"
      ],
      "year": "2005",
      "venue": "Proceedings. 2005 IEEE International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "5",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "6",
      "title": "Multimodal affective analysis using hierarchical attention strategy with word-level alignment",
      "authors": [
        "Yue Gu",
        "Kangning Yang",
        "Shiyu Fu",
        "Shuhong Chen",
        "Xinyu Li",
        "Ivan Marsic"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1207"
    },
    {
      "citation_id": "7",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "Will Hamilton",
        "Zhitao Ying",
        "Jure Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Facial expression analysis",
      "year": "2017",
      "venue": "Facial expression analysis"
    },
    {
      "citation_id": "9",
      "title": "Multimodal neural graph memory networks for visual question answering",
      "authors": [
        "Mahmoud Khademi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.643"
    },
    {
      "citation_id": "10",
      "title": "Multi-modal embeddings using multitask learning for emotion recognition",
      "authors": [
        "Aparna Khare",
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2020",
      "venue": "Multi-modal embeddings using multitask learning for emotion recognition",
      "doi": "10.21437/interspeech.2020-1827"
    },
    {
      "citation_id": "11",
      "title": "Semisupervised classification with graph convolutional networks",
      "authors": [
        "N Thomas",
        "Max Kipf",
        "Welling"
      ],
      "year": "2016",
      "venue": "Semisupervised classification with graph convolutional networks",
      "arxiv": "arXiv:1609.02907"
    },
    {
      "citation_id": "12",
      "title": "Understanding attention and generalization in graph neural networks",
      "authors": [
        "Boris Knyazev",
        "Graham Taylor",
        "Mohamed Amer"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Combining language and vision with a multimodal skip-gram model",
      "authors": [
        "Angeliki Lazaridou",
        "Nghia The Pham",
        "Marco Baroni"
      ],
      "year": "2015",
      "venue": "Combining language and vision with a multimodal skip-gram model",
      "arxiv": "arXiv:1501.02598"
    },
    {
      "citation_id": "14",
      "title": "Selfattention graph pooling",
      "authors": [
        "J Lee",
        "Inyeop Lee",
        "Jaewoo Kang"
      ],
      "year": "2019",
      "venue": "ICML"
    },
    {
      "citation_id": "15",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "Paul Pu Liang",
        "Ziyin Liu",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Multimodal language analysis with recurrent multistage fusion",
      "arxiv": "arXiv:1808.03920"
    },
    {
      "citation_id": "16",
      "title": "Multimodal deep learning",
      "authors": [
        "Jiquan Ngiam",
        "Aditya Khosla",
        "Mingyu Kim",
        "Juhan Nam",
        "Honglak Lee",
        "Andrew Ng"
      ],
      "year": "2011",
      "venue": "ICML"
    },
    {
      "citation_id": "17",
      "title": "Recurrent space-time graph neural networks",
      "authors": [
        "Andrei Nicolicioiu",
        "Iulia Duta",
        "Marius Leordeanu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "19",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "21",
      "title": "The graph neural network model",
      "authors": [
        "Franco Scarselli",
        "Marco Gori",
        "Ah Chung Tsoi",
        "Markus Hagenbuchner",
        "Gabriele Monfardini"
      ],
      "year": "2008",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "22",
      "title": "Modeling relational data with graph convolutional networks",
      "authors": [
        "Michael Schlichtkrull",
        "Thomas Kipf",
        "Peter Bloem",
        "Rianne Van Den",
        "Ivan Berg",
        "Max Titov",
        "Welling"
      ],
      "year": "2017",
      "venue": "Modeling relational data with graph convolutional networks",
      "arxiv": "arXiv:1703.06103"
    },
    {
      "citation_id": "23",
      "title": "A survey of heterogeneous information network analysis",
      "authors": [
        "Chuan Shi",
        "Yitong Li",
        "Jiawei Zhang",
        "Yizhou Sun",
        "S Yu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "24",
      "title": "Jointly fine-tuning \"bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "Andrew Reis"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning \"bert-like\" self supervised models to improve multimodal speech emotion recognition"
    },
    {
      "citation_id": "25",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1656"
    },
    {
      "citation_id": "26",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Learning factorized multimodal representations"
    },
    {
      "citation_id": "27",
      "title": "Graph Attention Networks. International Conference on Learning Representations",
      "authors": [
        "Petar Veličković",
        "Guillem Cucurull",
        "Arantxa Casanova",
        "Adriana Romero",
        "Pietro Liò",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "Graph Attention Networks. International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "Heterogeneous Graph Attention Network",
      "authors": [
        "Xiao Wang",
        "Houye Ji",
        "Chuan Shi",
        "Bai Wang",
        "Yanfang Ye",
        "Peng Cui",
        "Philip Yu"
      ],
      "year": "2019",
      "venue": "The World Wide Web Conference on -WWW '19",
      "doi": "10.1145/3308558.3313562"
    },
    {
      "citation_id": "29",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Yansen Wang",
        "Ying Shen",
        "Zhun Liu",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia",
      "doi": "10.1145/3343031.3351034"
    },
    {
      "citation_id": "31",
      "title": "Cmbert: Cross-modal bert for text-audio sentiment analysis",
      "authors": [
        "Kaicheng Yang",
        "Hua Xu",
        "Kai Gao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, MM '20",
      "doi": "10.1145/3394171.3413690"
    },
    {
      "citation_id": "32",
      "title": "A novel graph-based multi-modal fusion encoder for neural machine translation",
      "authors": [
        "Yongjing Yin",
        "Fandong Meng",
        "Jinsong Su",
        "Chulun Zhou",
        "Zhengyuan Yang",
        "Jie Zhou",
        "Jiebo Luo"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.273"
    },
    {
      "citation_id": "33",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "34",
      "title": "Cross-modality relevance for reasoning on language and vision",
      "authors": [
        "Chen Zheng",
        "Quan Guo",
        "Parisa Kordjamshidi"
      ],
      "year": "2020",
      "venue": "Cross-modality relevance for reasoning on language and vision"
    }
  ]
}