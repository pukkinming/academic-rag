{
  "paper_id": "2201.06868v1",
  "title": "A Study On The Ambiguity In Human Annotation Of German Oral History Interviews For Perceived Emotion Recognition And Sentiment Analysis",
  "published": "2022-01-18T10:53:07Z",
  "authors": [
    "Michael Gref",
    "Nike Matthiesen",
    "Sreenivasa Hikkal Venugopala",
    "Shalaka Satheesh",
    "Aswinkumar Vijayananth",
    "Duc Bach Ha",
    "Sven Behnke",
    "Joachim Köhler"
  ],
  "keywords": [
    "emotion recognition",
    "sentiment analysis",
    "language",
    "oral history",
    "speech emotion recognition",
    "facial emotion recognition",
    "annotation",
    "ambiguity"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "For research in audiovisual interview archives often it is not only of interest what is said but also how. Sentiment analysis and emotion recognition can help capture, categorize and make these different facets searchable. In particular, for oral history archives, such indexing technologies can be of great interest. These technologies can help understand the role of emotions in historical remembering. However, humans often perceive sentiments and emotions ambiguously and subjectively. Moreover, oral history interviews have multi-layered levels of complex, sometimes contradictory, sometimes very subtle facets of emotions. Therefore, the question arises of the chance machines and humans have capturing and assigning these into predefined categories. This paper investigates the ambiguity in human perception of emotions and sentiment in German oral history interviews and the impact on machine learning systems. Our experiments reveal substantial differences in human perception for different emotions. Furthermore, we report from ongoing machine learning experiments with different modalities. We show that the human perceptual ambiguity and other challenges, such as class imbalance and lack of training data, currently limit the opportunities of these technologies for oral history archives. Nonetheless, our work uncovers promising observations and possibilities for further research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Oral history archives are often large audiovisual data repositories composing numerous interviews of contemporary witnesses to historical events. Deep learning can help to make these archives more accessible quantitatively and qualitatively. A prominent example in recent years is automatic speech recognition for transcription of oral history interviews. However, many potential deep learning applications for oral history interviews are still untapped. In a recent survey,  Pessanha and Salah (2022)  discuss potential applications of computational technologies for oral history archives. Among other aspects, the authors point out the potential benefits of these technologies in understanding the changes in emotions during remembering, storytelling, and conversing. In conjunction with the transcriptions, researchers can better infer not only what is being said but also how. In a recent research project, we study sentiment analysis and emotion recognition for German oral history interviews as the foundation for such complex search and indexing approaches for archives. Various challenges arise when transferring research results to real-world, \"in-the-wild\" applications. As with many AI-based approaches, suitable representative training data of adequate scale is one of the key challenges. For natural data sets, the current gold stan-dard for data annotation is to use other people's perception of emotion as the learning target, cf.  (Schuller, 2018) . However, even the annotation is a significant challenge since it is ambiguous and subjective. Emotions actually felt by the recorded persons and the emotions perceived by annotators may differ-and human recognition rates usually do not exceed 90 %, cf. Akc ¸ay and Oguz (2020). We assumed this value to be an upper limit.  Dupré et al. (2020)  compared the emotion recognition performance of humans and eight commercial systems using facial expression videos. The experiments show an overall human recognition accuracy of 72 % for the six basic Ekman emotions classes  (Ekman et al., 1980) . A 48-62 % range in recognition accuracy was observed for eight different tested commercial systems. The authors found that the machines' accuracy was consistently lower for spontaneous expressions.  Krumhuber et al. (2020)  studied human and machine emotion recognition using fourteen different dynamic facial expressions data sets-nine with posed/acted and five with spontaneous emotions. For posed emotions, they report mean human recognition scores of roughly 60-80 %. However, for the spontaneous five corpora, the mean human scores are roughly 35-65 %. Human-annotated training data is the crucial building block for emotion recognition and sentiment analysis machine learning systems. However, since the human perception of spontaneous emotions is ambiguous and subjective, we address this issue for oral history interviews in this paper. We investigate to what extent different persons perceive and annotate the emotions and sentiment of interviewees in oral history recordings differently, comparing annotations of three different persons on a German oral history data set. We believe this contributes to assessing the general capabilities of such approaches for oral history interviews. With initial experiments on three different modalities, we further study the influence of the annotation ambiguity and class imbalance for these tasks. We uncover challenges of sentiment analysis and emotion recognition for the oral history use case that need to be addressed in further research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Hdg Zeitzeugenportal",
      "text": "Zeitzeugenportal 1 (Portal of Oral History) is a German online service by Haus der Geschichte (House of the History) Foundation (HdG) that offers a central collection of contemporary German oral history interviews. More than 8,000 clips from around 1,000 interviews can currently be found at Zeitzeugenportal.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Mining For Oral History",
      "text": "In the research project Multi-Modal Mining of German Oral History Interviews for the Indexing of Audiovisual Cultural Heritage, Fraunhofer IAIS cooperates HdG to investigate complex search modalities for indexing oral history interviews. These audiovisual interviews illustrate the individual processing of history and demonstrate the multiperspectivity of personal views and experiences. Emotions are an important factor in the memory process, so automated analysis can help better understand the role emotions play in historical remembering.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Hdg Oral History Data Set",
      "text": "We selected 10 hours of German oral history interviews from the HdG Zeitzeugenportal for our experiments. Our HdG data set comprises 164 different interview videos of 147 interviewees. The selected interviews were recorded between 2010 and 2020. Thus, the selection is representative of the more recent videos on the portal-including both professional and nonprofessional speakers. In addition, we aimed to represent different emotions in the selection and create a heterogeneous data set in terms of age and gender. For prepossessing the HdG data set for annotation, we apply our current automatic transcription system Fraunhofer IAIS Audio Mining  (Schmidt et al., 2016)  with our latest robust broadcast ASR model to create a raw ASR transcript, including punctuation. We use the ASR result to chunk the interviews into short segments at the longest speech pauses until we obtain segments of 30 seconds or less.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Annotation Of Perceived Emotions And Sentiment In Oral History",
      "text": "As discussed, human perceptions of emotion and sentiment are often subjective, ambiguous and may differ greatly from the speaker-internal state. We use the phrase recognition of perceived emotion to emphasize this issue and how machine learning systems are trained on such annotated data. Such systems merely reproduce human decoding competence that works with different levels to decode emotions: the verbal (text), para verbal (voice), and nonverbal (face) level, similar to the Shannon-Weaver Model of Communication  (Shannon and Weaver, 1949) .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion And Sentiment Annotation",
      "text": "The annotation of emotion and sentiment is done in the same pass and on the same segments as correction of ASR transcripts. We use the six Ekman classes  (Ekman et al., 1980)  for the raw emotion annotation: happiness, sadness, anger, surprise, disgust, and fear. Per segment, the three annotators assign a score on a 4point Likert scale from 0 (no perception) to 3 (strong) for each of the six emotion classes following  (Zadeh et al., 2018) . The annotation is done independently for each emotion class so that multiple emotions can appear in each segment to different degrees. Similar to the emotions, sentiment annotation is done on a Likert scale from -3 (very negative) to 3 (very positive).\n\nFigure  1  shows the distribution of annotated scores per emotion class for each of the three annotators. The distribution follows a pattern expected for natural, realworld data: A neutral score dominates for all emotions. With increasing score, the segment-count decreases.\n\nAlthough emotions play a decisive role in remembering, contemporary witnesses are often very composed when narrating. Therefore, a strong expression of emo- Figure  2  presents the histogram of for the sentiment scores. As for emotions, the neutral score is most dominant. Unlike many other unstructured, real-world data sets, our data have negative sentiment more pronounced. This is likely due to the nature of the interviews: many interviews cover war and post-war experiences when Germany was divided into two states.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Correlation Analysis Of Annotation Pairs",
      "text": "Table  2  shows the class-wise relationship between the annotation for each pair of annotators in terms of correlation. We use the Spearman rank-order correlation coefficient measuring the monotony instead of a linear relationship between two annotators. Overall, the values for each annotator pair are in a similar range of values with no strong outliers. Therefore, we assume no fundamentally different understanding of the task or approach to annotation for any of the three annotators. Sentiment has the strongest correlation among all classes. Thus, the annotators seemed to have comparatively the same perceptions regarding the sentiment. However, the correlation is just above moderate, with a mean value of 0.63, indicating some substantial differences between all three annotators. Emotion is often considered more ambiguous and subjective than sentiment, as evidenced by the systematically lower correlation of these classes. Thus, there seem to be greater differences in perception or interpretation of emotions in our interviews. Happiness and Table  2 : Spearman rank-order correlation coefficients between the annotated labels of two transcribers.\n\nsadness have the highest correlation coefficient with 0.55 and 0.47, respectively. Even if there is no consensus, we assume a fundamental agreement in a sufficient number of segments.\n\nThe annotators seemed to have severely different perceptions for the other four emotion classes-with surprise having the lowest correlation. Since even two humans seem to have only a conditionally identical perception for these emotions, we hypothesize that this ambiguity in annotation severely limits the recognition performance for oral history interviews-at least using these predefined classes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Inter-Class Correlation Analysis",
      "text": "In a further correlation analysis, we investigate the cooccurrence of different emotions and sentiment. We combine the three annotations for this analysis by taking the arithmetic mean for each segment. A correlation matrix for the different classes is shown in Figure  3 . A moderate correlation exists between emotions and sentiment. In particular, happiness and a positive sentiment have a moderate correlation. An analogous correlation exists between negative sentiment and anger, disgust, and fear. In most cases, the correlation between the emotion classes is in the range of coincidence. Exceptions are disgust with anger (0.47) and fear with sadness (0.36). Thus, these class pairs seem to occur together more than just by coincidence and could be of interest for a detailed, qualitative analysis of the oral history interviews. We illuminate possible causes for these correlations by surveying the annotators.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Qualitative Survey Of Annotators",
      "text": "In a qualitative survey, the annotators reported various challenges. One challenge is that the narrative structure of oral history interviews has different levels. Accordingly, emotions become visible in different ways, such as those that arise during remembering or reported emotional situations. In terms of the different emotions, the annotators agreed that the given Ekman classes are insufficient to reflect the complexity of emotions in oral history interviews. Nuances of emotions do not fit into the six categories. Therefore, the persons intuitively combined multiple emotions to represent more complex emotions, such as hate (disgust + anger), despair/helplessness (fear + sadness), scorn (happiness + disgust), and overwhelm (happiness + surprise) in the annotation. For example, overwhelm was identified as an important emotion in some interviews in which interviewees have talked about the Fall of the Berlin Wall. In combination, disgust and anger occurred more frequently in narratives reporting oppression or persecution.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mapping To Single-Label Data",
      "text": "The mean scores of the raw annotations were to corresponding classes for classification-system training.\n\nOur goal for the initial experiments was to keep it simple and compatible with common data sets to better understand the effects of ambiguity during training. Therefore, we aim to classify only the most prevalent emotion (single-label). For this, we use the arithmetic mean of the three annotations and proceed as follows:\n\nIf the mean scores of all emotion classes are below 0.5, we assign this segment to neutral. This aims to consider only emotion classes with trustworthy annotation, where at least two of three annotators have given a score of 1 (0.6 on average), or at least one person has given a score of 2 and above. For non-neutral segments, we choose the class with the highest score. In exceptional cases, if two or more classes have the same score above the threshold, we mark them as ambiguous and do not use them in the current training. For the sentiment score s, we apply a similar threshold and mapping: negative, if s [-3, -0.5]; positive, if s [0.5, 3]; neutral, if s (-0.5, 0.5).\n\nFigure  4  shows the shares of each class in the entire HdG data set for both emotion and sentiment. Overall, the HdG data set is heavily imbalanced-both for the emotion and sentiment tasks. In the following three sections, we report results of initial, ongoing experiments and analysis with machine learning systems on our HdG data set. Experiments are performed on three different modalities: Speech, facial expressions, and text.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Text-Based Sentiment And Emotion Recognition",
      "text": "Sentiment analysis deals with mining opinions from an observer's point of view on content such as videos, speech, text, images. Opinion mining can be classified as polarity mining (sentiment polarity mining) and emotion mining.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "Sentiment and emotion analysis have been explored in various fields ranging from neuroscience, psychology, to human-machine interface. Comprehensive survey papers  (Pang and Lee, 2007; Kao et al., 2009; Salah et al., 2019; Norambuena et al., 2019; Hemmatian and Sohrabi, 2019; Yadollahi et al., 2017)  cover various approaches and methods on sentiment and emotion analysis tasks. Some of the comprehensive works on emotion analysis on text data are   al. (2013) . These works mainly consider that a document or a sentence consists of a single emotion. Only a few approaches deal with multi-label emotion analysis tasks  Bhowmick (2009; Liew and Turtle (2016; Khanpour and Caragea (2018) , and Schoene (2020).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology And Implementation",
      "text": "The pipeline used in our approach for sentiment analysis and emotion recognition starts with a BERT model to extract the embeddings from the tokenized text segments. We feed them into a classifier head consisting of two ReLU layers with a dropout layer in-between.\n\nWe use the bert-base-german-cased pre-trained model 2 as our base model. We apply a multi-stage training approach using the German part of the CMU-MOSEAS  (Zadeh et al., 2020)  data set, mapped to single-label, and the HdG data set for fine-tuning in subsequent stages. In the first stage, we use the German CMU-MOSEAS subset comprising 1,000 videos with 480 distinct speakers distributed across 250 topics and 10,000 annotated sentences. In the second stage, we fine-tune the model using the HdG data set. We use the raw ASR transcriptions from the pre-processing and not the humancorrected transcripts as inputs in the second stage. We aim to use the model as a subsequent analysis system after automatic transcription of large oral history data collections. On average, the HdG data set has a 16 to 17 % word error rate with our ASR system. To handle the class imbalance issue, we estimate the class weights using the compute class weights function from the sklearn library that uses a heuristic inspired by logistic regression.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Inference",
      "text": "The results of the sentiment and emotion classification are presented in Table  3 . Our approach achieves a decent accuracy on the HdG sets for sentiment, however, only a low accuracy for the emotion recognition. The respective confusion matrices for both models are shown in Figure  5 . For the sentiment model, we see only a few segments confused between all polarities. However, the emotion recognition model has only learned to distinguish between neutral and happiness. Interestingly, we observe a slightly increased misclassification of the actual class sadness with fear. As our previous correlation analysis of the human annotations  in Figure  3  has shown, these two classes have an increased correlation. The reason is that the annotators often intuitively combine these two emotions to express other emotions such as despair or helplessness. Therefore, this misclassification may not be a system failure but a limitation of the single-label approach. Overall, recognizing emotions from oral history interviews on text alone seems very limited. Nevertheless, interesting observations emerge that deserve further research. This research should reveal whether the poor performance is due to the character of the interviews, the heavy class imbalance in training, or the modality text not conveying emotions appropriately without additional modalities. It might also be that the main reason is the ambiguity of the human annotation we observed on our data. We observe very similar results with the other modalities recognizing seven emotion classes. Therefore, we investigate these modalities with a subset of the classes in the following sections to uncover fundamental problems. The text-based sentiment analysis works well on our unstructured, imbalanced oral history data. As the data analysis in Section 3.3 indicated, there appeared to be a greater consensus among the three annotators on sentiment than emotion. This tendency seems to be confirmed by the experiment. In particular, we find it noteworthy that the classification works well given that we use raw, erroneous ASR transcripts as input to the model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Speech emotion recognition (SER) is a branch of affective computing that deals with identifying and rec- ognizing the emotional content in speech. One of the significant challenges in this field is identifying appropriate features in the speech signal that best represent its emotional content.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Related Work",
      "text": "A detailed overview of SER is given, for example, by  Ayadi et al. (2011) ,  (Schuller, 2018) , and (Akc ¸ay and Oguz, 2020). Current approaches utilize convolutional neural network (CNN) or bidirectional recurrent neural network (biRNN) layers for SER-or combining both, such as  Dai et al. (2019) . The proposed method represents emotion in speech in an end-to-end manner  (Zhang et al., 2017) . Furthermore, this method focuses on only four categories of emotion: anger, happy, sad, and neutral, which are identified as the most discriminatory ones. Further,  (Li et al., 2019)  propose a Diluted Residual Network (DRN) with multi-head self-attention. The authors employ Low-Level Descriptors (LLDs) extracted from the audio signal as input.  (Wang et al., 2020)  propose a model consisting of two jointly trained LSTMs: each of these models is separately used to process MFCC features and Mel-spectrograms. Both models predict an output class (emotion) that is averaged to arrive at the result. Some of the currently used techniques also use transfer learning to boost the performance (Akc ¸ay and Oguz, 2020).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methodology And Implementation",
      "text": "In this experiment, we train a hybrid model for SER, which combines traditional machine learning with deep learning. As for the text-based model, we utilize pretrained models and multiple data sets to cope with the lack of training data. We apply a VGG-19 model pretrained on the ImageNet data set and use log-Mel spectrograms treated as grayscale images as input features.\n\nThe pre-trained VGG-19 model is first fine-tuned on the HdG training set. Then we use a combined data set to extract the embeddings from the fine-tuned VGG-19. These embeddings are used as input for the SVM model. The combined data set contains the HdG train set, the German part of CMU-MOSEAS  (Zadeh et al., 2020) , CMU-MOSEI  (Zadeh et al., 2018) , and Berlin Emotional Database (Berlin EmoDB)  (Burkhardt et al., 2005)   sets have natural emotions. For data balancing, we apply data augmentation with 10 dB SNR additive white noise and downsampling the overrepresented. The distribution of the emotional classes in the combined train data set is presented in Table  4 . As already shown for the text modality, we have not achieved satisfactory recognition performance on our dataset so far with seven classes. Therefore, in this experiment, we only present results considering happiness, sadness, and neutral. This aims to assess the problems of training better.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Inference",
      "text": "The model yields a training accuracy 40.6 % and 32.0 % on the HdG test set for the three-class classification. The results are presented as a confusion matrix in the top row of Figure  6 . We observe that the neutral class is often confused with other emotions for both the training and test set. The results of the text modality already indicated this. However, it becomes more substantial in this experiment for the audio modality with three classes. We hypothesize that the neutral class cannot be sufficiently differentiated from subtle emotions in natural speech, leading to confusion in training. We conducted another experiment in which the neutral class is removed to investigate this issue further. For two-class classification (happy and sad) the accuracy improves to 63.8 % and 66.0 % for the training and test set, respectively. As shown at the bottom of Figure  6 , removing the neutral class results in a structural improvement. However, this does not lead to happiness and sadness being distinguished substantially better for the test set. The high accuracy is mainly attributed to the class imbalance towards happiness. Still, the system favors the happiness class over sadness. A subjective evaluation of the HdG samples shows that it is challenging to differentiate emotions based on audio samples alone. Thus, we hypothesize that particular attention might have been paid to other modalities, presumably facial expressions, annotating the interviews.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Facial Emotion Recognition",
      "text": "Facial emotion recognition (FER) is the task of recognizing human emotions from facial expressions. The immense number of visual clues present in the human face to identify underlying emotions makes FER an integral part of many emotion recognition systems.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Related Work",
      "text": "FER methods are categorized based on two dimensions: traditional feature-vs. representation-learningbased, and static vs. dynamic methods. Traditional feature learning-based FER methods rely on handcrafted features. In contrast, representation-learningbased methods use a system such as a neural network to learn features from training data automatically. Dynamic methods utilize temporal relationships between frames of an input video while static ones treat every frame independently. Dynamic representation-learning approaches possess an inherent advantage and become potential candidates for further consideration. To perform the task at hand, we shortlisted  Meng et al. (2019; Kuo et al. (2018; Gera and Balasubramanian (2020; Savchenko (2021) , and  Kuhnke et al. (2020)  based on factors such as performance on open-source FER data sets like CK+  (Lucey et al., 2010)  and AFEW  (Kossaifi et al., 2017) , depth of the neural network used (determines the minimum amount of data required for training), and reproducibility of results claimed by authors. Out of the five, Frame Attention Networks (FAN)  (Meng et al., 2019)  is chosen for its state-of-the-art accuracy on CK+ (99 %) and AFEW (51.18 %) data sets, and its simple yet effective construction.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Methodology",
      "text": "The HdG videos are pre-processed using the d-lib based face detection and alignment modules to crop out and align faces. These sequences of images are used as inputs for the FAN. The FAN network architecture consists of a CNN-based feature embedding module (ResNet-18) and a subsequent frame attention network.  Meng et al. (2019)  offer three variants of FAN: baseline, self-attention, and the combination of self and relation attention. The authors report a slightly superior performance of the self-relation-attention variant over the other two. However, we currently use the baseline and self-attention variants due to their simple design, enabling us to better understand their work.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Design Of Experiments",
      "text": "In addition to the challenges of emotion recognition in general and our HdG dataset in particular, we hypothesize an additional, specific challenge for FER. Most of the frames in a typical interview video carry faces with neutral or a subtle version of a particular emotion. This  The first experiment was conducted with the already balanced pair of happy and sad classes, with an intent to study these classes' effect on the classifier's predictive performance. In the next experiment, the neutral class was included after under-sampling it to match the other two sizes. Whereas, for the third experiment, the under-represented anger class was added along with the \"happy-sad\" pair to understand the bias induced from the class imbalance. The final experiment was conducted with unchanged training data to evaluate the current state of the classifier's performance. All experiments were conducted with both the baseline and self-attention variants of FAN. However, the results presented in the next section are limited to the baseline variant, which performed better in all of the conducted experiments. Models of both variants were pretrained with Microsoft FER+  (Barsoum et al., 2016)  and AFEW  (Kossaifi et al., 2017)  data sets using transfer learning.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Inference",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Summary And Conclusion",
      "text": "This work investigated the ambiguity in human perception of emotions and sentiment in German oral history interviews. Comparing the annotations of three persons using Ekman classes commonly used in emotion recognition revealed substantial differences in human perception. While the annotators in our experiment have a reasonably consistent understanding of the two most common emotions, happiness and sadness, we found very little correlation for other emotions. Given the ambiguity of the human annotation using predefined emotions classes, we question whether practical learning for machines is even possible. We further investigated co-occur of emotions in the annotation. An annotator survey revealed that Ekman classes were unanimously rated as insufficient for the complexity of multi-layered emotions in oral history interviews. The annotators intuitively combined different emotion classes to describe complex emotions not fitting in the predefined classes. This is reflected in an increased correlation of certain emotion classes, e.g., fear and sadness representing despair or helplessness.\n\nHate was intuitively annotated as a combination of disgust and anger. We also reported results from initial emotion recognition experiments for facial expressions, speech, and text. A facial emotion recognition system for oral history revealed the system could differentiate happiness and sadness in our interviews. However, adding a neutral class results in the system not being able to differentiate between the subtle emotions and the neutral class. This issue and the combination of emotions described earlier are limited by single-label training.\n\nIn future work with oral history, multi-label training should be considered to account for these aspects.\n\nSo far in our experiments, speech emotion recognition is behind facial emotion recognition. Even differentiating between happiness and sadness based on the voice appears challenging. For sentiment analysis based on raw ASR transcripts, on the other hand, we were able to achieve decent accuracy for our unstructured data. This is also consistent with the human perception, which was highest for sentiment between annotators in our experiments.\n\nIn addition to the human ambiguity, other challenges currently limit the application of emotion recognition for oral history. In particular, we identified class imbalance and lack of representative training data as the current primary challenges. The application of pre-trained models, a combination of multiple natural data sets, and fine-tuning of models were essential in our work.\n\nOverall, such indexing technologies for oral history archives seem to be quite limited so far. In oral history interviews, complex, subtle, and multi-layered emotions cannot yet be captured by our systems with the predefined, common classes. Perhaps fundamentally different approaches have to be chosen, e.g., limiting the indexing to recognizing specific patterns in human communication without interpreting them as emotions. However, users need to determine which patterns are relevant for their work in advance for meaningful application in archives. The results and observations of our work can provide initial impetus for this further research, which requires interdisciplinary collaboration between users of such archives and AI researchers.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Acknowledgments",
      "text": "The research project is funded by the German Federal Government Commissioner for Culture and Media.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the distribution of annotated scores per",
      "page": 2
    },
    {
      "caption": "Figure 1: Histograms of the annotation scores for each emotion. Each color bar represents a different annotator.",
      "page": 3
    },
    {
      "caption": "Figure 2: Sentiment annotation score histograms.",
      "page": 3
    },
    {
      "caption": "Figure 2: presents the histogram of for the sentiment",
      "page": 3
    },
    {
      "caption": "Figure 3: A moderate correlation exists between emotions and",
      "page": 3
    },
    {
      "caption": "Figure 3: Spearman correlation between the annotated",
      "page": 4
    },
    {
      "caption": "Figure 4: Percentages of emotion class and sentiment",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the shares of each class in the entire",
      "page": 4
    },
    {
      "caption": "Figure 5: For the sentiment model, we",
      "page": 5
    },
    {
      "caption": "Figure 5: Confusion matrix of the text-based sentiment",
      "page": 5
    },
    {
      "caption": "Figure 3: has shown, these two classes have an in-",
      "page": 5
    },
    {
      "caption": "Figure 6: Confusion matrices for 3 class (top row) and",
      "page": 6
    },
    {
      "caption": "Figure 6: We observe that the neutral",
      "page": 6
    },
    {
      "caption": "Figure 7: shows the confusion matrices of the base-",
      "page": 7
    },
    {
      "caption": "Figure 7: Confusion matrices from results of the FER experiments.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Spearman rank-order correlation coefficients",
      "data": [
        {
          "Column_1": "",
          "1\n97 71\n9": "72 4 6 541 951 471\n82 11 63"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: shows the class-wise relationship between the",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "7 09 22": "",
          "Column_4": "63 4 5 871 161\n35 08 46\n2"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Spearman rank-order correlation coefficients",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "A B C\n29 8": "081 4 43\n46 73 83\n1 2 6"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Spearman rank-order correlation coefficients",
      "data": [
        {
          "Column_1": "",
          "32": "5 251\n94 82 37 31\n5 1 5"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Spearman rank-order correlation coefficients",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "3": "262\n82\n07 92 33 88 71 61\n1"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Spearman rank-order correlation coefficients",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "7 1": "93 82\n97 15 38 67 71\n5 2"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Spearman rank-order correlation coefficients",
      "data": [
        {
          ",1\n7 44 718 36": "435 66 7\n552 192 112\n07 82 52",
          "Column_2": "",
          "Column_3": "653 7 334\n741 381 941\n82 51 22"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.45": "-0.33",
          "-0.17\n-0.13 0.097\n0.21 -0.088 0.065\n-0.13 0.039 0.47 0.088": "-0.17",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "0.45": "-0.43",
          "-0.17\n-0.13 0.097\n0.21 -0.088 0.065\n-0.13 0.039 0.47 0.088": "-0.13",
          "Column_3": "0.097",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "0.45": "0.068",
          "-0.17\n-0.13 0.097\n0.21 -0.088 0.065\n-0.13 0.039 0.47 0.088": "0.21",
          "Column_3": "-0.088",
          "Column_4": "0.065",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "0.45": "-0.41",
          "-0.17\n-0.13 0.097\n0.21 -0.088 0.065\n-0.13 0.039 0.47 0.088": "-0.13",
          "Column_3": "0.039",
          "Column_4": "0.47",
          "Column_5": "0.088",
          "Column_6": ""
        },
        {
          "0.45": "-0.4",
          "-0.17\n-0.13 0.097\n0.21 -0.088 0.065\n-0.13 0.039 0.47 0.088": "-0.16",
          "Column_3": "0.36",
          "Column_4": "0.076",
          "Column_5": "0.027",
          "Column_6": "0.081"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "18.6\n6.9",
          "Column_4": "",
          "Column_5": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "31.2",
          "Column_5": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "20.8",
          "Column_4": "",
          "Column_5": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Accuracy of the text-based sentiment and",
      "data": [
        {
          "153\n32.5%": "21\n4.5%",
          "30\n6.4%": "66\n14.0%",
          "33\n7.0%": "34\n7.2%"
        },
        {
          "153\n32.5%": "24\n5.1%",
          "30\n6.4%": "16\n3.4%",
          "33\n7.0%": "94\n20.0%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Our approach achieves",
      "data": [
        {
          "69\n16.1%": "14\n3.3%",
          "24\n5.6%": "46\n10.7%",
          "17\n4.0%": "9\n2.1%",
          "5\n1.2%": "7\n1.6%",
          "8\n1.9%": "12\n2.8%",
          "19\n4.4%": "16\n3.7%",
          "2\n0.5%": "5\n1.2%"
        },
        {
          "69\n16.1%": "16\n3.7%",
          "24\n5.6%": "11\n2.6%",
          "17\n4.0%": "6\n1.4%",
          "5\n1.2%": "4\n0.9%",
          "8\n1.9%": "1\n0.2%",
          "19\n4.4%": "27\n6.3%",
          "2\n0.5%": "2\n0.5%"
        },
        {
          "69\n16.1%": "9\n2.1%",
          "24\n5.6%": "8\n1.9%",
          "17\n4.0%": "6\n1.4%",
          "5\n1.2%": "5\n1.2%",
          "8\n1.9%": "4\n0.9%",
          "19\n4.4%": "4\n0.9%",
          "2\n0.5%": "0\n0.0%"
        },
        {
          "69\n16.1%": "4\n0.9%",
          "24\n5.6%": "3\n0.7%",
          "17\n4.0%": "5\n1.2%",
          "5\n1.2%": "4\n0.9%",
          "8\n1.9%": "7\n1.6%",
          "19\n4.4%": "10\n2.3%",
          "2\n0.5%": "1\n0.2%"
        },
        {
          "69\n16.1%": "1\n0.2%",
          "24\n5.6%": "2\n0.5%",
          "17\n4.0%": "5\n1.2%",
          "5\n1.2%": "4\n0.9%",
          "8\n1.9%": "1\n0.2%",
          "19\n4.4%": "12\n2.8%",
          "2\n0.5%": "0\n0.0%"
        },
        {
          "69\n16.1%": "2\n0.5%",
          "24\n5.6%": "3\n0.7%",
          "17\n4.0%": "0\n0.0%",
          "5\n1.2%": "1\n0.2%",
          "8\n1.9%": "1\n0.2%",
          "19\n4.4%": "5\n1.2%",
          "2\n0.5%": "1\n0.2%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "358\n4.0%": "277\n3.1%",
          "1025\n11.5%": "1261\n14.2%",
          "1655\n18.6%": "1501\n16.9%"
        },
        {
          "358\n4.0%": "134\n1.5%",
          "1025\n11.5%": "678\n7.6%",
          "1655\n18.6%": "1987\n22.4%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "17\n4.8%": "15\n4.2%",
          "67\n18.8%": "73\n20.5%",
          "84\n23.6%": "24\n6.7%"
        },
        {
          "17\n4.8%": "4\n1.1%",
          "67\n18.8%": "48\n13.5%",
          "84\n23.6%": "24\n6.7%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1953\n33.5%": "1026\n17.6%",
          "1086\n18.6%": "1773\n30.4%"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "106\n56.4%": "58\n30.9%",
          "6\n3.2%": "18\n9.6%"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "Bibliographical References Akc ¸ay",
        "M Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "2",
      "title": "Emotions from text: Machine learning for text-based emotion prediction",
      "authors": [
        "C Alm",
        "D Roth",
        "R Sproat"
      ],
      "year": "2005",
      "venue": "Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "18th ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "5",
      "title": "Reader perspective emotion analysis in text through ensemble based multi-label classification framework",
      "authors": [
        "P Bhowmick"
      ],
      "year": "2009",
      "venue": "Computer and Information Science"
    },
    {
      "citation_id": "6",
      "title": "Learning discriminative features from spectrograms using center loss for speech emotion recognition",
      "authors": [
        "D Dai",
        "Z Wu",
        "R Li",
        "X Wu",
        "J Jia",
        "H Meng"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "A performance comparison of eight commercially available automatic classifiers for facial affect recognition",
      "authors": [
        "D Dupré",
        "E Krumhuber",
        "D Küster",
        "G Mckeown"
      ],
      "year": "2020",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "8",
      "title": "Facial signs of emotional experience",
      "authors": [
        "P Ekman",
        "W Freisen",
        "S Ancoli"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "9",
      "title": "Affect expression behaviour analysis in the wild using spatiochannel attention and complementary context information",
      "authors": [
        "D Gera",
        "S Balasubramanian"
      ],
      "year": "2020",
      "venue": "Affect expression behaviour analysis in the wild using spatiochannel attention and complementary context information",
      "arxiv": "arXiv:2009.14440"
    },
    {
      "citation_id": "10",
      "title": "Emotion detection in email customer care",
      "authors": [
        "N Gupta",
        "M Gilbert",
        "G Fabbrizio"
      ],
      "year": "2013",
      "venue": "Computational Intelligence"
    },
    {
      "citation_id": "11",
      "title": "A survey on classification techniques for opinion mining and sentiment analysis",
      "authors": [
        "F Hemmatian",
        "M Sohrabi"
      ],
      "year": "2019",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "12",
      "title": "Towards text-based emotion detection: A survey and possible improvements",
      "authors": [
        "E Kao",
        "L Chieh Chun",
        "T.-H Yang",
        "C.-T Hsieh",
        "V.-W Soo"
      ],
      "year": "2009",
      "venue": "International Conference on Information Management and Engineering (ICIME)"
    },
    {
      "citation_id": "13",
      "title": "Fine-grained emotion detection in health-related online posts",
      "authors": [
        "H Khanpour",
        "C Caragea"
      ],
      "year": "2018",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "14",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "15",
      "title": "Human and machine validation of 14 databases of dynamic facial expressions",
      "authors": [
        "E Krumhuber",
        "D Küster",
        "S Namba",
        "L Skora"
      ],
      "year": "2020",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "16",
      "title": "Two-stream aural-visual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "year": "2020",
      "venue": "15th IEEE International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "17",
      "title": "A compact deep learning model for robust facial expression recognition",
      "authors": [
        "C.-M Kuo",
        "S.-H Lai",
        "M Sarkis"
      ],
      "year": "2018",
      "venue": "IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "18",
      "title": "Dilated residual network with multi-head self-attention for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Exploring finegrained emotion detection in tweets",
      "authors": [
        "J Liew",
        "H Turtle"
      ],
      "year": "2016",
      "venue": "NAACL Student Research Workshop"
    },
    {
      "citation_id": "20",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "21",
      "title": "Frame attention networks for facial expression recognition in videos",
      "authors": [
        "D Meng",
        "X Peng",
        "K Wang",
        "Y Qiao"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "22",
      "title": "Capturing global mood levels using blog posts",
      "authors": [
        "G Mishne",
        "M De Rijke"
      ],
      "year": "2006",
      "venue": "Computational Approaches to Analyzing Weblogs, Papers from the 2006 AAAI Spring Symposium"
    },
    {
      "citation_id": "23",
      "title": "Textual affect sensing for sociable and expressive online communication",
      "authors": [
        "A Neviarouskaya",
        "H Prendinger",
        "M Ishizuka"
      ],
      "year": "2007",
      "venue": "Affective Computing and Intelligent Interaction, Second International Conference (ACII)"
    },
    {
      "citation_id": "24",
      "title": "Sentiment analysis and opinion mining applied to scientific paper reviews",
      "authors": [
        "B Norambuena",
        "E Lettura",
        "C Villegas"
      ],
      "year": "2019",
      "venue": "Intell. Data Anal"
    },
    {
      "citation_id": "25",
      "title": "Opinion mining and sentiment analysis",
      "authors": [
        "B Pang",
        "L Lee"
      ],
      "year": "2007",
      "venue": "Found. Trends Inf. Retr"
    },
    {
      "citation_id": "26",
      "title": "A computational look at oral history archives",
      "authors": [
        "F Pessanha",
        "A Salah"
      ],
      "year": "2022",
      "venue": "Journal on Computing and Cultural Heritage"
    },
    {
      "citation_id": "27",
      "title": "Affective topic model for social emotion detection",
      "authors": [
        "Y Rao",
        "Q Li",
        "W Liu",
        "Q Wu",
        "X Quan"
      ],
      "year": "2014",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "28",
      "title": "A systematic review on opinion mining and sentiment analysis in social media",
      "authors": [
        "Z Salah",
        "A Al-Ghuwairi",
        "A Baarah",
        "A Al-Oqaily",
        "B Qadoumi",
        "M Alhayek",
        "B Alhijawi"
      ],
      "year": "2019",
      "venue": "Int. J. Bus. Inf. Syst"
    },
    {
      "citation_id": "29",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "arxiv": "arXiv:2103.17107"
    },
    {
      "citation_id": "30",
      "title": "The Fraunhofer IAIS audio mining system: Current state and future directions",
      "authors": [
        "C Schmidt",
        "M Stadtschnitzer",
        "J Köhler"
      ],
      "year": "2016",
      "venue": "12th ITG Conference on Speech Communication"
    },
    {
      "citation_id": "31",
      "title": "Hybrid approaches to finegrained emotion detection in social media data",
      "authors": [
        "A Schoene"
      ],
      "year": "2020",
      "venue": "34th Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "33",
      "title": "The Mathematical Theory of Communication",
      "authors": [
        "C Shannon",
        "W Weaver"
      ],
      "year": "1949",
      "venue": "The Mathematical Theory of Communication"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Current state of text sentiment analysis from opinion to emotion mining",
      "authors": [
        "A Yadollahi",
        "A Shahraki",
        "O Zaïane"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "36",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "37",
      "title": "Language Resource References",
      "venue": "Language Resource References"
    },
    {
      "citation_id": "38",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Technology"
    },
    {
      "citation_id": "39",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "40",
      "title": "CMU-MOSEAS: A multimodal language dataset for spanish, portuguese, german and french",
      "authors": [
        "A Zadeh",
        "Y Cao",
        "S Hessner",
        "P Liang",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2020",
      "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    }
  ]
}