{
  "paper_id": "2104.07252v2",
  "title": "Emotion Dynamics Modeling Via Bert",
  "published": "2021-04-15T05:58:48Z",
  "authors": [
    "Haiqin Yang",
    "Jianping Shen"
  ],
  "keywords": [
    "Emotion recognition in conversation",
    "emotion dynamics",
    "BERT"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion dynamics modeling is a significant task in emotion recognition in conversation. It aims to predict conversational emotions when building empathetic dialogue systems. Existing studies mainly develop models based on Recurrent Neural Networks (RNNs). They cannot benefit from the power of the recently-developed pre-training strategies for better token representation learning in conversations. More seriously, it is hard to distinguish the dependency of interlocutors and the emotional influence among interlocutors by simply assembling the features on top of RNNs. In this paper, we develop a series of BERT-based models to specifically capture the inter-interlocutor and intra-interlocutor dependencies of the conversational emotion dynamics. Concretely, we first substitute BERT for RNNs to enrich the token representations. Then, a Flat-structured BERT (F-BERT) is applied to link up utterances in a conversation directly, and a Hierarchically-structured BERT (H-BERT) is employed to distinguish the interlocutors when linking up utterances. More importantly, a Spatial-Temporal-structured BERT, namely ST-BERT, is proposed to further determine the emotional influence among interlocutors. Finally, we conduct extensive experiments on two popular emotion recognition in conversation benchmark datasets and demonstrate that our proposed models can attain around 5% and 10% improvement over the state-of-the-art baselines, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Recently, dialogue systems have achieved significant improvement in many areas thanks to the plethora of publicly available conversational data and the rapid advance of deep learning techniques  [8] ,  [9] ,  [21] . One of the critical challenges to enhancing the systems is generating more human-like conversation  [22] ,  [37] ,  [39] . Hence, a system should perceive users' emotion states and express the content in an empathetic manner, e.g., by selecting suitable responses from the database or automatically generating human-like responses  [16] ,  [27] .\n\nIn the literature, Emotion Recognition in Conversation (ERC) is a sub-field of emotion recognition and aims to automatically identify human emotions in conversational scenarios  [1] ,  [37] . A critical task in this field is to model emotion dynamics in conversation  [13] . The emotion dynamics explain the conversational emotion behaviors from two dependencies, i.e., the inter-interlocutor dependency and intrainterlocutor dependency. In a dialogue, the inter-interlocutor dependency describes the emotional influence among different interlocutors. That is, one interlocutor tries to coerce other interlocutors changing their emotions  [31] . For example, as illustrated in Fig.  1 , Person B's emotion at timestamp 6 changes from Neutral to Fear due to the consecutive impact of Person A's strong emotion of Surprise at timestamp 3 and Fig.  1 . An example of conversations with emotion dynamics  5 . Meanwhile, the intra-interlocutor dependency describes the emotional inertia within individual interlocutors. That is, one interlocutor tries to resist the change of their own emotion against external influence  [20] . For instance, in Fig.  1 , though Person A has changed the emotion to Surprise at timestamp 3, Person B remains the original emotion of Neutral.\n\nTo model emotion dynamics in conversation, researchers have explored various methods based on the Recurrent Neural Networks (RNNs)  [11] ,  [12] ,  [17] ,  [18] ,  [24] ,  [28] ,  [35] . Some studies have adopted the flat structured Recurrent Neural Networks (RNNs), i.e., concatenating utterances of different interlocutors in a single sequence, for context modeling  [24] ,  [35] . Meanwhile, other studies have established variants of hierarchically structured RNNs for context modeling  [11] ,  [12] ,  [17] ,  [18] ,  [28] , e.g., lining up a sequence of features extracted from a sub-sequence of utterances spoken by the same interlocutor. However, existing approaches contain the following limitations: (1) the flat structure cannot distinguish the interlocutors because they are blended in the same sequence during temporal modeling; (2) the hierarchical structure cannot distinguish the emotional influence among interlocutors because it applies a flat structure on the extracted sub-sequence features; (3) RNNs can hardly fulfill the power of pre-training language models on large-scale data  [32] ,  [33]  than recently-developed Transformer-based models  [23] ,  [25] ,  [44] , e.g., BERT  [7] .\n\nIn this paper, we propose a series of BERT-based models to tackle the above challenges. More specifically, we apply a Flat-structured BERT (F-BERT) to directly link up utterances in a conversation and extend the structure to a Hierarchicallystructured BERT (H-BERT) to distinguish different interlocu-tors in a conversation. Both F-BERT and H-BERT can facilitate the power of BERT and can learn better representations by fulfilling pre-training strategies than RNN counterparts. More importantly, we propose a Spatial-Temporal-structured BERT, namely ST-BERT, to further distinguish emotional influence among interlocutors by first independently capturing the intrainterlocutor and inter-interlocutor emotional influence and then unifying those emotional influence via fusion strategies.\n\nWe highlight the contributions of our work as follows:\n\n• We develop a series of BERT-based models to explore the potential of applying Transformer-based pretraining models to capture the inter-interlocutor and intrainterlocutor dependencies of the conversational emotion dynamics.\n\n• We propose two basic models, F-BERT and H-BERT, as the baselines of their RNN counterparts to exploit the effect of BERT with pre-training strategies. More importantly, we develop ST-BERT to overcome the weakness of existing work in distinguishing the emotional influence among interlocutors when modeling the emotion dynamics.\n\n• We conduct extensive experiments on two ERC benchmark datasets to illustrate the effectiveness of our proposed models and observe that we attain averaged accuracy of about 5% and 10% improvement on IEMOCAP and MELD, respectively, over the state-of-the-art (SOTA) baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotions are hidden mental states associated with human thoughts and feelings  [37] . Emotion recognition is an interdisciplinary field that spans psychology, cognitive science, machine learning, and natural language processing  [34] . The aim is to identify correct emotions from multi-modal expressions  [41] -  [43] ,  [45] .\n\nEmotion recognition in conversation (ERC) is to predict the emotion in conversational scenarios. Rather than treating emotions as static states, ERC involves emotion dynamics in a conversation. By comparing with the recent proposed ERC approaches  [15] ,  [28] , Poria et al.  [37]  discover that traditional emotion recognition methods  [6] ,  [19] ,  [30] ,  [40]  fail to perform well because the same utterance within different context may exhibit different emotions. Mao et al.  [29]  indicate that emotion expressions in different modalities exhibit different dependence on conversational context, where emotion dynamics mainly affect emotion expressions in textual modality. Recently, various methods have been proposed to tackle ERC in the natural language processing community. For example, the bi-directional Long Short-Term Memory (LSTM)  [35]  has been applied to capture the intra-interlocutor dependency. The intra-interlocutor and inter-interlocutor dependencies between dyadic interlocutors have been distinguished by leveraging the hierarchical Gated Recurrent Unit (GRU) and memory networks  [12] ,  [13] . Multiple GRUs with global attention mechanism have been designed and further developed in multiparty ERC  [28] . Graph Convolutional Networks (GCNs) have also been employed to mine complex interactions between interlocutors  [11] ,  [48] . GRU-based attention gated hierarchical memory networks have been proposed for ERC  [17] . However, these methods are mainly based on RNNs and do not sufficiently distinguish the emotional influence among interlocutors.\n\nTransformer-based pre-training language models have applied the Transformer architecture  [44]  to promote language understanding by a two-staged training strategy, i.e., a self-supervised pre-training on a general-domain text corpus and a fine-tune training on the downstream application data. Pioneers  [32] ,  [33]  conduct pre-training on bi-directional RNNs to obtain the contextualized representation of each token. However, RNN-based models are inefficient for longterm modeling and have limited lifting power when stacking more layers due to the limitation of the recurrent connections. Recently, due to the power of parallel computation and deep-model construction, Transformer-based models, e.g., GPT  [38] , BERT  [7] , and ELECTRA  [5] , have been deployed and achieved the SOTA performance in many downstream NLP applications. Several pieces of work, e.g., transfer learning ERC  [14] , utterance-level dialogue understanding  [10] , and contextualized emotion sequence tagging  [46] , have employed pre-training models as a feature extractor in the task of ERC. However, the potential of the Transformer-based models is less explored and does not address the problem of modeling emotion dynamics yet.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Task Definition",
      "text": "We first define the task of utterance-level emotion recognition in conversation.\n\nDefinition 1 (Emotion Recognition in Conversation):\n\nTτ is the τ -th utterance of T τ words spoken by the λ τ -th interlocutor from one of the total S interlocutors. The corresponding emotion type is y τ ∈ Y, where the emotion set Y consists of all emotions, such as anger, joy, and neutral. The goal is to train a model that can tag each utterance in a new conversation with a emotion label as accurately as possible.\n\nIn our work, we aim to capture emotion dynamics in conversations and need the following preliminary notions:\n\nAccording to the interlocutors that are involved, we define three types of context utterances within a sliding window of K as follows:\n\n• intra-context: the preceding utterances of the i-th utterance from the interlocutor λ i in U within the window size of K:\n\n• inter-context: the preceding utterances of the i-th utterance from the interlocutors, except λ i , in U within the window size of K:\n\n• conv-context: the preceding utterances of the i-th utterance in U within the window size of K:  I  presents an example of the three types of contexts in a conversation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Table I An Example Of Context Utterances In A Conversation When",
      "text": "L = 8, S = 3, AND K = 5.\n\nIV. BERT BERT is a powerful Transform-based language model and performs exceptionally well in many downstream NLP tasks  [7] . A standard BERT model consists of a stack of L identical Transformer blocks expanding in a sequence mode as illustrated in Fig.  2  to naturally exploit the temporal information. Each Transformer block consists of two sublayers, i.e., a multi-head self-attention sublayer and a position-wise fully connected feed-forward sublayer. Residual connection with layer normalization is employed on each of the two sublayers to avoid the problem of gradient vanishing in training. The final output is then computed by LayerNorm(x+Sublayer(x)), where Sublayer(x) is one of the above mentioned sublayers  [44] . In the following, we briefly elucidate the three main layers in BERT.\n\nEmbedding layer: given utterance-context pairs, BERT has 5 key operations in dealing with the input pairs: (1) packing the utterance and its context into a sequence of tokens by WordPiece tokenization; (2) adding [CLS] as the classification token at the head of a sequence; (3) adding the special token [SEP] and token type embeddings to differentiate the utterance and its context; (4) applying WordPiece embeddings  [47]  on the tokens; (5) adding position embeddings to maintain the order information in a sequence.\n\nMulti-head attention sublayer: the output of embedding layer is fed to the multi-head self-attention sublayer (the orange blocks in Fig.  2 ) to model temporal dependency. A multihead attention independently applies attention mechanism H times along with the query Q ∈ R d k , key K ∈ R d k , and value V ∈ R dv , respectively, and concatenates them together:\n\nis a scaling factor to avoid pushing the softmax function into small gradients regions when d k is large  [44] .\n\nare the weights of the projection matrices for computing the h-th attention operations. Practically, we set the dimension\n\nFully connected feed-forward sublayer: after applying the multi-head attention, a two-layer fully connected feed-forward network is computed by a RELU on the hidden state:\n\nwhere x is the output from the multi-head attention sublayer, V. OUR PROPOSAL In the following, we present our proposed series of BERTbased models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. F-Bert",
      "text": "The Flat-structured BERT (F-BERT), as illustrated in Fig.  3 (a), is to directly concatenate the target utterance with the conv-context while applying BERT afterwards. The input is a sequence of utterance-context pair, i.e., the target utterance of T sub-words,\n\nWe pack the pairs into a sequence of tokens:\n\nHence, X i includes the information of the target utterance with the conversation context, but does not distinguish the identity of interlocutors. After pre-processing, X i is fed to BERT and represented by the last hidden layer at the [CLS] token, denoted by\n\nwhere r i is the output representation of u λi i for emotional predictions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. H-Bert",
      "text": "The Hierarchically-structured BERT (H-BERT), as illustrated in Fig.  3(b) , first applies BERT to wrap up an utterance with its intra-context to capture the intra-interlocutor dependency as the branch feature. Next, it lines up the branch features in conversational order by the backbone Transformer to produce the final output representation. More specifically, the input of a branch BERT is a sequence of utterance-context pair, i.e., the target utterance of T sub-words, u λi i = w 1 • • • w T , and the intra-context of T ϕ sub-words, ϕ(u λi i , U, K) = ω 1 • • • ω Tϕ . After packing, we compute the input sequence by\n\nThus, X λi i includes the information of the target utterance with historical utterances only spoken by the target interlocutor. By feeding X λi i to BERT, we obtain\n\nwe can obtain f λi i , the intra-interlocutor dependency of the λ i -th interlocutor, which maintains the i-th emotion influence in a conversation.\n\nLet\n\nbe the sequence of the branch features of the entire conversation. The input of the backbone Transformer is a sliding window of K + 1 in F :\n\nwhere f λi i is the target feature at the last position.\n\ni-1 are the preceding K features that may produce emotional influence to the target. F i is fed to the backbone Transformer and represented by the last hidden layer at the target position. The computation of the Transformer is\n\nwhere r i is the representation of u λi i conditioned on the temporally wrapped emotion influence for making predictions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. St-Bert",
      "text": "The Spatial-Temporal-structured BERT (ST-BERT), as illustrated in Fig.  3(c ), first applies BERT to individually wrap up the inter-context and intra-context of the target utterance as two types of temporally captured emotion influence. After that, different fusion strategies are applied to combine the two temporally captured emotional influence into a single spatial representation.\n\nTemporal Construction. Given the target utterance u λi i = w 1 • • • w T of T sub-words, we construct two individual sequences of utterance-context pairs:\n\nwhere X ϕ i includes the information, which maintains the emotion inertia of the λ i -th interlocutor by incorporating the intra-context ϕ(\n\nX φ i includes the information, which produces emotional influence from the non-λ i interlocutor by incorporating the inter-context\n\nThe two types of token sequences are then fed to the BERT,\n\nwe can directly and explicitly capture the intra-interlocutor and inter-interlocutor dependencies in\n\nThe two temporally contextualized features are fused vertically in different spatial fusion strategies.\n\nSpatial Fusion. We explore three types of spatial fusion strategies, including direct concatenation, gate operation, and attention mechanism.\n\nDirect concatenation is a simple but effective strategy. It does not involve interactions between features and can be computed by a fully-connected network:\n\nwhere [•; •] is the concatenation. r i ∈ R d hidden is the output representation for emotional predictions. W C and b C are the projection matrix and bias, respectively. Gate operation is a neuron-level interactive weighting strategy. The computation can be formulated as\n\nwhere h ϕ i ∈ R d hidden and h φ i ∈ R d hidden are the projections of f ϕ i and f φ i , respectively. * refers to the Hadamard product whose function is to use neurons in one vector to weight the neurons of its counterpart at the same position. Here, z i ∈ R d hidden is to weight neurons in h ϕ i and 1-z i is to weight neurons in h φ i , where z i is computed by feature interactions between f ϕ i and f φ i , σ is to maps the z i to (0, 1). Note that the impact of \"1 -z i \" on h φ i is critical because it allows to learn the weights of the neurons in h ϕ i and h φ i contrastively  [2] . W ϕ and b ϕ , W φ and b φ , and W Z and b Z , are the corresponding projection matrices and biases, respectively. r i is the final output representation.\n\nAttention mechanism is a vector-level interactive weighting strategy. The insight is identical to that of the gate operation. The major difference is that gate operation allocate different weights to neurons in a vector while attention operation allocate the same weight to neurons in a vector. The attention operation can be computed by\n\nwhere\n\nis the scaling factor. d model is the dimention of f ϕ i , f φ i . α i is the attention weight and a scalar computed by dot-product between f ϕ i and f φ i . Again, the operation of \"1α i \" plays a trade-off on the weight for contrastive learning  [4] . r i is the final output representation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Discriminator",
      "text": "The discriminator is a two-layer perceptron with the hidden layer activated by the tanh function, which can be trained by minimizing the cross-entropy loss. Given the representation r i , the discriminator output emotion distributions computed by the softmax function:\n\nwhere ŷi is the predicted emotion. W O and W Pi are the corresponding projection matrices.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Experiments",
      "text": "In this section, we present the experiments with detailed analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "Two popular ERC datasets, IEMOCAP  [3]  and MELD  [36] , are adopted to evaluate our proposed models. Statistics on the two datasets are presented in Table  II .\n\nIEMOCAP consists of dyadic conversation videos between pairs of 10 speakers. Each utterance is annotated with one of the six emotion types, including happy, sad, neutral, angry, excited, and frustrated. Following  [28] , we apply the first four sessions in training and use the last session for test. The validation conversations are randomly selected from the training set with a ratio of 0.1.\n\nMELD consists of multi-party conversation videos collected from the Friends TV series. Each utterance is annotated with one of the seven emotion types, including anger, disgust, sadness, joy, neutral, surprise, and fear. We apply the official splits for training, validation, and test in this dataset.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "In the experiment, BERT Base is adopted as the fundamental sequential module, where it consists of 12 Transformer blocks, 12 self-attention heads, and 768 hidden-units. We employ the off-the-shelf implementation of BERT Base model in \"Transformers\"  2  . The pre-trained parameters are deployed for initializing BERT. Other parameters are randomly initialized. All the hyperparameters are keeping default. The backbone of the H-BERT is a 6-layer, 12-head-attention, and 768 hidden-unit Transformer encoder implemented using torch.nn.TransformerEncoder 3  in PyTorch. The parameters of the backbone Transformer are randomly initialized. We use AdamW  [26]  as the optimizer with the following setup: an initial learning rate of 6e -6, β 1 = 0.9, β 2 = 0.999, L2 weight decay of 0.01, learning rate warms up steps being 0, and linear decay of the learning rate. All the results are based on an average of 5 runs.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Comparing Methods And Metrics",
      "text": "We investigate previous ERC methods based only on the textual modality:\n\n• scLSTM  [35]  is the earliest study that we can track in the task of ERC. It makes predictions by only considering intra-interlocutor dependency. • TL-ERC  [14]  applies BERT as a transfer learning module for context modeling. That is, it simply employs BERT as a feature extractor and applies RNN for modeling emotion dynamics in conversation afterwards.\n\n• DRNN  [28]  is DialogueRNN, a hierarchical attentionbased model with three GRUs to capture the emotion dynamics. In the experiment, we compare both DRNN with the CNN and DRNN with the RoBERTa features  [10] ,  [25] , denoted by DRNN †.\n\n• DGCN  [11]  applies GCN to model utterance interactions among interlocutors by considering speaker positions in the historical conversation.\n\n• AGHMN  [17]  finetunes sentence representation and uses GRU to wrap the attention-weighted representations rather than summing them up. • CESTa  [46]  is the SOTA ERC method. It first applies a flat Transformer at the bottom layer to obtain contextualized representation for each utterance in the conversation. Next, it cascades a hierarchical LSTM upon the Transformer to distinguish contextualized representations of interlocutors. Following  [11] ,  [28] , we use the weighted accuracy (ACC) and weighted average F1 (F1) as the evaluation metrics:\n\nwhere p c is the percentage of the class c in the testing set, a c and F 1 c are the corresponding accuracy and F1 score for the class c, respectively. It is worth mentioning that we mainly focus on the average scores because all the methods have trade-off among individual emotion types.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Main Results",
      "text": "Table  III  reports the main results. We can observe that -The Transformer-based models, i.e., CESTa and our proposed models, achieve better performance than RNN-based models on IEMOCAP. Notice that, the results of F-BERT and H-BERT are competitive. By observing the average conversation length on IEMOCAP is 50, we can note that Transformer-based models can effectively capture information for long sentences. -The average ACC and F1 of our ST-BERT-GAT significantly (p < 0.05 in t-test) outperforms the SOTA baselines and attains 5% and 2% improvement on IEMOCAP in terms of ACC and F1 metrics, respectively, while achieving 10% and 8% improvement on MELD in terms of ACC and F1, respectively. By examining more details, we notice that -ST-BERT outperforms TL-ERC (applying BERT), DRNN † (using the RoBERTa features), and CESTa (applying Transformer), which exhibits the superiority of our model structure rather than simply using the pre-training models.\n\n-ST-BERT also outperforms models using the succeeding context, i.e., DRNN, DRNN †, DGCN, and CESTa. This implies that our model is more practical in real conversations.\n\n-ST-BERT also attains the best scores in many individual emotion types, especially \"neutral\". The type of \"neutral\" is hard to distinguish because the \"neutral\" utterances often contain emotional words. For example, the word \"excited\" colored in blue of Table V may mislead the recognition. On the contrary, if the model can distinguish the emotional influence among interlocutors, it could easily discover the \"frustrated\" emotion from the single word utterance \"Yeah.\" because most of the time, the emotions of \"frustrated\" and \"excited\" do not appear simultaneously in a conversation. The ST-BERT can distinguish the emotional influence through independent temporal modeling and different types of fusion. -By comparing different fusion mechanisms in ST-BERT, we notice that the gate mechanism outperforms others. Surprisingly, the attention mechanism performs worse than the one by direct concatenation. We conjecture that the linear combination may not sufficient absorb the two representations defined in Eq.  (18) . Though the attention mechanism in the Transformer can learn the complex interactions between interlocutors, the hierarchical structure does not provide more information gain than the Flat structure.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Ablation Study",
      "text": "Table  IV  shows the ablation study on IEMOCAP to test the effect of the components in ST-BERT, including target utterance, intra-context, inter-context, and pre-training. The first four cases are based on BERT without pre-training, i.e., randomly initializing the parameters. The first case is to test the vanilla BERT Base without including any context information. The following three cases are to test ST-BERT with intra-context, ST-BERT with inter-context, and ST-BERT with both intra-context and inter-context, respectively. Similarly, the last four cases are to test the above four cases by the pretraining BERT Base initialization. The average F1 scores reported in Table  IV  clearly show that\n\n• Without pre-training, our ST-BERT performs even worse than some baselines in Table  III . We conjecture that because IEMOCAP is a small dataset, the deep structure of BERT cannot be well-trained in this dataset. • Though the models perform poorly when applying randomly initialization, our ST-BERT still beats TL-ERC (applying pre-trained BERT). This implies that ST-BERT indeed sufficiently capture emotion dynamics to improve the model performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Case Study",
      "text": "We depict the confusion matrices in the form of heat maps to better understand our proposed BERT-based models. The confusion matrices are based on the results of BERT Base , F-BERT, H-BERT, and ST-BERT on IEMOCAP as shown in Fig.  4 . Notice that the types of \"happy\", \"excited\", and \"neutral\" comprise more easy-to-confuse cases in the positive emotion group while the types of \"sad\", \"angry\", \"frustrated\", and \"neutral\" comprises more easy-to-confuse cases in the negative emotion group. The \"neutral\" type belongs to both groups and is thus particularly hard for recognition. However, our proposed F-BERT, H-BERT, and ST-BERT perform better than BERT Base in recognizing \"neutral\". By comparing Fig.  4(c ) and Fig.  4 (d), we can notice that ST-BERT exhibits more power in distinguishing the negative emotions and obtain higher performance.\n\nTable  V  illustrates a conversation snippet classified by our proposed BERT-based models. There are two challenges in predicting the four utterances. One is to predict the emotion of a short utterance at the second turn. The other is to predict the emotion of an utterance with the misleading word \"excited\" at the fourth turn. The results show that ST-BERT correctly predicts the emotion in all four utterances. H-BERT cannot distinguish the emotion when there is a misleading word. Meanwhile, F-BERT fails in predicting both cases.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we develop a series of BERT-based models to model emotion dynamics in conversation. Two basics, a flatstructured and a hierarchically-structured BERT, are proposed to model the preceding utterance information and the direct dependencies in intra-interlocutors and inter-interlocutors. More importantly, a spatial-temporal-structured BERT is proposed to specifically distinguish emotional influence among interlocutors, so that we can effectively capture the emotion dynamics. We conduct extensive experiments on two popular ERC datasets and demonstrate that our proposed BERT-based models can significantly outperform the SOTA baselines. Detailed ablation study and case study have provided to verify our observations.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , Person B’s emotion at timestamp 6",
      "page": 1
    },
    {
      "caption": "Figure 1: An example of conversations with emotion dynamics",
      "page": 1
    },
    {
      "caption": "Figure 2: Architecture of the temporally unfolded Transformer blocks",
      "page": 3
    },
    {
      "caption": "Figure 2: to naturally exploit the temporal informa-",
      "page": 3
    },
    {
      "caption": "Figure 2: ) to model temporal dependency. A multi-",
      "page": 3
    },
    {
      "caption": "Figure 3: (a), is to directly concatenate the target utterance with",
      "page": 3
    },
    {
      "caption": "Figure 3: Architecture of the proposed BERT-based models given inputs of",
      "page": 4
    },
    {
      "caption": "Figure 3: (b), ﬁrst applies BERT to wrap up an utterance",
      "page": 4
    },
    {
      "caption": "Figure 3: (c), ﬁrst applies BERT to individually wrap",
      "page": 4
    },
    {
      "caption": "Figure 4: Heat map of confusion matrix of four BERT-based models.",
      "page": 7
    },
    {
      "caption": "Figure 4: Notice that the types of “happy\", “excited\", and",
      "page": 7
    },
    {
      "caption": "Figure 4: (d), we can notice that ST-BERT exhibits more",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "#1",
          "A": "",
          "B": "There’s like mystery here,\nthere’s magic. It’s like a\nlittle bit of the unexplain-\nable. I just can’t see how\nyou’re not interested.",
          "GT F H ST": "Neu.Neu.Neu.Neu."
        },
        {
          "Column_1": "#2",
          "A": "Yeah.",
          "B": "",
          "GT F H ST": "Fru. Neu.Fru. Fru."
        },
        {
          "Column_1": "#3",
          "A": "",
          "B": "God, I don’t get it. you\nknow the first time we\ncamehere,yousaiditwas\nthebestnightofyourlife?",
          "GT F H ST": "Neu.Neu.Neu.Neu."
        },
        {
          "Column_1": "#4",
          "A": "",
          "B": "Andlastyear,Iremember\ndistinctly you said, you\nweresoexcitedtogethere\nthat you don’t remember\nyoustubbedyourtoeuntil\nwe were in the car.",
          "GT F H ST": "Neu.Exc. Exc. Neu."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emonet: Fine-grained emotion detection with gated recurrent neural networks",
      "authors": [
        "Muhammad Abdul",
        "Lyle Ungar"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "2",
      "title": "Gated multimodal networks. Neural Computing and Applications",
      "authors": [
        "John Arevalo",
        "Thamar Solorio",
        "Manuel Montes-Y Gomez",
        "Fabio González"
      ],
      "year": "2020",
      "venue": "Gated multimodal networks. Neural Computing and Applications"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merriënboer"
      ],
      "year": "2014",
      "venue": "EMNLP"
    },
    {
      "citation_id": "5",
      "title": "Electra: Pre-training text encoders as discriminators rather than generators",
      "authors": [
        "Kevin Clark",
        "Minh-Thang Luong",
        "Quoc Le",
        "Christopher Manning"
      ],
      "year": "2020",
      "venue": "ICLR"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition on twitter: Comparative study and training a unison model",
      "authors": [
        "Niko Colneriĉ",
        "Janez Demsar"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Lee",
        "Toutanova",
        "Bert"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "8",
      "title": "Xiangnan He, Maarten de Rijke, and Tat-Seng Chua. Advances and challenges in conversational recommender systems: A survey",
      "authors": [
        "Chongming Gao",
        "Wenqiang Lei"
      ],
      "year": "2021",
      "venue": "Xiangnan He, Maarten de Rijke, and Tat-Seng Chua. Advances and challenges in conversational recommender systems: A survey",
      "arxiv": "arXiv:2101.09459"
    },
    {
      "citation_id": "9",
      "title": "Neural approaches to conversational AI",
      "authors": [
        "Jianfeng Gao",
        "Michel Galley",
        "Lihong Li"
      ],
      "year": "2019",
      "venue": "Foundations and Trends in Information Retrieval"
    },
    {
      "citation_id": "10",
      "title": "Rada Mihalcea, and Soujanya Poria. Utterance-level dialogue understanding: An empirical study",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Rada Mihalcea, and Soujanya Poria. Utterance-level dialogue understanding: An empirical study",
      "arxiv": "arXiv:2009.13902"
    },
    {
      "citation_id": "11",
      "title": "Dia-logueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "EMNLP"
    },
    {
      "citation_id": "12",
      "title": "Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "13",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Roger Zimmermann",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "arxiv": "arXiv:1910.04980"
    },
    {
      "citation_id": "15",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Chao-Chun",
        "Sheng-Yeh Hsu",
        "Chuan-Chun Chen",
        "Ting-Hao Kuo",
        "Lun-Wei Huang",
        "Ku"
      ],
      "year": "2018",
      "venue": "In ICLRE"
    },
    {
      "citation_id": "16",
      "title": "Challenges in building intelligent open-domain dialog systems",
      "authors": [
        "Minlie Huang",
        "Xiaoyan Zhu",
        "Jianfeng Gao"
      ],
      "year": "2020",
      "venue": "ACM Trans. Inf. Syst"
    },
    {
      "citation_id": "17",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "18",
      "title": "Higru: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "Wenxiang Jiao",
        "Haiqin Yang",
        "Irwin King",
        "Michael Lyu"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "19",
      "title": "Decision support with text-based emotion recognition: Deep learning for affective computing",
      "authors": [
        "Bernhard Kratzwald",
        "Suzana Ilic",
        "Mathias Kraus",
        "Stefan Feuerriegel",
        "Helmut Prendinger"
      ],
      "year": "2018",
      "venue": "Decision support with text-based emotion recognition: Deep learning for affective computing",
      "arxiv": "arXiv:1803.06397"
    },
    {
      "citation_id": "20",
      "title": "Emotional inertia and psychological maladjustment",
      "authors": [
        "Peter Kuppens",
        "Lisa Nicholas B Allen",
        "Sheeber"
      ],
      "year": "2010",
      "venue": "Psychological science"
    },
    {
      "citation_id": "21",
      "title": "Conversational recommendation: Formulation, methods, and evaluation",
      "authors": [
        "Wenqiang Lei",
        "Xiangnan He",
        "Maarten De Rijke",
        "Tat-Seng Chua"
      ],
      "year": "2020",
      "venue": "Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "22",
      "title": "Sequicity: Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures",
      "authors": [
        "Wenqiang Lei",
        "Xisen Jin",
        "Min-Yen Kan",
        "Zhaochun Ren",
        "Xiangnan He",
        "Dawei Yin"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Have we solved the hard problem? it's not easy! contextual lexical contrast as a means to probe neural coherence",
      "authors": [
        "Wenqiang Lei",
        "Yisong Miao",
        "Runpeng Xie",
        "Bonnie Webber",
        "Meichun Liu",
        "Tat-Seng Chua",
        "Nancy Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis",
      "authors": [
        "Wei Li",
        "Wei Shao",
        "Shaoxiong Ji",
        "Erik Cambria"
      ],
      "year": "2020",
      "venue": "Bieru: Bidirectional emotional recurrent unit for conversational sentiment analysis"
    },
    {
      "citation_id": "25",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "26",
      "title": "Fixing weight decay regularization in adam",
      "authors": [
        "I Loshchilov",
        "Hutter"
      ],
      "year": "2018",
      "venue": "Proceedings of the ICLR 2018 Conference"
    },
    {
      "citation_id": "27",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Yukun Ma",
        "Linh Khanh",
        "Frank Nguyen",
        "Erik Xing",
        "Cambria"
      ],
      "year": "2020",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "28",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "29",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Yuzhao Mao",
        "Qi Sun",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li",
        "Jianping Shen"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation"
    },
    {
      "citation_id": "30",
      "title": "Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon",
      "authors": [
        "M Saif",
        "Peter Mohammad",
        "Turney"
      ],
      "year": "2010",
      "venue": "NAACL workshop"
    },
    {
      "citation_id": "31",
      "title": "How emotions work: The social functions of emotional expression in negotiations",
      "authors": [
        "W Michael",
        "Dacher Morris",
        "Keltner"
      ],
      "year": "2000",
      "venue": "Research in organizational behavior"
    },
    {
      "citation_id": "32",
      "title": "Semi-supervised sequence tagging with bidirectional language models",
      "authors": [
        "Matthew Peters",
        "Waleed Ammar",
        "Chandra Bhagavatula",
        "Russell Power"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "33",
      "title": "Deep contextualized word representations",
      "authors": [
        "Matthew Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee"
      ],
      "year": "2018",
      "venue": "NAACL"
    },
    {
      "citation_id": "34",
      "title": "Affective computing: from laughter to ieee",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "36",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "38",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "39",
      "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "authors": [
        "V Iulian",
        "Alessandro Serban",
        "Yoshua Sordoni",
        "Aaron Bengio",
        "Joelle Courville",
        "Pineau"
      ],
      "year": "2016",
      "venue": "Building end-to-end dialogue systems using generative hierarchical neural network models"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition from text based on automatically generated rules",
      "authors": [
        "Wassim Shadi Shaheen",
        "Hazem El-Hajj",
        "Shady Hajj",
        "Elbassuoni"
      ],
      "year": "2014",
      "venue": "IEEE ICDM Workshop"
    },
    {
      "citation_id": "41",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "42",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "43",
      "title": "End-toend multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "Mihalis Nicolaou"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "45",
      "title": "Constructing learning maps for lecture videos by exploring wikipedia knowledge",
      "authors": [
        "Feng Wang",
        "Xiaoyan Li",
        "Wenqiang Lei",
        "Chen Huang",
        "Min Yin",
        "Ting-Chuen Pong"
      ],
      "year": "2015",
      "venue": "Pacific Rim Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "47",
      "title": "Google's neural machine translation system: Bridging the gap between human and machine translation",
      "authors": [
        "Yonghui Wu",
        "Mike Schuster",
        "Zhifeng Chen",
        "V Quoc",
        "Le"
      ],
      "year": "2016",
      "venue": "Google's neural machine translation system: Bridging the gap between human and machine translation"
    },
    {
      "citation_id": "48",
      "title": "Generalized relation learning with semantic correlation awareness for link prediction",
      "authors": [
        "Yao Zhang",
        "Xu Zhang",
        "Jun Wang",
        "Hongru Liang",
        "Wenqiang Lei",
        "Zhe Sun",
        "Adam Jatowt",
        "Zhenglu Yang"
      ],
      "year": "2020",
      "venue": "Generalized relation learning with semantic correlation awareness for link prediction",
      "arxiv": "arXiv:2012.11957"
    }
  ]
}