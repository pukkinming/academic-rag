{
  "paper_id": "2502.16485v1",
  "title": "Sda-Dda: Semi-Supervised Domain Adaptation With Dynamic Distribution Alignment Network For Emotion Recognition Using Eeg Signals",
  "published": "2025-02-23T07:54:31Z",
  "authors": [
    "Jiahao Tang"
  ],
  "keywords": [
    "Affective brain-computer interfaces",
    "Emotion recognition",
    "Electroencephalogram",
    "Individual variability",
    "Transfer learning",
    "Semi-supervised domain adaptation",
    "Dynamic distribution alignment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we focus on the challenge of individual variability in affective brain-computer interfaces (aBCI), which employs electroencephalogram (EEG) signals to monitor and recognize human emotional states, thereby facilitating the advancement of emotion-aware technologies. The variability in EEG data across individuals poses a significant barrier to the development of effective and widely applicable aBCI models. To tackle this issue, we propose a novel transfer learning framework called Semi-supervised Domain Adaptation with Dynamic Distribution Alignment (SDA-DDA). This approach aligns the marginal and conditional probability distribution of source and target domains using maximum mean discrepancy (MMD) and conditional maximum mean discrepancy (CMMD). We introduce a dynamic distribution alignment mechanism to adjust differences throughout training and enhance adaptation. Additionally, a pseudolabel confidence filtering module is integrated into the semisupervised process to refine pseudo-label generation and improve the estimation of conditional distributions. Extensive experiments on EEG benchmark databases (SEED, SEED-IV and DEAP) validate the robustness and effectiveness of SDA-DDA. The results demonstrate its superiority over existing methods in emotion recognition across various scenarios, including cross-subject and cross-session conditions. This advancement enhances the generalization and accuracy of emotion recognition, potentially fostering the development of personalized aBCI applications. The source code is accessible at https://github.com/XuanSuTrum/SDA-DDA.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are crucial to the human experience, exerting profound influence on cognitive processes and decision-making. They deeply affect human thoughts shaping, behaviors, and interpersonal relationships. The accurate monitoring, recognition, and regulation of emotions have profound influence in improving the lives of individuals, and also optimize the performance of specific task operators. Various characteristics have been adopted in recognizing human emotional states, such as facial expression, gestures, tone of voice, and physiological signals  [1] ,  [2] . Among these, physiological signals have gained widespread adoption for their objectivity of assessment. Discriminating emotional states with physiological signals can be achieved through various techniques, including ECG, EMG, EOG, respiration rate, and EEG  [3] . EEG, known for recording the brain's electrical activity, is commonly employed in emotion monitoring via affective brain-computer interfaces (aBCI), attracting considerable academic interest.  [4] .\n\nPresently, the inherent variations in individuals' physiological structures and brain activities present significant challenges in the development of affective recognition systems based on EEG signals. These variations lead to psychological states, emotional responses, and regulation strategies when individuals face emotional stimuli. Developing a model capable of adapting to individual differences with limited labeled data remains a major obstacle. A promising approach to tackle this challenge involves utilizing transfer learning techniques.  [5] . Transfer learning leverages knowledge obtained from one dataset or domain (commonly referred to as the source domain (D s )) to improve the performance of learning tasks in a different dataset or domain (referred to as the target domain (D t )). Specifically, in EEG-based emotion detection, the source domain typically consists of labeled EEG signals collected from multiple participants, whereas the target domain includes unlabeled EEG data  [6] . By incorporating transfer learning methods, the disparity in individual differences can be mitigated, facilitating improved emotional brain-computer interfaces  [7] . This methodology enables the model to utilize prior knowledge extracted from the source domain, which comprises data from diverse subjects, to enhance its ability to interpret emotional states in the target domain. This enhancement proves particularly valuable when the target domain contains only unlabeled EEG data  [8] . Domain adaptation, a critical area within transfer learning, provides an effective approach to bridging the disparities in data distributions between D s and D t datasets  [6] . In domain adaptation, it is assumed that the feature space, conditional probability distribution(CPD) and label space of both domains remain consistent, while variations exist in their margin probability distribution(MPD).\n\nBased on these assumptions and methodologies, researchers have successfully addressed the issue of individual differences between the source and target domains in emotional braincomputer interfaces  [9] -  [11] . Within the domain of EEG emotion recognition, multiple strategies have been developed to tackle challenges associated with domain adaptation and feature extraction. One noteworthy approach is multi-source marginal distribution adaptation (MS-MDA), which assumes that EEG data across various sources share low-level characteristics. It uses separate branches for each source domain to enable one-to-one domain alignment and to extract features specific to each domain  [12] . Another prominent method, adversarial discriminative temporal convolutional networks (AD-TCN), focuses on domain adaptation by maintaining consistent graph-based feature representations across domains while simultaneously capturing domain-specific variations  [13] . Collectively, these approaches aim to address the challenges of domain adaptation and feature extraction, contributing to the advancement of EEG affective recognition systems.\n\nWhile these methods show promise, the majority of research on EEG-based emotion recognition has focused primarily on aligning the MPD across domains, often neglecting the alignment of the CPD. Aligning the MPD alone fails to account for the intrinsic variations that exist within each emotion category across individuals. This limitation restricts the model's ability to capture discriminative, domain-invariant features that are crucial for accurate emotion recognition. As a result, the alignment of CPDs within emotional categories continues to be a critical but largely overlooked challenge in the field.\n\nThis paper introduces a novel transfer learning framework, SDA-DDA, specifically designed to enhance cross-subject emotion recognition using EEG dataset. SDA-DDA consists of four distinct modules: MPD alignment via maximum mean discrepancy, CPD alignment via conditional maximum mean discrepancy, joint distribution dynamic adjustment, and a semi-supervised pseudo-label optimization algorithm. The key contributions and innovations presented in this study are summarized below:\n\n(1)Our framework advances EEG-based emotion recognition by dynamically aligning both MPD and CPD between source and target domains. Contrary to traditional methods which use static alignment, SDA-DDA's dual distribution alignment approach continuously adapts to changes in both types of distributions throughout training. MMD handles marginal distribution alignment using kernelized moment matching, while CMMD is employed to minimize differences in conditional distributions for each emotion category. This dual approach enhances the model's adaptability to individual variations among subjects, providing a more robust solution for cross-subject emotion recognition.\n\n(2)To address the challenges of semi-supervised learning with EEG data, we introduce a dynamic confidence threshold adjustment mechanism. Pseudo-labels are generated from unlabeled data during training. To maintain the quality of pseudo labels, only high-confidence samples are selected to enhance the learning process. By dynamically adjusting this confidence threshold, our model selectively filters pseudolabels to improve the quality of semi-supervised learning, making it more robust to noise in the unlabeled data and enhancing the overall performance of the affective brainmachine interface.\n\nIn summary, the proposed SDA-DDA framework solves the limitations of existing methods by introducing dual distribution alignment and a dynamic confidence-based filtering mechanism. These innovations enable SDA-DDA to more effectively handle the unique challenges of cross-subject emotion recognition with EEG data, leading to improved classification accuracy and stability in affective brain-machine interfaces. Notations and descriptions used in this paper is shown in",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Background A. Eeg Emotion Recognition With Deep Learning",
      "text": "Many studies have used classical machine learning methods to realize EEG-based emotional state recognition. These methods mainly involve two typical steps: feature extraction and classifier classification. However, machine learning approaches for emotional EEG signal recognition have several drawbacks, such as their high-dependence on manual feature engineering, the requirement for domain expertise to extract relevant features from EEG signals, and limited applicability in large-scale data settings due to a lack of automatic feature learning  [14] .\n\nAdvancements in research have propelled deep learning techniques into widespread use, particularly in the field of affective detection. These methods exhibit remarkable performance across diverse tasks, particularly when utilized for analyzing EEG signals  [15] . In contrast to traditional machine learning methods, deep learning offers significant advantages, such as enhanced learning capabilities and adaptability for handling extensive datasets. One proposed approach focuses on an emotion detection method leveraging multi-channel EEG data. This technique creates a three-dimensional representation by combining spatial and spectral features. By integrating a feature fusion module with convolutional neural networks, it achieves notable accuracy in emotion identification, with classification rates of 89.67% for arousal and 90.93% for valence  [16] . Another innovative model addresses the limitations of low accuracy in emotion recognition systems for braincomputer interfaces. This approach incorporates the intricate characteristics of EEG signals, accounting for the brain's spatial organization and the temporal aspects of emotional states. EEG feature tensors, categorized by brain regions, are subsequently processed through a hybrid architecture combining convolutional neural networks with bidirectional long short-term memory networks. This structure effectively enhances the system's recognition accuracy. Simulation results further validate the model's performance, achieving an average accuracy of 94% on the DEAP dataset and 94.82% on the SEED dataset  [17] .\n\nThe methods mentioned above have demonstrated impressive accuracy in intra-individual affective detection. However, EEG-based affective detection models which achieved high accuracy with subjects may not generalize well on crosssubjects tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Eeg Emotion Recognition With Semi-Supervised Domain Adaptation",
      "text": "In real-world scenarios, obtaining fully labeled datasets poses a significant challenge, often leading to a large proportion of unlabeled data. Semi-supervised learning addresses this issue by acting as an intermediary between supervised and unsupervised approaches. By leveraging a small subset of labeled data in conjunction with a substantial amount of unlabeled data during training  [18] , semi-supervised learning emerges as a promising solution for scenarios with limited labeled information.\n\nIn the context of EEG-based affective detection, numerous models employ semi-supervised domain adaptation methods. Among these, domain adversarial neural networks and deep domain confusion (DDC) techniques have shown considerable success in earlier studies. The DDC method utilizes a deep convolutional neural network (DCNN) as its foundational model and incorporates a domain confusion layer to minimize discrepancies in feature distributions between source and target datasets. By simultaneously optimizing domain confusion and classification losses, the DDC approach enables the extraction of robust, domain-invariant features that remain unaffected by domain variations  [19] . This methodology equips the model to generalize effectively to new domains, making it suitable for both supervised and unsupervised adaptation tasks.\n\nResearchers have also explored other approaches to realizing domain invariance of encoded representations. For example, a multi-source learning architecture utilizing the maximum mean discrepancy loss has been proposed, which aligns domains through dataset-specific private encoders  [20] . Additionally, an adversarial domain adaptation technique utilizing a multi-branch capsule network (DA-CapsNet) is employed to further reduce discrepancies between the data distributions of source and target domains. This enhancement significantly improves cross-subject emotion recognition performance  [21] .\n\nTo further advance domain adaptation in emotion recognition, researchers have explored integrated frameworks that combine MPD and CPD. For instance, researcher proposed a unified framework, achieving joint probability distribution adaptation training without the need for D t label  [22] .\n\nIn contrast to existing methods, our proposed SDA-DDA framework offers several distinctive innovations that optimize cross-subject emotion recognition in EEG-based applications. First, SDA-DDA removes the need for complex adversarial training strategies and complicated feature extraction steps, making the framework more computationally efficient and easier to implement. Second, SDA-DDA employs a dynamic distribution adaptation mechanism that continuously adjusts according to discrepancies between MPD and CPD during model training. This adaptability enables the model to effectively capture the variable relationship between these distributions, improving its ability to generalize across subjects. Finally, our approach incorporates a pseudo-label filtering mechanism based on a dynamic confidence threshold. This mechanism systematically filters pseudo-labels to retain only the most reliable samples, thereby enhancing the accuracy of conditional distribution alignment and reinforcing the discriminative capacity of learned features. Collectively, these features make SDA-DDA a robust, scalable approach for emotion recognition through domain adaptation, offering significant advancements over existing methods.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. The Proposed Approach",
      "text": "This paper presents a novel learning framework for crosssubject affective detection based on EEG data. The framework consists of four key modules: The MPD alignment based on maximum mean discrepancy, the CPD alignment based on conditional maximum mean discrepancy, the joint distribution dynamic adjustment, and a semi-supervised pseudo-label optimization algorithm.\n\nAs depicted in Fig.  1 , the proposed framework begins by extracting features from the D s and D t , aligning their MPDs using the MMD algorithm. A classifier is then employed to generate label predictions for both domains. To enhance performance, the framework minimizes cross-entropy loss between the predicted and actual labels in the source domain. Additionally, it incorporates a loss term addressing the CPD of the predicted labels in the D t , ensuring effective domain adaptation. During training, the model dynamically adjusts the weights for both MPD and CPD distributions, considering their individual contributions to the overall adaptation process. Furthermore, a screening mechanism is employed to improve the quality of pseudo-labels assigned to the D t . The entire framework is trained using backward propagation, minimizing three aforementioned loss components. This training process enables adaptability adjustments and optimizes the classification performance of the model, facilitating accurate crosssubject emotion recognition based on EEG data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Feature Extraction",
      "text": "In the domain of EEG signal analysis, a meticulous process of feature extraction was conducted. Firstly, a specific time segment of interest was carefully selected from the EEG signal to capture relevant temporal information effectively. Subsequently, the short-time Fourier transform (STFT) was applied to decompose the EEG signal into five distinct frequency bands: δ, θ, α, β, and γ. Within each frequency band, the probability density function of the signal was computed to reveal the distribution of signal amplitudes.\n\nThe core of this feature extraction process lies in the calculation of the differential entropy, a metric that quantifies the level of uncertainty and complexity inherent in the EEG signal. The differential entropy was computed using the following formula:\n\nIn Eq.(  1 ), H(X) represents the differential entropy, p(x) denotes the probability density function that characterizes the amplitude distribution of the signal within the specific frequency band, and x represents the amplitude of the signal.\n\nThis feature extraction procedure was systematically applied to each of the frequency bands, calculating differential entropy values for each individual band. Consequently, each EEG sample is associated with a comprehensive 310-dimensional differential entropy feature vector, which incorporates information from the five frequency bands across the 62 EEG channels. This feature representation provides a rich and informative basis for in-depth analysis and interpretation of EEG data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Alignment Of Marginal Probability Distribution",
      "text": "In this model, aligning the MPDs of the D s and D t is crucial. To measure the divergence between these distributions, the Maximum Mean Discrepancy is utilized as a key metric. The MMD-based distance is formally defined as follows:\n\nHere, M M D (D s , D t ) represents the MMD-based distance between the D s and the D t . The measurement of this distance is carried out via the Φ function, which skillfully maps the data from both domains into a RKHS denoted as H. According to the theorem ||A|| 2 = tr(AA T ) and tr(AB) = tr(BA), equation (  2 ) can be rewritten as follows:\n\nWhere\n\nnm , otherwise The MMD is realized through Eq. (  4 ) as presented below:\n\nWe employ the Gaussian kernel function, denoted as\n\n, as our kernel function. The Gaussian kernel function and the MMD metric facilitate the efficient evaluation of distributional dissimilarity between the D s and D t .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Alignment Of Conditional Probability Distribution",
      "text": "In this section, we address a crucial aspect often neglected in the majority of transfer learning studies with respect to EEG signals. These studies predominantly concentrate on aligning MPDs but tend to overlook the pivotal role played by aligning CPD in overall data harmonization. To address this gap, we introduce a novel approach termed conditional maximum mean discrepancy. CMMD is specifically designed to facilitate alignment of CPD across distinct emotional categories within EEG data.\n\nEffective alignment of CPD plays an important role in achieving robust distributional adaptation. However, when dealing with Q t (y u t |x u t ) and P s y l s |x l s , we encounter two significant challenges. Firstly, obtaining the posterior probability distribution presents a formidable task. Moreover, the lack of labels in the D t adds another layer of complexity to the problem. In order to address these issues, this paper introduces two key assumptions to guide our approach:\n\n1) To mitigate the lack of labeled data in the D t , we introduce an approach based on pseudo-labeling. Here, the deep neural network's outputs, denoted as ŷu t = f (x u t ), act as pseudo-labels for D t samples. The reliability of these pseudo-labels increases incrementally during the iterative process. 2) Given the complexities in estimating posterior probabilities Q t (y u t |x u t ) and P s y l s |x l s , we shift our focus towards exploring the sufficient statistics of classconditional distributions, namely, P (x l s |y l s = c) and Q(x u t |y u t = c) for each class c ∈ {1, . . . , C}. This allows us to effectively match class-conditional distributions across D s and D t .\n\nAssuming that Φ and Ψ represent nonlinear mappings of D s and D t , respectively, we utilize conditional kernel mean embedding to project the CPD into a series of points in the RKHS. The conditional kernel mean embedding is defined as Eq. (  5 ). In this context, c signifies the c-th category among the C labels, where n c s and n c t indicate the number of samples belonging to category c in the source and target datasets, respectively. Additionally, x l s c corresponds to the i-th labeled sample in the D s datasets for category c, and (x u t ) c corresponds to the i-th pseudo-labeled sample in the D t datasets for the same category.\n\nAs depicted in Fig.  1 , the D t undergoes classification to obtain pseudo-labels, and the results of these pseudo-labels are input into the CMMD computation formula (Eq. (  5 )). Therefore, the quality of pseudo-labels significantly affects the efficiency of aligning CPD.\n\nTo enhance the performance of domain adaptation learning and allow the model to selectively retain D t samples with high classification confidence across different training stages, this paper proposes a dynamic confidence threshold strategy. High-quality pseudo-labels Dt can be selected using Eq. (  6 ).\n\nwhere τ ∈ [0, 1], Γ (•) is confidence threshold mechanism. The Γ (•) function is illustrated in Fig.  2 . The strategy is contingent upon the training stage, dynamically adjusting the confidence threshold accordingly. In the early stages (0 ≤ epoch < 10), a low threshold of 0 is set to fully includes all D t samples. During the training progresses, the threshold gradually increases to 0.5 in the middle stage (10 ≤ epoch < 40) and further to 0.75 in the late stage (40 ≤ epoch ≤ 85), aiming to selectively retain D t samples with higher classification confidence. Beyond this range, the threshold is set to 1, indicating a requirement for higher confidence in retaining samples.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Loss Function And Dynamic Adjustment Factor",
      "text": "The framework proposed in this study considers three distinct loss functions: the classification loss for the D s , the loss associated with MPD disparities, and the loss for differences in CPD.\n\nThe proposed framework primarily focuses on sample differentiation by optimizing the label loss. Specifically, it aims to minimize the cross-entropy loss between the true labels and the predicted labels of D s samples. The loss function can be defined as follows:\n\nHere, N represents the total number of samples, and C stands for the number of categories. In the context of this equation, y l s signifies the value located at the i-th position within the ground-truth label (in the form of a one-hot vector), while p l s c represents the predicted probability that the observation sample i belongs to category c.\n\nAs depicted in Fig.  1 , following the computation of MMD loss and CMMD loss, the relative significance of the MPD and CPD is dynamically adjusted through the introduction of weighting parameters α and β.\n\nThe parameter α adjusts the weight of the MMD loss throughout the training process. During the early stages of training, the model ensures comprehensive capture of the MPD differences between the D s and D t by setting the MMD weight to τ h . As the training progresses iteratively, the MMD weight is gradually reduced to balance its contribution with CMMD, allowing the model to adapt to the D t . As training progresses and the model reaches greater stability, decreasing the MMD weight to τ l increasingly emphasizes CPD alignment, mitigating the risk of overfitting of the D s to the D t . The weighting parameter β is given by:\n\nThe ε represents the step function. represents the step function. The above formulas dynamically adjusts the weighting factor of the CPD. The minimization of the above equations leads to a reduction in the CPD between the D s and D t , thereby contributing to the alignment of the CPD..\n\nThe overall algorithm of this paper is summarized in Algorithm 1.\n\nAlgorithm 1 : Algorithm of the SDA-DDA model\n\nOutput: The predicted label of the target domain 1: Random a mini-batch D s batch andD t batch from D s , D t respectively; 2: Extract the EEG common features and Calculate the MMD loss by Eq.(  2 ); 3: Generate predict pseudo-labels for each sample x u t ∈ D t batch by classifier; 4: Pseudo-labels are filtered using a dynamic confidence threshold adjustment mechanism during the training process by Eq.(  6 ). 5: Estimate the CMMD loss and cross-entropy loss by Eqs.\n\n(5) and (  7 ) respectively; 6: Update the network parameters by gradient descend to minimize Eq.(  9 ) 7: return The predicted label of the D t",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Emotion Datasets",
      "text": "The primary datasets utilized in this study are SEED and SEED IV, which are publicly accessible through the Brainlike Computing and Machine Intelligence Center at Shanghai Jiao Tong University. In the SEED dataset, experiments were conducted with fifteen Chinese participants, each completing three sessions comprising a total of fifteen trials. Participants were exposed to Chinese film clips designed to evoke a range of emotional states, including positive, neutral, and negative emotions  [23] . In the SEED IV dataset, experiments were conducted with 15 subjects across three sessions on separate days, with each session including 24 trials.Participants in each trial watched film clips designed to elicit emotions such as happiness, sadness, neutrality, or fear. EEG data for the SEED and SEED IV datasets were recorded with a 62-channel ESI NeuroScan System.  [24] .\n\nIn the DEAP dataset, 32 participants (equally split between male and female) viewed 40 one-minute music videos while their EEG signals were recorded from 32 electrodes. Postviewing, they rated arousal, valence, dominance, and liking using self-assessment manikins on a continuous scale from 1 to 9  [25]",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "In the model presented in this paper, the feature extractor is composed of a pair of fully connected layers. The initial layer, denoted as \"fc1,\" processes input data spanning 310 dimensions and reduces it to 64 dimensions using a ReLU activation function. Subsequently, the data flows through the second layer, labeled \"fc2,\" further reducing the dimensionality to 64, accompanied by an additional ReLU activation. To improve performance and prevent overfitting, dropout layers with a 0.25 dropout rate are applied after each fully connected layer. For the parameters α, τ h and τ l are 1, 0.01, respectively. For the Eq. (  8 ) , ρ 0 and ρ 1 are 0.1 and 0.15, respectively. For our experimental setup, we utilize all labeled source samples to represent the D s , while the unlabeled target samples constitute the D t . We evaluate and compare the average classification accuracy as a performance measure. Our training configuration encompasses several essential parameters and procedural steps. The batch size for training is configured to 32, and the training process spans 10 epochs. We employ two distinct learning rates, specifically 0.001 and 0.01. The SGD momentum parameter is set to 0.9. Moreover, the option to enable or disable CUDA training is provided to the user. Additionally, we initialize a random seed value of 3 to ensure result reproducibility, and the L2 weight decay is precisely set at 5×10 -4 . During training, we employ the SGD optimization method, with learning rates subject to dynamic adjustments at the beginning of each epoch. These adaptive changes in learning rates follow a specific mathematical formula. This iterative training process continues for the designated number of epochs, allowing the model to learn and adapt over time.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Experiment Setting",
      "text": "In this study, the SEED, SEED IV, and DEAP datasets served as the primary data sources for training models and assessing cross-subject emotion detection performance. To thoroughly assess the effectiveness of the proposed model, two distinct cross-validation strategies were adopted. These methodologies aimed to provide a comprehensive evaluation of the model's performance across various subjects.\n\nWe adopted a commonly used strategy for cross-subject affective detection. In this setup, the EEG data of one subject is designated as the D t , while the combined data from all other subjects form the D s . This procedure is repeated iteratively, allowing each subject to be tested as the D t .\n\nAdditionally, to thoroughly assess the model's performance across sessions for each subject, we adopted a cross-subject, cross-session leave-one-subject-out cross-validation approach. In each iteration, all session data from one subject served as the D t , while data from the remaining subjects' sessions formed the D s . This iterative approach ensured that every subject contributed as the D t at least once, enabling the calculation of average detection performance across all subjects.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Results",
      "text": "In Tables II, III and IV, we conducted a comprehensive evaluation of various representations on the SEED, SEED IV and DEAP datasets using the leave-one-subject-out crossvalidation method with a cross-subject single-session protocol. Our method demonstrated significant performance advantages on the SEED dataset, achieving an accuracy of 87.27%± 07.55%. Similarly, on the SEED IV dataset, our method exhibited competitive accuracy of 74.01%±11.34%. In the DEAP dataset, SDA DDA achieves 61.44±07.15 for valence and 62.86±10.58 for arousal. These results strongly validate the substantial performance improvements achieved by our method on both datasets, surpassing the field average and demonstrating notable potential in the field of emotion recognition tasks. These findings provide compelling evidence supporting the effective application of our method in realworld scenarios.\n\nAnother crucial consideration for emotion brain-computer interfaces is the substantial variability observed among different subjects across various sessions. The evaluation approach of cross-subject and cross-session poses a significant challenge for EEG-based emotion recognition models, requiring robust techniques for effective adaptation. To further validate this detection approach, which connect more closely with realworld application scenarios, we conducted experiments and obtained outstanding three-class classification performance on the SEED dataset, achieving an accuracy of 80.78%±06.12% (see Table  V ). Additionally, on the SEED-IV dataset, our model achieved a four-class accuracy of 69.55%±08.14% (see Table  V ). Compared to existing research, the proposed SDA-DDA method demonstrated industry-leading performance with a smaller standard deviation. These results indicate that the proposed SDA-DDA method exhibits excellent stability and generalization capabilities in handling subject and session differences.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Confusion Matrices",
      "text": "To comprehensively evaluate the performance of the proposed model across various emotional categories, we constructed confusion matrices and conducted a thorough comparative analysis with relevant literature. As illustrated in Fig.  3 , the model achieved impressive accuracy rates of 82.51%, 88.20%, and 91.10% for classifying negative, positive, and neutral emotions, respectively. It is noteworthy that the model has superior performance in classifying positive emotions compared to negative and neutral ones. Despite a slightly lower accuracy in negative emotion classification (82.51%) compared to the other two categories, it still surpasses the performance of existing affective detection algorithms. This highlights the model's effectiveness in recognizing negative emotions, showcasing its distinct advantage over alternative approaches in the field. These findings underscore the robustness of the model and its potential to outperform existing methods, particularly in the nuanced classification of emotions, thereby providing valuable insights for future advancements in emotion recognition research. Fig.  3 . Confusion matrices of different models:RGNN  [44] , JTSR  [45] , Da-CapsNet  [21]  and SDA-DDA. The predicted labels are shown on the horizontal axis, and the true labels are represented on the vertical axis.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "F. Ablation Study",
      "text": "This section presents a comprehensive set of ablation experiments to assess the efficacy of the proposed model.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "1) Experimental Setup:",
      "text": "To ensure fairness and reproducibility, the model parameters used in the ablation experiments are kept consistent throughout. The ablation realization models are described as follows:\n\nEXP1: The baseline model employs a feature extractor and classifier to process features from both the D s and D t , focusing on classification in the D t .\n\nEXP2: This variant enhances the baseline model by integrating the MMD mechanism to reduce the MPD gap between the D s and D t .\n\nEXP3: Building upon EXP2, this model incorporates CMMD to minimize differences in the CPD between the D s and D t .\n\nEXP4: Building on the baseline model, this variant incorporates MPD alignment and CPD alignment but excludes the use of dynamic distribution weights.\n\nEXP5: Extending the baseline model, this approach introduces αMMD and βCMMD without involving confidence filtering.\n\nEXP6: This model incorporates dynamic distribution alignment and a confidence filtering mechanism into the original model architecture.\n\nThese distinct experimental configurations enable a systematic exploration of the contributions of each component, Offering meaningful insights into the model's behavior across different scenarios.\n\n2) Ablation Results: The ablation experiments, detailed in Table  6 , were performed to evaluate the impact of the individual components of the proposed model. The baseline model (EXP1) demonstrated an accuracy of 81.35% ± 03.94%. The introduction of MMD in EXP2 improved accuracy to 86.47% ± 08.32%, demonstrating the effectiveness of reducing the distribution gap. By incorporating CMMD in EXP3, the emphasis shifted towards reducing differences in CPD, leading to an accuracy of 85.75% ± 06.59%. The combination of MMD and CMMD in EXP4 (85.20% ± 05.80%) demonstrated a harmonious performance in aligning both global and conditional distributions. Substantial improvement can be observed in EXP5 (86.50% ± 07.90%), where weighted MMD and CMMD were introduced without involving confidence filtering. The comprehensive model, EXP6, which integrated MMD, CMMD, and confidence filtering, exhibited the highest accuracy at 87.27% ± 07.55%, demonstrating the effectiveness of the confidence filtering mechanism in optimizing model performance. In summary, these findings provide nuanced insights into the contributions of each component, illustrating their roles in enhancing model accuracy across diverse experimental configurations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "G. Sensitivity Analysis For Hyperparameters 1) Sensitivity Analysis For Batch-Size And Training Epoch:",
      "text": "This section analyzes the model's accuracy sensitivity to variations in batch size and training epochs, helping to identify configurations that maximize performance while maintaining stability.\n\nAs illustrated in Figure  4 , the model's accuracy varies across different batch sizes  (32, 64, 128, and 256) . The analysis reveals that with a batch size of 32, the model achieves an average accuracy of approximately 84.66% with a standard deviation of about 7.40%, indicating relatively high variability and suggesting that smaller batches may lead to less stable training due to noisier gradient updates. Increasing the batch size to 64 improves the average accuracy slightly to around 85.91%, while reducing the standard deviation to approximately 6.47%, which points to improved stability as shown by narrower error bars. At a batch size of 128, the model reaches its highest average accuracy of approximately 87.27%, with a standard deviation of about 7.56%; this configuration appears optimal, achieving peak accuracy with moderate variability, as evidenced by relatively contained error bars. However, when the batch size is increased to 256, the average accuracy decreases to 83.43% and the standard deviation rises to 9.82, resulting in increased variability and decreased stability, illustrated by wider error bars. This trend suggests that while larger batch sizes, such as 256, may offer computational efficiency, they can also lead to less precise gradient estimates, potentially degrading model performance. In summary, based on these observations from Figure  4 , a batch size of 128 is recommended as it maximizes accuracy while maintaining training stability, balancing performance and variability more effectively than both smaller and larger batch sizes.\n\nAs illustrated in Figure  5 , we conducted a hyperparameter sensitivity analysis to assess the impact of training epochs on model performance. From epoch 50 to epoch 700, we recorded the model's average accuracy and standard deviation (std)  2) Sensitivity analysis for confidence: The confidence threshold settings in this study are illustrated in Figure  2 . During the early phases of training, a higher proportion of D t samples is included to help the model gain a broader understanding of the D t . During the training progress, it plays an important role to guiding the model to prioritize higher-confidence samples, thereby enhancing the stability and reliability of predictions. As training progresses, we gradually increase the confidence threshold. In the final stage, we set the confidence threshold to 1, meaning that only pseudo-labels with 100% confidence are selected. This strict criterion is used to consolidate the model's performance by refining it with only the most reliable D t samples. At the last stage, the model is mature and has captured the essential patterns. Limiting training to the highest-confidence samples helps to prevent any potential degradation of performance.\n\nWe analyzed the model's sensitivity to the confidence threshold hyperparameter by conducting a hyperparameter study over intermediate thresholds, with the batch size fixed at 128 and the number of training epochs set to 100. During this process, confidence1 and confidence2 correspond to different training phases, where confidence1 refers to the threshold between epochs 10 and 40, and confidence2 refers to the threshold between epochs 40 and 85. Specifically, confidence1 takes values between 0.4 and 0.7, while confidence2 takes values between 0.7 and 0.95. We performed a comprehensive grid search over these intermediate confidence thresholds and presented the average accuracy and standard deviation results across 15 subjects in Figure  7 . Following the implementation of the confidence threshold mechanism, both the model's accuracy and standard deviation improved significantly. For instance, by adjusting the combinations of confidence1 and confidence2, the highest accuracy reached 87.26%, with the lowest standard deviation at 7.06. In contrast, under all combinations of confidence threshold settings, the model's accuracy and stability outperformed the scenario without any thresholds (Exp5: 86.5% ± 7.9%).This enhancement is primarily due to the confidence threshold mechanism's ability to effectively filter out low-quality pseudo-labels, thus avoiding interference from unreliable samples, leading to more stable model training and ultimately reaching better generalization performance in the D t .\n\nThrough sensitivity analysis, we found that adjusting confi-dence1 and confidence2 within a reasonable range has a minor impact on model performance. Specifically, accuracy fluctuates between 86.62% and 87.26%, while the standard deviation ranges from 7.06 to 7.58.This result indicates that the selection of the confidence threshold is relatively insensitive to model performance; that is, the model can maintain good accuracy and stability across a broad parameter range. This robustness further confirms the effectiveness of the proposed confidence mechanism, allowing for greater flexibility in hyperparameter selection. It also means that in different application scenarios, strict tuning of confidence thresholds is not necessary to achieve stable performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "H. Representation Visualization",
      "text": "Figure  7  illustrates the experimental outcomes of six distinct model configurations, depicting the feature distributions learned by three participants in the source domain (D s ) and target domain (D t ) within a two-dimensional embedding space using t-SNE. These configurations correspond to the six strategies explored in the ablation study, each representing a unique method or adjustment for domain adaptation.\n\nIn EXP1, a preliminary clustering of data was observed, indicating a baseline level of similarity between D s and D t . This suggests some degree of overlap between the domains, but with no active alignment mechanisms, the separation between the domains was still noticeable. When the MMD algorithm was introduced in EXP2, a more distinct clustering pattern emerged, demonstrating enhanced alignment of features between D s and D t by aligning the marginal probability distributions (MPDs). This improvement indicates that the MMD algorithm effectively narrows the gap between the domains by aligning their marginal distributions, resulting in a more coherent feature representation.\n\nIn EXP3, the integration of the CMMD algorithm alone caused the data from different categories to exhibit clearer separation, with distinct boundaries observed between categories. This suggests that aligning the conditional probability distributions (CPD) facilitates better differentiation of the emotional states across domains, reducing the impact of label noise in affective detection from EEG signals. The alignment of CPDs specifically aids in handling intra-class variations, allowing the model to more accurately segregate emotional states.\n\nThe combined application of both the MMD and CMMD algorithms in EXP4 resulted in a more refined feature distribution. Distinct classifications of different emotional categories were observed in both the source and target domains, further confirming the improved domain adaptation performance. By aligning both MPDs and CPDs, the model was able to optimize feature representations, enhancing its ability to transfer knowledge across domains.\n\nIn EXP5, the introduction of a weighted influence factor allowed for a more fine-grained optimization of the alignment process. By dynamically adjusting the influence of MPDs and CPDs during training, the model improved the overlap between source and target domains. The feature distributions in EXP5 showed a more unified representation across domains, suggesting that the dynamic weighting of domain-specific features significantly enhanced the alignment.\n\nFinally, EXP6 introduced a pseudo-label confidence filtering mechanism, further refining the model's performance. This mechanism not only improved the confidence of pseudolabels but also contributed to a more robust domain adaptation process by ensuring that the model focused on more reliable examples during training. The visualized data distributions in EXP6 demonstrate even greater coherence between the source and target domains, highlighting the positive impact of confidence filtering in improving feature overlap.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "I. Computational Efficiency And Inference Speed",
      "text": "For real-time emotion recognition applications, a framework must maintain high efficiency and low latency to quickly respond to dynamic changes in emotional states. To assess the computational feasibility of our proposed method, we evaluated its floating-point operations (FLOPs), parameter count, batch size, and inference speed on D t . Our model demonstrated computational efficiency suitable for deployment on resource-limited devices, with a total computational load of 3.01 million FLOPs, a lightweight parameter count of only 0.01 million, and a batch size of 128 during testing. Notably, the model achieves an inference speed of approximately 20 milliseconds per batch on the D t data, underscoring its low latency. This combination of computational efficiency and rapid inference speed indicates that our model is promising candidate for real-time affective detection tasks in environments with limited computational resources.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "V. Discussion",
      "text": "The variability between subjects in EEG data poses a significant challenge to the development of effective and widely applicable adaptive brain-computer interface models. To address this issue, we introduce a novel transfer learning framework called Semi-supervised Domain Adaptation with Dynamic Distribution Alignment. To assess the performance of SDA-DDA, we conducted experiments using the SEED, SEED IV, and DEAP datasets. Two validation strategies were employed to ensure a thorough evaluation of the framework's effectiveness. We compared our method with several recently reported approaches, including machine learning-based methods (e.g., SVM, TKL, T-SVM, TCA, KPCA  [26] ,  [35] ,  [36] ) and deep learning-based methods (e.g., RGNN, BiHDM, SA, MFA-LR, DA-CapsNet, MS-MDA, SCSTM-DS, MSFR-GCN  [12] ,  [21] ,  [28] ,  [29] ,  [32] ,  [36] ,  [44] ). The comparison results, summarized in Tables  2 through 6 , highlight the superior performance of SDA-DDA. Our method combines conditional and marginal probability dynamic distribution alignment with a pseudo-label confidence threshold algorithm, achieving notable improvements in classification performance. To further validate the proposed method, we conducted additional studies, including ablation experiments, high-dimensional feature data visualization, and hyperparameter analysis. These analyses confirmed the strong performance of SDA-DDA, demonstrating its ability to effectively align distributions and enhance emotion classification accuracy.\n\nCurrent domain adaptation methods for aligning emotional data distributions predominantly focus on marginal probability alignment  [9] -  [13] , with some models incorporating conditional probability alignment. However, these approaches often treat conditional and marginal probabilities as a combined joint probability distribution  [27] ,  [34] . In this study, we propose a method leveraging MMD and CMMD modules to independently align MPD and CPD without increasing parameters or network complexity. Building on this, our dynamic distribution domain adaptation module comprehensively addresses the differences between these distributions during model training, achieving dynamic alignment of marginal and conditional probabilities. Traditional semi-supervised domain adaptation models for emotion recognition often neglect the impact of pseudo-label reliability on model performance throughout training  [46] -  [48] . To address this, we introduce a pseudo-label confidence threshold module, significantly enhancing classification performance. Notably, our experiments confirm that this pseudo-label selection mechanism is robust to hyperparameter variations. To evaluate the impact of each component in the SDA-DDA framework, we conducted ablation studies. These experiments assessed the contributions of marginal probability alignment, conditional probability alignment, dynamic distribution alignment, and the pseudolabel confidence threshold module to the accuracy of emotion classification. Additionally, we performed high-dimensional data visualization using t-SNE, which provided compelling evidence that our method effectively separates source and target domains as well as their respective categories. Importantly, our model maintains a low computational cost and demonstrates minimal inference latency, making it a promising candidate for deployment on microcontrollers. This opens up opportunities for high-precision emotion recognition in realworld applications.\n\nIn the Future, several promising directions is deserved to research. Firstly, improving Pseudo-label Quality: One possible enhancement is adjusting the pseudo-label confidence threshold during the later stages of model training to ensure higher label quality. However, setting an excessively high threshold may discard too many uncertain pseudo-labels, leading to an imbalance between categories and low pseudolabel utilization  [49] ,  [50] . Future work should focus on optimizing the balance between the quantity and quality of pseudo-labels, potentially through adaptive thresholds or other robust filtering techniques. Another important avenue is the real-time deployment of the SDA-DDA framework within an online Brain-Computer Interface system, facilitating dynamic user interaction. This would allow continuous model updates based on real-time EEG data, facilitating high-precision emotion recognition in dynamic, real-world settings. Testing and integrating our model into such an interactive online BCI environment would provide a strong foundation for personalized emotion recognition systems that can adapt in real-time.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper introduces a novel framework for affective brain-computer interface classification, referred to as Semisupervised Domain Adaptation with Dynamic Distribution Alignment. The framework addresses the challenges posed by data distribution differences, which hinder model accuracy, by aligning the marginal and conditional of the source and target domains. This is achieved through dynamic adjustment factors that balance the relative importance of these distributions. Additionally, a confidence filtering mechanism is incorporated to enhance the credibility of pseudo-labels, thereby improving classification performance. To validate the effectiveness of the proposed framework, extensive experiments were conducted using the SEED, SEED IV and DEAP datasets. Comparative analyses with existing methods demonstrate the superiority of SDA-DDA in terms of generalization and stability.\n\nThis contribution represents a significant advancement in affective brain-computer interfaces, with potential for further research and development. Future enhancements could enable this framework to address more complex transfer learning tasks in real-world scenarios.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed framework begins by",
      "page": 3
    },
    {
      "caption": "Figure 1: The flowchart of the proposed SDA-DDA framework.",
      "page": 4
    },
    {
      "caption": "Figure 2: The pseudo labels confidence threshold mechanism.",
      "page": 5
    },
    {
      "caption": "Figure 1: , the Dt undergoes classification to",
      "page": 5
    },
    {
      "caption": "Figure 2: The strategy is contingent",
      "page": 6
    },
    {
      "caption": "Figure 1: , following the computation of MMD",
      "page": 6
    },
    {
      "caption": "Figure 3: Confusion matrices of different models:RGNN [44], JTSR [45], Da-",
      "page": 8
    },
    {
      "caption": "Figure 4: Average Accuracy by Batch Size for Each Experiment.",
      "page": 9
    },
    {
      "caption": "Figure 4: , the model’s accuracy varies",
      "page": 9
    },
    {
      "caption": "Figure 5: , we conducted a hyperparameter",
      "page": 9
    },
    {
      "caption": "Figure 5: Effect of Epochs on Average Accuracy with Standard Deviatio.",
      "page": 9
    },
    {
      "caption": "Figure 6: Effect of Epochs on Average Accuracy with Standard Deviation.",
      "page": 9
    },
    {
      "caption": "Figure 2: During the early phases of training, a higher proportion of",
      "page": 9
    },
    {
      "caption": "Figure 7: The t-SNE visualization illustrates the 2D embedding space of the learned features from both the source and target domains across six different model",
      "page": 10
    },
    {
      "caption": "Figure 7: Following the implementation",
      "page": 10
    },
    {
      "caption": "Figure 7: illustrates the experimental outcomes of six dis-",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "TSVM* [35]",
          "Valance Pcc(%)": "61.77±08.93",
          "Arousal Pcc(%)": "56.59±11.98"
        },
        {
          "Method": "TPT* [26]",
          "Valance Pcc(%)": "57.43±14.54",
          "Arousal Pcc(%)": "54.76±12.48"
        },
        {
          "Method": "TCA* [36]",
          "Valance Pcc(%)": "56.23±14.33",
          "Arousal Pcc(%)": "51.81±15.03"
        },
        {
          "Method": "KPCA* [26]",
          "Valance Pcc(%)": "54.35±10.22",
          "Arousal Pcc(%)": "58.15±14.96"
        },
        {
          "Method": "SDA-DDA",
          "Valance Pcc(%)": "61.44±07.15",
          "Arousal Pcc(%)": "62.86±10.58"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "TPT [26]",
          "Pcc(%)": "75.17±12.83"
        },
        {
          "Method": "DANN [27]",
          "Pcc(%)": "81.65±09.92"
        },
        {
          "Method": "DA-CapsNet\n[21]",
          "Pcc(%)": "84.63±09.09"
        },
        {
          "Method": "SCSTM-DS [29]",
          "Pcc(%)": "82.29 ±03.60"
        },
        {
          "Method": "MS-ADA [31]",
          "Pcc(%)": "86.16 ±07.87"
        },
        {
          "Method": "SDA-DDA",
          "Pcc(%)": "87.27±07.55"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "KNN* [37]",
          "SEED": "60.66±07.93",
          "SEED IV": "40.83±07.28"
        },
        {
          "Method": "SA* [38]",
          "SEED": "61.41±09.75",
          "SEED IV": "64.44±09.46"
        },
        {
          "Method": "GFK* [26]",
          "SEED": "66.02±07.59",
          "SEED IV": "45.89±08.27"
        },
        {
          "Method": "TCA* [39]",
          "SEED": "64.02±07.96",
          "SEED IV": "56.56±13.77"
        },
        {
          "Method": "CORAL* [40]",
          "SEED": "68.15±07.83",
          "SEED IV": "49.44±09.09"
        },
        {
          "Method": "RF* [41]",
          "SEED": "69.60±07.64",
          "SEED IV": "50.98±09.20"
        },
        {
          "Method": "SVM* [42]",
          "SEED": "68.15±07.38",
          "SEED IV": "51.78±12.85"
        },
        {
          "Method": "DANN* [43]",
          "SEED": "81.08±05.88",
          "SEED IV": "54.63±08.03"
        },
        {
          "Method": "SDA-DDA",
          "SEED": "80.78%±06.12% 69.55%±08.14",
          "SEED IV": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial expression recognition with automatic segmentation of face regions using a fuzzy based classification approach",
      "authors": [
        "Andres Hernandez-Matamoros",
        "Andrea Bonarini",
        "Enrique Escamilla-Hernandez",
        "Mariko Nakano-Miyatake",
        "Hector Perez-Meana"
      ],
      "year": "2016",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "2",
      "title": "A multimodal hierarchical approach to speech emotion recognition from audio and text",
      "authors": [
        "Prabhav Singh",
        "Ridam Srivastava",
        "Rana Kps",
        "Vineet Kumar"
      ],
      "year": "2021",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "3",
      "title": "Infrared navigation-part i: An assessment of feasibility",
      "year": "1959",
      "venue": "IEEE Trans. Electron Devices"
    },
    {
      "citation_id": "4",
      "title": "Affective brain-computer interfaces (abcis): A tutorial",
      "authors": [
        "Dongrui Wu",
        "Bao-Liang Lu",
        "Bin Hu",
        "Zhigang Zeng"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "5",
      "title": "Transfer learning for eegbased brain-computer interfaces: A review of progress made since",
      "authors": [
        "Dongrui Wu",
        "Yifan Xu",
        "Bao-Liang Lu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "6",
      "title": "A survey on transfer learning",
      "authors": [
        "Jialin Sinno",
        "Qiang Pan",
        "Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "7",
      "title": "A review on transfer learning in eeg signal analysis",
      "authors": [
        "Zitong Wan",
        "Rui Yang",
        "Mengjie Huang",
        "Nianyin Zeng",
        "Xiaohui Liu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "8",
      "title": "Can emotion be transferred?-a review on transfer learning for eeg-based emotion recognition",
      "authors": [
        "Wei Li",
        "Wei Huan",
        "Bowen Hou",
        "Ye Tian",
        "Zhen Zhang",
        "Aiguo Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "9",
      "title": "A review of domain adaptation without target labels",
      "authors": [
        "M Wouter",
        "Marco Kouw",
        "Loog"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "10",
      "title": "Domain adaptation for medical image analysis: a survey",
      "authors": [
        "Hao Guan",
        "Mingxia Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "11",
      "title": "St-scgnn: a spatio-temporal selfconstructing graph neural network for cross-subject eeg-based emotion recognition and consciousness detection",
      "authors": [
        "Jiahui Pan",
        "Rongming Liang",
        "Zhipeng He",
        "Jingcong Li",
        "Yan Liang",
        "Xinjie Zhou",
        "Yanbin He",
        "Yuanqing Li"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "12",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Ming Hao Chen",
        "Zhunan Jin",
        "Cunhang Li",
        "Jinpeng Fan",
        "Huiguang Li",
        "He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "13",
      "title": "An adversarial discriminative temporal convolutional network for eeg-based cross-domain emotion recognition",
      "authors": [
        "Zhipeng He",
        "Yongshi Zhong",
        "Jiahui Pan"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition with machine learning using eeg signals",
      "authors": [
        "Omid Bazgir",
        "Zeynab Mohammadi",
        "Seyed Amir",
        "Hassan Habibi"
      ],
      "year": "2018",
      "venue": "2018 25th national and 3rd international iranian conference on biomedical engineering (ICBME)"
    },
    {
      "citation_id": "15",
      "title": "Deep learning for electroencephalogram (eeg) classification tasks: a review",
      "authors": [
        "Alexander Craik",
        "Yongtian He",
        "Jose L Contreras- Vidal"
      ],
      "year": "2019",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "16",
      "title": "A feature-fused convolutional neural network for emotion recognition from multichannel eeg signals",
      "authors": [
        "Qunli Yao",
        "Heng Gu",
        "Shaodi Wang",
        "Xiaoli Li"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "17",
      "title": "A novel de-cnnbilstm multi-fusion model for eeg emotion recognition",
      "authors": [
        "Cui",
        "W Wang",
        "Y Ding",
        "L Chen",
        "Huang"
      ],
      "year": "2022",
      "venue": "mathematics"
    },
    {
      "citation_id": "18",
      "title": "A survey on deep semi-supervised learning",
      "authors": [
        "Xiangli Yang",
        "Zixing Song",
        "Irwin King",
        "Zenglin Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "19",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "Eric Tzeng",
        "Judy Hoffman",
        "Ning Zhang",
        "Kate Saenko",
        "Trevor Darrell"
      ],
      "year": "2014",
      "venue": "Deep domain confusion: Maximizing for domain invariance",
      "arxiv": "arXiv:1412.3474"
    },
    {
      "citation_id": "20",
      "title": "Domaininvariant representation learning from eeg with private encoders",
      "authors": [
        "David Bethge",
        "Philipp Hallgarten",
        "Tobias Grosse-Puppendahl",
        "Mohamed Kari",
        "Ralf Mikut",
        "Albrecht Schmidt",
        "Ozan Özdenizci"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Da-capsnet: A multi-branch capsule network based on adversarial domain adaption for cross-subject eeg emotion recognition",
      "authors": [
        "Shuaiqi Liu",
        "Zeyao Wang",
        "Yanling An",
        "Bing Li",
        "Xinrui Wang",
        "Yudong Zhang"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "22",
      "title": "Cross-subject eeg-based emotion recognition via semi-supervised multi-source joint distribution adaptation",
      "authors": [
        "Magdiel Jiménez",
        "Gibran Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "23",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "24",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W Zheng",
        "W Liu",
        "Y Lu",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "25",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "26",
      "title": "Crosssubject emotion recognition using deep adaptation networks",
      "authors": [
        "He Li",
        "Yi-Ming Jin",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing: 25th International Conference"
    },
    {
      "citation_id": "27",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "Jinpeng Li",
        "Shuang Qiu",
        "Changde Du",
        "Yixin Wang",
        "Huiguang He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "28",
      "title": "Learning a robust unified domain adaptation framework for cross-subject eegbased emotion recognition",
      "authors": [
        "Magdiel Jiménez",
        "Gibran Fuentes-Pineda"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "29",
      "title": "Similarity constraint style transfer mapping for emotion recognition",
      "authors": [
        "Lei Chen",
        "Qingshan She",
        "Ming Meng",
        "Qizhong Zhang",
        "Jianhai Zhang"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "30",
      "title": "Generalized contrastive partial label learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "Wei Li",
        "Lingmin Fan",
        "Shitong Shao",
        "Aiguo Song"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "31",
      "title": "Multisource associate domain adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Qingshan She",
        "Chenqi Zhang",
        "Feng Fang",
        "Yuliang Ma",
        "Yingchun Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "32",
      "title": "Msfr-gcn: A multi-scale feature reconstruction graph convolutional network for eeg emotion and cognition recognition",
      "authors": [
        "Haohao Deng Pan",
        "Feifan Zheng",
        "Yu Xu",
        "Zhe Ouyang",
        "Chu Jia",
        "Hong Wang",
        "Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "33",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "Jinpeng Li",
        "Shuang Qiu",
        "Yuan-Yuan",
        "Cheng-Lin Shen",
        "Huiguang Liu",
        "He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "34",
      "title": "Generalization across subjects and sessions for eeg-based emotion recognition using multi-source attention-based dynamic residual transfer",
      "authors": [
        "Wanqing Jiang",
        "Gaofeng Meng",
        "Tianzi Jiang",
        "Nianming Zuo"
      ],
      "year": "2023",
      "venue": "2023 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "35",
      "title": "A bi-hemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Yang Li",
        "Wenming Zheng",
        "Yuan Zong",
        "Zhen Cui",
        "Tong Zhang",
        "Xiaoyan Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Yang Li",
        "Lei Wang",
        "Wenming Zheng",
        "Yuan Zong",
        "Lei Qi",
        "Zhen Cui",
        "Tong Zhang",
        "Tengfei Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "37",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "authors": [
        "Danny Coomans",
        "Désiré Luc"
      ],
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "38",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "Basura Fernando",
        "Amaury Habrard",
        "Marc Sebban",
        "Tinne Tuytelaars"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "39",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "Ivor Sinno Jialin Pan",
        "James Tsang",
        "Qiang Kwok",
        "Yang"
      ],
      "year": "2010",
      "venue": "Domain adaptation via transfer component analysis"
    },
    {
      "citation_id": "40",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "Baochen Sun",
        "Jiashi Feng",
        "Kate Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "41",
      "title": "Random forests",
      "authors": [
        "Leo Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "42",
      "title": "Least squares support vector machine classifiers. Neural processing letters",
      "authors": [
        "A Johan",
        "Joos Suykens",
        "Vandewalle"
      ],
      "year": "1999",
      "venue": "Least squares support vector machine classifiers. Neural processing letters"
    },
    {
      "citation_id": "43",
      "title": "Franc ¸ois Laviolette, Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "44",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Joint eeg feature transfer and semi-supervised cross-subject emotion recognition",
      "authors": [
        "Yong Peng",
        "Honggang Liu",
        "Wanzeng Kong",
        "Feiping Nie",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "46",
      "title": "Progressive low-rank subspace alignment based on semi-supervised joint domain adaption for personalized emotion recognition",
      "authors": [
        "Junhai Luo",
        "Man Wu",
        "Zhiyan Wang",
        "Yanping Chen",
        "Yang Yang"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "47",
      "title": "Dynamic domain adaptation for class-aware crosssubject and cross-session eeg emotion recognition",
      "authors": [
        "Zhunan Li",
        "Enwei Zhu",
        "Ming Jin",
        "Cunhang Fan",
        "Huiguang He",
        "Ting Cai",
        "Jinpeng Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "48",
      "title": "A novel semi-supervised meta learning method for subject-transfer braincomputer interface",
      "authors": [
        "Jingcong Li",
        "Fei Wang",
        "Haiyun Huang",
        "Feifei Qi",
        "Jiahui Pan"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "49",
      "title": "Flexmatch: Boosting semisupervised learning with curriculum pseudo labeling",
      "authors": [
        "Bowen Zhang",
        "Yidong Wang",
        "Wenxin Hou",
        "Hao Wu",
        "Jindong Wang",
        "Manabu Okumura",
        "Takahiro Shinozaki"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
      "authors": [
        "Yidong Wang",
        "Hao Chen",
        "Qiang Heng",
        "Wenxin Hou",
        "Yue Fan",
        "Zhen Wu",
        "Jindong Wang",
        "Marios Savvides",
        "Takahiro Shinozaki",
        "Bhiksha Raj"
      ],
      "year": "2022",
      "venue": "Freematch: Self-adaptive thresholding for semi-supervised learning",
      "arxiv": "arXiv:2205.07246"
    }
  ]
}