{
  "paper_id": "2211.07290v1",
  "title": "Ai-Based Emotion Recognition: Promise, Peril, And Prescriptions For Prosocial Path",
  "published": "2022-11-14T11:43:10Z",
  "authors": [
    "Siddique Latif",
    "Hafiz Shehbaz Ali",
    "Muhammad Usama",
    "Rajib Rana",
    "Bj√∂rn Schuller",
    "Junaid Qadir"
  ],
  "keywords": [
    "automated emotion recognition",
    "artificial intelligence",
    "ethical concerns",
    "prosocial perspectives"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automated emotion recognition (AER) technology can detect humans' emotional states in real-time using facial expressions, voice attributes, text, body movements, and neurological signals and has a broad range of applications across many sectors. It helps businesses get a much deeper understanding of their customers, enables monitoring of individuals' moods in healthcare, education, or the automotive industry, and enables identification of violence and threat in forensics, to name a few. However, AER technology also risks using artificial intelligence (AI) to interpret sensitive human emotions. It can be used for economic and political power and against individual rights. Human emotions are highly personal, and users have justifiable concerns about privacy invasion, emotional manipulation, and bias. In this paper, we present the promises and perils of AER applications. We discuss the ethical challenges related to the data and AER systems and highlight the prescriptions for prosocial perspectives for future AER applications. We hope this work will help AI researchers and developers design prosocial AER applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automated emotion recognition (AER) is an emerging multidisciplinary research area that leverages advances in artificial intelligence (AI) to algorithmically retrieve a person's emotional state using knowledge from psychology, linguistics, signal processing, and machine learning (ML). Development of AER capabilities can have a transformative effect on society with wide-ranging implications due to the critical role emotions play in human lives ranging from perception, learning, and decision-making  [1]  [2]  [3] . AER is an umbrella term that encompasses various related terms such as affective computing, affect recognition, emotional AI, or artificial emotional intelligence (AEI) that have been proposed in the literature for automated recognition of human emotions  [4] .\n\nIn this paper, we use the term AER and focus on humantarget AER. Human-targeted AER starts with active or passive sensors (e.g., a video camera, microphone, physiological sensor) that mainly captures contextual and behavioural data related to affective facial expressions, speech signals, body pose, gestures, gait, or physiological signals. It is imperative to utilise contextual information while identifying emotions  [5] . The data obtained through the sensing devices extract emotional cues by the AI systems for categorical or dimensional emotion recognition as depicted in Figure  1 .\n\nEmail: siddique.latif@usq.edu.au AER has evolved over the years and achieved remarkable advances; however, it faces various complex and critical challenges that escalate the need for further research to design more trustful and beneficial systems  [11] . Some major challenges faced in AER are:   [6]  (2014) AER Feng et al.  [7]  (2020) AER Batliner et al.  [8]  (2020)\n\nComputational paralinguistics Liu et al.  [9]  (2021) AER (EEG only) Mohammad et al.  [10]  (2022) AER Our paper (2022) AER 1) Unavailability of large datasets, which restricts the exploitation of powerful DL models to achieve better performance  [12] ; 2) Collection of real-life data for modalities such as brain activity and neurotransmitters is very challenging; 3) Varied idiosyncratic nature of human emotions due to which it is hard to accurately recognise them; 4) Judging varying emotions in real-time is hard as most vision and speech-based AER algorithms focus on identifying the peak high-intensity expression by ignoring lower-intensity ones, which can result in inaccuracies; and 5) Cultural differences in manifesting emotions, which makes the problem of developing global AER very difficult.\n\nThe public use of AER services also raises multiple privacy and security-related concerns due to the intimate nature that the AER systems detect, process, recognise, and communicate. This paper presents promising applications of AER and discusses its various perils. In particular, we focus on presenting the ethical concerns related to AER systems and databases to highlight the prescriptions for future AER prosocial systems.\n\nWe note here the specificity or universality of human emotions has been a long-standing debate  [13] . The proponents of the universality of emotions suggest that emotions can be recognised regardless of the different cultural backgrounds. While theoretical studies  [14] ,  [15]  on multicultural studies have suggested six basic universal emotions, current AER systems do not perform well in multicultural settings.\n\nThe novelty and contributions of our paper are highlighted in Table  1 , where we compare this paper with the existing articles on AER. The article by Mohammad et al.  [10]  enlists the ethical challenges for AER and suggests future directions but does not cover AER applications or the challenges related to bias, adversarial attacks, explainability, etc. Similarly, other articles only focus on modalityspecific applications  [9]  or general challenges  [7]  without focusing on ethical issues. In this paper, we attempt to present AER's promise, perils, and ethical concerns. We also provide prescriptions for designing prosocial AER systems. We hope this paper will guide navigating research and ethical implementation choices for anyone who wants to build or use AER for research or commercial purposes.\n\nThe rest of the paper is organised as follows. The promise of AER is described in Section 2. The various perils of AERs are detailed in Section 3. The major ethical concerns with AER are discussed in Section 4. We elucidate a prospective path to a prosocial future for AER in Section 5. The paper is concluded in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Promise Of Aer",
      "text": "AER has a wide range of applications in fields such as healthcare, entertainment, advertising, customer service, transportation, employment decisions, tutoring systems, law enforcement, and human-computer interaction. Applications of AER are classified according to the input signal provided in Table  2 . A broad description of frequently targeted AER applications in different domains is presented in Table  3  and briefly discussed below.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Healthcare",
      "text": "Developing AER methods in healthcare can greatly enhance the quality of life, enable individuals to better understand and control their affective states (e.g., fear, happiness, loneliness, anger, interest, and alertness), and mitigate various psychological issues (that could have resulted in incidents of suicide, homicide, disease, and accident)  [16] . This can greatly improve quality of life and help achieve long-term goals  [13] . It can help save many lives by monitoring people and regulating their emotions through stressful times (e.g., in pandemics or economic crises). It also minimises counterproductive behaviour such as suicidal tendencies and or anti-social behaviour. In healthcare settings, AER services play a pivotal role in shaping the healthcare functionality and communication among professionals, thereby improving professional-patient relations  [17] . It can help design assessment and monitoring of emotional consequences due to different illnesses. For example, AER systems can be",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Law Enforcement And Forensics",
      "text": "Helps identify threats of violence and terrorism. Provides additional aid in criminal investigation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Advertising And Retail",
      "text": "Helps maximise customers' engagement.\n\nHelps retailers to make decisions on product pricing, packaging, branding, etc. Helps improve advertisement strategies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotional And Social Intelligence",
      "text": "Helps influence the mood of the overall population. Helps leaders and decision-makers to handle highly challenging situations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Monitoring And Evaluation",
      "text": "Enables monitoring of employees performance.\n\nEnables monitoring of major psychiatric problems in both military and civilian.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Gaming",
      "text": "Monitors players' emotional states and dynamics during gameplay. Helps design affect-aware video games.\n\npotentially used to monitor the patient-physician relationship in chronic diseases  [18] . This will help improve the management of chronic illnesses. The importance of AER technology has also come to the fore amid the ongoing global economic and public health crisis during the COVID-19 pandemic  [19] . The pandemic situation impacts people physically, mentally, and economically. AER systems can help to analyse and understand emotional responses during such crises affecting mental health. Studies  [20] ,  [21] ,  [22]  show that the negative emotions among the population increase during the pandemic, i.e., COVID-19, and people become optimistic over time by adapting to the pandemic. In global crisis situations like COVID-19, AER systems can help measure cross-cultural emotional trends to learn the correlation among populations despite the socio-economic and cultural differences  [21] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Education",
      "text": "Emotions are very crucial in the education systems due to their important role in the cognitive processes responsible for assimilating and learning new information  [23] . Unfortunately, the current education system fails to track students' emotions and hidden indicators of their internal feelings, thus, making it delicate to adapt and keep the communication channel intact. It has been found that the identification and monitoring of the learner's emotional state greatly facilitates the teacher in taking actions that significantly enhance the tutoring quality and execution and improve student-teacher interactions  [24] . Therefore, it is worthwhile to utilise smart systems that can model the relations between emotions, cognition, and action in the learning context  [25] . In this regard, AER systems can be considered by schools to quantify student moods and engagement in the classroom  [26] . AER could help to reinforce students' attention, motivation, and self-regulation toward studies. It could also help promote effective learning by increasing students' interest  [25] . On the other hand, AER systems can improve certain emotional qualities teachers must have to facilitate pedagogical approaches in education.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Safety",
      "text": "Emotions are directly linked to human problem-solving abilities  [27] . Safety behaviours can be predicted from the individuals' ability to manage and process emotions during a time of stress. There is ample evidence that negative emotions such as anger, fear, and anxiety strongly affect human behaviour and occupational safety  [28] . For example, emotions can impact workplace safety and health. In the workplace, the negative mood of a person can contaminate an entire team or group. This may damage workplace safety and impair team performance. If such behaviours are left unaddressed, negative emotions can be a workplace hazard, with visible effects on team safety. AER systems can provide better solutions to monitor an individual's mood and emotions. In addition, these systems can help find workers who might need help.\n\nIn transport, AER systems can be utilised to improve the safety of drivers as well as anyone on the road. Driving occupies a large portion of our daily life and is often associated with the cognitive load that can trigger emotions like anger or stress, which can badly impact human health and road safety  [29] ,  [30] . Studies  [31] ,  [32]  show that induced negative emotions like anger can decrease a driver's perceived safety and performance compared to neutral and fearful emotional states. AER services are being utilised in automotive environment to monitor the drivers' fatigue, alertness, attention, and stress level to improve automotive and industrial safety by avoiding serious accidents  [29] . Mass adoption of AER systems to monitor psychological and physiological parameters can significantly enhance the detection of dangerous situations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Law Enforcement And Forensics",
      "text": "AER systems are increasingly being used for law enforcement, and forensics, where such systems have many possible applications in identifying threats of violence and terrorism and detecting lies and fraud  [33] . In a forensic investigation, a lie can arise from denial, evasion, distortion, outright fabrication, and concealment by offenders to appear non-accountable for their exertions  [34] . AER systems can help law enforcement agencies to detect deception or malingering by identifying reliable emotional clues. In this way, AER systems provide additional aid and insights to law-enforcement agencies while pursuing criminal investigations  [35] . AER systems can also help detect and differentiate between acted and genuine victims  [34] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Advertising And Retail",
      "text": "In marketing, one of the best ways to sell products is to engage the customers emotionally. Companies employ vast resources for affective marketing by maximising user engagement with AI. They attempt to understand and appeal to the customers' interests, and emotions  [36] . In order to gauge a shopper's emotion, AER systems use sensing devices installed in high-traffic locations, including entrances, aisles, checkouts, etc. AER systems detect the emotional responses of individual shoppers, which help retailers in making decisions on crucial factors, including product pricing, packaging, branding, or shelf placement. In this way, AER systems help retailers understand how consumers communicate both verbally and non-verbally, which may help fuel customers' buying decisions.\n\nEmotions highly impact individuals' responses to receiving marketing messages. Therefore, sending an emotionally tailored message to the target audience increases the customers' attention to the advertisement. This helps companies to increase the product's appeal and achieve a higher level of brand recall  [37] . Indeed, advertisements with emotional content have more potential to be remembered than those conveying notification  [38] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional And Social Intelligence",
      "text": "Emotional and social intelligence involves understanding inside oneself, observing, and interpreting others for cognitive and emotional empathy, and responding constructively in a given situation. There is great interest in politics to capture and influence the mood of the overall population or community to understand patterns of emotional contagion  [39] . Emotional and social intelligence can help leaders and decision-makers pick up emotional cues from a population and handle highly challenging situations. Social networks are particularly utilised to understand population behaviours  [40] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Monitoring And Evaluation",
      "text": "AER systems are being utilised to screen candidates in interviews  [41]  and to evaluate and monitor employees' fatigue, stress, happiness, and job performance  [42] . It is widely accepted that emotional intelligence directly influences an employee's intellectual capital, organisational reactivity and retentively, production, employee appeal and ability to provide good customer service  [43] . AER systems can contribute to assess a candidate's suitability for a job and measure important traits like dependability and cognitive abilities. In particular, embedded AER systems enabled through IoT can provide fine-grained analysis of emotions and sentiments  [44] , which can be used in various ways for monitoring and evaluations. In the military and other defence-related departments, AER systems are partially used to track how sets of people or countries 'feel' about a government or other entities  [10] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gaming",
      "text": "Video games are related to the burgeoning area of entertainment applications. Millions of users across the globe are entertained by violent games  [45] , and most selling games contain violence and aggression. These video games are played by adolescents  [46] . For instance, in the United States, 81% of adolescents have access to digital games, and on average, a gamer spends 6 to 8 hours a week playing video games  [47] . AER systems are highly suited to be utilised for the design of affect-aware gaming platforms that can monitor players' emotional states and dynamically change the game's theme to more effectively engage the player  [48] . In these ways, affect-aware video games with an entertainment character can also be utilised to initiate pro-social behaviour by preventing anti-social actions along with various applications such as e-learning, marketing systems, and psychological training or therapy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Perils Of Aer",
      "text": "AER technology has a wide range of potentially intrusive applications, as discussed in the previous Section 2. It uses biometric data that may be used to reveal private information about individuals' physical or mental states, feelings, and thoughts. It can also be used to interfere with the formation of beliefs, ideas, and opinions. Modern AER systems often use deep learning (DL) models to obtain state-of-theart performance. However, such DL models are known to be inscrutable and are also not robust and are vulnerable to bias and poor performance in the face of distribution shifts and adversarial attacks  [49] . This raises concerns about using the validity of AER services since it is not uncommon to see that even well-intentioned technologies can have negative consequences and how technologies can end up harming rather than benefitting people  [50] ,  [51] . We discuss some of the prominent perils of AER next.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Risk Of Exploitative Manipulation",
      "text": "AER technology can be exploited and used to influence and control driving markets, politics, and violence. Already, there is a big concern in the community about major technology companies morphing into empires of behaviour modification  [52] . With AER having access to intimate human emotions, the risk of exploitative manipulation rises further as such information can be used to interfere with the formation of beliefs, ideas, opinions, and identity by influencing emotions or feelings  [53] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Lack Of Consent And Privacy Violations",
      "text": "AER systems utilise AI technology in their design with biometrics or other kinds of personal data (speech, facial image, among others). This allows for information about physical or mental health, thoughts, or feelings-which an individual may not want to choose to share-to be automatically inferred without the person's consent or knowledge. This has grave privacy concerns and can be used to establish and strengthen oppressive dystopian societies.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Lack Of Explainability/Accountability",
      "text": "AER systems usually lack explainability due to the complex internal mechanics of the AI model and the wide-scale adoption of BlackBox models based on \"deep learning\" technology. This inability to understand how AI performs in AER systems hinders its deployment in law, healthcare, and enterprises from handling sensitive consumer data. Understanding how AER data is handled and how AI has reached a particular decision is even more critical for data protection regulation. Explainability of AER services will allow companies to track AI decisions and monitor biased actions. This will also ensure that AER processes align with regulation and that decision-making is more systematic and accountable.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vulnerability To Adversarial Attacks",
      "text": "Modern AER-based tools typically rely on \"deep learning\" based models such as those built on deep neural networks (DNNs), which are composed of multiple hidden layers. DNNs are also quite fragile to very small specially-crafted adversarial perturbations to their inputs. This can cause false prediction in AER systems  [54] , which might have adverse consequences. For instance, an adversarially crafted example can cause an AER system to diagnose mental diseases inaccurately. This is one of the critical concerns of integrating AI-based services like AER in real-life.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vulnerability To Bias",
      "text": "There is scepticism in the community regarding the efficacy of AER and fears that using AER may accentuate and institutionalise bias  [55] . Since getting accurately labelled data is very expensive and time-consuming, any embedded bias in large annotated emotional training data is likely to be built into any systems developed using such data. Most of the AER systems use laboratory-designed datasets based on actors simulating emotional states in front of a camera. Furthermore, the labels used by ML models typically represent perceived emotion rather than felt emotion since the majority of the existing AER datasets are labelled by human annotators based on their perception  [53] . For instance, in facial emotion recognition, the labels for a photograph are provided by annotators, not by the individual in the photographs  [53] . This might not represent genuine inner emotions and may contain hidden biases that may become apparent only after deployment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Reductionist Emotional Models",
      "text": "AER algorithms base their working on basic emotion theories  [56]  that have been widely critiqued  [57] . For instance, the widely applied theory posited by Paul Ekman regarding six universal emotions (happiness, sadness, fear, anger, surprise, and disgust) that can be recognised across cultures from facial expressions has been criticised by experts as being too reductionist  [58] . An automatic link between facial movements and emotions is assumed-e. g., a smile means someone is happy. However, this might not always be true. For instance, in the US and many other parts of the world  [59] , it is common to smile at strangers, which might not represent inner feelings or states. It follows that more contextual details are required to understand the emotion, potentially requiring more data and invasive practices.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ai'S White Guy Problem Or Neo-Colonialism",
      "text": "Some findings indicate that AI technology suffers from problems such as sexism, racism, and other forms of discrimination  [60] . A major aspect related to this arises from homogeneous or unrepresentative data. Another reason could be focusing on the majority class since optimising for the majority class will usually improve overall accuracy. Unfortunately, this translates into discrimination against the minority classes as AI models typically do not automatically provide fairness unless constraints are placed for ensuring fairness (in which case, the overall accuracy will usually reduce as fairness and accuracy are different objectives, and it is not uncommon for them to have tradeoffs)  [61] . If we do not work to make AI more inclusive, we risk creating machine intelligence that \"mirrors a narrow and privileged vision of society, with its old, familiar biases and stereotypes\" (Kate Crawford, New York Times, https://tinyurl.com/2h6fu8dv)). Experts are now calling out for using decolonial theory as a tool for sociotechnical foresight in AI to ensure that the hegemony resulting from the domination of the AI industry by a limited number of demographic groups and nations does not have harmful effects globally  [62] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ethical Concerns With Aer",
      "text": "Giving emotions to a computer is another term for AER technology  [63] . It is exciting and a pipe dream to have a human-like or superior emotion detection system. In the last decade, techniques based on advanced techniques in ML and deep learning have outperformed almost all classical methods in recognising and understanding human emotions from facial, speech, and text inputs. These advanced learning techniques have produced effective and efficient results in AER and automated the whole process. AER systems are used in the commercial market for understanding user engagement, sentiment analysis, attention tracking, behaviour understanding, etc. However, these AER systems are also prone to shortcomings and biases in the training and testing data. The literature on the shortcomings of traditional and deep ML techniques suggests that data and algorithmic biases can impact the performance of these learning techniques  [58] . AER systems are developed using data acquired from humans, and human biases are likely to be translated into the learning process, impairing AER system judgements  [64] . There is a need to enforce responsible AI practices  [65] , and ethical guidelines for the design, development, and integration of AER systems in the wild  [66] .\n\nThe use of AER for emotional surveillance raises many ethical concerns, which motivates the need to identify basic ethical principles and guidelines that address ethical issues arising from the use of AER technology on human subjects to ensure that human subjects are not exploited or manipulated. In this regard, we can look at a traditional consensus on basic principles such as those expressed in the Belmont Report produced in 1979 by the US National Commission for the Protection of Human Subjects of Biomedical and Behavioural Research (https://tinyurl.com/5pr5rpe9). The Belmont Report identified three main principles-(1) respect for persons; (2) beneficence; and (3) justice-in their study focused on documenting the basic ethical principles and guidelines that should direct the conduct of biomedical and behavioural research involving human subjects. In light of the described perils of uncritical use of technology and the various ethical and moral dilemmas posed by AI  [67] , a lot of attention has focused on incorporating ethics in the field of AI leading to a proliferation of AI ethics principles and code of ethics. Interestingly, Jobin et al.  [68]  have highlighted 84 such codes of ethics related to AI in 2019 and found that four high-level ethical principles-beneficence, non-maleficence, autonomy, and justice-capture the essence of most AI declarations with Floridi and Cowls  [69]  also adding explicability as a high-level principle demanding that AI models should not work as inscrutable blackboxes. We summarise the AER-related ethical concerns in Figure  2  and discuss these concerns in detail next.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ethical Concerns Related To Justice",
      "text": "AI is being used in every facet of daily life, including criminal justice, social media, social justice, health care, smart cities, and urban computing. Although it has been well stated in the literature that AI-based systems are incapable of understanding the concepts of justice and social standards  [70] . Buolamwini et al.  [71]  emphasise that the AI-based facial detection system discriminates against gender and people of colour. They also demonstrated that commercial AI-based facial detection systems need a firmer grasp of ethics and auditing  [72] . Cathy O'Neil et al.  [51]  exposed the flaws in employing big data and AI-based algorithms to make choices with real-life consequences, and these consequences are leading to a societal split and shattered democracy. The ethics of applying AI in law and its obstacles are discussed in  [73] ,  [74] ,  [75] ,  [76] . In contrast, the ethical issues of employing AER systems for learning expressions and privacy concerns are detailed in  [77] . Wright  [78]  explains the opacity of algorithms employed in AER systems, the inadequacy of AI in comprehending human emotions, and how these failings lead to an unjust society  [79] . Carrillo  [80]  discusses the ethical AI debate from the standpoint of law and how AI shortcomings impede the general application of the AI-based judicial system. Finally, Khan et al.  [81]  present a thorough discussion of AI-enabled face recognition systems and their ethical implications in the criminal justice system.\n\nIn the last few years, AI-based predictive policing tools are becoming a part of global criminal justice systems. These systems are largely based on facial recognition technology with added emotion recognition and DNA matching. These tools have many ethical issues  [51] ,  [71] . Millerai  [82]  provides a comprehensive discussion on the ethical issues of predictive policing and facial recognition systems in criminal justice systems. They argued that these systems violate privacy rights, autonomy rights, and basic human morality. They also discuss the misuse of AI-based predictive policing and facial/emotion recognition tools in liberal democracies and the dangers of similar technology in authoritarian states. In order to use With AI-based systems making critical judgements about individuals (hiring process, advertising process, etc.), it is vital to consider and address ethical concerns. Automated physiognomy refers to the use of AI models to identify a person's gender, emotional state, level of intellect, and other characteristics from only one photograph of them. Engelmann et al.  [83]  debated the fairness and ethical concerns of automated physiognomy with a comprehensive experiment in which thousands of non-AI individuals were invited to respond on what AI should ethically infer from faces. The questions also include the number of characteristics inferred from faces by well-known AI models (including AER models), such as gender, emotional expression, likeability, assertiveness, intellect, colour, trustworthiness, and use of spectacles. Because all these characteristics are subjective, participants were asked to provide a Likert scale score and a written explanation of why a particular score was awarded for two specific use cases: advertising and hiring  [83] . The overall findings show that individuals, independent of context, substantially disagree with the automated physiognomy regarding assertiveness, likeability, trustworthiness, and intellect. Participants were also observed to be more dissatisfied (ethically) with the AI inferences about race, gender, emotional expression, and wearing spectacles in the hiring use case  [83] . AER systems suffer from the same issue, and the results reveal that a lack of auditing will result in an unfair automated judgement, which will have far-reaching effects on the social justice system.\n\nPodoletz  [84]  investigated the use of emotional AI (a blend of affective computing  [63]  and AI that gives probabilistic predictions of a person's or community's emotional state based on data points about the individual or community) in criminology, police, and surveillance. Given the ethical concerns, algorithmic biases, and annotation issues, Podoletz urge that emotional AI not be implemented in public spaces since these technologies will expand policing authority, raise privacy concerns, and operate as an oppressive instrument in authoritarian states. Podoletz goes on to claim that deploying emotional AI tools like AER would result in a highly regulated and controlled society, causing a severe schism in the social justice system. Lastly,  [84]  discusses the repercussions of using emotional AI tools in crime predictions and preemptive deception detection. Minsky  [85]  in his famous book \"The emotion machine: commonsense thinking, artificial intelligence, and the future of the human mind\" talked about emotional AI and its relation to basic cognition and neuroscience. He also talks about the ethical challenges in AI systems designed for emotion recognition. Emotional AI (affective computing paired with AI) technologies are used for reading, interpreting, replicating, and influencing human behaviour and sentiments, according to Yonck  [86]  in his book \"Heart of the machine: our future in a world of artificial intelligence.\" The author also discusses the moral dilemmas raised by the commercial application of these technologies. He further contends that the code of ethics designed for emotional AI tools like AER systems would be subverted in markets in favour of monetary and political gains, thus undermining the sociopolitical justice of society  [86] .  Van [87]  elaborated upon the ethical issues in AI-based facial recognition technologies (face, gender, class, race classification, AER systems, and others). The report demonstrates how one could use face recognition technology as an instrument of oppression, with a huge surveillance engine created to monitor and classify minorities and, by extension, a whole country.\n\nThe AER sector is predicted to be worth $26 billion by 2026  [88] . Crawford et al.  [88]  recommend that AER systems be regulated as soon as possible. She claims that several technology businesses used the pandemic as a justification to introduce emotion detection systems to assess the emotional state of employees and even children. She presented the example of an AER system called 4 Little Things 1 , which is used to infer children's emotions while carrying out their classwork with no supervision or regulation. She also states that with AER systems now being widely employed in many socioeconomic areas (hiring, healthcare, education, advertising, among others), it is important that this industry be regulated to avoid injustice and the fostering of an unjust society. A report on the ethical issues related to biometric applications (including AER) in public settings was published by the Citizens' Biometrics Council  [89] . The suggestions are based on conversations in public concerning the ethics of using AER and other biometric technology. The report urges the establishment of a comprehensive regulatory framework for biometric systems, a credible oversight agency, and minimum standards for designing and deploying face and AER systems  [89] .\n\nAs previously described, it has been observed in the literature that AI models do not automatically provide fairness or justice unless it is explicitly asked for  [90] . As Stuart Russell describes in his book, a problem underlying the model of conventional optimisation-based AI is that you only get what you explicitly ask for with the unstated assumption being that you implicitly agree that you do not care at all about everything you do not specify  [91] . The 1. https://www.4littletrees.com/ author calls this the King Midas problem of AI referring to the Greek mythological story in which King Midas gets all that he specifies, but the situation still ends unacceptably since he did not specify exactly what he did not want (and unacceptable values were incorrectly inferred)  [91] . Various studies have shown that AER technology is prone to bias and can suffer from a lack of fairness, accountability, and transparency. This has real consequences when such technology is used for critical decisions, such as in judicial systems for making judgements about sentencing  [51] ,  [58] . Therefore, AER technology requires a continued and concerted effort to address such issues, because misreading an individual's emotions can cause severe consequences in specific scenarios.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ethical Concerns Related To Beneficence/ Nonmaleficence",
      "text": "Ethical principles of beneficence (\"do only good\") and nonmaleficence (\"do no harm\") are closely related. Beneficence encourages the creation of AI services to promote the wellbeing of humanity and the planet, while non-maleficence concerns the negative consequences of AI  [92] . These concerns are also important in the designing and deployment of AER technology. Therefore, AER services should avoid causing both foreseeable and unintentional harm. This requires a complete understanding of AER technology and its scientific limitations to manage the attendant risks. The services should be designed to benefit human beings and increase their well-being to make AER prosocial.\n\nDesigning a Prosocial AER system requires mitigation of ethical concerns highlighted in the literature  [78] . With the unprecedented penetration of social media applications and the use of surveillance technologies, the opt-in and optout model of data sharing is long gone. Now, most of the applications gather data irrespective of permissions, and the written conditions that one agrees to upon usage are written in a language that is a challenge for the regular user. It is problematic, and many incidents of unethical use of the data are being reported in the literature. Unfortunately, the idea of beneficence / non-maleficence is not considered as vital as it should have been in designing AER systems.\n\nBeneficence / non-maleficence principles are based on moral conscientiousness, social good, and trustworthiness of people, companies, and algorithms. Raquib et al.  [93]  propose a virtue-based ethical design of AI systems, although the debate is philosophical and many areas of the subject suffer from a lack of generality. The topic of virtue-based ethical systems and the ethical quandaries raised are also pertinent to AER systems. Because AER systems are meant to learn from user behaviour and how that behaviour may be watched, hugged, and altered, the essential nature of the data and the influence of the AER system on society necessitates an AER design that is founded on beneficence / nonmaleficence. Examination supervision technologies have saturated the market under the guise of COVID-19. These tools are often AI-based, with face and emotion recognition algorithms used to monitor exam participants. Though these methods are intended to assure that the examination is conducted correctly, they lack core ethical standards such as privacy, transparency, fairness, and beneficence. Coghlan et al.  [94]  examined and reported ethical challenges with AI-based examination supervision tools, arguing that the issues will not be resolved until ethical standards are not included in the basic design principles of AI-based automated systems like facial recognition and AER. Similarly, the reality of social robots is just around the horizon, and numerous AER-based robots are presently being employed in a variety of social contexts, and the number of these robots is rapidly increasing. The ethical challenges raised by social robots originate from the fundamental debate about the uncertainty and responsibility of AI systems. Bosch et al.  [95]  provide a brief description of the ethical risks involved in social robotics, including how the concept of doing only good and not harm is required for social robots, as well as various technological and social challenges associated with developing such ethics in robots.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Concerns Related To Privacy",
      "text": "AER services mostly use DL algorithms that are trained on masses of data to learn and perform decision-making. Ethical concerns related to privacy require protecting individuals' data and preserving their privacy. Over the last two decades, the rise of surveillance capitalism went largely unchallenged. Tech companies like Google and Facebook provide free online services and use personal data for mass surveillance over the internet. Such companies collect and scrutinise users' online behaviours including searches, purchases, likes, dislikes, and more, to predict, modify, and control users' behaviours. Lanier has coined the term BUM-MER 2 , or \"Behaviours of Users Modified, and Made into an Empire for Rent\", for the economic model followed by big tech corporations in the world of surveillance capitalism.\n\nThe design of AER systems depends heavily on face recognition technologies, and it is advised in the literature that emotion recognition systems should be regularly updated and audited  [88] . Bowyer  [96]  discuss facial recognition systems' security vs the privacy dilemma. The right to privacy is a fundamental right guaranteed by practically every country's constitution. Many countries use facial recognition systems, and by extension AER systems, for mass surveillance without the agreement or scrutiny of regulatory organisations, which is a serious concern in the domain of technology's social effect. Bowyer  [96]  argues that utilising these recognition technologies violates the constitutionally guaranteed right to privacy. AER systems not only employ facial recognition technologies but also infer the emotional state and other aspects of the face without consent, which is an abuse of power and a blatant violation of the fundamental right to privacy. The effectiveness of a security surveillance system is determined by the performance of the facial recognition system and a combination of the algorithms to measure the underlying emotional states and motives from just an image of the face and body. These algorithms and facial recognition systems have shown to be biased and unreliable in the literature  [97] ; false positives and negatives have life-threatening repercussions, and privacy infringement concerns are unprecedented  [98] .\n\n2. https://www.theguardian.com/technology/2018/may/27/ jaron-lanier-six-reasons-why-social-media-is-a-bummer\n\nThe discussion of privacy and the right to privacy has become prevalent with the advent and adoption of new technological applications such as internet-of-things (IoT), robotics, pervasive technologies, biometric technologies, augmented and virtual reality, and digital platforms  [99] . AER systems are used in homes, health care facilities, childcare centres, social media apps, and other digital platforms for monitoring, data collecting, emotion inference, and feedback translation. Because the data collected by AER systems is the property of the device manufacturers, these spaces are becoming more open, and prone to privacy violation  [99] . Though there are a few traditional privacy limitations in place, it is challenging to ensure privacy when AI-driven inference is involved without suitable monitoring and regulatory mechanisms. Camera-based assistive aids are quickly becoming popular among the sight impaired. AI-based vision technologies and, in certain situations, AER systems are actively used in these assistive technologies. Akter et al.  [100]  conducted a couple of surveys on the privacy and ethical considerations associated with these assistive devices. According to their surveys, the majority of respondents were concerned about the fairness, privacy, and other ethical concerns associated with these assistive technologies.\n\nIn the last few years, ethical concerns related to privacy have become a promising area of research thanks to the active integration of AI-enabled applications such as camera-based surveillance systems, AER systems, and others. Ribaric et al.  [101]  surveyed de-identification techniques for ensuring privacy in vision-based applications such as AER systems and healthcare applications where privacy is critical and provide an insightful discussion on how deidentification can help resolve ethical challenges. Das et al.  [102]  provide a procedure for identifying and mitigating privacy-related concerns in camera-based IoT devices in digital homes and other places through privacy-aware notifications and infrastructure. Their work also outlines the technique for privacy-aware video streaming and policyrelated guidelines for ensuring privacy and mitigation of the risks involved in vision data (surveillance data, AER systems, etc.) being misused by adversaries. Hunkenschroer et al.  [103]  conducted a systematic review of the literature on ethical problems in AI-based hiring procedures. Though the emphasised problems concern the employment process, some of the issues (human and algorithmic bias, privacy and data leakage hazards, and fairness) are also prevalent in AER systems. Boutros et al.  [104]  used class conditional generative models to generate a privacy-friendly synthetic faces dataset and trained facial recognition models and tested its performance in three different experimental settings: multi-class classification, label-free knowledge transfer, and combined learning settings. Their results indicate that the synthetic dataset showed a promising performance, and the authors recommend that privacy-friendly synthetic data is good enough to train facial recognition systems.\n\nPrivacy is not just about hiding information; it is about the agency: the agency to opt-in or opt-out. Unfortunately, the concept of agency is frequently overlooked in the design thinking component of AER systems, resulting in biased and untrustworthy AER systems. Because AER systems predict/infer a person's or a social group's emotions, the agency to convey the emotional data (through any input methods such as voice, video, picture, and language) should be with the individual or the social group. Woensel et al.  [105]  raised the problem of agency in AER systems and linked it to data gathering from people and social groupings without proper consent and agency. The critical concern raised in the paper was the potential of using AER systems for targeted and mass surveillance, which in any rational society is considered a violation of social standards, privacy, and ethics. The paper recommended imposing strict controls on data collection for AER systems or for prohibiting them until the necessary ethical standards are satisfied. Cavoukian et al.  [106]  outlined seven rules for introducing privacy by design in systems. We show these rules in Figure  3 . These rules can help improve privacy while providing a reasonable design path toward ethics-centred privacy for AER systems  [10] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ethical Concerns Related To Autonomy",
      "text": "When we adopt AER services in daily life, we willingly cede some of our decision-making power to AI. This may undermine the flourishing of human autonomy with artificial autonomy. Therefore, it is crucial to balance the decisionmaking power delegated to AER agents and that we retain for ourselves. AER systems must not impair the freedom of their users so they can live according to their standards and norms.\n\nFor AI to yield any benefits for the human race, it must be focused on the autonomy of humans rather than the popular belief of giving more autonomy to machines  [107] . This argument stems from the classical discussions on whether AI techniques are tools to help improve life by making tasks easier or AI understanding the problems by itself and fixing them without categorically consenting the humans. Here, it is essential to understand what autonomy means. Autonomy is described as the sense of willingness and a cognitive process of committing to a course of action. Calvo et al.  [108]  take a closer look at human autonomy and technology under the pretext of ethics. They highlight that, in 2019, most of the literature around autonomy was focused on machine autonomy, whereas, now, this trend is shifted towards human autonomy-based technology design after critical technical and ethical issues with machine autonomy and design of machine autonomy were highlighted. AER systems are designed to translate the state-of-the-art in human psychology using AI and psycho graphs techniques. Unfortunately, human autonomy and ethical questions such as willingness to interact and adopt are not appropriately addressed. Abbass  [109] , and  [110]  argue that since AI techniques are now being integrated into various aspects of society, it is paramount to prefer humans in the loop or humans on the loop-based algorithms for decision making. It will ensure that human autonomy and ethical practices are followed in making critical decisions.\n\nEmotion recognition systems are trained on the data harvested from social media and digital platforms to understand and infer emotions. Andalibi et al.  [111]  surveyed 13 social media users about the fact that the data from social media applications are used for training emotion recognition systems without getting users' consent. Even if consent is taken, it is collected through a 'terms and condition' form, which is mainly forced and in a legal language that is not user-friendly. Their results indicate that most of the participants viewed it as scary, invasive, unethical, and a loss of power and human autonomy. The paper further recommends that ethical usage be ensured in these critical applications at an individual and societal level. Gender bias is another ethical quandary in the AER system, and using these tools in the field necessitates a gender bias evaluation in emotion recognition systems. Domnich et al.  [65]  assessed the performance of several AI approaches and showed which kind of networks are employed for certain types of emotions. The results of the experiments revealed that specific AI designs are discriminatory, with significant differences in performance between males and females in terms of emotion recognition. Another vertical of this discussion on the autonomy-related AER system is the categorisation of complex human emotions into a set of classes and then the offering solutions/interventions based on these categories. Unfortunately, this classification concept has a fundamental weakness since human emotions (both as individuals and as social groupings) are complicated, private, unique, and occasionally indefinable, and reducing these aspects to a data point and using it to tweak the behaviour raises various ethical issues.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Path To A Prosocial Future For Aer",
      "text": "As motivated in the previous sections, AER systems are promising for contributing to social good in a wide variety of applications such as healthcare, education, safety, and law enforcement, but at the same time, it is beset with several risks and perils, which must be addressed. Qadir et al.  [112]  have stressed the need for a more humane humancentred AI that is accountable and have outlined promising directions for achieving accountable human-centred AI. In this section, we highlight some approaches we can adopt to pave the way for a prosocial future for AER systems.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Better Awareness And Education",
      "text": "The development of AER software is bringing enormous changes to society through data analysis. AER Technology has the positive effect of revolutionising many areas by solving various existing problems. On the other hand, AER technologies are two-sided, which can also cause problems. Therefore, it is crucial to raise awareness among the broader population about AER's role in our lives and the use and purchase of AER services. This can help to achieve largescale adoption of AER services among the general population and minimise the risk of being negatively profiled by AER technology.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Auditable Aer",
      "text": "The auditability of AI describes the possibility of evaluating models, algorithms, and datasets in terms of operation, results, and effects. It has two parts, including technical and ethical. The technical part assesses the reliability and accuracy of results; however, the ethical part apprehends its individual and collective impacts and checks the risks of breaching certain principles, including equality or privacy. AER systems learn from the data they are exposed to and make decisions using ML algorithms. They can develop, or even amplify, biases and discrimination. Therefore, it is essential to audit and test AER algorithms throughout their life cycle to pinpoint the origin of errors and detect risks to avoid their impact on the lives of individuals and society. It will help to systematically probe AER systems, uncover biases, and avoid undesirable behaviour.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Explainable And Interpretable Aer",
      "text": "A key reason behind the fragility of AER services is the black-box nature of ML models used for the decisionmaking process. These ML models are neither explainable nor their outcomes interpretable. To realise the real potential of AER systems, it is highly desirable to make them explainable in a human-understandable way. In recent years, significant research has been devoted to developing novel methods for explaining and interpreting ML models. In the literature, different explainable approaches can be broadly classified as white-box and black-box explanation methods. The white-box explanation method describes the model by identifying the most critical features that contributed to a specific prediction  [113] . Another method for white-box explanation is to compute the prediction's gradient concerning individual input samples to discover the prediction's relevant features. White-box explanation mainly provides the model-specific explanation, while the black-box technique provides local explanations of a model for a prediction  [114] . Explaining ML models and their decision is critical, as it is the key enabling factor for building trust and ensuring fairness in decision making. This is also important for AER applications, where decisions directly impact human life.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Privacy Preserving Aer",
      "text": "In AER services, the privacy of the users' data is a growing concern, mainly when AER is performed on cloud platforms. AER companies gather a large amount of user data to perform emotion analysis. The data gathered by these companies is kept forever and the user does mostly not have any or little control over it. The images, video, and voice samples, but also textual bits also contain sensitive background information such as faces, gender, language, etc. The leakage of this data can be used maliciously without the user's consent by an eavesdropping adversary and may cause threatening consequences to people's lives. Therefore, it is crucial to utilise privacy-preserving AI models in AER systems to protect users' privacy. The methods and techniques for developing AI systems that ensure privacy falls under the umbrella of privacy-preserving AI.\n\nPrivacy-preserving AI has four major pillars:\n\n1) Training data privacy, which can be ensured by Differentially Private Stochastic Gradient Descent (DPSGD) and PATE  [115]  and similar solutions. 2) Input privacy, which can be ensured via Homomorphic Encryption, Secure Multiparty Computation (MPC), and Federated Learning. 3) Output privacy, which can be ensured by using Homomorphic Encryption, Secure Multiparty Computation (MPC), and Federated Learning 4) Model privacy, which can be ensured by applying differential privacy on the output of an AI model.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ethical Framework For Aer",
      "text": "In recent times, there has been much work on developing ethical principles and frameworks for AI  [68] . A report on the ethics of AI and the applications of automated emotional intelligence and its risk is presented in  [53] . There is a need for similar efforts focused on developing an ethical framework for AI-based AER, which can enable various benefits as presented in Table  4 . We propose that in order to operate an ethical, privacyprotective AER system, an entity should embrace the following principles:\n\n‚Ä¢ Transparency: An entity must describe its policies related to the duration it retains data, how the data is used, how the government might access the data, and the necessary technical specifications to verify accountability.\n\n‚Ä¢ Full disclosure: An entity must receive informed, written, and specific consent from individuals before enrolling her, him, or them in an AER database. Enrolment is the storage of personal data such as voice and face prints to perform emotion recognition or identification.\n\n‚Ä¢ Personal consent: An entity must receive informed, written consent from an individual before using the individual's data in a manner that was not mentioned in the existing consent. When individuals consent to use an AER system for one purpose, an entity must seek consent from that individual for using AER technology for another purpose. However, users should be free to withdraw their consent at any time. An entity must not use the AER system to determine an individual's colour, race, religion, gender, age, nationality, or disability.\n\n‚Ä¢ Ethical data sharing: Individuals' data should not be shared or sold without the informed, written consent of the individual whose information is being shared or sold.\n\n‚Ä¢ Data ownership: An individual must have the right to access, correct, and remove his or her data print.\n\n‚Ä¢ Security and privacy: AER data must be kept secure and private by the entity maintaining the data. Simply defining principles is not sufficient. These principles should be embedded into practice and operationalised. An entity must maintain a system that measures compliance with these principles, including an audit trail memorialising the collection, use, and sharing of information in an AER system. The audit trail must include a record of the date, location, consent verification, and provenance of emotional data. It must also allow evaluation of the AER algorithm for accuracy. This data may also be incorporated in a watermark to ease the ability to audit.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusions And Recommendations",
      "text": "This paper discussed the promises and perils of artificial intelligence-based automatic emotion recognition systems. We believe AER technology has a wide range of reallife applications; however, we aim to caution AER users and service providers about ethical concerns. AER systems have biases that can lead to incorrect results, just like any other artificial intelligence (AI) based intelligent systems. We cannot fully rely on AER systems in making decisions; however, help from them can be taken to improve the final decision. We also must carefully consider AER systems' fairness, transparency, accountability, and ethics during their development and applications. For this, we proposed guidelines for designing future prosocial AER solutions. We are summarising below the recommendations for designing such responsible AER systems:\n\n‚Ä¢ Full examination across various dimensions is required for the data used by AER systems. Expressions of emotion are variable across different languages. This variability must be taken into account while designing datasets, systems, and deployment of AER systems.\n\n‚Ä¢ One needs to examine the choice of AI techniques across interpretability, concerns, privacy, energy efficiency, and data needs. AI tends to perform well for individuals who are well-represented in the data but fails for others. Therefore, it is crucial to explore inclusive methods to avoid spurious correlations that perpetuate sexism, racism, and stereotypes.\n\n‚Ä¢ AER systems are often trained on static data; however, emotions, perceptions, and behaviour change over time. It is important to incorporate adaptability in AER services for predictions on current data. This may include drifting target learning approaches.\n\n‚Ä¢ Privacy is not only secrecy but also a personal choice. Applying AER to a mass gathering without personal consent is an invasion of privacy, harmful to the individual, and dangerous to society. Therefore, it is important to follow suited privacy principles such as the seven by Cavoukian while designing AER systems.\n\n‚Ä¢ It is crucial to realise ethical concerns related to privacy, manipulation, and bias while designing AER systems. Therefore, anonymisation of information at various levels is required.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "‚Ä¢",
      "text": "The use of AER for fully automated decision-making is unsuited. AER systems may be utilised for assistance in decision-making. AER services should be transparent to all stakeholders. These recommendations are primarily for the researchers, engineers, educators, and developers who build, make use of or teach about AER technologies. These guidelines will help engender trust with customers and also improve the profitable drive growth of AER technology. With all these guidelines in mind, we shall be ready to fully benefit and enjoy the many good Automatic Emotion Recognition holds as promise.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Email: siddique.latif@usq.edu.au",
      "page": 1
    },
    {
      "caption": "Figure 1: An overview of AER systems, which can process emotional infor-",
      "page": 1
    },
    {
      "caption": "Figure 2: Summary of ethical concerns associated with AER.",
      "page": 6
    },
    {
      "caption": "Figure 3: These rules can help improve privacy while providing",
      "page": 9
    },
    {
      "caption": "Figure 3: Seven rules for introducing privacy by design in systems [106].",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 2: A broad description of frequently",
      "data": [
        {
          "Paper/Author (Year)": "",
          "Focus": "Focus",
          "Prescriptions": ""
        },
        {
          "Paper/Author (Year)": "Kolakowska et al. [6] (2014)",
          "Focus": "AER",
          "Prescriptions": "(cid:55)"
        },
        {
          "Paper/Author (Year)": "Feng et al. [7] (2020)",
          "Focus": "AER",
          "Prescriptions": "(cid:55)"
        },
        {
          "Paper/Author (Year)": "Batliner et al. [8] (2020)",
          "Focus": "Computational paralinguistics",
          "Prescriptions": "(cid:55)"
        },
        {
          "Paper/Author (Year)": "Liu et al. [9] (2021)",
          "Focus": "AER (EEG only)",
          "Prescriptions": "(cid:55)"
        },
        {
          "Paper/Author (Year)": "Mohammad et al. [10] (2022)",
          "Focus": "AER",
          "Prescriptions": ""
        },
        {
          "Paper/Author (Year)": "Our paper (2022)",
          "Focus": "AER",
          "Prescriptions": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: A broad description of frequently",
      "data": [
        {
          "Human\nexpression": "Vocal\nexpressions",
          "Data": "Audio",
          "Possible applications": "Call centres, meetings, voice\nassistants, social robots, educations,\nhuman resource, healthcare, etc."
        },
        {
          "Human\nexpression": "Facial\nexpressions",
          "Data": "Visual",
          "Possible applications": "Autonomous vehicles, industrial\nand social robots, surveillance,\nsocial media, gaming,\neducation, healthcare."
        },
        {
          "Human\nexpression": "Body movements\nand posture",
          "Data": "Visual",
          "Possible applications": "Surveillance, education, healthcare."
        },
        {
          "Human\nexpression": "Physiological\nsignals",
          "Data": "EEG and ECG\nrecords, heart rate",
          "Possible applications": "Wearable devices and\n.\nmedical equipment"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Domains": "Healthcare",
          "Promise": "Monitoring people and regulating emotion.\nImproves patient-physician relationships.\nAnalyses and understands emotions in\nnatural disasters and crises."
        },
        {
          "Domains": "Education",
          "Promise": "Improves student-teacher interaction.\nQuantiÔ¨Åes student moods and engagement\nin the classroom.\nPromotes effective learning and increase\nstudents‚Äô interest."
        },
        {
          "Domains": "Safety",
          "Promise": "Improves workplace safety.\nEnables help for emotionally suffered\nco-workers.\nMonitors the drivers‚Äô fatigue, stress, etc."
        },
        {
          "Domains": "Law enforcement\nand forensics",
          "Promise": "Helps identify threats of violence\nand terrorism.\nProvides additional aid in criminal\ninvestigation."
        },
        {
          "Domains": "Advertising and Retail",
          "Promise": "Helps maximise customers‚Äô engagement.\nHelps retailers to make decisions on\nproduct pricing, packaging, branding, etc.\nHelps improve advertisement strategies."
        },
        {
          "Domains": "Emotional and Social\nIntelligence",
          "Promise": "Helps inÔ¨Çuence the mood of the\noverall population.\nHelps leaders and decision-makers to\nhandle highly challenging situations."
        },
        {
          "Domains": "Monitoring and Evaluation",
          "Promise": "Enables monitoring of employees\nperformance.\nEnables monitoring of major psychiatric\nproblems in both military and civilian."
        },
        {
          "Domains": "Gaming",
          "Promise": "Monitors players‚Äô emotional states\nand dynamics during gameplay.\nHelps design affect-aware video games."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ethical principles": "Transparency",
          "Advantages": "Reduces risk\nIncreases fairness\nSatisÔ¨Åes regulatory and compliance laws"
        },
        {
          "Ethical principles": "Full disclosure",
          "Advantages": "Improves explainability\nIncreases understanding"
        },
        {
          "Ethical principles": "Personal consent",
          "Advantages": "Improves reliability and safety\nIncreases regulation\nProtects vulnerable participants"
        },
        {
          "Ethical principles": "Ethical data sharing",
          "Advantages": "Creates an ethical imperative\nIncreases trust"
        },
        {
          "Ethical principles": "Data ownership",
          "Advantages": "Establishes accountability\nAssigns responsibility"
        },
        {
          "Ethical principles": "Security and privacy",
          "Advantages": "Consent-based data collection\nRegulated surveillance\nImproved privacy"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Perspectives on emotional development ii: A functionalist approach to emotions",
      "authors": [
        "K Barrett",
        "J Campos"
      ],
      "year": "1987",
      "venue": "Perspectives on emotional development ii: A functionalist approach to emotions"
    },
    {
      "citation_id": "2",
      "title": "Automatic emotion recognition based on body movement analysis: a survey",
      "authors": [
        "H Zacharatos",
        "C Gatzoulis",
        "Y Chrysanthou"
      ],
      "year": "2014",
      "venue": "IEEE computer graphics and applications"
    },
    {
      "citation_id": "3",
      "title": "Emotional: The New Thinking About Feelings. Penguin UK",
      "authors": [
        "L Mlodinow"
      ],
      "year": "2022",
      "venue": "Emotional: The New Thinking About Feelings. Penguin UK"
    },
    {
      "citation_id": "4",
      "title": "The Age of Artificial Emotional Intelligence",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE Computer Magazine"
    },
    {
      "citation_id": "5",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proceedings of the 20th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition and its applications",
      "authors": [
        "A Ko≈Çakowska",
        "A Landowska",
        "M Szwoch",
        "W Szwoch",
        "M Wrobel"
      ],
      "year": "2014",
      "venue": "Human-Computer Systems Interaction: Backgrounds and Applications 3"
    },
    {
      "citation_id": "7",
      "title": "A review of generalizable transfer learning in automatic emotion recognition",
      "authors": [
        "K Feng",
        "T Chaspari"
      ],
      "year": "2020",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "8",
      "title": "Ethics and good practice in computational paralinguistics",
      "authors": [
        "A Batliner",
        "S Hantke",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Real-time eeg-based emotion recognition and its applications",
      "authors": [
        "Y Liu",
        "O Sourina",
        "M Nguyen"
      ],
      "year": "2011",
      "venue": "Transactions on computational science XII"
    },
    {
      "citation_id": "10",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "S Mohammad"
      ],
      "year": "2022",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "H Gunes",
        "B Schuller",
        "M Pantic",
        "R Cowie"
      ],
      "year": "2011",
      "venue": "Face and Gesture"
    },
    {
      "citation_id": "12",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin"
      ],
      "year": "2015",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "14",
      "title": "Strong evidence for universals in facial expressions: a reply to russell's mistaken critique",
      "authors": [
        "P Ekman"
      ],
      "year": "1994",
      "venue": "Strong evidence for universals in facial expressions: a reply to russell's mistaken critique"
    },
    {
      "citation_id": "15",
      "title": "Are there basic emotions",
      "year": "1992",
      "venue": "Are there basic emotions"
    },
    {
      "citation_id": "16",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "S Latif",
        "J Qadir",
        "A Qayyum",
        "M Usama",
        "S Younis"
      ],
      "year": "2020",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "17",
      "title": "An effective training to increase accurate recognition of patient emotion cues",
      "authors": [
        "D Blanch-Hartigan"
      ],
      "year": "2012",
      "venue": "Patient education and counseling"
    },
    {
      "citation_id": "18",
      "title": "not by our feeling, but by other's seeing\": Sentiment analysis technique in cardiology-an exploratory review",
      "authors": [
        "A Brezulianu",
        "A Burlacu",
        "I Popa",
        "M Arif",
        "O Geman"
      ],
      "year": "2022",
      "venue": "Frontiers in Public Health"
    },
    {
      "citation_id": "19",
      "title": "Emotion-aware and intelligent internet of medical things towards emotion recognition during covid-19 pandemic",
      "authors": [
        "T Zhang",
        "M Liu",
        "T Yuan",
        "N Al-Nabhan"
      ],
      "year": "2020",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "20",
      "title": "Emocov: Machine learning for emotion detection, analysis and visualization using covid-19 tweets",
      "authors": [
        "M Kabir",
        "S Madria"
      ],
      "year": "2021",
      "venue": "Online Social Networks and Media"
    },
    {
      "citation_id": "21",
      "title": "Crosscultural polarity and emotion detection using sentiment analysis and deep learning on covid-19 related tweets",
      "authors": [
        "A Imran",
        "S Daudpota",
        "Z Kastrati",
        "R Batra"
      ],
      "year": "2020",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "22",
      "title": "Monitoring people's emotions and symptoms from arabic tweets during the covid-19 pandemic",
      "authors": [
        "A Al-Laith",
        "M Alenezi"
      ],
      "year": "2021",
      "venue": "Information"
    },
    {
      "citation_id": "23",
      "title": "Emotions recognition as innovative tool for improving students' performance and learning approaches",
      "authors": [
        "M Bouhlal",
        "K Aarika",
        "R Abdelouahid",
        "S Elfilali",
        "E Benlahmar"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "24",
      "title": "The effects of merging student emotion recognition with learning management systems on learners' motivation and academic achievements",
      "authors": [
        "M √ñzek"
      ],
      "year": "2018",
      "venue": "Computer applications in engineering education"
    },
    {
      "citation_id": "25",
      "title": "Using emotional and social factors to predict student success",
      "authors": [
        "M Pritchard",
        "G Wilson"
      ],
      "year": "2003",
      "venue": "Journal of college student development"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition development: Preliminary evidence for an effect of school pedagogical practices",
      "authors": [
        "S Denervaud",
        "C Mumenthaler",
        "E Gentaz",
        "D Sander"
      ],
      "year": "2020",
      "venue": "Learning and Instruction"
    },
    {
      "citation_id": "27",
      "title": "How emotions inform judgment and regulate thought",
      "authors": [
        "G Clore",
        "J Huntsinger"
      ],
      "year": "2007",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "28",
      "title": "The effect of corporate social responsibility on hotel employee safety behavior during covid-19: The moderation of belief restoration and negative emotions",
      "authors": [
        "J Zhang",
        "C Xie",
        "A Morrison"
      ],
      "year": "2021",
      "venue": "Journal of Hospitality and Tourism Management"
    },
    {
      "citation_id": "29",
      "title": "Driver emotion recognition for intelligent vehicles: a survey",
      "authors": [
        "S Zepf",
        "J Hernandez",
        "A Schmitt",
        "W Minker",
        "R Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "30",
      "title": "Driving: a road to unhealthy lifestyles and poor health outcomes",
      "authors": [
        "D Ding",
        "K Gebel",
        "P Phongsavan",
        "A Bauman",
        "D Merom"
      ],
      "year": "2014",
      "venue": "PloS one"
    },
    {
      "citation_id": "31",
      "title": "On driver behavior recognition for increased safety: a roadmap",
      "authors": [
        "L Davoli",
        "A Cilfone",
        "L Belli",
        "G Ferrari",
        "R Presta",
        "R Montanari",
        "M Mengoni",
        "L Giraldi",
        "E Amparore"
      ],
      "year": "2020",
      "venue": "Safety"
    },
    {
      "citation_id": "32",
      "title": "Effects of specific emotions on subjective judgment, driving performance, and perceived workload",
      "authors": [
        "M Jeon",
        "B Walker",
        "J.-B Yim"
      ],
      "year": "2014",
      "venue": "Transportation research part F: traffic psychology and behaviour"
    },
    {
      "citation_id": "33",
      "title": "Forensic voice identification",
      "authors": [
        "H Hollien"
      ],
      "year": "2002",
      "venue": "Forensic voice identification"
    },
    {
      "citation_id": "34",
      "title": "A forensic phonetic study of the vocal responses of individuals in distress",
      "authors": [
        "L Roberts"
      ],
      "year": "2012",
      "venue": "A forensic phonetic study of the vocal responses of individuals in distress"
    },
    {
      "citation_id": "35",
      "title": "Federal law enforcement use of facial recognition technology",
      "year": "2021",
      "venue": "Federal law enforcement use of facial recognition technology"
    },
    {
      "citation_id": "36",
      "title": "Shopmobia: An emotion-based shop rating system",
      "authors": [
        "N Alajmi",
        "E Kanjo",
        "N Mawass",
        "A Chamberlain"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "37",
      "title": "The emotional effectiveness of advertisement",
      "authors": [
        "F Otamendi",
        "D Sutil Mart√≠n"
      ],
      "year": "2020",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "38",
      "title": "The memory impact of commercials varying in emotional appeal and product involvement",
      "authors": [
        "T Page",
        "E Thorson",
        "M Heide"
      ],
      "year": "1990",
      "venue": "Emotion in advertising"
    },
    {
      "citation_id": "39",
      "title": "Practical intelligence, emotional intelligence, and social intelligence",
      "authors": [
        "F Lievens",
        "D Chan"
      ],
      "year": "2017",
      "venue": "Practical intelligence, emotional intelligence, and social intelligence"
    },
    {
      "citation_id": "40",
      "title": "Social network theory in the behavioural sciences: potential applications",
      "authors": [
        "J Krause",
        "D Croft",
        "R James"
      ],
      "year": "2007",
      "venue": "Behavioral Ecology and Sociobiology"
    },
    {
      "citation_id": "41",
      "title": "Predicting behavioral competencies automatically from facial expressions in real-time video-recorded interviews",
      "authors": [
        "Y.-S Su",
        "H.-Y Suen",
        "K.-E Hung"
      ],
      "year": "2021",
      "venue": "Journal of Real-Time Image Processing"
    },
    {
      "citation_id": "42",
      "title": "It pays to have an eye for emotions: Emotion recognition ability indirectly predicts annual income",
      "authors": [
        "T Momm",
        "G Blickle",
        "Y Liu",
        "A Wihler",
        "M Kholin",
        "J Menges"
      ],
      "year": "2015",
      "venue": "Journal of Organizational Behavior"
    },
    {
      "citation_id": "43",
      "title": "Analyzing and detecting employee's emotion for amelioration of organizations",
      "authors": [
        "R Subhashini",
        "P Niveditha"
      ],
      "year": "2015",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "44",
      "title": "Exploiting iot services by integrating emotion recognition in web of objects",
      "authors": [
        "M Jarwar",
        "I Chong"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Information Networking (ICOIN)"
    },
    {
      "citation_id": "45",
      "title": "A longitudinal study of link between exposure to violent video games and aggression in chinese adolescents: The mediating role of moral disengagement",
      "authors": [
        "Z Teng",
        "Q Nie",
        "C Guo",
        "Q Zhang",
        "Y Liu",
        "B Bushman"
      ],
      "year": "2019",
      "venue": "Developmental Psychology"
    },
    {
      "citation_id": "46",
      "title": "Pathological video game symptoms from adolescence to emerging adulthood: A 6-year longitudinal study of trajectories, predictors, and outcomes",
      "authors": [
        "S Coyne",
        "L Stockdale",
        "W Warburton",
        "D Gentile",
        "C Yang",
        "B Merrill"
      ],
      "year": "2020",
      "venue": "Developmental psychology"
    },
    {
      "citation_id": "47",
      "title": "Insensitive players? a relationship between violent video game exposure and recognition of negative emotions",
      "authors": [
        "E Miedzobrodzka",
        "J Buczny",
        "E Konijn",
        "L Krabbendam"
      ],
      "year": "2021",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "48",
      "title": "Emotion recognition for affect aware video games",
      "authors": [
        "M Szwoch",
        "W Szwoch"
      ],
      "year": "2015",
      "venue": "Image Processing & Communications Challenges 6"
    },
    {
      "citation_id": "49",
      "title": "A taxonomy and survey of attacks against machine learning",
      "authors": [
        "N Pitropakis",
        "E Panaousis",
        "T Giannetsos",
        "E Anastasiadis",
        "G Loukas"
      ],
      "year": "2019",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "50",
      "title": "Caveat emptor: the risks of using big data for human development",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usama",
        "J Qadir",
        "A Zwitter",
        "M Shahzad"
      ],
      "year": "2019",
      "venue": "IEEE Technology and Society Magazine"
    },
    {
      "citation_id": "51",
      "title": "Weapons of math destruction: How big data increases inequality and threatens democracy",
      "authors": [
        "C O'neil"
      ],
      "year": "2016",
      "venue": "Weapons of math destruction: How big data increases inequality and threatens democracy"
    },
    {
      "citation_id": "52",
      "title": "Human compatible: Artificial intelligence and the problem of control",
      "authors": [
        "S Russell"
      ],
      "year": "2019",
      "venue": "Human compatible: Artificial intelligence and the problem of control"
    },
    {
      "citation_id": "53",
      "title": "The ethics of AI and emotional intelligence: Data sources, applications, and questions for evaluating ethics risk",
      "authors": [
        "G Greene"
      ],
      "year": "2020",
      "venue": "The ethics of AI and emotional intelligence: Data sources, applications, and questions for evaluating ethics risk"
    },
    {
      "citation_id": "54",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "55",
      "title": "The risks of using ai to interpret human emotions",
      "authors": [
        "M Purdy",
        "J Zealley",
        "O Maseli"
      ],
      "year": "2019",
      "venue": "Harvard Business Review"
    },
    {
      "citation_id": "56",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "57",
      "title": "The turn to affect: A critique",
      "authors": [
        "R Leys"
      ],
      "year": "2011",
      "venue": "Critical inquiry"
    },
    {
      "citation_id": "58",
      "title": "The atlas of AI: Power, politics, and the planetary costs of artificial intelligence",
      "authors": [
        "K Crawford"
      ],
      "year": "2021",
      "venue": "The atlas of AI: Power, politics, and the planetary costs of artificial intelligence"
    },
    {
      "citation_id": "59",
      "title": "Leaders' smiles reflect cultural differences in ideal affect",
      "authors": [
        "J Tsai",
        "J Ang",
        "E Blevins",
        "J Goernandt",
        "H Fung",
        "D Jiang",
        "J Elliott",
        "A √ñlzer",
        "Y Uchida",
        "Y.-C Lee"
      ],
      "year": "2016",
      "venue": "Emotion"
    },
    {
      "citation_id": "60",
      "title": "Sexist ai: an experiment integrating casa and elm",
      "authors": [
        "J.-W Hong",
        "S Choi",
        "D Williams"
      ],
      "year": "2020",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "61",
      "title": "Artificial intelligence and the risk of new colonialism",
      "authors": [
        "U Sahbaz"
      ],
      "year": "2019",
      "venue": "Horizons: Journal of International Relations and Sustainable Development"
    },
    {
      "citation_id": "62",
      "title": "Decolonial AI: decolonial theory as sociotechnical foresight in artificial intelligence",
      "authors": [
        "S Mohamed",
        "M.-T Png",
        "W Isaac"
      ],
      "year": "2020",
      "venue": "Philosophy & Technology"
    },
    {
      "citation_id": "63",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "64",
      "title": "The Oxford handbook of ethics of AI",
      "authors": [
        "M Dubber",
        "F Pasquale",
        "S Das"
      ],
      "year": "2020",
      "venue": "The Oxford handbook of ethics of AI"
    },
    {
      "citation_id": "65",
      "title": "Responsible ai: Gender bias assessment in emotion recognition",
      "authors": [
        "A Domnich",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "Responsible ai: Gender bias assessment in emotion recognition",
      "arxiv": "arXiv:2103.11436"
    },
    {
      "citation_id": "66",
      "title": "Artificial intelligence and ethics",
      "authors": [
        "J Shaw"
      ],
      "year": "2019",
      "venue": "Harvard Magazine"
    },
    {
      "citation_id": "67",
      "title": "Dilemmas of artificial intelligence",
      "authors": [
        "P Denning",
        "D Denning"
      ],
      "year": "2020",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "68",
      "title": "The global landscape of AI ethics guidelines",
      "authors": [
        "A Jobin",
        "M Ienca",
        "E Vayena"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "69",
      "title": "A unified framework of five principles for AI in society",
      "authors": [
        "L Floridi",
        "J Cowls"
      ],
      "venue": "Harvard Data Science Review"
    },
    {
      "citation_id": "70",
      "title": "Machine learning and law: an overview",
      "authors": [
        "H Surden"
      ],
      "year": "2021",
      "venue": "Machine learning and law: an overview"
    },
    {
      "citation_id": "71",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency"
    },
    {
      "citation_id": "72",
      "title": "Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products",
      "authors": [
        "I Raji",
        "J Buolamwini"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "73",
      "title": "Ethics of ai in law",
      "authors": [
        "H Surden"
      ],
      "year": "2020",
      "venue": "The Oxford handbook of ethics of AI"
    },
    {
      "citation_id": "74",
      "title": "Artificial intelligence and law: An overview",
      "year": "2019",
      "venue": "Georgia State University Law Review"
    },
    {
      "citation_id": "75",
      "title": "Machine learning and law",
      "year": "2014",
      "venue": "Wash. L. Rev"
    },
    {
      "citation_id": "76",
      "title": "Values embedded in legal artificial intelligence",
      "year": "2022",
      "venue": "IEEE Technology and Society Magazine"
    },
    {
      "citation_id": "77",
      "title": "Emotional ai, soft biometrics and the surveillance of emotional life: An unusual consensus on privacy",
      "authors": [
        "A Mcstay"
      ],
      "year": "2020",
      "venue": "Big Data & Society"
    },
    {
      "citation_id": "78",
      "title": "Suspect ai: Vibraimage, emotion recognition technology and algorithmic opacity",
      "authors": [
        "J Wright"
      ],
      "year": "2021",
      "venue": "Science, Technology and Society"
    },
    {
      "citation_id": "79",
      "title": "Artificial intelligence is misreading human emotion",
      "authors": [
        "K Crawford"
      ],
      "year": "2021",
      "venue": "The Atlantic"
    },
    {
      "citation_id": "80",
      "title": "Artificial intelligence: From ethics to law",
      "authors": [
        "M Carrillo"
      ],
      "year": "2020",
      "venue": "Telecommunications Policy"
    },
    {
      "citation_id": "81",
      "title": "Ai-based facial recognition technology and criminal justice issues and challenges",
      "authors": [
        "Z Khan",
        "A Rizvi"
      ],
      "year": "2021",
      "venue": "Turkish Journal of Computer and Mathematics Education (TURCOMAT)"
    },
    {
      "citation_id": "82",
      "title": "Ai and criminal justice",
      "authors": [
        "S Miller"
      ],
      "venue": "Ai and criminal justice"
    },
    {
      "citation_id": "83",
      "title": "What people think ai should infer from faces",
      "authors": [
        "S Engelmann",
        "C Ullstein",
        "O Papakyriakopoulos",
        "J Grossklags"
      ],
      "year": "2022",
      "venue": "2022 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "84",
      "title": "We have to talk about emotional ai and crime",
      "authors": [
        "L Podoletz"
      ],
      "year": "2022",
      "venue": "AI & SOCIETY"
    },
    {
      "citation_id": "85",
      "title": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind",
      "authors": [
        "M Minsky"
      ],
      "year": "2007",
      "venue": "The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind"
    },
    {
      "citation_id": "86",
      "title": "Heart of the machine: Our future in a world of artificial emotional intelligence",
      "authors": [
        "R Yonck"
      ],
      "year": "2020",
      "venue": "Heart of the machine: Our future in a world of artificial emotional intelligence"
    },
    {
      "citation_id": "87",
      "title": "The ethical questions that haunt facialrecognition research",
      "authors": [
        "R Van Noorden"
      ],
      "year": "2020",
      "venue": "Nature"
    },
    {
      "citation_id": "88",
      "title": "Time to regulate ai that interprets human emotions",
      "authors": [
        "K Crawford"
      ],
      "year": "2021",
      "venue": "Nature"
    },
    {
      "citation_id": "89",
      "title": "The citizens' biometrics council: Recommendations and findings of a public deliberation on biometrics technology, policy, and governance",
      "authors": [
        "A Institute"
      ],
      "year": "2021",
      "venue": "The citizens' biometrics council: Recommendations and findings of a public deliberation on biometrics technology, policy, and governance"
    },
    {
      "citation_id": "90",
      "title": "The ethical algorithm: The science of socially aware algorithm design",
      "authors": [
        "M Kearns",
        "A Roth"
      ],
      "year": "2019",
      "venue": "The ethical algorithm: The science of socially aware algorithm design"
    },
    {
      "citation_id": "91",
      "title": "Artificial intelligence a modern approach",
      "authors": [
        "S Russell"
      ],
      "year": "2010",
      "venue": "Artificial intelligence a modern approach"
    },
    {
      "citation_id": "92",
      "title": "A unified framework of five principles for ai in society",
      "authors": [
        "L Floridi",
        "J Cowls"
      ],
      "year": "2019",
      "venue": "SSRN 3831321"
    },
    {
      "citation_id": "93",
      "title": "Islamic virtuebased ethics for artificial intelligence",
      "authors": [
        "A Raquib",
        "B Channa",
        "T Zubair",
        "J Qadir"
      ],
      "year": "2022",
      "venue": "Discover Artificial Intelligence"
    },
    {
      "citation_id": "94",
      "title": "Good proctor or \"big brother\"? ethics of online exam supervision technologies",
      "authors": [
        "S Coghlan",
        "T Miller",
        "J Paterson"
      ],
      "year": "2021",
      "venue": "Philosophy & Technology"
    },
    {
      "citation_id": "95",
      "title": "A robotic new hope: Opportunities, challenges, and ethical considerations of social robots",
      "authors": [
        "A Boch",
        "L Lucaj",
        "C Corrigan"
      ],
      "venue": "A robotic new hope: Opportunities, challenges, and ethical considerations of social robots"
    },
    {
      "citation_id": "96",
      "title": "Face recognition technology: security versus privacy",
      "authors": [
        "K Bowyer"
      ],
      "year": "2004",
      "venue": "IEEE Technology and society magazine"
    },
    {
      "citation_id": "97",
      "title": "Facial recognition, emotion and race in animated social media",
      "authors": [
        "L Stark"
      ],
      "year": "2018",
      "venue": "Facial recognition, emotion and race in animated social media"
    },
    {
      "citation_id": "98",
      "title": "'affective'computing and emotion recognition systems: the future of biometric surveillance",
      "authors": [
        "J Bullington"
      ],
      "year": "2005",
      "venue": "Proceedings of the 2nd annual conference on Information security curriculum development"
    },
    {
      "citation_id": "99",
      "title": "Societal and ethical issues of digitization",
      "authors": [
        "L Royakkers",
        "J Timmer",
        "L Kool",
        "R Van Est"
      ],
      "year": "2018",
      "venue": "Ethics and Information Technology"
    },
    {
      "citation_id": "100",
      "title": "Shared privacy concerns of the visually impaired and sighted bystanders with camera-based assistive technologies",
      "authors": [
        "T Akter",
        "T Ahmed",
        "A Kapadia",
        "M Swaminathan"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Accessible Computing (TACCESS)"
    },
    {
      "citation_id": "101",
      "title": "De-identification for privacy protection in multimedia content: A survey",
      "authors": [
        "S Ribaric",
        "A Ariyaeeinia",
        "N Pavesic"
      ],
      "year": "2016",
      "venue": "Signal Processing: Image Communication"
    },
    {
      "citation_id": "102",
      "title": "Assisting users in a world full of cameras: A privacy-aware infrastructure for computer vision applications",
      "authors": [
        "A Das",
        "M Degeling",
        "X Wang",
        "J Wang",
        "N Sadeh",
        "M Satyanarayanan"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "103",
      "title": "Ethics of ai-enabled recruiting and selection: A review and research agenda",
      "authors": [
        "A Hunkenschroer",
        "C Luetge"
      ],
      "year": "2022",
      "venue": "Journal of Business Ethics"
    },
    {
      "citation_id": "104",
      "title": "Sface: Privacy-friendly and accurate face recognition using synthetic data",
      "authors": [
        "F Boutros",
        "M Huber",
        "P Siebke",
        "T Rieber",
        "N Damer"
      ],
      "year": "2022",
      "venue": "Sface: Privacy-friendly and accurate face recognition using synthetic data",
      "arxiv": "arXiv:2206.10520"
    },
    {
      "citation_id": "105",
      "title": "What if your emotions were tracked to spy on you",
      "authors": [
        "L Woensel",
        "N Nevil"
      ],
      "year": "2019",
      "venue": "What if your emotions were tracked to spy on you"
    },
    {
      "citation_id": "106",
      "title": "Privacy by design: The 7 foundational principles",
      "authors": [
        "A Cavoukian"
      ],
      "year": "2009",
      "venue": "Information and privacy commissioner of Ontario"
    },
    {
      "citation_id": "107",
      "title": "Human autonomy in the age of artificial intelligence",
      "authors": [
        "C Prunkl"
      ],
      "year": "2022",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "108",
      "title": "Supporting human autonomy in ai systems: A framework for ethical enquiry",
      "authors": [
        "R Calvo",
        "D Peters",
        "K Vold",
        "R Ryan"
      ],
      "year": "2020",
      "venue": "Ethics of Digital Well-Being"
    },
    {
      "citation_id": "109",
      "title": "Social integration of artificial intelligence: functions, automation allocation logic and human-autonomy trust",
      "authors": [
        "H Abbass"
      ],
      "year": "2019",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "110",
      "title": "Robot autonomy vs. human autonomy: Social robots, artificial intelligence (ai), and the nature of autonomy",
      "authors": [
        "P Formosa"
      ],
      "year": "2021",
      "venue": "Minds and Machines"
    },
    {
      "citation_id": "111",
      "title": "The human in emotion recognition on social media: Attitudes, outcomes, risks",
      "authors": [
        "N Andalibi",
        "J Buss"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "112",
      "title": "Toward accountable human-centered ai: rationale and promising directions",
      "authors": [
        "J Qadir",
        "M Islam",
        "A Al-Fuqaha"
      ],
      "year": "2022",
      "venue": "Journal of Information, Communication and Ethics in Society"
    },
    {
      "citation_id": "113",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "M Zeiler",
        "R Fergus"
      ],
      "year": "2014",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "114",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "115",
      "title": "Scalable private learning with PATE",
      "authors": [
        "N Papernot",
        "S Song",
        "I Mironov",
        "A Raghunathan",
        "K Talwar",
        "√ö Erlingsson"
      ],
      "year": "2018",
      "venue": "Scalable private learning with PATE",
      "arxiv": "arXiv:1802.08908"
    }
  ]
}