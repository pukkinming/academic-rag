{
  "paper_id": "2306.06760v1",
  "title": "Estimating The Uncertainty In Emotion Attributes Using Deep Evidential Regression",
  "published": "2023-06-11T20:07:29Z",
  "authors": [
    "Wen Wu",
    "Chao Zhang",
    "Philip C. Woodland"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In automatic emotion recognition (AER), labels assigned by different human annotators to the same utterance are often inconsistent due to the inherent complexity of emotion and the subjectivity of perception. Though deterministic labels generated by averaging or voting are often used as the ground truth, it ignores the intrinsic uncertainty revealed by the inconsistent labels. This paper proposes a Bayesian approach, deep evidential emotion regression (DEER), to estimate the uncertainty in emotion attributes. Treating the emotion attribute labels of an utterance as samples drawn from an unknown Gaussian distribution, DEER places an utterance-specific normal-inverse gamma prior over the Gaussian likelihood and predicts its hyper-parameters using a deep neural network model. It enables a joint estimation of emotion attributes along with the aleatoric and epistemic uncertainties. AER experiments on the widely used MSP-Podcast and IEMOCAP datasets showed DEER produced state-of-theart results for both the mean values and the distribution of emotion attributes 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition (AER) is the task that enables computers to predict human emotional states based on multimodal signals, such as audio, video and text. An emotional state is defined based on either categorical or dimensional theory. The categorical theory claims the existence of a small number of basic discrete emotions ( i.e. anger and happy) that are inherent in our brain and universally recognised  (Gunes et al., 2011; Plutchik, 2001) . Dimensional emotion theory characterises emotional states by a small number of roughly orthogonal fundamental continuousvalued bipolar dimensions  (Schlosberg, 1954; Nicolaou et al., 2011)  such as valence-arousal and approach-avoidance  (Russell and Mehrabian, 1977;   1 Code available: https://github.com/W-Wu/DEER  Russell, 1980; Grimm et al., 2007) . These dimensions are also known as emotion attributes, which allow us to model more subtle and complex emotions and are thus more common in psychological studies. As a result, AER includes a classification approach based on emotion-class-based labels and a regression approach based on attribute-based labels. This paper focuses on attribute-based AER with speech input.\n\nEmotion annotation is challenging due to the inherent ambiguity of mixed emotion, the personal variations in emotion expression, the subjectivity in emotion perception, etc. Most AER datasets use multiple human annotators to label each utterance, which often results in inconsistent labels, either as emotion categories or attributes. This is also a typical manifestation of the intrinsic data uncertainty, also referred to as aleatoric uncertainty  (Matthies, 2007; Der Kiureghian and Ditlevsen, 2009) , that arises from the natural complexity of emotion data. It is common to replace such inconsistent labels with deterministic labels obtained by majority voting  (Busso et al., 2008 (Busso et al., , 2017) )  or (weighted) averages  (Ringeval et al., 2013; Lotfian and Busso, 2019; Kossaifi et al., 2019; Grimm and Kroschel, 2005) . However, this causes a loss of data samples when a majority agreed emotion class doesn't exist  (Majumder et al., 2018; Poria et al., 2018; Wu et al., 2021)  and also ignores the discrepancies between annotators and the aleatoric uncertainty in emotion data.\n\nIn this paper, we propose to model the uncertainty in emotion attributes with a Bayesian approach based on deep evidential regression  (Amini et al., 2020) , denoted deep evidential emotion regression (DEER). In DEER, the inconsistent human labels of each utterance are considered as observations drawn independently from an unknown Gaussian distribution. To probabilistically estimate the mean and variance of the Gaussian distribution, a normal inverse-gamma (NIG) prior is introduced, which places a Gaussian prior over the mean and an inverse-gamma prior over the variance. The AER system is trained to predict the hyper-parameters of the NIG prior for each utterance by maximising the per-observation-based marginal likelihood of each observed label under this prior. As a result, DEER not only models the distribution of emotion attributes but also learns both the aleatoric uncertainty and the epistemic uncertainty  (Der Kiureghian and Ditlevsen, 2009)  without repeating the inference procedure for sampling. Epistemic uncertainty, also known as model uncertainty, is associated with uncertainty in model parameters that best explain the observed data. Aleatoric and epistemic uncertainty are combined to induce the total uncertainty, also called predictive uncertainty, that measures the confidence of attribute predictions. As a further improvement, a novel regulariser is proposed based on the mean and variance of the observed labels to better calibrate the uncertainty estimation. The proposed methods were evaluated on the MSP-Podcast and IEMOCAP datasets.\n\nThe rest of the paper is organised as follows. Section 2 summarises related work. Section 3 introduces the proposed DEER approach. Sections 4 and 5 present the experimental setup and results respectively, followed by the conclusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "There has been previous work by AER researchers to address the issue of inconsistent labels. For emotion categories, a single ground-truth label can be obtained as either a continuous-valued mean vector representing emotion intensities  (Fayek et al., 2016; Ando et al., 2018) , or as a multi-hot vector obtained based on the existence of emotions  (Zhang et al., 2020; Ju et al., 2020) . Recently, distribution-based approaches have been proposed, which consider the labels as samples drawn from emotion distributions  (Chou et al., 2022; Wu et al., 2022b) .\n\nFor emotion attributes, annotators often assign different values to the same attribute of each utterance.  Davani et al. (2022)  proposed a multiannotator model which contains multiple heads to predict each annotator's judgement. This approach is computationally viable only when the number of annotators is relatively small. The method requires sufficient annotations from each annotator to be effective.  Deng et al. (2012)  derived confidence measures based on annotator agreement to build emotion-scoring models.  Han et al. (2017 Han et al. ( , 2021) )  proposed predicting the standard deviation of the attribute label values as an extra task in the multitask training framework.  Dang et al. (2017 Dang et al. ( , 2018) )  included annotator variability as a representation of uncertainty in a Gaussian mixture regression model. These techniques take the variance of human annotations either as an extra target or as an extra input. More recently, Bayesian deep learning has been introduced to the task, which models the uncertainty in emotion annotation without explicitly using the variance of human annotations. These include the use of Gaussian processes  (Atcheson et al., 2018 (Atcheson et al., , 2019)) , variational auto-encoders  (Sridhar et al., 2021) , Bayesian neural networks  (Prabhu et al., 2021) , Monte-Carlo dropout  (Sridhar and Busso, 2020b)  and sequential Monte-Carlo methods  (Markov et al., 2015; Wu et al., 2022a) .\n\nSo far, these methods have not distinguished aleatoric uncertainty from epistemic uncertainty which are defined in the introduction. Our proposed DEER approach can simultaneously model these two uncertainties. In addition, our approach is more generic. It has no limits on the number of annotators, the number of annotators per utterance, and the number of annotations per annotator, and thus can cope with large crowd-sourced datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Deep Evidential Emotion Regression",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Setup",
      "text": "In contrast to Bayesian neural networks that place priors on model parameters  (Blundell et al., 2015; Kendall and Gal, 2017) , evidential deep learning  (Sensoy et al., 2018; Malinin and Gales, 2018; Amini et al., 2020)  places priors over the likelihood function. Every training sample adds support to a learned higher-order prior distribution called the evidential distribution. Sampling from this distribution gives instances of lower-order likelihood functions from which the data was drawn.\n\nConsider an input utterance x with M emotion attribute labels y (1) , . . . , y (M ) provided by multiple annotators. Assuming y (1) , . . . , y (M ) are observations drawn i.i.d. from a Gaussian distribution with unknown mean µ and unknown variance σ 2 , where µ is drawn from a Gaussian prior and σ 2 is drawn from an inverse-gamma prior:\n\nwhere γ ∈ R, υ > 0, and Γ(•) is the gamma function with α > 1 and β > 0.\n\nDenote {µ, σ 2 } and {γ, υ, α, β} as Ψ and Ω. The posterior p(Ψ|Ω) is a NIG distribution, which is the Gaussian conjugate prior:\n\nDrawing a sample Ψ i from the NIG distribution yields a single instance of the likelihood function\n\nThe NIG distribution therefore serves as the higher-order, evidential distribution on top of the unknown lower-order likelihood distribution from which the observations are drawn. The NIG hyper-parameters Ω determine not only the location but also the uncertainty associated with the inferred likelihood function.\n\nBy training a deep neural network model to output the hyper-parameters of the evidential distribution, evidential deep learning allows the uncertainties to be found by analytic computation of the maximum likelihood Gaussian without the need for repeated inference for sampling  (Amini et al., 2020) . Furthermore, it also allows an effective estimate of the aleatoric uncertainty computed as the expectation of the variance of the Gaussian distribution, as well as the epistemic uncertainty defined as the variance of the predicted Gaussian mean. Given an NIG distribution, the prediction, aleatoric, and epistemic uncertainty can be computed as:\n\n, ∀ α > 1",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Training",
      "text": "The training of DEER is structured as fitting the model to the data while enforcing the prior to calibrate the uncertainty when the prediction is wrong.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Maximising The Data Fit",
      "text": "The likelihood of an observation y given the evidential distribution hyper-parameters Ω is computed by marginalising over the likelihood parameters Ψ:\n\nAn analytical solution exists in the case of placing an NIG prior on the Gaussian likelihood function:\n\nwhere St ν (t|r, s) is the Student's t-distribution evaluated at t with location parameter r, scale parameter s, and ν degrees of freedom. The predicted mean and variance can be computed analytically as\n\nVar[y] represents the total uncertainty of model prediction, which is equal to the summation of the aleatoric uncertainty E[σ 2 ] and epistemic uncertainty Var[µ] according to the law of total variance:\n\nTo fit the NIG distribution, the model is trained by maximising the sum of the marginal likelihoods of each human label y (m) . The negative log likelihood (NLL) loss can be computed as\n\nThis is our proposed per-observation-based NLL loss, which takes each observed label into consideration for AER. This loss serves as the first part of the objective function for training a deep neural network model Θ to predict the hyper-parameters {γ, υ, α, β} to fit all observed labels of x.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Calibrating The Uncertainty On Errors",
      "text": "The second part of the objective function regularises training by calibrating the uncertainty based on the incorrect predictions. A novel regulariser is formulated which contains two terms: L µ and L σ that respectively regularises the errors on the estimation of the mean µ and the variance σ 2 of the Gaussian likelihood.\n\nThe first term L µ is proportional to the error between the model prediction and the average of the observations: m) is the averaged label which is usually used as the ground truth in regression-based AER, and Φ is an uncertainty measure associated with the inferred posterior. The reciprocal of the total uncertainty is used as Φ in this paper, which can be calculated as\n\nThe regulariser imposes a penalty when there's an error in prediction and dynamically scales it by dividing by the total uncertainty of inferred posterior.\n\nIt penalises the cases where the model produces an incorrect prediction with a small uncertainty, thus preventing the model from being over-confident.\n\nFor instance, if the model produces an error with a small predicted variance, Φ is large, resulting in a large penalty. Minimising the regularisation term enforces the model to produce accurate prediction or increase uncertainty when the error is large.\n\nIn addition to imposing a penalty on the mean prediction as in  Amini et al. (2020) , a second term L σ is proposed in order to calibrate the estimation of the aleatoric uncertainty. As discussed in the introduction, aleatoric uncertainty in AER is shown by the different emotional labels given to the same utterance by different human annotators. This paper uses the variance of the observations to describe the aleatoric uncertainty in the emotion data. The second regularising term is defined as:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Summary And Implementation Details",
      "text": "For an AER task that consists of N emotion attributes, DEER trains a deep neural network model to simultaneously predict the hyperparameters {Ω 1 , . . . , Ω N } associated with the N attribute-specific NIG distributions, where Ω n = {γ n , υ n , α n , β n }. A DEER model thus has 4N output units. The system is trained by minimising the total loss w.r.t. Θ as:\n\nwhere ϵ n is the weight satisfying N n=1 ϵ n = 1, λ n is the scale coefficient that trades off the training between data fit and uncertainty regulation.\n\nAt test-time, the predictive posteriors are N separate Student's t-distributions p(y|Ω 1 ), p(y|Ω 2 ) , . . . , p(y|Ω N ), each of the same form as derived in Eqn. (2) 2  . Apart from obtaining a distribution over the emotion attribute of the speaker, DEER also allows analytic computation of the uncertainty terms, as summarised in Table  1 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Term Expression",
      "text": "Predicted mean\n\nAleatoric uncertainty\n\nTable  1 : Summary of the uncertainty terms.\n\n4 Experimental Setup",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "The MSP-Podcast  (Lotfian and Busso, 2019)  and IEMOCAP datasets  (Busso et al., 2008)  were used in this paper. The annotations of both datasets use N = 3 with valence, arousal (also called activation), and dominance as the emotion attributes. MSP-Podcast contains natural English speech from podcast recordings and is one of the largest publicly available datasets in speech emotion recognition.\n\nA seven-point Likert scale was used to evaluate valence (1-negative vs 7-positive), arousal (1-calm vs 7-active), and dominance (1-weak vs 7-strong).\n\nThe corpus was annotated using crowd-sourcing. Each utterance was labelled by at least 5 human annotators and has an average of 6.7 annotations per utterance. Ground-truth labels were defined by the average value. Release 1.8 was used in the experiments, which contains 73,042 utterances from 1,285 speakers amounting to more than 110 hours of speech. The average variance of the labels assigned to each sentence is 0.975, 1.122, 0.889 for valence, arousal, and dominance respectively. The standard splits for training (44,879 segments), validation (7,800 segments) and testing (15,326 segments) were used in the experiments.\n\nThe IEMOCAP corpus is one of the most widely used AER datasets. It consists of approximately 12 hours of English speech including 5 dyadic conversational sessions performed by 10 professional actors with a session being a conversation between two speakers. There are in total 151 dialogues including 10,039 utterances. Each utterance was annotated by three human annotators using a fivepoint Likert scale. Again, ground-truth labels were determined by taking the average. The average variance of the labels assigned to each sentence is 0.130, 0.225, 0.300 for valence, arousal, and dominance respectively. Unless otherwise mentioned, systems on IEMOCAP were evaluated by training on Session 1-4 and testing on Session 5.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Structure",
      "text": "The model structure used in this paper follows the upstream-downstream framework  (wen Yang et al., 2021) , as illustrated in Figure  1 . WavLM  (Chen et al., 2022)  was used as the upstream model, which is a speech foundation model pre-trained by selfsupervised learning. The BASE+ version 3 of the model was used in this paper which has 12 Transformer encoder blocks with 768-dimensional hidden states and 8 attention heads. The parameters of the pre-trained model were frozen and the weighted sum of the outputs of the 12 Transformer encoder blocks was used as the speech embeddings and fed into the downstream model.\n\nThe downstream model consists of two 128dimensional Transformer encoder blocks with 4head self-attention, followed by an evidential layer that contains four output units for each of the three attributes, which has a total of 12 output units. The model contains 0.3M trainable parameters. A Softplus activaton 4 was applied to {υ, α, β} to ensure υ, α, β > 0 with an additional +1 added to α to ensure α > 1. A linear activation was used for γ ∈ R. The proposed DEER model was trained to simultaneously learn three evidential distributions for the three attributes. The weights in Eqn. (5) were set as A dropout rate of 0.3 was applied to the transformer parameters. The system was implemented using PyTorch and the SpeechBrain toolkit  (Ravanelli et al., 2021) . The Adam optimizer was used with an initial learning rate set to 0.001. Training took ∼ 8 hours on an NVIDIA A100 GPU.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mean Prediction",
      "text": "Following prior work in continuous emotion recognition  (Ringeval et al., 2015 (Ringeval et al., , 2017;; Sridhar and Busso, 2020a; Leem et al., 2022) , the concordance correlation coefficient (CCC) was used to evaluate the predicted mean. CCC combines the Pearson's correlation coefficient with the square difference between the mean of the two compared sequences:\n\nwhere ρ is the Pearson correlation coefficient between a hypothesis sequence (system predictions) and a reference sequence, where µ hyp and µ ref are the mean values, and σ 2 hyp and σ 2 ref are the variance values of the two sequences. Hypotheses that are well correlated with the reference but shifted in value are penalised in proportion to the deviation. The value of CCC ranges from -1 (perfect disagreement) to 1 (perfect agreement).   6 ). 'v' , 'a', 'd' stands for valence, arousal, dominance. '↑' denotes the higher the better, '↓' denotes the lower the better. The 'L in Eqn. (  6 )' row systems used the complete total loss of DEER. The 'L σ = 0' row systems had no L σ regularisation term in the total loss. The 'L NLL = LNLL ' row systems replaced the individual human labels with LNLL in the total loss.\n\nThe root mean square error (RMSE) averaged over the test set is also reported. Since the average of the human labels, ȳ, is defined as the ground truth in both datasets, ȳ were used as the reference in computing the CCC and RMSE. However, using ȳ also indicates that these metrics are less informative when the aleatoric uncertainty is large.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Uncertainty Estimation",
      "text": "It is common to use NLL to measure the uncertainty estimation ability  (Gal and Ghahramani, 2016; Amini et al., 2020) . NLL is computed by fitting data to the predictive posterior q(y).\n\nIn this paper, NLL(avg) defined as -log q(ȳ) and NLL(all) defined as -1 M M m=1 log q(y (m) ) are both used. NLL(avg) measures how much the averaged label ȳ fits into the predicted posterior distribution, and NLL(all) measures how much every single human label y (m) fits into the predicted posterior. A lower NLL indicates better uncertainty estimation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Effect Of The Aleatoric Regulariser L Σ",
      "text": "First, by setting L σ = 0 in the total loss, an ablation study of the effect of the proposed extra regularising term L σ is performed. The results are given in the 'L σ = 0' rows in Table  2 . In this case, only L µ is used to regularise L NLL and the results are compared to those trained using the complete loss defined in Eqn. (  6 ), which are shown in the 'L in Eqn. (6)' rows. From the results, L σ improves the performance in CCC and NLL(all), but not in NLL(avg), as expected.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effect Of The Per-Observation-Based L Nll",
      "text": "Next, the effect of our proposed per-observationbased NLL loss defined in Eqn. (4), L NLL , is compared to an alternative. Instead of using L NLL , LNLL = -log p(ȳ|Ω) is used to compute the total loss during training, and the results are given in the 'L NLL = LNLL ' rows in Table  2 . While L NLL considers the likelihood of fitting each individual observation into the predicted posterior, LNLL only considers the averaged observation. Therefore, it is expected that using LNLL instead of L NLL yields a smaller NLL(avg) but larger NLL(all), which have been validated by the results in the table.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baseline Comparisons",
      "text": "Three baseline systems were built:\n\n• A Gaussian Process (GP) with a radial basis function kernel, trained by maximising the per-observation-based marginal likelihood.\n\n• A Monte Carlo dropout (MCdp) system with a dropout rate of 0.4. During inference, the system was forwarded 50 times with different dropout random seeds to obtain 50 samples.\n\n• An ensemble of 10 systems initialised and trained with 10 different random seeds.\n\nThe MCdp and ensemble baselines used the same model structure as the DEER system, except that the evidential output layer was replaced by a standard fully-connected output layer with three output units to predict the values of valence, arousal and dominance respectively. Following prior work  (Al-Badawy and Kim, 2018; Atmaja and Akagi, 2020b; Sridhar and Busso, 2020b) , the CCC loss,\n\nwas used for training the MCdp and ensemble baselines. The CCC loss was computed based on the sequence within each mini-batch of training data.\n\nThe CCC loss has been shown by previous studies to improve the continuous emotion predictions compared to the RMSE loss  (Povolny et al., 2016; Trigeorgis et al., 2016; Le et al., 2017) . For MCdp and ensemble, the predicted distribution of the emotion attributes were estimated based on the obtained samples by kernel density estimation.\n\nThe results are listed in Table  3 . The proposed DEER system outperforms the baselines on most of the attributes and the overall values. In particular, DEER outperforms all baselines consistently in the NLL(all) metric.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Cross Comparison Of Mean Prediction",
      "text": "Table  4  compares results obtained with those previously published in terms of the CCC value. Previous papers have reported results on both version 1.6 and 1.8 of the MSP-Podcast dataset. For comparison, we also conducted experiments on version 1.6 for comparison. Version 1.6 of MSP-Podcast database is a subset of version 1.8 and contains 34,280 segments for training, 5,958 segments for validation and 10,124 segments for testing. For IEMOCAP, apart from training on Session 1-4 and testing on Session 5 (Ses05), we also evaluated the proposed system by a 5-fold cross-validation (5CV) based on a \"leave-one-session-out\" strategy.\n\nIn each fold, one session was left out for testing and the others were used for training. The configuration is speaker-exclusive for both settings. As shown in Table  4 , our DEER systems achieved state-of-theart results on both versions of MSP-Podcast and both test settings of IEMOCAP.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Analysis Of Uncertainty Estimation",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Visualisation",
      "text": "Based on a randomly selected subset test set of MSP-Podcast version 1.8, the aleatoric, epistemic and total uncertainty of the dominance attribute predicted by our proposed DEER system are shown in Figure  2 . Figure  2  (a) shows the predicted mean ± square root of the predicted aleatoric uncertainty (E[µ] ± E[σ 2 ]) and the average label ± the standard deviation of the human labels (ȳ ± σ). It can be seen that the predicted aleatoric uncertainty (blue) overlaps with the label standard deviation (grey) and the overlapping is more evident when the mean predictions are accurate ( i.e. samples around index 80-100).\n\nFigure  2  (b) shows the predicted mean ± square root of the predicted epistemic uncertainty (E[µ] ± Var[µ]). The epistemic uncertainty is high when the predicted mean deviates from the target ( i.e. samples around index 40-50) while low then the predicted mean matches the target ( i.e. samples around index 80-100).\n\nFigure  2  (c) shows the predicted mean ± square root of the total epistemic uncertainty (E[y] ± Var[y]) which combines the aleatoric and epistemic uncertainty. The total uncertainty is high either when the input utterance is complex or the model is not confident.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Reject Option",
      "text": "A reject option was applied to analyse the uncertainty estimation performance, where the system has the option to accept or decline a test sample based on the uncertainty prediction. Since the evaluation of CCC is based on the whole sequence rather than individual samples, its computation would be affected when the sequence is modified by rejection  (Wu et al., 2022a) . Therefore, the reject option is performed based on RMSE. Confidence is measured by the total uncertainty given in Eqn. (3). Figure  3  shows the performance of the proposed DEER system with a reject option on MSP-Podcast and IEMOCAP. A percentage of utterances with the largest predicted variance were rejected. The results at 0% rejection corresponds to the RMSE achieved on the entire test data. As the percentage of rejection increases, test coverage decreases and the average RMSE decreases showing the predicted variance succeeded in confidence estimation. The system then trades off between the test coverage and performance.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "Two types of uncertainty exist in AER: (i) aleatoric uncertainty arising from the inherent ambiguity of emotion and personal variations in emotion expres-sion; (ii) epistemic uncertainty associated with the estimated network parameters given the observed data. This paper proposes DEER for estimating those uncertainties in emotion attributes. Treating observed attribute-based annotations as samples drawn from a Gaussian distribution, DEER places a normal-inverse gamma (NIG) prior over the Gaussian likelihood. A novel training loss is proposed which combines a per-observation-based NLL loss with a regulariser on both the mean and the variance of the Gaussian likelihood. Experiments on the MSP-Podcast and IEMOCAP datasets show that DEER can produce state-of-the-art results in estimating both the mean value and the distribution of emotion attributes. The use of NIG, the conjugate prior to the Gaussian distribution, leads to tractable analytic computation of the marginal likelihood as well as aleatoric and epistemic uncertainty associated with attribute prediction. Uncertainty estimation is analysed by visualisation and a reject option. Beyond the scope of AER, DEER could also be applied to other tasks with subjective evaluations yielding inconsistent labels.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitations",
      "text": "The proposed approach (along with other methods for estimating uncertainty in inconsistent annotations) is only viable when the raw labels from different human annotators for each sentence are provided by the datasets. However, some multipleannotated datasets only released the majority vote or averaged label for each sentence ( i.e.  Poria et al., 2019) .\n\nThe proposed method made a Gaussian assumption on the likelihood function for the analytic computation of the uncertainties. The results show that this modelling approach is effective. Despite the effectiveness of the proposed method, other distributions could also be considered.\n\nData collection processes for AER datasets vary in terms of recording conditions, emotional elicitation scheme, and annotation procedure, etc. This work was tested on two typical datasets: IEMO-CAP and MSP-Podcast. The two datasets are both publicly available and differ in various aspects:\n\n• IEMOCAP contains emotion acted by professional actors while MSP-Podcast contains natural emotion.\n\n• IEMOCAP contains dyadic conversations while MSP-Podcast contains Podcast recordings.\n\n• IEMOCAP contains 10 speakers and MSP-Podcast contains 1285 speakers.\n\n• IEMOCAP contains about 12 hours of speech and MSP-Podcast contains more than 110 hours of speech.\n\n• IEMOCAP was annotated by six professional evaluators with each sentence being annotated by three evaluators. MSP-Podcast was annotated by crowd-sourcing where a total of 11,799 workers were involved and each work annotated 41.5 sentences on average.\n\nThe proposed approach has shown effective over both datasets. We believe the proposed technique should be generic. Furthermore, although validated only for AER, the proposed method could also be applied to other tasks with disagreements in subjective annotations such as hate speech detection and language assessment.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ethics Statement",
      "text": "In tasks involving subjective evaluations such as emotion recognition, it is common to employ multiple human annotators to give multiple annotations to each data instance. When annotators disagree, majority voting and averaging are commonly used to derive single ground truth labels for training supervised machine learning systems. However, in many subjective tasks, there is usually no single \"correct\" answer. By enforcing a single ground truth, there's a potential risk of ignoring the valuable nuance in each annotator's evaluation and their disagreements. This can cause minority views to be under-represented. The DEER approach proposed in this work could be beneficial to this concern as it models uncertainty in annotator disagreements and provides some explainability of the predictions. While our method helps preserve minority perspectives, misuse of this technique might lead to ethical concerns. Emotion recognition is at risk of exposing a person's inner state to others and this information could be abused. Furthermore, since the proposed approach takes each annotation into consideration, it is important to protect the anonymity of annotators.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Derivation Of The Predictive Posterior",
      "text": "Since NIG is the Gaussian conjugate prior,\n\nIn DEER, the predictive posterior and posterior are both conditioned on Ω, written as p(y * |D, Ω) and p(Ψ|D, Ω) to be precise. Also, the information of D is contained in Ω * since Ω * = f Θ(x * ) and Θ is the optimal model parameters obtained by training on D. Then the predictive posterior can be written as p(y * |Ω * ). Given the conjugate prior, the predictive posterior in DEER can be computed by directly substituting the predicted Ω * into the expression of marginal likelihood derived in Eqn. (2), skipping the step of calculating the posterior.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B Fusion With Text Modality",
      "text": "This appendix presents bi-modal experiments that incorporate text information into the DEER model. Transcriptions were obtained from a publicly available automatic speech recognition (ASR) model \"wav2vec2-base-960h\" 6 which fine-tuned the wav2vec 2.0  (Baevski et al., 2020)  model on 960 hours Librispeech data  (Panayotov et al., 2015) . Transcriptions were first encoded by a RoBERTa model  (Liu et al., 2019)  and fed into another twolayer Transformer encoder. As shown in Figure  4 , outputs from the text Transformer were concatenated with the outputs from the audio Transformer encoder and fed into the evidential output layer. Results are shown in Table  5 . Incorporating text information improves the estimation of valence but not necessarily for arousal and dominance. Similar phenomena were observed by  (Triantafyllopoulos et al., 2022) . A possible explanation is that text is effective for sentiment analysis (positive or negative) but may not be as informative as audio to determine a speaker's level of excitement. CCC for dominance improves more for IEMOCAP than MSP-Podcast possibly because IEMOCAP is an acted dataset and the emotion may be exaggerated compared with MSP-Podcast which contains natural emotion.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: WavLM (Chen",
      "page": 5
    },
    {
      "caption": "Figure 1: Illustration of the model structure. Weights",
      "page": 5
    },
    {
      "caption": "Figure 2: Visualisation of (a) aleatoric (b) epistemic (c)",
      "page": 7
    },
    {
      "caption": "Figure 2: Figure 2 (a) shows the predicted mean ± square",
      "page": 8
    },
    {
      "caption": "Figure 2: (b) shows the predicted mean ± square",
      "page": 8
    },
    {
      "caption": "Figure 2: (c) shows the predicted mean ± square",
      "page": 8
    },
    {
      "caption": "Figure 3: Reject Option of RMSE based on predicted",
      "page": 8
    },
    {
      "caption": "Figure 3: shows the performance",
      "page": 8
    },
    {
      "caption": "Figure 4: Model structure for bi-modal experiments.",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "distribution of emotion attributes1."
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "1\nIntroduction"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "Automatic emotion recognition (AER) is the task"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "that enables computers to predict human emotional"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "states based on multimodal signals,\nsuch as au-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "dio, video and text.\nAn emotional\nstate is de-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "fined based on either categorical or dimensional"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "theory.\nThe categorical\ntheory claims\nthe exis-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "tence of a small number of basic discrete emo-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "tions ( i.e.\nanger and happy) that are inherent\nin"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "our brain and universally recognised (Gunes et al.,"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "2011; Plutchik, 2001). Dimensional emotion the-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "ory characterises emotional states by a small num-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "ber of roughly orthogonal fundamental continuous-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "valued bipolar dimensions (Schlosberg, 1954; Nico-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "laou et al., 2011) such as valence-arousal and ap-"
        },
        {
          "art\nresults for both the mean values and the": ""
        },
        {
          "art\nresults for both the mean values and the": "proach–avoidance (Russell and Mehrabian, 1977;"
        },
        {
          "art\nresults for both the mean values and the": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "{ww368, pcw}@eng.cam.ac.uk;\ncz277@tsinghua.edu.cn"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "Russell, 1980; Grimm et al., 2007). These dimen-\nAbstract"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "sions are also known as emotion attributes, which"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "In automatic emotion recognition (AER),\nla-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "allow us to model more subtle and complex emo-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "bels assigned by different human annotators to"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "tions and are thus more common in psychological"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "the same utterance are often inconsistent due"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "studies. As a result, AER includes a classification"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "to the inherent complexity of emotion and the"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "approach based on emotion-class-based labels and\nsubjectivity of perception. Though determin-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "istic labels generated by averaging or voting\na regression approach based on attribute-based la-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "are often used as the ground truth,\nit\nignores"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "bels. This paper focuses on attribute-based AER"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "the intrinsic uncertainty revealed by the incon-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "with speech input."
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "sistent labels. This paper proposes a Bayesian"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "Emotion annotation is challenging due to the in-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "approach, deep evidential emotion regression"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "herent ambiguity of mixed emotion, the personal\n(DEER), to estimate the uncertainty in emotion"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "variations in emotion expression, the subjectivity\nattributes.\nTreating the emotion attribute la-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "bels of an utterance as samples drawn from an\nin emotion perception, etc. Most AER datasets use"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "unknown Gaussian distribution, DEER places"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "multiple human annotators to label each utterance,"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "an utterance-specific normal-inverse gamma"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "which often results in inconsistent labels, either as"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "prior over the Gaussian likelihood and predicts"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "emotion categories or attributes. This is also a typi-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "its hyper-parameters using a deep neural net-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "cal manifestation of the intrinsic data uncertainty,"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "work model.\nIt enables a joint estimation of"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "also referred to as aleatoric uncertainty (Matthies,"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "emotion attributes along with the aleatoric and"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "2007; Der Kiureghian and Ditlevsen, 2009),\nthat\nepistemic uncertainties. AER experiments on"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "the widely used MSP-Podcast and IEMOCAP\narises from the natural complexity of emotion data."
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "datasets showed DEER produced state-of-the-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "It\nis common to replace such inconsistent\nlabels"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "art\nresults for both the mean values and the"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "with deterministic labels obtained by majority vot-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "distribution of emotion attributes1."
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "ing (Busso et al., 2008, 2017) or\n(weighted) av-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "erages (Ringeval et al., 2013; Lotfian and Busso,"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "1\nIntroduction"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "2019; Kossaifi et al., 2019; Grimm and Kroschel,"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "Automatic emotion recognition (AER) is the task"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "2005). However,\nthis causes a loss of data sam-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "that enables computers to predict human emotional"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "ples when a majority agreed emotion class doesn’t"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "states based on multimodal signals,\nsuch as au-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "exist\n(Majumder et al., 2018; Poria et al., 2018;"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "dio, video and text.\nAn emotional\nstate is de-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "Wu et al., 2021) and also ignores the discrepancies"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "fined based on either categorical or dimensional"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "between annotators and the aleatoric uncertainty in"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "theory.\nThe categorical\ntheory claims\nthe exis-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "emotion data."
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "tence of a small number of basic discrete emo-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "In this paper, we propose to model\nthe uncer-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "tions ( i.e.\nanger and happy) that are inherent\nin"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "tainty in emotion attributes with a Bayesian ap-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "our brain and universally recognised (Gunes et al.,"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "proach based on deep evidential regression (Amini"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "2011; Plutchik, 2001). Dimensional emotion the-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "et al., 2020), denoted deep evidential emotion re-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "ory characterises emotional states by a small num-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "gression (DEER). In DEER,\nthe inconsistent hu-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "ber of roughly orthogonal fundamental continuous-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "man labels of each utterance are considered as ob-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "valued bipolar dimensions (Schlosberg, 1954; Nico-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "servations drawn independently from an unknown"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "laou et al., 2011) such as valence-arousal and ap-"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "Gaussian distribution. To probabilistically estimate"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "proach–avoidance (Russell and Mehrabian, 1977;"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "the mean and variance of the Gaussian distribution,"
        },
        {
          "2 Department of Electrical Engineering, Tsinghua University, Beijing, China": "1Code available: https://github.com/W-Wu/DEER\na normal inverse-gamma (NIG) prior is introduced,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "which places a Gaussian prior over the mean and an": "inverse-gamma prior over the variance. The AER",
          "proposed predicting the standard deviation of the": "attribute label values as an extra task in the multi-"
        },
        {
          "which places a Gaussian prior over the mean and an": "system is trained to predict the hyper-parameters",
          "proposed predicting the standard deviation of the": "task training framework. Dang et al. (2017, 2018)"
        },
        {
          "which places a Gaussian prior over the mean and an": "of the NIG prior for each utterance by maximising",
          "proposed predicting the standard deviation of the": "included annotator variability as a representation of"
        },
        {
          "which places a Gaussian prior over the mean and an": "the per-observation-based marginal\nlikelihood of",
          "proposed predicting the standard deviation of the": "uncertainty in a Gaussian mixture regression model."
        },
        {
          "which places a Gaussian prior over the mean and an": "each observed label under this prior. As a result,",
          "proposed predicting the standard deviation of the": "These techniques take the variance of human an-"
        },
        {
          "which places a Gaussian prior over the mean and an": "DEER not only models the distribution of emo-",
          "proposed predicting the standard deviation of the": "notations either as an extra target or as an extra"
        },
        {
          "which places a Gaussian prior over the mean and an": "tion attributes but also learns both the aleatoric",
          "proposed predicting the standard deviation of the": "input. More recently, Bayesian deep learning has"
        },
        {
          "which places a Gaussian prior over the mean and an": "uncertainty and the epistemic uncertainty (Der Ki-",
          "proposed predicting the standard deviation of the": "been introduced to the task, which models the un-"
        },
        {
          "which places a Gaussian prior over the mean and an": "ureghian and Ditlevsen, 2009) without repeating",
          "proposed predicting the standard deviation of the": "certainty in emotion annotation without explicitly"
        },
        {
          "which places a Gaussian prior over the mean and an": "the inference procedure for sampling. Epistemic",
          "proposed predicting the standard deviation of the": "using the variance of human annotations. These"
        },
        {
          "which places a Gaussian prior over the mean and an": "uncertainty, also known as model uncertainty, is as-",
          "proposed predicting the standard deviation of the": "include the use of Gaussian processes (Atcheson"
        },
        {
          "which places a Gaussian prior over the mean and an": "sociated with uncertainty in model parameters that",
          "proposed predicting the standard deviation of the": "et al., 2018, 2019), variational auto-encoders (Srid-"
        },
        {
          "which places a Gaussian prior over the mean and an": "best explain the observed data. Aleatoric and epis-",
          "proposed predicting the standard deviation of the": "har et al., 2021), Bayesian neural networks (Prabhu"
        },
        {
          "which places a Gaussian prior over the mean and an": "temic uncertainty are combined to induce the total",
          "proposed predicting the standard deviation of the": "et al., 2021), Monte-Carlo dropout\n(Sridhar and"
        },
        {
          "which places a Gaussian prior over the mean and an": "uncertainty, also called predictive uncertainty, that",
          "proposed predicting the standard deviation of the": "Busso, 2020b) and sequential Monte-Carlo meth-"
        },
        {
          "which places a Gaussian prior over the mean and an": "measures the confidence of attribute predictions.",
          "proposed predicting the standard deviation of the": "ods (Markov et al., 2015; Wu et al., 2022a)."
        },
        {
          "which places a Gaussian prior over the mean and an": "As a further improvement, a novel regulariser is",
          "proposed predicting the standard deviation of the": "So far,\nthese methods have not distinguished"
        },
        {
          "which places a Gaussian prior over the mean and an": "proposed based on the mean and variance of the",
          "proposed predicting the standard deviation of the": "aleatoric uncertainty from epistemic uncertainty"
        },
        {
          "which places a Gaussian prior over the mean and an": "observed labels to better calibrate the uncertainty",
          "proposed predicting the standard deviation of the": "which are defined in the introduction. Our proposed"
        },
        {
          "which places a Gaussian prior over the mean and an": "estimation. The proposed methods were evaluated",
          "proposed predicting the standard deviation of the": "DEER approach can simultaneously model these"
        },
        {
          "which places a Gaussian prior over the mean and an": "on the MSP-Podcast and IEMOCAP datasets.",
          "proposed predicting the standard deviation of the": "two uncertainties.\nIn addition, our approach is"
        },
        {
          "which places a Gaussian prior over the mean and an": "The rest of\nthe paper\nis organised as follows.",
          "proposed predicting the standard deviation of the": "more generic.\nIt has no limits on the number of"
        },
        {
          "which places a Gaussian prior over the mean and an": "Section 2 summarises related work. Section 3 in-",
          "proposed predicting the standard deviation of the": "annotators, the number of annotators per utterance,"
        },
        {
          "which places a Gaussian prior over the mean and an": "troduces the proposed DEER approach. Sections 4",
          "proposed predicting the standard deviation of the": "and the number of annotations per annotator, and"
        },
        {
          "which places a Gaussian prior over the mean and an": "and 5 present\nthe experimental setup and results",
          "proposed predicting the standard deviation of the": "thus can cope with large crowd-sourced datasets."
        },
        {
          "which places a Gaussian prior over the mean and an": "respectively, followed by the conclusion.",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "3\nDeep Evidential Emotion Regression"
        },
        {
          "which places a Gaussian prior over the mean and an": "2\nRelated Work",
          "proposed predicting the standard deviation of the": "3.1\nProblem setup"
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "In contrast to Bayesian neural networks that place"
        },
        {
          "which places a Gaussian prior over the mean and an": "There has been previous work by AER researchers",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "priors on model parameters (Blundell et al., 2015;"
        },
        {
          "which places a Gaussian prior over the mean and an": "to address the issue of inconsistent labels. For emo-",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "Kendall\nand Gal, 2017),\nevidential deep learn-"
        },
        {
          "which places a Gaussian prior over the mean and an": "tion categories, a single ground-truth label can be",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "ing (Sensoy et al., 2018; Malinin and Gales, 2018;"
        },
        {
          "which places a Gaussian prior over the mean and an": "obtained as either a continuous-valued mean vector",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "Amini et al., 2020) places priors over the likelihood"
        },
        {
          "which places a Gaussian prior over the mean and an": "representing emotion intensities (Fayek et al., 2016;",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "function. Every training sample adds support to a"
        },
        {
          "which places a Gaussian prior over the mean and an": "Ando et al., 2018), or as a multi-hot vector obtained",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "learned higher-order prior distribution called the"
        },
        {
          "which places a Gaussian prior over the mean and an": "based on the existence of emotions (Zhang et al.,",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "evidential distribution. Sampling from this distri-"
        },
        {
          "which places a Gaussian prior over the mean and an": "2020; Ju et al., 2020). Recently, distribution-based",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "bution gives instances of\nlower-order\nlikelihood"
        },
        {
          "which places a Gaussian prior over the mean and an": "approaches have been proposed, which consider the",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "functions from which the data was drawn."
        },
        {
          "which places a Gaussian prior over the mean and an": "labels as samples drawn from emotion distributions",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "Consider an input utterance x with M emotion"
        },
        {
          "which places a Gaussian prior over the mean and an": "(Chou et al., 2022; Wu et al., 2022b).",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "attribute labels y(1), . . . , y(M ) provided by multiple"
        },
        {
          "which places a Gaussian prior over the mean and an": "For emotion attributes, annotators often assign",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "annotators. Assuming y(1), . . . , y(M ) are observa-"
        },
        {
          "which places a Gaussian prior over the mean and an": "different values to the same attribute of each ut-",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "tions drawn i.i.d. from a Gaussian distribution with"
        },
        {
          "which places a Gaussian prior over the mean and an": "terance. Davani et al.\n(2022) proposed a multi-",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "unknown mean µ and unknown variance σ2, where"
        },
        {
          "which places a Gaussian prior over the mean and an": "annotator model which contains multiple heads to",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "µ is drawn from a Gaussian prior and σ2 is drawn"
        },
        {
          "which places a Gaussian prior over the mean and an": "predict each annotator’s judgement. This approach",
          "proposed predicting the standard deviation of the": ""
        },
        {
          "which places a Gaussian prior over the mean and an": "",
          "proposed predicting the standard deviation of the": "from an inverse-gamma prior:"
        },
        {
          "which places a Gaussian prior over the mean and an": "is computationally viable only when the number of",
          "proposed predicting the standard deviation of the": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "The posterior p(Ψ|Ω) is a NIG distribution, which",
          "An analytical solution exists in the case of placing": "an NIG prior on the Gaussian likelihood function:"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "is the Gaussian conjugate prior:",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "(cid:114) υ\nΓ(1/2 + α)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "p(Ψ|Ω) = p(µ|σ2, Ω) p(σ2|Ω)",
          "An analytical solution exists in the case of placing": "p(y|Ω) =\n(2β(1 + υ))α"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "Γ(α)\nπ"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "= N (γ, σ2υ−1) Γ−1(α, β)",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "2 +α)\n· (cid:0)υ(y − γ)2 + 2β(1 + υ)(cid:1)−( 1"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "(cid:19)α+1\n(cid:18) 1\nβα√\nυ",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "(cid:18)\n(cid:19)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "√\n=",
          "An analytical solution exists in the case of placing": "β(1 + υ)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "σ2",
          "An analytical solution exists in the case of placing": "y|γ,\n(2)\n= St2α"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "Γ(α)\n2πσ2",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "υ α"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "(cid:26)\n(cid:27)",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "2β + υ(γ − µ)2",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "· exp\n−",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "2σ2",
          "An analytical solution exists in the case of placing": "is\nthe Student’s\nt-distribution\nwhere Stν (t|r, s)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "evaluated at t with location parameter r, scale pa-"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "from the NIG distribution\nDrawing a sample Ψi",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "rameter s, and ν degrees of freedom. The predicted"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "yields a single instance of the likelihood function",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "mean and variance can be computed analytically as"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "N (µi, σ2\ni ). The NIG distribution therefore serves",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "as the higher-order, evidential distribution on top",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "β(1 + υ)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "of the unknown lower-order likelihood distribution",
          "An analytical solution exists in the case of placing": "E[y] = γ,\nVar[y] =\n(3)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "υ(α − 1)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "from which the observations are drawn. The NIG",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "hyper-parameters Ω determine not only the loca-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "Var[y]\nrepresents the total uncertainty of model"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "tion but also the uncertainty associated with the",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "prediction, which is equal to the summation of the"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "inferred likelihood function.",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "aleatoric uncertainty E[σ2] and epistemic uncer-"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "By training a deep neural network model to out-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "tainty Var[µ] according to the law of total variance:"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "put\nthe hyper-parameters of the evidential distri-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "bution, evidential deep learning allows the uncer-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "Var[y] = E[Var[y|Ψ]] + Var[E[y|Ψ]]"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "tainties to be found by analytic computation of the",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "= E[σ2] + Var[µ]"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "maximum likelihood Gaussian without\nthe need",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "for repeated inference for sampling (Amini et al.,",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "To fit the NIG distribution, the model is trained"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "2020). Furthermore, it also allows an effective esti-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "by maximising the sum of the marginal likelihoods"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "mate of the aleatoric uncertainty computed as the",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "of each human label y(m). The negative log likeli-"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "expectation of the variance of the Gaussian distribu-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "hood (NLL) loss can be computed as"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "tion, as well as the epistemic uncertainty defined as",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "the variance of the predicted Gaussian mean. Given",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "an NIG distribution, the prediction, aleatoric, and",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "1 M\nM(cid:88) m\nLNLL(Θ) = −\nlog p(y(m)|Ω)\n(4)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "epistemic uncertainty can be computed as:",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "=1"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "(cid:19)(cid:21)\n(cid:20)\n(cid:18)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "Prediction: E[µ] = γ",
          "An analytical solution exists in the case of placing": "β(1 + υ)"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "1 M\nM(cid:88) m\n= −\nlog\ny(m)|γ,\nSt2α"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "β",
          "An analytical solution exists in the case of placing": "υ α"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": ",\n∀ α > 1\nAleatoric: E[σ2] =",
          "An analytical solution exists in the case of placing": "=1"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "α − 1",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "β",
          "An analytical solution exists in the case of placing": "This is our proposed per-observation-based NLL"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": ",\n∀ α > 1\nEpistemic: Var[µ] =",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "υ(α − 1)",
          "An analytical solution exists in the case of placing": "loss, which takes each observed label into consid-"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "eration for AER. This loss serves as the first part"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "3.2\nTraining",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "of the objective function for training a deep neural"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "The training of DEER is structured as fitting the",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "network model Θ to predict the hyper-parameters"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "model to the data while enforcing the prior to cali-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "{γ, υ, α, β} to fit all observed labels of x."
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "brate the uncertainty when the prediction is wrong.",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "3.2.2\nCalibrating the uncertainty on errors"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "3.2.1\nMaximising the data fit",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "The second part of\nthe objective function regu-"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "The likelihood of an observation y given the eviden-",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "larises training by calibrating the uncertainty based"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "tial distribution hyper-parameters Ω is computed",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "on the incorrect predictions. A novel regulariser"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "by marginalising over the likelihood parameters Ψ:",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "is formulated which contains two terms: Lµ and"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "(cid:90)",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "",
          "An analytical solution exists in the case of placing": "Lσ that respectively regularises the errors on the"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "p(y|Ω) =\np(y|Ψ)p(Ψ|Ω) dΨ",
          "An analytical solution exists in the case of placing": ""
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "Ψ\n(1)",
          "An analytical solution exists in the case of placing": "estimation of the mean µ and the variance σ2 of"
        },
        {
          "Denote {µ, σ2} and {γ, υ, α, β} as Ψ and Ω.": "= Ep(Ψ|Ω)[p(y|Ψ)]",
          "An analytical solution exists in the case of placing": "the Gaussian likelihood."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": "Var[y]\nβ(1 + υ)"
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": "The regulariser imposes a penalty when there’s an"
        },
        {
          "Φ =\n=": "error in prediction and dynamically scales it by di-"
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": "viding by the total uncertainty of inferred posterior."
        },
        {
          "Φ =\n=": "It penalises the cases where the model produces an"
        },
        {
          "Φ =\n=": "incorrect prediction with a small uncertainty, thus"
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": "preventing the model from being over-confident."
        },
        {
          "Φ =\n=": "For instance, if the model produces an error with a"
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": "small predicted variance, Φ is large, resulting in a"
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": ""
        },
        {
          "Φ =\n=": "large penalty. Minimising the regularisation term"
        },
        {
          "Φ =\n=": "enforces the model to produce accurate prediction"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "tion of the aleatoric uncertainty. As discussed in"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "the introduction, aleatoric uncertainty in AER is"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "shown by the different emotional\nlabels given to"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "the same utterance by different human annotators."
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "This paper uses the variance of the observations to"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "describe the aleatoric uncertainty in the emotion"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "data. The second regularising term is defined as:"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "Lσ(Θ) = Φ |¯σ2 − E[σ2]|"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "(cid:80)M"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "where ¯σ2 = 1"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "m=1(y(m) − ¯y)2.\nM"
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": ""
        },
        {
          "Lσ\nis proposed in order\nto calibrate the estima-": "3.3\nSummary and implementation details"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: While LNLL considers the like-",
      "data": [
        {
          "CCC ↑": "a",
          "RMSE ↓": "a",
          "NLL(avg) ↓": "a",
          "NLL(all) ↓": "a"
        },
        {
          "CCC ↑": "0.698",
          "RMSE ↓": "0.680",
          "NLL(avg) ↓": "1.285",
          "NLL(all) ↓": "1.692"
        },
        {
          "CCC ↑": "0.687",
          "RMSE ↓": "0.679",
          "NLL(avg) ↓": "1.277",
          "NLL(all) ↓": "1.705"
        },
        {
          "CCC ↑": "0.682",
          "RMSE ↓": "0.673",
          "NLL(avg) ↓": "1.060",
          "NLL(all) ↓": "2.089"
        },
        {
          "CCC ↑": "a",
          "RMSE ↓": "a",
          "NLL(avg) ↓": "a",
          "NLL(all) ↓": "a"
        },
        {
          "CCC ↑": "0.755",
          "RMSE ↓": "0.457",
          "NLL(avg) ↓": "0.795",
          "NLL(all) ↓": "1.053"
        },
        {
          "CCC ↑": "0.752",
          "RMSE ↓": "0.466",
          "NLL(avg) ↓": "0.773",
          "NLL(all) ↓": "1.069"
        },
        {
          "CCC ↑": "0.759",
          "RMSE ↓": "0.444",
          "NLL(avg) ↓": "0.727",
          "NLL(all) ↓": "1.329"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: While LNLL considers the like-",
      "data": [
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "Lσ = 0\n0.582\n0.752\n0.553\n0.772\n0.466",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "0.655\n1.180\n0.773\n1.061\n1.408\n1.069\n1.294"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "LNLL = ¯LNLL\n0.585\n0.759\n0.555\n0.786\n0.444",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "0.633\n1.001\n0.727\n1.036\n1.627\n1.329\n1.441"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "Table 2: DEER results variations of the loss in Eqn. (6). ‘v’ , ‘a’, ‘d’ stands for valence, arousal, dominance. ‘↑’",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "denotes the higher the better, ‘↓’ denotes the lower the better. The ‘L in Eqn. (6)’ row systems used the complete",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "total loss of DEER. The ‘Lσ = 0’ row systems had no Lσ regularisation term in the total loss. The ‘LNLL = ¯LNLL’"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "row systems replaced the individual human labels with ¯LNLL in the total loss.",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "The root mean square error (RMSE) averaged",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "5.2\nEffect of the per-observation-based LNLL"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "over the test set is also reported. Since the average",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "Next,\nthe effect of our proposed per-observation-"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "of the human labels, ¯y,\nis defined as the ground",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "based NLL loss defined in Eqn. (4), LNLL, is com-"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "truth in both datasets, ¯y were used as the reference",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "pared to an alternative. Instead of using LNLL,"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "in computing the CCC and RMSE. However, using",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "y also indicates that these metrics are less informa-",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "LNLL = − log p(¯y|Ω)"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "tive when the aleatoric uncertainty is large.",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "is used to compute the total\nloss during training,"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "4.3.2\nUncertainty estimation",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "and the results are given in the ‘LNLL = ¯LNLL’"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "rows in Table 2. While LNLL considers the like-"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "It is common to use NLL to measure the uncertainty",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "lihood of fitting each individual observation into"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "estimation ability (Gal\nand Ghahramani, 2016;",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "the predicted posterior,\nLNLL only considers the"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "Amini et al., 2020). NLL is computed by fitting",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "averaged observation.\nTherefore,\nit\nis expected"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "data to the predictive posterior q(y).",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "that using\nLNLL instead of LNLL yields a smaller"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "In this paper, NLL(avg) defined as − log q(¯y)",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "NLL(avg) but\nlarger NLL(all), which have been"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "(cid:80)M",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "and NLL(all) defined as − 1",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "m=1 log q(y(m))\nM",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "validated by the results in the table."
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "are both used. NLL(avg) measures how much the",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "averaged label ¯y fits into the predicted posterior",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "5.3\nBaseline comparisons"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "distribution, and NLL(all) measures how much ev-",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "Three baseline systems were built:"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "ery single human label y(m) fits into the predicted",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "posterior. A lower NLL indicates better uncertainty",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "• A Gaussian Process (GP) with a radial basis"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "estimation.",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "function kernel,\ntrained by maximising the"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "per-observation-based marginal likelihood."
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "5\nExperiments and Results",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "• A Monte Carlo dropout (MCdp) system with"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "5.1\nEffect of the aleatoric regulariser Lσ",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "a dropout rate of 0.4. During inference,\nthe"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "system was forwarded 50 times with different"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "First, by setting Lσ = 0 in the total loss, an ablation",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "dropout random seeds to obtain 50 samples."
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "study of the effect of the proposed extra regularis-",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "ing term Lσ is performed. The results are given in",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "• An ensemble of 10 systems initialised and"
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "the ‘Lσ = 0’ rows in Table 2.\nIn this case, only",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": "trained with 10 different random seeds."
        },
        {
          "L in Eqn. (6)\n0.596\n0.755\n0.569\n0.755\n0.457": "Lµ is used to regularise LNLL and the results are",
          "0.638\n1.070\n0.795\n1.035\n1.275\n1.053\n1.283": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CCC ↑": "a",
          "RMSE ↓": "a",
          "NLL(avg) ↓": "a",
          "NLL(all) ↓": "a"
        },
        {
          "CCC ↑": "0.698",
          "RMSE ↓": "0.680",
          "NLL(avg) ↓": "1.285",
          "NLL(all) ↓": "1.692"
        },
        {
          "CCC ↑": "0.595",
          "RMSE ↓": "0.673",
          "NLL(avg) ↓": "1.408",
          "NLL(all) ↓": "1.808"
        },
        {
          "CCC ↑": "0.667",
          "RMSE ↓": "0.702",
          "NLL(avg) ↓": "1.300",
          "NLL(all) ↓": "2.027"
        },
        {
          "CCC ↑": "0.679",
          "RMSE ↓": "0.692",
          "NLL(avg) ↓": "1.384",
          "NLL(all) ↓": "2.066"
        },
        {
          "CCC ↑": "a",
          "RMSE ↓": "a",
          "NLL(avg) ↓": "a",
          "NLL(all) ↓": "a"
        },
        {
          "CCC ↑": "0.756",
          "RMSE ↓": "0.457",
          "NLL(avg) ↓": "0.795",
          "NLL(all) ↓": "1.053"
        },
        {
          "CCC ↑": "0.717",
          "RMSE ↓": "0.479",
          "NLL(avg) ↓": "0.791",
          "NLL(all) ↓": "1.205"
        },
        {
          "CCC ↑": "0.724",
          "RMSE ↓": "0.561",
          "NLL(avg) ↓": "0.849",
          "NLL(all) ↓": "1.325"
        },
        {
          "CCC ↑": "0.754",
          "RMSE ↓": "0.476",
          "NLL(avg) ↓": "0.864",
          "NLL(all) ↓": "1.218"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "MCdp\n0.539\n0.724\n0.568\n0.786\n0.561",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "0.702\n1.291\n0.849\n1.133\n1.549\n1.325\n1.747"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "Ensemble\n0.580\n0.754\n0.560\n0.778\n0.476",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "0.686\n1.296\n0.864\n1.110\n1.584\n1.218\n1.749"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "Table 3: Comparison with the baselines. ‘v’, ‘a’, ‘d’ stands for valence, arousal, dominance. ‘↑’ denotes the higher",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "the better, ‘↓’ denotes the lower the better. Best results in each column shown in bold.",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "dominance respectively. Following prior work (Al-",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "y\nE[\n]\n2]\nE["
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "Badawy and Kim, 2018; Atmaja and Akagi, 2020b;",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "Sridhar and Busso, 2020b), the CCC loss,",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "6"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "4"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "Lccc = 1 − ρccc",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "0\n20\n40\n60\n80\n100"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "was used for training the MCdp and ensemble base-",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "(a) Aleatoric uncertainty"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "lines. The CCC loss was computed based on the",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "y\nE[\n]\nVar[\n]"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "sequence within each mini-batch of training data.",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "The CCC loss has been shown by previous stud-",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "6"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "ies to improve the continuous emotion predictions",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "4"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "compared to the RMSE loss (Povolny et al., 2016;",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "Trigeorgis et al., 2016; Le et al., 2017). For MCdp",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "0\n20\n40\n60\n80\n100"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "and ensemble, the predicted distribution of the emo-",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "(b) Epistemic uncertainty"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "tion attributes were estimated based on the obtained",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "y\nE[y]\nVar[y]"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "samples by kernel density estimation.",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "The results are listed in Table 3. The proposed",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "6"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "DEER system outperforms the baselines on most of",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "4"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "the attributes and the overall values. In particular,",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "DEER outperforms all baselines consistently in the",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "0\n20\n40\n60\n80\n100"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "(c) Total uncertainty"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "NLL(all) metric.",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "Figure 2: Visualisation of (a) aleatoric (b) epistemic (c)"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "5.4\nCross comparison of mean prediction",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "total uncertainty of dominance for MSP-Podcast. x-asix"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "Table 4 compares results obtained with those pre-",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "is the test utterance index."
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "viously published in terms of the CCC value. Pre-",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "the others were used for training. The configuration"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "vious papers have reported results on both version",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "is speaker-exclusive for both settings. As shown in"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "1.6 and 1.8 of the MSP-Podcast dataset. For com-",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "Table 4, our DEER systems achieved state-of-the-"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "parison, we also conducted experiments on version",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "art results on both versions of MSP-Podcast and"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "1.6 for comparison. Version 1.6 of MSP-Podcast",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "both test settings of IEMOCAP."
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "database is a subset of version 1.8 and contains",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "34,280 segments for training, 5,958 segments for",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "5.5\nAnalysis of uncertainty estimation"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "validation and 10,124 segments for\ntesting.\nFor",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "5.5.1\nVisualisation"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "IEMOCAP, apart from training on Session 1-4 and",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": ""
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "testing on Session 5 (Ses05), we also evaluated",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "Based on a randomly selected subset\ntest set of"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "the proposed system by a 5-fold cross-validation",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "MSP-Podcast version 1.8, the aleatoric, epistemic"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "(5CV) based on a “leave-one-session-out” strategy.",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "and total uncertainty of\nthe dominance attribute"
        },
        {
          "GP\n0.535\n0.717\n0.512\n0.763\n0.479": "In each fold, one session was left out for testing and",
          "0.791\n0.657\n1.209\n1.047\n1.295\n1.205\n1.380": "predicted by our proposed DEER system are shown"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 4: Cross comparison of the CCC value on MSP-Podcast and IEMOCAP. ‘v’, ‘a’, ‘d’ stands for valence,",
      "data": [
        {
          "Paper": "Ghriss et al. (2022)",
          "Version": "1.6",
          "v": "0.412",
          "a": "0.679",
          "d": "0.564",
          "Average": "0.552"
        },
        {
          "Paper": "Mitra et al. (2022)",
          "Version": "1.6",
          "v": "0.57",
          "a": "0.75",
          "d": "0.67",
          "Average": "0.663"
        },
        {
          "Paper": "Srinivasan et al. (2022)",
          "Version": "1.6",
          "v": "0.627",
          "a": "0.757",
          "d": "0.671",
          "Average": "0.685"
        },
        {
          "Paper": "DEER",
          "Version": "1.6",
          "v": "0.629",
          "a": "0.777",
          "d": "0.684",
          "Average": "0.697"
        },
        {
          "Paper": "Leem et al. (2022)",
          "Version": "1.8",
          "v": "0.212",
          "a": "0.572",
          "d": "0.505",
          "Average": "0.430"
        },
        {
          "Paper": "DEER",
          "Version": "1.8",
          "v": "0.506",
          "a": "0.698",
          "d": "0.613",
          "Average": "0.606"
        },
        {
          "Paper": "Paper",
          "Version": "Setting",
          "v": "v",
          "a": "a",
          "d": "d",
          "Average": "Average"
        },
        {
          "Paper": "Atmaja and Akagi (2020a)",
          "Version": "Ses05",
          "v": "0.421",
          "a": "0.590",
          "d": "0.484",
          "Average": "0.498"
        },
        {
          "Paper": "Atmaja and Akagi (2021)",
          "Version": "Ses05",
          "v": "0.553",
          "a": "0.579",
          "d": "0.465",
          "Average": "0.532"
        },
        {
          "Paper": "",
          "Version": "",
          "v": "",
          "a": "",
          "d": "",
          "Average": ""
        },
        {
          "Paper": "DEER",
          "Version": "Ses05",
          "v": "0.596",
          "a": "0.756",
          "d": "0.569",
          "Average": "0.640"
        },
        {
          "Paper": "Srinivasan et al. (2022)",
          "Version": "5CV",
          "v": "0.582",
          "a": "0.667",
          "d": "0.545",
          "Average": "0.598"
        },
        {
          "Paper": "DEER",
          "Version": "5CV",
          "v": "0.625",
          "a": "0.720",
          "d": "0.548",
          "Average": "0.631"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: Cross comparison of the CCC value on MSP-Podcast and IEMOCAP. ‘v’, ‘a’, ‘d’ stands for valence,",
      "data": [
        {
          "Srinivasan et al. (2022)": "DEER",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.625\n0.720\n0.548\n0.631\n5CV"
        },
        {
          "Srinivasan et al. (2022)": "Table 4: Cross comparison of the CCC value on MSP-Podcast and IEMOCAP. ‘v’, ‘a’, ‘d’ stands for valence,",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "arousal, dominance. ‘Version’ of MSP-Podcast denotes the release version of the dataset, and only the results from",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "the same dateset version are comparable.",
          "5CV\n0.582\n0.667\n0.545\n0.598": "‘Test set’ of IEMOCAP denotes the train/test split.\n‘Ses05’ denotes"
        },
        {
          "Srinivasan et al. (2022)": "training on Session 1-4 and testing on Session 5. ‘5CV’ denotes leave-one-session-out 5-fold cross validation.",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "in Figure 2.",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.75"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "v a d\nv a d\n0.75"
        },
        {
          "Srinivasan et al. (2022)": "Figure 2 (a) shows the predicted mean ± square",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.70"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.70"
        },
        {
          "Srinivasan et al. (2022)": "root of the predicted aleatoric uncertainty (E[µ] ±",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.65"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.65"
        },
        {
          "Srinivasan et al. (2022)": "(cid:112)E[σ2]) and the average label ± the standard de-",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.60"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.60\n0.55"
        },
        {
          "Srinivasan et al. (2022)": "viation of the human labels (¯y ± ¯σ). It can be seen",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.50\n0.55"
        },
        {
          "Srinivasan et al. (2022)": "that the predicted aleatoric uncertainty (blue) over-",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.45"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0.50"
        },
        {
          "Srinivasan et al. (2022)": "laps with the label standard deviation (grey) and",
          "5CV\n0.582\n0.667\n0.545\n0.598": "0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90"
        },
        {
          "Srinivasan et al. (2022)": "the overlapping is more evident when the mean",
          "5CV\n0.582\n0.667\n0.545\n0.598": "Percentage of rejection (%)\nPercentage of rejection (%)"
        },
        {
          "Srinivasan et al. (2022)": "predictions are accurate ( i.e. samples around index",
          "5CV\n0.582\n0.667\n0.545\n0.598": "(a) MSP-Podcast\n(b) IEMOCAP"
        },
        {
          "Srinivasan et al. (2022)": "80-100).",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "Figure 3: Reject Option of RMSE based on predicted"
        },
        {
          "Srinivasan et al. (2022)": "Figure 2 (b) shows the predicted mean ± square",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "variance for (a) MSP-Podcast and (b) IEMOCAP."
        },
        {
          "Srinivasan et al. (2022)": "root of the predicted epistemic uncertainty (E[µ] ±",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "(cid:112)Var[µ]). The epistemic uncertainty is high when",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "the predicted mean deviates from the target ( i.e.",
          "5CV\n0.582\n0.667\n0.545\n0.598": "by rejection (Wu et al., 2022a).\nTherefore,\nthe"
        },
        {
          "Srinivasan et al. (2022)": "samples around index 40-50) while low then the",
          "5CV\n0.582\n0.667\n0.545\n0.598": "reject option is performed based on RMSE."
        },
        {
          "Srinivasan et al. (2022)": "predicted mean matches the target ( i.e.\nsamples",
          "5CV\n0.582\n0.667\n0.545\n0.598": "Confidence is measured by the total uncertainty"
        },
        {
          "Srinivasan et al. (2022)": "around index 80-100).",
          "5CV\n0.582\n0.667\n0.545\n0.598": "given in Eqn. (3). Figure 3 shows the performance"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "of the proposed DEER system with a reject option"
        },
        {
          "Srinivasan et al. (2022)": "Figure 2 (c) shows the predicted mean ± square",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "on MSP-Podcast and IEMOCAP. A percentage of"
        },
        {
          "Srinivasan et al. (2022)": "root of\nthe\ntotal epistemic uncertainty (E[y] ±",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "(cid:112)Var[y]) which combines the aleatoric and epis-",
          "5CV\n0.582\n0.667\n0.545\n0.598": "utterances with the largest predicted variance were"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "rejected. The results at 0% rejection corresponds"
        },
        {
          "Srinivasan et al. (2022)": "temic uncertainty.\nThe total uncertainty is high",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "to the RMSE achieved on the entire test data. As"
        },
        {
          "Srinivasan et al. (2022)": "either when the input utterance is complex or the",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "the percentage of rejection increases, test coverage"
        },
        {
          "Srinivasan et al. (2022)": "model is not confident.",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "decreases and the average RMSE decreases show-"
        },
        {
          "Srinivasan et al. (2022)": "5.5.2\nReject option",
          "5CV\n0.582\n0.667\n0.545\n0.598": "ing the predicted variance succeeded in confidence"
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "estimation. The system then trades off between the"
        },
        {
          "Srinivasan et al. (2022)": "A reject option was applied to analyse the uncer-",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "test coverage and performance."
        },
        {
          "Srinivasan et al. (2022)": "tainty estimation performance, where the system",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "has the option to accept or decline a test sample",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "",
          "5CV\n0.582\n0.667\n0.545\n0.598": "6\nConclusions"
        },
        {
          "Srinivasan et al. (2022)": "based on the uncertainty prediction. Since the eval-",
          "5CV\n0.582\n0.667\n0.545\n0.598": ""
        },
        {
          "Srinivasan et al. (2022)": "uation of CCC is based on the whole sequence",
          "5CV\n0.582\n0.667\n0.545\n0.598": "Two types of uncertainty exist in AER: (i) aleatoric"
        },
        {
          "Srinivasan et al. (2022)": "rather\nthan individual\nsamples,\nits computation",
          "5CV\n0.582\n0.667\n0.545\n0.598": "uncertainty arising from the inherent ambiguity of"
        },
        {
          "Srinivasan et al. (2022)": "would be affected when the sequence is modified",
          "5CV\n0.582\n0.667\n0.545\n0.598": "emotion and personal variations in emotion expres-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sion; (ii) epistemic uncertainty associated with the": "estimated network parameters given the observed",
          "IEMOCAP contains 10 speakers and MSP-\n•": "Podcast contains 1285 speakers."
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "data. This paper proposes DEER for estimating",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "•\nIEMOCAP contains about 12 hours of speech"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "those uncertainties in emotion attributes. Treating",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "and MSP-Podcast contains more than 110"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "observed attribute-based annotations as samples",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "hours of speech."
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "drawn from a Gaussian distribution, DEER places",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "a normal-inverse gamma (NIG) prior over the Gaus-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "•\nIEMOCAP was annotated by six professional"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "sian likelihood. A novel training loss is proposed",
          "IEMOCAP contains 10 speakers and MSP-\n•": "evaluators with each sentence being annotated"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "which combines a per-observation-based NLL loss",
          "IEMOCAP contains 10 speakers and MSP-\n•": "by three evaluators. MSP-Podcast was an-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "with a regulariser on both the mean and the vari-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "notated by crowd-sourcing where a total of"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "ance of the Gaussian likelihood. Experiments on",
          "IEMOCAP contains 10 speakers and MSP-\n•": "11,799 workers were involved and each work"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "the MSP-Podcast and IEMOCAP datasets show",
          "IEMOCAP contains 10 speakers and MSP-\n•": "annotated 41.5 sentences on average."
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "that DEER can produce state-of-the-art results in",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "The proposed approach has been shown effective"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "estimating both the mean value and the distribu-",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "over both datasets. We believe the proposed tech-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "tion of emotion attributes. The use of NIG,\nthe",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "nique should be generic. Furthermore, although"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "conjugate prior to the Gaussian distribution, leads",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "validated only for AER, the proposed method could"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "to tractable analytic computation of the marginal",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "also be applied to other tasks with disagreements"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "likelihood as well as aleatoric and epistemic uncer-",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "in subjective annotations such as hate speech detec-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "tainty associated with attribute prediction. Uncer-",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "tion and language assessment."
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "tainty estimation is analysed by visualisation and",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "a reject option. Beyond the scope of AER, DEER",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "Ethics Statement"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "could also be applied to other tasks with subjective",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "evaluations yielding inconsistent labels.",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "In tasks involving subjective evaluations such as"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "emotion recognition, it is common to employ mul-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "Limitations",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "tiple human annotators to give multiple annotations"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "The proposed approach (along with other meth-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "to each data instance. When annotators disagree,"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "ods for estimating uncertainty in inconsistent an-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "majority voting and averaging are commonly used"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "notations) is only viable when the raw labels from",
          "IEMOCAP contains 10 speakers and MSP-\n•": "to derive single ground truth labels for training su-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "different human annotators for each sentence are",
          "IEMOCAP contains 10 speakers and MSP-\n•": "pervised machine learning systems. However,\nin"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "provided by the datasets. However, some multiple-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "many subjective tasks,\nthere is usually no single"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "annotated datasets only released the majority vote",
          "IEMOCAP contains 10 speakers and MSP-\n•": "“correct” answer. By enforcing a single ground"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "or averaged label\nfor each sentence (\ni.e.\nPoria",
          "IEMOCAP contains 10 speakers and MSP-\n•": "truth, there’s a potential risk of ignoring the valu-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "et al., 2019).",
          "IEMOCAP contains 10 speakers and MSP-\n•": "able nuance in each annotator’s evaluation and their"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "The proposed method made a Gaussian assump-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "disagreements. This can cause minority views to be"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "tion on the likelihood function for the analytic com-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "under-represented. The DEER approach proposed"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "putation of the uncertainties. The results show that",
          "IEMOCAP contains 10 speakers and MSP-\n•": "in this work could be beneficial to this concern as it"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "this modelling approach is effective. Despite the",
          "IEMOCAP contains 10 speakers and MSP-\n•": "models uncertainty in annotator disagreements and"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "effectiveness of the proposed method, other distri-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "provides some explainability of the predictions."
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "butions could also be considered.",
          "IEMOCAP contains 10 speakers and MSP-\n•": "While our method helps preserve minority per-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "Data collection processes for AER datasets vary",
          "IEMOCAP contains 10 speakers and MSP-\n•": "spectives, misuse of this technique might\nlead to"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "in terms of recording conditions, emotional elicita-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "ethical concerns. Emotion recognition is at risk of"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "tion scheme, and annotation procedure, etc. This",
          "IEMOCAP contains 10 speakers and MSP-\n•": "exposing a person’s inner state to others and this in-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "work was tested on two typical datasets:\nIEMO-",
          "IEMOCAP contains 10 speakers and MSP-\n•": "formation could be abused. Furthermore, since the"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "CAP and MSP-Podcast. The two datasets are both",
          "IEMOCAP contains 10 speakers and MSP-\n•": "proposed approach takes each annotation into con-"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "publicly available and differ in various aspects:",
          "IEMOCAP contains 10 speakers and MSP-\n•": "sideration, it is important to protect the anonymity"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "of annotators."
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "•\nIEMOCAP contains emotion acted by pro-",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "fessional actors while MSP-Podcast contains",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "Acknowledgements"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "natural emotion.",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "Wen Wu is supported by a Cambridge International"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "•\nIEMOCAP\ncontains\ndyadic\nconversations",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "Scholarship from the Cambridge Trust. This work"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "while MSP-Podcast contains Podcast record-",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "",
          "IEMOCAP contains 10 speakers and MSP-\n•": "has been performed using resources provided by"
        },
        {
          "sion; (ii) epistemic uncertainty associated with the": "ings.",
          "IEMOCAP contains 10 speakers and MSP-\n•": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Kavukcuoglu, and Daan Wierstra. 2015. Weight"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "versity of Cambridge Research Computing Service",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "uncertainty in neural network.\nIn Proc. ICML, Lille."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "(www.hpc.cam.ac.uk)\nfunded by EPSRC Tier-2",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "capital grant EP/T022159/1.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "The MSP-Podcast data was provided by The",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Provost,\nS. Kim,\nJ.N. Chang,\nS. Lee,\nand S.S."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Narayanan. 2008.\nIEMOCAP: Interactive emotional"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "University of Texas at Dallas through the Multi-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Language Re-\ndyadic motion capture database."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "modal Signal Processing Lab.\nThis material\nis",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "sources and Evaluation, 42:335–359."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "based upon work supported by the National Sci-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "ence Foundation under Grants No.\nIIS-1453781",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Carlos Busso, Srinivas Parthasarathy, Alec Burmania,"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Mohammed AbdelWahab, Najmeh Sadoughi, and"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "and CNS-1823166. Any opinions, findings, and",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Emily Mower Provost. 2017. MSP-IMPROV: An"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "conclusions or recommendations expressed in this",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "acted corpus of dyadic interactions to study emotion"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "material are those of the author(s) and do not nec-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "IEEE Transactions on Affective Comput-\nperception."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "essarily reflect the views of the National Science",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "ing, 8(1):67–80."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Foundation or The University of Texas at Dallas.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Sanyuan Chen, Chengyi Wang,\nZhengyang Chen,"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Kanda, Takuya Yoshioka, Xiong Xiao, et al. 2022."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "References",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "WavLM: Large-scale self-supervised pre-training for"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "IEEE Journal of Se-\nfull stack speech processing."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Ehab A AlBadawy and Yelin Kim. 2018. Joint discrete",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "lected Topics in Signal Processing."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "and continuous emotion prediction using ensemble",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "and end-to-end approaches.\nIn Proc. ICMI, Boulder.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Huang-Cheng Chou, Wei-Cheng Lin, Chi-Chun Lee,"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "and Carlos Busso. 2022.\nExploiting annotators’"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Alexander Amini, Wilko Schwarting, Ava Soleimany,",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "typed description of emotion perception to maximize"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "and Daniela Rus. 2020. Deep evidential regression.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "utilization of ratings for speech emotion recognition."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "In Proc. NeurIPS, Vancouver.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "In Proc. ICASSP, Singapore."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Atsushi\nAndo,\nSatoshi\nKobashikawa,\nHosana",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Ting Dang, Vidhyasaharan Sethu, and Eliathamby Am-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Kamiyama, Ryo Masumura, Yusuke Ijima, and Yushi",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "bikairajah. 2018. Dynamic multi-rater Gaussian mix-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Aono. 2018.\nSoft-target\ntraining with ambiguous",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "ture regression incorporating temporal dependencies"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "emotional utterances for DNN-based speech emotion",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "of emotion uncertainty using Kalman filters.\nIn Proc."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "classification.\nIn Proc. ICASSP, Brighton.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "ICASSP, Calgary."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Mia Atcheson, Vidhyasaharan Sethu, and Julien Epps.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Ting Dang, Vidhyasaharan Sethu,\nJulien Epps,\nand"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "2018. Demonstrating and modelling systematic time-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Eliathamby Ambikairajah. 2017. An investigation of"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "varying annotator disagreement in continuous emo-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "emotion prediction uncertainty using gaussian mix-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "tion annotation.\nIn Proc. Interspeech, Hyderabad.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "ture regression.\nIn Proc. Interspeech, Stockholm."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Mia Atcheson, Vidhyasaharan Sethu, and Julien Epps.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Aida Mostafazadeh Davani, Mark Díaz, and Vinodku-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "2019. Using Gaussian processes with LSTM neu-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "mar Prabhakaran. 2022. Dealing with disagreements:"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "ral networks to predict continuous-time, dimensional",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Looking beyond the majority vote in subjective an-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "emotion in ambiguous speech.\nIn Proc. ACII, Cam-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "notations. Transactions of the Association for Com-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "bridge.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "putational Linguistics, 10:92–110."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Bagus Tris Atmaja and Masato Akagi. 2020a.\nImprov-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "ing valence prediction in dimensional speech emotion",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Jun Deng, Wenjing Han, and Björn Schuller. 2012. Con-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "recognition using linguistic information.\nIn Proc. O-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "fidence measures for speech emotion recognition: A"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "COCOSDA, Yangon.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "ITG Sympo-\nstart.\nIn Speech Communication; 10."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "sium, pages 1–4. VDE."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Bagus Tris Atmaja and Masato Akagi. 2020b. Multi-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": ""
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "task learning and multistage fusion for dimensional",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Armen Der Kiureghian\nand Ove Ditlevsen.\n2009."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "audiovisual emotion recognition.\nIn Proc. ICASSP,",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Structural\nAleatory or epistemic? does it matter?"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Conference held virtually.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Safety, 31(2):105–112."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Bagus Tris Atmaja and Masato Akagi. 2021.\nTwo-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "H.M. Fayek, M. Lech, and L. Cavedon. 2016. Model-"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "stage dimensional emotion recognition by fusing pre-",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "ing subjectiveness in emotion recognition with deep"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "dictions of acoustic and text networks using svm.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "neural networks: Ensembles vs soft labels.\nIn Proc."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Speech Communication, 126:9–21.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "IJCNN, Vancouver."
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "Yarin Gal and Zoubin Ghahramani. 2016.\nDropout"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "and Michael Auli. 2020. Wav2Vec 2.0: A framework",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "as a bayesian approximation: Representing model"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "for self-supervised learning of speech representations.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "uncertainty in deep learning.\nIn Proc. ICML, New"
        },
        {
          "the Cambridge Tier-2 system operated by the Uni-": "In Proc. NeurIPS, Conference held virtually.",
          "Charles\nBlundell,\nJulien\nCornebise,\nKoray": "York City."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Shriberg, and Chao Wang. 2022. Sentiment-aware",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "tic emotionally balanced speech corpus by retriev-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "automatic speech recognition pre-training for en-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "ing emotional speech from existing podcast record-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "hanced\nspeech\nemotion\nrecognition.\nIn Proc.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "ings.\nIEEE Transactions on Affective Computing,"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "ICASSP, Singapore.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "10(4):471–483."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Michael Grimm and Kristian Kroschel. 2005.\nEval-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria,"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "uation of natural emotions using self assessment",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "and S. Poria. 2018. Multimodal sentiment analy-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "manikins.\nIn Proc. ASRU, Cancun.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "sis using hierarchical fusion with context modeling."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Knowledge-Based Systems, 161:124–133."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Michael Grimm, Kristian Kroschel, Emily Mower, and",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Shrikanth Narayanan. 2007. Primitives-based evalu-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Andrey Malinin and Mark Gales. 2018. Predictive un-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "ation and estimation of emotions in speech. Speech",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "certainty estimation via prior networks.\nIn Proc."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Communication, 49(10-11):787–800.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "NeurIPS, Montréal."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Konstantin Markov, Tomoko Matsui, Francois Septier,"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Hatice Gunes, Björn Schuller, Maja Pantic, and Roddy",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "and Gareth Peters. 2015. Dynamic speech emotion"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Cowie. 2011. Emotion representation, analysis and",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "recognition with state-space models.\nIn Proc. EU-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "synthesis in continuous space: A survey.\nIn Proc.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "SIPCO, Nice."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "FG, Santa Barbara.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Hermann G Matthies. 2007. Quantifying uncertainty:"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Jing Han, Zixing Zhang, Zhao Ren, and Björn Schuller.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Modern computational representation of probability"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "2021. Exploring perception uncertainty for emotion",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "and applications.\nIn Extreme man-made and natural"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "recognition in dyadic conversation and music listen-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "hazards in dynamics of structures, pages 105–135."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "ing. Cognitive Computation, 13(2):231–240.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Springer."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Jing Han, Zixing Zhang, Maximilian Schmitt, Maja",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Vikramjit Mitra, Hsiang-Yun Sherry Chien, Vasudha"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Pantic, and Björn Schuller. 2017. From hard to soft:",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Kowtha,\nJoseph Yitan Cheng,\nand Erdrin Azemi."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Towards more human-like emotion recognition by",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "2022.\nSpeech Emotion:\nInvestigating Model Rep-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "modelling the perception uncertainty.\nIn Proc. ACM",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "resentations, Multi-Task Learning and Knowledge"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "MM, Mountain View.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Distillation.\nIn Proc. Interspeech, Incheon."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Xincheng Ju, Dong Zhang, Junhui Li, and Guodong",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Mihalis A Nicolaou, Hatice Gunes, and Maja Pantic."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Zhou. 2020. Transformer-based label set generation",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "2011. Continuous prediction of spontaneous affect"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "for multi-modal multi-label emotion detection.\nIn",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "from multiple cues and modalities in valence-arousal"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Proc. ACM MM, Seattle.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "space.\nIEEE Transactions on Affective Computing,"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "2(2):92–105."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Alex Kendall and Yarin Gal. 2017. What uncertainties",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "do we need in bayesian deep learning for computer",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "vision? In Proc. NeurIPS, Long Beach.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Sanjeev Khudanpur. 2015.\nLibrispeech: An ASR"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "corpus based on public domain audio books.\nIn Proc."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "ICASSP, South Brisbane."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Shen, Maximilian Schmitt, Fabien Ringeval,\nJing",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Han, Vedhas Pandit, Antoine Toisoul, Björn Schuller,",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Robert Plutchik. 2001. The nature of emotions: Human"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "et al. 2019. SEWA DB: A rich database for audio-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "emotions have deep evolutionary roots, a fact\nthat"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "visual emotion and sentiment research in the wild.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "may explain their complexity and provide tools for"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "IEEE Transactions on Pattern Analysis and Machine",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "clinical practice. American Scientist, 89(4):344–350."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Intelligence, 43(3):1022–1040.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": ""
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Soujanya Poria, Devamanyu Hazarika, Navonil Ma-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Duc Le, Zakaria Aldeneh, and Emily Mower Provost.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "jumder, Gautam Naik, Erik Cambria, and Rada Mi-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "2017. Discretized continuous speech emotion recog-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "halcea. 2019. MELD: A multimodal multi-party"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "nition with multi-task deep recurrent neural network.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "dataset for emotion recognition in conversations.\nIn"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "In Proc. Interspeech, Stockholm.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Proc. ACL, Florence."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Seong-Gyun Leem, Daniel Fulford, Jukka-Pekka On-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Soujanya Poria, Navonil Majumder, Devamanyu Haz-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "nela, David Gard, and Carlos Busso. 2022. Not all",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "arika, Erik Cambria, Alexander Gelbukh, and Amir"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "features are equal: Selection of robust features for",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Hussain. 2018. Multimodal sentiment analysis: Ad-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "speech emotion recognition in noisy environments.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "dressing key issues\nand setting up the baselines."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "In Proc. ICASSP, Singapore.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "IEEE Intelligent Systems, 33(6):17–25."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "Filip Povolny, Pavel Matejka, Michal Hradis, Anna Pop-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "dar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis,",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "ková, Lubomír Otrusina, Pavel Smrz, Ian Wood, Ce-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "Luke Zettlemoyer,\nand Veselin\nStoyanov.\n2019.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "cile Robin, and Lori Lamel. 2016. Multimodal emo-"
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "RoBERTa: A robustly optimized BERT pretraining",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "tion recognition for avec 2016 challenge.\nIn Proc."
        },
        {
          "Ayoub Ghriss, Bo Yang, Viktor Rozgic, Elizabeth": "approach. arXiv preprint arXiv:1907.11692.",
          "R. Lotfian and C. Busso. 2019.\nBuilding naturalis-": "ACM MM, Amsterdam."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Willenbrock, and Timo Gerkmann. 2021.\nEnd-to-",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "gapore."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "end label uncertainty modeling for speech emotion",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "recognition using bayesian neural networks. arXiv",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Andreas Triantafyllopoulos, Johannes Wagner, Hagen"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "preprint arXiv:2110.03299.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Wierstorf, Maximilian Schmitt, Uwe Reichel, Florian"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Eyben, Felix Burkhardt, and Björn W. Schuller. 2022."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Mirco Ravanelli, Titouan Parcollet, Peter Plantinga,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Probing speech emotion recognition transformers for"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "linguistic knowledge.\nIn Proc. Interspeech, Incheon."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Subakan, Nauman Dawalatabad, Abdelwahab Heba,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Jianyuan Zhong,\nJu-Chieh Chou, Sung-Lin Yeh,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "George Trigeorgis, Fabien Ringeval, Raymond Brueck-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Szu-Wei Fu, Chien-Feng Liao, Elena Rastorgueva,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "ner, Erik Marchi, Mihalis A Nicolaou, Björn Schuller,"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "François Grondin, William Aris, Hwidong Na, Yan",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "and Stefanos Zafeiriou. 2016. Adieu features? end-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Gao, Renato De Mori, and Yoshua Bengio. 2021.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "to-end speech emotion recognition using a deep"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "SpeechBrain:\nA general-purpose\nspeech\ntoolkit.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "convolutional recurrent network.\nIn Proc. ICASSP,"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "ArXiv:2106.04624.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Shanghai."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Fabien Ringeval, Björn Schuller, Michel Valstar, Roddy",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Cowie, and Maja Pantic. 2015. AVEC 2015: The",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu,"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "5th international audio/visual emotion challenge and",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "workshop.\nIn Proc. ACM MM, Brisbane.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li,"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Fabien Ringeval,\nBjörn\nSchuller, Michel Valstar,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Shinji Watanabe, Abdelrahman Mohamed, and Hung"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Jonathan Gratch, Roddy Cowie,\nStefan Scherer,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "yi Lee. 2021. SUPERB: Speech processing universal"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Sharon Mozgai, Nicholas Cummins, Maximilian",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "performance benchmark.\nIn Proc. Interspeech, Brno."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Schmitt, and Maja Pantic. 2017. AVEC 2017: Real-",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "life depression, and affect recognition workshop and",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Jingyao Wu, Ting Dang, Vidhyasaharan Sethu,\nand"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "challenge.\nIn Proc. ACM MM, Mountain View.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Eliathamby Ambikairajah. 2022a. A novel sequential"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Fabien Ringeval, Andreas Sonderegger, Jürgen Sauer,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Monte Carlo framework for predicting ambiguous"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "and Denis Lalanne. 2013.\nIntroducing the RECOLA",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "emotion states.\nIn Proc. ICASSP, Singapore."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "multimodal corpus of remote collaborative and affec-",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "tive interactions.\nIn Proc. FG, Shanghai.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Wen Wu, Chao Zhang, and Philip C. Woodland. 2021."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Emotion recognition by fusing time synchronous and"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "James A Russell. 1980.\nA circumplex model of af-",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "time asynchronous representations.\nIn Proc. ICASSP,"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "fect. Journal of Personality and Social Psychology,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Toronto."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "39(6):1161.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Wen Wu, Chao Zhang, Xixin Wu, and Philip C. Wood-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "James A Russell and Albert Mehrabian. 1977. Evidence",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "land. 2022b. Estimating the uncertainty in emotion"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Journal of\nfor a three-factor\ntheory of emotions.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "class labels with utterance-specific dirichlet priors."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Research in Personality, 11(3):273–294.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "IEEE Transactions on Affective Computing, Early"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "access."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Harold Schlosberg. 1954. Three dimensions of emotion.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Psychological Review, 61(2):81.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Dong Zhang, Xincheng Ju,\nJunhui Li, Shoushan Li,"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Qiaoming Zhu, and Guodong Zhou. 2020. Multi-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Murat Sensoy, Lance Kaplan,\nand Melih Kandemir.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "modal multi-label emotion detection with modality"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "2018.\nEvidential deep learning to quantify classi-",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "and label dependence.\nIn Proc. EMNLP, Conference"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "fication uncertainty.\nIn Proc. NeurIPS, Montréal.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "held virtually."
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Kusha Sridhar and Carlos Busso. 2020a. Ensemble of",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "students taught by probabilistic teachers to improve",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "A\nDerivation of the predictive posterior"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "speech emotion recognition.\nIn Proc. Interspeech,",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Shanghai.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "Since NIG is the Gaussian conjugate prior,"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Kusha Sridhar and Carlos Busso. 2020b. Modeling",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "uncertainty in predicting emotional attributes from",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "p(Ψ|Ω) = N (γ, σ2υ−1) Γ−1(α, β)"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "spontaneous speech.\nIn Proc. ICASSP, Conference",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "(cid:19)α+1\nβα√\n(cid:18) 1\nυ"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "held virtually.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "√\n="
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "σ2\nΓ(α)\n2πσ2"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Kusha Sridhar, Wei-Cheng Lin, and Carlos Busso. 2021.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "(cid:26)\n(cid:27)"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "2β + υ(γ − µ)2"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Generative approach using soft-labels to learn un-",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "· exp\n−"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "certainty in predicting emotional attributes.\nIn Proc.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "2σ2"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "ACII, Chicago.",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "its posterior p(Ψ|D) is in the same parametric fam-"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Sundararajan Srinivasan, Zhaocheng Huang, and Katrin",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "ily as the prior p(Ψ|Ω). Therefore, given a test"
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "Kirchhoff. 2022. Representation learning through",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": ""
        },
        {
          "Navin Raj Prabhu, Guillaume Carbajal, Nale Lehmann-": "cross-modal conditional teacher-student training for",
          "speech emotion recognition.\nIn Proc. ICASSP, Sin-": "utterance x∗, the predictive posterior p(y∗|D) has"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 5: Incorporating text",
      "data": [
        {
          "Transformer": "Frames",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "encoder block 1",
          "Modality\nv\na\nd": "IEMOCAP"
        },
        {
          "Transformer": "…\n…\n…",
          "Modality\nv\na\nd": "A\n0.596\n0.756\n0.569"
        },
        {
          "Transformer": "…",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "…",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "𝑤𝑤1",
          "Modality\nv\na\nd": "A+T\n0.609\n0.754\n0.575"
        },
        {
          "Transformer": "…\nTransformer",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "encoder block 12\nAudio \nText",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "𝑤𝑤12\n…",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "Transformer\nTransformer",
          "Modality\nv\na\nd": "Table 5: CCC value for bi-modal experiments. ‘A’ and"
        },
        {
          "Transformer": "Upstream model",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "‘T’ stands for audio and text. ‘v’, ‘a’, and ‘d’ stand for"
        },
        {
          "Transformer": "Downstream",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "Evidential layer",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "model",
          "Modality\nv\na\nd": "valence, arousal, and dominance. Release 1.8 is used"
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "for MSP-Podcast.\n‘Ses05’ setup used for IEMOCAP"
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "that trains on Session 1-4 and tests on Session 5."
        },
        {
          "Transformer": "𝛾𝛾v\n𝜐𝜐v 𝛼𝛼v 𝛽𝛽v",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "𝛾𝛾d\n𝛾𝛾a\n𝜐𝜐d 𝛼𝛼d 𝛽𝛽d\n𝜐𝜐a 𝛼𝛼a\n𝛾𝛾v\n𝛽𝛽a\n𝜐𝜐v 𝛼𝛼v 𝛽𝛽v"
        },
        {
          "Transformer": "𝛾𝛾a\n𝜐𝜐a 𝛼𝛼a\n𝛽𝛽a",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "𝛾𝛾d\n𝜐𝜐d 𝛼𝛼d 𝛽𝛽d",
          "Modality\nv\na\nd": "𝛾𝛾d\n𝛾𝛾a\n𝜐𝜐d 𝛼𝛼d 𝛽𝛽d\n𝜐𝜐a 𝛼𝛼a\n𝛾𝛾v\n𝛽𝛽a\n𝜐𝜐v 𝛼𝛼v 𝛽𝛽v"
        },
        {
          "Transformer": "Figure 4: Model structure for bi-modal experiments.",
          "Modality\nv\na\nd": "information improves the estimation of valence but"
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "not necessarily for arousal and dominance. Similar"
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "phenomena were observed by (Triantafyllopoulos"
        },
        {
          "Transformer": "the same form as the marginal likelihood p(y|Ω),",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "et al., 2022). A possible explanation is that\ntext"
        },
        {
          "Transformer": "where D denotes the training set.",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "is effective for sentiment analysis (positive or neg-"
        },
        {
          "Transformer": "(cid:90)",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "ative) but may not be as informative as audio to"
        },
        {
          "Transformer": "(7)\np(y∗|Ψ)p(Ψ|D) dΨ\np(y∗|D) =",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "determine a speaker’s level of excitement. CCC"
        },
        {
          "Transformer": "(cid:90)",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "for dominance improves more for IEMOCAP than"
        },
        {
          "Transformer": "p(y|Ω) =\np(y|Ψ)p(Ψ|Ω) dΨ\n(8)",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "MSP-Podcast possibly because IEMOCAP is an"
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "acted dataset and the emotion may be exaggerated"
        },
        {
          "Transformer": "In DEER, the predictive posterior and posterior",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "compared with MSP-Podcast which contains natu-"
        },
        {
          "Transformer": "are both conditioned on Ω, written as p(y∗|D, Ω)",
          "Modality\nv\na\nd": ""
        },
        {
          "Transformer": "",
          "Modality\nv\na\nd": "ral emotion."
        },
        {
          "Transformer": "and p(Ψ|D, Ω) to be precise. Also,\nthe informa-",
          "Modality\nv\na\nd": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Joint discrete and continuous emotion prediction using ensemble and end-to-end approaches",
      "authors": [
        "A Ehab",
        "Yelin Albadawy",
        "Kim"
      ],
      "year": "2018",
      "venue": "Proc. ICMI"
    },
    {
      "citation_id": "2",
      "title": "Deep evidential regression",
      "authors": [
        "Alexander Amini",
        "Wilko Schwarting",
        "Ava Soleimany",
        "Daniela Rus"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "3",
      "title": "Soft-target training with ambiguous emotional utterances for DNN-based speech emotion classification",
      "authors": [
        "Atsushi Ando",
        "Satoshi Kobashikawa",
        "Hosana Kamiyama",
        "Ryo Masumura"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Demonstrating and modelling systematic timevarying annotator disagreement in continuous emotion annotation",
      "authors": [
        "Mia Atcheson",
        "Vidhyasaharan Sethu",
        "Julien Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Using Gaussian processes with LSTM neural networks to predict continuous-time, dimensional emotion in ambiguous speech",
      "authors": [
        "Mia Atcheson",
        "Vidhyasaharan Sethu",
        "Julien Epps"
      ],
      "year": "2019",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "6",
      "title": "Improving valence prediction in dimensional speech emotion recognition using linguistic information",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2020",
      "venue": "Proc. O-COCOSDA"
    },
    {
      "citation_id": "7",
      "title": "Multitask learning and multistage fusion for dimensional audiovisual emotion recognition",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP, Conference held virtually"
    },
    {
      "citation_id": "8",
      "title": "Twostage dimensional emotion recognition by fusing predictions of acoustic and text networks using svm",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2021",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "9",
      "title": "Wav2Vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS, Conference held virtually. Charles Blundell"
    },
    {
      "citation_id": "10",
      "title": "Weight uncertainty in neural network",
      "authors": [
        "Daan Kavukcuoglu",
        "Wierstra"
      ],
      "year": "2015",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "11",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "12",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Exploiting annotators' typed description of emotion perception to maximize utilization of ratings for speech emotion recognition",
      "authors": [
        "Huang-Cheng Chou",
        "Wei-Cheng Lin",
        "Chi-Chun Lee",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Dynamic multi-rater Gaussian mixture regression incorporating temporal dependencies of emotion uncertainty using Kalman filters",
      "authors": [
        "Ting Dang",
        "Vidhyasaharan Sethu",
        "Eliathamby Ambikairajah"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "16",
      "title": "An investigation of emotion prediction uncertainty using gaussian mixture regression",
      "authors": [
        "Ting Dang",
        "Vidhyasaharan Sethu",
        "Julien Epps",
        "Eliathamby Ambikairajah"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations",
      "authors": [
        "Aida Mostafazadeh Davani",
        "Mark Díaz",
        "Vinodku"
      ],
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "18",
      "title": "Confidence measures for speech emotion recognition: A start",
      "authors": [
        "Jun Deng",
        "Wenjing Han",
        "Björn Schuller"
      ],
      "year": "2012",
      "venue": "Speech Communication; 10. ITG Symposium"
    },
    {
      "citation_id": "19",
      "title": "Aleatory or epistemic? does it matter? Structural Safety",
      "authors": [
        "Armen Der",
        "Ove Ditlevsen"
      ],
      "year": "2009",
      "venue": "Aleatory or epistemic? does it matter? Structural Safety"
    },
    {
      "citation_id": "20",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2016",
      "venue": "Proc. IJCNN"
    },
    {
      "citation_id": "21",
      "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "authors": [
        "Yarin Gal",
        "Zoubin Ghahramani"
      ],
      "year": "2016",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "22",
      "title": "Sentiment-aware automatic speech recognition pre-training for enhanced speech emotion recognition",
      "authors": [
        "Ayoub Ghriss",
        "Bo Yang",
        "Viktor Rozgic",
        "Elizabeth Shriberg",
        "Chao Wang"
      ],
      "year": "2022",
      "venue": "Proc"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Singapore Icassp"
      ],
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "Evaluation of natural emotions using self assessment manikins",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel"
      ],
      "year": "2005",
      "venue": "Proc. ASRU"
    },
    {
      "citation_id": "25",
      "title": "Primitives-based evaluation and estimation of emotions in speech",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel",
        "Emily Mower",
        "Shrikanth Narayanan"
      ],
      "year": "2007",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "26",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "Hatice Gunes",
        "Björn Schuller",
        "Maja Pantic",
        "Roddy Cowie"
      ],
      "year": "2011",
      "venue": "Proc. FG"
    },
    {
      "citation_id": "27",
      "title": "Exploring perception uncertainty for emotion recognition in dyadic conversation and music listening",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Zhao Ren",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "28",
      "title": "From hard to soft: Towards more human-like emotion recognition by modelling the perception uncertainty",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Maximilian Schmitt",
        "Maja Pantic",
        "Björn Schuller"
      ],
      "venue": "Proc. ACM MM"
    },
    {
      "citation_id": "29",
      "title": "Transformer-based label set generation for multi-modal multi-label emotion detection",
      "authors": [
        "Xincheng Ju",
        "Dong Zhang",
        "Junhui Li",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proc. ACM MM"
    },
    {
      "citation_id": "30",
      "title": "What uncertainties do we need in bayesian deep learning for computer vision?",
      "authors": [
        "Alex Kendall",
        "Yarin Gal"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "31",
      "title": "SEWA DB: A rich database for audiovisual emotion and sentiment research in the wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Discretized continuous speech emotion recognition with multi-task deep recurrent neural network",
      "authors": [
        "Duc Le",
        "Zakaria Aldeneh",
        "Emily Provost"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "33",
      "title": "Not all features are equal: Selection of robust features for speech emotion recognition in noisy environments",
      "authors": [
        "Seong-Gyun Leem",
        "Daniel Fulford",
        "Jukka-Pekka Onnela",
        "David Gard",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "34",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "35",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "37",
      "title": "Predictive uncertainty estimation via prior networks",
      "authors": [
        "Andrey Malinin",
        "Mark Gales"
      ],
      "year": "2018",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "38",
      "title": "Dynamic speech emotion recognition with state-space models",
      "authors": [
        "Konstantin Markov",
        "Tomoko Matsui",
        "Francois Septier",
        "Gareth Peters"
      ],
      "year": "2015",
      "venue": "Proc. EU-SIPCO"
    },
    {
      "citation_id": "39",
      "title": "Quantifying uncertainty: Modern computational representation of probability and applications",
      "authors": [
        "G Hermann",
        "Matthies"
      ],
      "year": "2007",
      "venue": "Extreme man-made and natural hazards in dynamics of structures"
    },
    {
      "citation_id": "40",
      "title": "Multi-Task Learning and Knowledge Distillation",
      "authors": [
        "Mitra Vikramjit",
        "Vasudha Hsiang-Yun Sherry Chien",
        "Joseph Kowtha",
        "Erdrin Cheng",
        "Azemi"
      ],
      "year": "2022",
      "venue": "Speech Emotion: Investigating Model Representations"
    },
    {
      "citation_id": "41",
      "title": "Continuous prediction of spontaneous affect from multiple cues and modalities in valence-arousal space",
      "authors": [
        "A Mihalis",
        "Hatice Nicolaou",
        "Maja Gunes",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "43",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2001",
      "venue": "American Scientist"
    },
    {
      "citation_id": "44",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "45",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Erik Cambria",
        "Alexander Gelbukh",
        "Amir Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "46",
      "title": "Multimodal emotion recognition for avec 2016 challenge",
      "authors": [
        "Filip Povolny",
        "Pavel Matejka",
        "Michal Hradis",
        "Anna Popková",
        "Lubomír Otrusina",
        "Pavel Smrz",
        "Ian Wood"
      ],
      "year": "2016",
      "venue": "Proc. ACM MM"
    },
    {
      "citation_id": "47",
      "title": "End-toend label uncertainty modeling for speech emotion recognition using bayesian neural networks",
      "authors": [
        "Raj Navin",
        "Guillaume Prabhu",
        "Nale Carbajal",
        "Timo Lehmann-Willenbrock",
        "Gerkmann"
      ],
      "year": "2021",
      "venue": "End-toend label uncertainty modeling for speech emotion recognition using bayesian neural networks",
      "arxiv": "arXiv:2110.03299"
    },
    {
      "citation_id": "48",
      "title": "SpeechBrain: A general-purpose speech toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong",
        "Ju-Chieh Chou",
        "Sung-Lin Yeh",
        "Szu-Wei Fu",
        "Chien-Feng Liao",
        "Elena Rastorgueva",
        "François Grondin",
        "William Aris",
        "Hwidong Na",
        "Yan Gao",
        "Renato Mori",
        "Yoshua Bengio"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A general-purpose speech toolkit"
    },
    {
      "citation_id": "49",
      "title": "AVEC 2015: The 5th international audio/visual emotion challenge and workshop",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2015",
      "venue": "Proc. ACM MM"
    },
    {
      "citation_id": "50",
      "title": "AVEC 2017: Reallife depression, and affect recognition workshop and challenge",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Jonathan Gratch",
        "Roddy Cowie",
        "Stefan Scherer",
        "Sharon Mozgai",
        "Nicholas Cummins",
        "Maximilian Schmitt",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Proc. ACM MM"
    },
    {
      "citation_id": "51",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Jürgen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "Proc. FG"
    },
    {
      "citation_id": "52",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "53",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "A James",
        "Albert Russell",
        "Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "54",
      "title": "Three dimensions of emotion",
      "authors": [
        "Harold Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "55",
      "title": "Evidential deep learning to quantify classification uncertainty",
      "authors": [
        "Murat Sensoy",
        "Lance Kaplan",
        "Melih Kandemir"
      ],
      "year": "2018",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "56",
      "title": "Ensemble of students taught by probabilistic teachers to improve speech emotion recognition",
      "authors": [
        "Kusha Sridhar",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "57",
      "title": "Modeling uncertainty in predicting emotional attributes from spontaneous speech",
      "authors": [
        "Kusha Sridhar",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP, Conference held virtually"
    },
    {
      "citation_id": "58",
      "title": "Generative approach using soft-labels to learn uncertainty in predicting emotional attributes",
      "authors": [
        "Kusha Sridhar",
        "Wei-Cheng Lin",
        "Carlos Busso"
      ],
      "year": "2021",
      "venue": "Proc. ACII"
    },
    {
      "citation_id": "59",
      "title": "Representation learning through cross-modal conditional teacher-student training for speech emotion recognition",
      "authors": [
        "Sundararajan Srinivasan",
        "Zhaocheng Huang",
        "Katrin Kirchhoff"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP, Singapore"
    },
    {
      "citation_id": "60",
      "title": "Probing speech emotion recognition transformers for linguistic knowledge",
      "authors": [
        "Andreas Triantafyllopoulos",
        "Johannes Wagner",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Uwe Reichel",
        "Florian Eyben",
        "Felix Burkhardt",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "61",
      "title": "Adieu features? endto-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "George Trigeorgis",
        "Fabien Ringeval",
        "Raymond Brueckner",
        "Erik Marchi",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "62",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "Yang Shu Wen",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Yist Lin",
        "Andy Liu",
        "Jiatong Shi",
        "Xuankai Chang",
        "Guan-Ting Lin",
        "Tzu-Hsien Huang",
        "Wei-Cheng Tseng",
        "Da-Rong Ko Tik Lee",
        "Zili Liu",
        "Shuyan Huang",
        "Shang-Wen Dong",
        "Shinji Li",
        "Abdelrahman Watanabe",
        "Hung Mohamed",
        "Lee Yi"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "63",
      "title": "Vidhyasaharan Sethu, and Eliathamby Ambikairajah. 2022a. A novel sequential Monte Carlo framework for predicting ambiguous emotion states",
      "authors": [
        "Jingyao Wu",
        "Ting Dang"
      ],
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "64",
      "title": "Emotion recognition by fusing time synchronous and time asynchronous representations",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "65",
      "title": "Estimating the uncertainty in emotion class labels with utterance-specific dirichlet priors",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Xixin Wu",
        "Philip Woodland"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "66",
      "title": "Multimodal multi-label emotion detection with modality and label dependence",
      "authors": [
        "Dong Zhang",
        "Xincheng Ju",
        "Junhui Li",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2020",
      "venue": "Proc. EMNLP, Conference held virtually"
    }
  ]
}