{
  "paper_id": "2112.00702v2",
  "title": "Semi-Supervised Music Emotion Recognition Using Noisy Student Training And Harmonic Pitch Class Profiles",
  "published": "2021-12-01T18:25:51Z",
  "authors": [
    "Hao Hao Tan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present Mirable's submission to the 2021 Emotions and Themes in Music challenge. In this work, we intend to address the question: can we leverage semi-supervised learning techniques on music emotion recognition? With that, we experiment with noisy student training, which has improved model performance in the image classification domain. As the noisy student method requires a strong teacher model, we further delve into the factors including (i) input training length and (ii) complementary music representations to further boost the performance of the teacher model. For (i), we find that models trained with short input length perform better in PR-AUC, whereas those trained with long input length perform better in ROC-AUC. For (ii), we find that using harmonic pitch class profiles (HPCP) consistently improve tagging performance, which suggests that harmonic representation is useful for music emotion tagging. Finally, we find that noisy student method only improves tagging results for the case of long training length. Additionally, we find that ensembling representations trained with different training lengths can improve tagging results significantly, which suggest a possible direction to explore incorporating multiple temporal resolutions in the network architecture for future work. \n APPROACH 2.1 Pre-Processing and Augmentation We extract Mel-spectrograms with 128 bins from raw audio using a sampling rate of 44.1kHz, and the Mel-spectrograms are downsampled with an averaging factor of 10 along the temporal dimension. The number of time steps for each Mel-spectrogram vary",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions and themes are high-level musical attributes that are abstract and highly subjective. Obtaining emotion labels typically require human annotation, which can be time consuming and potentially costly. Is it possible to use semi-supervised learning techniques, such that we can leverage on unlabelled music tracks to learn emotion tags, while only using a small amount of labelled data? Following this question, we intend to explore the usage of noisy student training  [9]  on music emotion recognition. Recently,  [8]  proposed the music tagging transformer, which also uses noisy student training, but it is applied to general music tagging and does not focus on emotion and theme related tags. Additionally, we explore two other factors to improve the tagging performance of the teacher model: (i) the input training length; (ii) adding music representations to complement the learning of music emotion. according to the training strategy, which will be discussed in Section 2.3. For data augmentation, we perform time masking and frequency masking, similar to the idea in SpecAugment  [5] . The maximum possible length of both masks vary between 20 to 60, and the value is being sampled randomly for each training batch.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Training",
      "text": "As shown in Figure  1 , our base model architecture is similar to CRNN  [2] , with some revisions which include adding residual connections to our ConvBlock, and using GeMPool  [6]  instead of Max-Pool. We train all of our models for a maximum of 100 epochs, with an Adam optimizer and learning rate of 0.0001. Early stopping is performed when the validation ROC-AUC does not improve for 5 epochs, and we store the model weights from the epoch with the best ROC-AUC evaluated on the validation set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Long Vs Short Training Length",
      "text": "For the long training length mode, we use the first â‰ˆ 185 seconds of the track, which corresponds to 1600 time steps in the Melspectrogram after average pooling. For the short training length mode, we chunk each track into samples of length â‰ˆ 9.25 seconds, which corresponds to 80 time steps in the Mel-spectrogram after average pooling. During evaluation, we average the logits of all chunks to obtain the final output for each track.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Harmonic Pitch Class Profiles (Hpcp)",
      "text": "HPCP  [3]  is a type of chroma feature that describes tonality and harmonic content of a music track. We extract HPCP with 12 pitch classes from raw audio using a sampling rate of 44.1kHz. We do not apply average pooling along the temporal dimension of HPCP. The corresponding number of time steps for HPCP are 4000 and 200 for both long and short training length mode respectively. We concatenate the learnt latent features from the Mel-spectrogram and the HPCP block, each with dimension ð‘‘ = 256, and pass through two linear layers to obtain the fused output.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Noisy Student Training",
      "text": "Noisy student training  [9]  is an extension of self-training, with the usage of equal-or-larger student models and added noise to improve the representation learnt from the teacher model. To add noise, we enhance data augmentation by increasing the maximum possible masking length to between 30 and 90 for both time and frequency masking, as well as adding standard Gaussian noise with a weight of 0.01. To implement stochastic depth  [4] , we use 3 StochasticConvBlocks which are ConvBlocks that could be randomly bypassed with a probability of 0.1 each. During evaluation, all the layers will be passed through. StochasticConvBlock also has an additional dropout of probability 0.1 after the ReLU layer.\n\nIn this work, we use the corresponding HPCP models for each long and short training length mode as the teacher model. We only use the predictions which are > 0.1 as positive pseudo-labels, and those < 1ð‘’ -6 as negative pseudo-labels. Both decision thresholds are determined by conducting an empirical evaluation on the predicted value distribution using the teacher model, carried out on the training and validation set. We take the leftmost 5% percentile for the negative label distribution, and the rightmost 5% percentile for the positive label distribution to ensure better confidence.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Model Ensemble",
      "text": "Finally, we investigate the results of combining the output of both long and short training length models, by simply taking the weighted sum of their best models: ð‘™ ð‘“ ð‘–ð‘›ð‘Žð‘™ = ð›¼ â€¢ ð‘™ ð‘ â„Žð‘œð‘Ÿð‘¡ + (1 -ð›¼) â€¢ ð‘™ ð‘™ð‘œð‘›ð‘” . We use the validation set to find the ratio ð›¼ which gives the best results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results And Analysis",
      "text": "For the training length factor, we find that models trained with long input length perform better in ROC-AUC, but models trained with short input length perform significantly better in PR-AUC. According to Table  2 , this is because the former has a higher TNR, while the latter has a higher TPR. Since PR-AUC focuses more on the minority class (in this case the positive class) and ROC-AUC focuses on both, the latter model scores better in PR-AUC. We also find that adding HPCP improves tagging results consistently for both cases, which suggests that harmonic representation is important for music emotion recognition.\n\nFor noisy student training, the results are rather inconclusive. We find slight improvements in the long training length case, but the result degrades for the short training length case. Also, we only run noisy student training for 1 iteration, as we find the results consistently degrade for subsequent iterations. Additionally, we try to add more unlabelled tracks from the Lakh MP3 dataset (â‰ˆ 45, 000 30 seconds track) to increase the training dataset size, but we do not observe any performance improvement. We infer that noisy student method might not necessarily work well for music emotion recognition tasks, due to the abstract nature and subjectivity of emotion and theme labels. Hence, a small subset of emotion labels might not be sufficient to represent the full dataset.\n\nFor model ensembling, we choose to ensemble the 'long-noisy' model and the 'short-normal' model. We find that ð›¼ = 0.7 is optimal through our validation set, hence suggesting that the final output gives more weightage to the short training length model. From the test set results, we can also see that this ensemble method improves the tagging performance significantly, which suggest that combining different views of audio in terms of temporal resolution can produce better learnt representations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Discussion And Outlook",
      "text": "While investigating the related work, we find that this work still uses a relatively long training length (even for short length we use â‰ˆ 9 seconds, as compared to previous works with â‰ˆ 2 to 5 seconds), and low temporal resolution, which we intend to change in our future work. For future work, we are interested in tweaking the network architecture to capture views of different temporal resolutions in the audio sample. We would also like to explore using noisy student training with different model architectures and datasets of a much larger scale.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , our base model architecture is similar to",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of our model.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hao Hao Tan": "helloharry66@gmail.com"
        },
        {
          "Hao Hao Tan": "ABSTRACT"
        },
        {
          "Hao Hao Tan": "We present Mirableâ€™s submission to the 2021 Emotions and Themes"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "in Music challenge. In this work, we intend to address the question:"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "can we leverage semi-supervised learning techniques on music"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "emotion recognition? With that, we experiment with noisy student"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "training, which has improved model performance in the image"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "classification domain. As the noisy student method requires a strong"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "teacher model, we further delve into the factors including (i) input"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "training length and (ii) complementary music representations to"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "further boost\nthe performance of\nthe teacher model. For (i), we"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "find that models trained with short input length perform better in"
        },
        {
          "Hao Hao Tan": "PR-AUC, whereas those trained with long input length perform"
        },
        {
          "Hao Hao Tan": "better in ROC-AUC. For (ii), we find that using harmonic pitch class"
        },
        {
          "Hao Hao Tan": "profiles (HPCP) consistently improve tagging performance, which"
        },
        {
          "Hao Hao Tan": "suggests that harmonic representation is useful for music emotion"
        },
        {
          "Hao Hao Tan": "tagging. Finally, we find that noisy student method only improves"
        },
        {
          "Hao Hao Tan": "tagging results for the case of long training length. Additionally, we"
        },
        {
          "Hao Hao Tan": "find that ensembling representations trained with different training"
        },
        {
          "Hao Hao Tan": "lengths can improve tagging results significantly, which suggest"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "a possible direction to explore incorporating multiple temporal"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "resolutions in the network architecture for future work."
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "1\nINTRODUCTION"
        },
        {
          "Hao Hao Tan": "Emotions and themes are high-level musical attributes that are"
        },
        {
          "Hao Hao Tan": "abstract and highly subjective. Obtaining emotion labels typically"
        },
        {
          "Hao Hao Tan": "require human annotation, which can be time consuming and po-"
        },
        {
          "Hao Hao Tan": "tentially costly. Is it possible to use semi-supervised learning tech-"
        },
        {
          "Hao Hao Tan": "niques, such that we can leverage on unlabelled music tracks to"
        },
        {
          "Hao Hao Tan": "learn emotion tags, while only using a small amount of labelled"
        },
        {
          "Hao Hao Tan": "data? Following this question, we intend to explore the usage of"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "noisy student training [9] on music emotion recognition. Recently,"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "[8] proposed the music tagging transformer, which also uses noisy"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "student\ntraining, but\nit\nis applied to general music tagging and"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "does not focus on emotion and theme related tags. Additionally, we"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "explore two other factors to improve the tagging performance of"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "the teacher model: (i) the input training length; (ii) adding music"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "representations to complement the learning of music emotion."
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "2\nAPPROACH"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "2.1\nPre-Processing and Augmentation"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "We extract Mel-spectrograms with 128 bins from raw audio using"
        },
        {
          "Hao Hao Tan": "a sampling rate of 44.1kHz, and the Mel-spectrograms are down-"
        },
        {
          "Hao Hao Tan": "sampled with an averaging factor of 10 along the temporal dimen-"
        },
        {
          "Hao Hao Tan": "sion. The number of\ntime steps for each Mel-spectrogram vary"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "Copyright 2021 for this paper by its authors. Use permitted under Creative Commons"
        },
        {
          "Hao Hao Tan": ""
        },
        {
          "Hao Hao Tan": "License Attribution 4.0 International (CC BY 4.0)."
        },
        {
          "Hao Hao Tan": "MediaEvalâ€™21, December 13-15 2021, Online"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Overview of our model.": "Model\nAvg TPR\nAvg TNR"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "0.8851\nlong-hpcp-noisy\n0.3645"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "short-normal\n0.3842\n0.8737"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "0.4099\nensemble\n0.8671"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "Table 2: Average true positive rate (TPR) and true negative"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "rate (TNR) for each model across all labels."
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "2.5\nNoisy Student Training"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "Noisy student\ntraining [9] is an extension of self-training, with"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "the usage of equal-or-larger student models and added noise to"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "improve the representation learnt from the teacher model. To add"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "noise, we enhance data augmentation by increasing the maximum"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "possible masking length to between 30 and 90 for both time and"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "frequency masking, as well as adding standard Gaussian noise"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "with a weight of 0.01. To implement stochastic depth [4], we use"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "3 StochasticConvBlocks which are ConvBlocks that could be ran-"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "domly bypassed with a probability of 0.1 each. During evaluation,"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "all the layers will be passed through. StochasticConvBlock also has"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "an additional dropout of probability 0.1 after the ReLU layer."
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "In this work, we use the corresponding HPCP models for each"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "long and short training length mode as the teacher model. We only"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "use the predictions which are > 0.1 as positive pseudo-labels, and"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "those < 1ð‘’âˆ’6 as negative pseudo-labels. Both decision thresholds"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "are determined by conducting an empirical evaluation on the pre-"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "dicted value distribution using the teacher model, carried out on"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "the training and validation set. We take the leftmost 5% percentile"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "for the negative label distribution, and the rightmost 5% percentile"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "for the positive label distribution to ensure better confidence."
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "2.6\nModel Ensemble"
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "Finally, we investigate the results of combining the output of both"
        },
        {
          "Figure 1: Overview of our model.": "long and short training length models, by simply taking the weighted"
        },
        {
          "Figure 1: Overview of our model.": "sum of their best models: ð‘™ð‘“ ð‘–ð‘›ð‘Žð‘™ = ð›¼ Â· ð‘™ð‘ â„Žð‘œð‘Ÿð‘¡ + (1 âˆ’ ð›¼) Â· ð‘™ð‘™ð‘œð‘›ð‘”. We use"
        },
        {
          "Figure 1: Overview of our model.": "the validation set to find the ratio ð›¼ which gives the best results."
        },
        {
          "Figure 1: Overview of our model.": ""
        },
        {
          "Figure 1: Overview of our model.": "3\nRESULTS AND ANALYSIS"
        },
        {
          "Figure 1: Overview of our model.": "For the training length factor, we find that models trained with"
        },
        {
          "Figure 1: Overview of our model.": "long input length perform better in ROC-AUC, but models trained"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotions and Themes in Music": "REFERENCES"
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": "[3]"
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": "[6]"
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": "[7]"
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        },
        {
          "Emotions and Themes in Music": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The MTG-Jamendo Dataset for Automatic Music Tagging",
      "authors": [
        "Dmitry Bogdanov",
        "Minz Won",
        "Philip Tovstogan",
        "Alastair Porter",
        "Xavier Serra"
      ],
      "year": "2019",
      "venue": "Machine Learning for Music Discovery Workshop, International Conference on Machine Learning"
    },
    {
      "citation_id": "2",
      "title": "Convolutional recurrent neural networks for music classification",
      "authors": [
        "Keunwoo Choi",
        "GyÃ¶rgy Fazekas",
        "Mark Sandler",
        "Kyunghyun Cho"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "3",
      "title": "Tonal description of polyphonic audio for music content processing",
      "authors": [
        "Emilia GÃ³mez"
      ],
      "year": "2006",
      "venue": "INFORMS Journal on Computing"
    },
    {
      "citation_id": "4",
      "title": "Deep networks with stochastic depth",
      "authors": [
        "Gao Huang",
        "Yu Sun",
        "Zhuang Liu",
        "Daniel Sedra",
        "Kilian Weinberger"
      ],
      "year": "2016",
      "venue": "Deep networks with stochastic depth"
    },
    {
      "citation_id": "5",
      "title": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "William Daniel S Park",
        "Yu Chan",
        "Chung-Cheng Zhang",
        "Barret Chiu",
        "Ekin Zoph",
        "Quoc V Cubuk",
        "Le"
      ],
      "year": "2019",
      "venue": "Specaugment: A simple data augmentation method for automatic speech recognition",
      "arxiv": "arXiv:1904.08779"
    },
    {
      "citation_id": "6",
      "title": "Fine-tuning CNN image retrieval with no human annotation",
      "authors": [
        "Filip RadenoviÄ‡",
        "Giorgos Tolias",
        "OndÅ™ej Chum"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "7",
      "title": "Media-Eval 2021: Emotion and Theme Recognition in Music Using Jamendo",
      "authors": [
        "Philip Tovstogan",
        "Dmitry Bogdanov",
        "Alastair Porter"
      ],
      "year": "2021",
      "venue": "Proc. of the MediaEval 2021 Workshop"
    },
    {
      "citation_id": "8",
      "title": "Semi-supervised music tagging transformer",
      "authors": [
        "Minz Won",
        "Keunwoo Choi",
        "Xavier Serra"
      ],
      "year": "2021",
      "venue": "Proc. of International Society for Music Information Retrieval"
    },
    {
      "citation_id": "9",
      "title": "Self-training with noisy student improves imagenet classification",
      "authors": [
        "Qizhe Xie",
        "Minh-Thang Luong",
        "Eduard Hovy",
        "Quoc V Le"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}