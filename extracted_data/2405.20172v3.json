{
  "paper_id": "2405.20172v3",
  "title": "Iterative Feature Boosting For Explainable Speech Emotion Recognition",
  "published": "2024-05-30T15:44:27Z",
  "authors": [
    "Alaa Nfissi",
    "Wassim Bouachir",
    "Nizar Bouguila",
    "Brian Mishara"
  ],
  "keywords": [
    "Feature selection",
    "Supervised learning",
    "Speech emotion recognition",
    "Acoustic analysis",
    "Explainable AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In speech emotion recognition (SER), using predefined features without considering their practical importance may lead to high dimensional datasets, including redundant and irrelevant information. Consequently, high-dimensional learning often results in decreasing model accuracy while increasing computational complexity. Our work underlines the importance of carefully considering and analyzing features in order to build efficient SER systems. We present a new supervised SER method based on an efficient feature engineering approach. We pay particular attention to the explainability of results to evaluate feature relevance and refine feature sets. This is performed iteratively through feature evaluation loop, using Shapley values to boost feature selection and improve overall framework performance. Our approach allows thus to balance the benefits between model performance and transparency. The proposed method outperforms human-level performance (HLP) and stateof-the-art machine learning methods in emotion recognition on the TESS dataset. The source code of this paper is publicly available at Iterative-Feature-Boosting-for-Explainable-Speech-Emotion-Recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human emotions are complex and essential aspects of human behavior. They correspond to subjective experiences characterized by physiological, behavioral, and cognitive changes, and can be influenced by various factors, such as social interactions, and cultural considerations. Emotions play an important role in various aspects of human life. Therefore, emotion recognition and interpretation is considered as a key problem in psychology, with a wide range of AI applications, including human-computer interaction, affective computing, and mental disorder detection  [1] .\n\nTo solve the speech emotion recognition (SER) problem, various machine learning algorithms have been explored, including support vector machines  [2] , hidden Markov models  [3] , and deep neural networks  [4] . These algorithms are trained on large datasets, and use various feature representations, such as spectral features, prosodic features, and spectral envelope features  [5] . Given the multitude of features and their several categories, one of the main challenges is to find the appropriate feature representation for a given SER task. In fact, the use of large predefined feature sets often leads to high-dimensional datasets  [6] , making it difficult for the model to learn ef-fectively from data, in addition to increasing computational complexity  [7] .\n\nIn the literature, little attention has been paid to finding relevant features for SER as most of existing works used principal component analysis (PCA) for dimensionality reduction  [8] , while some other works relied on 1D convolutions through end-to-end deep learning models  [9] . In our work, we argue that the performance of SER systems heavily depends on the used features. We thus present a comprehensive approach, placing a special emphasis on the feature extraction and selection process.\n\nThe proposed framework comprises three main components : 1) a feature boosting module guided by the feedback loop of the third component to extract and select features, 2) a classification module using a supervised classification model, and 3) an explainability module where the contribution of features to the classification decision is evaluated using SHapley Additive exPlanations (SHAP)  [10] . This explainability component serves as a feedback mechanism at the end of each iteration, to continuously refine and boost the feature set in the first component.\n\nAccordingly, our main contributions can be summarized as follows:\n\n• A new SER approach with an emphasis on feature selection through iterative feature boosting. • Incorporation of model explainability and SHAP technique for identifying the most relevant features for the SER task, as well as for transparency purposes. • An experimental evaluation of the proposed method by comparison to human-level performance and state-of-theart algorithms. • The source code of our framework for reproducibility and future research on SER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Modern SER methods are mostly based on supervised learning approaches. They can be broadly divided into two categories: traditional machine learning methods  [11]    [12] , and deep learning methods  [13]  [14]  [9] . Several techniques related to feature extraction have been explored in previous works. They are mostly based on handcrafted features, which Fig.  1 . The proposed method are designed by incorporating expert knowledge or domainspecific insights, and using traditional feature selection techniques. One of the major challenges is handling the high dimensionality of the data and ensuring that the used features are meaningful and relevant. This is because a large number of features are often extracted from signals without considering their practical importance and suitability for the emotion classification task.\n\nIn  [11] , the authors propose to use Mel Frequency Cepstral Coefficients (MFCC) for feature extraction and Support Vector Machines (SVM) for classification. MFCCs are calculated through a series of steps including sampling and quantization, windowing, discrete Fourier transform, and a Mel filter bank. The resulting MFCCs are then used for emotion classification using an SVM classifier. A sensitivity analysis is also conducted to evaluate the impact of different feature combinations on the classification performance.\n\nIn  [13] , the authors present two-way approach for SER. The first involves the direct extraction of features from the audio dataset using a combination of mel-scale related features. Then, PCA is used to reduce data dimensionality and eliminate correlated variables. The resulting features are then fed into a deep neural network (DNN) for classification. The authors observed that PCA allowed to significantly reduce overfitting, which in turn leads to a more effective training of the DNN. Their second approach is based on using 2D representation of spectrograms considered as images for classification, which is then carried out by the VGG16 CNN  [15]  model retrained on Mel-Spectrogram images to classify emotions.\n\nIn  [16] , the authors present the robust discriminative sparse regression (RDSR) approach to deal with feature selection and emotion classification in a joint learning framework. The RDSR algorithm is designed to select the most discriminative feature subset from the original high-dimensional feature set. It uses sparse regression to improve model robustness to outliers and noise, and introduces a feature selection regularization constraint to select the most relevant features.\n\nIn  [17] , an SER model based on continuous hidden Markov model (CHMM) was proposed, as the random generation of states in HMMs allows for statistical modeling of the sequential nature of the data. The model extracts 33-dimensional feature parameters based on temporal sequence and uses PCA to reduce the dimensionality of initial feature set. The experimental results showed that the PCA-CHMM model improves emotion recognition performance compared to the standard HMM model using the entire feature set.\n\nIn  [9] , the authors propose a hybrid end-to-end deep learning model for feature extraction and emotion classification. The proposed model consists of two main components: onedimensional convolutional neural network (1D-CNN) and Gated Recurrent Unit (GRU). The 1D-CNN serves for spatial features extraction through convolutional layers, while the GRU component captures time-distributed features due to memory and gate principles.\n\nPrevious research has shown that extracting efficient acoustic characteristics is crucial for capturing various emotional aspects of speech in SER. However, existing works mostly relied on predefined features, without fully investigating their relevance for SER, and how they can be used to improve performance. Some deep learning-based works attempted to address the feature selection issue implicitly through 1D convolutions, while other supervised learning methods aimed to handle the high dimensionality problem by merely applying PCA to compute principal components. We present a new approach for supervised SER with a focus on feature importance and model explainability over the entire framework.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method A. Motivation And Overview",
      "text": "Both the quality and quantity of used features can significantly impact the performance of emotion recognition. Using too many irrelevant or redundant features can lead to overfitting and lack of generalization, while using few or insufficient features can result in underfitting and poor performance. Therefore, it is important to carefully select the features that are most useful for emotion recognition, by considering the voice signal characteristics that are most indicative of emotions.\n\nOur method is based on domain-specific voice features and comprises three main components. A feature boosting module is firstly used to compute a preliminary feature set assumed to be useful for emotion recognition, these features are iteratively refined afterwards via a feedback mechanism. Then, we identify optimal feature combinations from the resulting feature set and reduce dimensionality. Secondly, a classification module formulates the SER task as a supervised classification problem. Classification decisions are then analyzed by an explainability module, which utilizes Shapley values to evaluate the importance of features in the classification decision, thus, to provide insights into the feature boosting module. By incorporating explainability into the SER process, we aim to enhance the performance of the model with a better understanding of the contribution of each feature to the final decision. The obtained explainability results are then used iteratively via a feedback mechanism to further boost the feature selection process in the first module. This is achieved through a feedback loop at the end of each iteration.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Method",
      "text": "The proposed method is illustrated in Fig.  1 . In the feature boosting module, we compute a preliminary feature set representation including pitch, energy, and rhythm related characteristics, which we assume being meaningful for SER. We also calculate related statistical characteristics including the mean, median, standard deviation, minimum and maximum  [18] , resulting in a set of n initial features. To further improve the performance and interpretability of our technique, we select m optimal distinct combinations of p features from the dataset according to following steps. We use PCA to reduce data dimensionality of each combination i and remove noise by transforming it using an eigenvector matrix (A i ) and a corresponding eigenvalues vector (λ i ). Each column of the eigenvector matrix represents a principal component (P C ij ) capturing specific data information and determining the dimension (r i ) of the reduced subset (see eq. 1 and 2).\n\nThe percentage of variance (EV ij ) explained by j th principal component of the i th combination (P C ij ) can be evaluated using eq. 3:\n\nwhere λ ij represents the eigenvalue and the amount of variance of the j th principal component of the i th combination (P C ij ). The expression of (P C ij ) is given by eq. 4, where X ik is the k th feature of the i th combination:\n\nTherefore, we can determine which features contribute the most to each principal component. This is done in order to identify the best combination of features representing information in our dataset. We use a threshold α on the sum of the first c explained variances as a criterion to determine feature combinations that are most informative. The resulting feature set consists of the principal components of each selected combination. In this way, we eliminate redundant and less informative features, and use only the most relevant ones. This above detailed process is refined iteratively through the feedback loop of our architecture's explainability module, which we will describe later in this section.\n\nIn the classification module we compare the performance of M candidate classification models on the resulting features, these models are detailed in section IV-A. Our choice of classification models aligns with those commonly used classifiers in the SER literature. The selected machine learning models have been widely used in various domains for SER and have been known to achieve good results  [5] . The process of comparing The features on the y-axis in Fig.  2  are represented as principal components, which are the result of applying PCA to the optimal combinations of selected features using the explained variance threshold. In Fig.  3 , the y-axis represents the initial feature importance.\n\nthe performance of M different classification models on the initial dataset and the resulting feature set helps us determine how well our approach performs, to consequently select the appropriate model. Beyond SER, the framework could be applied to a wide range of classification problems, where other candidate classification models could be evaluated.\n\nIn the explainability module we incorporate explainable artificial intelligence (XAI)  [19]  capabilities into the SER system. Thus, we create a system that is transparent and understandable in terms of prediction and decision making. To achieve this, we use the Shapley explanation values to explain the model's predictions. Shapley values allow to understand the contribution of each P C ij in the resulting feature set to a model's prediction, which enables us to identify which combination's principal components are most important for the emotion recognition. We define the contribution of each P C ij in the resulting feature set denoted ϕ P Cij as:\n\nwhere m is the number of combinations, J i is the number of principal components used from each combination i with J i ≥ 1, S is a subset of principal components indices, z is the input vector, and f (z) is the output of the classification model for input z. The first summation term (f ij (z s∪ij )) computes the expected output of the model when P C ij is included in the subset, while the second term (f (z s )) computes the expected output of the model when P C ij is discarded from S. As a result, the difference between the two terms represents the contribution of P C ij to the output, which we use to conduct a feature importance analysis. This allows to get insight into how the model works and what factors are most important.\n\nThe process involves an iterative feedback loop where we use the explainability module to determine the most relevant principal components that capture the essential information for emotion recognition and eliminate less important ones. We then incorporate the contribution of the initial features to each of the relevant principal components to identify the most participating features to the principal components, and thus, to the classification decision process. We eliminate features that do not contribute significantly to the model's performance and iterate the process until convergence.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset And Experimental Setup",
      "text": "We use the Toronto Emotional Speech Set (TESS)  [20]  dataset including 2800 audio recordings of two participants expressing 200 target phrases in different emotional states. The emotions included in the dataset are anger, disgust, pleasant surprise, fear, sadness, happiness, and neutral, where each emotion is represented by 400 recordings.\n\nWe first set the sampling rate of the audio data to 16 KHz using a mono-channel format. This ensures that audio signals are properly processed and analyzed by our system, as most SER algorithms require specific sampling rates and number of channels for each audio signal. After going through the feature extraction and selection process, we use stratified random sampling  [21]  to divide both original dataset and boosted features dataset into three homogeneous groups (or strata): training, validation, and testing. We keep 10% of the data as unseen to be used for testing, 80% for training, and 10% for validation. This ensures that the distribution of classes is maintained across all subsets. Then, we use 10fold cross-validation  [22]  to train M = 7 machine learning models: Extra Trees (ET), Light Gradient Boosting Machine (LGBM), Random Forest (RF), Quadratic Discriminant Analysis (QDA), Gradient Boosting Classifier (GBC), Linear Discriminant Analysis (LDA), and Decision Tree (DT), on both datasets to select the optimal model for each. By using crossvalidation, our performance evaluation should be less sensitive to data partitioning.\n\nIn order to improve the performance of our best-performing machine learning models, we use the grid search technique, which involves exhaustively searching through a specified parameter space to find the best combination of hyperparameters for a given model  [23] . In this way, we are able to fine-tune the model by adjusting its hyperparameters to increase robustness. We thus find the optimal set of hyperparameters producing the highest performance on the validation dataset.\n\nWe then assess the performance of the final models on the testing set. The testing performance is an indicator of how well the model would perform on unseen data without overfitting the training set. Finally, we use the SHAP approach in our explainability module to evaluate the feature importance in the predictions of the optimal model. This allows us to understand how the model is making its predictions and to identify which features are most important for determining emotions. For performance evaluation, we use accuracy, recall, precision, and F1-Score metrics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Model Explainability",
      "text": "One key aspect of our approach is the use of feature boosting module to select the optimal combinations of features that best capture the variance and information in our dataset. To demonstrate this, we show importance values in figures 2 and 3. We can observe the contribution of each feature to the predicted emotion class. The principal components in Fig.  2  are labeled as P C {combination index}{P C index} , i.e. P C ij refers to the j th principal component of the i th feature combination. Additionally, it can also help in understanding which principal components are not important and can be removed without relatively affecting the performance of the model. By examining Fig.  2 , we can determine the principal components that have the highest impact on the output of our model. Through this analysis, we can also identify the feature combination that correspond to each principal component and the contribution of each feature to a principal component. This leads us ultimately to determine the most relevant features for our SER decision (see Fig.  3 ). Thus, we can identify the features that contribute the most to each of the important principal components, this information is valuable because it allows us to refine our SER system by eliminating less relevant features and improving the accuracy of the classification decision in the next iteration.\n\nWe set a threshold α = 0.8 on the cumulative explained variance by the first two principal components of each of the feature combinations. This means that we select only the combinations that explain at least 80% of the data variance. Thus, we obtain the best 19 combinations representing the data. As an example, we have found a combination of features that are particularly informative. This feature set captures 84.56% of the total explained variance in the first two principal components and 98.22% in the first four.\n\nThe biplot in Fig.  4  displays the data points on a 2D scatter plot, with the position of each point representing the values of the first two principal components of the optimal feature combination data. Additionally, the biplot also shows the directions and the lengths of the arrows representing the optimal features in the transformed space. Analyzing the biplot allow us to",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Are In Bold Font",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Compared Methods",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Tess Dataset",
      "text": "Test Accuracy (%) F1-score (%) Aggarwal et al.  [13]  97.6 97 Praseetha et al.  [14]  95.8 NA Choudhary et al.  [24]  97.1 96 Huang et al.  [25]  85 NA Iqbal et al.  [11]  97 NA Kapoor et al.  [26]  97.5 97.4 Krishnan et al.  [12]  93.3 NA Dupuis et al. (HLP)  [27]  82 NA Our method 98.7 98.7\n\nunderstand how the optimal features are related to the principal components as the direction of the arrow indicates the sign of the contribution (positive or negative), while the length indicates the magnitude of the contribution. This, informs us on how these features contribute to the overall variance of the data. Moreover, the cumulative explained variance can be added to the biplot, which indicates the percentage of the total variance in the optimal feature combination data explained by each principal component. This is helpful in determining the optimal number of principal components to retain when performing PCA to the selected feature combinations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "1) Comparison With Sota Methods:",
      "text": "The results of our proposed method on the TESS dataset are compared with other state-of-the-art methods, as presented in Table  I . The performance metrics of these state-of-the-art methods are taken from the original papers. We have two main evaluation approaches for our method. Firstly, we compare our method against human-level performance (HLP) on the TESS dataset, as evaluated in  [27] . The authors used 56 human annotators to recognize emotions, and we use their results to evaluate our model's performance. Secondly, we compare our method against machine learning-based SER methods. As previously discussed, SER methods typically involve two main stages: feature extraction and classification. Many of the compared methods in the literature use MFCC for feature extraction  [11] ,  [14] ,  [25] ,  [24] , while some others use spectrograms  [26]  combined with PCA  [13]  or Empirical Mode Decomposition (EMD)  [12] . For classification, some methods employ traditional machine learning techniques such as SVM  [11]  or Latent Dirichlet Allocation (LDA)  [12] , while others use deep neural networks  [13] ,  [25] ,  [24] ,  [26] ,  [14] . By comparing our results to the HLP, we can see that our model is able to perform emotion recognition tasks better than humans according to the results in  [27] . Our method also outperformed the compared machine learning methods, achieving the highest accuracy and F1-score as shown in Table  I .\n\n2) Importance of feature boosting and model explainability: In Table  II , we can see the performance of various machine learning models without feature boosting and model explainability on the initially computed features. It is clear from the table that the ET classifier performs the best, as it achieves The LGBM also performs well with 95% accuracy. RF and GBC also have a very high accuracy at 94.6% and 94.3%, respectively. This, indicates that the initially computed features do indeed contain valuable information for the SER task. Therefor, we can conclude that the ET classifier and LGBM are the best models for this dataset, and we can use them to achieve high performance.\n\nThe confusion matrix in Fig.  5 , values between \"( )\", shows the results of the ET classifier. The matrix indicates that the model performs well overall, as can be seen by the high rate of correct predictions on the diagonal elements. For example, 100% of the actual class \"sad\" are correctly predicted as \"sad\". However, there are also some misclassifications, such as when 5.45% of the actual class \"surprise\" is predicted as \"happy\" and 8.33% of the actual class \"happy\" is predicted as \"surprise\". This means that \"happy\" and \"surprise\" shares some acoustic characteristics.\n\nTable  III  compares the performance of the same machine learning models using the boosted feature set and the incorporation of model explainability feedback. The best performing model in terms of accuracy is the ET classifier with an accuracy of 98.7% and F1-score of 98.7%. The second best performing model is LGBM with an accuracy and F1-score of 95.9% and 96%, respectively. RF also has relatively high accuracy of 95%, and good scores for the other evaluation metrics. Some other models, have lower accuracy and less favorable scores for the other evaluation metrics. Few of these low-performing models appear to lose their effectiveness when feature boosting and model explainability are employed. This may be due to their need for more complex datasets in order to achieve reasonable output, resulting in increased computational complexity. This suggests that these models may be less suitable for SERs than the models that perform well under the same conditions. In summary, the ET classifier and LGBM are the best performing models on this dataset with high accuracy and F1-score.\n\nThe confusion matrix in Fig.  5 , values between \"[ ]\", shows the results of the ET classifier, which performs well overall with a high number of correct predictions on the diagonal elements for each emotion. For example, 100% of the actual class \"neutral\" are correctly predicted as \"neural\" and 99.09% of the actual class \"fear\" are correctly predicted as \"fear\". However, there are also some misclassifications, such as 1.87% of \"disgust\" being predicted as \"sad\" and 2.78% of \"happy\" being predicted as \"surprise\". This means that \"happy\" shares characteristics with \"surprise\" as well as \"disgust\" with \"sad\".\n\nFurthermore, the process of hyperparameters tuning can help to optimize the performance of a model by finding the optimal values for the hyperparameters that control the model's complexity and generalization performance, such as the case of our compared models. We achieve this by using random grid search technique, which involves training the model using a range of hyperparameter values and evaluating the performance of each model using cross-validation set. The best set of hyperparameters can then be selected based on the model's performance on the validation set.\n\nTo sum up, the use of feature boosting and model explainability to refine and boost feature selection results in a significant increase and robustness in the performance of the models, which can be attributed to several factors. One of the main reasons is that PCA is able to reduce the dimensionality of the data, which can help to remove noise and redundancy. This induces a more compact and informative representation of the data that is better suited for the machine learning models. Additionally, the explainability module drives the feedback loop by identifying the most important combinations of features that are driving the classification. This allows us to iteratively boost the feature selection process, taking into account new insights and understanding gained through XAI techniques. This iterative process leads to even better representation of the speech signal via the most important features for the SER task, resulting in improved model generalization.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Discussion",
      "text": "In this work, the objective is to extract the most relevant features for accurately detecting emotions in speech. We noticed a lack of consensus in the existing literature regarding a clear set of features that effectively capture emotional information from speech signals. This knowledge gap highlights the importance of our research in addressing this issue and providing valuable insights into feature selection for SER. Through this study, we make significant contributions to the field by exploring and identifying the features that play a crucial role in detecting and distinguishing emotional states in speech.\n\nOur analysis focused on identifying features with high discriminative power and informativeness for differentiating between various emotional categories and how can we explain and interpret that. We employ a rigorous feature selection process to identify the most relevant features. By using an One of the key features that emerges as highly relevant in our study is MFCC, which capture the spectral characteristics of speech and have been widely used in speech analysis tasks. MFCCs are known to effectively capture the distinctive characteristics and spectral variations associated with different patterns. Another important feature that we find valuable for SER is pitch or fundamental frequency (F0). Variations in pitch convey important emotional cues and can help discriminate between different emotional states. Analyzing pitch-related features such as pitch contour, pitch range, or pitch dynamics can provide valuable information for emotion classification. Additionally, we find that energy and intensity measures play a significant role in capturing emotional intensity and arousal. These features reflect the overall energy distribution and loudness of speech, which are closely related to emotional expressiveness. Furthermore, temporal features such as speech rate or duration demonstrated relevance in capturing the temporal patterns and dynamics of emotional speech. The rate at which speech is produced and the duration of specific speech segments can provide important cues for SER.\n\nOne limitation of our study is that it was tested solely on the TESS dataset. While the results obtained from this dataset are promising, it is important to validate our approach on multiple datasets and in real-world scenarios to ensure the generalizability of our findings. Validation on diverse datasets would provide a more comprehensive assessment of the effectiveness of our feature boosting approach in different contexts and with different speech samples. It would enable us to evaluate the robustness and reliability of the selected features across various data sources, potentially uncovering any dataset-specific biases or limitations. Furthermore, real-world scenarios often present additional challenges, such as varying recording conditions, speaker characteristics, and noise levels.\n\nThese factors can impact the performance of the SER system and the relevance of the selected features.\n\nHowever, it is worth noting that our research provides a comprehensive analysis of the feature selection process and highlights the rationale behind selecting specific features. By presenting these findings, we contribute to the development of a standardized feature set for SER, which can serve as a foundation for future research in the field. This standardized set of features will enable researchers to focus on these key features when designing and implementing robust SERs, ultimately enhancing the accuracy and effectiveness of trustworthy emotion detection in real-world applications.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this study, we presented a new approach for supervised SER based on acoustic features of the voice and their statistical characteristics values. Our work highlights the importance of feature selection and the role of explainability in improving the accuracy of SER. Our method involves several steps including computing speech features, selecting the optimal feature combinations, boosting the final feature subset, and applying various machine learning models to evaluate their performance and fine-tune the best performing model. Additionally, we incorporate XAI into SER to create a system that is more understandable, to boost feature selection process via the feedback loop. To the best of our knowledge, this is the first work incorporating model explainability into an SER framework. Our work provides a comprehensive SER approach that aims to balance the benefits of advanced machine learning techniques with the need for transparency and comprehensibility. Our future work aims to investigate feature boosting within deep learning frameworks. Moreover, we aim to generalize our feature boosting approach for other classification problems, addressing the feature relevance and high dimensionality problems.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed method",
      "page": 2
    },
    {
      "caption": "Figure 1: In the fea-",
      "page": 3
    },
    {
      "caption": "Figure 2: Boosted features impor-",
      "page": 3
    },
    {
      "caption": "Figure 3: Initial features importance",
      "page": 3
    },
    {
      "caption": "Figure 2: are represented as principal components, which are",
      "page": 3
    },
    {
      "caption": "Figure 3: , the y-axis represents the initial feature",
      "page": 3
    },
    {
      "caption": "Figure 4: Biplot of TESS optimal feature combination",
      "page": 4
    },
    {
      "caption": "Figure 2: are labeled as PC{combination index}{P C index}, i.e.",
      "page": 4
    },
    {
      "caption": "Figure 2: , we can determine the principal",
      "page": 4
    },
    {
      "caption": "Figure 3: ). Thus, we can identify",
      "page": 4
    },
    {
      "caption": "Figure 4: displays the data points on a 2D scatter",
      "page": 4
    },
    {
      "caption": "Figure 5: , values between ”( )”, shows",
      "page": 5
    },
    {
      "caption": "Figure 5: , values between ”[ ]”, shows",
      "page": 6
    },
    {
      "caption": "Figure 5: Confusion matrices for the Extra Trees classifier. Values between ”(",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "ET",
          "Acc.": "95.8",
          "Recall": "95.8",
          "Prec.": "95.8",
          "F1": "95.8"
        },
        {
          "Model": "LGBM",
          "Acc.": "95",
          "Recall": "94.9",
          "Prec.": "95",
          "F1": "95"
        },
        {
          "Model": "RF",
          "Acc.": "94.6",
          "Recall": "94.6",
          "Prec.": "94.7",
          "F1": "94.5"
        },
        {
          "Model": "GBC",
          "Acc.": "94.3",
          "Recall": "94.3",
          "Prec.": "94.3",
          "F1": "94.3"
        },
        {
          "Model": "LDA",
          "Acc.": "92.9",
          "Recall": "93",
          "Prec.": "93.3",
          "F1": "92.9"
        },
        {
          "Model": "DT",
          "Acc.": "92.9",
          "Recall": "92.1",
          "Prec.": "92.3",
          "F1": "92.1"
        },
        {
          "Model": "QDA",
          "Acc.": "83.2",
          "Recall": "83.3",
          "Prec.": "85.7",
          "F1": "82"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Compared methods": "TESS dataset"
        },
        {
          "Compared methods": "Aggarwal et al.\n[13]"
        },
        {
          "Compared methods": "Praseetha et al.\n[14]"
        },
        {
          "Compared methods": "Choudhary et al.\n[24]"
        },
        {
          "Compared methods": "Huang et al.\n[25]"
        },
        {
          "Compared methods": "Iqbal et al.\n[11]"
        },
        {
          "Compared methods": "Kapoor et al.\n[26]"
        },
        {
          "Compared methods": "Krishnan et al.\n[12]"
        },
        {
          "Compared methods": "Dupuis et al.\n(HLP)\n[27]"
        },
        {
          "Compared methods": "Our method"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "ET",
          "Acc.": "98.7",
          "Recall": "98.6",
          "Prec.": "98.7",
          "F1": "98.7"
        },
        {
          "Model": "LGBM",
          "Acc.": "95.9",
          "Recall": "96",
          "Prec.": "96.3",
          "F1": "96"
        },
        {
          "Model": "RF",
          "Acc.": "95",
          "Recall": "95",
          "Prec.": "95",
          "F1": "95"
        },
        {
          "Model": "LDA",
          "Acc.": "94.5",
          "Recall": "94.5",
          "Prec.": "94.6",
          "F1": "94.5"
        },
        {
          "Model": "GBC",
          "Acc.": "93.6",
          "Recall": "93.6",
          "Prec.": "93.7",
          "F1": "93.7"
        },
        {
          "Model": "QDA",
          "Acc.": "92.6",
          "Recall": "92.7",
          "Prec.": "92.7",
          "F1": "92.6"
        },
        {
          "Model": "DT",
          "Acc.": "90.3",
          "Recall": "90.3",
          "Prec.": "90.6",
          "F1": "90.2"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion knowledge: further exploration of a prototype approach",
      "authors": [
        "P Shaver",
        "J Schwartz",
        "D Kirson",
        "C O'connor"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition using support vector machines",
      "authors": [
        "T Seehapoch",
        "S Wongthanavasu"
      ],
      "year": "2013",
      "venue": "2013 5th International Conference on Knowledge and Smart Technology (KST)"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using hidden markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "4",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "5",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Automatic speech emotion recognition-feature space dimensionality and classification challenges",
      "authors": [
        "A Al-Talabani"
      ],
      "year": "2015",
      "venue": "Automatic speech emotion recognition-feature space dimensionality and classification challenges"
    },
    {
      "citation_id": "7",
      "title": "Handling high dimensional features by ensemble learning for emotion identification from speech signal",
      "authors": [
        "K Kumar"
      ],
      "year": "2022",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "8",
      "title": "Dimensionality reduction for emotional speech recognition",
      "authors": [
        "P Fewzee",
        "F Karray"
      ],
      "year": "2012",
      "venue": "2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing"
    },
    {
      "citation_id": "9",
      "title": "Cnn-n-gru: endto-end speech emotion recognition from raw waveform signal using cnns and gated recurrent unit networks",
      "authors": [
        "A Nfissi",
        "W Bouachir",
        "N Bouguila",
        "B Mishara"
      ],
      "year": "2022",
      "venue": "Proceedings of the 21st IEEE International Conference on Machine Learning and Applications"
    },
    {
      "citation_id": "10",
      "title": "The Shapley value: essays in honor of Lloyd S. Shapley",
      "authors": [
        "A Roth"
      ],
      "year": "1988",
      "venue": "The Shapley value: essays in honor of Lloyd S. Shapley"
    },
    {
      "citation_id": "11",
      "title": "Mfcc and machine learning based speech emotion recognition over tess and iemocap datasets",
      "year": "2020",
      "venue": "Foundation University Journal of Engineering and Applied Sciences"
    },
    {
      "citation_id": "12",
      "title": "Emotion classification from speech signal based on empirical mode decomposition and non-linear features",
      "authors": [
        "P Krishnan",
        "A Joseph Raj",
        "V Rajangam"
      ],
      "year": "2021",
      "venue": "Complex & Intelligent Systems"
    },
    {
      "citation_id": "13",
      "title": "Two-way feature extraction for speech emotion recognition using deep learning",
      "authors": [
        "A Aggarwal",
        "A Srivastava",
        "A Agarwal",
        "N Chahal",
        "D Singh",
        "A Alnuaim",
        "A Alhadlaq",
        "H.-N Lee"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "14",
      "title": "Deep learning models for speech emotion recognition",
      "authors": [
        "V Praseetha",
        "S Vadivel"
      ],
      "year": "2018",
      "venue": "Journal of Computer Science"
    },
    {
      "citation_id": "15",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition based on robust discriminative sparse regression",
      "authors": [
        "P Song",
        "W Zheng",
        "Y Yu",
        "S Ou"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition based on pca and chmm",
      "authors": [
        "X Ke",
        "B Cao",
        "J Bai",
        "Q Yu",
        "D Yang"
      ],
      "year": "2019",
      "venue": "2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)"
    },
    {
      "citation_id": "18",
      "title": "An acoustic study of emotions expressed in speech",
      "authors": [
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "Z Deng",
        "S Lee",
        "S Narayanan",
        "C Busso"
      ],
      "year": "2004",
      "venue": "Eighth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Explainable AI: interpreting, explaining and visualizing deep learning",
      "authors": [
        "W Samek",
        "G Montavon",
        "A Vedaldi",
        "L Hansen",
        "K.-R Müller"
      ],
      "year": "2019",
      "venue": "Explainable AI: interpreting, explaining and visualizing deep learning"
    },
    {
      "citation_id": "20",
      "title": "Toronto emotional speech set (tess)",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Scholars Portal Dataverse"
    },
    {
      "citation_id": "21",
      "title": "A study of stratified random sampling",
      "authors": [
        "H Aoyama"
      ],
      "year": "1954",
      "venue": "Ann. Inst. Stat. Math"
    },
    {
      "citation_id": "22",
      "title": "Encyclopedia of database systems",
      "authors": [
        "P Refaeilzadeh",
        "L Tang",
        "H Liu"
      ],
      "year": "2009",
      "venue": "Encyclopedia of database systems"
    },
    {
      "citation_id": "23",
      "title": "On hyperparameter optimization of machine learning algorithms: Theory and practice",
      "authors": [
        "L Yang",
        "A Shami"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion based sentiment recognition using deep neural networks",
      "authors": [
        "R Choudhary",
        "G Meena",
        "K Mohbey"
      ],
      "year": "2022",
      "venue": "Journal of Physics: Conference Series"
    },
    {
      "citation_id": "25",
      "title": "Human vocal sentiment analysis",
      "authors": [
        "A Huang",
        "P Bao"
      ],
      "year": "2019",
      "venue": "Human vocal sentiment analysis"
    },
    {
      "citation_id": "26",
      "title": "Fusing traditionally extracted features with deep learned features from the speech spectrogram for anger and stress detection using convolution neural network",
      "authors": [
        "S Kapoor",
        "T Kumar"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "27",
      "title": "Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2011",
      "venue": "Canadian Acoustics"
    }
  ]
}