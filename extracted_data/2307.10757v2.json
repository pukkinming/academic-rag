{
  "paper_id": "2307.10757v2",
  "title": "Vesper: A Compact And Effective Pretrained Model For Speech Emotion Recognition",
  "published": "2023-07-20T10:42:16Z",
  "authors": [
    "Weidong Chen",
    "Xiaofen Xing",
    "Peihao Chen",
    "Xiangmin Xu"
  ],
  "keywords": [
    "Pretrained model",
    "speech emotion recognition",
    "self-supervised learning",
    "representation learning = Objective Task-specific Space General Space Task-specific Pretrained Model General Pretrained Model Task-specific Obj. Model Size Specialization (c) Ours (Compression + Label-free Adaptation)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improVed emotionspecific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper employs hierarchical and cross-layer self-supervision to improve its ability to capture acoustic and semantic representations, both of which are crucial for emotion recognition. Experimental results on the IEMOCAP, MELD, and CREMA-D datasets demonstrate that Vesper with 4 layers outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers surpasses that of WavLM Large with 24 layers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "P REVALENT in the realm of artificial intelligence are large-scale pretrained models (PTMs) comprising hundreds of millions and, in some cases, billions of parameters, which have exhibited remarkable performance across various tasks  [1] ,  [2] . PTMs are recognized as key components of artificial general intelligence due to their ability to solve multiple tasks simultaneously  [3] . PTMs are pretrained using extensive amounts of unlabeled data and then generalized to specific tasks to achieve human-like performance. Among them, PTMs such as BERT  [4] , GPT-3  [3] , and CLIP  [5]  have achieved exceptional results across a wide range of natural language processing and computer vision tasks. Similarly, in the speech signal processing domain, PTMs such as wav2vec  [6] , wav2vec 2.0  [7] , HuBERT  [8] , and WavLM  [9]  have pushed the boundaries of various speech tasks and delivered promising performance.\n\nThe triumph of large-scale PTMs has sparked researchers' interest in acquiring greater amounts of unlabeled data to pretrain larger models with billions of parameters  [3] ,  [10] ,  [11] . However, current PTMs are pretrained in a task-agnostic manner because they are designed to capture general representations that can be applied to various tasks. Consequently, there is always room for improvement on specific tasks. Moreover, the substantial computational and storage resources required for large-scale PTMs make them challenging to apply in practical scenarios.\n\nThe future development of PTMs will not only focus on creating large-scale general PTMs but also explore another research direction, which is generating task-specific PTMs by additional pretraining of general PTMs using task-specific objectives. Task-specific PTMs are expected to exhibit the following features: (1) Task-specific pretraining enables taskspecific PTMs to capture critical task-specific representations from the general representations learned from massive unlabeled data. (2) Task-specific PTMs are lightweight because they focus on one specific task. In summary, task-specific PTMs should be compact and effective.\n\nCurrent studies  [12] -  [19]  on using general PTMs for specific tasks have explored two main directions:  (1)  As shown in Fig.  1 (a), researchers have attempted to reduce the model size of PTMs to overcome the latency and capacity constraints. In particular, the knowledge distillation technique is applied to transfer knowledge from a large-scale PTM to a compact model  [16] ,  [18] ,  [19] . Despite model compression, knowledge distillation fails to incorporate the characteristics of specific tasks, and thus, the compact model is still taskagnostic, leading to an incompatibility between the general pretrained objectives and specific downstream tasks.  (2)  As shown in Fig.  1 (b), the large-scale PTM is directly adapted to downstream tasks by fine-tuning on labeled task-related corpora  [13] ,  [14] . However, acquiring sufficient labeled data for downstream tasks is difficult and costly. Additionally, the model size of the PTM remains unchanged, which limits practical deployment. To overcome these limitations, we integrate compression and label-free adaptation into a single pipeline to generate a task-specific PTM that is both compact and effective, as illustrated in Fig.  1(c) .\n\nWe focus on the speech emotion recognition task and present an improVed emotion-specific pretrained encoder called Vesper. Vesper is pretrained further on the basis of WavLM  [9]  in a self-supervised manner with an emotionrelated training strategy. It is initialized with the parameters of WavLM and achieves compression by reducing the number of employed layers. Masked prediction is applied as the label-free training objective. To improve Vesper's sensitivity to emotional information, we design a novel emotion-guided masking strategy. In particular, we utilize the energy of the speech signal to identify regions that contain emotional information with high probability and apply masking only within these regions. Recent research  [20]  has shown that the shallow layers of speech PTMs tend to capture acoustic features, while the deep layers tend to capture semantic features. As both acoustic and semantic features are crucial for emotion recognition  [21] ,  [22] , we employ hierarchical self-supervision to separately supervise the shallow and deep layers of Vesper. To enrich acoustic information in the deep layer output, we propose cross-layer self-supervision to make the output representation more informative and balanced.\n\nThe contributions of this paper can be summarized as follows:\n\n• We propose a new pipeline that generalizes large-scale pretrained models on speech emotion recognition task by compression and emotion-specific adaptation. We hope that the pipeline inspires researchers to generate compact and effective pretrained models for various speech tasks. • We focus on the speech emotion recognition task and propose an emotion-specific pretrained encoder called Vesper. To enhance Vesper's sensitivity to emotional information, we introduce an emotion-guided masking strategy during pretraining, leveraging the energy of the input speech signal to identify the regions that need masking. We also propose a hierarchical self-supervision approach to enhance Vesper's capability to capture both acoustic and semantic information and present a crosslayer self-supervision approach to improve the informativeness and balance of the final output representation. The remainder of this paper is organized as follows: In Section II, we provide a literature review on large-scale pretrained models and their applications. In Section III, we elaborate on the proposed Vesper. In Section IV, we describe the experimental corpora and setup in detail. In Section V, we present our experimental results and analyses. Finally, we present our conclusions in Section VI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we present an overview of large-scale pretrained models in artificial intelligence and review the various ways researchers employ them in downstream tasks. Subsequently, we systematically introduce Transformer and WavLM, as these two frameworks are the basis of Vesper.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Large-Scale Pretrained Models In Artificial Intelligence",
      "text": "In recent years, the development of large-scale pretrained models has revolutionized the field of artificial intelligence. PTMs leverage vast amounts of unlabeled data and computational power to learn general representations for various tasks. BERT  [4] , RoBERTa  [23] , T5  [24] , and GPT-3  [3]  are the most popular PTMs in the field of natural language processing. They have achieved remarkable results in text classification tasks such as named entity recognition  [25]  and question answering  [13] , as well as generative tasks such as machine translation  [26]  and abstractive summarization  [15] . GPT-3  [3]  sheds new light on artificial general intelligence and has spawned numerous practical applications. PTMs have also made significant strides in the field of computer vision, ushering in a new era. Vision PTMs such as MoCo  [27] , ViT  [28] , simCLR  [29] , and video models  [30] ,  [31]  capture high-level visual features to generalize on diverse visual tasks, including object detection  [32] , image segmentation  [33] , and image captioning  [34] . ViT  [28]  has emerged as a highly influential and widely adopted architecture in computer vision. Visionlanguage pretrained models jointly learn universal vision and language features by pretraining on large-scale image-text pairs. SAM  [35] , CLIP  [5] , Flamingo  [36] , and DALL-E  [37]  achieve strong performance in cross-modal matching, crossmodal reasoning, and cross-modal generation. Various PTMs in the speech domain have also been proposed, including wav2vec 2.0  [7] , HuBERT  [8] , and WavLM  [9] . Equipped with the above speech PTMs, researchers have showcased their superior performance in a wide range of speech-related tasks, including automatic speech recognition  [7] ,  [9] ,  [38] , speech enhancement  [39] ,  [40] , and speech emotion recognition  [12] ,  [41] -  [43] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Utilizing Pretrained Models For Specific Tasks",
      "text": "After large-scale PTMs became available, many researchers started exploring their potential applications for various specific tasks. They have mainly focused on the following three directions.\n\n1) Transfer Learning: One key advantage of large-scale PTMs is their remarkable capacity for transfer learning. By leveraging the universal knowledge learned from large-scale unlabeled data, PTMs can be used as a powerful starting point for a new task with limited labeled data  [13] ,  [15] ,  [32] ,  [44] . Fine-tuning is one of the most widely used implementations of transfer learning. Cao et al.  [13]  initialized a model with the pretraining weights of BERT  [4]  and subsequently performed fine-tuning on the target corpus. Fabbri et al.  [15]  created dataset-specific unlabeled data for fine-tuning by leveraging the characteristics of the target dataset to improve zeroshot learning of the model. The transferability of pretrained representations reduces the need for extensive labeled data in specific tasks. However, updating the PTM is resourceintensive due to the substantial model size of the PTM. The PTM is also difficult to deploy in resource-constrained realworld applications.\n\n2) Knowledge Distillation: PTMs can serve as \"teachers\" to transfer their knowledge to a smaller \"student\" model using the knowledge distillation technique  [16] -  [18] ,  [45] -  [47] . Generally, the objective function employed for this purpose is the Kullback-Leibler (KL) divergence loss, which constrains the probability distribution of the student model to mimic the teacher's prediction. Typically, Sanh et al.  [18]  introduced a distilled version of BERT known as DistilBERT, which achieved 97% of the performance of BERT while utilizing only 60% of the model size. Liu et al.  [45]  adopted a selfdistillation mechanism and achieved promising results across twelve datasets. Although the model has been successfully compressed, the compact student model is still a general model that lacks specificity for specific tasks. To obtain a task-specific PTM, labeled data from the target domain are necessary. Zhang et al.  [17]  utilized adversarial samples to augment limited labeled data from the target domain to enhance taskspecific knowledge transfer.\n\n3) Feature Extraction: PTMs can also serve as feature extractors, where the pretrained model is frozen and utilized to extract acoustic features from raw input data  [41] ,  [48] -  [53] . The extracted features are fed into a separate classifier or downstream model designed for the given task. Rahman et al.  [49]  employed three pretrained CNN models to extract features from chest X-ray images. Chen et al.  [41]  adopted WavLM  [9]  to extract the acoustic features from raw audio samples and achieved remarkable results for speech emotion recognition. Even though the downstream model is usually quite small, using PTM as a feature extractor greatly increases the overall size of the system. Furthermore, general representations may not be effectively mapped to a specific space by using the simple classifier only. The task-specific downstream model requires manual design and considerable manual tuning.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Revisiting Transformer And Wavlm",
      "text": "Transformer  [54]  originally consisted of an encoder and decoder. In this paper, we describe the encoder part only since it is what is needed to implement our proposed architecture. The Transformer encoder contains a multihead self-attention (MSA) module and a fully connected feed-forward network Task-Specific Pretraining by Self-Supervision Fig.  2 . The proposed paradigm for generating a task-specific pretrained model that is both compact and effective based on a large-scale pretrained model. The paradigm consists of two steps: compression and task-specific pretraining.\n\n(FFN). The input of Transformer, x ∈ R T ×d , is an arbitrary sequence, where T indicates the sequence length and d indicates the feature dimension. The feed-forward network (FFN) consists of two linear projections with a ReLU activation  [55]  in between. The computational process of Transformer encoder is depicted as follows:\n\nwhere T r represents the Transformer encoder. More details can be found in  [54] . Given that WavLM  [9]  exhibits state-of-the-art results on the SUPERB benchmark  [56] , we select WavLM as the foundational model in this paper. WavLM  [9]  is a self-supervised pretrained model built on the Transformer encoder, which is pretrained on several large corpora and learns to encode audio for general purposes. Compared to other pretrained models in the speech domain that focus mainly on phoneme classification and automatic speech recognition tasks, WavLM utilizes a speech denoising objective in addition to masked speech prediction in the pretraining procedure to jointly extract speech content and acoustic information. Benefitting from denoising modeling, WavLM is extended to full stack speech processing tasks. Additionally, WavLM applies gated relative position bias to the Transformer structure to better model the sequential information of the input speech signal. WavLM comes in two versions, Base and Large, with different model sizes and computational complexities. WavLM Base consists of 12 Transformer layers, and WavLM Large consists of 24 Transformer layers. Detailed configurations can be found in  [9] . Finally, WavLM is evaluated on the SUPERB  [56]  benchmark. The experimental results demonstrate that WavLM achieves promising performance across 14 downstream tasks, including automatic speech recognition, speaker verification, speech separation, and speech emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Paradigm And Vesper",
      "text": "As shown in Fig  2 , the proposed paradigm consists of two main steps: compression at initialization and task-specific pretraining by self-supervision. The first step combines initialization with compression and generates a compact model based on the large-scale pretrained model. In the second step, the compact model is further pretrained by incorporating the characteristics of the downstream task in a self-supervised manner. Eventually, a task-specific pretrained model that is both compact and effective is produced. In this paper, we focus on speech emotion recognition task and create an emotion-specific Vesper following the above paradigm. Vesper is built upon the pretrained WavLM Large model. The model architectures of Vesper and WavLM are identical except for the difference in the number of Transformer layers employed. In the following subsections, to each step involved in building Vesper is comprehensively described.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Compression At Initialization",
      "text": "To take full advantage of the knowledge obtained from massive unlabeled speech data in the large-scale pretrained model, current studies apply mainly knowledge distillation techniques to implement knowledge transfer. As illustrated in Fig.  3 (a), the pretrained WavLM Large model serves as the \"teacher\", while Vesper acts as the \"student\". To train the student model, KL loss is utilized to ensure that the student output mimics the distribution of the teacher output. The distillation operation generates a compact student model that retains general representation and can be further pretrained according to task-specific objectives.\n\nGiven that Vesper and WavLM possess the same model architecture, it is worth investigating the possibility of directly initializing Vesper with WavLM's parameters. As shown in Fig.  3",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Task-Specific Pretraining By Self-Supervision",
      "text": "By employing the initialization method above, we can obtain a compact but general Vesper. To ensure that Vesper becomes emotion-specific, it is essential to employ the emotionguided masking strategy and further pretrain Vesper by the following self-supervised approaches. The overall pretraining strategy for Vesper is illustrated in Fig.  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1) Emotion-Guided Masking Strategy:",
      "text": "To enhance Vesper's sensitivity to emotional information, we propose an emotionguided masking strategy that is both computationally efficient and effective. In contrast to previous masking strategies that employ random selection of mask positions, we determine the positions to be masked based on the root mean square (rms) energy of the input speech signal. The reason is that the energy of input speech, which represents the loudness and intensity of a speaker's voice, can provide valuable information about the speaker's emotional state. Changes in signal energy indicate variations in emotional states  [57] -  [59] . For example, more intense emotional states such as anger or excitement are usually associated with higher signal energy, while more depressed emotional states such as sadness or depression have lower signal energy compared to the normal state. The rms energy is defined as:\n\nwhere A is the input audio, A f denotes the f -th frame of A with frame length L and E(f ) denotes the rms energy of the f -th frame.\n\nGiven the energy E, we first divide it by its maximum value to limit the value of E to [0, 1]. As depicted in Fig.  5 , we then partition the energy into three zones, i.e., a high energy zone with a range of (0.5, 1], a low energy zone with a range of (0.2, 0.5], and a noise zone with a range of [0, 0.2]. As mentioned, frames located in the high-energy zone have a high probability of containing intense emotions, while those located in the low-energy zone have a high probability of containing depressed emotions. Therefore, we randomly select half of the mask positions from the high-energy zone and the other half from the low-energy zone to enable Vesper to better focus on emotional information. Eventually, the selected mask position will become the center of the masking region.\n\nDuring the masking process, we hope to integrate the inherent structural characteristics of speech signals to enhance the interpretability of Vesper. Following SpeechFormer++  [60] , which initially focuses on phoneme-level modeling followed by word-level modeling, we propose a similar strategy of applying phoneme-level masking first, followed by word-level masking. Specifically, as shown in Fig.  5  and the left part of Fig.  4 , a phoneme-level mask, with a mask span of 160 ms, is first applied to the input of the first Transformer layer in Vesper to promote fine-grained information learning. An additional word-level mask, with a mask span of 800 ms, is applied to the middle Transformer layer to enhance coarse-grained feature learning. The decision to use mask spans of 160 ms and 800 ms is based on the statistical analysis of phoneme and word durations conducted in  [60] . The word-level mask inherits the masking positions from the phoneme-level mask. However, we decrease the number of masking positions in the wordlevel mask due to its larger mask span. The masking strategy functions as an auxiliary component, filtering out non-essential regions to facilitate the following self-supervised learning.\n\n2) Hierarchical Self-Supervision: To incorporate emotional specificity into Vesper, we must determine how emotion is expressed in the speech signal. Note that acoustic features such as rhythm and articulation convey many messages about emotion. Meanwhile, semantic information contained in speech reveals the emotional state of the speaker. Therefore, there is an urgent need to enhance Vesper's ability to extract both acoustic and semantic information for emotion recognition.\n\nRecent research  [20]  has reported that the shallow layers of pretrained models tend to learn acoustic features, while the deep layers tend to extract the semantic features of speech signals. Accordingly, as shown in Fig.  4 , we propose the hierarchical self-supervision approach to supervise the shallow and deep layers of Vesper separately. The shallow and deep layers of the frozen WavLM Large model are utilized to generate the targets during the pretraining stage of Vesper.\n\nSpecifically, suppose the j-th Transformer layer in WavLM Large is T r W j . The calculation flow in WavLM Large is:\n\nwhere j ∈ [1, M ]; the CNN encoder consumes audio A to yield the latent representation y 0 ∈ R T ×d , and T and d are the sequence length and dimension, respectively. Suppose the i-th Transformer layer in Vesper is T r V i , the masked indices in the phoneme-level mask are I p , and the masked indices in the word-level mask are I w . We assume the mask embedding to be M K ∈ R d . The calculation flow in Vesper is as follows:\n\nx ′ 0 = Add M ask(x 0 , M K, I p )\n\nx\n\nx\n\nx\n\nwhere i ∈ [1, N ], Add M ask(x, M K, I) replaces the embedding in x with M K according to the position index I.\n\nDuring pretraining, the intermediate and final outputs of Vesper are required to predict the intermediate and final outputs of the WavLM Large model, respectively. The mean squared error (MSE) is utilized to calculate the training loss on the masked regions. Formally, the loss is defined as:\n\nwhere P 1 and P 2 are predictors consisting of two linear layers with an activation layer in between.\n\n3) Cross-Layer Self-Supervision: Although hierarchical self-supervision enhances the extraction of both acoustic and semantic information, information bias still persists. The shallow layers of the model excel at extracting acoustic information, while the deep layers are more adept at capturing semantic information. This information bias leads to the representations of different layers in Vesper being somewhat complementary. To make the final representation of Vesper more informative, i.e., containing both acoustic and semantic information, we propose the cross-layer self-supervision approach to provide additional supervision for the learning process of the last layer in Vesper. Formally, the additional cross-layer self-supervision loss is given by\n\nwhere P 3 is a predictor consuming the final output of Vesper to predict the intermediate output of WavLM Large. Note that L x is applied to all positions, including both the masked and unmasked parts. The output of Vesper now encompasses both rich semantic information and sufficient acoustic information. Finally, the objective for training Vesper can be written as:\n\nwhere λ l , λ h and λ x are hyperparameters employed to balance different loss components.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Datasets",
      "text": "Vesper is first pretrained on the LSSED dataset and then fine-tuned on the IEMOCAP, MELD, and CREMA-D datasets to evaluate the performance of speech emotion recognition.\n\nLSSED  [61]  is a recently released large-scale English speech emotion dataset. It comprises data collected from 820 subjects and consists of 147,025 samples. The average duration of all samples in the LSSED is 5.05 s, and the total duration is approximately 206 hours. We do not utilize the sentiment labels provided in the LSSED dataset, as Vesper is pretrained in a self-supervised manner.\n\nIEMOCAP  [62]  dataset is widely used in the field of speech emotion recognition. It comprises 12 hours of audio data divided into five sessions, each containing one male and one female speaker. In this study, we focus on 5,531 utterances from four emotion categories: angry, neutral, happy 1  , and sad. We employ the speaker-independent 5-fold cross-validation strategy to evaluate model performance. The reported results are the average scores of the 5-fold experiments.\n\nMELD  [63]  consists of 13,708 utterances extracted from the Friends TV series and is categorized into seven emotion classes: anger, disgust, sadness, joy, neutral, surprise, and fear. MELD is officially split into training, validation, and testing sets. We utilize the validation set for hyperparameter tuning. The model with the best performance on the validation set is then evaluated on the testing set. The reported results are the scores achieved on the testing set.\n\nCREMA-D  [64]  is an audiovisual dataset with 7442 original clips from 91 actors (48 males and 43 females) between the ages of 20 and 74 across diverse races and ethnicities. The actors deliver their lines from a predefined set of 12 sentences, encompassing six different emotional states (anger, disgust, fear, happiness, neutral, and sadness) across four emotional levels (low, medium, high, and unspecified). The model is tested using the standard rule of 80/20, where 80% of the samples are used for training and 20% are used for testing. The scores achieved on the testing set are reported.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "We apply three widely used evaluation metrics to evaluate the performance of speech emotion recognition: weighted accuracy (WA), unweighted accuracy (UA), and weighted average F1 (WF1). The metrics are computed as follows:\n\nwhere N c denotes the number of samples of the c-th category and Acc(c) and F 1(c) are the classification accuracy and F1 score of the c-th category, respectively. We adopt the WA as our primary metric to evaluate the IEMOCAP and CREMA-D datasets. For the MELD dataset, we select WF1 as our primary evaluation metric because of the sample imbalance issue.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Implementation Details",
      "text": "When fine-tuning on the downstream datasets, the crossentropy loss is employed as the objective function. Unless  otherwise stated, the downstream model is implemented as a simple classifier consisting of two fully connected layers with an average pooling layer in between, which is consistent with the SUPERB benchmark  [56] . The classifier is placed on top of the pretrained Vesper to predict the emotion state. The hidden dimension of the classifier is set to 256. Note that the pretrained Vesper remains fixed, and only the classifier is trained during the fine-tuning process. In addition, we freeze the CNN encoder throughout the process, including during pretraining and fine-tuning. To align with the SUPERB benchmark, the representations of each layer are weighted by trainable weights to generate the input of the downstream classifier, unless otherwise stated. The audio samples in LSSED are cropped or padded to 5s for pretraining. The audio samples in IEMOCAP, MELD, and CREMA-D are cropped or padded to 6.5s, 4.5s, and 3s for fine-tuning, respectively. We introduce two versions, Vesper-4 and Vesper-12, with 4 and 12 Transformer layers, respectively. Note that the number of Transformer layers employed in WavLM Base is 12, and the number of Transformer layers used in WavLM Large is 24. Due to the shared model structure between Vesper and WavLM, the configurations of Vesper-4 and Vesper-12 remain consistent with WavLM Large, except for the variation in the number of layers employed. The number of masking positions is set to 20 for the phoneme-level mask and 4 for the wordlevel mask. λ l , λ h and λ x are empirically set to 1.0, 0.1 and 1.0, respectively. More detailed training hyperparameters are shown in Table  I . The platform used for model training and testing is an Ubuntu 18.04 server equipped with GeForce RTX 2080 Ti GPU. The program is implemented based on Pytorch.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Performance And Computational Efficiency",
      "text": "We compare the proposed Vesper-4 with the general pretrained WavLM Base and compare Vesper-12 with WavLM Large in terms of performance and computational efficiency. Specifically, the number of parameters and the theoretical computational complexity (FLOPs) are listed in Table  II , and the performance of speech emotion recognition is reported in Table  III . Compared to the WavLM Base model, Vesper-4 has a smaller model size (63.52 M vs. 94.70 M, relative reduction of 32.9%) but is able to deliver superior performances in all metrics. Specifically, our Vesper-4 results in a 2.5% WA gain over WavLM Base on the IEMOCAP dataset and 13.5% WA gain over WavLM Base on the CREMA-D dataset. On the MELD dataset, Vesper-4 improves the accuracies (0.2% in WA, 4.9% in UA and 5.7% in WF1) compared to WavLM Base. Similarly, comparing our Vesper-12 with the WavLM Large model shows that the model size of Vesper-12 is approximately half that of the WavLM Large model (164.29 M vs. 316.62 M, relative reduction of 48.1%). However, Vesper-12 achieves comparable results to WavLM Large (0.707 vs. 0.706 in WA on the IEMOCAP dataset, 0.480 vs. 0.476 in WF1 on the MELD dataset, and 0.772 vs. 0.757 in WA on the CREMA-D dataset). In addition, Vesper-4 demonstrates a 22.1% reduction in computational burden compared to WavLM Base, and the computational burden of Vesper-12 is reduced by 41.9% compared to WavLM Large. The experimental results indicate that there is indeed room for improvement in using a general pretrained model for specific downstream tasks. Through taskspecific continuous pretraining, the general pretrained WavLM model is transformed into Vesper, which is both compact and effective for speech emotion recognition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Performance Of Downstream Models With Vesper'S Features",
      "text": "We replace the simple classifier used in the SUPERB benchmark with two state-of-the-art approaches, namely, Shiftformer  [42]  and SpeechFormer  [68] , to further evaluate the effectiveness of Vesper. The experimental results are shown in Table  IV . When employing the downstream model with Vesper-4 features on the IEMOCAP dataset, we observe an improvement of approximately 10% across all metrics compared to WavLM Base. Compared to WavLM Large, utilizing Vesper-12 on the IEMOCAP dataset results in an improvement of 1.0% in WA for Shiftformer and 0.8% for SpeechFormer. For the MELD dataset, Vesper-4 outperforms WavLM Base by 2.1% in WF1 for Shiftformer and 2.7% in WF1 for Finally, Shiftformer using Vesper-12 achieves a classification accuracy of 80.6%. We conclude that Vesper yields improved performance across various downstream models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Comparison With Some Known Systems",
      "text": "We compare our top-performing model (Vesper-12) against previous works on IEMOCAP, MELD, and CREMA-D datasets and the results are presented in Table  V . It is important to note that the prior systems compared with Vesper are specifically tailored for the speech emotion recognition task and demand significant manual tuning, whereas our Vesper, followed by two fully connected layers (FC), is simply and directly applied in the target datasets. As seen in Table  V , our Vesper with FC is able to attain comparable performance against existing methods on the IEMOCAP and MELD datasets. More concretely, our Vesper outperforms MCFN  [69] , ISNet  [70] , and DAAE  [46]  systems and achieves comparable results to CA-MSER  [51]  on IEMOCAP. In the MELD dataset, Vesper's performance demonstrates only a marginal difference compared to SpeechFormer++  [60]  in the UA metric. However, across all other metrics, Vesper with FC consistently outperforms its competitors. For the CREMA-D dataset, Vesper with FC outperforms the existing methods by a substantial margin in all the listed metrics. Particularly, the enhancements observed in WA, UA, and WF1 are 2.5%, 18.9%, and 19.1% respectively. The above results once again confirm the superiority and robustness of Vesper. Additionally, when Vesper is augmented with the advanced downstream model Shiftformer  [42] , significant improvements in the results are observed on the IEMOCAP and CREMA-D datasets. More details and analysis can be found in Section V-B.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "D. Ablation Study",
      "text": "In this section, we conduct a comprehensive ablation study to demonstrate the effectiveness of the various components of Vesper. The downstream classifier utilized in the following experiments is consistent with the SUPERB benchmark.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "1) Comparison Of Different Compression Methods: Taking Advantage Of The Common Dimensionality Between Wavlm",
      "text": "Large and Vesper networks, we directly employ the parameters of WavLM Large to initialize Vesper and simultaneously achieve compression. Specifically, direct compression includes uniform extraction and uniform averaging. To probe the effect of different compression methods, we compare direct compression with knowledge distillation. The results are shown in Table  VI . All compression methods are described in Section III-A. We also implement random initialization, which assigns random initial parameters to Vesper, for a comprehensive comparison. The pretraining process is consistent for all experiments. As shown in Table  VI , random initialization leads to the worst performance in all cases due to the lack of training data. Thus, considering the knowledge acquired from the pretrained WavLM is essential. Distillation-based compression leverages universal knowledge and obtains 0.665 WA on IEMOCAP, 0.437 WF1 on MELD and 0.712 on CREMA-D for Vesper-4. For Vesper-12, distillation compression yields 0.690 WA on IEMOCAP, 0.463 WF1 on MELD, and 0.766 WA on CREMA-D. Nevertheless, the uniform averaging method achieves comparable performance to distillation with simpler operations on the three datasets, exhibiting a difference in evaluation metrics of less than 1%. Uniform extraction, which directly copies the parameters of WavLM Large to Vesper, further simplifies the compression process and achieves the best performance in all cases. The results indicate that directly initializing Vesper with the parameters of the pretrained model is not only the simplest approach but also the most effective. Hence, we apply uniform extraction as the compression and initialization method by default.\n\n2) Effectiveness of the Emotion-Guided Masking Strategy: In contrast to the traditional masking strategy, which randomly determines the masking position, the proposed emotion-guided masking strategy enhances Vesper's sensitivity to emotional information by determining the masking position according to the rms energy. Table VII compares these two masking strategies. For Vesper-4 on the IEMOCAP dataset, the emotionguided masking strategy shows an absolute improvement of 2.8% in WA, 2.6% in UA, and 2.1% in WF1 over the traditional random masking strategy. For the MELD dataset, although the random strategy obtains a slightly higher WA score (0.506 vs. 0.501 in WA), the primary evaluation metric WF1 decreases by 1.2% compared to the emotion-guided masking strategy. For the CREMA-D dataset, the proposed strategy outperforms random masking by 1.6% on all evaluation metrics. In the case of Vesper-12, the proposed masking strategy shows an absolute improvement of 1.9% in WA, 1.7% in UA, and 2.1% in WF1 over the random strategy on the IEMOCAP dataset. For the MELD dataset, Vesper-12 equipped with the emotion-guided masking strategy achieves a gain of 1.8% in WA, 0.8% in UA, and 1.1% in WF1. For the CREMA-D dataset, our strategy outperforms random masking by 1.0% on WA, 2.0% on UA, and 0.8% on WF1. These results verify the effectiveness of the emotion-guided masking strategy for speech emotion recognition.\n\n3) Effectiveness of Hierarchical Self-Supervision: To verify the potency of the hierarchical self-supervision, we conduct experiments by discarding the loss L l or solely applying the loss L h . The experimental results are shown in Table  VIII . It can be seen that the absence of L l leads to decreased performance on all datasets. For example, Vesper-4 exhibits a decrease of 2.3% in WA on the IEMOCAP dataset, a decrease of 0.6% in WF1 on the MELD dataset, and a decrease of 2.0% in WA on the CREMA-D dataset when L l is omitted. A similar trend is observed in Vesper-12 when L l is omitted, which results in an absolute reduction of 1.6% in WA on IEMOCAP, 1.0% in WF1 on MELD, and 1.0% in WA on the CREMA-D. Self-supervision of the lower layers is completely eliminated when applying only the L h loss. However, the results presented in Table VIII demonstrate that relying solely on the L h loss yields the poorest performance across all datasets, with an absolute reduction of 1.0%∼3.7% in all evaluation metrics. These results confirm the effectiveness of the proposed hierarchical self-supervision approach for speech emotion recognition.\n\n4) Effectiveness of Cross-Layer Self-Supervision: To validate the indispensability of the cross-layer self-supervision, we discard the loss L x to invalidate it. The results are summarized in Table  VIII . Taking Vesper-4 as an example, on the IEMOCAP dataset, when the loss L x is disabled, WA decreases from 0.684 to 0.668, and UA decreases from 0.693 to 0.682. On the MELD dataset, WF1 decreases from 0.457 to 0.452 under the same conditions. For the CREMA-D dataset, disabling the loss L x leads to a decrease of 0.8% in WA, 1.0% in UA, and 1.1% in WF1. The performance of Vesper-12 is also weakened on all datasets when cross-layer self-supervision is disabled. In particular, on the IEMOCAP dataset, omitting the loss L x results in a decrease of 0.8% in WA. Similarly, on the MELD dataset, the primary evaluation metric WF1 decreases by 1.6%. On the CREMA-D dataset, WA decreases by 0.6% and UA decreases by 1.6%. These experimental results confirm the necessity of cross-layer selfsupervision.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Incorporating Pitch Changes In The Masking Strategy",
      "text": "In the pretraining phase, Vesper has to predict the latent representations of the masked regions, which are selected based on the energy of the speech signal. As shown in  [57] , when coupled with changes in pitch, energy can be useful for emotion recognition. In this subsection, we integrate both energy and pitch changes into the masking strategy, requiring the selected regions to encompass substantial pitch variations. We aim to investigate whether these additional pitch cues enhance the current masking strategy to be more \"emotionguided\". The experimental results are shown in Table  IX .\n\nWhen the masking strategy incorporates information about pitch changes, we can observe that the improvement ranges from 0.2% to 0.4% in the majority of cases. These results indicate that incorporating additional emotion-relevant speech features in the masking strategy can be advantageous. This inclusion aids in identifying crucial regions containing emotional information with high probability, thereby enhancing Vesper's sensitivity to emotional cues. In conclusion, this subsection demonstrates the potential of the emotion-guided masking strategy and suggests the possibility of further improvement.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "F. Introducing Extra Supervision Signals From More Layers",
      "text": "Currently we supervise the intermediate and the final layers of Vesper. We are interested in assessing the performance of introducing additional supervision signals from WavLM to guide the learning of other layers in Vesper. Therefore, we use the 18th Transformer layer output from WavLM to supervise the learning of Vesper-4's 3rd layer or Vesper-12's 9th layer. Moreover, we employ the 6th layer output from WavLM to guide the learning of Vesper-4's 1st layer or Vesper-12's 3rd layer. Note that the proposed supervision for the intermediate and final layers is consistently applied. The recognition results on three corpora are reported in Table  X",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "G. Information Richness Of The Last Layer Representation",
      "text": "Benefitting from the adoption of cross-layer selfsupervision, the final output representation of Vesper contains both semantic information from the deep layers and acoustic information from the shallow layers. Hence, Vesper is expected to yield comparable performance when feeding only the representation of the last layer to the downstream classifier. To validate this hypothesis, we evaluate the performance when using last layer representation as input to the downstream classifier. The results are presented in Table  XI . On the IEMOCAP dataset, notable performance degradation is observed when only the representation from the last layer of WavLM is used. Specifically, there is a decrease of 10.2% in WA for WavLM Base and 2.0% in WA for WavLM Large. In contrast, Vesper using only the last layer representation displays only a minor decrease in performance (-0.2%∼-0.3% in WA and -0.1%∼-0.4% in WF1), and in the UA metric, it even exhibits improvement (+0.6% for Vesper-4 and +0.4% for Vesper-12). On the MELD dataset, using only the last layer representation from WavLM Base exhibits a severe decrease in performance (-1.8% in WA, -5.8% in UA, and -8.7% in WF1). WavLM Large also exhibits a decrease of -2.2% in the primary metric WF1. Remarkably, utilizing the last layer representation from Vesper-4 yields an improvement across all metrics, with a notable enhancement of +1.1% in WF1. For Vesper-12 on the MELD dataset, the performance decrease caused by merely using the last layer representation is kept within the range of -0.2% to -0.9%. On the CREMA-D dataset, employing only the last layer representation of WavLM Base leads to a decrease of 3.5%∼3.6% in all metrics. Similarly, using only the last layer representation of WavLM Large results in a decrease of 6.2%∼6.6% in all evaluation metrics. For Vesper-4, solely using the last layer representation leads to a decrease of 1.4%∼1.9% in all metrics, where the performance degradation is mitigated. When considering Vesper-12, the performance with the last layer representation increases by 0.2%∼0.4%. Given the experimental results, we conclude that the last layer representation of Vesper is informative enough to perform speech emotion recognition. This characteristic simplifies the utilization of the pretrained model, as it is no longer necessary to extract representations from each layer individually. Instead, relying solely on the representation from the last layer of Vesper proves to be adequate and sufficient.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "H. Visualization Of Deep Representations",
      "text": "To intuitively demonstrate Vesper's emotional specificity, we use the t-SNE  [74]  algorithm to visualize the deep representations from the last layer of the WavLM Base, WavLM Large, Vesper-4, and Vesper-12 models on the IEMOCAP dataset. The visualization results are illustrated in Fig.  6 . Similar results are observed on the CREMA-D dataset, which are not included in the paper to save space. Additionally, we omit the MELD dataset because the relatively poor performances of all models observed on the MELD dataset make the visualization indistinguishable. The visualization results show that the representations of all samples obtained by WavLM Base are mixed together and completely indistinguishable from each other. Our Vesper-4 demonstrates the capability to differentiate between happy and sad samples and differentiate ‡ Vesper is initialized and supervised by HuBERT Large  [8] .\n\nbetween angry and sad samples to some extent. However, the representations of happy and angry emotions remain confusing. Similarities in acoustic features between happy and angry samples, such as high energy and a fast speech rate, present a challenge for recognition. Additionally, the neutral and happy samples exhibit better clustering compared to WavLM Base. In WavLM Large, the angry, happy, and sad samples can be approximately distinguished. However, the neutral representations from WavLM Large are still severely confused with other emotional representations. This issue is effectively solved to some extent by Vesper-12. Although there is some overlap in the margins of different emotional samples, Vesper-12 successfully distinguishes most samples in terms of the four different emotional states. The visualization results once again confirm the effectiveness of the proposed Vesper.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "I. Replacing Wavlm With Hubert",
      "text": "To further validate the feasibility of the proposed paradigm, we replace WavLM with HuBERT  [8] , another widely utilized large-scale pretrained model in the speech domain. Similar to WavLM, HuBERT is available in two versions: Base and Large, comprising 12 and 24 layers respectively. In this subsection, Vesper is initialized with the parameters of HuBERT Large and guided by the intermediate and final outputs of HuBERT Large through label-free self-supervision. Subsequently, we compare the performances of the resulting Vesper with the original HuBERT model on the used datasets. As shown in Table XII, our Vesper-4 with 4 layers consistently outperforms HuBERT Base with 12 layers across all three datasets. Specifically, our Vesper-4 achieves a 1.8% gain in WA over HuBERT Base on the IEMOCAP dataset, a 5.7% gain in WF1 on the MELD dataset, and a 10.7% gain in WA on the CREMA-D dataset. Similar results are observed when comparing the 12-layer Vesper-12 with the 24-layer HuBERT Large. Across all datasets, Vesper-12 demonstrates significantly superior performance to HuBERT Large in all metrics, with the exception of the less crucial WA metric in the MELD dataset, where HuBERT Large slightly outperforms Vesper-12. The results suggest that the proposed paradigm is also effective when applied to the pretrained HuBERT model, confirming the feasibility and universality of our paradigm.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose a new paradigm to generate taskspecific pretrained models for speech emotion recognition by applying compression and label-free adaptation. We produce an improved emotion-specific pretrained encoder called Vesper. Building upon the large-scale pretrained WavLM, Vesper directly utilizes the parameters of WavLM for initialization and improves the specificity for emotion recognition by employing an emotion-guided masking strategy during emotionspecific pretraining. In addition, hierarchical self-supervision and cross-layer self-supervision are proposed to learn the acoustic and semantic information embedded in speech signals. Experimental results on three emotion recognition corpora demonstrate that our Vesper with 4 layers substantially outperforms WavLM Base with 12 layers, and the performance of Vesper with 12 layers exceeds that of WavLM Large with 24 layers. Additionally, ablation experiments are conducted to analyze the individual contributions of each component. In the future, we intend to develop task-specific pretrained models for other tasks, including speaker recognition and speech enhancement, by using the paradigm presented in this paper and redesigning the task-specific objectives and training strategies based on the characteristics of the target tasks.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Compressing large-scale pretrained model by knowledge distil-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a), researchers have attempted to reduce the model",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), the large-scale PTM is directly adapted",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed paradigm for generating a task-specific pretrained model",
      "page": 3
    },
    {
      "caption": "Figure 2: , the proposed paradigm consists of",
      "page": 3
    },
    {
      "caption": "Figure 3: (a), the pretrained WavLM Large model serves as",
      "page": 4
    },
    {
      "caption": "Figure 3: (b), the CNN encoder in Vesper is directly taken",
      "page": 4
    },
    {
      "caption": "Figure 4: 1) Emotion-Guided Masking Strategy: To enhance Vesper’s",
      "page": 4
    },
    {
      "caption": "Figure 3: Two types of compression approaches. The dashed line in (b)",
      "page": 4
    },
    {
      "caption": "Figure 5: and the left part of",
      "page": 4
    },
    {
      "caption": "Figure 4: , a phoneme-level mask, with a mask span of 160 ms, is",
      "page": 4
    },
    {
      "caption": "Figure 4: The task-specific self-supervised pretraining strategy of the proposed Vesper, which mainly consists of emotion-guided masking strategy, hierarchical",
      "page": 5
    },
    {
      "caption": "Figure 5: The emotion-guided masking strategy used for pretraining Vesper.",
      "page": 5
    },
    {
      "caption": "Figure 4: , we propose the",
      "page": 5
    },
    {
      "caption": "Figure 6: Visualization of the deep representations from the last layer of WavLM",
      "page": 11
    },
    {
      "caption": "Figure 6: Similar results are observed on the CREMA-D dataset, which",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Method": "WavLM Base\nVesper-4",
          "WA\nUA\nWF1": "0.659†\n-\n-\n0.684\n0.693\n0.683"
        },
        {
          "Dataset": "",
          "Method": "WavLM Large\nVesper-12",
          "WA\nUA\nWF1": "0.706†\n-\n-\n0.707\n0.708\n0.706"
        },
        {
          "Dataset": "MELD",
          "Method": "WavLM Base\nVesper-4",
          "WA\nUA\nWF1": "0.499\n0.201\n0.400\n0.501\n0.250\n0.457"
        },
        {
          "Dataset": "",
          "Method": "WavLM Large\nVesper-12",
          "WA\nUA\nWF1": "0.542\n0.253\n0.476\n0.268\n0.480\n0.535"
        },
        {
          "Dataset": "CREMA-D",
          "Method": "WavLM Base\nVesper-4",
          "WA\nUA\nWF1": "0.599\n0.599\n0.600\n0.734\n0.737\n0.733"
        },
        {
          "Dataset": "",
          "Method": "WavLM Large\nVesper-12",
          "WA\nUA\nWF1": "0.757\n0.762\n0.755\n0.772\n0.776\n0.768"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Upstream": "WavLM Base\nVesper-4",
          "Downstream": "SpeechFormer-S [68]",
          "WA\nUA\nWF1": "0.608\n0.613\n0.602\n0.711\n0.725\n0.711"
        },
        {
          "Dataset": "",
          "Upstream": "WavLM Large\nVesper-12",
          "Downstream": "",
          "WA\nUA\nWF1": "0.721\n0.729\n0.719\n0.729\n0.731\n0.725"
        },
        {
          "Dataset": "MELD",
          "Upstream": "WavLM Base\nVesper-4",
          "Downstream": "",
          "WA\nUA\nWF1": "0.481\n0.208\n0.414\n0.489\n0.260\n0.441"
        },
        {
          "Dataset": "",
          "Upstream": "WavLM Large\nVesper-12",
          "Downstream": "",
          "WA\nUA\nWF1": "0.281\n0.493\n0.474\n0.507\n0.477\n0.276"
        },
        {
          "Dataset": "CREMA-D",
          "Upstream": "WavLM Base\nVesper-4",
          "Downstream": "",
          "WA\nUA\nWF1": "0.696\n0.698\n0.691\n0.761\n0.764\n0.760"
        },
        {
          "Dataset": "",
          "Upstream": "WavLM Large\nVesper-12",
          "Downstream": "",
          "WA\nUA\nWF1": "0.770\n0.772\n0.769\n0.794\n0.797\n0.795"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Method": "MCFN [69]\n† ISNet\n[70]\nDAAE [46]\nCA-MSER [51]",
          "WA\nUA\nWF1": "0.621\n0.603\n-\n0.704\n0.650\n-\n0.701\n0.707\n-\n0.698\n0.711\n-"
        },
        {
          "Dataset": "",
          "Method": "Vesper\n(FC)\nVesper\n(Shiftformer)",
          "WA\nUA\nWF1": "0.707\n0.708\n0.706\n0.737\n0.743\n0.735"
        },
        {
          "Dataset": "MELD",
          "Method": "MCFN [69]\n† ‡ DECN [71]\n† ‡ SCFA [72]\nSpeechFormer++ [60]",
          "WA\nUA\nWF1": "0.481\n-\n0.367\n0.493\n-\n0.439\n0.474\n-\n0.442\n0.273\n0.510\n0.470"
        },
        {
          "Dataset": "",
          "Method": "Vesper\n(FC)\nVesper\n(Shiftformer)",
          "WA\nUA\nWF1": "0.535\n0.480\n0.268\n0.530\n0.262\n0.479"
        },
        {
          "Dataset": "CREMA-D",
          "Method": "IG-CNN [73]\nLanSER [47]\nPhukan [52]\nfusion\ncat xwc [53]",
          "WA\nUA\nWF1": "0.580\n0.585\n0.577\n-\n0.587\n-\n0.705\n-\n-\n0.747\n-\n-"
        },
        {
          "Dataset": "",
          "Method": "Vesper\n(FC)\nVesper\n(Shiftformer)",
          "WA\nUA\nWF1": "0.772\n0.776\n0.768\n0.806\n0.809\n0.804"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Vesper-4",
          "Dataset": "IEMOCAP",
          "Compression": "Random\nDistillation\nAveraging\nExtraction (used)",
          "WA\nUA\nWF1": "0.534\n0.551\n0.524\n0.665\n0.670\n0.667\n0.659\n0.663\n0.654\n0.684\n0.693\n0.683"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Compression": "Random\nDistillation\nAveraging\nExtraction (used)",
          "WA\nUA\nWF1": "0.438\n0.162\n0.291\n0.499\n0.225\n0.437\n0.261\n0.488\n0.440\n0.501\n0.457\n0.250"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Compression": "Random\nDistillation\nAveraging\nExtraction (used)",
          "WA\nUA\nWF1": "0.595\n0.616\n0.596\n0.712\n0.722\n0.714\n0.715\n0.707\n0.709\n0.734\n0.737\n0.733"
        },
        {
          "Method": "Vesper-12",
          "Dataset": "IEMOCAP",
          "Compression": "Random\nDistillation\nAveraging\nExtraction (used)",
          "WA\nUA\nWF1": "0.552\n0.562\n0.547\n0.690\n0.700\n0.687\n0.687\n0.697\n0.679\n0.707\n0.708\n0.706"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Compression": "Random\nDistillation\nAveraging\nExtraction (used)",
          "WA\nUA\nWF1": "0.484\n0.154\n0.336\n0.516\n0.255\n0.463\n0.277\n0.512\n0.461\n0.535\n0.480\n0.268"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Compression": "Random\nDistillation\nAveraging\nExtraction (used)",
          "WA\nUA\nWF1": "0.620\n0.623\n0.616\n0.766\n0.761\n0.762\n0.759\n0.760\n0.761\n0.772\n0.776\n0.768"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Vesper-4",
          "Dataset": "IEMOCAP",
          "Masking": "Random\nEmotion-Guided",
          "WA\nUA\nWF1": "0.656\n0.667\n0.662\n0.684\n0.693\n0.683"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Masking": "Random\nEmotion-Guided",
          "WA\nUA\nWF1": "0.506\n0.227\n0.445\n0.250\n0.457\n0.501"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Masking": "Random\nEmotion-Guided",
          "WA\nUA\nWF1": "0.718\n0.721\n0.717\n0.734\n0.737\n0.733"
        },
        {
          "Method": "Vesper-12",
          "Dataset": "IEMOCAP",
          "Masking": "Random\nEmotion-Guided",
          "WA\nUA\nWF1": "0.688\n0.691\n0.685\n0.707\n0.708\n0.706"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Masking": "Random\nEmotion-Guided",
          "WA\nUA\nWF1": "0.517\n0.260\n0.469\n0.535\n0.268\n0.480"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Masking": "Random\nEmotion-Guided",
          "WA\nUA\nWF1": "0.762\n0.756\n0.760\n0.772\n0.776\n0.768"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Vesper-4",
          "Dataset": "IEMOCAP",
          "Masking": "Energy\nEnergy + Pitch",
          "WA\nUA\nWF1": "0.684\n0.693\n0.683\n0.686\n0.696\n0.688"
        },
        {
          "Method": "",
          "Dataset": "",
          "Masking": "∆",
          "WA\nUA\nWF1": "+0.2%\n+0.3%\n+0.5%"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Masking": "Energy\nEnergy + Pitch",
          "WA\nUA\nWF1": "0.501\n0.250\n0.457\n0.502\n0.254\n0.460"
        },
        {
          "Method": "",
          "Dataset": "",
          "Masking": "∆",
          "WA\nUA\nWF1": "+0.1%\n+0.4%\n+0.3%"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Masking": "Energy\nEnergy + Pitch",
          "WA\nUA\nWF1": "0.734\n0.737\n0.733\n0.737\n0.739\n0.737"
        },
        {
          "Method": "",
          "Dataset": "",
          "Masking": "∆",
          "WA\nUA\nWF1": "+0.3%\n+0.2%\n+0.4%"
        },
        {
          "Method": "Vesper-12",
          "Dataset": "IEMOCAP",
          "Masking": "Energy\nEnergy + Pitch",
          "WA\nUA\nWF1": "0.707\n0.708\n0.706\n0.709\n0.710\n0.709"
        },
        {
          "Method": "",
          "Dataset": "",
          "Masking": "∆",
          "WA\nUA\nWF1": "+0.2%\n+0.2%\n+0.3%"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Masking": "Energy\nEnergy + Pitch",
          "WA\nUA\nWF1": "0.535\n0.268\n0.480\n0.538\n0.272\n0.483"
        },
        {
          "Method": "",
          "Dataset": "",
          "Masking": "∆",
          "WA\nUA\nWF1": "+0.3%\n+0.4%\n+0.3%"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Masking": "Energy\nEnergy + Pitch",
          "WA\nUA\nWF1": "0.772\n0.776\n0.768\n0.775\n0.778\n0.773"
        },
        {
          "Method": "",
          "Dataset": "",
          "Masking": "∆",
          "WA\nUA\nWF1": "+0.4%\n+0.2%\n+0.5%"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Vesper-4",
          "Dataset": "IEMOCAP",
          "Lx\nLl\nLh\n√": "√\n√\n√\n√\n√\n√\n√\n√",
          "WA\nUA\nWF1": "0.647\n0.662\n0.643\n0.668\n0.682\n0.663\n0.661\n0.673\n0.658\n0.684\n0.693\n0.683"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Lx\nLl\nLh\n√": "√\n√\n√\n√\n√\n√\n√\n√",
          "WA\nUA\nWF1": "0.506\n0.233\n0.447\n0.499\n0.249\n0.452\n0.508\n0.240\n0.451\n0.250\n0.457\n0.501"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Lx\nLl\nLh\n√": "√\n√\n√\n√\n√\n√\n√\n√",
          "WA\nUA\nWF1": "0.702\n0.705\n0.702\n0.723\n0.727\n0.722\n0.714\n0.718\n0.712\n0.734\n0.737\n0.733"
        },
        {
          "Method": "Vesper-12",
          "Dataset": "IEMOCAP",
          "Lx\nLl\nLh\n√": "√\n√\n√\n√\n√\n√\n√\n√",
          "WA\nUA\nWF1": "0.682\n0.700\n0.678\n0.699\n0.704\n0.701\n0.691\n0.698\n0.691\n0.707\n0.708\n0.706"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Lx\nLl\nLh\n√": "√\n√\n√\n√\n√\n√\n√\n√",
          "WA\nUA\nWF1": "0.515\n0.261\n0.469\n0.523\n0.249\n0.464\n0.522\n0.256\n0.470\n0.535\n0.268\n0.480"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Lx\nLl\nLh\n√": "√\n√\n√\n√\n√\n√\n√",
          "WA\nUA\nWF1": "0.755\n0.759\n0.751\n0.766\n0.760\n0.766\n0.762\n0.766\n0.760\n0.772\n0.776\n0.768"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Vesper-4",
          "Dataset": "IEMOCAP",
          "Target Layers": "12, 24\n12, 18, 24\n6, 12, 18, 24",
          "WA\nUA\nWF1": "0.684\n0.693\n0.683\n0.682\n0.697\n0.681\n0.686\n0.699\n0.687"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Target Layers": "12, 24\n12, 18, 24\n6, 12, 18, 24",
          "WA\nUA\nWF1": "0.250\n0.501\n0.457\n0.505\n0.248\n0.458\n0.507\n0.460\n0.249"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Target Layers": "12, 24\n12, 18, 24\n6, 12, 18, 24",
          "WA\nUA\nWF1": "0.734\n0.737\n0.733\n0.730\n0.735\n0.728\n0.731\n0.736\n0.730"
        },
        {
          "Method": "Vesper-12",
          "Dataset": "IEMOCAP",
          "Target Layers": "12, 24\n12, 18, 24\n6, 12, 18, 24",
          "WA\nUA\nWF1": "0.707\n0.706\n0.708\n0.704\n0.706\n0.698\n0.710\n0.702\n0.696"
        },
        {
          "Method": "",
          "Dataset": "MELD",
          "Target Layers": "12, 24\n12, 18, 24\n6, 12, 18, 24",
          "WA\nUA\nWF1": "0.535\n0.480\n0.268\n0.270\n0.529\n0.479\n0.521\n0.269\n0.477"
        },
        {
          "Method": "",
          "Dataset": "CREMA-D",
          "Target Layers": "12, 24\n12, 18, 24\n6, 12, 18, 24",
          "WA\nUA\nWF1": "0.772\n0.776\n0.768\n0.769\n0.771\n0.775\n0.764\n0.768\n0.761"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Method": "WavLM Base",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.659†\n-\n-\n0.557\n0.558\n0.546"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-10.2%\n-\n-"
        },
        {
          "Dataset": "",
          "Method": "WavLM Large",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.706†\n-\n-\n0.686\n0.701\n0.683"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-2.0%\n-\n-"
        },
        {
          "Dataset": "",
          "Method": "Vesper-4",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.684\n0.693\n0.683\n0.681\n0.699\n0.679"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-0.3%\n+0.6%\n-0.4%"
        },
        {
          "Dataset": "",
          "Method": "Vesper-12",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.707\n0.708\n0.706\n0.705\n0.712\n0.705"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-0.2%\n+0.4%\n-0.1%"
        },
        {
          "Dataset": "MELD",
          "Method": "WavLM Base",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.499\n0.201\n0.400\n0.481\n0.143\n0.313"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-1.8%\n-5.8%\n-8.7%"
        },
        {
          "Dataset": "",
          "Method": "WavLM Large",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.542\n0.253\n0.476\n0.522\n0.263\n0.454"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-2.0%\n+1.0%\n-2.2%"
        },
        {
          "Dataset": "",
          "Method": "Vesper-4",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.501\n0.250\n0.457\n0.504\n0.260\n0.468"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "+0.3%\n+1.0%\n+1.1%"
        },
        {
          "Dataset": "",
          "Method": "Vesper-12",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.535\n0.268\n0.480\n0.526\n0.266\n0.476"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-0.9%\n-0.2%\n-0.4%"
        },
        {
          "Dataset": "CREMA-D",
          "Method": "WavLM Base",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.599\n0.599\n0.600\n0.563\n0.564\n0.565"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-3.6%\n-3.5%\n-3.5%"
        },
        {
          "Dataset": "",
          "Method": "WavLM Large",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.757\n0.762\n0.755\n0.694\n0.700\n0.689"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-6.3%\n-6.2%\n-6.6%"
        },
        {
          "Dataset": "",
          "Method": "Vesper-4",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.734\n‘0.737\n0.733\n0.719\n0.723\n0.714"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "-1.5%\n-1.4%\n-1.9%"
        },
        {
          "Dataset": "",
          "Method": "Vesper-12",
          "In. of C.": "Weighted\nLast",
          "WA\nUA\nWF1": "0.772\n0.776\n0.768\n0.774\n0.778\n0.772"
        },
        {
          "Dataset": "",
          "Method": "",
          "In. of C.": "∆",
          "WA\nUA\nWF1": "+0.2%\n+0.2%\n+0.4%"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Method": "HuBERT Base\n‡ Vesper-4",
          "WA\nUA\nWF1": "0.649†\n-\n-\n0.667\n0.670\n0.653"
        },
        {
          "Dataset": "",
          "Method": "HuBERT Large\n‡ Vesper-12",
          "WA\nUA\nWF1": "0.676†\n-\n-\n0.688\n0.699\n0.686"
        },
        {
          "Dataset": "MELD",
          "Method": "HuBERT Base\n‡ Vesper-4",
          "WA\nUA\nWF1": "0.483\n0.193\n0.386\n0.498\n0.241\n0.443"
        },
        {
          "Dataset": "",
          "Method": "HuBERT Large\n‡ Vesper-12",
          "WA\nUA\nWF1": "0.515\n0.206\n0.422\n0.248\n0.448\n0.509"
        },
        {
          "Dataset": "CREMA-D",
          "Method": "HuBERT Base\n‡ Vesper-4",
          "WA\nUA\nWF1": "0.614\n0.617\n0.610\n0.721\n0.723\n0.720"
        },
        {
          "Dataset": "",
          "Method": "HuBERT Large\n‡ Vesper-12",
          "WA\nUA\nWF1": "0.745\n0.743\n0.750\n0.762\n0.765\n0.761"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Audio self-supervised learning: A survey",
      "authors": [
        "S Liu",
        "A Mallol-Ragolta",
        "E Parada-Cabaleiro",
        "K Qian",
        "X Jing",
        "A Kathan",
        "B Hu",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Patterns"
    },
    {
      "citation_id": "2",
      "title": "A survey of vision-language pre-trained models",
      "authors": [
        "Y Du",
        "Z Liu",
        "J Li",
        "W Zhao"
      ],
      "year": "2022",
      "venue": "A survey of vision-language pre-trained models",
      "arxiv": "arXiv:2202.10936"
    },
    {
      "citation_id": "3",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "5",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "6",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "wav2vec: Unsupervised pre-training for speech recognition"
    },
    {
      "citation_id": "7",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "9",
      "title": "WavLM: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "A Joulin",
        "E Grave",
        "G Lample"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "11",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "A Chowdhery",
        "S Narang",
        "J Devlin",
        "M Bosma",
        "G Mishra",
        "A Roberts",
        "P Barham",
        "H Chung",
        "C Sutton",
        "S Gehrmann",
        "P Schuh",
        "K Shi",
        "S Tsvyashchenko",
        "J Maynez",
        "A Rao",
        "P Barnes",
        "Y Tay",
        "N Shazeer",
        "V Prabhakaran",
        "E Reif",
        "N Du",
        "B Hutchinson",
        "R Pope",
        "J Bradbury",
        "J Austin",
        "M Isard",
        "G Gur-Ari",
        "P Yin",
        "T Duke",
        "A Levskaya",
        "S Ghemawat",
        "S Dev",
        "H Michalewski",
        "X Garcia",
        "V Misra",
        "K Robinson",
        "L Fedus",
        "D Zhou",
        "D Ippolito",
        "D Luan",
        "H Lim",
        "B Zoph",
        "A Spiridonov",
        "R Sepassi",
        "D Dohan",
        "S Agrawal",
        "M Omernick",
        "A Dai",
        "T Pillai",
        "M Pellat",
        "A Lewkowycz",
        "E Moreira",
        "R Child",
        "O Polozov",
        "K Lee",
        "Z Zhou",
        "X Wang",
        "B Saeta",
        "M Diaz",
        "O Firat",
        "M Catasta",
        "J Wei",
        "K Meier-Hellstern",
        "D Eck",
        "J Dean",
        "S Petrov",
        "N Fiedel"
      ],
      "year": "2022",
      "venue": "Palm: Scaling language modeling with pathways",
      "arxiv": "arXiv:2204.02311"
    },
    {
      "citation_id": "12",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "DeFormer: Decomposing pre-trained transformers for faster question answering",
      "authors": [
        "Q Cao",
        "H Trivedi",
        "A Balasubramanian",
        "N Balasubramanian"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Acquiring knowledge from pre-trained model to neural machine translation",
      "authors": [
        "R Weng",
        "H Yu",
        "S Huang",
        "S Cheng",
        "W Luo"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "15",
      "title": "Improving zero and few-shot abstractive summarization with intermediate fine-tuning and data augmentation",
      "authors": [
        "A Fabbri",
        "S Han",
        "H Li",
        "H Li",
        "M Ghazvininejad",
        "S Joty",
        "D Radev",
        "Y Mehdad"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter"
    },
    {
      "citation_id": "16",
      "title": "Distilling task-specific knowledge from BERT into simple neural networks",
      "authors": [
        "R Tang",
        "Y Lu",
        "L Liu",
        "L Mou",
        "O Vechtomova",
        "J Lin"
      ],
      "year": "2019",
      "venue": "Distilling task-specific knowledge from BERT into simple neural networks",
      "arxiv": "arXiv:1903.12136"
    },
    {
      "citation_id": "17",
      "title": "Adversarial data augmentation for task-specific knowledge distillation of pre-trained transformers",
      "authors": [
        "M Zhang",
        "N Naresh",
        "Y He"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "19",
      "title": "Adabert: Task-adaptive BERT compression with differentiable neural architecture search",
      "authors": [
        "D Chen",
        "Y Li",
        "M Qiu",
        "Z Wang",
        "B Li",
        "B Ding",
        "H Deng",
        "J Huang",
        "W Lin",
        "J Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "What all do audio transformer models hear? Probing acoustic representations for language delivery and its structure",
      "authors": [
        "J Shah",
        "Y Singla",
        "C Chen",
        "R Shah"
      ],
      "year": "2021",
      "venue": "What all do audio transformer models hear? Probing acoustic representations for language delivery and its structure",
      "arxiv": "arXiv:2101.00387"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition using semantic information",
      "authors": [
        "P Tzirakis",
        "A Nguyen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "23",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "24",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "25",
      "title": "Bioalbert: A simple and effective pre-trained language model for biomedical named entity recognition",
      "authors": [
        "U Naseem",
        "M Khushi",
        "V Reddy",
        "S Rajendran",
        "I Razzak",
        "J Kim"
      ],
      "year": "2021",
      "venue": "International Joint Conference on Neural Networks"
    },
    {
      "citation_id": "26",
      "title": "Zero-shot cross-lingual transfer of neural machine translation with multilingual pretrained encoders",
      "authors": [
        "G Chen",
        "S Ma",
        "Y Chen",
        "L Dong",
        "D Zhang",
        "J Pan",
        "W Wang",
        "F Wei"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "27",
      "title": "Momentum contrast for unsupervised visual representation learning",
      "authors": [
        "K He",
        "H Fan",
        "Y Wu",
        "S Xie",
        "R Girshick"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Masked motion encoding for self-supervised video representation learning",
      "authors": [
        "X Sun",
        "P Chen",
        "L Chen",
        "C Li",
        "T Li",
        "M Tan",
        "C Gan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Rspnet: Relative speed perception for unsupervised video representation learning",
      "authors": [
        "P Chen",
        "D Huang",
        "D He",
        "X Long",
        "R Zeng",
        "S Wen",
        "M Tan",
        "C Gan"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Toward transformer-based object detection",
      "authors": [
        "J Beal",
        "E Kim",
        "E Tzeng",
        "D Park",
        "A Zhai",
        "D Kislyuk"
      ],
      "year": "2020",
      "venue": "Toward transformer-based object detection",
      "arxiv": "arXiv:2012.09958"
    },
    {
      "citation_id": "33",
      "title": "Image segmentation using deep learning: A survey",
      "authors": [
        "S Minaee",
        "Y Boykov",
        "F Porikli",
        "A Plaza",
        "N Kehtarnavaz",
        "D Terzopoulos"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "VisualGPT: Dataefficient adaptation of pretrained language models for image captioning",
      "authors": [
        "J Chen",
        "H Guo",
        "K Yi",
        "B Li",
        "M Elhoseiny"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Segment anything",
      "authors": [
        "A Kirillov",
        "E Mintun",
        "N Ravi",
        "H Mao",
        "C Rolland",
        "L Gustafson",
        "T Xiao",
        "S Whitehead",
        "A Berg",
        "W.-Y Lo"
      ],
      "year": "2023",
      "venue": "Segment anything",
      "arxiv": "arXiv:2304.02643"
    },
    {
      "citation_id": "36",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J.-B Alayrac",
        "J Donahue",
        "P Luc",
        "A Miech",
        "I Barr",
        "Y Hasson",
        "K Lenc",
        "A Mensch",
        "K Millican",
        "M Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Zero-shot text-to-image generation",
      "authors": [
        "A Ramesh",
        "M Pavlov",
        "G Goh",
        "S Gray",
        "C Voss",
        "A Radford",
        "M Chen",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "38",
      "title": "A joint speech enhancement and self-supervised representation learning framework for noise-robust speech recognition",
      "authors": [
        "Q.-S Zhu",
        "J Zhang",
        "Z.-Q Zhang",
        "L.-R Dai"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Speech enhancement using selfsupervised pre-trained model and vector quantization",
      "authors": [
        "X.-Y Zhao",
        "Q.-S Zhu",
        "J Zhang"
      ],
      "year": "2022",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "40",
      "title": "Deep learning-based non-intrusive multi-objective speech assessment model with cross-domain features",
      "authors": [
        "R Zezario",
        "S.-W Fu",
        "F Chen",
        "C.-S Fuh",
        "H.-M Wang",
        "Y Tsao"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "DST: Deformable speech transformer for emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Mingling or misalignment? Temporal shift for speech emotion recognition with pre-trained representations",
      "authors": [
        "S Shen",
        "F Liu",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "43",
      "title": "An enroll-to-verify approach for cross-task unseen emotion class recognition",
      "authors": [
        "J.-L Li",
        "C.-C Lee"
      ],
      "venue": "IEEE Transactions on Affective Computing, to be published",
      "doi": "10.1109/TAFFC.2022.3183166"
    },
    {
      "citation_id": "44",
      "title": "Csslm: A contrastive framework for semi-supervised fine-tuning of pre-trained language models",
      "authors": [
        "Y Su",
        "X Han",
        "Y Lin",
        "Z Zhang",
        "Z Liu",
        "P Li",
        "J Zhou",
        "M Sun"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "45",
      "title": "FastBERT: a self-distilling BERT with adaptive inference time",
      "authors": [
        "W Liu",
        "P Zhou",
        "Z Wang",
        "Z Zhao",
        "H Deng",
        "Q Ju"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "46",
      "title": "Domain-adversarial autoencoder with attention based feature level fusion for speech emotion recognition",
      "authors": [
        "Y Gao",
        "J Liu",
        "L Wang",
        "J Dang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "LanSER: Language-model supported speech emotion recognition",
      "authors": [
        "T Gong",
        "J Belanich",
        "K Somandepalli",
        "A Nagrani",
        "B Eoff",
        "B Jou"
      ],
      "year": "2023",
      "venue": "LanSER: Language-model supported speech emotion recognition"
    },
    {
      "citation_id": "48",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Deep pre-trained networks as a feature extractor with xgboost to detect tuberculosis from chest x-ray",
      "authors": [
        "M Rahman",
        "Y Cao",
        "X Sun",
        "B Li",
        "Y Hao"
      ],
      "year": "2021",
      "venue": "Computers & Electrical Engineering"
    },
    {
      "citation_id": "50",
      "title": "Audio-aware spoken multiple-choice question answering with pre-trained language models",
      "authors": [
        "C.-C Kuo",
        "K.-Y Chen",
        "S.-B Luo"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "51",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "52",
      "title": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks",
      "authors": [
        "O Chetia Phukan",
        "A Balaji",
        "R Buduru",
        "Sharma"
      ],
      "year": "2023",
      "venue": "Transforming the embeddings: A lightweight technique for speech emotion recognition tasks"
    },
    {
      "citation_id": "53",
      "title": "The ability of self-supervised speech models for audio representations",
      "authors": [
        "T.-Y Wu",
        "C.-A Li",
        "T.-H Lin",
        "T.-Y Hsu",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "The ability of self-supervised speech models for audio representations",
      "arxiv": "arXiv:2209.12900"
    },
    {
      "citation_id": "54",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "55",
      "title": "Deep sparse rectifier neural networks",
      "authors": [
        "X Glorot",
        "A Bordes",
        "Y Bengio"
      ],
      "year": "2011",
      "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "56",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H Yi Lee"
      ],
      "venue": "SUPERB: Speech processing universal performance benchmark"
    },
    {
      "citation_id": "57",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O.-W Kwon",
        "K Chan",
        "J Hao",
        "T.-W Lee"
      ],
      "year": "2003",
      "venue": "Emotion recognition by speech signals"
    },
    {
      "citation_id": "58",
      "title": "Emotion recognition from speech",
      "authors": [
        "K Venkataramanan",
        "H Rajamohan"
      ],
      "year": "2019",
      "venue": "Emotion recognition from speech",
      "arxiv": "arXiv:1912.10458"
    },
    {
      "citation_id": "59",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "60",
      "title": "SpeechFormer++: A hierarchical efficient framework for paralinguistic speech processing",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "61",
      "title": "LSSED: a largescale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "62",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "63",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "64",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "66",
      "title": "A stochastic approximation method",
      "authors": [
        "H Robbins",
        "S Monro"
      ],
      "year": "1951",
      "venue": "The annals of mathematical statistics"
    },
    {
      "citation_id": "67",
      "title": "Sgdr: Stochastic gradient descent with warm restarts",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2016",
      "venue": "Sgdr: Stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "68",
      "title": "SpeechFormer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2022",
      "venue": "SpeechFormer: A hierarchical efficient framework incorporating the characteristics of speech"
    },
    {
      "citation_id": "69",
      "title": "A dual attention-based modality-collaborative fusion network for emotion recognition",
      "authors": [
        "X Zhang",
        "Y Li"
      ],
      "year": "2023",
      "venue": "A dual attention-based modality-collaborative fusion network for emotion recognition"
    },
    {
      "citation_id": "70",
      "title": "ISNet: Individual standardization network for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "B Cai",
        "X Xing"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "71",
      "title": "DECN: Dialogical emotion correction network for conversational emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "72",
      "title": "Speaker-aware cross-modal fusion architecture for conversational emotion recognition",
      "authors": [
        "H Zhao",
        "B Li",
        "Z Zhang"
      ],
      "year": "2023",
      "venue": "Speaker-aware cross-modal fusion architecture for conversational emotion recognition"
    },
    {
      "citation_id": "73",
      "title": "A deep interpretable representation learning method for speech emotion recognition",
      "authors": [
        "E Jing",
        "Y Liu",
        "Y Chai",
        "J Sun",
        "S Samtani",
        "Y Jiang",
        "Y Qian"
      ],
      "year": "2023",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "74",
      "title": "He is currently working toward the M.E. degree with the School of Electronic and Information Engineering, South China University of Technology. His research interests include speech emotion recognition, multimodal emotion recognition, and deep learning in speech processing",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}