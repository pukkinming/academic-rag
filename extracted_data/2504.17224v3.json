{
  "paper_id": "2504.17224v3",
  "title": "Visual And Textual Prompts In Vllms For Enhancing Emotion Recognition",
  "published": "2025-04-24T03:26:30Z",
  "authors": [
    "Zhifeng Wang",
    "Qixuan Zhang",
    "Peter Zhang",
    "Wenjia Niu",
    "Kaihao Zhang",
    "Ramesh Sankaranarayana",
    "Sabrina Caldwell",
    "Tom Gedeon"
  ],
  "keywords": [
    "Vision Large Language Models",
    "Zero-Shot Emotion Recognition",
    "Multi-Modal Prompting"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Vision Large Language Models (VLLMs) exhibit promising potential for multi-modal understanding, yet their application to video-based emotion recognition remains limited by insufficient spatial and contextual awareness. Traditional approaches, which prioritize isolated facial features, often neglect critical non-verbal cues such as body language, environmental context, and social interactions, leading to reduced robustness in real-world scenarios. To address this gap, we propose Setof-Vision-Text Prompting (SoVTP), a novel framework that enhances zero-shot emotion recognition by integrating spatial annotations (e.g., bounding boxes, facial landmarks), physiological signals (facial action units), and contextual cues (body posture, scene dynamics, others' emotions) into a unified prompting strategy. SoVTP preserves holistic scene information while enabling fine-grained analysis of facial muscle movements and interpersonal dynamics. Extensive experiments show that SoVTP achieves substantial improvements over existing visual prompting methods, demonstrating its effectiveness in enhancing VLLMs' video emotion recognition capabilities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion recognition is a key area in computer vision  [1] , enabling systems to understand and respond to human emotions in applications like virtual reality, HCI  [2] , surveillance  [3] ,  [4] , and mental health  [5] . Robust emotion understanding requires not only analyzing facial expressions but also interpreting body language and contextual interactions-often under challenging visual conditions  [6] ,  [7] -and thus benefits from advanced restoration techniques  [8] . Traditional methods for emotion recognition  [9] ,  [10] ,  [11] ,  [12]  primarily focus on facial features, using handcrafted features or shallow machine learning models  [13] . While these approaches have shown success in controlled environments, they often fail in realworld settings where emotions are influenced by intricate interpersonal dynamics, environmental factors, and cultural variations. Recent studies  [14] ,  [15] ,  [16]  have explored zeroshot prompting techniques to enhance image-language tasks by guiding Large Language Models (LLMs) with specific visual markers. For example, RedCircle  [17]  employs red circles around objects to focus a Vision-Language Model's Fig.  1 . Illustration of the SoVTP prompting for enhancing facial expression recognition in VLLMs. The approach incrementally adds (1) body masks for person identification and tracking, (2) face boxes with numbering to ground and differentiate individuals, (3) facial landmarks to analyze and locate the spatial relationships of facial action units, and (4) facial action units for detailed muscle movement analysis. This visual prompting strategy enables fine-grained emotion recognition by preserving contextual understanding and improving model interpretability in complex scenes. attention on specific regions, improving tasks like zero-shot referring expression comprehension and keypoint localization. Similarly, ReCLIP  [18]  uses a region-scoring approach that combines colorful boxes for cropping and blurring irrelevant areas to enhance performance. Yang et al.  [14]  introduced fine-grained visual prompting with segmentation masks and a 'Blur Reverse Mask' technique that blurs regions outside the target area to preserve spatial context and reduce distractions. While these advances have demonstrated success in tasks like object grounding and segmentation, they face limitations when applied to emotion recognition. Methods like RedCircle and ReCLIP rely heavily on cropping techniques, which can result in the loss of critical contextual information essential for understanding emotions. Similarly, fine-grained visual prompts using Blur Reverse Mask can obscure facial features, introduce ambiguities, and hinder the accurate interpretation of nuanced emotional expressions. Although these methods hold considerable promise, their effectiveness in the domain of emotion recognition has yet to be systematically explored.\n\nTo address these limitations, this paper systematically examines various forms of visual and textual prompting. Specifically, we propose a novel framework, Set-of-Vision-Text Prompting (SoVTP), illustrated in Fig.  1 . SoVTP utilizes spatial information, including numerical labels, bounding boxes, facial landmarks, and action units, to precisely identify Fig.  2 . Comparative analysis of emotion recognition in a complex setting, assessing the accuracy of facial emotion classification using plain text prompts versus SoVTP prompts, which integrates context, others' emotions, body language, and facial muscle movement cues. Top: plain text prompts. Bottom: enhanced results using SoVTP. It demonstrates improved precision that incorporates environmental and social context, as well as facial muscle analysis.\n\ntargets while preserving the broader background context. This approach significantly enhances the zero-shot performance of facial expression recognition, enabling more robust and nuanced emotion analysis. The top part of Fig.  2  presents results derived from plain text prompts, indicating basic contextual information and the overall emotional assessment of individuals, but it yields incorrect results. In contrast, the bottom part of the Fig.  2  shows enhanced emotion recognition outcomes using SoVTP, which incorporates detailed analysis such as facial action units to identify specific muscle movements indicative of happiness. This integrated framework demonstrates improved precision in capturing nuanced emotional states by utilizing environmental, social, body language and anatomical cues, leading to more accurate differentiation between emotions such as 'neutral' and 'happy'.\n\nTo summarize, our main contributions are:\n\n• The paper proposes a novel Set-of-Vision-Text Prompting (SoVTP) approach that preserves holistic scene context rather than relying on isolated facial-cropping methods by overlaying spatial annotations (e.g., face IDs, landmarks, action units) on full-scene inputs, and integrates body posture, environmental cues, and social dynamics to enrich multimodal signals.\n\n• The SoVTP framework introduces a multi-stage prompting that treats zero-shot emotion recognition as structured inference over chained reasoning steps, enabling VLLMs to marginalize intermediate evidence without fine-tuning. • We present comprehensive experimental results that demonstrate SoVTP significantly improves VLLMs' emotion recognition accuracy, achieving substantial gains over existing approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Vision Large Language Models",
      "text": "Recent advances in VLLMs have been inspired by the success of traditional LLMs such as LLaMA  [19] , PaLM  [20]  and GPT-4  [21] , which have shown remarkable abilities in zero-shot generalization in a range of NLP tasks. These LLMs have demonstrated the ability to understand complex instructions and perform diverse tasks without explicit finetuning. The success of these models has motivated the development of VLLMs, which extend these capabilities to the visual domain by using large-scale video-text paired datasets. VideoLLaMA2  [22]  employs an image-level CLIP encoder as the vision backbone with a novel Spatial-Temporal Convolution Connector (STC Connector) for enhanced spatialtemporal representation learning, allowing effective aggregation of frame features while preserving local details without generating excessive video tokens. Video-LLaVA  [23]  unifies visual representations into the language feature space to create a robust VLLM, achieving significant improvements over existing models on both video and image benchmarks by leveraging mixed datasets for mutual enhancement of image and video understanding. Recent advances in action recognition and affective video analysis have laid important groundwork for emotion recognition: compositional action recognition methods that leverage progressive instance-aware feature learning  [24]  and hierarchical graph-based crossinference for group activity recognition  [25]  demonstrate the value of modeling interactions and temporal dynamics, while skeleton-based techniques such as motion-aware mask feature reconstruction  [26] ,  [27]  and multi-granularity anchorcontrastive representation learning in semi-supervised settings  [28]  provide robust strategies for capturing fine-grained body",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Sovtp Prompting Models",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Happy",
      "text": "Step 2 -Others' Emotions",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Step 4-Body Language",
      "text": "To extract and analyze the body language of the masked individual for emotion recognition.   4 ) Body Language, assessing posture and gestures for additional emotional cues. The SoVTP prompting approach integrates these visual and contextual elements to improve accuracy in a complex scene motion cues. In parallel, decade-scale surveys of affective video content analysis  [29]  and frameworks like CogAware  [30] , transformer-based models  [31]  or CLIP-based VLLMs  [32]  for emotion recognition, social media sentiment integration in domain-specific contexts  [33] , and unsupervised wordlevel affect propagation over lexical graphs  [34]  underscore the importance of combining multimodal signals and knowledgedriven semantics. Furthermore, visual emotion analysis networks-such as object-aroused emotion analysis for images  [35]  and user-centered approaches integrating emotional states with visual features  [36] -highlight effective techniques for emotion recognition. Despite these advancements, existing methods still require substantial fine-tuning of their visual and text encoders to adapt to new tasks, especially those involving nuanced visual content like emotion recognition. In contrast, our work proposes a novel zero-shot architecture for emotion recognition, aimed at using the inherent generalization abilities of VLLMs without additional training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Prompting Methods",
      "text": "Prompt engineering is a commonly used approach in NLP  [37] ,  [38] ,  [39] . ToT  [40]  is a novel inference framework that extends the Chain of Thought approach by enabling language models to explore multiple reasoning paths, make deliberate decisions, and evaluate alternatives, significantly enhancing performance in tasks requiring strategic planning or exploration, such as Creative Writing. Although prompts for large language models have been extensively explored, prompts for vision tasks have received less attention and investigation. Liu et al.  [41]  addresses limitations in current prompt tuning methods for vision-language models (VLMs), which use a single prompt and struggle to capture diverse visual information. While using multiple prompts could improve diversity, it risks overfitting and imbalances between base and new task performance. AnyRef  [42]  is a general Multimodal Large Language Model (MLLM) capable of generating pixelwise object perceptions and natural language descriptions from diverse multi-modality references, employing a refocusing mechanism to improve grounding and referring expression tasks without requiring modality-specific adaptations. FGVP  [14]  introduces a new zero-shot framework using pixel-level annotations from a generalist segmentation model, leveraging Blur Reverse Mask as an effective visual prompting strategy to enhance instance-level recognition tasks. While these methods have shown promising results in tasks such as semantic segmentation and object grounding, they often struggle with emotion recognition. This challenge arises from their focus on analyzing objects in isolation, neglecting essential global context and detailed facial features necessary for accurately interpreting emotional expressions. To address these shortcomings, our proposed approach emphasizes the extraction of finegrained facial features across the entire image, maintaining spatial information through the use of spatial annotation such as boxes, numerical labels, facial landmarks, and action units.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Problem Definition",
      "text": "The task involves matching a video to the emotions of a specific visible face within a given video sequences. This requires several steps, including human tracking, face detection, action unit analysis, context extraction, body language assessment, and emotion recognition in video sequences. Typically, VLLMs, represented as Θ, process a video sequence V ∈ R N ×H×W ×3 along with a textual query T i as input. The model outputs an answer A o , representing the emotion, which can be expressed as (Eq. 1):\n\n(\n\nOur task focuses on identifying the best matching videoemotion pairs (V, A o ) for the tracked individual. Traditionally, this process involves tracking the person and cropping their face from the image using face detection methods. However, with the proposed SoVTP prompting, it is now possible to directly highlight facial regions within the entire image, preserving the background context and avoiding the occlusion of faces. The proposed SoVTP approach overlays visual prompts onto facial regions in video sequences while simultaneously updating text prompts for VLLMs. This process transforms the original video sequence V into an enhanced sequence V new = SoV T P (V ), as illustrated in Fig.  2 . The method can be formally expressed as (Eq. 2):\n\n(2)  return visible faces B. Set of Vision Prompts 1) Box detection: After obtaining the video sequence, the next step is to generate visual prompts that will be used by VLLMs for emotion recognition. To achieve this, we utilize the RetinaFace  [43]  algorithm to detect faces within the video sequence. Let B = {b 1 , b 2 , . . . , b n } represent the set of detected face bounding boxes, where b i denotes the bounding box for the i-th face. This process can be expressed mathematically in the Eq. (  3 )\n\nwhere V represents the video sequence, α i denotes the hyperparameters for the RetinaFace model F, and b i corresponds to the bounding box of the i-th face.\n\n2) Face Overlap Handling Algorithm: The face detection algorithm, while effective, often introduces overlaps or conflicts in densely populated video frames, particularly when faces overlap or one face is partially obscured by another. Such scenarios can create confusion for VLLMs, as illustrated in Fig.  4 . To address this issue, we propose a face overlap resolution algorithm, detailed in Algorithm 1.\n\nThe algorithm begins by processing a set of bounding boxes B = {b 1 , b 2 , . . . , b n }. The area of each bounding box b i is calculated, and the bounding boxes are sorted in descending order based on their areas (line 12) as shown in Eq. (  4 ):\n\nwhere Area(b 1 ) ≥ Area(b 2 ) ≥ . . . ≥ Area(b n ), prioritizing larger bounding boxes for further processing. The algorithm then iterates through the sorted list B sorted , examining potential overlaps between the current face F [k] and faces already included in the final set F f inal . Overlap is evaluated using the metric in Eq. (  5 ):\n\nwhere ϵ defines the threshold for significant overlap. F [k] is the current face being considered for inclusion. F [j] is an already accepted face in the final set F f inal . The face F a is set to the current face F [k] only if the overlap with the already accepted face F [j] is less than or equal to ϵ. Otherwise, F a is set to None, meaning the current face is discarded because it is considered occluded or overlaps too much, as determined by Eq. (  6 ):\n\nThen, F a (if not None) is added to the final set F f inal using Eq. (  7 ):\n\nThis process ensures that only larger faces free from obstructions are retained. After resolving overlaps and refining the bounding box set B = {b 1 , b 2 , . . . , b n }, unique IDs N = {1, 2, . . . , n} are assigned to each detected face. These IDs are then used to track faces across the video sequence, enabling distinguishable and interpretable vision prompts for VLLMs. This methodology enhances the model's ability to interpret complex visual scenes. What is the emotion for this group of people?\n\nIs Person 1's face visible? What's the emotion of Person 1?\n\nNo, Person 1's face is not fully visible as she is turned away from the camera, facing the group. Although the face is not fully visible, her posture and partial profile suggest she is engaged and possibly happy, as inferred from the context and interaction with others.\n\nWhat's body language for person 2?\n\nPerson 2 is similarly relaxed and shows a joyful expression. Her posture is open, and she engages directly with those around her, emphasizing her comfort and happiness in the group setting. Her hands hold a cup, contributing to a casual, at-ease stance. What's body language for person 3? 3) Muscle Movement Detection and Analysis: The muscle movement step focuses on analyzing facial muscle activity by detecting and marking key facial action units (AUs). This process begins with the identification of the individual of interest within the scene using face detection models. Once the face is detected, we use Py-Feat  [44]  to extract facial landmarks.\n\nThen we analyze facial muscle deformations through action unit activation. For detected face B, the landmark detector L extracts a set of facial landmarks M = {l 1 , l 2 , . . . , l n } by Eq. (  8 ):\n\nThese landmarks correspond to action units, such as AU1 (Inner Brow Raiser), which are located at facial landmarks  (20, 21, 22, 23, 24, 25) . The detection model D  [44]  outputs the activation scores A = {a 1 , a 2 , . . . , a m } for m action units as defined in Eq. (  9 ):\n\nOnce the activation scores are computed, they are ranked to identify the key prominent action units P ac , based on the initial dominant emotion extracted by Py-Feat. Here, P ac is a subset of A = {a 1 , a 2 , . . . , a m } as defined in Eq. (  10 ):\n\nThe ranked action units P ac are then overlaid onto the original face F at the positions of related facial landmarks to generate a visualization V by Eq. (  11 ):\n\nThis step ensures that nuanced facial expressions, driven by subtle muscle movements, are captured accurately by visualization V . The process is illustrated in Fig.  3 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Text And Vision Prompts 1) Plain Text Prompts:",
      "text": "The left section in Fig.  5  utilizes a plain-text prompt to infer group-level emotions, capturing the collective sentiment of the individuals. It highlights that the group of people shares a joyful and relaxed time together, showcasing a unified emotional atmosphere. This method is effective for identifying general group emotions in shared contexts.\n\n2) Combined Vision-Text Prompts: The right section in Fig.  5  highlights the process of recognizing the emotions of specific individuals within a group setting, utilizing numbered labels to distinguish each person. For example, Person 2 and Person 3 display similar emotions, such as joy and contentment, which are inferred through their body language and interactions with others in the group. By capturing these interpersonal interactions and emotional exchanges, this method offers a more comprehensive analysis of social and emotional behavior within group environments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Final Prompting Strategy",
      "text": "In this section, we introduce preliminaries of reasoning with VLLM prompting. For standard prompting, given the reasoning question Q, prompt T , and a parameterized probabilistic vision large language model p vllm , we aim to maximize the likelihood of an answer A:\n\nwhere a i is the i-th answer token and |A| is the answer length.\n\nFor the proposed multi-stage prompting strategy that involves K reasoning stages, each stage i corresponds to a question-answer pair (Q i , A i ). In addition, we incorporate Q: What's the emotion for the person highlighted with mask by using facial action unit, context, body language and other's emotion information in the video?",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Stage",
      "text": "Selfcorrection C: The facial expression of the person highlighted with a mask for emotion recognition appears to be in a positive mood. They are smiling and seem to be engaged in a conversation with others. The body language suggests that they are comfortable and enjoying the interaction. A: The emotion for the person highlighted with mask is Happy. Fig.  6 . Overview of the multi-stage prompting strategy for emotion recognition using Vision Large Language Models (VLLMs). The approach decomposes the reasoning process into five sequential stages: (1) contextual scene understanding, (2) body language interpretation, (3) inference of emotions from surrounding individuals, (4) detection of facial action units (AUs), and (5) final emotion prediction through self-correction by integrating information from previous stages. Each stage consists of a specific question Q i , intermediate contextual reasoning C i , and answer A i , facilitating a structured chain-ofthought that enhances robustness and accuracy in emotion recognition within complex social environments.\n\nintermediate reasoning steps C i chain-of-thought at each stage. The overall prompt T is thus composed as:\n\n, where\n\n• Q i : the question at stage i (e.g., context, body language, other's emotions, facial action units, final integration), • C i : intermediate reasoning or evidence at stage i, • A i : the answer at stage i. The final answer likelihood is expressed by marginalizing over all possible reasoning trajectories C = {C i } K i=1 :\n\nwith the two factors defined by\n\nwhere c i denotes the i-th step out of the |C| total reasoning steps.\n\nIn the proposed multi-stage SoVTP Prompting, Stage 1 (Context): Q 1 asks for scene context; answer A 1 summarizes setting.Stage 2 (Body Language): Q 2 queries body language; A 2 gives nonverbal cues. Stage 3 (Other's Emotions): Q 3 assesses surrounding individuals' emotions; A 3 characterizes social dynamics. Stage 4 (Facial Action Units): Q 4 identifies key facial action units; A 4 maps to emotional muscle activations. Stage 5 (Self-correction stage): Q 5 integrates prior stages via C i and outputs final emotion A 5 . The whole process can be described in Fig.  6 .\n\nThe final prompting strategy combines visual and textual elements. Visual Prompts: Overlay target face annotations (numbered bounding box, facial landmarks, AUs) onto the frame, preserving background context (Fig.  2  and Fig.  3 ). Textual Prompts include T context , T body , T AU and E others\n\nThe enhanced input (V new , T SoVTP new ) is fed into the VLLM Θ. The model analyzes and critiques body language, context, action units, and others' emotions via chain-of-thought reasoning, then outputs the target emotion A o as:\n\nThis integrated approach ensures the model leverages both fine-grained facial dynamics and holistic contextual information, addressing the limitations of prior methods that rely on isolated facial cropping.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments A. Models And Settings",
      "text": "Our approach does not require training any models. We evaluate the proposed method's performance in a zero-shot setting using VLLMs. Specifically, we evaluate the capabilities of several advanced VLLMs, including commercial models such as Qwen2-VL-7B-Instruct  [45]",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Dataset Details",
      "text": "We collect the original videos from the ABC News website  7  . After collection, we conduct detailed preprocessing to ensure data quality such as removing duplicate and blurry videos. To reduce the effort and cost associated with manual data annotation, we utilize DeepFace  [49]  for initial emotion labeling. These labels are then reviewed and refined by two human annotators. To ensure the accuracy of the annotations, a third annotator with expertise in psychology validate the facial expression labels based on their domain knowledge. This comprehensive process ensures high-quality annotations, which are essential for constructing reliable benchmarks. The dataset in Table  II  for evaluating the zero-shot emotion recognition model is divided into three tiers of increasing difficulty: Easy, Medium, and Hard. The Easy tier comprises videos featuring a larger number of faces per frame, allowing the model to leverage contextual emotional cues from multiple individuals. The Medium tier includes videos with a moderate number of faces per frame, while the Hard tier contains videos with very few faces, posing a greater challenge for the model to interpret emotions due to limited social or contextual information. Across all tiers, the dataset spans a diverse collection of videos and frames, all standardized to the same resolution.   [46] , VIDEOLLAMA2  [22] , VILA  [47] , QWEN2-VL  [45] , VIDEO-LLAVA  [23] , LLAVA-NEXT  [48] , AND THE PROPOSED SOVTP-ENHANCED LLAVA-NEXT MODEL AND QWEN2-VL MODEL. RESULTS ARE REPORTED FOR ACCURACY (ACC%) AND F1-SCORE (F@1) ACROSS DATASETS WITH EASY, MEDIUM, AND HARD DIFFICULTY LEVELS.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Methods",
      "text": "Backbone Easy Medium Hard Total Acc(%) F@1 Acc(%) F@1 Acc(%) F@1 Acc(%) F@1\n\nShareGPT4V  LLaVA-NeXT  [48] , Video-LLAVA  [23] , and ShareGPT4V  [46] .\n\nIt encompasses a range of emotions, including Surprise, Fear, Disgust, Anger, Happiness, Sadness, and Neutral. Performance is assessed using Accuracy and F1-Score as key metrics, ensuring a balanced evaluation of the model's capability to detect and classify emotions. The consistent application of structured prompts across difficulty levels ensures a fair and unified testing framework, emphasizing the model's zero-shot adaptability-its ability to generalize without prior exposure to the dataset during training. The tiered structure reflects how the number of visible faces influences task complexity, with fewer faces demanding more nuanced emotion recognition from limited visual data. DFEW  [50]  is a large-scale, unconstrained video database specifically designed to advance dynamic facial expression recognition in real-world conditions. It comprises 16,372 clips extracted from over 1,500 high-definition movies, each depicting one of the seven basic emotions-anger, disgust, fear, happiness, neutral, sadness, and surprise-under challenging interferences such as extreme illumination, occlusions, and pose variations. It offer a platform for the development of spatiotemporal deep learning methods for dynamic facial expression recognition in the wild.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Action Units",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Quantitative Results",
      "text": "As shown in Table  I , incorporating SoVTP into existing VLLMs yields substantial gains in zero-shot emotion recognition across all difficulty tiers. Qwen2-VL+SoVTP achieves 70.59% accuracy and 58.00% F@1 on the Easy subset, 48.00% accuracy and 44.94% F@1 on the Medium subset, and 38.57% accuracy and 28.25% F@1 on the Hard subset-dramatically Fig.  8 . Performance comparison of various VLLMs across seven emotion categories in zero-shot emotion recognition on DFEW dataset  [50] . The bar chart depicts the accuracy of VLLMs models including Qwen2-VL+Ours, Qwen2-VL  [45] , Video-LLAVA  [23] , VILA  [47]  and ShareGPT4V  [46] .\n\noutperforming the strongest baselines (e.g., ShareGPT4V: 41.18%/14.58%, 28.00%/12.06%, and 27.14%/10.65%, respectively). On the overall benchmark, Qwen2-VL+SoVTP reaches 45.54% accuracy and 33.53% F@1, improving total accuracy by 16.08% and F1 by 15.25% over the best non-SoVTP model. LLaVA-NeXT+SoVTP similarly boosts performance, with overall accuracy rising to 33.93% (from 17.86%) and F@1 to 23.84% (from 11.05%). These results underscore the efficacy of SoVTP in improving generalization across varying facial density contexts, with substantial gains in both recognition accuracy and robustness. Fig.  7  compares the zero-shot emotion recognition performance of various Vision-Language Large Models (VLLMs) across seven emotion categories: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutral. The proposed Qwen2-VL+Ours framework achieves the highest accuracy in most categories, with particularly notable performance in Happy (72%), Surprise (54%), and Neutral (44%). In contrast, baseline models exhibit significant limitations: Qwen2-VL attains moderate accuracy in Happy (35%) and Neutral (31%) but fails to detect Angry, Fear, Sad and Surprise (0% accuracy). LLaVA-NeXT and Video-LLaVA show marginal performance, with peak accuracies of 44% (Sad) and 24% (Disgust), respectively, while ShareGPT4V demonstrates minimal efficacy across all categories (≤ 10% F@1). The proposed method also addresses critical gaps in underrepresented emotions, achieving 53% accuracy in Angry, 27% accuracy in Disgust and 54% in Sad, where baseline models consistently score ≤ 14%. These results highlight the superior generalization of the Qwen2-VL+Ours framework, particularly in capturing nuanced emotional states, and underscore the limitations of existing VLLMs in handling zero-shot emotion recognition for low-frequency or complex emotions.\n\nTable IV summarizes the zero-shot emotion recognition performance of several vision-language models on the DFEW dataset, reporting overall accuracy and F@1 scores. Baseline methods such as ShareGPT4V, VILA, Video-LLAVA, Qwen2-VL, and LLAVA-NeXT achieve accuracies ranging from approximately 24.70% to 40.80% and corresponding F@1 scores of 16.22% to 35.28%. By contrast, the SoVTP-enhanced Qwen2-VL and LLAVA-NeXT attain substantially higher results, with Qwen2-VL+SoVTP reaching 42.30% accuracy and 42.55% F@1 and LLAVA-NeXT+SoVTP achieving 46.50% accuracy and 46.79% F@1. Fig.  8  further illustrates these gains at the level of individual emotion categories (Happy, Angry, Sad, Neutral, Surprise, Fear, Disgust), showing that SoVTP-enhanced models consistently outperform their base counterparts across all categories-particularly for Happy, Angry and Surprise-thereby demonstrating the effectiveness of holistic scene prompting in improving zero-shot recognition robustness on a diverse set of emotional expressions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Visual Prompting",
      "text": "Table V presents a comparative analysis of state-of-the-art (SOTA) methods for zero-shot emotion recognition, evaluating performance across Easy, Medium, and Hard difficulty tiers using Accuracy (Acc) and F1-score (F@1) metrics. It compare the effectiveness of different visual prompting strategies utilized by Qwen2-VL. The proposed SoVTP (Ours) method, which integrates numeric annotations (N), bounding boxes (O), and facial action units (A) as visual prompts, achieves superior performance, significantly outperforming existing approaches. On the Easy tier, SoVTP attains 70.59% Accuracy and 58.00% F@1, a substantial improvement over the secondbest method, ReCLIP (35.29% Acc, 24.85% F@1). Similarly, in the Medium tier, SoVTP achieves 48.00% Accuracy and 44.94% F@1, surpassing RedCircle (36.00% Acc, 33.20% F@1) and ReCLIP (36.00% Acc, 32.29% F@1). For the challenging Hard tier, SoVTP maintains robustness with 38.57% Accuracy and 28.25% F@1, outperforming ReCLIP (34.29% Accuracy, 20.75% F@1) and RedCircle (31.43% Acc, 17.71% F@1). In total metrics, SoVTP achieves 45.54% Accuracy and 33.53% F@1, representing a 10.72% absolute Accuracy gain over ReCLIP (34.82% Accuracy, 23.13% F@1) and a 13.40% Accuracy gain over RedCircle (32.14% Accuracy, 19.33% F@1). Notably, the baseline method (Qwen2-VL) with plain text prompts lags significantly across all tiers (Total: 23.21% Accuracy, 10.06% F@1), underscoring the critical role of structured visual prompts. The comparatively lower performance of RedCircle and ReCLIP can be attributed to their reliance on cropping and blurring techniques. This approach increases the model's focus on specific facial features but sacrifices essential body language and contextual cues, which are vital in more complex situations where emotional cues extend beyond the face. In contrast, the SoVTP approach retains a broader field of view, capturing body language and contextual information. This holistic approach accounts for the significant performance gains observed with SoVTP, especially in challenging conditions, where both facial and contextual cues are crucial for accurate emotion recognition. Fig.  9  illustrates a comparative analysis of state-of-the-art (SOTA) visual prompting techniques for zero-shot emotion recognition, contrasting RedCircle, ReCLIP, and the proposed SoVTP framework. RedCircle and ReCLIP employ attentionlocalization strategies by blurring non-facial regions to isolate facial features (e.g., eyes, mouth), prioritizing expressioncentric analysis. While this approach enhances focus on microexpressions, it eliminates contextual cues such as body posture, hand gestures, and environmental interactions, which are critical for interpreting emotions in socially complex scenarios. In contrast, SoVTP retains the full scene without spatial or contextual obfuscation, preserving both facial features and holistic environmental information. This enables the model to integrate multi-modal cues-such as body language, spatial relationships between individuals, and scene dynamics-into its emotion recognition pipeline. For instance, the image depicts a nuclear family of four-mother, father, daughter and son-seated around a well-lit kitchen dining table laden with a variety of home-cooked dishes. The parents sit opposite each other, smiling warmly as they engage with their children: the mother leans in attentively toward her son, who appears to be speaking or laughing, while the father holds a fork mid-air, looking toward his daughter with a gentle, encouraging expression. The daughter, seated at the table's end, gazes at her parents with an open, cheerful countenance. The overall scene conveys a relaxed, intimate mealtime atmosphere, with each family member exhibiting positive affect-smiles, eye contact, and open body language-suggesting feelings of warmth, belonging, and mutual enjoyment. However, the contextual cues are obscured in RedCircle and ReCLIP, they remain visible in SoVTP, allowing the model to infer emotions like happiness or mutual enjoyment more accurately. The figure underscores SoVTP's methodological advantage in scenarios requiring contextual awareness (e.g., group interactions, ambiguous expressions), where auxiliary non-verbal signals significantly augment recognition robustness. Quantitative improvements observed in prior experiments (e.g., 45.54% total accuracy for SoVTP vs. ≤ 34.82% for SOTA baselines) align with this qualitative analysis, validating the importance of scene preservation for generalizable emotion inference. The comparison highlights a paradigm shift from isolated facial analysis to context-aware modeling, addressing a critical limitation in existing visual prompting frameworks. Fig.  10  presents a comparative bar chart evaluating the emotion recognition accuracy of three visual prompting methods: SoVTP (Ours), ReCLIP, and RedCircle, across distinct emotion categories. The chart highlights SoVTP's superior performance, particularly in complex or context-dependent emotions such as Angry, Disgust, Happy and Sad, where it achieves markedly higher accuracy compared to baseline methods. For instance, SoVTP demonstrates robust performance in Happiness (e.g., 72% accuracy) and Sad ( 54%), leveraging its scene-preservation strategy to incorporate contextual cues like body posture and environmental interactions. In contrast, ReCLIP and RedCircle, which rely on facialregion isolation (e.g., blurring non-face areas), exhibit reduced accuracy in these categories (e.g., ≤ 54% for Happiness in ReCLIP, ≤ 35% for Sad in RedCircle), as their localized focus neglects auxiliary non-verbal signals. Notably, SoVTP also addresses critical gaps in underrepresented emotions like Disgust, achieving non-zero accuracy (e.g., 30% for Disgust), whereas baseline methods fail entirely (10% accuracy) due to their inability to interpret contextual or physiological cues (e.g., facial action units, spatial relationships). The progressive decline in accuracy for all methods from high-frequency emotions (e.g.,Happy, Sad) to low-frequency or ambiguous ones (e.g., Angry, Disgust) underscores the inherent challenges of zero-shot emotion recognition. However, SoVTP mitigates this trend through its holistic integration of facial features, Muscle: Based on the presence of AU06 (Cheek raise), AU12 (Lip corner pull up), and AU25 (Lip parting), the emotion selected for the person highlighted with the mask is Happy.\n\nMuscle+Context: Based on the facial action units (AU) and the context provided, the person highlighted with the mask appears to be expressing happiness.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Muslce+Context+Body:",
      "text": "The combination of AU06 (cheek raise), AU12 (lip corner pull up), and AU25 (lip parting) along with the positive body language and context suggests that the emotion is Happy. SoVTP: The facial expression of the person highlighted with the mask for emotion recognition appears to be a combination of happiness and engagement. The cheeks are raised (AU06), the lips are parted (AU25), and the corners of the lips are pulled up (AU12), which are all indicators of happiness. Additionally, the person is looking towards the camera, which suggests engagement and attentiveness. Based on the facial action units and body language, the emotion selected for the person highlighted with the mask is Happy. body language, and scene dynamics, validating its methodological advancement in balancing specificity and contextual awareness.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "F. Ablation Study",
      "text": "Table VI presents an ablation study of vision and text prompt components on the Qwen2-VL framework, evaluating the incremental impact of integrating multi-modal cues for zero-shot emotion recognition. The baseline model, devoid of additional prompts, achieves minimal performance (Total: 23.21% Acc, 10.06% F@1), particularly struggling in Easy (11.76% Acc) and Medium (20.00% Acc) tiers. Introducing Muscle prompts (facial action units) improves total accuracy to 30.36% and F@1 to 19.65%, with notable gains in Hard scenarios (35.71% Acc vs. baseline's 27.14%). However, Muscle+Context prompts yield mixed results: while Medium accuracy surges to 40.00% (F@1: 33.50%), Easy performance drops to 17.65% Acc, suggesting contextual cues alone may introduce noise without complementary modalities. The integration of Muscle+Context+Body prompts, which combine facial action units (physiological cues), environmental context, and body language, significantly enhances model robustness, achieving 41.07% total accuracy and 31.81% F@1. This configuration demonstrates peak performance in the Easy tier (52.94% Acc, 49.95% F@1), underscoring the synergistic value of multi-modal integration. However, the proposed SoVTP (Ours), which further incorporates others' emotions into the prompt ensemble, achieves state-of-the-art results: 70.59% Acc (Easy), 48.00% Acc (Medium), and 45.54% total Acc (33.53% F@1). Notably, while SoVTP matches Mus-cle+Context+Body in Hard scenarios (38.57% Acc), its F@1 slightly declines (28.25% vs. 28.50%). This marginal dip reflects the inherent challenge of the Hard tier, where videos contain sparse or ambiguous social cues (e.g., limited visible faces or interactions). Introducing others' emotions in such scenarios risks introducing noise due to insufficient emotional cues, slightly reducing precision. Nevertheless, SoVTP's holistic design-integrating physiological, environmental, behavioral, and social-emotional cues-achieves superior generalization overall, validating its efficacy in balancing specificity and robustness across diverse emotion recognition tasks. The study highlights the critical role of progressive integration: each added modality (action units → context → body language → others' emotions) systematically enhances model robustness.\n\nThis Finally, SoVTP (Ours) synthesizes AU data, contextual signals, body language, and social engagement cues (e.g., gaze direction toward the camera, indicative of attentiveness) to produce a holistic emotion profile. While all tiers converge on Happy, SoVTP distinguishes itself by capturing nuanced behavioral dynamics (e.g., engagement) that refine specificity. For instance, the combined presence of AU06, AU12, and  AU25, alongside directed gaze and contextually aligned body language, resolves ambiguities that might arise from isolated AU analysis (e.g., distinguishing genuine happiness from social masking). This incremental framework underscores the necessity of integrating physiological, environmental, behavioral, and social-interactive cues for robust emotion recognition. The case study exemplifies how SoVTP transcends traditional facial-centric models, addressing limitations in scenarios where emotions are contextually or socially mediated. By systematically layering cues, the framework mitigates false positives and enhances interpretability, demonstrating its methodological superiority in real-world applications requiring nuanced affective computing.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "G. Inference Time",
      "text": "From Table  VII , we observe that richer visual and textural prompts progressively increase inference time-from 1.44s for the baseline with plain text prompt to 3.97s for SoVTP-while yielding steadily higher accuracy (23.31% to 45.54%) and F1 scores (10.06% to 33.53%). Initial additions (e.g., Muscle, Muscle+Context+Body) deliver substantial absolute gains relative to added latency, but the final step to SoVTP shows diminishing returns: a modest 4.5 percentage-point accuracy increase for an extra 0.8s of inference. We believe this tradeoff is acceptable in many applications where higher recognition performance is critical. Moreover, our ablation offers intermediate options: if lower latency is required, one may Acc (%)   H. Impacts of key hyperparameters 1) Impacts of face overlap threshold: Table VIII evaluates the robustness of Qwen2-VL combined with SoVTP prompting across varying crowd densities-categorized by the number of faces (N > 6, 3 < N <= 6, and N <= 3)-and different face overlap thresholds (ϵ = 0.0, 0.2, 0.4). For each density regime, we report accuracy (Acc %) and F@1 scores at increasing overlap tolerances. The results show that performance is highest in high-density settings (N > 6) with no overlap (ϵ = 0.0) and degrades as ϵ increases, reflecting sensitivity to occlusion and overlapping detections. Medium-and low-density scenarios exhibit lower absolute metrics overall and similarly decline under larger overlap thresholds. These findings highlight both the benefits of richer others' emotional cues when many faces are present, illustrating SoVTP's behavior under different crowd and occlusion conditions.\n\n2) Ablation study of pixel size and maximum token: Fig.  12  presents a systematic ablation of two critical SoVTP hyperparameters on Qwen2-VL's zero-shot emotion recognition accuracy. In panel (a), varying the maximum pixel size from (90,105) to (1440,1680) yields a sharp rise in accuracy-from approximately 44.6% at the smallest pixel size to 45.54% at (180,210)-after which performance plateaus around 45.54%, indicating diminishing returns for very large pixel-group ranges. Panel (b) shows that increasing the maximum output token limit progressively improves accuracy, climbing from 34.8% at 128 tokens to a peak of 45.5% at 1024 tokens, and C: The facial expression of the person highlighted with a mask for emotion recognition appears to be one of distress or discomfort. The furrow in the brow and the slightly hunched posture suggest concern or worry. Given the context of the scene, which involves a call to action or a moment of defiance. A: The emotion that best fits the person's expression is likely to be \"Sad\" or \"Neutral\" as they might be feeling overwhelmed or conflicted by the situation.  C: Based on the analysis of the facial expression, body language, and context, the person highlighted with a mask appears to be experiencing a state of distress or discomfort. The combination of their body language (standing with hands on hips) and facial expression (conveying worry or concern) suggests that they might be feeling anxious or worried about their situation. Given the context of the hospital or medical setting, it is likely that they are experiencing a medical condition or injury that is causing them distress. A: The emotion selected for the person highlighted with mask is Sad.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "(C) Sad",
      "text": "C: The highlighted person appears to be in a crowded and chaotic environment, possibly a disaster or emergency situation. The body language suggests that they are running or moving quickly, possibly in response to an emergency or urgent situation. The raised inner and outer brows (AU01, AU02), lowered brow (AU04), raised upper lid (AU05), tightened lids (AU07), and stretched lips (AU20) are indicative of fear. The context of a chaotic and urgent environment, combined with the individual's quick movement and protective mask, further supports the conclusion that the person is experiencing fear. A: Therefore, the emotion selected for the person highlighted with the mask is Fear.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "(B) Fear",
      "text": "C: The person highlighted with the mask appears to be experiencing a state of distress or discomfort. The body language suggests that they might be experiencing pain or discomfort, possibly due to an injury or illness. The person's facial expression and body posture indicate a sense of discomfort or unease. A: Based on the analysis, the emotion selected for the person highlighted with the mask is Sad.\n\n(f) Fear then declining slightly to 42.8% at 2048 tokens, suggesting an optimal token budget of around 1024.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "I. Qualitative Observations",
      "text": "The Fig.  13  compares two prompting methods: SoVTP prompting (Top) and mask-based prompting (Bottom). In the SoVTP prompting approach, the model effectively highlights individual faces with numbered bounding boxes, ensuring that facial expressions remain unobscured and visually clear. This enables a nuanced analysis of social interactions and body language, such as identifying warm, genuine smiles, caring gestures, and expressions of joy. Such clarity facilitates precise emotion recognition and captures the dynamic interpersonal interactions within the scene. In contrast, the maskbased prompting approach overlays segmentation masks on the image, obscuring detailed facial features and expressions. While this methodology is useful for general scene-level interpretations, such as categorizing the setting as a 'dining area' or recognizing activities like eating, it limits the ability to discern fine-grained emotional cues due to the occlusion of critical facial details. Consequently, the mask-based approach is more suitable for scene descriptions rather than emotional analysis.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "J. Limitations",
      "text": "In Fig.  14 , the SoVTP method succeeds in most examples by leveraging multi-person emotional states, action units, body posture, and contextual cues-for example, correctly identifying 'Fear' in a chaotic scene with visible widened eyes and tense posture, or 'Happy' in a group setting with open eyes and raised mouth corners-demonstrating its holistic reasoning. However, in subfigures (a) and (f), the target faces are blurred or occluded, providing limited context and preventing reliable extraction of facial details and action units; this lack of critical fine-grained cues leads to incorrect emotion labels. Future work could incorporate mechanisms to recover or compensate for missing facial details, such as applying generative inpainting models to approximate occluded regions or using deblurring techniques to enhance video quality when fine-grained cues are unreliable.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper introduced Set-of-Vision-Text Prompting (SoVTP), a novel multi-modal framework that significantly enhances zero-shot emotion recognition in Vision Large Language Models (VLLMs) by integrating spatial, physiological, contextual, and social-interactive cues. By systematically combining facial action units (AUs), body language, scene context, and others' emotions into a unified prompting strategy, SoVTP addresses the limitations of traditional facial-centric approaches, which often neglect critical non-verbal and environmental signals. Extensive experiments across a real-world benchmark dataset-spanning Easy, Medium, and Hard difficulty tiers-demonstrate that SoVTP achieves competitive performance. Ablation studies validate the necessity of progressive multi-modal integration, revealing that each added cue (AUs → context → body language → others' emotions) incrementally enhances model robustness. Qualitative analyses further highlight SoVTP's superiority in preserving holistic scene information, enabling nuanced interpretation of socially mediated emotions-such as distinguishing genuine happiness from social masking through gaze direction and group dynamics. While the framework exhibits minor performance dips in Hard tiers due to sparse emotional cues, its design mitigates these challenges through balanced specificity and contextual awareness. Future work could explore adaptive cue weighting to optimize performance in data-sparse environments and extend SoVTP to cross-cultural emotion recognition. The release of our benchmark dataset and code provides a foundation for advancing research in context-aware affective computing. By bridging the gap between isolated facial analysis and holistic scene understanding, SoVTP establishes a new paradigm for emotion recognition in real-world, socially complex applications.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of the SoVTP prompting for enhancing facial",
      "page": 1
    },
    {
      "caption": "Figure 1: SoVTP uti-",
      "page": 1
    },
    {
      "caption": "Figure 2: Comparative analysis of emotion recognition in a complex setting, assessing the accuracy of facial emotion classification using plain text prompts",
      "page": 2
    },
    {
      "caption": "Figure 2: presents results",
      "page": 2
    },
    {
      "caption": "Figure 2: shows enhanced emotion recognition outcomes using",
      "page": 2
    },
    {
      "caption": "Figure 3: Overview of the proposed SoVTP framework for emotion recognition, illustrating a multi-step approach combining vision and text prompts. The",
      "page": 3
    },
    {
      "caption": "Figure 2: The method",
      "page": 4
    },
    {
      "caption": "Figure 4: Face detection often leads to overlaps or conflicts that can",
      "page": 4
    },
    {
      "caption": "Figure 4: To address this issue, we propose a face overlap",
      "page": 4
    },
    {
      "caption": "Figure 5: The figure presents two prompting approaches for emotion recognition within a social group. Left: It utilizes a plain-text prompt to infer group-level",
      "page": 5
    },
    {
      "caption": "Figure 3: C. Text and Vision Prompts",
      "page": 5
    },
    {
      "caption": "Figure 5: highlights the process of recognizing the emotions of",
      "page": 5
    },
    {
      "caption": "Figure 6: Overview of the multi-stage prompting strategy for emotion recogni-",
      "page": 6
    },
    {
      "caption": "Figure 6: The final prompting strategy combines visual and textual",
      "page": 6
    },
    {
      "caption": "Figure 2: and Fig. 3).",
      "page": 6
    },
    {
      "caption": "Figure 7: Performance comparison of various VLLMs across seven emotion",
      "page": 7
    },
    {
      "caption": "Figure 8: Performance comparison of various VLLMs across seven emotion",
      "page": 8
    },
    {
      "caption": "Figure 7: compares the zero-shot emotion recognition perfor-",
      "page": 8
    },
    {
      "caption": "Figure 8: further illustrates these",
      "page": 8
    },
    {
      "caption": "Figure 9: Comparison of SOTA Visual Prompting Techniques in Emotion",
      "page": 9
    },
    {
      "caption": "Figure 10: The bar chart displays the accuracy of different visual prompting",
      "page": 9
    },
    {
      "caption": "Figure 9: illustrates a comparative analysis of state-of-the-art",
      "page": 9
    },
    {
      "caption": "Figure 10: presents a comparative bar chart evaluating the",
      "page": 9
    },
    {
      "caption": "Figure 11: Ablation Study of Incremental Multi-Modal Cue Integration in",
      "page": 10
    },
    {
      "caption": "Figure 11: delineates",
      "page": 10
    },
    {
      "caption": "Figure 12: Ablation study of key hyperparameters. We evaluate the impact of",
      "page": 11
    },
    {
      "caption": "Figure 13: Comparative Analysis of SoVTP Prompting and Mask Prompting for",
      "page": 11
    },
    {
      "caption": "Figure 12: presents a systematic ablation of two critical SoVTP hy-",
      "page": 11
    },
    {
      "caption": "Figure 14: Visualize the SoVTP-enhanced Qwen2-VL generated samples for different expressions: the left side shows a target individual highlighted by a",
      "page": 12
    },
    {
      "caption": "Figure 13: compares two prompting methods: SoVTP",
      "page": 12
    },
    {
      "caption": "Figure 14: , the SoVTP method succeeds in most examples",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\n2": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Basic Expressions": "Surprise",
          "Involved Action Units": "AU 1, 2, 4, 5, 25, 26",
          "Action Units Description": "Inner Brow Raiser, Outer Brow Raiser, Brow\nLowerer, Upper Lid Raiser, Lip Part, Jaw Drop"
        },
        {
          "Basic Expressions": "Fear",
          "Involved Action Units": "AU 1, 2, 4, 5, 7, 11, 20, 25,\n26",
          "Action Units Description": "Inner Brow Raiser, Outer Brow Raiser, Brow\nLowerer, Upper Lid Raiser, Lid Tightener, Na-\nsolabial Deepener, Lip Stretcher, Lip Part,\nJaw\nDrop"
        },
        {
          "Basic Expressions": "Disgust",
          "Involved Action Units": "AU 6, 9, 11, 15, 17",
          "Action Units Description": "Cheek Raiser, Nose Wrinkler, Nasolabial Deep-\nener, Lip Corner Depressor, Chin Raiser"
        },
        {
          "Basic Expressions": "Anger",
          "Involved Action Units": "AU 4, 5, 7, 23",
          "Action Units Description": "Brow Lowerer, Upper Lid Raiser, Lid Tightener"
        },
        {
          "Basic Expressions": "Happiness",
          "Involved Action Units": "AU 6, 12, 25",
          "Action Units Description": "Cheek Raiser, Lip Corner Puller, Lip Part"
        },
        {
          "Basic Expressions": "Sadness",
          "Involved Action Units": "AU 1, 4, 15",
          "Action Units Description": "Inner Brow Raiser, Brow Lowerer, Lip Corner\nDepressor"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multimodal decoupled distillation graph neural network for emotion recognition in conversation",
      "authors": [
        "Y Dai",
        "Y Li",
        "D Chen",
        "J Li",
        "G Lu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "2",
      "title": "Lrdif: Diffusion models for under-display camera emotion recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "3",
      "title": "Surveillance video-and-language understanding: from small to large multimodal models",
      "authors": [
        "T Yuan",
        "X Zhang",
        "B Liu",
        "K Liu",
        "J Jin",
        "Z Jiao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "4",
      "title": "Geometric-aware facial landmark emotion recognition",
      "authors": [
        "Q Zhang",
        "Z Wang",
        "Y Liu",
        "Z Qin",
        "K Zhang",
        "T Gedeon"
      ],
      "year": "2023",
      "venue": "2023 6th International Conference on Software Engineering and Computer Science (CSECS)"
    },
    {
      "citation_id": "5",
      "title": "Depformer: Multimodal depression recognition based on facial expressions and audio features via emotional changes",
      "authors": [
        "J Ye",
        "Y Yu",
        "L Lu",
        "H Wang",
        "Y Zheng",
        "Y Liu",
        "Q Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "6",
      "title": "Enhanced spatiotemporal interaction learning for video deraining: faster and better",
      "authors": [
        "K Zhang",
        "D Li",
        "W Luo",
        "W Ren",
        "W Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "7",
      "title": "Deep dense multi-scale network for snow removal using semantic and depth priors",
      "authors": [
        "K Zhang",
        "R Li",
        "Y Yu",
        "W Luo",
        "C Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "8",
      "title": "Adversarial spatio-temporal learning for video deblurring",
      "authors": [
        "K Zhang",
        "W Luo",
        "Y Zhong",
        "L Ma",
        "W Liu",
        "H Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "9",
      "title": "Context de-confounded emotion recognition",
      "authors": [
        "D Yang",
        "Z Chen",
        "Y Wang",
        "S Wang",
        "M Li",
        "S Liu",
        "X Zhao",
        "S Huang",
        "Z Dong",
        "P Zhai"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Micron-bert: Bert-based facial micro-expression recognition",
      "authors": [
        "X.-B Nguyen",
        "C Duong",
        "X Li",
        "S Gauch",
        "H.-S Seo",
        "K Luu"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "11",
      "title": "Lldif: Diffusion models for low-light facial expression recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "International Conference on Pattern Recognition"
    },
    {
      "citation_id": "12",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "13",
      "title": "Htnet for microexpression recognition",
      "authors": [
        "Z Wang",
        "K Zhang",
        "W Luo",
        "R Sankaranarayana"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "14",
      "title": "Fine-grained visual prompting",
      "authors": [
        "L Yang",
        "Y Wang",
        "X Li",
        "X Wang",
        "J Yang"
      ],
      "year": "2024",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "15",
      "title": "Segment everything everywhere all at once",
      "authors": [
        "X Zou",
        "J Yang",
        "H Zhang",
        "F Li",
        "L Li",
        "J Wang",
        "L Wang",
        "J Gao",
        "Y Lee"
      ],
      "year": "2024",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "16",
      "title": "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v",
      "authors": [
        "J Yang",
        "H Zhang",
        "F Li",
        "X Zou",
        "C Li",
        "J Gao"
      ],
      "year": "2023",
      "venue": "Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v",
      "arxiv": "arXiv:2310.11441"
    },
    {
      "citation_id": "17",
      "title": "What does clip know about a red circle? visual prompt engineering for vlms",
      "authors": [
        "A Shtedritski",
        "C Rupprecht",
        "A Vedaldi"
      ],
      "year": "2023",
      "venue": "ICCV"
    },
    {
      "citation_id": "18",
      "title": "Reclip: A strong zero-shot baseline for referring expression comprehension",
      "authors": [
        "S Subramanian",
        "W Merrill",
        "T Darrell",
        "M Gardner",
        "S Singh",
        "A Rohrbach"
      ],
      "year": "2022",
      "venue": "ACL"
    },
    {
      "citation_id": "19",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "20",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "A Chowdhery",
        "S Narang",
        "J Devlin",
        "M Bosma",
        "G Mishra",
        "A Roberts",
        "P Barham",
        "H Chung",
        "C Sutton",
        "S Gehrmann"
      ],
      "year": "2023",
      "venue": "JMLR"
    },
    {
      "citation_id": "21",
      "title": "GPT-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "22",
      "title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in videollms",
      "authors": [
        "Z Cheng",
        "S Leng",
        "H Zhang",
        "Y Xin",
        "X Li",
        "G Chen",
        "Y Zhu",
        "W Zhang",
        "Z Luo",
        "D Zhao",
        "L Bing"
      ],
      "year": "2024",
      "venue": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in videollms",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "23",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "B Lin",
        "B Zhu",
        "Y Ye",
        "M Ning",
        "P Jin",
        "L Yuan"
      ],
      "year": "2008",
      "venue": "EMNLP"
    },
    {
      "citation_id": "24",
      "title": "Progressive instanceaware feature learning for compositional action recognition",
      "authors": [
        "R Yan",
        "L Xie",
        "X Shu",
        "L Zhang",
        "J Tang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Higcin: Hierarchical graph-based cross inference network for group activity recognition",
      "authors": [
        "R Yan",
        "L Xie",
        "J Tang",
        "X Shu",
        "Q Tian"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "26",
      "title": "Motion-aware mask feature reconstruction for skeleton-based action recognition",
      "authors": [
        "X Zhu",
        "X Shu",
        "J Tang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "27",
      "title": "Dm-hap: Diffusion model for accurate hand pose prediction",
      "authors": [
        "Z Wang",
        "K Zhang",
        "R Sankaranarayana"
      ],
      "year": "2025",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "28",
      "title": "Multi-granularity anchorcontrastive representation learning for semi-supervised skeleton-based action recognition",
      "authors": [
        "X Shu",
        "B Xu",
        "L Zhang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Affective video content analysis: Decade review and new perspectives",
      "authors": [
        "J Xue",
        "J Wang",
        "X Liu",
        "Q Zhang",
        "X Wu"
      ],
      "year": "2024",
      "venue": "Big Data Mining and Analytics"
    },
    {
      "citation_id": "30",
      "title": "Cogaware: Cognitionaware framework for sentiment analysis with textual representations",
      "authors": [
        "Z Zhang",
        "C Wu",
        "H Chen",
        "H Chen"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "31",
      "title": "Vllms provide better context for emotion understanding through common sense reasoning",
      "authors": [
        "A Xenos",
        "N Foteinopoulou",
        "I Ntinou",
        "I Patras",
        "G Tzimiropoulos"
      ],
      "year": "2024",
      "venue": "Vllms provide better context for emotion understanding through common sense reasoning",
      "arxiv": "arXiv:2404.07078"
    },
    {
      "citation_id": "32",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "S Zhang",
        "Y Pan",
        "J Wang"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "33",
      "title": "Sentiment analysis on twitter data integrating textblob and deep learning models: The case of us airline industry",
      "authors": [
        "W Aljedaani",
        "F Rustam",
        "M Mkaouer",
        "A Ghallab",
        "V Rupapara",
        "P Washington",
        "E Lee",
        "I Ashraf"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "34",
      "title": "Unsupervised word-level affect analysis and propagation in a lexical knowledge graph",
      "authors": [
        "M Fares",
        "A Moufarrej",
        "E Jreij",
        "J Tekli",
        "W Grosky"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "35",
      "title": "Object aroused emotion analysis network for image sentiment analysis",
      "authors": [
        "J Zhang",
        "J Liu",
        "W Ding",
        "Z Wang"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "36",
      "title": "Enhancing image sentiment analysis: A user-centered approach through user emotions and visual features",
      "authors": [
        "S Liang",
        "D Wu",
        "C Zhang"
      ],
      "year": "2024",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "37",
      "title": "Delving into multimodal prompting for fine-grained visual classification",
      "authors": [
        "X Jiang",
        "H Tang",
        "J Gao",
        "X Du",
        "S He",
        "Z Li"
      ],
      "year": "2024",
      "venue": "AAAI"
    },
    {
      "citation_id": "38",
      "title": "Generalizable whole slide image classification with fine-grained visualsemantic interaction",
      "authors": [
        "H Li",
        "Y Chen",
        "Y Chen",
        "R Yu",
        "W Yang",
        "L Wang",
        "B Ding",
        "Y Han"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "39",
      "title": "Visual prompting in llms for enhancing emotion recognition",
      "authors": [
        "Q Zhang",
        "Z Wang",
        "D Zhang",
        "W Niu",
        "S Caldwell",
        "T Gedeon",
        "Y Liu",
        "Z Qin"
      ],
      "venue": "EMNLP"
    },
    {
      "citation_id": "40",
      "title": "Tree of thoughts: Deliberate problem solving with large language models",
      "authors": [
        "S Yao",
        "D Yu",
        "J Zhao",
        "I Shafran",
        "T Griffiths",
        "Y Cao",
        "K Narasimhan"
      ],
      "year": "2024",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "41",
      "title": "Progressive multi-prompt learning for vision-language models",
      "authors": [
        "J Liu",
        "Z Lu",
        "H Luo",
        "Z Lu",
        "Y Zheng"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "42",
      "title": "Multi-modal instruction tuned llms with fine-grained visual perception",
      "authors": [
        "J He",
        "Y Wang",
        "L Wang",
        "H Lu",
        "J.-Y He",
        "J.-P Lan",
        "B Luo",
        "X Xie"
      ],
      "year": "2024",
      "venue": "CVPR"
    },
    {
      "citation_id": "43",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "44",
      "title": "Py-feat: Python facial expression analysis toolbox",
      "authors": [
        "J Cheong",
        "E Jolly",
        "T Xie",
        "S Byrne",
        "M Kenney",
        "L Chang"
      ],
      "year": "2023",
      "venue": "Affective Science"
    },
    {
      "citation_id": "45",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "P Wang",
        "S Bai",
        "S Tan",
        "S Wang",
        "Z Fan",
        "J Bai",
        "K Chen",
        "X Liu",
        "J Wang",
        "W Ge",
        "Y Fan",
        "K Dang",
        "M Du",
        "X Ren",
        "R Men",
        "D Liu",
        "C Zhou",
        "J Zhou",
        "J Lin"
      ],
      "year": "2009",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "46",
      "title": "Sharegpt4video: Improving video understanding and generation with better captions",
      "authors": [
        "L Chen",
        "X Wei",
        "J Li",
        "X Dong",
        "P Zhang",
        "Y Zang",
        "Z Chen",
        "H Duan",
        "B Lin",
        "Z Tang"
      ],
      "year": "2024",
      "venue": "ECCV"
    },
    {
      "citation_id": "47",
      "title": "Vila: On pre-training for visual language models",
      "authors": [
        "J Lin",
        "H Yin",
        "W Ping",
        "Y Lu",
        "P Molchanov",
        "A Tao",
        "H Mao",
        "J Kautz",
        "M Shoeybi",
        "S Han"
      ],
      "year": "2023",
      "venue": "Vila: On pre-training for visual language models"
    },
    {
      "citation_id": "48",
      "title": "Llava-next: A strong zero-shot video understanding model",
      "authors": [
        "Y Zhang",
        "B Li",
        "H Liu",
        "Y Lee",
        "L Gui",
        "D Fu",
        "J Feng",
        "Z Liu",
        "C Li"
      ],
      "year": "2024",
      "venue": "Llava-next: A strong zero-shot video understanding model"
    },
    {
      "citation_id": "49",
      "title": "A benchmark of facial recognition pipelines and co-usability performances of modules",
      "authors": [
        "S Serengil",
        "A Özpınar"
      ],
      "year": "2024",
      "venue": "Bilis ¸im Teknolojileri Dergisi"
    },
    {
      "citation_id": "50",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    }
  ]
}