{
  "paper_id": "2205.10237v1",
  "title": "M 3 Ed: Multi-Modal Multi-Scene Multi-Label Emotional Dialogue Database",
  "published": "2022-05-09T06:52:51Z",
  "authors": [
    "Jinming Zhao",
    "Tenggan Zhang",
    "Jingwen Hu",
    "Yuchen Liu",
    "Qin Jin",
    "Xinchao Wang",
    "Haizhou Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M 3 ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances. M 3 ED is annotated with 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral) at utterance level, and encompasses acoustic, visual, and textual modalities. To the best of our knowledge, M 3 ED is the first multimodal emotional dialogue dataset in Chinese. It is valuable for cross-culture emotion analysis and recognition. We apply several state-of-the-art methods on the M 3 ED dataset to verify the validity and quality of the dataset. We also propose a general Multimodal Dialogue-aware Interaction framework, MDI, to model the dialogue context for emotion recognition, which achieves comparable performance to the stateof-the-art methods on the M 3 ED. The full dataset and codes are available 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion Recognition in Conversation (ERC) aims to automatically identify and track the emotional status of speakers during a dialogue  (Poria et al., 2019b) . It is a crucial component to improve natural human-computer interactions and has a wide range of applications in interaction scenarios, including call-center dialogue systems  (Danieli et al., 2015) , conversational agents  (Fragopanagos and Taylor, 2005)  and mental health diagnoses  (Ringeval et al., 2018) , etc. Different from traditional multimodal emotion recognition on isolated utterances, multimodal ERC is a more challenging problem, because there are many influencing factors that affect the speakers' emotional state in a dialogue, including the dialogue context from multi-modalities, the scene, the topic, and even the personality of subjects, etc.  (Poria et al., 2019b; Scherer, 2005; Koval et al., 2015) . It has been proved in recent works  (Majumder et al., 2019; Ghosal et al., 2019; Hu et al., 2021; Shen et al., 2020)  that contextual information plays an important role in ERC tasks and brings significant improvements over baselines that only consider isolated utterances. DialogueRNN  (Majumder et al., 2019)  uses recurrent networks to model global and speaker-specific temporal-context information. Di-alogueGCN  (Ghosal et al., 2019)  and MMGCN  (Hu et al., 2021)  use graph-based networks to capture conversational dependencies between utterances in dialogues. DialogXL  (Shen et al., 2020)  arXiv:2205.10237v1 [cs.CL] 9 May 2022 applies a strong pre-trained language model XLNet  (Yang et al., 2019)  to ERC and proposes a dialogaware self-attention method for modeling the context information. The IEMOCAP  (Busso et al., 2008)  and MELD  (Poria et al., 2019a)  are two multimodal emotional dialogue benchmark datasets, which are widely used in the above-mentioned works and promote research in the affective computing field. However, both of them are limited in size and diversity. The videos in MELD are collected only from the Friends TV series, and the videos in IEMOCAP are recorded in laboratory environments from ten actors performing scripted and spontaneous dialogues. These limitations not only affect the investigation of generalization and robustness of the models, but also limit the exploration of other important influencing factors in dialogues, such as dialogue scene, dialogue topic, emotional influence from interlocutors, and so on.\n\nIn this work, we construct a large-scale Multimodal Multi-scene and Multi-label Emotional Dialogue dataset, M 3 ED, which consists of 990 emotional dyadic dialogue video clips from 56 different TV series (about 500 episodes), ensuring that there are various dialogue scenes and topics. We also consider the blended annotations of emotions, which are commonly observed in real-life human interactions  (Devillers et al., 2005; Vidrascu and Devillers, 2005) . M 3 ED contains 24449 utterances in total, which are more than three times larger than IEMOCAP and almost two times larger than MELD. There are rich emotional interaction phenomena in M 3 ED dialogues, for example, 5,396 and 2,696 inter-turn emotion-shift and emotioninertia scenarios respectively, and 2,879 and 10,891 intra-turn emotion-shift and emotion-inertia scenarios respectively. To the best of our knowledge, M 3 ED is the first large-scale multi-modal emotional dialogue dataset in Chinese, which can promote research of affective computing for the Chinese language. It is also a valuable addition for cross-cultural emotion analysis and recognition.\n\nWe further perform the sanity check of the dataset quality. Specifically, we evaluate our proposed M 3 ED dataset on several state-of-the-art approaches, including DialogueRNN, DialogueGCN, and MMGCN. The experimental results show that both context information and multiple modalities can help model the speakers' emotional states and significantly improve the recognition performance, in which context information and multi-ple modalities are two salient factors of a multimodal emotion dialogue dataset. Furthermore, motivated by the masking strategies of self-attention used in DialogXL  (Shen et al., 2020) , we propose a general Multimodal Dialogue-aware Interaction (MDI) framework which considers multimodal fusion, global-local context modeling, and speaker interactions modeling and achieves state-of-the-art performance.\n\nAll in all, M 3 ED is a large, diverse, high-quality, and comprehensive multimodal emotional dialogue dataset, which can support more explorations in the related research directions, such as multi-label learning, interpretability of emotional changes in dialogues, cross-culture emotion recognition, etc. The main contributions of this work are as follows:\n\n• We build a large-scale Multi-modal Multiscene and Multi-label Emotional Dialogue dataset called M 3 ED, which can support more explorations in the affective computing field.\n\n• We perform a comprehensive sanity check of the dataset quality by running several state-ofthe-art approaches on M 3 ED and the experimental results prove the validity and quality of the dataset.\n\n• We propose a general Multimodal Dialogueaware Interaction framework, MDI, which involves multimodal fusion, global-local context and speaker interaction modeling, and it achieves comparable performance to other state-of-the-art approaches.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Datasets",
      "text": "Table  1  summarizes some of the most important emotion datasets related to this work. The EmoryNLP (Zahiri and Choi, 2018), Emotion-Lines  (Chen et al., 2018) , and DailyDialog  (Li et al., 2017)  are emotional dialogue datasets in only text modality, which have been widely used in the ERC tasks. The CMU-MOSEI  (Zadeh et al., 2018) , AFEW  (Dhall et al., 2012) , MEC  (Li et al., 2018) , and CH-SIMS  (Yu et al., 2020)  contain multiple modalities and have been wildly used for multimodal emotion recognition, but they are not conversational and can not support explorations of dialogue emotional analysis. The IEMO-CAP  (Busso et al., 2008) , MSP-IMPROV  (Busso et al., 2016)  and MELD  (Poria et al., 2019a)  are the currently available multimodal emotional dialogue datasets. The IEMOCAP and MSP-IMPROV datasets are recorded from ten/twelve actors performing scripted and spontaneous dyadic dialogues, and each utterance is manually labeled with discrete emotion categories. The MELD  (Poria et al., 2019a ) is a multi-modal multi-party emotional dialogue dataset extended from the text-based Emo-tionLines dataset  (Chen et al., 2018) , which is derived only from the Friends TV series.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Methods",
      "text": "Previous works on ERC focus on modeling context information in a conversation with different frameworks. BC-LSTM  (Poria et al., 2017)  employs a Bi-directional LSTM to capture temporalcontext information in conversations. CMN  (Hazarika et al., 2018b)  and ICON  (Hazarika et al., 2018a)  use distinct GRUs to model the global and speaker-specific temporal-context, and apply memory networks to model speaker emotional states. DialogueRNN  (Majumder et al., 2019)  uses distinct GRUs to model global and speakerspecific temporal-context, and global emotional states tracking respectively. DialogueGCN  (Ghosal et al., 2019)  captures conversational dependencies between utterances with a graph-based structure. MMGCN  (Hu et al., 2021)  further proposes a GCNbased multimodal fusion method for multimodal ERC tasks to improve recognition performance. Di-alogXL  (Shen et al., 2020)  first introduces a strong pre-trained language model XLNet for text-based ERC. It also proposes several masking strategies of self-attention to model the global, local, interspeaker, and intra-speaker interactions.\n\n3 Dataset Construction",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialogue Selection",
      "text": "In order to build a large-scale, diversified, and highquality multimodal emotional dialogue dataset, we collect video dialogue clips from different TV series, which can simulate spontaneous emotional behavior in the real-world environment  (Dhall et al., 2012; Li et al., 2018; Poria et al., 2019a) .\n\nSince high-quality conversation video clips are very important, we require the crowd workers to follow the strict selection requirements, including the following major aspects: 1) The required TV series should belong to these categories, such as family, romance, soap opera, and modern opera, which have rich and natural emotional expressions.\n\n2) The workers are required to select 15 ∼ 25 highquality emotional dialogue video clips from each TV series. 3) Each dialogue should have at least 3 rounds of interaction and a clear conversation topic. 4) In order to ensure the quality of the visual and acoustic modalities, the workers are required to select two-person dialogue scenes with clear facial expressions and intelligible voices.\n\nAfter the dialogue selection, we randomly check several dialogues for each TV series and filter out the low-quality dialogues or ask the crowd workers to correct the inappropriate start and end timestamps.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Annotation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Text And Speaker Annotation",
      "text": "In order to facilitate the process of emotion annotation, we first require the crowd workers to correct the text content and annotate the speaker info of each utterance. Since the videos of TV series do not have embedded subtitles, we use the OCR-based (Optical Character Recognition) method 2  to automatically generate the text content and the corresponding timestamps. For speaker annotations, the first speaker in the dialogue is annotated as \"A\", and the other speaker is annotated as \"B\". In addition, we annotate the role names, ages and genders of these speakers as well.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Annotation",
      "text": "We annotate each utterance based on Ekman's six basic emotions (happy, surprise, sad, disgust, anger, and fear) and an additional emotion label neutral, which is an annotation scheme widely used in previous works  (Poria et al., 2019a; Busso et al., 2008) . The annotators are asked to sequentially annotate the utterances, after watching the videos. Thus, the textual, acoustic and visual information, and the previous utterances in the dialogue are available for emotional annotation. The annotators are allowed to select more than one emotional label to account for blended emotions (e.g., anger&sad), which are commonly observed in real-life human interactions  (Devillers et al., 2005) . If none of the seven emotion categories can accurately describe the emotion status of the utterance, a special other category can be annotated.\n\nIn order to obtain high-quality annotations, we together with several emotional psychology experts design an annotation tutorial with reference to previous guidelines  (Ekman, 1992; Campos et al., 2013) . We train the annotators and provide them with an examination, and only those who pass the exam can participate in the annotation stage. The vast majority of the dataset is annotated by university students and all the annotators are native Mandarin speakers. We assign three annotators to each dialogue.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Annotation Finalization",
      "text": "We apply the majority voting strategy over all the annotations of an utterance to produce its final emotion label. Please note that annotators are allowed to assign more than one emotion label to an utterance, and the importance of these labels is in descending order. We simply assign an importance value to the emotion label of each utterance in descending order, e.g. I(e) = 7 for the first emotion label, I(e) = 6 for the second emotion label, and so on. If a label is not assigned to the utterance, its importance value I(e) = 0. An emotion label e is assigned as one of the final emotion labels for an utterance, if it is assigned to the utterance by at least two annotators. And its importance value is decided by averaging its importance ranking from all annotators: I(e) = 3 k=1 I k (e), where I k (e) is its importance value from annotator k.\n\nTo further ensure annotation quality, we design two strategies to review and revise incorrect annotations. 1) We calculate the annotation agreement between the annotators of each dialogue. For the dialogues with a poor agreement, we require all relevant annotators to review the annotations again and make corrections if necessary. 2) For the utterances (0.5% of all utterances) that don't have a majority annotators' agreement, we ask several high-quality annotators to review them and make a final emotion annotation decision for these utterances.\n\nFinally, we analyze the inter-annotators agreement and achieve an overall Fleiss' Kappa  (Fleiss et al., 2013)  statistic of k = 0.59 for a sevenclass emotion problem, which is higher than other datasets, such as k = 0.43 in MELD, k = 0.48 in  IEMOCAP and k = 0.49 in MSP-IMPROV.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Dataset Statistics",
      "text": "Table  2  presents several basic statistics of the M 3  ED dataset. It contains 990 dialogues, 9,082 turns, 24,449 utterances derived from 56 different TV series (about 500 episodes), which ensures the scale and diversity of the dataset. We adopt the TV-independent data split manner in order to avoid any TV-dependent bias, which means there is no overlap of TV series across training, validation, and testing sets. The basic statistics are similar across these three data splits. There are rich emotional interactions phenomena in the M 3 ED, for example, 5,396 and 2,696 inter-turn emotion-shift and emotion-inertia scenarios respectively, and 2,879 and 10,891 intra-turn emotion-shift and emotioninertia scenarios. The emotion shift and emotion inertia are two important factors in dialogues, which are challenging and worthy of exploration  (Poria et al., 2019a) . As shown in the table, 89% of utterances are assigned with one emotion label, and 11% of utterances are assigned with blended emotions 3 . Table  3  presents the single emotion distribution statistics. The distribution of each emotion cate-gory is similar across train/val/test sets. As shown in Table  4 , there are in total 626 different speakers in M 3 ED with balanced gender distribution. Among all the speakers, young and middle-aged speakers account for more than 80%.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Proposed Framework",
      "text": "A dialogue can be defined as a sequence of utterances D = {utt 1 , utt 2 , ..., utt N }, where N is the number of utterances. Each utterance consists of textual (l), acoustic (a) and visual (v) modalities. We denote u A t  [a, v, l]  as the utterance-level feature of utterance utt t from speaker A with the textual, acoustic and visual modality respectively. The task aims to predict the emotional state for each utterance in the dialogue based on all existing modalities. Figure  2  illustrates our proposed Multimodal Dialogue-aware Interaction (MDI) framework, which contains three main modules: 1) Multimodal Fusion module aims to generate the utterance-level multimodal representation from different modalities. 2) Dialog-aware Interaction module aims to model the interactions in the dialogue; 3) Interaction Fusion and Classification module fuses the different interaction information from the outputs of the Dialog-aware Interaction module, and then makes the emotional state prediction based on the fused interaction information. Multimodal Fusion Module: Based on the modality-specific feature representations from different modalities, we apply early fusion of these modalities features to produce the multimodal feature representation:\n\nFigure  3 : Illustration of the four masking strategies corresponding to the four interaction sub-modules respectively. A i denotes the i-th utterance of the speaker A.\n\nThe yellow blocks denote the current utterances. Utterances that can be accessed by the current utterance are marked as green, while those can not be accessed are marked as white.\n\nDialog-aware Interaction Module: In order to adequately capture the contextual information in the dialogue, we propose the Dialog-aware Interaction Module which consists of L dialog-aware interaction blocks (gray block in Figure  2 ). In each block, we adopt four sub-modules, Global Interaction, Local Interaction, Intra-speaker Interaction and Inter-speaker Interaction, to model the global, local, intra-speaker and inter-speaker interactions in the dialogue respectively. We implement these four types of interactions in one Transformer layer by skillfully changing the masking strategies of self-attention  (Shen et al., 2020; Li et al., 2020)  as illustrated in Figure  3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Interaction Fusion And Classification:",
      "text": "As the Dialog-aware Interaction Module produces different outputs that carry various interaction contextual information, we fuse these outputs via simple addition. Finally, we use one fully connected layer as a classifier to predict the emotional state based on the fused interaction information.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature Extraction",
      "text": "We investigate the state-of-the-art features of different modalities including textual, acoustic, and visual features for emotion recognition tasks 4 .\n\n4 More detailed description of the feature extractors can be found in the supplementary material. A.2\n\nTextual Features: We extract the word-level features from a pre-trained RoBERTa model  (Yu et al., 2020) . Furthermore, to get more efficient emotional features, we extract the finetuned features (\"[CLS]\" position) from the finetuned RoBERTa model trained on M 3 ED. We refer to the word-level and finetuned utterance-level textual features as \"L_Frm\", and \"L_Utt\" respectively.\n\nAcoustic Features: We extract the frame-level features from a pre-trained Wav2Vec2.0 model  (Baevski et al., 2020) . We extract the finetuned features (the last time step) from the Wav2Vec2.0 model finetuned on M 3 ED. We refer to the framelevel and finetuned utterance-level acoustic features as \"A_Frm\" and \"A_Utt\" respectively.\n\nVisual Features: We first propose a two-stage strategy to detect the speaker's faces 5 . We then extract the face-level features via a pre-trained DenseNet model  (Huang et al., 2017)  for each utterance based on the detected speaker's faces. DenseNet was trained on two facial expression benchmark corpus, FER+  (Barsoum et al., 2016)  and AffectNet  (Mollahosseini et al., 2017) . We average the face-level features within one utterance to get the averaged utterance-level features. We refer to the face-level, averaged utterance-level visual features as \"V_Frm\", \"V_Utt\" respectively.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Baseline Models",
      "text": "We evaluate several state-of-the-art methods including utterance-level recognition methods and dialoglevel recognition methods on the proposed M 3 ED dataset, and they are listed as follows:\n\nMultiEnc: A flexible and efficient utterancelevel multimodal emotion recognition framework  (Zhao et al., 2021)  that consists of several modalityspecific encoders (LSTM, LSTM and TextCNN for acoustic, visual and textual modalities respectively) and a fusion encoder (several fully-connected layers) for emotion prediction. For the utterance-level modality features, three DNN encoders are used for the three modalities respectively.\n\nDialogueRNN: A state-of-the-art RNN-based ERC framework proposed in  (Majumder et al., 2019) , which captures the global and speakerspecific temporal context information, and global emotional state information via different GRUs. For the multimodal experiments, the early-fusion method that concatenates different modality features as input is adopted in this work.\n\nDialogueGCN: A state-of-the-art GCN-based ERC framework proposed in  (Ghosal et al., 2019) , which models long-distance dependency and speaker interactions via direct edges and different designed relations respectively. For the multimodal experiments, we also adopt the early-fusion method in this work.\n\nMMGCN: A state-of-the-art GCN-based multimodal ERC framework proposed in  (Hu et al., 2021) . For the uni-modal experiments, we only model the fully connected graph.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiment Setup",
      "text": "We split the M 3 ED dataset into training, validation, testing sets in a TV-independent manner, which is a more challenging experiment setting. The distribution of the data splits is shown in Table  3 . We use the weighted-F1 score (WF1) as the evaluation metrics. We tune the parameters on the validation set and report the performance on the testing set. We run each model three times and report the average performance to alleviate the influence of random parameter initialization.\n\nWe conduct two sets of experiments, including 1) the utterance-level baseline experiments of emotion recognition on isolated utterances without considering dialogue context, which aims to check the quality of each modality and compare the effectiveness of multimodal information for emotion recognition, and 2) the dialogue-level experiments of emotion recognition in the dialogue, which aims to compare our proposed general MDI framework with the state-of-the-art models in modeling dialogue context for emotion recognition. For the utterance-level experiments, we adopt the Multi-Enc (Section 5.2) framework as the baseline model.\n\nFor the dialogue-level experiments, we compare to DialogueRNN, DialogueGCN, and MMGCN models.\n\nSince different modality features are used in this work, we have tried different hidden sizes (such as 180, 256, and 512) in our experiments. For the experiments on the proposed Multimodal Dialogaware Interaction framework (Section 4), we use the Adam optimizer with learning rate of 3e-5. We set the dropout as 0.1, the hidden size as 384 in the unimodal experiments and 512 in the multimodal experiments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Utterance Baseline Experiments",
      "text": "Table  5  presents the utterance-level baseline results. Among the different unimodal features, the finetuned utterance-level features achieve significant improvement on textual and acoustic modalities. The multimodal information can bring significant performance improvement over unimodal. However, for the multimodal experiments, the finetuned features do not show much improvement over the frame-level features. It is mainly because the finetuned features retain more classification information and lose some modality-specific information, which limits the complementarity between the modalities.\n\nIn addition, we observe that there is no big gap between the performances on different modalities, which indicates the good quality of different modalities in our M 3 ED dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dialogue Experiments",
      "text": "Since the state-of-the-art dialogue-level methods mainly focus on modeling the dialogue context information based on the utterance-level features, we adopt the finetuned utterance-level features (\"Utt_ft\") in the following experiments. Table  6  presents the dialogue-level experiment results. The results show that context information and multiple modalities, the two salient factors of a multimodal emotion dialogue dataset, both bring significant performance improvement, which also proves the validity and quality of the M 3 ED dataset to some extend.\n\nCompared to the state-of-the-art models, our proposed general MDI framework achieves superior performance in the textual, acoustic, and visual unimodal experiments. It demonstrates that the four dialogue-aware interaction strategies which consider both the global-and local-context interactions and the intra-and inter-speaker interactions have better dialogue modeling ability than only considering part of these interactions, which demon- strates the strong dialogue context modeling ability of MDI. However, MDI does not outperform other models under the multimodal conditions, which may be due to the limited training dataset size and the limited ability of the vanilla multimodal fusion strategy in interaction modeling. In the future, we will explore more effective multimodal fusion module and interaction modeling module within the MDI framework to improve its performance under multimodal conditions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Future Directions",
      "text": "The M 3 ED dataset is a large, diversified, highquality, and comprehensive multimodal emotional dialogue dataset. Based on the characteristics of the dataset and the analysis from the extensive experiments, we believe that M 3 ED can support a number of related explorations in affective computing field.\n\n• Based on the experiment results, we think that the finetuned features lack sufficient modalityspecific information, which limits the performance under the multimodal conditions. Therefore, it is worth exploring to realize a more efficient multimodal fusion module based on the raw frame-level features and make the above proposed general Multimodal Dialog-aware Interaction (MDI) framework an end-to-end model.\n\n• According to psychological and behavioral studies, emotional inertia and stimulus (external/internal) are important factors that affect the speaker's emotional state in dialogues. The emotional inertia and emotional stimulus can explain how one speaker's emotion affects his own or the other speaker's emotion. There are rich emotional interaction phenomena including inter-and intra-turn emotion shifts in the M 3 ED dataset. Therefore, it can support the exploration of interpretability of emotional changes in a Dialogue.\n\n• The blended emotions are commonly observed in human real-life dialogues, and multi-label learning can help reveal and model the relevance between different emotions. Therefore, the M 3 ED dataset can support the exploration of multi-label emotion recognition in conversations.\n\n• Emotional expression varies across different languages and cultures. The M 3 ED dataset in Chinese is a valuable addition to the existing benchmark datasets in other languages. It can promote the research of cross-culture emotion analysis and recognition.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Ethical Considerations",
      "text": "This work presents M 3 ED, free and open dataset for the research community to study the multimodal emotion recognition in dialogues. Data in M 3 ED are collected from TV series in Chinese. To ensure that crowd workers were fairly compensated, we paid them at an hourly rate of 40 yuan ($6.25 USD) per hour, which is a fair and reasonable hourly wage in Beijing. First, to select high-quality dialogues from 56 TV-series, we recruited 12 Chinese college students (5 males and 7 females). Each student was paid 100 yuan ($15.625 USD) for selecting about 18 dialogues from each TV series.\n\nTo annotate the emotional status of the selected dialogues, we recruited 14 Chinese college students (6 males and 8 females). Each student was paid 200 yuan ($31.25 USD) for annotating about 18 dialogues from each TV series with emotion labels, text correction, speaker, gender, and age information. If only the emotion labels were annotated, the payment for each TV series was 100 yuan ($15.625 USD). Considering the copy-right issue of TV-series, we will only release the name list of the TV-series and our annotations. To facilitate future comparison research on this dataset, we will provide our extracted visual expression features and acoustic features. We anticipate that the highquality and rich annotation labels in the dataset will advance research in multimodal emotion recognition. Visual Feature Extractor: We adopt a pretrained facial expression recognition DenseNet model to extract the face-level visual features, which is trained on the combination of the FER+ and AffectNet two benchmark corpus (Tabel. 7 ). It achieves the Weighted-accuracy and F1-score performance of 63.54% and 52.94% on the combined validation set respectively.\n\nA.3 Extra Experimental Results Analysis.\n\nFigure. 4 presents the confusion matrices of Dia-logueRNN and our MDI dialogue-level models under the {l, a, v} multimodal condition. Both models perform badly for recognizing the fear emotion, which relates to the limited number of training instances for the fear emotion. It demonstrates the class imbalance issue is a challenging problem for both models. We also observe a high confusing rate between sad, anger, and disgust emotion categories since these emotions are more likely to occur at the same time (the top 5 blended emotions indeed come from these 3 categories), which makes them more difficult to disambiguate. In the future, we will explore effective solutions to deal with the emotion imbalance challenge and learn multi-label emotion classification.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of a dialogue, showing the rich",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustration of the Multimodal Dialog-aware Interaction (MDI) framework (taking one round as an ex-",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates our proposed Multi-",
      "page": 5
    },
    {
      "caption": "Figure 3: Illustration of the four masking strategies cor-",
      "page": 6
    },
    {
      "caption": "Figure 2: ). In each",
      "page": 6
    },
    {
      "caption": "Figure 3: Interaction Fusion and Classiﬁcation: As the",
      "page": 6
    },
    {
      "caption": "Figure 4: Confusion matrices of DialogueRNN and",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 5: Utterance-level baseline performance (WF1)",
      "data": [
        {
          "Modalities": "{l}\n{a}\n{v}",
          "Frm\nval\ntest": "42.24\n43.23\n42.56\n40.96\n41.25\n43.79",
          "Utt\nval\ntest": "44.41\n44.67\n46.09\n48.56\n42.32\n41.09"
        },
        {
          "Modalities": "{l, a}\n{l, v}\n{a, v}",
          "Frm\nval\ntest": "48.10\n46.53\n48.17\n50.73\n49.66\n46.19",
          "Utt\nval\ntest": "48.68\n51.58\n50.48\n47.68\n46.28\n49.66"
        },
        {
          "Modalities": "{l, a, v}",
          "Frm\nval\ntest": "49.48\n54.55",
          "Utt\nval\ntest": "52.15\n48.90"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Metric": "",
          "Modalities": "{l}\n{a}\n{v}\n{l, a}\n{l, v}\n{a, v}\n{l, a, v}"
        },
        {
          "Model": "UttBaseline",
          "Metric": "val\ntest",
          "Modalities": "44.67\n48.56\n42.32\n51.58\n50.48\n49.66\n52.15\n44.41\n46.09\n41.09\n48.68\n47.68\n46.28\n48.90"
        },
        {
          "Model": "DialogueGCN",
          "Metric": "val\ntest",
          "Modalities": "50.77 (+6.1)\n50.96 (+2.4)\n33.82\n53.91 (+2.3)\n54.26 (+3.8)\n50.80 (+1.1)\n54.58 (+2.4)\n46.09 (+1.7)\n46.45 (+0.4)\n27.79\n49.44 (+0.8)\n49.26 (+1.6)\n47.09 (+0.8)\n49.93 (+1.0)"
        },
        {
          "Model": "MMGCN",
          "Metric": "val\ntest",
          "Modalities": "50.83 (+6.2)\n52.93 (+4.4)\n37.05\n55.62 (+4.0)\n54.75 (+4.3)\n54.71 (+5.1)\n56.67 (+4.5)\n46.49 (+2.1)\n47.77 (+1.7)\n32.87\n49.44 (+0.8)\n50.42 (+2.7)\n48.55 (+2.3)\n51.18 (+2.3)"
        },
        {
          "Model": "DialogueRNN",
          "Metric": "val\ntest",
          "Modalities": "53.65 (+9.0)\n52.09 (+3.5)\n36.03\n55.85 (+4.3)\n58.70 (+8.2)\n52.69 (+3.0)\n56.52 (+4.4)\n48.80 (+4.4)\n47.65 (+1.6)\n28.38\n51.87 (+3.2)\n52.28 (+4.6)\n47.49 (+1.2)\n51.66 (+2.8)"
        },
        {
          "Model": "Ours",
          "Metric": "val\ntest",
          "Modalities": "51.37 (+6.7)\n51.5 (+2.9)\n45.97 (+3.6)\n54.27 (+2.7)\n55.69 (+5.2)\n51.34 (+1.7)\n54.09 (+1.9)\n49.42 (+5.0)\n48.03 (+1.9)\n41.33 (+0.2)\n50.24 (+1.6)\n52.07 (+4.4)\n47.64 (+1.4)\n50.99 (+2.1)"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 7: The distribution of facial expression recogni-",
      "data": [
        {
          "FER+\ntrain\nval": "10,342\n1,342\n7,526\n898\n3,576\n458\n3,530\n416\n2,464\n319\n654\n36\n193\n75\n167\n25",
          "AffectNet\ntrain\nval": "74,874\n500\n134,415\n500\n14,090\n500\n25,459\n500\n24,882\n500\n3,803\n500\n6,378\n500\n3,750\n499",
          "Total\ntrain\nval": "85,216\n1842\n141,941\n1398\n17,666\n958\n28,989\n916\n27,346\n819\n3,996\n536\n7,032\n575\n3,917\n524"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "2",
      "title": "Training deep networks for facial expression recognition with crowdsourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "What is shared, what is different? core relational themes and expressive displays of eight positive emotions",
      "authors": [
        "Belinda Campos",
        "Michelle Shiota",
        "Dacher Keltner",
        "Gian Gonzaga",
        "Jennifer Goetz"
      ],
      "year": "2013",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "6",
      "title": "Emotionlines: An emotion corpus of multi-party conversations",
      "authors": [
        "Sheng-Yeh Chen",
        "Chao-Chun Hsu",
        "Chuan-Chun Kuo",
        "Lun-Wei Ku"
      ],
      "year": "2018",
      "venue": "Emotionlines: An emotion corpus of multi-party conversations",
      "arxiv": "arXiv:1802.08379"
    },
    {
      "citation_id": "7",
      "title": "Emotion unfolding and affective scenes: A case study in spoken conversations",
      "authors": [
        "Morena Danieli",
        "Giuseppe Riccardi",
        "Firoj Alam"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Workshop on Emotion Representations and Modelling for Companion Technologies"
    },
    {
      "citation_id": "8",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "Laurence Devillers",
        "Laurence Vidrascu",
        "Lori Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "9",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "10",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "11",
      "title": "Statistical methods for rates and proportions",
      "authors": [
        "Bruce Joseph L Fleiss",
        "Myunghee Levin",
        "Paik Cho"
      ],
      "year": "2013",
      "venue": "Statistical methods for rates and proportions"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Nickolaos Fragopanagos",
        "John Taylor"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "13",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "14",
      "title": "2018a. Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "15",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "16",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "18",
      "title": "Emotional inertia and external events: The roles of exposure, reactivity, and recovery",
      "authors": [
        "Peter Koval",
        "Annette Brose",
        "Madeline Pe",
        "Marlies Houben",
        "Yasemin Erbas",
        "Dominique Champagne",
        "Peter Kuppens"
      ],
      "year": "2015",
      "venue": "Emotion"
    },
    {
      "citation_id": "19",
      "title": "A hierarchical transformer with speaker modeling for emotion recognition in conversation",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Qingyi Si",
        "Weiping Wang"
      ],
      "year": "2020",
      "venue": "A hierarchical transformer with speaker modeling for emotion recognition in conversation",
      "arxiv": "arXiv:2012.14781"
    },
    {
      "citation_id": "20",
      "title": "Mec 2017: Multimodal emotion recognition challenge",
      "authors": [
        "Ya Li",
        "Jianhua Tao",
        "Björn Schuller",
        "Shiguang Shan",
        "Dongmei Jiang",
        "Jia Jia"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia)"
    },
    {
      "citation_id": "21",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "25",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and crosscultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "Björn Schuller",
        "Michel Valstar",
        "Roddy Cowie",
        "Heysem Kaya",
        "Maximilian Schmitt",
        "Shahin Amiriparian",
        "Nicholas Cummins",
        "Denis Lalanne",
        "Adrien Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "28",
      "title": "What are emotions? and how can they be measured? Social science information",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2005",
      "venue": "What are emotions? and how can they be measured? Social science information"
    },
    {
      "citation_id": "29",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "arxiv": "arXiv:2012.08695"
    },
    {
      "citation_id": "30",
      "title": "Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection",
      "authors": [
        "Ruijie Tao",
        "Zexu Pan",
        "Rohan Kumar Das",
        "Xinyuan Qian",
        "Mike Shou",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection",
      "arxiv": "arXiv:2107.06592"
    },
    {
      "citation_id": "31",
      "title": "Annotation and detection of blended emotions in real human-human dialogs recorded in a call center",
      "authors": [
        "Laurence Vidrascu",
        "Laurence Devillers"
      ],
      "year": "2005",
      "venue": "2005 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "32",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "34",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "35",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "36",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    }
  ]
}