{
  "paper_id": "2507.20737v1",
  "title": "Multi-Masked Querying Network For Robust Emotion Recognition From Incomplete Multi-Modal Physiological Signals",
  "published": "2025-07-28T11:41:15Z",
  "authors": [
    "Geng-Xin Xu",
    "Xiang Zuo",
    "Ye Li"
  ],
  "keywords": [
    "Multi-modal emotion recognition",
    "Physiological signals",
    "Missing data"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition from physiological data is crucial for mental health assessment, yet it faces two significant challenges: incomplete multi-modal signals and interference from body movements and artifacts. This paper presents a novel Multi-Masked Querying Network (MMQ-Net) to address these issues by integrating multiple querying mechanisms into a unified framework. Specifically, it uses modality queries to reconstruct missing data from incomplete signals, category queries to focus on emotional state features, and interference queries to separate relevant information from noise. Extensive experiment results demonstrate the superior emotion recognition performance of MMQ-Net compared to existing approaches, particularly under high levels of data incompleteness.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Mental disorders, such as anxiety disorders and post-traumatic stress disorder, are often accompanied by dysfunctions in emotion processing, which can impair individuals' social abilities and quality of life  [23, 15, 8] . Neurobiological studies have demonstrated that emotion generation is closely associated with the activity of brain regions such as the limbic system and the prefrontal cortex. Moreover, the interactions between these brain regions and the peripheral physiological system are reflected in physiological indicators such as heart rate, skin conductance response, and respiration  [17] . Therefore, accurately identifying emotional states is crucial for mental health assessment and intervention.\n\nWith advancements in electroencephalography (EEG) and peripheral physiological signal monitoring technologies, emotion recognition methods based on multi-modal physiological signals provide more objective and real-time emotion monitoring tools for clinical applications  [26, 24, 16, 22] . In recent years, many researchers have explored emotion recognition using multi-modal approaches  [10, 19, 4] . Despite the promising results of these studies, there remain two main challenges in this field.\n\nIncomplete Multi-Modal Signals. A key challenge in emotion recognition from physiological signals is the incomplete multi-modal signals. Multi-modal emotion recognition typically involves combining physiological signals from different sources. However, in practical scenarios, these signals are often incomplete due to technical issues, sensor malfunctions, or loss of data during transmission  [9, 7] . For instance, the galvanic skin response (GSR) or photoplethysmography (PPG) signals may have missing values or even entire segments that are unusable. As shown in Fig.  1 (a), the signals from different modalities can have gaps (indicated by dashed boxes), which result in the incomplete data. This problem significantly hampers the learning process.\n\nInterference from Body Movements and Artifacts. Another major challenge is interference caused by body movements and other external artifacts. When individuals move their body, cough, or experience any other physical disturbance, it can generate noise in the physiological signals, especially in EEG and GSR data. This type of interference can be caused by external factors such as improper sensor placement, muscle contractions, or environmental disturbances  [14] . As illustrated in Fig.  1(b) , body movements and artifacts complicate accurate emotion recognition by introducing noise into the signal data, which reduces the signal's reliability. This issue is particularly troublesome when working with real-time or in-the-wild emotion recognition systems, where controlling or eliminating all physical movements is impractical. Consequently, artifacts can lead to incorrect or ambiguous conclusions about the emotional state, further complicating the task of robust emotion recognition.\n\nTo address these challenges, we propose a Multi-Masked Querying Network (MMQ-Net) for robust emotion recognition from incomplete multi-modal physiological signals. The core idea of MMQ-Net is to utilize multiple queries within a single framework. Specifically, to handle incomplete multi-modal signals, MMQ-Net uses masked modality queries to reconstruct missing data from the available modalities. To address the interference problem, it incorporates masked category query and interference query to separate emotional state features from irrelevant noise. By combining these three masked querying mechanisms in a unified framework, MMQ-Net is able to robustly handle missing data and interference, thereby improving the accuracy and reliability of emotion recognition in challenging settings.\n\nOverall, the main contributions of this work can be summarized as follows. (1) A novel method named MMQ-Net, is proposed for robust emotion recognition from multi-modal physiological signals that are affected by missing data and interference noise. (2) A multi-masked querying transformer is designed to simultaneously reconstruct incomplete multi-modal features and reduce interference from emotion-irrelevant information, thereby enhancing the robustness of emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview",
      "text": "Let {(x i , a i , y i )} n i=1 denote the multi-modal data with a sample size of n, where\n\n] represents the M modalities of physiological signals, and x (m) i is the data from the m-th modality. The vector a i ∈ {0, 1} M indicates whether the M modalities are present for sample i (1 if available, 0 if missing). The vector y i is a one-hot categorical variable that indicates the specific emotional state. The task of this work is to develop a robust model to predict the emotional state y, considering that the multi-modal physiological signals x may contain missing data and interference noise.\n\nAn overview of the proposed MMQ-Net is shown in Fig.  2 . The input includes various physiological signals, such as EEG, GSR, PPG, etc. These signals are processed through respective encoders to extract multi-modal features. The encoded feature vectors are then fed into a Multi-Masked Quering Transformer for the learning of incomplete multi-modal information and reduction of interference. The output is used to compute three loss functions: the multi-modal learning loss function L R , the discriminative learning loss function L C , and the interference reduction loss function L I .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "In the preprocessing stage of physiological signals, we first perform data cleaning and filtering based on Steve Luck's procedures  [12]  to exclude noise components. A notch filter is applied to remove the 50 Hz power line interference, and a 4-45",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality-Queries",
      "text": "Category-Query Interference-Query",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "̂",
      "text": "x (1)   x (2)   x (M)\n\nf (1)   f (2)    For feature extraction, two methods are primarily used: differential entropy (DE) and power spectral density (PSD). These methods enhance feature representation across five frequency bands: θ (4-7 Hz), α (8-10 Hz), slow α (8-13 Hz), β (14-29 Hz), and γ (30-45 Hz). DE extraction assumes a Gaussian distribution, with the standard deviation computed every 2 seconds as the DE value. PSD extraction uses the Welch method, applying the Hanning window function to segment the signal and perform a fast Fourier transform to calculate the power spectral density for each frequency band. Finally, the DE and PSD features are fused using the multi-head attention mechanism  [25]  to generate a new feature vector for subsequent analysis.\n\nFor simplicity, let f m represent the feature extraction function for modality m, yielding the feature f (m) = f m (x (m) ). After feature extraction across all modalities, we obtain F M = [f (1) , . . . , f (M) ].",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Masked Querying Transformer",
      "text": "The Multi-Masked Querying Transformer is designed to handle incomplete multimodal physiological representations and reduce emotion-irrelevant information. Specifically, this module uses multiple masked queries: modality queries (Q M ), category queries (Q C ), and interference queries (Q I ). Masked modality queries are used to learn missing modalities from available ones, while masked category and interference queries are used to learn features related to emotional states and irrelevant features, respectively.\n\nTo learn multi-modal features from incomplete physiological representations, modality queries Q M are used as learnable parameters to replace missing data in F M , encouraging modality completion. To perform attention computation within a unified framework, category queries Q C and interference queries Q I are concatenated as additional tokens with the features, yielding the following representation:\n\nwhere ⊙ denotes the Hadamard product, and the concatenation operation is implemented at the modality level.\n\nTo prevent those missing modalities from affecting attention mechanism, a mask matrix is introduced, ensuring that the queries only learn representations from the available modalities. Let D be the identity matrix, and 1 the vector of ones. The attention mask matrix M is derived from the modality indicator vector a as follows:\n\nwhere the last two elements of 1 correspond to the category query Q C and interference query Q I . Based on the multiple queries and the modality mask matrix, the attention mechanism in the multi-masked querying transformer is computed as:\n\nThis process uses the multi-head attention mechanism  [20] . For convenience, the output Z is decomposed into three parts, namely single-modality features FM , emotional state features FC , and interference features FI :",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Objective Function",
      "text": "The objective function of MMQ-Net consists of three terms: multi-modal reconstruction loss L R , discriminative learning loss L C , and interference reduction loss L I . For the first term, we perform multi-modal feature reconstruction at the feature level:\n\nFor the second term, emotional state features FC are passed through a multilayer perceptron (MLP) to obtain the final predicted result, which is then used to compute the cross-entropy loss with the ground truth label y:\n\nFor the third term, we aim to maximize the correlation between the label y and the emotional state features FC , while minimizing the correlation between the label y and the interference features FI . This is computed using the mutual information criterion:\n\nThus, the final loss function in MMQ-Net is:\n\nwhere λ 1 , λ 2 , and λ 3 are non-negative trade-off parameters.\n\n3 Experiments and Results",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We performed experiments utilizing two multi-modal physiological datasets, i.e., DEAP dataset  [5]  and MAHNOB-HCI dataset  [18] . Both datasets elicit emotional responses through multimedia content.\n\nDEAP dataset encompasses recordings from 32 individuals who were exposed to multimedia stimuli. Each subject participated in 40 sessions, during which they watched a one-minute music video per session. The recordings captured their physiological responses both before and during the viewing, with each session's data consisting of a 3-second pre-session phase and a 60-second session phase. This dataset includes 32 channels of EEG data alongside 8 channels of peripheral physiological signals, such as EOG, EMG, GSR, respiration, and temperature readings.\n\nMAHNOB-HCI dataset gathers data from 27 participants subjected to multimedia stimuli. For this collection, each participant viewed 20 video clips while their physiological reactions were recorded across 20 trials. These clips varied in length from 34.9 seconds to 117 seconds, averaging 81.4 seconds with a standard deviation of 22.5 seconds. The dataset features 32 channels of EEG signals and 6 channels of peripheral physiological signals, including ECG, GSR, respiration, and temperature measurements.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "The multi-masked querying transformer consists of an embedding layer, positional encoding, transformer encoder layers, and a classification head. Specifically, input features are projected into a 16-dimensional space using linear transformations, followed by adding positional encodings to maintain sequence order.\n\nIn the multi-head attention mechanism, the head is set to 4 and the feed-forward dimension is 128. All experiments were implemented in PyTorch, utilizing an Adam optimizer with a learning rate of 6e-4, β 1 = 0.9, β 2 = 0.999, over 5000 epochs and a batch size of 1024. Hyper-parameters were selected based on crossvalidation results, with λ 1 and λ 2 both set to 1, and λ 3 tuned to 0.01.\n\nTo comprehensively evaluate the performance of the proposed method, we designed an extensive experimental setup. Specifically, we assessed the effectiveness of CCA  [3] , KCCA  [11] , DCCA  [1] , AE  [6] , SMIL  [13] , ShaSpe  [21] , TAE  [2] , and our MMQ-Net on two categories, namely Valence and Arousal. The experiments were conducted with varying missing rates ranging from 0.1 to 0.7 to simulate real-world data conditions where missing values are common. This design allowed us to systematically compare the robustness and accuracy of each method under different levels of data incompleteness, providing insights into their suitability for handling missing data in emotional signal processing tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With The State-Of-The-Art Methods",
      "text": "Table  1  compares the performance of various methods at different missing rates on the DEAP dataset. As the missing rate increases, MMQ-Net's advantage becomes more evident. For example, at a 0.7 missing rate, MMQ-Net improves accuracy by up to 5.95% for Valence and 4.44% for Arousal over the next best method. This demonstrates MMQ-Net's robustness in handling high levels of data incompleteness, making it a strong candidate for emotion recognition in challenging conditions.\n\nTable  2  presents the performance comparison on the MAHNOB-HCI dataset. The results indicate that MMQ-Net consistently outperforms other methods across all missing rates. Specifically, MMQ-Net achieves the highest accuracy for both Valence and Arousal categories, with improvements ranging from 4.20% to 6.76% over the next best-performing method at different missing rates. This also demonstrates the robustness and effectiveness of MMQ-Net in handling varying levels of missing data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To examine the effectiveness of each component in MMQ-Net, we carried out ablation studies by removing each of them from the whole framework under a missing rate of 0.3. As shown in Table  3 , removing L R leads to a noticeable drop in performance, with accuracy decreasing by approximately 5% for Valence and by around 4% for Arousal on both datasets. This highlights the critical importance of L R for incomplete multi-modal learning. Similarly, when L I is removed, the performance also decreases. These findings underscore the necessity of L I in mitigating interference, thereby improving overall model performance.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We introduced the Multi-Masked Querying Network (MMQ-Net), a novel method for robust emotion recognition from incomplete multi-modal physiological signals. MMQ-Net combines multiple querying mechanisms to address challenges of missing data and interference. Modality queries reconstruct missing data, while category and interference queries distinguish relevant emotional features from noise. Extensive experiments on benchmark datasets show MMQ-Net's superior performance, particularly under high levels of missing data. Our results demonstrate its effectiveness in enhancing emotion recognition accuracy and reliability, making it a promising solution for emotion analysis and mental health monitoring in real-world applications.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Challenges in robust emotion recognition from incomplete multi-modal data.",
      "page": 2
    },
    {
      "caption": "Figure 1: (a), the signals from diﬀerent modalities can have",
      "page": 2
    },
    {
      "caption": "Figure 1: (b), body movements and artifacts complicate",
      "page": 2
    },
    {
      "caption": "Figure 2: The input in-",
      "page": 3
    },
    {
      "caption": "Figure 2: Flowchart of the MMQ-Net. Multi-model physiological signals are processed",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison under various missing rates on the DEAP dataset.",
      "page": 7
    },
    {
      "caption": "Table 1: compares the performance of various methods at diﬀerent missing rates",
      "page": 7
    },
    {
      "caption": "Table 2: presents the performance comparison on the MAHNOB-HCI dataset.",
      "page": 7
    },
    {
      "caption": "Table 2: Comparison under various missing rates on the MAHNOB-HCI dataset.",
      "page": 8
    },
    {
      "caption": "Table 3: Ablation results of MMQ-Net.",
      "page": 8
    },
    {
      "caption": "Table 3: , removing LR leads to a noticeable",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep canonical correlation analysis",
      "authors": [
        "G Andrew",
        "R Arora",
        "J Bilmes",
        "K Livescu"
      ],
      "year": "2013",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "2",
      "title": "A novel transformer autoencoder for multi-modal emotion recognition with incomplete data",
      "authors": [
        "C Cheng",
        "W Liu",
        "Z Fan",
        "L Feng",
        "Z Jia"
      ],
      "year": "2024",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "3",
      "title": "Relations between two sets of variates",
      "authors": [
        "H Hotelling"
      ],
      "year": "1992",
      "venue": "Breakthroughs in statistics: methodology and distribution"
    },
    {
      "citation_id": "4",
      "title": "Multi-level disentangling network for cross-subject emotion recognition based on multimodal physiological signals",
      "authors": [
        "Z Jia",
        "F Zhao",
        "Y Guo",
        "H Chen",
        "T Jiang",
        "B Center"
      ],
      "year": "2024",
      "venue": "Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "5",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Audio feature generation for missing modality problem in video action recognition",
      "authors": [
        "H Lee",
        "C Lin",
        "P Hsu",
        "W Hsu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Z Lian",
        "L Chen",
        "L Sun",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Brain region knowledge based dual-stream transformer for eeg emotion recognition",
      "authors": [
        "Y Lin",
        "G Xu",
        "H Liang",
        "Y Wang",
        "F Wan",
        "Y Li"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "9",
      "title": "Contrastive learning based modality-invariant feature acquisition for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Emotionkd: a cross-modal knowledge distillation framework for emotion recognition based on physiological signals",
      "authors": [
        "Y Liu",
        "Z Jia",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Randomized nonlinear component analysis",
      "authors": [
        "D Lopez-Paz",
        "S Sra",
        "A Smola",
        "Z Ghahramani",
        "B Schölkopf"
      ],
      "year": "2014",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "12",
      "title": "An introduction to the event-related potential technique",
      "authors": [
        "S Luck"
      ],
      "year": "2014",
      "venue": "An introduction to the event-related potential technique"
    },
    {
      "citation_id": "13",
      "title": "SMIL: Multimodal learning with severely missing modality",
      "authors": [
        "M Ma",
        "J Ren",
        "L Zhao",
        "S Tulyakov",
        "C Wu",
        "X Peng"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Automated cca-mwf algorithm for unsupervised identification and removal of eog artifacts from eeg",
      "authors": [
        "M Miao",
        "W Hu",
        "B Xu",
        "J Zhang",
        "J Rodrigues",
        "V De Albuquerque"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "15",
      "title": "Quality of life in posttraumatic stress disorder: The role of posttraumatic anhedonia and depressive symptoms in a treatment-seeking community sample",
      "authors": [
        "C Miller",
        "J Mcdonald",
        "P Grau",
        "C Wetterneck"
      ],
      "year": "2024",
      "venue": "Trauma Care"
    },
    {
      "citation_id": "16",
      "title": "Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "F Zhang",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "17",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "19",
      "title": "Hierarchical multimodal-fusion of physiological signals for emotion recognition with scenario adaption and contrastive alignment",
      "authors": [
        "J Tang",
        "Z Ma",
        "K Gan",
        "J Zhang",
        "Z Yin"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need. Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need. Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Multi-modal learning with missing modality via shared-specific feature modelling",
      "authors": [
        "H Wang",
        "Y Chen",
        "C Ma",
        "J Avery",
        "L Hull",
        "G Carneiro"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Generative ai enables eeg super-resolution via spatio-temporal adaptive diffusion learning",
      "authors": [
        "S Wang",
        "T Zhou",
        "Y Shen",
        "Y Li",
        "G Huang",
        "Y Hu"
      ],
      "year": "2025",
      "venue": "Generative ai enables eeg super-resolution via spatio-temporal adaptive diffusion learning"
    },
    {
      "citation_id": "23",
      "title": "Correlates of quality of life in anxiety disorders: review of recent research",
      "authors": [
        "M Wilmer",
        "K Anderson",
        "M Reynolds"
      ],
      "year": "2021",
      "venue": "Current psychiatry reports"
    },
    {
      "citation_id": "24",
      "title": "Camel: capturing metaphorical alignment with context disentangling for multimodal emotion recognition",
      "authors": [
        "L Zhang",
        "L Jin",
        "G Xu",
        "X Li",
        "C Xu",
        "K Wei",
        "N Liu",
        "H Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Feature fusion based on mutual-cross-attention mechanism for eeg emotion recognition",
      "authors": [
        "Y Zhao",
        "J Gu"
      ],
      "year": "2024",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention"
    },
    {
      "citation_id": "26",
      "title": "Dynamic confidence-aware multi-modal emotion recognition",
      "authors": [
        "Q Zhu",
        "C Zheng",
        "Z Zhang",
        "W Shao",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}