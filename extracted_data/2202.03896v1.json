{
  "paper_id": "2202.03896v1",
  "title": "Speech Emotion Recognition Using Self-Supervised Features",
  "published": "2022-02-07T00:50:07Z",
  "authors": [
    "Edmilson Morais",
    "Ron Hoory",
    "Weizhong Zhu",
    "Itai Gat",
    "Matheus Damasceno",
    "Hagai Aronowitz"
  ],
  "keywords": [
    "Speech emotion recognition",
    "selfsupervised features",
    "end-to-end systems"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Self-supervised pre-trained features have consistently delivered state-of-art results in the field of natural language processing (NLP); however, their merits in the field of speech emotion recognition (SER) still need further investigation. In this paper we introduce a modular End-to-End (E2E) SER system based on an Upstream + Downstream architecture paradigm, which allows easy use/integration of a large variety of self-supervised features. Several SER experiments for predicting categorical emotion classes from the IEMOCAP dataset are performed. These experiments investigate interactions among fine-tuning of self-supervised feature models, aggregation of frame-level features into utterance-level features and back-end classification networks. The proposed monomodal speechonly based system not only achieves SOTA results, but also brings light to the possibility of powerful and well finetuned self-supervised acoustic features that reach results similar to the results achieved by SOTA multimodal systems using both Speech and Text modalities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) is an essential capability for Human Computer Interaction (HCI) and has attracted much attention due to its wide range of potential application in e.g. call center conversation analysis, mental health and spoken dialogue systems. Although significant progress has been made  [1] , SER is still a challenging research problem since human emotions are inherently complex, ambiguous, and highly personal. Humans often express their emotions using multiple simultaneous cues, such as voice characteristics, linguistic content, facial expression, and body actions, which makes SER by nature a complex multimodal task  [2] . In addition to that, due to the difficulties in data collection, publicly available datasets often do not have enough speakers to properly cover personal variations in emotion expression. As a consequence of that, some of the most common Deep Learning techniques that have been incorporated to SER are related to the fields of: transfer learning  [3, 4] ; multitask learning  [5] ; multimodal systems  [6, 7] , and more powerful model architecture  [8, 9] . Despite the success of self-supervised features in the field of natural language processing (NLP) in the last year and, more recently, in speech recognition and speaker identification tasks, only few works have investigated this kind of features in the context of SER  [5, 10, 11] . Therefore, we argue that the merits of selfsupervised features in the field of SER still need further investigation. Moreover, in applications where speech is the only modality available, such as in call center conversation analysis, the other modalities such as facial expression and body actions are unusable and, therefore, the SER system must rely only on the information available on the speech signal.\n\nGiven this scenario, the main goals of this paper are: (1) to introduce a modular End-to-End (E2E) SER system based on an Upstream + Downstream architecture model paradigm which allows easy use/integration of a large variety of selfsupervised features, (2) to present a sequence of experiments comparing/analyzing the performance of the proposed E2E system under different configurations and (3) to show that, despite using only the speech modality, the proposed E2E system can reach SOTA results compared to the SOTA results achieved by multimodal systems which use both Speech and Text modalities.\n\nThe rest of this paper is organized as follow: Session 2 presents the proposed End-to-End SER model; The experimental setup is presented in Section 3. Section 4 presents the results. Finally, conclusions and future works are presented in Section 5",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Model",
      "text": "In this paper we formulate the SER problem as a mapping from the continuous speech domain into the discrete domain of categorical labels of emotion. The paradigm used here is the Upstream + Downstream Model which is illustrated in Figure  1 .\n\nThis Upstream + Downstream model is very similar to what has largely been used by the NLP community since the introduction of BERT  [12]  and it has now become more popular for the speech community with the advent of works like Wav2vec 2.0  [13, 14] , TERA  [15] , Mockingjay  [16] , huBERT  [17]  and projects like SUPERB  [18]  and SpeechBrain  [19] . In our SER model, the Upstream is a task-independent model, it is pre-trained in self-supervised fashion and it works as an Encoder or Front-End model responsible for feature extraction; on the other hand, the Downstream is a task-dependent model, it works as a Decoder or Back-End model and it is responsible for the final task of classifying the features generated by the Upstream model into categorical labels of emotion.\n\nThe Self-Supervised Upstream Models used in this work are the latest release of Wav2Vec 2.0  [14]  and the Hidden Unit BERT (huBERT)  [17] . The Downstream models used in this work are: (1) A simple Mean Average Pooling aggregator followed by a linear classifier and (2) the ECAPA-TDNN aggregator  [20]  followed by a linear classifier.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "The IEMOCAP  [21]  corpus used in this paper is a multimodal dyadic conversational dataset. It consists of approximately 12 hours of multimodal data, including speech, text transcriptions and facial recordings. IEMOCAP contains a total of 5 sessions and 10 different speakers, with a session being a conversation of two exclusive speakers. To be consistent and to be able to compare with previous studies, only utterances with ground truth labels belonging to \"angry\", \"happy\", \"excited, \"sad\", and \"neutral\" were used. The \"excited\" class was merged with \"happy\" to better balance the size of each emotion class, which results in a total of 5,531 utterances (happy 1636, angry 1,103, sad 1,084, neutral 1,708). Unless otherwise stated, leave-onesession-out 5-fold cross validation (CV) is used and the average result reported. At each fold of the 5-fold CV setup, 2 speakers are used for testing and the samples from the 8 speakers remaining are randomly split into 80% for training and 20% for validation. The splitting done here is exactly the same as the one done by SUPERB  [18] , which splits each of the 5 IEMOCAP folds into three subsets: a training-set, a validation-set and a test-set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fine-Tuning Of The Upstream Models",
      "text": "In order to boost the performance of our SER system we fine-tuned our Upstream models using the categorical labels of emotion from the IEMOCAP dataset described in Section 2.1. The fine-tuning of our Upstream model is performed by training it jointly with a simple Mean-Average Pooling Network followed by a Linear Classifier, as described in Figure  2 . This fine-tuning is performed for each of the 5-folds of IEMOCAP dataset. Therefore, at the end of the fine-tuning process we come up with 5 different fine-tuned Upstream models (one for each of the 5-folds).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Average Of Checkpoints",
      "text": "In addition to the process of fine-tuning, in order to minimize the output variance of the Upstream models, the 5 best fine-tuned Upstream Model checkpoints are averaged  [22] . This processing is done separately for each of the 5folds from the IEMOCAP dataset. The selection of the 5 best checkpoints to be averaged (for each of the 5-folds) is done solely based on the accuracy of the fine-tuned Upstream Model on the validation-set. The test-set is not observed by this process at any moment. This procedure of fine-tuning (FT) followed by Checkpoint Averaging (AVG) has been performed for both Wav2Vec 2.0 and huBERT, generating a total of 10 finetuned and averaged Upstream Models: 5 FT-AVG Wav2Vec 2.0 and 5 FT-AVG huBERT models.\n\nGiven these fine-tuned and averaged Upstream Models for Wav2Vec 2.0 and huBERT and the two possible variations of our Downstream model we have conducted several experiments which are described in Figure  3 , Figure  4  and in Table  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments For Evaluation",
      "text": "Among the several experiments that can be performed using our SER model, we have selected the experiments illustrated in Figure  3  and Table  1  in the Sets (1.A) and (1.B). The goal of these experiments is to understand the following: (1) the importance of fine-tuning the Upstream model; (2) the importance of averaging the Upstream and Downstream Model Checkpoints; (3) how Wav2vec 2.0 and huBERT can be combined to boost SER performance and (4) the performance of the two aggregators used: Mean Pooling and ECAPA-TDNN. Unless otherwise stated, in all following experiments both Upstream and Downstream model checkpoints are averaged following the same procedure described in Section 2.3  [22] .\n\nReferring to Figure  3 , in experiments (1-4) the Upstream model used is either Wav2vec 2.0 or huBERT and the Downstream model is composed by a Mean Average Aggregator followed by a linear classifier. In experiments in (1-2) neither Upstream nor Downstream model are averaged at all. In experiments in (3-4) both Upstream and Downstream models are averaged.\n\nExperiments  (5) (6)  are similar to experiments (3-4), the only difference is the Aggregator used, which is now the ECAPA-TDNN.\n\nIn the experiment  (7)  we have an early fusion between the Wav2vec 2.0 and the huBERT features, which is done just before going through the ECAPA-TDNN aggregator.\n\nIn the experiment  (8)  we have a later fusion between the utterance embeddings generated by two ECAPA-TDNNs, the first one operating on the Wav2vec 2.0 features and the second one operating on the huBERT features.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments To Be Used As Baselines",
      "text": "In addition to the experiments from Figure  3  we have also prepared the baseline/reference experiments presented in Figure  4 . These baseline models standard hand-crafted Filter-Bank features for the acoustic modality and a BERT model to extract feature embeddings for the text modality. This BERT model has also been fine-tuned and averaged using the same processes describe in Sections 2.2 and 2.3. It is important to emphasize that the Fbank used here does not have explicit pitch information attached to it and that the fine-tuning optimization process of the BERT model may not follow the most advanced SOTA techniques available nowadays. However, despite not being as carefully prepared as it could be, these baseline models can help us to obtain insight on how powerful these fine-tuned and averaged Wav2vec 2.0 and huBERT features are.\n\nReferring to Figure  4 , in experiment (  9 ) a standard Filter-bank is used as the Upstream Model. In experiment  (10)  BERT model is used as the Upstream model. Finally, in experiment  (11)  a standard Fbank for the acoustic modality and BERT features for the text modality are used jointly in a later fusion fashion. In addition to the baselines in Figure  4  a carefully comparison between the results achieved by our SER model and the SOTA results available on the literature will be done in Section 4.1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "Results of the experiments presented in Figure  3  and 4  are summarized in Table  1  and described in detail bellow:\n\nIn column 2 of Table  1 , under the term (#), we indicate the number of the 11 experiments evaluated. In column 3 we indicate the input modality used in each experiment.\n\nIn Since the test sets of IEMOCAP are slightly imbalanced between different emotion categories, in column 6 of Table  1  under the term Accuracy we report both Weighted Accuracy (WACC) and Unweighted Accuracy (UACC). Finally, in column 1 of Table  1  under the term SET we have: in (1.A) the subset of experiments from Figure  3  that use Mean Pooling as Aggregator; in (1.B) the subset of experiments from Figure  3  that use ECAPA-TDNN as Aggregator and in (2) the baseline experiments described in Figure  4 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Set",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussions",
      "text": "From the results presented in Table  1  we can highlight the following observations:\n\nâ€¢   2  shows a comparison between the results achieved by our method (experiment 7) against the best baselines found on the literature for 5-fold CV on IEMOCAP. The first 4 baselines use audio-only and the fifth baseline uses audio+text. However, the fifth baseline  [1]  uses the ground truth transcriptions and in addition to that they also use context dependent text embeddings with a window size of  [-3,3] . As a summary, we can state that we have reached SOTA results for exp. 7 and to the best of our knowledge, the result achieved by exp. 7 is the best result reported so far for SER using 5-fold CV on the IEMOCAP dataset for the case of speech-only input modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions And Future Works",
      "text": "In this work we presented an E2E SER model based on an Upstream + Downstream model paradigm which allow easy use of pretrained, fine-tuned and averaged Upstream models. Several experiments using 5-fold CV IEMOCAP dataset were performed, and we have clearly shown that well designed combinations of carefully fine-tuned and averaged Upstream models and averaged Downstream models can significantly improve the performance of E2E SER models. We believe that these results extend even further the possibility of improving E2E SER models by exploiting huge amount of unlabeled data available for pre-training self-supervised acoustic features.\n\nIn future work we intend to extend our proposed Upstream + Downstream SER model to (i) support multitask learning and (ii) exploit multimodality, such as speech + text modalities and speech + text + visual modalities.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This Upstream + Downstream model is very similar to",
      "page": 1
    },
    {
      "caption": "Figure 1: Proposed SER model architecture",
      "page": 2
    },
    {
      "caption": "Figure 2: This fine-tuning is performed for each of the 5-folds of",
      "page": 2
    },
    {
      "caption": "Figure 2: Upstream model fine-tuning process",
      "page": 2
    },
    {
      "caption": "Figure 4: and in Table 1.",
      "page": 2
    },
    {
      "caption": "Figure 3: and Table 1 in the Sets (1.A) and (1.B). The goal",
      "page": 2
    },
    {
      "caption": "Figure 3: , in experiments (1-4) the",
      "page": 2
    },
    {
      "caption": "Figure 3: Experiments selected to evaluate our SER model.",
      "page": 3
    },
    {
      "caption": "Figure 3: we have also",
      "page": 3
    },
    {
      "caption": "Figure 4: These baseline models use standard hand-crafted",
      "page": 3
    },
    {
      "caption": "Figure 4: , in experiment (9) a standard",
      "page": 3
    },
    {
      "caption": "Figure 4: Experiments used as baseline/reference models to be",
      "page": 3
    },
    {
      "caption": "Figure 3: In addition to the baselines in Figure 4 a carefully",
      "page": 3
    },
    {
      "caption": "Figure 3: and 4 are",
      "page": 3
    },
    {
      "caption": "Figure 3: that use Mean Pooling as Aggregator; in (1.B) the subset",
      "page": 4
    },
    {
      "caption": "Figure 3: that use ECAPA-TDNN as",
      "page": 4
    },
    {
      "caption": "Figure 4: 4.1 Discussions",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Selected experiments to evaluate our SER model: Sets 1.A and 1.B. Baseline experiments: Set 2.",
      "data": [
        {
          "Set": "",
          "#": "",
          "Input \nmodality": "",
          "Upstream model": "Input feature",
          "Downstream model": "AGG",
          "(Accuracy %)": "WACC"
        },
        {
          "Set": "1.A",
          "#": "1",
          "Input \nmodality": "S",
          "Upstream model": "W2V2",
          "Downstream model": "Mean",
          "(Accuracy %)": "74.09"
        },
        {
          "Set": "",
          "#": "2",
          "Input \nmodality": "S",
          "Upstream model": "huBERT",
          "Downstream model": "Mean",
          "(Accuracy %)": "72.99"
        },
        {
          "Set": "",
          "#": "3",
          "Input \nmodality": "S",
          "Upstream model": "W2V2",
          "Downstream model": "Mean",
          "(Accuracy %)": "76.47"
        },
        {
          "Set": "",
          "#": "4",
          "Input \nmodality": "S",
          "Upstream model": "huBERT",
          "Downstream model": "Mean",
          "(Accuracy %)": "75.20"
        },
        {
          "Set": "1.B",
          "#": "5",
          "Input \nmodality": "S",
          "Upstream model": "W2V2",
          "Downstream model": "ECAPA",
          "(Accuracy %)": "76.58"
        },
        {
          "Set": "",
          "#": "6",
          "Input \nmodality": "S",
          "Upstream model": "huBERT",
          "Downstream model": "ECAPA",
          "(Accuracy %)": "75.56"
        },
        {
          "Set": "",
          "#": "7",
          "Input \nmodality": "S",
          "Upstream model": "huBERT + W2V2",
          "Downstream model": "ECAPA",
          "(Accuracy %)": "77.36"
        },
        {
          "Set": "",
          "#": "8",
          "Input \nmodality": "S",
          "Upstream model": "huBERT & W2V2",
          "Downstream model": "ECAPA",
          "(Accuracy %)": "77.04"
        },
        {
          "Set": "2",
          "#": "9",
          "Input \nmodality": "S",
          "Upstream model": "Fbank",
          "Downstream model": "ECAPA",
          "(Accuracy %)": "56.52"
        },
        {
          "Set": "",
          "#": "10",
          "Input \nmodality": "T",
          "Upstream model": "BERT",
          "Downstream model": "ECAPA",
          "(Accuracy %)": "69.34"
        },
        {
          "Set": "",
          "#": "11",
          "Input \nmodality": "S + T",
          "Upstream model": "Fbank & BERT",
          "Downstream model": "ECAPA",
          "(Accuracy %)": "70.56"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Selected experiments to evaluate our SER model: Sets 1.A and 1.B. Baseline experiments: Set 2.",
      "data": [
        {
          "#": "1",
          "Method": "Sajjad et al. [23]",
          "Modalities": "Audio",
          "(UACC %)": "72.25"
        },
        {
          "#": "2",
          "Method": "Wang et al. [24]",
          "Modalities": "Audio",
          "(UACC %)": "73.30"
        },
        {
          "#": "3",
          "Method": "Liu et al. [25]",
          "Modalities": "Audio",
          "(UACC %)": "70.78"
        },
        {
          "#": "4",
          "Method": "Zhao et al. [26]",
          "Modalities": "Audio",
          "(UACC %)": "71.70"
        },
        {
          "#": "5",
          "Method": "Wu et al. [6]",
          "Modalities": "Audio + Text",
          "(UACC %)": "78.30"
        },
        {
          "#": "",
          "Method": "Ours (exp. 7)",
          "Modalities": "Audio",
          "(UACC %)": "77.76"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "M3ER: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya"
      ],
      "year": "2020",
      "venue": "Proc.AAAI"
    },
    {
      "citation_id": "4",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "Shahzad Younis"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end ASR models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Speech Emotion Recognition with Multi-task Learning",
      "authors": [
        "X Cai",
        "J Yuan",
        "R Zheng"
      ],
      "year": "2021",
      "venue": "Speech Emotion Recognition with Multi-task Learning"
    },
    {
      "citation_id": "7",
      "title": "Emotion Recognition by Fusing Time Synchronous and Time Asynchronous Representations",
      "authors": [
        "W Wu",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2021",
      "venue": "Emotion Recognition by Fusing Time Synchronous and Time Asynchronous Representations"
    },
    {
      "citation_id": "8",
      "title": "Jointly Fine-Tuning BERT-Like Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weersasekera"
      ],
      "year": "2021",
      "venue": "Jointly Fine-Tuning BERT-Like Self Supervised Models to Improve Multimodal Speech Emotion Recognition"
    },
    {
      "citation_id": "9",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion Recognition from Speech Using Wav2vec 2.0 Embeddings"
    },
    {
      "citation_id": "12",
      "title": "Temporal Context in Speech Emotion Recognition",
      "authors": [
        "Y Xia",
        "Li Chen",
        "A Rudnicky",
        "R Stern"
      ],
      "year": "2021",
      "venue": "Temporal Context in Speech Emotion Recognition"
    },
    {
      "citation_id": "13",
      "title": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee"
      ],
      "year": "2019",
      "venue": "BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805v2"
    },
    {
      "citation_id": "14",
      "title": "Wav2vec",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "venue": "Wav2vec"
    },
    {
      "citation_id": "15",
      "title": "Framework for Self-Supervised Learning of Speech Representations",
      "year": "2020",
      "venue": "Framework for Self-Supervised Learning of Speech Representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "16",
      "title": "Robust Wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "authors": [
        "W Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko"
      ],
      "year": "2021",
      "venue": "Robust Wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "arxiv": "arXiv:2104.01027v2"
    },
    {
      "citation_id": "17",
      "title": "TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech",
      "authors": [
        "A Liu",
        "S Li",
        "H Lee"
      ],
      "year": "2021",
      "venue": "Transaction on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Mockingjay: Unsupervised Speech Representation Learning with Beep Bidirectional Transformer Encoders",
      "authors": [
        "A Liu",
        "S Yang"
      ],
      "year": "2020",
      "venue": "Mockingjay: Unsupervised Speech Representation Learning with Beep Bidirectional Transformer Encoders"
    },
    {
      "citation_id": "19",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W Hsu",
        "B Bolte"
      ],
      "year": "2021",
      "venue": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "arxiv": "arXiv:2106.07447v1"
    },
    {
      "citation_id": "20",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Yang",
        "P Chi",
        "Y Chuang"
      ],
      "year": "2021",
      "venue": "SUPERB: Speech Processing Universal PERformance Benchmark"
    },
    {
      "citation_id": "21",
      "title": "SpeechBrain: A General-Purpose Speech Toolkit",
      "authors": [
        "M Ravanelli"
      ],
      "year": "2021",
      "venue": "SpeechBrain: A General-Purpose Speech Toolkit",
      "arxiv": "arXiv:2106.04624v1"
    },
    {
      "citation_id": "22",
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2021",
      "venue": "ISCA"
    },
    {
      "citation_id": "23",
      "title": "IEMOCAP: Interactive Emotional Dyadic Motion Capture Database",
      "authors": [
        "C Busso",
        "Ml Bulut",
        "C Lee"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation"
    },
    {
      "citation_id": "24",
      "title": "Training Tips for the Transformer Model",
      "authors": [
        "M Popel",
        "O Bojar"
      ],
      "year": "2018",
      "venue": "Training Tips for the Transformer Model",
      "arxiv": "arXiv:1804.00247v1[cs.CL]"
    },
    {
      "citation_id": "25",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition with local-global aware deep representation learning",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition",
      "authors": [
        "Z Zhao",
        "Z Bao",
        "Z Zhang"
      ],
      "year": "2019",
      "venue": "Attention-Enhanced Connectionist Temporal Classification for Discrete Speech Emotion Recognition"
    }
  ]
}