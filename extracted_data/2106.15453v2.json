{
  "paper_id": "2106.15453v2",
  "title": "Critically Examining The Domain Generalizability Of Facial Expression Recognition Models",
  "published": "2021-06-29T14:41:19Z",
  "authors": [
    "Varsha Suresh",
    "Gerard Yeo",
    "Desmond C. Ong"
  ],
  "keywords": [
    "Affective Computing",
    "Facial Expressions",
    "Ethical/Societal Implications",
    "Transfer Learning",
    "Domain Generalization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Expression Recognition is a commercially-important application, but one under-appreciated limitation is that such applications require making predictions on out-of-sample distributions, where target images have different properties from the images the model was trained on. How well-or how badly-do facial expression recognition models do on unseen target domains? We provide a systematic and critical evaluation of transfer learning-specifically, domain generalization-in facial expression recognition. Using a state-of-the-art model with twelve datasets (six collected in-lab and six \"in-the-wild\"), we conduct extensive round-robin-style experiments to evaluate classification accuracies when given new data from an unseen dataset. We also perform multi-source experiments to examine a model's ability to generalize from multiple source datasets, including (i) within-setting (e.g., lab to lab), (ii) cross-setting (e.g., in-the-wild to lab), and (iii) leave-one-out settings. Finally, we compare our results with three commercially-available software. We find sobering results: the accuracy of single-and multi-source domain generalization is only modest. Even for the best-performing multi-source settings, we observe average classification accuracies of 65.6% (range: 34.6%-88.6%; chance: 14.3%), corresponding to an average drop of 10.8 percentage points from the within-corpus classification performance (mean: 76.4%). We discuss the need for regular, systematic investigations into the generalizability of affective computing models and applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent technological advancements in machine learning and computer vision have resulted in the proliferation of facial expression recognition technologies that claims to accurately recognize emotions from facial expressions. Startups and large technology firms alike offer technologies that claim to predict what a person is feeling, based on their facial expressions alone-often, from a still image with no context. And more and more companies are using these services to power their own \"emotionally-intelligent\" offerings.\n\nIntuitively, such a technological capability seems plausible and well within the capabilities of modern deep learning methods. Consider object recognition: deep learning models seem to be able to classify scores of objects at accuracies and speeds that outperform humans  [1] , and at mind-boggling scales. Moreover, people are excellent at reading emotions from faces, and this ease might confer a confidence that machines could learn to do this task as well.\n\nYet, there has been growing evidence that facial expression recognition is far from being a solved problem, and for many reasons. A recent comprehensive review argues that the inference of emotional states from facial expressions may itself be problematic, because there is considerable variability: there exists not a one-to-one mapping between emotions and \"facial configurations\", but a complex, many-to-many mapping  [2] . Directly citing the previous paper, a report from the AI Now Institute even called for regulators to ban the use of affect recognition in decisions that could impact people's lives, citing the \"contested scientific foundations of affect recognition technology\"  [3] .\n\nGiven the increasing ubiquity of facial expression recognition technology, it is essential to critically examine the scientific validity of such technology. Here, we conduct a thorough and systematic review of one aspect of validity: how well do these models perform when asked to make predictions on novel data?\n\nAn under-appreciated limitation of these models is external generalizability. These state-of-the-art systems, especially deep learning-based systems  [4] , are trained using supervised learning, which involves minimizing the classification error on a dataset of faces and emotion labels. Even though it is standard practice to \"hold out\" a portion of the dataset to evaluate the test accuracy of a classifier, such cross-validation techniques provide a measure of the expected classification accuracy only for new samples drawn from the same distribution as the training data. But in many applications, the data that one wishes to make predictions on may be very different-along both emotion-relevant as well as emotion-irrelevant dimensions-from the training distribution. Multiple factors such as differences in the features that are predictive of emotions when the expressions are posed or generated spontaneously (an important distinction for facial expression recognition datasets), changes in facial feature distributions due to ethnicity, gender, and age; or even \"emotion-irrelevant\" changes in pose, background complexity, and illumination, can all contribute to what are called \"dataset shifts\", \"feature-space shifts\", or \"distribution shifts\", which can impact a model's performance on different datasets  [5] ,  [6] ,  [7] ,  [8] ,  [9] ,  [10] .\n\nUsing a model trained in one context (e.g., in one domain or on a specific task) to make predictions in another context (e.g., to a different domain or task) is broadly known as transfer learning  [11] ,  [12] ,  [13] , where the knowledge in a trained model is \"transferred\". Here, we focus on the particular case of transfer learning called domain generalization  [10] ,  [14] , where the facial expression recognition models are performing the same task (facial expression recognition), but in a new domain-a target dataset that is different from the source dataset that it was trained on. Specifically, in this investigation, we restrict the model to not see any training examples from the target dataset before having to make predictions on the target dataset (as opposed to domain adaptation, in which the model is given examples from the target dataset to learn to 'adapt' to the target domain). We chose this bar because domain generalization is precisely what commercially available software claims to do: users can simply upload any image (of a face) without any context, and these software would make a prediction of the emotion \"present\" in the image.\n\nIn this paper, we conduct the largest-to-date systematic empirical evaluation of domain generalization using twelve facial expression recognition datasets, six of which are collected in the laboratory, and six are collected inthe-wild, i.e., with more naturalistic variation (see Fig.  1 ). We used three state-of-the-art deep learning-based models: The first, ResNet50  [15]  with pre-trained weights from VG-GFace2  [16] ; the second: Inception-ResNet  [17]  pre-trained on CASIA-WebFace dataset  [18] ; and ResNet-50 with entropy regularisation as a domain generalization technique  [19] . For brevity, we report only the results of the first model in the main text; the latter two models qualitatively replicate the results of the first model and demonstrate that our results are not specific to one model architecture; these are reported in full in the Appendix.\n\nWe conducted three experiments to evaluate the generalization performances of facial expression recognition models. In Experiment 1, we investigate the cross-corpus domain generalization performance when the model is trained on one source dataset and evaluated on a different target dataset-with twelve datasets, this produces (12*11=) 132 permutations for which the source and target are different.\n\nIn Experiment 2, we evaluate multiple-source training, which is a common practice for domain generalization approaches in machine learning fields like Natural Language Processing  [20] ,  [21]  and Computer Vision  [22] ,  [23] ,  [24] . This is to empirically verify if training on multiple facial expression recognition datasets improves model generalization. We perform experiments in a variety of setups: 1) within-setting (e.g., train on in-lab data, test on in-lab data), 2) cross-setting (e.g., train on in-lab data, test on \"in-thewild\" data), and 3) a leave-one-out setup where we train on eleven datasets and evaluate on the last dataset.\n\nIn Experiment 3, we evaluate the generalization of three widely available commercial Application Programming Interfaces, or APIs (Amazon Rekognition, Megvii Face++, and Microsoft Azure) on the datasets included in this study, and compare their results with our Experiments 1 and 2. We end with a frank discussion about the implications of our work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Psychological Evidence",
      "text": "Humans are emotionally expressive, and the most visible expressions of emotions occurs on our faces  [25] ,  [26] ,  [27] . Facial expressions provide important information about others' emotions: for example, a smile or a frown could lead one to infer that a person is feeling happy, or angry, respectively. For decades, psychologists have studied how people recognize emotions from observed facial configurations  [28] ,  [29] ,  [30] . Early work also seemed to support a theory that some emotions may be universally expressed and recognized across cultures  [31] , and this thinking has been very influential in computer science. However, other recent work, such as a meta-analysis conducted by  [25]  found that although these \"basic emotions\" (namely, anger, disgust, fear, joy, sadness, and surprise  [31] ) were recognized at betterthan-chance level, there were still cultural variations in the accuracy of emotion recognition, and these accuracies are far from being perfect (depending on the specific experiment). Another meta-analysis  [32]  suggested that the correlations between the \"prototypical\" facial configurations associated with emotions (as operationalized by Action Unit profiles) and actual emotional experiences is low, between r=13. to r=.30 depending on the analysis. This issue of variability was examined in even greater detail by  [2] , who critically examined the evidence for the scientific validity of inferring emotional states from facial expressions, arguing that facial configurations are not \"fingerprints\" that signal particular emotional states across contexts and cultures: instead these associations are variable, and heavily dependent on context.\n\nThe recent psychological evidence suggests that emotion recognition from faces is not a straightforward task: faces are not expressed or interpreted the same way across contexts, or across individuals from different cultures. There is obviously information in facial expressions that allow people to accurately perceive the underlying emotions, but in real life, people often make these inferences with more information (information from other modalities, knowledge about the context, cultural knowledge). Thus, the machine learning challenge of inferring emotions from facial expressions alone should be thought of more as an underdetermined inference problem  [33] , instead of being similar to object classification. We return to this point in the Discussion.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Automated Facial Expression Recognition",
      "text": "That said, automated facial expression recognition has been explored over several decades and using many approaches  [34] ,  [35] . Early approaches to facial expression recognition used hand-crafted features: some examples are Active Appearance Model (AAM)  [36] , Scale Invariant Feature Transform (SIFT)  [37] , Local Binary Patterns (LBP)  [38] , and Histogram of Oriented Gradients (HOG)  [39]  which is used, for example, by Affectiva's AffDex  [40] . These features are then subsequently used to classify an image into one of several emotion categories.\n\nIn recent years, the availability of large facial expression recognition datasets and the increase in computational capabilities have enabled the training of end-to-end deep learning models such as CNN-based architectures like ResNet  [15]  and AlexNet  [41] . Deep learning approaches have emerged as the most successful approach to facial expression recognition  [4] . The modal approach today start with models pre-trained on large facial recognition datasets such as VGGFace2  [16] , and then fine-tune them to facial expression recognition using a facial expression recognition dataset  [42] ,  [43] ,  [44] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Domain Facial Expression Recognition",
      "text": "Deep learning models are trained to minimize some objective function on a held-out subset of their labeled data, where the training data and the held-out data have similar distributions of features. Thus, these models will only work well in scenarios and applications where the new data for which predicted labels are desired are drawn from the same feature distribution as the data the models were trained on. It is important to understand how these models perform in, or \"transfer to\", new contexts that have a different data distribution compared to the training data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Domain Adaptation In Facial Expression Recognition",
      "text": "One such transfer task is formally known as domain adaptation. In domain adaptation, the aim is to improve the performance of a model that is trained using a source data from some domain (e.g., Dataset A) on an target data from a different domain (e.g., Dataset B), using some amount of data from the target domain, which allows the model to learn to \"adapt\" its learning to the target domain. A number of methods have been proposed  [75] ,  [76] ,  [77] ,  [78] . Specifically for facial expression recognition, one class of approaches focus on reweighting training instances based how different they are from the target samples  [67] ,  [68] . Other approaches focus on minimizing the discrepancy between the source and target distributions using methods such as subspace matching  [66] , or by incorporating discrepancy metrics such as Maximum Mean Discrepancy to the objective function  [69] ,  [70] ,  [71] ,  [74] . A third class of techniques use adversarial methods to \"un-learn\" the domain-specific discriminative features  [72] ,  [73] . These approaches, however, assume that some amount of target data is available (either labeled or unlabeled), which allows the model to adapt to the target domain.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Domain Generalization",
      "text": "A similar but stricter task tries to get the model to generalize to new domains where there is no target data available in any form. These are called domain generalization approaches, and have been studied widely outside of facial expression recognition  [10] ,  [14] . Some approaches focus on increasing the number and diversity of data samples either by dataaugmentation  [79] ,  [80] ,  [81]  or synthetic data generation  [82] ,  [83] ,  [84] . Another set of approaches uses data from multiple domains (i.e., multiple source datasets) to learn domain-invariant features using methods like adversarial losses  [19] ,  [85] ,  [86] ,  [87] , or moment-matching  [88] ,  [89] .\n\nWithin facial expression recognition, there are only three papers that we know of that studied domain generalization techniques.  [23]  used dual feature-extractors to find domaininvariant features: one extractor minimized the distances between the features of samples across different domains that belong to the same class, while the second aimed to find distinguishing features amongst the classes.  [22]  used adversarial learning where a gradient-reversal layer was used to learn domain-invariant features. In addition, their model also minimized the feature distances between samples across multiple domains that belong to same class.  [24]  used a triplet loss to minimise the distances between the feature vectors of samples from the same classes.\n\nDomain generalization is a difficult problem, but it needs to be solved to apply facial expression recognition in the real world. The main results in the current paper use a commonly-used deep-learning architecture for predicting across different domains; We specifically do not incorporate any generalization techniques (e.g., learning domaininvariant features) to simulate the performance of real-life applications that may not have considered generalization issues. But to also show how explicit domain-generalization training could improve our results, we adapted  [19] 's Entropy Regularization technique to facial expression recognition, and we report this model in the Appendix.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Summary Of Cross-Domain Work",
      "text": "We summarize the work on cross-domain facial expression recognition in Table  1 . In addition to the domain adaptation and domain generalization papers cited above, we also include papers that report any cross-corpus prediction results TABLE 1 Summary of previous research on cross-domain prediction in facial expression recognition, including the number of datasets they used, a brief description of their single-source and multi-source experiments, and whether they considered domain adaptation/generalization techniques (\"None\" indicates that the paper did not use any transfer learning techniques). Leave-one-out means that one dataset is used as target, while the remaining datasets are used as source datasets, and this is repeated for all target datasets. and which did not use any specific transfer techniques  [38] ,  [45] ,  [46] ,  [47] ,  [48] ,  [49] ,  [50] ,  [51] ,  [52] ,  [53] ,  [54] ,  [55] ,  [56] ,  [57] ,  [58] ,  [59] ,  [60] ,  [61] ,  [62] ,  [63] ,  [64] ,  [65] . (These papers often report the results of their models on several target datasets as evidence for the generalizability of their methods. We note that these models were not trained or fine-tuned on the target dataset.)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Citation",
      "text": "In summarizing the large number of citations in Table  1 , we wish to point out two key observations. First, the vast majority of the 34 papers we identified considered very few datasets, with only 9 papers  [22] ,  [23] ,  [24] ,  [63] ,  [64] ,  [65] ,  [72] ,  [73] ,  [74]  that considered 5 or more datasets in their experiments, with the maximum number being eight datasets  [24] ,  [74] . But even amongst all these papers, there is not much systematicity in the source-target combinations that they considered, with many papers only examining a small subset of the possibilities. For example, of the papers that examined 5 or more datasets, only  [73]  considered all sourcetarget combinations for single-source transfer, and only  [64]  considered all leave-one-out combinations for multi-source transfer. For single-source transfer, the remaining papers often fix one dataset as a source and examine transfer to other datasets, which introduces the possibility that their results may be due to the idiosyncracies with that chosen source dataset. For multi-source transfer, it is even harder to describe the trend: each paper seems to have their own considerations for making their (non-exhaustive) choices.\n\nThese observations raise two important and related points. One: We need systematic investigations using a larger number of datasets, and two: given a large set of datasets, we need to study much more source-target combinations-ideally, we would do so exhaustively. Of course, there are practical reasons why this is difficult: even considering single-source transfer, the number of sourcetarget combinations scales with the square of the number of datasets (for n datasets, this is (n)(n -1)): the scaling for multi-source transfer is even worse. But think about why we need to do so: we know that datasets vary widely with respect to how the data was collected, the contexts in which the emotions are expressed, and the demographics represented in the sample. Understanding such heterogeniety is key to understanding generalization across different domains. And so we need systematic investigations into this issue if we are ever to prove that facial expression recognition is generalizable (across contexts), which is a cornerstone of scientific validity  [2] ,  [90] .\n\nThis paper addresses both gaps, by using 12 datasets (a 50% increase over the previous maximum of 8), and by systematically considering all source-target combinations for single-source transfer, and including a \"leave-one-out\" design for multi-source transfer.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Empirical Evaluation Details",
      "text": "We designed three experiments to investigate the generalization of models trained on one subset of datasets, to a target dataset (Fig.  2 ). In these experiments, we primarily varied the choice and combination of source datasets. We evaluated models on the test split of the target datasets. In the simplest, single-source case, Experiment 1, we trained a model on a single dataset. In Experiment 2, we trained a model using multiple source datasets. We performed three types of source-dataset combinations. We combined multiple datasets from same setting (i.e., either in-the-wild or in-lab) to test the generalizability within and across settings. And lastly, we combined multiple datasets from multiple settings (both in-the-wild and in-lab), in a leaveone-out manner where we use eleven source datasets. In Experiment 3, we assessed three commercial APIs, Amazon Rekognition, Megvii Face++, and Microsoft Azure. For the within-setting and cross-setting conditions, we used 5 source datasets with the same setting (i.e., 5 in-lab or 5 in-the-wild datasets), where the difference is only whether the target dataset comes from the same or different setting. For the Leave-one-out condition, we used the maximum number  (11)  of source datasets, and tested on the last target dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We used six in-lab datasets: JAFFE, GEMEP, CK+, Oulu-CASIA, KDEF and, IASLab and six in-the-wild datasets: EmotioNet, SFEW, RAF-DB, Aff-Wild2, FER-2013 and Af-fectNet (see Fig.  1 ). For all datasets, we use the same subset of seven classes, six emotions and neutral: {anger, disgust, fear, happiness, sadness, surprise} and {neutral}. The exceptions are EmotioNet and GEMEP which do not have a neutral class.\n\n1) The JApanese Female Facial Expression (JAFFE)  [91]  dataset contains of 213 images from 10 different Japanese female participants, who were asked to show facial expressions corresponding to six emotions, as well as neutral expressions. The original paper does not provide a train/validation/test split, so we created a 80/10/10 split for our experiments.\n\n2) The GEneva Multimodal Emotion Portrayals (GEMEP)  [92]  dataset is a video dataset that has 10 actors portraying 18 affective states in the lab. As the GEMEP dataset contains videos with a frame rate of 25 frames per second, we sampled a still frame for every 12 frames from all the videos. Each video is labelled with one of six emotions, and so we label frames with the label of the video. In total, we extracted 265 frames from the GEMEP dataset. We note that GEMEP does not have neutral images. The original paper does not provide a train/validation/test split, so we created a 80/10/10 split for our experiments.\n\n3) The extended Cohn-Kanade (CK+)  [93]  dataset comprises posed expression sequences (from neutral to peak expression). We select a subset of 309 sequences that are labelled with the six emotions listed above. In each sequence, we extract the final (peak) image, which is labelled with one of six emotions. In addition, one neutral image is taken from each participant. The original paper does not provide a train/validation/test split, so we created a 80/10/10 split for our experiments. 4) The Oulu-CASIA  [94]  dataset consists of image sequence (neutral to peak expression) taken from 80 subjects. Each subject has an image sequence labelled with one of six emotion categories (80*6 = 480 images).\n\nFor our experiment we take the last frame of each sequence, along with one neutral image per participant. This dataset also consists of images taken in both visible and near infrared with three types of illumination, but for the purposes of our experiment (and following previous papers  [24] ,  [74] ), we only consider images taken in visible light with normal illumination. The original paper does not provide a train/validation/test split, so we created a 80/10/10 split for our experiments. 5) The Karolinska Directed Emotional Faces (KDEF)  [95]  dataset consists of still images from 70 amateur actors. Each image is annotated with the six emotions we are interested in along with neutral, and also taken from 5 different angles. For our experiments (and following previous papers  [24] ,  [65] , we use the 980 images that consists of only the frontal face images.   [97]  dataset consists of images extracted from the video dataset Acted Facial Expressions in the Wild (AFEW). This dataset consists of 1,365 images extracted from movies, where each image is labelled with one of the six emotions or neutral. We used the validation set provided by the authors as our held-out test set, and we divided the original training set into a 80:20 training/validation split. 9) The Real-world Affective Faces Database (RAF-DB,  [98] ) dataset contains 30K images collected from flickr and filtered using keywords such as \"smile\", \"shocked\", and \"disgust\". We used the version of the dataset annotated with the six emotions, and neutral. We used the test set provided by the authors, and we divided the original training set into a 80:20 training/validation split. 10) The Aff-Wild2  [99]  dataset consists of YouTube reaction videos and was released as a part of the ABAW challenge 1 . In our work we use the frame images and frame-level annotations provided by the authors of the dataset  [99] . Frames are annotated with the six emotions and neutral. These frame images were extracted from the videos that have an average frame rate of 30 frames per second and we sampled after every 30 frames. In total, we extracted 26,553 frames. We used the validation set provided by the authors as our heldout test set, and we divided the original training set into  Note that these numbers are for the seven classes that we consider, so they may differ from the dataset sizes reported in the original papers.\n\na 80:20 training/validation split. 11) The FER-2013  [100]  dataset contains 35,887 images in seven categories, obtained using Google Search from a set of 184 emotion-related keywords like \"blissful\" and \"enraged\". We use the train/validation/test split provided by the authors for our evaluation. 12) The AffectNet  [101]  dataset contains facial images collected by querying 3 major search engines using 1,250 emotion related keywords in six different languages. We use a subset of 460,039 images that are labelled with the six emotions and neutral. We used the validation set provided by the author as our held-out test set, and we divided the original training set into a 80:20 training/validation split. The distribution of labels are given in Table  2 , and we note that some datasets are heavily imbalanced (i.e., they do not contain an equal distribution among the emotion classes). We summarize the dataset sizes in Table  3 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Architectures",
      "text": "We choose two model architectures: (i) ResNet-50  [15]  and (ii) Inception-ResNet  [17] . These models are commonly used as base architectures and baselines for Facial Expression Recognition  [55] ,  [74] ,  [102] ,  [103] . Previous research has found better facial expression recognition performance when these models are pre-trained on facial recognition datasets, as opposed to generic object recognition datasets like ImageNet. Hence, we used a ResNet-50 model pretrained on VGGFace2 dataset  [16]  and an Inception-ResNet  Diagonal entries (underlined) indicate the reference performance when model is trained and tested on the same dataset (i.e., within-corpus performance) while the off-diagonal entries are for cross-corpus to the target dataset. Best-performing values for each column, excluding the diagonals, are bolded. Models trained on GEMEP and EmotioNet as source did not see any neutral class during training. The last two rows give the minimum difference in cross-corpus performance compared to within-corpus (i.e., bolded valuesunderlined) and the averaged difference in cross-corpus performance compared to within-corpus (i.e., average of non-underlined valuesunderlined).\n\nmodel pre-trained on CASIA-WebFace dataset  [18] . And for our multi-source experiments, we also implement a recent approach proposed for domain generalization using Entropy Regularisation  [19] . We integrate entropy regularisation into a ResNet-50 model pre-trained on VGGFace2. Because our results were qualitatively similar across all three models, we report only the first ResNet-50 model (without domain generalization) in the main text. We report the results of the Inception-ResNet model and the ResNet-50 model with domain generalization in the Appendix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "All images were first pre-processed with MTCNN  [104]  to detect faces and facial landmarks. All images were aligned using an affine transformation with the landmark location of the eyes, and were all subsequently converted to grayscale. The images were resized to a 224 x 224 pixel square for the ResNet50 model, and 160 x 160 pixel square for the Inception-ResNet model. We performed horizontal flipping for a randomly selected 50% of the images to augment training and prevent overfitting.\n\nTo train our models, we utilised a NVIDIA RTX6000 GPU. We set the learning rate to 0.001 and used a step learning rate scheduler with a step size of 10 and a decay factor γ of 0.5. We trained our models using Stochastic Gradient Descent, with a momentum value of 0.9 for 30 epochs. We used early stopping to choose the best-performing model, based on validation accuracy. For single-source experiments, we used a batch size of 64, while for all multi-source experiments, we used a batch size of 128. For training using multiple datasets (Experiment 2), we combined and shuffled the training sets of the respective source datasets.\n\nTo evaluate our models, we report the top-1 accuracy, which is simply the proportion of correctly predicted samples. As most of the datasets are imbalanced, we also report the weighted F1-score in the Appendix.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment 1: Single-Source Generalization",
      "text": "In Experiment 1, we test whether models trained on one dataset generalize well to other datasets. The top-1 accura-cies of the ResNet-50 model on every source-target combination are given in Table  4 , with the source datasets on the rows and the target datasets on the columns. The diagonal entries give the within-corpus performance when the model is trained on the training partition and evaluated on the test partition of the same dataset-these values serve as the reference. The off-diagonal entries give the cross-corpus performance where the model is trained and validated on a given source dataset's train/validation partition (rows) and evaluated on a target dataset's test partition (columns). We reiterate that the off-diagonal values show the performance of the model when predicting labels on the target dataset without seeing any target samples during training.\n\nFirst, as a sanity check, we observed that the withincorpus performance for all twelve datasets, with two exceptions (discussed below), is the highest in each columnthat is, the maximum performance in each column is the diagonal entry-which is not surprising as the evaluation data comes from the same distribution as the training data. We also note that the within-corpus performances of the inlab datasets are relatively high (from 83.1 to 91.2%, with Oulu-CASIA being an outlier at 65.4%), and are higher than the corresponding performances of the in-the-wild datasets (55.8% to 89.1%, with SFEW being an outlier at 45.7%). We note that the aim of this paper is to assess the domain generalizability of a commonly-used model, instead of innovating on model architecture to surpass state-of-the-art performance: that said, our model performs comparably to other papers that use similar architectures (e.g., if we focus on the more challenging naturalistic datasets: AffectNet, ours: 58.6, reported: 52%-58.4%  [74] ,  [105] ; Aff-Wild2: ours: 57.7% reported: 50%-56%  [106] ). (Later, we also compare all our results against the literature, in Table  7 ).\n\nOur first result is that cross-corpus performance is generally poor compared to the within-corpus model performance. The last two rows in Table  4  give the minimum and average drop in cross-corpus performance compared to the within-corpus model. Even if we consider only the minimum drop in performance-i.e., the difference between the best-performing cross-corpus model for that dataset (column) and the within-corpus model-we see differences TABLE 5 Experiment 2: Results of the Within-Setting and Cross-Setting conditions using ResNet50 pre-trained on VGGFace2, where each row indicates a model that is trained on five datasets from the same setting (i.e., five in-lab or five in-the-wild datasets). For example, the first row gives the model trained on five in-lab datasets (excluding JAFFE), and we have one performance value for \"within-setting\" (i.e., tested on JAFFE) and six values for \"cross-setting\" (i.e., tested on the six in-the-wild datasets). Values indicate the average of seven-class classification accuracy over 5 runs, with standard deviation given in parentheses. Best-performing values for each column, are bolded. The last two rows give the non-transfer reference performance (from Experiment 1, i.e., the diagonal values from Table  4 ) and the difference compared with the best-performing (bold) values.\n\nin performance of approximately -12.3% on average, with the highest being a 46.9% drop in classification accuracy (where GEMEP is the target dataset). The only exceptions are (i) the model trained on AffectNet and tested on SFEW, which outperformed the within-corpus performance by 7.3 percentage points, and (ii) the model trained on AffectNet and tested on Aff-Wild2, which differs from the withincorpus performance by a small 0.3 percentage points.\n\nHowever, taking the best-performing single-source cross-corpus model may be too generous a comparison, as we would require oracular knowledge of which would be the best-performing model in the first place. If we instead look at the average change in performance, we see something more sobering: the mean classification accuracy across all twelve target datasets is only 42.0% (compared to chance at 14.3%), which translates to an average decrease in classification accuracy, compared to the within-corpus performance, of -34.4 percentage points (with a range of decreases from -14.7 to -60.1 percentage points). We notice that 8 of the 12 best-performing cross-corpus performance (i.e., the bolded values) come from the same source dataset: AffectNet -we can see this in the last row of Table  5 . AffectNet also happens to be the largest dataset, so perhaps one interim observation is that we can improve cross-corpus performance by increasing the number of training samples.\n\nWe also note that there exists substantial heterogeneity in the results, which suggests that some datasets may be more idiosyncratic. For example, the classification accuracies, including the within-corpus accuracy, on Oulu-CASIA are lower compared to the other in-lab datasets, and the performances on SFEW are lower compared to the other inthe-wild datasets. Other datasets have high within-corpus accuracy, but are significantly more difficult for other crosscorpus models to transfer to, such as JAFFE and GEMEP. This analysis highlights the limitations of drawing inferences about transfer learning when using a single source dataset (or small number of source datasets), as each dataset has its own idiosyncrasies (see most studies in Table  1 ).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment 2: Multi-Source Generalization",
      "text": "Experiment 1 examined training on only one source dataset. Next, Experiment 2 focuses on increasing the number of source datasets. This could help the model learn domaininvariant features from different distributions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Within-And Cross-Setting Conditions",
      "text": "In the first two conditions, we were interested in testing the intuition that the setting in which the dataset is collected matters. Researchers have traditionally distinguished posed expressions (by actors or naïve volunteers) that are collected in the lab from naturalistic, spontaneous expressions collected in the wild. Presumably, because images from in-lab datasets are more controlled and are visually similar, we might expect higher performance when a model trained on in-lab data is tested on other in-lab data (\"within-setting\"), than when tested on in-the-wild data (\"cross-setting\"). And similarly for models trained on in-the-wild data.\n\nAs we have six in-lab and six in-the-wild datasets, we chose to maximize the number of source datasets that would still allow us to test this hypothesis. Thus, we chose to fix the number of source datasets at five. Specifically, we would train our model on five in-lab datasets; we can then evaluate it on the last in-lab dataset (to give us a value for withinsetting cross-corpus transfer), on all six in-the-wild datasets (to give us six values for cross-setting cross-corpus transfer). We can do the same for the other in-lab datasets; as well as for all the in-the-wild datasets.\n\nThese results are given in Table  5 . All of the bestperforming combinations (i.e., bold values in Table  5 ) were obtained from the models trained using in-the-wild datasets, irrespective of the evaluation type (i.e., whether they were evaluated in within-setting cross-setting). This is somewhat surprising given the strong distinction between \"in-lab\" and \"in-the-wild\" datasets: one would expect that training on data collected in more similar settings would yield better performance due to the the similarity in training/test data distributions. However, there is a confound in that all the in-the-wild are larger than the in-lab datasets. The results is thus consistent with an alternate hypothesis, that increasing the number of training samples, irrespective of their setting, yields better performance.\n\nCompared to the best single-source transfer, we note that the best 5-source multi-source performance improves or remains the same for ten of the twelve datasets, except for a slight dip in performance for JAFFE (51.8 to 50.9) and RAF-DB (79.8 to 78.9). But when we then compare to the withincorpus performance (last row of Table  5 ), we see that again for ten of the twelve datasets, the 5-source performance still underperforms the within-corpus performance (except for Oulu-CASIA and Aff-Wild2).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Leave-One-Out: Maximum Number Of Source Datasets",
      "text": "In this condition we wanted to see what cross-corpus performance the model could achieve with the maximum amount of training data we had. For each target dataset, we train on the remaining 11 source datasets and evaluate the performance on this target dataset. The results are given in Table  6  (with a full table for the additional two models provided in the Appendix).\n\nFor ten of the twelve datasets, the leave-one-out accuracy still underperforms the within-corpus classification performance. Averaging across all twelve datasets, the mean seven-class classification accuracy is 65.6%, which corresponds to a mean performance drop of 10.8 percentage points, compared to the within-corpus classification. In general, we see that this experiment also corroborates the finding from the previous two conditions, that the larger the amount of data generally improves the cross-corpus performance for all datasets. In order to see this more clearly, we examine these trends statistically in the next analysis.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Statistically Testing The Effect Of Dataset Size",
      "text": "The results of Experiment 2 suggest that the size of the training corpus seems to be an important factor that affects cross-corpus performance (i.e. greater the size of the pre-trained on VGGFace2. In this condition we train on 11 source datasets and report the classification accuracies on the test partition of the last, held-out test set. We also report the differences in classification accuracies compared with the within-corpus results from Experiment 1. Negative values indicate that the leave-one-out condition performed worse than the within-corpus.\n\ntraining corpus, the better the cross-corpus performance), and perhaps more so than the hypothesis we started with about the setting of the datasets (in-lab or in-the-wild). In order to statistically verify the dependence of the crosscorpus classification performance with the size of the training corpus, we ran statistical analyses using mixed-effects linear models, attempting to predict classification accuracy (as the dependent variable) using corpus size. As dataset sizes varied over orders of magnitude, we converted the sizes of the training corpus to its base-10 logarithm.\n\nWe obtained all the cross-corpus performance for each target dataset from Experiments 1 and 2. For each target dataset, we have 19 data points-From Experiment 1, 11 data points for single-source cross-corpus prediction, and from Experiment 2, 1 data point for the \"within-setting\" condition, 6 data points for the \"cross-setting\" condition, TABLE 7 Summary of cross-corpus facial expression recognition performance from the literature. Values indicate classification accuracies. For the second and third columns, we reproduce the within-corpus results from Experiment 1, and the cross-corpus leave-one-out performance from Experiment 2. For each of the papers (with their citations in square brackets), we report the performance corresponding to the best-performing model they report in their paper. Like Table  1 , we group papers into three groups: if they did not incorporate any domain generalization or adaptaion mechanism (\"None\"), if they incorporated domain generalization techniques (\"Generalization\") or if they incorporated domain adaptation techniques (\"Adaptation\"). Adaptation approaches use a small amount of target samples (as compared to seeing no target samples at all for None and Generalization, as well as our cross-corpus results), and hence we would expect the Adaptation results to be higher.\n\nand 1 data point for the leave-one-out condition. We note several characteristics of this data: One, there is idiosyncratic variation across target datasets that we would like to model (e.g., some datasets are more difficult than others). Two, individual-cross corpus performances are nested within a specific target dataset (e.g., all of the crosscorpus results on KDEF share that commonality with each other, but not with the other data points). Thus, we used a mixed-effects linear model controlling for random intercepts and random slopes nested within target dataset.\n\nWe found a statistically significant association of corpus size with performance accuracy on the target dataset (b = 10.2 [95% Confidence Interval = 8.72, 11.7], t = 13.6, p < .001). We can interpret this coefficient as suggesting that, on average across the datasets (and assuming a linear trend), that every order of magnitude increase in the training corpus size translates to a 10.2 percentage-point increase in accuracy on the target dataset.\n\nSee Figure  3  for a visual illustration of the relationship between training corpus size and cross-corpus classification performance across all the datasets. We note two positive and two sobering observations. First, we can observe the results of the statistical model: how, for all the target datasets, increasing the training corpus size improves crosscorpus classification accuracy-even when this particular model has no specific domain-generalization mechanism. Second, for most of the target datasets, the performance of the models trained on the largest corpus sizes approaches the within-corpus performance. For some, like SFEW and AffWild2, the best cross-corpus performance can match or slightly surpass the within-corpus performance. But we also note that for others like JAFFE and GEMEP and to a smaller extent, some in-the-wild datasets like FER2013 and AffectNet, there still exists large performance gaps. The first of the two sobering conclusions is that the raw magnitudes of the classification accuracies are not very high, especially for some of the in-the-wild datasets. For SFEW, even though the cross-corpus classification accuracy for the largest corpus size matches and even slightly surpasses the within-corpus accuracy, both these accuracies are still only in the 50% range for a 7-class classification. Finally, there also exists substantial heterogeniety across datasets. This makes it even more important for future papers to describe their training datasets in great detail, as well as to conduct systematic investigations into how their models transfer to different populations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparing Experiment 2 Results To The Literature",
      "text": "We end this section with a comparison of our results with results that were previously reported in the literature and which we earlier summarized in Table  1 . In Table  7 , we document the cross-corpus results reported by each paper. In providing this table, we acknowledge that there is a lot of heterogeneity that we are aggregating out: each paper uses different models with different training protocols and with different source datasets. For example, for the most wellstudied example, CK+, the 24 classification accuracies from the literature range from 38.6% to 93.5%.\n\nTable  7  demonstrates that our results are not a uniquely poor characterization of corss-corpus performance in facial expression recognition. In fact, looking across each row of Table  7  (corresponding to each dataset), our results are all near the higher end of the results reported in the literature.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiment 3: Using Commercial Apis",
      "text": "Finally, we wanted to evaluate how commerical APIs would do, on the set of datasets that we had collected. Dupré and colleagues  [107]  recently compared eight commercial products with human ratings collected on two databases, one posed (BU-4DFE) and one spontaneous (UT-Dallas). They considered classification of the same six emotions, but did not include neutral. They found that the performance of the automatic classifiers ranged from 48% to 62%, with the classifiers doing better on the posed, compared to the spontaneous expressions. (An interesting result they found is that human observers achieved a classification accuracy of 72.5%, which is higher than the commercial products, but perhaps not as high as one might have expected.)\n\nIn Experiment 3, we evaluated the generalization of three commercially-available APIs: Face++  [108] , Amazon Rekognition  [109] , and Microsoft Azure  [110] . The use of such APIs spans a wide range of applications in domains such as market research, healthcare, education, and finance  [111] . Aside from facial expressions, these APIs can also \"detect\" other attributes, such as age and gender, presence of facial hair and eye glasses, and facial landmarks. We focus only on emotion attributes returned by the APIs.\n\nThe three APIs classify facial images into a set of emotion classes along with the confidence level for each emotion. We take the emotion class with the highest confidence as that API's prediction. For Face++, the emotions classes returned map exactly to the six emotions plus neutral that we have considered in this paper. Amazon Rekognition returns a slightly different set of emotions (nine classes), namely {anger, calm, confused, disgust, fear, happy, sad, surprised, and unknown}. The six emotions we considered are a subset of the nine classes that Rekognition returns, but Rekognition does not have a neutral class. For the case when the Rekognition API returns a result with the highest confidence level on calm, confused, or unknown, we took the emotion with the next highest confidence level that is in our set as the label for that image (we note that empirically, none of the images were labelled by Rekognition as unknown). Azure predicts the six emotions we have considered in this paper plus neutral and additionally contempt. If Azure returns a result with the highest confidence level on contempt, we took the emotion with the next highest confidence level as the label for that image. On 21 June 2022, Microsoft Azure discontinued their emotion recognition API service (discussed more in the Ethical Impact Statement below), and so we only have results for seven of the twelve datasets.\n\nWe used only the test-set images of each dataset as input into the APIs. In Table  8 , we report the percentage of images on which the APIs did not detect a face, and hence could not predict the emotion. For many of the datasets, this non-detection rate was 0% or low, with one outlier: For an unknown reason, Face++ was unable to detect 67.9% of the images on FER-2013 (the corresponding rates for Azure and for Rekognition were 13.9% and 0.8%).\n\nFigure  4  presents the performance of the commercial APIs. Generally, the API performances are quite similar for some datasets (e.g., JAFFE, IASLab, AffectNet), and seem to follow another pattern where performance increases from Rekognition to Face++ to Azure (e.g., CK+, RAF-DB, AffWild2, FER2013). However, we do not want to read too much into the numerical reuslts as these APIs are being updated all the time  [112]  (or even being discontinued like Azure), and there were limitations (e.g., mismatch in the set of emotion classes, especially the lack of neutral for Rekognition). But across all the datasets, the API performances still underperform the within-corpus results by an average of roughly 25 percentage points (For Rekognition, mean: -30pp., range: [-57, -7]; for Face++, mean: -25pp., range:  [-79, -11]  ; for Azure, mean: -17pp., range: [-59, +6]).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Discussion",
      "text": "In this study, we conducted the largest-to-date systematic investigation of domain generalization (cross-corpus prediction) using 12 facial expression recognition datasets and a comprehensive suite of experiments. The results are sobering. Generally, we conclude that single-source, single-target generalization performance for facial expression recognition is poor, achieving a mean accuracy of 42.0% for a sevenclass classification. This translates to a mean decrease in classification accuracy of 34.4 percentage points (Range: -14 to -60pp.) compared to the within-corpus performance (mean: 76.4%). This by itself may not be surprising given how idiosyncractic individual datasets may be. But even if we pooled data from multiple datasets together (in Experiment 2), we still find that the best-performing models still achieve an average accuracy of 65.6%, which corresponds to a mean performance drop of 10.8 percentage points. And even commercial APIs today underperform the withincorpus results by on average 25 percentage points.\n\nWe discuss the specific takeaways from our experiments with respect to training these classifiers. Then we discuss some limitations of our experiments in particular, and limitations of the datasets and the state of the FER field more broadly. Finally, we discuss broader implications of the work, especially in light of recent psychological evidence.\n\nFirst, we show that modern deep learning models perform better when trained with larger amount of data from different domains, even without specific domain generalization mechanisms. We verified across all our experiments that increasing the size of the training corpus improves crosscorpus classification accuracy, but with diminishing returns: every order of magnitude increase in corpus size translates to a 10 percentage point increase, on average.\n\nOur results also highlight the dangers of relying on a single or small number of target-datasets for making claims about generalization. Some of the datasets in our study did not transfer well to other datasets, with very low transfer performance and/or very variable performance. We specifically chose to do a \"round-robin\" format and to discuss general trends, rather than focusing on specific datasets. Moving forward, affective computing research should prioritize the need to evaluate their models on a range of different datasets, in order to validate their generalizability.\n\nOur study, though systematic and rigorous, has several limitations. First, although we examined twelve datasetsthe largest such examination in domain adaptation and generalization in facial expression recognition-there were many other datasets that we were not able to include in this study, for numerous reasons, such as they were labelled with different emotion categories than those we were interested in. Second, facial expression recognition datasets tend to have imbalanced emotion label distributions. Of the twelve datasets we examined, IASLAB was the most well balanced (with each of seven classes making up between 13.7-14.8% of the dataset), while others like AffectNet were severely imbalanced (47.4% Happy and 26.4% Neutral, with the remaining 26.2% divided among 5 classes). These dataset imbalances are known to introduce bias and impact the robustness of the models trained on these datasets, and this is a major concern especially if these or similarly-imbalanced datasets provide the training data for commercially-deployed models. Future work could examine other state-of-the-art facial expression recognition models to assess their generalizability. It could be the case that adding more datasets, having more balanced datasets, or trying different model architectures / training protocols could change several of the quantitative results we found in this study. However, we believe that addressing these limitations will lead to differences in degree, but not differences of kind, and hence our generic conclusions should generalize.\n\nOne huge limitation of the entire endeavour of facial expression recognition (beyond our study) is that it is confined to a small set of emotion categories, and neglects variation across cultures and demographic groups. This extensive study with so many datapoints was possible only when restricted to a small set of six emotions (plus neutral): once we consider some of the myriad other emotions that we experience in daily life, there is far fewer data out there.\n\nBut recent psychological evidence should also challenge the field to reconsider the validity of the notion of \"facial expression recognition\"  [2] . In our opinion, the terminology suggests an equivalence to \"object recognition\", but the two tasks are not close. While there is an objective truth to an object label (\"cat\"), and perhaps there could also be an objective label of facial movements like \"frown\", it is far more complicated to attribute an emotional state to the person, or to perceive an emotion in that person. Emotions arise from subjective responses to events in the world, and are affected by a host of contextual factors, including the social and cultural settings. We should consider facial expressions as only one cue from which we have to solve this underdetermined inference problem of inferring what someone is feeling  [33] . When we conceptualize the problem in this way, it naturally guides us to think about context, or multimodal inference, or considering the temporal history preceding the emotional event, or a host of other factors that suggest a more complete understanding of (perceiving) someone's emotional state. And it suggests a reason as to why \"emotion recognition\" from decontextualised faces alone may even prove to be a fools' errand (and why even with today's technologies, this is not yet a \"solved problem\").",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "The proliferation of commercial facial expression recognition technology has raised concerns both about the scientific basis of inferring emotions from facial expressions  [2] , as well as ethical concerns surrounding the potential misuses of such technology  [3] ,  [90] ,  [113] . The lack of systematic investigations-like this study-on the out-ofdomain generalizability of deep learning models, a prerequisite for scientific validity, should be another cause for worry. Researchers all know that transfer performance on out-of-distribution examples will suffer, and by definition all deployed technology operates in this regime, where new data comes from a different distribution as the data the models were trained on. But, until the present extensive study, we do not know just how much performance really suffers for facial expression recognition. Importantly, such technologies are already being deployed in a variety of applications, and if we cannot guarantee that our AI models can accurately recognize facial expressions even on a mix of well-controlled and curated datasets, how much confidence can we have that they will work when deployed?\n\nIronically, the incentives in academic publishing are even set up against these types of investigations, because such well-controlled systematic research using existing models and existing datasets are deemed \"not novel\". As a field, we need to \"slow down\" to conduct these extensive investigations to systematically assess the generalizability of our technologies as it is a necessary pre-requisite for ethical deployment  [90] . But if we look outside academia and consider commercially-deployed products, there are even less incentives for such systematic investigations-especially if the results suggest that more years of research are needed before revenue can start coming in. We need to pressure commercial offerings to similarly demonstrate such generalizability. One possible solution is through government regulation or auditing, for example creating a parallel model to the US National Institutes of Standards and Technology's Facial Recognition Vendor Test (FRVT) program to assess potential vendors of facial recognition technology. Another solution that could work through academic or non-profit partners would be to create audit or \"accreditation\" programs with carefully curated (and secret) data that ensures the desired diversity (e.g., of demographics or emotions). Note that we think these are possible solutions that may not, by themselves, be sufficient to deem facial expression technology \"ethical\" by any means: these are just to ensure one necessary criterion, which is that of generalizability.\n\nAs we were in the middle of conducting this research, Microsoft Azure announced in a blogpost  [114]  on 21 June 2022, that they were ceasing to offer their emotion recognition API services, citing as one of their reasons the \"inability to generalize the linkage between facial expression and emotional state across use cases, regions, and demographics\". Although the service stoppage directly impacted the present study (producing in some missing values in Experiment 3), we agree with their assessment. We hope that our study will also prompt other companies to reconsider the validity of these API offerings, and to constantly evaluate and assess their offerings in light of scientific evidence.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "We presented the largest-to-date systematic empirical evaluation of domain generalization in facial expression recognition using twelve datasets with single-source and multi-source training setups. Our analysis shows that singlesource, single-target generalization is poor. We can improve the performance by training models on multiple source datasets, but even in our best-performing settings (train on eleven datasets and test on one dataset), we find that mean seven-class classification performance (of ∼65.6%) is still significantly lower than the corresponding withincorpus accuracies (∼76.4%), with a mean performance drop of 10.8 percentage points! Commercially-available products also underperform the within-corpus accuracies by ∼25 percentage points. Real-world performance on novel data will almost certainly be worse, especially when we consider other sources of variability like extensions to other emotions (beyond these most commonly-studied emotions), diverse cultures and demographic groups (beyond those represented in these datasets), and so forth. We urge a great deal of caution when making or evaluating assertions about the generalizability of current and future facial expression recognition models.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Table 9",
      "text": "Replication of Experiment 1 with a second model. Results of single-source, single-target experiment using Inception-ResNet. Values indicate the average of seven-class classification accuracy over 5 runs, with standard deviation given in parentheses. Rows correspond to source datasets, and columns to target datasets. Diagonal entries (underlined) indicate the reference performance when model is trained and tested on the same dataset (i.e., within-corpus performance) while the off-diagonal entries are for cross-corpus to the target dataset. Best-performing values for each column, excluding the diagonals, are bolded. Models trained on GEMEP and EmotioNet as source did not see any neutral class during training. The last two rows give the minimum difference in cross-corpus performance compared to within-corpus (i.e., bolded valuesunderlined) and the averaged difference in cross-corpus performance compared to within-corpus (i.e., average of non-underlined valuesunderlined).",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Sample images from 11 of the 12 datasets (GEMEP does",
      "page": 2
    },
    {
      "caption": "Figure 2: ). In these experiments, we primarily",
      "page": 5
    },
    {
      "caption": "Figure 2: Schematic of experiments. One example permutation is shown.",
      "page": 5
    },
    {
      "caption": "Figure 1: ). For all datasets, we use the same",
      "page": 5
    },
    {
      "caption": "Figure 3: Plots of cross-corpus classiﬁcation accuracy on the vertical axis against training corpus size on the horizontal axis, by target dataset. Data-",
      "page": 9
    },
    {
      "caption": "Figure 3: for a visual illustration of the relationship",
      "page": 10
    },
    {
      "caption": "Figure 4: Bar graphs showing the classiﬁcation accuracies of the three commercial APIs tested, arranged by target dataset. The within-corpus and",
      "page": 11
    },
    {
      "caption": "Figure 4: presents the performance of the commercial",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Citation": "Gu et al., 2012 [45]\nLi et al., 2017 [46]\nYang et al., 2020 [47]\nCruz et al., 2014 [48]\nZhang et al., 2015 [49]\nLi et al., 2020 [50]\nMavani et al., 2017 [51]\nMeng et al., 2017 [52]\nShan et al., 2009 [38]\nAli et al., [53]\nZhu et al., 2015 [54]\nHasani et al., 2017 [55]\nMayer et al., 2014 [56]\nLiu et al., 2015 [57]\nLopes et al., 2017 [58]\nWen et al., 2017 [59]\nda Silva et al., 2015 [60]\nXie et al., 2019 [61]\nBarros et al., 2020 [62]\nZeng et al., 2018 [63]\nMollahosseini et al., 2016 [64]\nZavarez et al., 2017 [65]",
          "# datasets": "2\n2\n2\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n3\n3\n4\n4\n4\n4\n7\n7\n7",
          "Single-Source": "All source-target combinations\nAll source-target combinations\nAll source-target combinations\nAll source-target combinations\nAll source-target combinations\nAll source-target combinations\nOne ﬁxed source\nAll source-target combinations\nOne ﬁxed source\nAll source-target combinations\nOne ﬁxed source\n-\nAll source-target combinations\nTwo datasets ﬁxed as source\nOne ﬁxed source\nOne ﬁxed source\nAll source-target combinations\nAll source-target combinations\nOne ﬁxed source\n-\n-\n-",
          "Multi-Source": "-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nLeave-one-out\n-\n-\n-\n-\n-\n-\nTwo datasets ﬁxed as source\nLeave-one-out\nNon-exhaustive leave-one-out",
          "Transfer": "None\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone"
        },
        {
          "Citation": "Yan et al., 2016 [66]\nMiao et al., 2012 [67]\nZhou et al., 2020 [68]\nYan et al., 2019 [69]\nZhu et al., 2016 [70]\nLi et al., 2018 [71]\nLi et al., 2021 [72]\nChen et al., 2021 [73]\nLi et al., 2020 [74]",
          "# datasets": "3\n3\n4\n4\n4\n6\n7\n7\n8",
          "Single-Source": "All source-target combinations\nOne ﬁxed source\nOne ﬁxed source\nNon-exhaustive combinations\nOne ﬁxed source\nOne ﬁxed source\nOne ﬁxed source\nAll source-target combinations\nOne ﬁxed source\n(All combinations only for baseline)",
          "Multi-Source": "-\nNon-exhaustive leave-one-out\nOnly one combination\n-\n-\n-\n-\n-\nOnly one combination",
          "Transfer": "Adaptation\nAdaptation\nAdaptation\nAdaptation\nAdaptation\nAdaptation\nAdaptation\nAdaptation\nAdaptation"
        },
        {
          "Citation": "Wang et al., 2019 [22]\nJi et al., 2019 [23]\nDias et al., 2022 [24]",
          "# datasets": "4\n5\n8",
          "Single-Source": "-\nNon-exhaustive combinations\n-",
          "Multi-Source": "Non-exhaustive leave-one-out\nNon-exhaustive leave-one-out\nThree datasets ﬁxed as source",
          "Transfer": "Generalization\nGeneralization\nGeneralization"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: trainourmodelonfivein-labdatasets;wecanthenevaluate",
      "data": [
        {
          "Source Datasets": "In-Lab Datasets ...\n... excluding JAFFE\n... excluding GEMEP\n... excluding CK+\n... excluding Oulu-CASIA\n... excluding KDEF\n... excluding IASLab\nIn-the-wild Datasets ...\n... excluding EmotioNet\n... excluding SFEW\n... excluding RAF-DB\n... excluding Aff-Wild2\n... excluding FER-2013\n... excluding AffectNet",
          "Target Dataset\nJAFFE\nGEMEP\nCK+\nOulu-CASIA\nKDEF\nIASLab\nEmotioNet\nSFEW\nRAF-DB\nAff-Wild2\nFER-2013\nAffectNet": "Within-setting\nCross-setting\n48.2 (5.2)\n-\n-\n-\n-\n-\n78.4 (2.0)\n29.9 (2.4)\n49.3 (1.8)\n31.3 (2.9)\n39.6 (1.6)\n43.3 (0.8)\n-\n23.1 (3.8)\n-\n-\n-\n-\n76.7 (2.7)\n26.0 (3.3)\n48.0 (1.2)\n32.5 (1.9)\n34.5 (0.4)\n42.8 (0.9)\n-\n-\n87.7 (3.4)\n-\n-\n-\n75.8 (3.3)\n29.0 (2.4)\n47.2 (2.3)\n30.1 (2.9)\n37.1 (1.5)\n42.1 (0.8)\n-\n-\n-\n55.0 (1.5)\n-\n-\n80.0 (0.7)\n30.2 (2.7)\n50.6 (1.2)\n35.7 (3.7)\n40.7 (1.8)\n43.2 (0.8)\n-\n-\n-\n-\n76.9 (2.1)\n-\n74.6 (2.0)\n27.9 (2.5)\n49.1 (1.7)\n30.1 (2.9)\n39.6 (1.3)\n41.7 (0.7)\n-\n-\n-\n-\n-\n66.5 (2.9)\n72.1 (1.9)\n31.1 (2.4)\n49.9 (1.1)\n26.9 (2.0)\n37.7 (1.5)\n40.4 (1.0)\nCross-setting\nWithin-setting\n83.2 (1.9)\n50.9 (3.8)\n30.8 (7.2)\n87.3 (3.4)\n67.1 (2.0)\n77.3 (2.8)\n77.9 (2.9)\n-\n-\n-\n-\n-\n53.5 (1.3)\n50.0 (3.2)\n30.0 (3.2)\n88.2 (3.0)\n66.4 (3.9)\n78.0 (1.7)\n79.0 (2.0)\n-\n-\n-\n-\n-\n78.2 (4.1)\n80.4 (0.8)\n78.9 (1.2)\n47.3 (2.5)\n34.6 (6.1)\n86.4 (3.6)\n58.6 (4.3)\n-\n-\n-\n-\n-\n50.9 (2.0)\n90.5 (3.0)\n67.5 (2.6)\n60.6 (2.3)\n33.8 (5.0)\n77.1 (2.7)\n74.6 (1.4)\n-\n-\n-\n-\n-\n36.2 (4.4)\n58.7 (0.3)\n50.0 (4.5)\n86.4 (4.3)\n63.6 (4.5)\n77.1 (0.6)\n76.9 (1.5)\n-\n-\n-\n-\n-\n48.0 (0.6)\n40.0 (8.7)\n35.4 (3.2)\n78.2 (6.7)\n58.2 (1.6)\n64.7 (4.6)\n65.8 (3.1)\n-\n-\n-\n-\n-"
        },
        {
          "Source Datasets": "Within-corpus performance\nfrom Exp. 1\nMinimum difference",
          "Target Dataset\nJAFFE\nGEMEP\nCK+\nOulu-CASIA\nKDEF\nIASLab\nEmotioNet\nSFEW\nRAF-DB\nAff-Wild2\nFER-2013\nAffectNet": "90.9 (0.0)\n83.1 (3.1)\n90.9 (2.9)\n65.4 (3.3)\n89.8 (1.7)\n91.2 (0.9)\n89.1 (1.1)\n45.7 (1.7)\n85.3 (0.2)\n58.5 (0.7)\n70.5 (0.5)\n55.8 (0.9)\n-40\n-46.9\n-0.4\n2.1\n-11.6\n-10.8\n-5.9\n7.8\n-6.4\n2.1\n-11.8\n-7.8"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 1: In Table 7, we",
      "data": [
        {
          "JAFFE": "GEMEP",
          "90.9": "83.1",
          "51.8": "34.6",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "-",
          "48.13 [22]": "-",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "-"
        },
        {
          "JAFFE": "CK+",
          "90.9": "90.9",
          "51.8": "88.6",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "38.57 [47], 46.84 [54], 54.05 [45],\n57.6 [60], 61.2 [49], 62.0 [46],\n64.2 [64], 64.4 [50], 66.2 [56],\n71.29 [52], 73.9 [55], 76.05 [59],\n84.47 [61], 85.8 [48], 88.58 [65],\n91.67 [63], 93.46 [57]",
          "48.13 [22]": "85.75 [22], 88.7 [23]",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "78.83 [71], 84.42 [68], 85.71 [73],\n88.51 [72], 89.69 [74]"
        },
        {
          "JAFFE": "Oulu-CASIA",
          "90.9": "65.4",
          "51.8": "64.6",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "50.83 [61], 61.02 [63]",
          "48.13 [22]": "-",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "63.97 [74], 64.38 [72]"
        },
        {
          "JAFFE": "KDEF",
          "90.9": "89.8",
          "51.8": "83.9",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "72.55 [65]",
          "48.13 [22]": "76.45 [22], 84.9 [24]",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "-"
        },
        {
          "JAFFE": "IASLab",
          "90.9": "91.2",
          "51.8": "80.4",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "-",
          "48.13 [22]": "-",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "-"
        },
        {
          "JAFFE": "EmotioNet",
          "90.9": "89.1",
          "51.8": "82.5",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "-",
          "48.13 [22]": "62.3 [23]",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "-"
        },
        {
          "JAFFE": "SFEW",
          "90.9": "45.7",
          "51.8": "54.7",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "29.43 [57], 39.8 [64], 58.29 [63]",
          "48.13 [22]": "49.4 [23]",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "47.55 [71], 51.7 [69], 53.21 [68],\n54.34 [74], 56.43 [73]"
        },
        {
          "JAFFE": "RAF-DB",
          "90.9": "85.3",
          "51.8": "78.6",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "39.0 [46]",
          "48.13 [22]": "43.8 [23]",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "53.1 [69]"
        },
        {
          "JAFFE": "Aff-Wild2",
          "90.9": "58.5",
          "51.8": "60.3",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "-",
          "48.13 [22]": "-",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "-"
        },
        {
          "JAFFE": "FER-2013",
          "90.9": "70.5",
          "51.8": "58.5",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "34.0 [64], 60.19 [61]",
          "48.13 [22]": "-",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "52.37 [71], 58.21 [74], 58.63 [72],\n58.95 [73]"
        },
        {
          "JAFFE": "AffectNet",
          "90.9": "55.8",
          "51.8": "48.6",
          "22.02 [54], 37.36 [58], 40.98 [50],\n41.3 [38], 42.3 [60], 44.32 [65],\n48.67 [53], 50.23 [47], 50.7 [59],\n55.87 [45]": "-",
          "48.13 [22]": "",
          "57.75 [71], 58.51 [67], 61.5 [73],\n61.94 [74], 63.38 [70], 68.54 [72],\n74.07 [68]": "51.84 [74], 52.54 [72]"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source Datasets": "In-Lab Datasets ...\n... excluding JAFFE\n... excluding GEMEP\n... excluding CK+\n... excluding Oulu-CASIA\n... excluding KDEF\n... excluding IASLab\nIn-the-wild Datasets ...\n... excluding EmotioNet\n... excluding SFEW\n... excluding RAF-DB\n... excluding Aff-Wild2\n... excluding FER-2013\n... excluding AffectNet",
          "Target Dataset\nJAFFE\nGEMEP\nCK+\nOulu-CASIA\nKDEF\nIASLab\nEmotioNet\nSFEW\nRAF-DB\nAff-Wild2\nFER-2013\nAffectNet": "Within-setting\nCross-setting\n47.3 (5.2)\n-\n-\n-\n-\n-\n73.3 (2.0)\n30.5 (2.6)\n50.8 (1.6)\n32.0 (3.5)\n35.5 (2.2)\n42.2 (0.8)\n-\n20.0 (3.2)\n-\n-\n-\n-\n74.4 (3.4)\n26.6 (0.9)\n48.1 (1.4)\n29.1 (3.4)\n31.1 (2.8)\n42.6 (0.6)\n-\n-\n83.6 (4.1)\n-\n-\n-\n73.2 (2.7)\n31.0 (2.7)\n49.1 (1.0)\n29.7 (3.8)\n33.5 (2.0)\n40.6 (1.1)\n-\n-\n-\n54.6 (4.5)\n-\n-\n73.5 (3.3)\n29.3 (2.3)\n49.0 (1.8)\n33.0 (5.1)\n32.3 (3.6)\n40.6 (1.3)\n81.0 (1.9)\n-\n-\n-\n-\n-\n73.9 (1.9)\n28.9 (1.0)\n49.6 (1.5)\n31.3 (4.6)\n35.6 (2.0)\n40.4 (0.8)\n-\n-\n-\n-\n-\n66.1 (2.5)\n72.1 (3.7)\n28.3 (1.9)\n48.3 (1.6)\n24.0 (3.1)\n31.9 (3.5)\n39.5 (1.0)\nCross-setting\nWithin-setting\n36.9 (3.4)\n80.3 (1.8)\n80.2 (1.0)\n47.3 (2.5)\n83.2 (2.0)\n61.8 (4.7)\n75.9 (2.1)\n-\n-\n-\n-\n-\n63.6 (3.0)\n52.0 (2.4)\n47.3 (2.5)\n36.9 (8.0)\n83.2 (3.8)\n75.7 (2.2)\n78.5 (2.3)\n-\n-\n-\n-\n-\n77.6 (0.5)\n42.7 (4.1)\n34.6 (2.7)\n84.5 (4.4)\n52.9 (2.4)\n79.2 (2.2)\n77.6 (2.8)\n-\n-\n-\n-\n-\n84.5 (1.9)\n59.3 (1.0)\n44.5 (3.8)\n36.2 (5.2)\n63.6 (5.0)\n76.7 (1.3)\n79.0 (1.2)\n-\n-\n-\n-\n-\n49.1 (2.0)\n55.7 (0.6)\n36.2 (3.4)\n82.3 (1.9)\n63.2 (3.0)\n76.1 (1.9)\n76.5 (1.5)\n-\n-\n-\n-\n-\n47.9 (0.3)\n42.7 (2.5)\n33.8 (3.2)\n77.7 (1.9)\n59.6 (4.1)\n61.0 (4.0)\n59.2 (0.6)\n-\n-\n-\n-\n-"
        },
        {
          "Source Datasets": "Within-corpus performance\nfrom Exp. 1\nMinimum difference",
          "Target Dataset\nJAFFE\nGEMEP\nCK+\nOulu-CASIA\nKDEF\nIASLab\nEmotioNet\nSFEW\nRAF-DB\nAff-Wild2\nFER-2013\nAffectNet": "80.9 (15.2)\n71.5 (6.4)\n88.6 (4.3)\n59.6 (2.7)\n89.2 (1.2)\n90.4 (1.1)\n88.2 (0.8)\n42.2 (1.7)\n83.2 (0.4)\n55.0 (1.2)\n67.0 (0.7)\n55.6 (1.1)\n-31.8\n-34.6\n-4.1\n4.0\n-8.2\n-10.1\n-8.0\n9.8\n-5.6\n4.3\n-11.3\n-7.7"
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Imagenet large scale visual recognition challenge",
      "authors": [
        "O Russakovsky",
        "J Deng",
        "H Su",
        "J Krause",
        "S Satheesh",
        "S Ma",
        "Z Huang",
        "A Karpathy",
        "A Khosla",
        "M Bernstein"
      ],
      "year": "2015",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "2",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "3",
      "title": "AI Now",
      "authors": [
        "K Crawford",
        "R Dobbe",
        "T Dryer",
        "G Fried",
        "B Green",
        "E Kaziunas",
        "A Kak",
        "V Mathur",
        "E Mcelroy",
        "A Sánchez",
        "D Raji",
        "J Rankin",
        "R Richardson",
        "J Schultz",
        "M Whittaker"
      ],
      "year": "2019",
      "venue": "AI Now"
    },
    {
      "citation_id": "4",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Recent advances in transfer learning for cross-dataset visual recognition: A problemoriented perspective",
      "authors": [
        "J Zhang",
        "W Li",
        "P Ogunbona",
        "D Xu"
      ],
      "year": "2019",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "6",
      "title": "WILDS: A benchmark of in-the-wild distribution shifts",
      "authors": [
        "P Koh",
        "S Sagawa",
        "H Marklund",
        "S Xie",
        "M Zhang",
        "A Balsubramani",
        "W Hu",
        "M Yasunaga",
        "R Phillips",
        "I Gao"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "7",
      "title": "Failing loudly: An empirical study of methods for detecting dataset shift",
      "authors": [
        "S Rabanser",
        "Z Lipton"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "An empirical study of pre-trained vision models on out-of-distribution generalization",
      "authors": [
        "Y Yu",
        "H Jiang",
        "D Bahri",
        "H Mobahi",
        "S Kim",
        "A Rawat",
        "A Veit",
        "Y Ma"
      ],
      "year": "2021",
      "venue": "NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications"
    },
    {
      "citation_id": "9",
      "title": "Face Set was supported by the National Institutes of Health Director's Pioneer Award (DP1OD003312",
      "venue": "Face Set was supported by the National Institutes of Health Director's Pioneer Award (DP1OD003312"
    },
    {
      "citation_id": "10",
      "title": "Distributionally robust neural networks",
      "authors": [
        "S Sagawa",
        "P Koh",
        "T Hashimoto",
        "P Liang"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "Generalizing to unseen domains: A survey on domain generalization",
      "authors": [
        "J Wang",
        "C Lan",
        "C Liu",
        "Y Ouyang",
        "T Qin",
        "W Lu",
        "Y Chen",
        "W Zeng",
        "P Yu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "12",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "13",
      "title": "A comprehensive survey on transfer learning",
      "authors": [
        "F Zhuang",
        "Z Qi",
        "K Duan",
        "D Xi",
        "Y Zhu",
        "H Zhu",
        "H Xiong",
        "Q He"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "14",
      "title": "A survey of transfer learning",
      "authors": [
        "K Weiss",
        "T Khoshgoftaar",
        "D Wang"
      ],
      "year": "2016",
      "venue": "Journal of Big data"
    },
    {
      "citation_id": "15",
      "title": "Domain generalization: A survey",
      "authors": [
        "K Zhou",
        "Z Liu",
        "Y Qiao",
        "T Xiang",
        "C Loy"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "17",
      "title": "VGGFace2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "18",
      "title": "Inceptionv4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "Thirty-first AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Learning face representation from scratch",
      "authors": [
        "D Yi",
        "Z Lei",
        "S Liao",
        "S Li"
      ],
      "year": "2014",
      "venue": "Learning face representation from scratch",
      "arxiv": "arXiv:1411.7923"
    },
    {
      "citation_id": "20",
      "title": "Domain generalization via entropy regularization",
      "authors": [
        "S Zhao",
        "M Gong",
        "T Liu",
        "H Fu",
        "D Tao"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "21",
      "title": "MultiQA: An empirical investigation of generalization and transfer in reading comprehension",
      "authors": [
        "A Talmor",
        "J Berant"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Generalizing question answering system with pre-trained language model fine-tuning",
      "authors": [
        "D Su",
        "Y Xu",
        "G Winata",
        "P Xu",
        "H Kim",
        "Z Liu",
        "P Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2nd Workshop on Machine Reading for Question Answering"
    },
    {
      "citation_id": "23",
      "title": "Cross-database facial expression recognition with domain alignment and compact feature learning",
      "authors": [
        "L Wang",
        "J Su",
        "K Zhang"
      ],
      "year": "2019",
      "venue": "International Symposium on Neural Networks"
    },
    {
      "citation_id": "24",
      "title": "Cross-domain facial expression recognition via an intra-category common feature and inter-category distinction feature fusion network",
      "authors": [
        "Y Ji",
        "Y Hu",
        "Y Yang",
        "F Shen",
        "H Shen"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "25",
      "title": "Cross-dataset emotion recognition from facial expressions through convolutional neural networks",
      "authors": [
        "W Dias",
        "R Padilha",
        "G Bertocco",
        "W Almeida",
        "P Costa",
        "A Rocha"
      ],
      "year": "2022",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "26",
      "title": "On the universality and cultural specificity of emotion recognition: a meta-analysis",
      "authors": [
        "H Elfenbein",
        "N Ambady"
      ],
      "year": "2002",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "27",
      "title": "Facial and vocal expressions of emotion",
      "authors": [
        "J Russell",
        "J.-A Bachorowski",
        "J.-M Fernández-Dols"
      ],
      "year": "2003",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "28",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "Facial expression and emotion"
    },
    {
      "citation_id": "29",
      "title": "Universals and cultural differences in the judgments of facial expressions of emotion",
      "authors": [
        "P Ekman",
        "W Friesen",
        "M O'sullivan",
        "A Chan",
        "I Diacoyanni-Tarlatzis",
        "K Heider",
        "R Krause",
        "W Lecompte",
        "T Pitcairn",
        "P Ricci-Bitti"
      ],
      "year": "1987",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "30",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1972",
      "venue": "nebraska symposium on motivation 1971"
    },
    {
      "citation_id": "31",
      "title": "The face of emotion",
      "authors": [
        "C Izard"
      ],
      "year": "1971",
      "venue": "The face of emotion"
    },
    {
      "citation_id": "32",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "33",
      "title": "Do emotions result in their predicted facial expressions? A meta-analysis of studies on the co-occurrence of expression and emotion",
      "authors": [
        "J Durán",
        "J.-M Fernández-Dols"
      ],
      "year": "2021",
      "venue": "Emotion"
    },
    {
      "citation_id": "34",
      "title": "Affective cognition: Exploring lay theories of emotion",
      "authors": [
        "D Ong",
        "J Zaki",
        "N Goodman"
      ],
      "year": "2015",
      "venue": "Cognition"
    },
    {
      "citation_id": "35",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Survey on RGB, 3D, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu",
        "M Sim Ón",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Natural facial expression recognition using differential-aam and manifold learning",
      "authors": [
        "Y Cheon",
        "D Kim"
      ],
      "year": "2009",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Person-specific sift features for face recognition",
      "authors": [
        "J Luo",
        "Y Ma",
        "E Takikawa",
        "S Lao",
        "M Kawade",
        "B.-L Lu"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "39",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "40",
      "title": "Facial expression recognition based on facial components detection and hog features",
      "authors": [
        "J Chen",
        "Z Chen",
        "Z Chi",
        "H Fu"
      ],
      "year": "2014",
      "venue": "International Workshops on Electrical and Computer Engineering Subfields"
    },
    {
      "citation_id": "41",
      "title": "AFFDEX SDK: A cross-platform real-time multiface expression recognition toolkit",
      "authors": [
        "D Mcduff",
        "A Mahmoud",
        "M Mavadati",
        "M Amr",
        "J Turcot",
        "R Kaliouby"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "42",
      "title": "ImageNet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "44",
      "title": "A CNN model for head pose recognition using wholes and regions",
      "authors": [
        "A Behera",
        "A Gidney",
        "Z Wharton",
        "D Robinson",
        "K Quinn"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "45",
      "title": "Face2Exp: Combating data biases for facial expression recognition",
      "authors": [
        "D Zeng",
        "Z Lin",
        "X Yan",
        "Y Liu",
        "F Wang",
        "B Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Facial expression recognition using radial encoding of local gabor features and classifier synthesis",
      "authors": [
        "W Gu",
        "C Xiang",
        "Y Venkatesh",
        "D Huang",
        "H Lin"
      ],
      "year": "2012",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "47",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "Effects of region features on the accuracy of cross-database facial expression recognition",
      "authors": [
        "Y Yang",
        "B Vuksanovic",
        "H Ma"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th International Conference on Agents and Artificial Intelligence. SCITEPRESS-Science and Technology Publications"
    },
    {
      "citation_id": "49",
      "title": "One shot emotion scores for facial emotion recognition",
      "authors": [
        "A Cruz",
        "B Bhanu",
        "N Thakoor"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "50",
      "title": "Facial expression recognition using lp-norm MKL multiclass-SVM",
      "authors": [
        "X Zhang",
        "M Mahoor",
        "S Mavadati"
      ],
      "year": "2015",
      "venue": "Machine Vision and Applications"
    },
    {
      "citation_id": "51",
      "title": "Facial expression recognition with convolutional neural networks via a new face cropping and rotation strategy",
      "authors": [
        "K Li",
        "Y Jin",
        "M Akram",
        "R Han",
        "J Chen"
      ],
      "year": "2020",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "52",
      "title": "Facial expression recognition using visual saliency and deep learning",
      "authors": [
        "V Mavani",
        "S Raman",
        "K Miyapuram"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "53",
      "title": "Identity-aware convolutional neural network for facial expression recognition",
      "authors": [
        "Z Meng",
        "P Liu",
        "J Cai",
        "S Han",
        "Y Tong"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "54",
      "title": "Boosted NNE collections for multicultural facial expression recognition",
      "authors": [
        "G Ali",
        "M Iqbal",
        "T.-S Choi"
      ],
      "year": "2016",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "A transfer learning approach to cross-database facial expression recognition",
      "authors": [
        "R Zhu",
        "T Zhang",
        "Q Zhao",
        "Z Wu"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Biometrics (ICB)"
    },
    {
      "citation_id": "56",
      "title": "Spatio-temporal facial expression recognition using convolutional neural networks and conditional random fields",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "57",
      "title": "Cross-database evaluation for facial expression recognition",
      "authors": [
        "C Mayer",
        "M Eggers",
        "B Radig"
      ],
      "year": "2014",
      "venue": "Pattern recognition and image analysis"
    },
    {
      "citation_id": "58",
      "title": "Au-inspired deep networks for facial expression feature learning",
      "authors": [
        "M Liu",
        "S Li",
        "S Shan",
        "X Chen"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "59",
      "title": "Facial expression recognition with convolutional neural networks: coping with few data and the training sample order",
      "authors": [
        "A Lopes",
        "E Aguiar",
        "A Souza",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "60",
      "title": "Ensemble of deep neural networks with probability-based fusion for facial expression recognition",
      "authors": [
        "G Wen",
        "Z Hou",
        "H Li",
        "D Li",
        "L Jiang",
        "E Xun"
      ],
      "year": "2017",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "61",
      "title": "Effects of cultural characteristics on building an emotion classifier through facial expression analysis",
      "authors": [
        "F Da Silva",
        "H Pedrini"
      ],
      "year": "2015",
      "venue": "Journal of Electronic Imaging"
    },
    {
      "citation_id": "62",
      "title": "Sparse deep feature learning for facial expression recognition",
      "authors": [
        "W Xie",
        "X Jia",
        "L Shen",
        "M Yang"
      ],
      "year": "2019",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "63",
      "title": "The FaceChannel: A fast and furious deep neural network for facial expression recognition",
      "authors": [
        "P Barros",
        "N Churamani",
        "A Sciutti"
      ],
      "year": "2020",
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "64",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "65",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "66",
      "title": "Crossdatabase facial expression recognition based on fine-tuned deep convolutional network",
      "authors": [
        "M Zavarez",
        "R Berriel",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "2017 30th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)"
    },
    {
      "citation_id": "67",
      "title": "Transfer subspace learning for cross-dataset facial expression recognition",
      "authors": [
        "H Yan"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "68",
      "title": "Cross-domain facial expression recognition using supervised kernel mean matching",
      "authors": [
        "Y.-Q Miao",
        "R Araujo",
        "M Kamel"
      ],
      "year": "2012",
      "venue": "2012 11th International Conference on Machine Learning and Applications"
    },
    {
      "citation_id": "69",
      "title": "Uncertainty-aware cross-dataset facial expression recognition via regularized conditional alignment",
      "authors": [
        "L Zhou",
        "X Fan",
        "Y Ma",
        "T Tjahjadi",
        "Q Ye"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "70",
      "title": "Cross-domain facial expression recognition based on transductive deep transfer learning",
      "authors": [
        "K Yan",
        "W Zheng",
        "T Zhang",
        "Y Zong",
        "C Tang",
        "C Lu",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "71",
      "title": "Discriminative feature adaptation for cross-domain facial expression recognition",
      "authors": [
        "R Zhu",
        "G Sang",
        "Q Zhao"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Biometrics (ICB)"
    },
    {
      "citation_id": "72",
      "title": "Deep emotion transfer network for crossdatabase facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "73",
      "title": "JDMAN: Joint discriminative and mutual adaptation networks for crossdomain facial expression recognition",
      "authors": [
        "Y Li",
        "Y Gao",
        "B Chen",
        "Z Zhang",
        "L Zhu",
        "G Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "74",
      "title": "Cross-domain facial expression recognition: A unified evaluation benchmark and adversarial graph learning",
      "authors": [
        "T Chen",
        "T Pu",
        "H Wu",
        "Y Xie",
        "L Liu",
        "L Lin"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "75",
      "title": "A deeper look at facial expression dataset bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "76",
      "title": "Domainadversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "77",
      "title": "Adversarial discriminative domain adaptation",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "78",
      "title": "Dynamic weighted learning for unsupervised domain adaptation",
      "authors": [
        "N Xiao",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "79",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "80",
      "title": "Generalizing to unseen domains via adversarial data augmentation",
      "authors": [
        "R Volpi",
        "H Namkoong",
        "O Sener",
        "J Duchi",
        "V Murino",
        "S Savarese"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "81",
      "title": "Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data",
      "authors": [
        "X Yue",
        "Y Zhang",
        "S Zhao",
        "A Sangiovanni-Vincentelli",
        "K Keutzer",
        "B Gong"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "82",
      "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
      "authors": [
        "J Tobin",
        "R Fong",
        "A Ray",
        "J Schneider",
        "W Zaremba",
        "P Abbeel"
      ],
      "year": "2017",
      "venue": "2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)"
    },
    {
      "citation_id": "83",
      "title": "Progressive domain expansion network for single domain generalization",
      "authors": [
        "L Li",
        "K Gao",
        "J Cao",
        "Z Huang",
        "Y Weng",
        "X Mi",
        "Z Yu",
        "X Li",
        "B Xia"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "84",
      "title": "Com-boGAN: Unrestrained scalability for image domain translation",
      "authors": [
        "A Anoosheh",
        "E Agustsson",
        "R Timofte",
        "L Van Gool"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "85",
      "title": "International Conference on Learning Representations",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "86",
      "title": "Multi-adversarial discriminative deep domain generalization for face presentation attack detection",
      "authors": [
        "R Shao",
        "X Lan",
        "J Li",
        "P Yuen"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "87",
      "title": "Domain generalization using a mixture of multiple latent domains",
      "authors": [
        "T Matsuura",
        "T Harada"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "88",
      "title": "Domain generalization with adversarial feature learning",
      "authors": [
        "H Li",
        "S Pan",
        "S Wang",
        "A Kot"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "89",
      "title": "Robust domain-free domain generalization with class-aware alignment",
      "authors": [
        "W Zhang",
        "M Ragab",
        "R Sagarna"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "90",
      "title": "Domain generalization using causal matching",
      "authors": [
        "D Mahajan",
        "S Tople",
        "A Sharma"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "91",
      "title": "An ethical framework for guiding the development of affectively-aware artificial intelligence",
      "authors": [
        "D Ong"
      ],
      "venue": "9th International Conference on Affective Computuing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "92",
      "title": "Coding facial expressions with Gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "93",
      "title": "Introducing the Geneva multimodal expression corpus for experimental research on emotion perception",
      "authors": [
        "T Bänziger",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "Emotion"
    },
    {
      "citation_id": "94",
      "title": "The extended Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops"
    },
    {
      "citation_id": "95",
      "title": "Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li",
        "M Pietikäinen"
      ],
      "year": "2011",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "96",
      "title": "Karolinska directed emotional faces",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "97",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "98",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE Multimedia"
    },
    {
      "citation_id": "99",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for unconstrained facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "100",
      "title": "Aff-Wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-Wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "101",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "102",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "103",
      "title": "Analysing affective behavior in the second ABAW2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "104",
      "title": "Stcam: Spatial-temporal and channel attention module for dynamic facial expression recognition",
      "authors": [
        "W Chen",
        "D Zhang",
        "M Li",
        "D.-J Lee"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "105",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "106",
      "title": "Facial expression recognition on static images",
      "authors": [
        "T Ngo",
        "S Yoon"
      ],
      "year": "2019",
      "venue": "International Conference on Future Data and Security Engineering"
    },
    {
      "citation_id": "107",
      "title": "Affective expression analysis in-the-wild using multi-task temporal statistical deep learning model",
      "authors": [
        "N.-T Do",
        "T.-T Nguyen-Quynh",
        "S.-H Kim"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "108",
      "title": "A performance comparison of eight commercially available automatic classifiers for facial affect recognition",
      "authors": [
        "D Dupré",
        "E Krumhuber",
        "G Mckeown"
      ],
      "year": "2020",
      "venue": "PloS one"
    },
    {
      "citation_id": "109",
      "title": "Face++",
      "venue": "Face++"
    },
    {
      "citation_id": "110",
      "title": "Emotion",
      "authors": [
        "Amazon"
      ],
      "venue": "Emotion"
    },
    {
      "citation_id": "111",
      "title": "Facial recognition: Microsoft azure",
      "authors": [
        "Microsoft"
      ],
      "venue": "Facial recognition: Microsoft azure"
    },
    {
      "citation_id": "112",
      "title": "Accuracy of three commercial automatic emotion recognition systems across different individuals and their facial expressions",
      "authors": [
        "D Dupré",
        "N Andelic",
        "G Morrison",
        "G Mckeown"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)"
    },
    {
      "citation_id": "113",
      "title": "How did the model change? efficiently assessing machine learning api shifts",
      "authors": [
        "L Chen",
        "M Zaharia",
        "J Zou"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "114",
      "title": "Guidelines for assessing and minimizing risks of emotion recognition applications",
      "authors": [
        "J Hernandez",
        "J Lovejoy",
        "D Mcduff",
        "J Suh",
        "T O'brien",
        "A Sethumadhavan",
        "G Greene",
        "R Picard",
        "M Czerwinski"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "115",
      "title": "Responsible AI investments and safeguards for facial recognition",
      "authors": [
        "S Bird"
      ],
      "venue": "Responsible AI investments and safeguards for facial recognition"
    }
  ]
}