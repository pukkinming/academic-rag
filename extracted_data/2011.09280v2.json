{
  "paper_id": "2011.09280v2",
  "title": "Continuous Emotion Recognition With Spatiotemporal Convolutional Neural Networks",
  "published": "2020-11-18T13:42:05Z",
  "authors": [
    "Thomas Teixeira",
    "Eric Granger",
    "Alessandro Lameiras Koerich"
  ],
  "keywords": [
    "Facial Expression Recognition",
    "Deep Learning",
    "Convolutional Recurrent Neural Networks",
    "Inflated 3D-CNNs",
    "Dimensional Emotion Representation",
    "Long Short Term-Memory"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expressions are one of the most powerful ways for depicting specific patterns in human behavior and describing human emotional state. Despite the impressive advances of affective computing over the last decade, automatic video-based systems for facial expression recognition still cannot handle properly variations in facial expression among individuals as well as cross-cultural and demographic aspects. Nevertheless, recognizing facial expressions is a difficult task even for humans. In this paper, we investigate the suitability of state-of-the-art deep learning architectures based on convolutional neural networks (CNNs) for continuous emotion recognition using long video sequences captured in-the-wild. This study focuses on deep learning models that allow encoding spatiotemporal relations in videos considering a complex and multi-dimensional emotion space, where values of valence and arousal must be predicted. We have developed and evaluated convolutional recurrent neural networks combining 2D-CNNs and long short term-memory units, and inflated 3D-CNN models, which are built by inflating the weights of a pre-trained 2D-CNN model during fine-tuning, using application-specific videos. Experimental results on the challenging SEWA-DB dataset have shown that these architectures can effectively be fine-tuned to encode the spatiotemporal information from successive raw pixel images and achieve state-of-the-art results on such a dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expressions are the results of peculiar positions and movements of facial muscles over time. According to previous studies, face images and videos provide an important source of information for representing the emotional state of an individual  (Li & Deng, 2018) . Facial expression recognition (FER) has attracted a growing interest in recent years. The detection of spontaneous facial expressions in-the-wild is a very challenging task, where performance depends on several factors such as variations among individuals, identity bias of subjects such as gender, age, culture and ethnicity, and the quality of recordings (illumination, resolution, head pose, capture conditions).\n\nEarly research on FER systems was inspired by the fundamentals of human affect theory  (Ekman & Friesen, 1971; Ekman, 1994) , where discrete models are employed to classify facial images into discrete categories, such as anger, disgust, fear, happiness, sadness, surprise, that can be recognizable across cultures. The limited generalization capacity of these models has paved the way for multi-dimensional spaces that can improve the representativeness and accuracy for describing emotions  (Jack et al., 2012) .\n\nThere are three levels for describing emotion: pleasantness, attention and levels of activation. Specifically, emotion recognition datasets annotate emotional state with two values named valence and arousal, where the former represents the level of pleasantness and the latter represents the level of activation, each of these values lying into [-1; 1] range. With these values, we are able to project an individual emotional state into a 2D space called the circumplex model, which is shown in Figure  1 . The level of arousal is represented on the vertical axis, whereas the level of valence is represented on the horizontal axis.\n\nEarly FER systems relied on shallow approaches, which combined handcrafted features, such as eigenfaces  (Oliveira et al., 2011) , Gabor wavelets  (Zavaschi et al., 2011) , local binary patterns (LBP)  (Shan et al., 2009; Zavaschi et al., 2013)  and its variants such as LBP-TOP  Zhao & Pietikainen (2007)  and LBP-SIP  (Wang et al., 2014) , and Weber local descriptor  (Cossetin et al., 2016)  with support vector machines (SVM). However, recent advances in deep learning (DL) and computing technologies have led FER systems to learn discriminant representations directly from face images. Previous studies, firstly exploited static FER datasets, where subjects are associated to discrete and mutually exclusive categories of emotions. Stateof-the-art performance on benchmarking datasets such as FER2013  (Goodfellow et al., 2013) ,   (Warr et al., 2014) .\n\nTFD  (Susskind et al., 2010) , and SFEW  (Dhall et al., 2011)  were achieved using CNN-based approaches  (Georgescu et al., 2019; Kim et al., 2015; Liu et al., 2017; Zhang et al., 2015; Guo et al., 2016; Kim et al., 2016; Pramerdorfer & Kampel, 2016) .\n\nTemporal information and dynamic facial components can play a crucial role for describing facial expressions  (Li & Deng, 2018) . By definition, people express emotions in a dynamic process, and learning spatiotemporal structures has become the current trend. Recent studies have proposed deep architectures for FER, which were trained on video data  (Fayolle & Droit-Volet, 2014 ). In the literature, video sequences have been mostly processed using aggregation and concatenation of features for each facial image into clips, yet they cannot leverage the temporal dependencies over a video  (Bargal et al., 2016; Ding et al., 2016) . To circumvent this issue, convolutional recurrent neural networks (CRNN) and 3D-CNN architectures have been proposed to encode spatiotemporal relations among video frames  (Ayral et al., 2021; Carneiro de Melo et al., 2020) .\n\nIn this paper, state-of-the-art DL models based on CNNs are investigated and compared for video-based emotion recognition where affective states are represented as continuous values in the bi-dimensional space of valence and arousal. This study focuses on state-of-the-art architectures developed based on pre-trained 2D-CNN that allow encoding spatiotemporal relations of facial features in videos, thereby improving the predictions for continuous values of emotion. Starting from pre-trained 2D-CNNs, we fine-tune two types of DL models with videos: (i) a 2D-CNN combined with a long short -term memory (LSTM) structure; (ii) a 2D-CNN inflated into a 3D-CNN. These models predict emotions through regression of valence and arousal values. We assume that long video sequences are captured in-the-wild, and split each sequence into several clips, not only for augmenting the amount of training data, but also by isolating unique facial expressions. For proof-of-concept, CNN architectures such as VGG and ResNet have been pre-trained with ImageNet and RAF-DB datasets, and finetuned with with multiple video sequences from the SEWA-DB dataset to address the problem of predicting valence and arousal. Experiments were conducted over different clip lengths, overlapping ratios, and strategies for fusing annotations. This paper is organized as follows. Section 2 provides a review of DL models proposed for emotion recognition in videos captured in-the-wild, focusing on models that allow encoding the spatio-temporal relations in facial emotions. Section 3 presents an overview of DL models that are proposed for continuous emotion recognition, including pre-processing steps, model architectures, pre-training, fine-tuning procedures and post-processing steps. Section 4 describes the experimental methodology (e.g., protocol, datasets and performance metrics) used to validate DL models for continuous emotion recognition, as well as the experimental results.\n\nSection 5 presents a discussion and a comparison with the state-of-the-art. Conclusions are presented in the last section.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "A conventional approach for dealing with video frames is to aggregate features extracted from each frame into a clip before final emotion prediction  Ding et al. (2016) . In addition to the feature aggregation,  Bargal et al. (2016)  also aggregated mean, variance, minimum and maximum over a sequence of features thus adding some statistical information. However, since feature aggregation cannot exploit inter-correlations between frames and is not able to depict temporal dependencies, this approach has strong limitations. To circumvent this issue, recurrent neural networks (RNN) such as LSTM or 3D-CNN architectures can integrate data series as input, provided that data are sequentially ordered and transitions have a substantial potential of information. While LSTMs can deal with sequential data of variable length in both directions, 3D-CNNs exploit textured variations from sequence of images by extending convolutional kernels to a third dimension. Hence, 3D-CNNs are well suited to encode spatiotemporal information in video-based FER applications.  Tran et al. (2015)  proposed a 3D-CNN model for action recognition on the UCF101 dataset, which encompasses videos classified over 101 action categories. They have shown that 3D-CNNs can outperform 2D-CNNs on different video analysis benchmarks, and could bring efficient and compact features. Several recent studies  (Abbasnejad et al., 2017; Fan et al., 2016; Nguyen et al., 2017; Liu et al., 2018; Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017)  have proposed approaches based on 3D-CNNs for FER, nevertheless all of them deal with discrete emotion prediction.\n\nA popular approach for dealing with temporal sequences of frames is a cascaded network, in which architectures for representation learning and discrimination are stacked on top of each other, thus various levels of features are leaned by each block and processed by the following until the final prediction. Particularly, the combination of CNNs and LSTM units has been shown effectiveness to learn spatiotemporal representations  (Ji et al., 2013; Tran et al., 2015) .\n\nFor instance,  Ouyang et al. (2017)  used a VGG-16 CNN to extract 16-frame sequence of features and fed an LSTM unit to predict six emotion categories. They pre-processed video frames with a multi-task cascade CNN (MTCNN) to detect faces and described each video by a single 16-frame window. Similarly,  Vielzeuf et al. (2017)  used a VGG-16 CNN and an LSTM unit as part of an ensemble with a 3D-CNN, and an audio network. Particularly, they used a method called multi-instance learning (MIL) method to create bag-of-windows for each video with a specific overlapping ratio. Each sequence was described by a single label and contribute to the overall prediction of the matching video clip. Since deep neural networks (DNNs) are highly data-dependent, there are strong limitations for designing FER systems based on DNNs, even more since FER datasets are often small and task-oriented  (Li & Deng, 2018) . Considering this fact, training deep models on FER datasets usually leads to overfitting. In other words, end-to-end training is not feasible if one may learn representation and a discriminant with deep architectures on images with few pre-processing.\n\nIn this way, some previous work showed that additional task-oriented data for pre-training networks or fine-tuning on well-known pre-trained models could greatly help on building better FER models  (Campos et al., 2015; Xu et al., 2014) . Pre-training deep neural networks is then essential for not leading DL models to overfitting. In this way, several state-of-the-art models have been developed and shared for research purposes. VGG-Face  (Parkhi et al., 2015)  is a CNN based on the VGG-16 architecture  Simonyan & Zisserman (2015) , with the purpose of circumventing the lack of data by building an architecture for face identification and verification and make it available for the research community. This CNN was trained on about three million images of 2,600 different subjects, which makes this architecture specially adapted both for face and emotion recognition. Recent works that have performed well in FER challenges such as EmotiW  (Dhall et al., 2018)  or AVEC  (Ringeval et al., 2015)  are based on VGG-Face architecture.  Wan et al. (2017)  combined linear discriminant analysis and weighted principal component analysis with VGG-Face for feature extraction and dimension reduction in a face recognition task.  Knyazev et al. (2017) , as part of the EmotiW challenge, finetuned a VGG-Face on the FER2013 dataset  (Goodfellow et al., 2013) , and aggregated frame features for classifying emotions on video sequences with a linear SVM. Finally,  Ding et al. (2017)  proposed peculiar fine-tuning techniques with VGG-Face. They constrained their own network to act like VGG-Face network by transferring the distribution of outputs from late layers rather than transferring the weights.\n\nRecent works used 3D convolutional kernels for spatiotemporal description of visual information. The first 3D-CNN models were developed for action recognition task  (Ji et al., 2013; Tran et al., 2015; Liu et al., 2018) . 3D-CNNs pre-trained on action recognition datasets were then made available and transferred to affect computing research  (Fan et al., 2016; Nguyen et al., 2017) .  Ouyang et al. (2017)  combined VGG-Face and LSTM among other CNN-RNN and 3D-CNN networks for building an ensemble network for multi-modality fusion (video+audio), which predicts seven categories of emotions. We belied no 3D-CNN model has ever been evaluated for predicting continuous emotions, and only few approaches have been proposed for discrete emotion prediction. This is mainly due to the lack of video datasets for FER, which allow exploiting the temporal dimension. To circumvent this issue,  Carreira & Zisserman (2017)  developed the i3D network, which is able to learn 3D feature representations based on 2D datasets. They inflated a 2D Inception CNN to extend learned weights in 2D to a third dimension. In this way, they developed several pre-trained networks based on the same architecture with combination of ImageNet and Kinetics datasets either on images or optical flow inputs. As demonstrated by Carneiro de  Melo et al. (2020) , convolutional 3D networks (C3D) proposed in previous studies  (Fan et al., 2016; Nguyen et al., 2017; Ouyang et al., 2017)  for emotion recognition and depression detection, has a lower capacity to produce discriminant spatio-temporal features than i3D  (Carreira & Zisserman, 2017) . This is mainly due to the fact that the i3D-CNN is deeper and it benefits of efficient neural connections through the inception module. In this way, with inflated 2D networks we are able to build efficient 3D-CNNs from competitive 2D architectures. Carneiro de  Melo et al. (2020)  proposed a deep maximization-differentiation network (MDN) and compared this architecture with i3D-CNN and T-3D-CNN  (Diba et al., 2017) , showing that i3D requires fewer parameters than other models and is computationally faster. Finally,  Praveen et al. (2020a,b)  applied i3D networks in the context of pain intensity estimation with ordinal regression. Their approach achieved state-of-the-art results notably by using deep weakly-supervised domain adaptation based on adversarial learning.\n\nMost of the previous studies on FER are based on the categorical representation of emotion but some studies have also dealt with continuous representations, which have been proved to be effective on both image and video datasets. Discrete models are a very simple representation of emotion and for instance, they do not generalize well across cultures. For instance, smiles can be either attributed to happiness or to fearness or to disgust depending on the context.\n\nOn the other hand, dimensional models can distinguish emotions upon a better basis, which are levels of arousal and valence  (Cardinal et al., 2015) . These two values, widely used in the psychology field, can assign a wider range of emotional states. Researchers have shown that low and high-level features complement each other and their combination could shrink the affective gap, which is defined as the concordance between signal properties or features and the desired output values. For instance,  Simonyan & Zisserman (2014)  and  Kim et al. (2018) , built feed-forward networks combining color features, texture features (LBP) and shape features (SIFT descriptors). Other works focused on emotion recognition at group level by studying not only facial expressions but also body posture or context  (Mou et al., 2015) , as well as by exploring various physiological signals such as electrocardiogram and respiration volume  (Ben Henia & Lachiri, 2017; Cardinal et al., 2015) .  Kollias & Zafeiriou (2018)  compared and used exhaustive variations of CNN-RNN models for valence and arousal prediction on the Aff-Wild dataset  (Kollias et al., 2019) . Past studies have particularly worked on full-length short video clips in order to predict a unique categorical label  (Dhall et al., 2018; Ringeval et al., 2015) . However with current datasets and dimensional models, almost every frame is annotated and several peaks of emotions can be distinguished  (Kossaifi et al., 2019) . Therefore, a unique label cannot be attributed to a single video clip. The straightforward approach is to split videos into several clips and averaging the predictions on consecutive frames of the sequence to come out at a unique continuous value. Nevertheless, the duration of emotion is not standardized and it is almost totally dependent on random events such as environmental context or subject identity. In this way, windowing video clips is challenging since detecting the most significant sequence for a single unity of emotion is not straightforward. Therefore, fixing arbitrary sequence lengths could bring important biases in emotion prediction and can lead to a loss of information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Spatiotemporal Models For Continuous Emotion Recognition",
      "text": "We propose a two-step approach for continuous emotion prediction. In the first step, to circumvent the lack of sequences of continuous labeled videos, we rely on three source  In the second step, we adapt such baseline models for spatiotemporal continuous emotion recognition and we fine-tune them on a target dataset. We use two strategies to model sequential information of videos, as shown in Figure  3 : (i) a cascade approach where an LSTM unit is added after the last convolutional layer of the 2D-CNNs to form a 2D-CNN-LSTM; (ii) inflating the 2D convolutional layers of the 2D-CNNs to a third dimension to build a i3D-CNN. This second step also includes pre-processing of the videos frames, as well as post-processing of the predictions. The rest of this section provides additional information on the pre-training and fine-tuning of 2D-CNN models, the pre-processing steps used to locate face images within video frames and to build the sequences of frames to feed the spatiotemporal models, the DL models for continuous emotion recognition, and post-processing of the emotion predictions.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Pre-Training And Fine-Tuning Of 2D-Cnns",
      "text": "Training CNNs on small datasets systematically leads to overfitting. To circumvent this issue, CNNs can be pre-trained or fine-tuned on datasets similar or not to the target task  (Campos et al., 2015; Xu et al., 2014) . Well-known CNN architectures such as AlexNet  (Krizhevsky et al., 2017) , VGG  (Simonyan & Zisserman, 2015) , and GoogleNet form an important set of baselines for a large number of tasks, particularly pre-training such networks on ImageNet dataset constitutes a powerful tool for representation learning. However, recent FER studies have shown that VGG-Face architectures, which are trained on a very large dataset of face images overwhelms architectures trained on ImageNet for FER applications  (Kaya et al., 2017) . Furthermore,  Li & Deng (2018)  have shown that multi-stage fine-tuning can provide an even better performance. We can particularly mention FER2013  (Goodfellow et al., 2013) , TFD  (Susskind et al., 2010)  or more recently RAF-DB  (Li et al., 2017a; Li & Deng, 2019)   On the other, we had to re-train such architectures on RAF-DB. We have evaluated several configurations for training and fine-tuning different CNN architectures with RAF-DB to find out how multi-stage fine-tuning can be well performed. In detail, we fine-tuned CNN architectures by freezing the weights of certain early layers while optimizing deeper ones. As architectures are divided into convolution blocks, we have frozen weights according to these blocks. The proposed architecture kept convolution blocks but classification layers (i.e fully connected layers) were replaced by a stack of two fully connected layers with 512 and 128 units respectively, and an output layer with seven units, since there are seven different emotion categories in RAF-DB: surprise, fear, disgust, happiness, sadness, anger, and neutral.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Pre-Processing",
      "text": "Face images are usually affected by background variations such as illumination, head pose, and face patterns linked to some identity bias. In this way, alignment and normalization are the two most commonly used preprocessing methods in face recognition, which may aid learning discriminant features. For instance, the RAF-DB dataset contains aligned faces, while the subjects in the SEWA-DB dataset are naturally facing a web camera. Then, face alignment is not an important issue for this study. Furthermore, normalization only consists in scaling pixel values between 0 to 1 and to standardize input dimensions, faces have been resized to 100×80 pixels, which is the average dimension of faces founded in the target dataset. On the other hand, we detail other essential steps for face expression recognition in video sequences such as frame and face extraction, and window bagging.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Frame And Face Extraction",
      "text": "The videos of the target dataset (SEWA-DB) have been recorded at 50 frames per second (fps). On the other hand, the valence and arousal annotations are available at each 10 ms, which corresponds to 10 fps. Therefore, it is necessary to replicate annotations for non-labeled frames when using 50 fps.\n\nFor locating and extracting faces from the frames of the SEWA-DB videos, we used a multitask cascaded CNN (MTCNN)  (Zhang et al., 2016)",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Sequence Learning",
      "text": "The target dataset contains long video sequences showing a variety of emotions along records of face expressions from single subjects. Duration of emotions is not clearly established and it varies for each individual. Several studies have been previously carried out in order to get expression intensity variations by pointing peak and non-peak expressions along sequences. However, while whole video sequences represent multiple annotations at a specific sampling rate and not a single label, to represent a succession of diverse emotional states, we split the video sequences into several clips of fixed length with a specific overlapping ratio. This has two main advantages: (i) it increases the amount of data for training CNNs; (ii) it allows the investigation of which window settings is better for training spatiotemporal CNNs to learn from long sequences of valence and arousal annotations. Based on an exploratory study, we have chosen two sequence lengths (16 and 64 consecutive frames of a single video) and three overlapping ratios for each sequence length (0.2, 0.5 and 0.8). For instance, a window of 16 consecutive frames with an overlapping of 0.5 contains the last eight frames of the previous window.\n\nIt was also important to check the integrity of contiguous video frames. Indeed some frames are discarded because no face was detected within them, hence damaging the continuity of the temporal information of emotion between each frame. The proposed strategy to divide videos into clips may introduce important temporal gaps between two consecutive frames.\n\nTherefore, we applied a tolerance (a temporal difference between close frames) to select clips that give sense to a unique emotion unit. Globally, the MTCNN can detect faces in clips and in average, 90% of the frames are kept, depending on the sequence length, overlapping ratio and frame rate. Figure  5  presents the number of clips available in training, validation and test sets according to such parameters. Finally, the last preprocessing step is to fuse annotations of multiple frames in one clip to get a single emotion label for each window. For such aim we use either the average of labels or the extremum value of the labels to obtain a single label for each continuous emotion (a single value of valence and a single value of arousal.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Spatiotemporal Models",
      "text": "We have developed two spatiotemporal models: (i) a cascaded network based on a VGG-16 network pre-trained on VGG-Face that can be fine-tuned or not on RAF-DB; (ii) an inflated network based on either VGG-11, VGG-16, or ResNet50 architectures pre-trained on different datasets (VGG-Face, RAF-DB, ImageNet).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Cascaded Networks (2D-Cnn-Lstm)",
      "text": "Long short term memory units (LSTMs) are a special kind of RNN, capable of learning order dependence as we may find in a sequence of frames from a video. The core of LSTMs is a cell state, which adds or removes information depending on the input, output and forget",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Inflated 3D-Cnn (I3D-Cnn)",
      "text": "The need to analyze a sequence of frames led us to the use of 3D-CNNs. 3D-CNNs produce activation maps that allow analyzing data where temporal information is relevant. The main advantage of 3D-CNNs is to learn representation from clips that can strengthen the spatiotemporal relationship between frames. Different from 2D-CNNs, 3D-CNNs are directly trained on batches of frame sequences rather than batches of frames. On the other hand, adding a third dimension to the CNN architecture increases the number of parameters of the model and that requires much larger training datasets than those required by 2D models. The main downside of using such an architecture for FER tasks is the lack of pre-trained models. Besides that, we cannot consider training 3D-CNN architectures in an end-to-end fashion for continuous emotion recognition due to the limited amount of training data. Therefore, a feasible solution is to resort to weight inflation of 2D-CNN pre-trained models  (Carreira & Zisserman, 2017) . Inflating a 2D-CNN minimizes the need for large amounts of data for training properly a 3D-CNN as the inflation process reuses the weights of the 2D-CNNs. Figure  7  shows that the weight inflation consists of enlarging kernels of each convolution filter by one dimension. Regarding our target task, it means to extend the receptive field of each neuron to the time dimension (a.k.a. a sequence of frames). 2D convolutional kernels are then replicated as many times as necessary to fit the third dimension and form a 3D convolutional kernel. At first glance, pre-trained weights are just copied through the time dimension and provide better approximation for initialization than randomness but do not constitute yet an adequate distribution for the time dimension. With this in mind, the next issue is to find a method that fits best the transfer learning to time dimension with weight inflation by varying some parameters, such as: initialization, masking, multiplier and dilation.\n\nInitialization:. When replicating kernels for weight inflation, it is possible to simply copy the weights n times (n being the dimension of time axis) or to center the weights. Centering means copying once weights of a 2D kernel and initializing the weights of the surrounding kernels that form the 3D filter either randomly (with a uniform distribution) or with zeros. We assume that pre-trained 2D kernels have a good capacity of generalization for images, then giving sufficiently distant distribution for all but one 2D kernel from the copied 2D kernel could have a positive impact on model convergence.\n\nMasking:. Assuming that copied 2D kernels have been pre-trained properly considering a very similar task and that they perform well on images, the idea of masking is to train adequately inflated weights on time dimension. Then we consider not modifying centered weights during training in order to disseminate the spatial representation learned from pre-trained weights to inflated weights.\n\nMultiplier:. The distribution of CNN weights and the range of targeted values for regression are closely related. Since values of valence and arousal range from -1 to 1 and standard values of the weights often take values between 10 -3 and 10 -1 , then rising targeted values by a factor could allow to scale up the distribution space and improve convergence.\n\nDilation:. As suggested by  Yu & Koltun (2016) , we used dilated convolutions on our models.\n\nThe dilation was performed only on time dimension. We divided the architectures into four blocks with increasing levels of dilation starting from level 1 for convolutional layers (no dilation) then 2, 4 and 8 for top convolutional layers. Dilated convolution consists of receptive fields larger than conventional ones. In other words, neuron connections of one convolutional layer are spread among neurons of previous layers. Notably, this kind of implementation has shown a good performance for segmentation and object recognition task.",
      "page_start": 12,
      "page_end": 15
    },
    {
      "section_name": "Post-Processing",
      "text": "The post-processing aims to improve the quality of the prediction by using some statistical information of the target dataset to reduce variance among datasets  (Ortega et al., 2019) .\n\nFinally, time delay is used to compensate some offset between the labels and the predictions due to the reaction-lag of annotators. Valence and arousal predictions (y (f )) at frame f are shifted over t frames (precedent or subsequent) in order to align predictions and labels temporally as:\n\nwhere t is an integer in  [-10, 10] .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section we present a brief description of the two FER datasets used in the experiments: RAF-DB and SEWA-DB. Next, we present the performance measures and summarize our experimental setting and the results achieved by the proposed 2D-CNN-LSTM and i3D-CNN models.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Facial Expression Datasets",
      "text": "Real World Affective Faces Database (RAF-DB) is a real world dataset that contains 29,672 images downloaded from the Internet  (Li et al., 2017b) . Each image has been labeled\n\nby around 40 annotators. The dataset has two types of annotation: seven classes of basic emotions and 12 classes of compound emotions. We have only used the seven basic emotions (face images and labels). Other metadata such as facial landmarks, bounding box and identity bias such as age, gender, race are also provided but they have not been used in any step of the proposed approach. RAF-DB was used to fine-tune the pre-trained 2D-CNNs.\n\nSEWA-DB is a large and richly annotated dataset consisting of six groups of subjects (around 30 people per group), from six different cultural backgrounds (British, German, Hungarian, Greek, Serbian, and Chinese) and divided into pairs of subjects  (Kossaifi et al., 2019) .\n\nEach pair had to discuss their emotional state and sentiment toward four adverts previously Annotations are given for valence, arousal and levels of liking. We only used valence and arousal annotations since previous studies have indicated that the level of liking is not well related with facial expressions.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Performance Metrics",
      "text": "The y is given by:\n\nThe MAPE for a set of labels y and predictions y is given by:\n\nThe PCC is given as:\n\nwhere n is the number of samples, y i is the i-th label, y i is the i-th prediction, and y and y are the mean of labels and mean of predictions, respectively.\n\nThe CCC combines the PCC with the squared difference between the mean of predictions y and the mean of the labels y. CCC shows the degree of correspondence between the label and prediction distributions based on the covariance and correspondence. The CCC between a set of labels y and predictions y is given by:\n\nwhere s 2 y and s 2 y are the variance of y and y respectively.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Training And Fine-Tuning 2D-Cnns",
      "text": "Our first task is to specialize the three pre-trained CNN architectures (VGG-11, VGG-16 and ResNet50) for emotion recognition by fine-tuning them with the RAF-DB dataset. These three architectures were pre-trained either on VGG-Face or ImageNet. For fine-tuning the pre-trained 2D-CNNs on RAF-DB, video frames have been resized to 100×80×3, which is the mean dimension of video frames of the target dataset (SEWA-DB). Learning rate has been fixed to 1e -5 and batches of size 16. Optimization has been performed with Adam optimizer.\n\nWe have assigned different weights to each class, according to the number of samples, to deal with the data imbalance found in RAF-DB. This allows that classes with few samples can affect the weights of the model to the same extent as classes with many more samples. Moreover, we have observed that low-level data augmentation such as like rotation, flipping, highlight variations, could help improve the performance. Although data augmentation cannot bring significant information for emotion recognition, it can prevent overfitting on a single sample and improve model distributions.\n\nThe performance achieved by the 2D-CNNs after fine-tuning on RAF-DB is presented in Table  1 , where the suffix BN refers to batch normalization layers added to the original architectures after each convolutional layer to improve model convergence and reduce overfitting.\n\nFurthermore, we indicate for each architecture the dataset used for pre-training as well as the convolution block (2 1 to 5 1) from which we start fine-tuning the architectures. In general, most of the fine-tuned models achieved accuracy higher than the baseline models  (Jyoti et al., 2019) . Other recent works, which employed attention networks have achieved better performances  (Wang et al., 2020; Li et al., 2019) . In the proposed approach, we did not have considered two common problems we may found in face analysis in real-world scenarios: occlusions and pose variations. On the contrary,  Wang et al. (2020)  and  Li et al. (2019)  addressed these problems by using region-based attention networks. Attention modules are used to extract compact face representations based on several regions cropped from the face and they adaptively adjusts the importance of facial parts. Therefore, these models learn to discriminate occluded and non-occluded faces while improving emotion detection in both cases.\n\n18  have also considered CCC as cost function  (Ortega et al., 2019)  since it provides information about correspondence and correlation between predictions and annotations. However, we observed a better convergence while using the MSE.\n\nTable  2  shows the results in terms of PCC and CCC considering different frames rates (fps), sequence lengths (SL), overlapping ratios (OR) and fusion modes (FM). In general, both extremum and mean fusion performed well and the best results for both valence and arousal were achieved for sequences of 64 frames at 10 fps. The VGG-16 architecture benefited from fine-tuning on the RAF-DB and it achieved CCC values of 0.625 for valence and 0.557 for arousal on the validation set of SEWA-DB. In addition to the correlation metrics, the proposed 2D-CNN-LSTM achieved an overall MAE of 0.06 (amongst 2D-CNN-LSTM models), which also indicates a good correspondence between predictions and annotations. Since the best performance has been obtained with post-processing steps, thus remodeling our set of predictions and annotations, we have also computed MAPE to evaluate the error ratio between predictions and annotations.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "I3D-Cnn Architecture",
      "text": "Another alternative for spatiotemporal modeling is to use the i3D-CNN. In this way, strong spatiotemporal correlations between frames are directly learned from video clips by a single network. Thanks to weight inflation, we are able to use the pre-trained 2D-CNNs to build i3D-CNN architectures. The inflation method allows us to transpose learned information from various static tasks to dynamic ones, and therefore to perform the essential transfer learning for learning spatiotemporal features. With this in mind, we reused the 2D-CNN architectures shown in Table  1  and expand their convolutional layers to build i3D-CNNs considering two configurations, denoted as C1 and C2 in Table  3 .\n\nDue to the high number of trainable parameters, i3D-CNNs are particularly time-consuming to train and therefore we had to fix the value of some basic hyperparameters instead of performing exploratory experiments to set them. Therefore, we evaluated only the best configuration found for the 2D-CNN-LSTM, as shown in Table  1 , which uses batch of size 8, sequence length of 64 frames, overlapping ratio of 0.8, and frame rate of 10 fps. This is the main downside of our approach based on i3D CNNs, as the number of trainable parameters of i3D-CNNs is three times greater than the counterpart 2D-CNNs. Tables  4  and 5  show the best configurations of each architecture for valence and arousal prediction, respectively. Globally, different values of inflation, masking, and dilation did not shown any impact on the results achieved by i3D models. Table  6  shows the best performance obtained for each architecture for valence and arousal in terms of PCC and CCC values.\n\nInflated 3D-CNNs for regression seem to be very sensitivity to some configurations for training regarding the range of results achieved by different base models and datasets used in their initialization. In these conditions, it is difficult to state on the effect of a single parameter for inflation. VGG-16 with batch normalization and ResNet50 achieved the best results for both valence and arousal and have shown a good ability to predict these values compared to other base models. Surprisingly, the VGG-16 pre-trained on ImageNet achieved higher PCC and CCC for both valence and arousal than those base models pre-trained on VGG-Face and RAF-DB, which are source datasets closer to the target one. On the hand, ResNet50 benefited from the initialization with VGG-Face. In summary, the best results range from 0.313 to 0.406 for PCC and from 0.253 to 0.326 for CCC. These performances still show a poor correlation between predictions and annotations but are comparable to the performance achieved by other studies on continuous emotion prediction that use the SEWA-DB dataset.",
      "page_start": 20,
      "page_end": 22
    },
    {
      "section_name": "Discussion",
      "text": "The experiments carried out on SEWA-DB have shown that the 2D-CNN-LSTM architectures achieved (Table  2 ) better results than i3D-CNN architectures (Table  6 ). Notably, for the former, valence was better predicted than arousal in terms of CCC. On the contrary, for the latter, arousal was better predicted than valence, also in terms of CCC. Previous works Regarding the complexity of the two proposed approaches for continuous emotion recognition, we had to make some trade-off that certainly have impacted the quality of the results provided by i3D-CNN architectures. We have also observed a high sensitivity in the training of this type of architecture according to various configurations. This implies that i3D-CNN architectures are very flexible and further improvement could lie in better initialization and tuning of the number and quality of parameters regarding the potential of this model. Furthermore, inflated weights provided good initialization for action recognition tasks which suggested that we could also take advantage of this method for emotion recognition. However, the main difference is that for action recognition, researchers had hundreds of various short videos for a classification task while we have relatively long videos of few subjects for a regression task.\n\nNevertheless, the experimental results have shown a great potential for further improvement if more data is available for fine-tuning the i3D models. On the other hand, the performance of 2D-CNN-LSTM architectures were very satisfying and this type of architecture is still a good choice for FER applications. Table  7  shows the best results achieved by the proposed 2D-CNN-LSTM and i3D-CNN models and compare them with the baseline models proposed by  Kossaifi et al. (2019) . For ResNet18, they have evaluated Root Mean Squared Error (RMSE) and CCC as loss function.\n\nThe CCCs achieved by the i3D-CNN are slightly higher than those achieved by all models of  Kossaifi et al. (2019) . On the other hand, the CCC values achieved by the 2D-CNN-LSTM are almost twice than the best results achieved by the best model (ResNet18) of  Kossaifi et al. (2019) . Table  7 also   It should be noticed that in this work, subjects were mostly facing the camera with relative clear view of the whole face. To some extent, this could imply some bias in the results when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN architecture could at this time be a drag for live applications. Finally, to the best of our knowledge, it was the first time that 3D-CNNs were used in regression applications for predicting valence and arousal values for emotion recognition.\n\nThere are some promising directions to expand the approaches proposed in this paper.\n\nOne could take advantage of the development of huge and complex cross-cultural datasets such as the Aff-Wild dataset to exploit occlusion cases, pose variations or even scene breaks.\n\nIn particular, with i3D-CNN architectures, we believe deep learning algorithms possess the capacity and robustness to deal with these specific cases and benefit from an adequate flexibility to analyze both the separability and combination of discriminant spatiotemporal features.\n\nFinally, we have shown a peculiar and flexible way of fine-tuning inflated CNNs and maybe this strategy could be transferred to other applications such as object and action recognition on video sequences.",
      "page_start": 22,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The level of arousal is represented on the vertical axis,",
      "page": 2
    },
    {
      "caption": "Figure 1: The circumplex model (Warr et al., 2014).",
      "page": 3
    },
    {
      "caption": "Figure 2: Such 2D-CNNs will be used as baseline models",
      "page": 8
    },
    {
      "caption": "Figure 2: Overview of pre-training and ﬁne-tuning of 2D-CNN architectures for discrete emotion prediction.",
      "page": 8
    },
    {
      "caption": "Figure 3: (i) a cascade approach where an LSTM",
      "page": 8
    },
    {
      "caption": "Figure 3: Overview of the two used deep architectures for depicting spatiotemporal features on video sequences:",
      "page": 8
    },
    {
      "caption": "Figure 4: Only frames showing whole faces are kept, while other frames are discarded.",
      "page": 10
    },
    {
      "caption": "Figure 4: Exhaustive MTCNN architecture composed of a stack of CNNs (Zhang et al., 2016).",
      "page": 10
    },
    {
      "caption": "Figure 5: presents the number of clips available in training, validation and test",
      "page": 11
    },
    {
      "caption": "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:",
      "page": 12
    },
    {
      "caption": "Figure 6: The LSTM has a single layer with 1,024 units, with random and uniform distribution",
      "page": 12
    },
    {
      "caption": "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at",
      "page": 13
    },
    {
      "caption": "Figure 7: shows that the weight",
      "page": 13
    },
    {
      "caption": "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are",
      "page": 14
    },
    {
      "caption": "Figure 8: The proposed i3D-CNN based on inﬂation of 2D convolutional kernels of a pre-trained VGG-16 CNN.",
      "page": 15
    },
    {
      "caption": "Figure 8: shows the architecture of the proposed i3D-CNN, which is based on the inﬂation",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Facial expressions are one of the most powerful ways for depicting speciﬁc patterns in human"
        },
        {
          "Abstract": "behavior and describing human emotional state. Despite the impressive advances of aﬀective"
        },
        {
          "Abstract": "computing over the last decade, automatic video-based systems for facial expression recogni-"
        },
        {
          "Abstract": "tion still cannot handle properly variations in facial expression among individuals as well as"
        },
        {
          "Abstract": "cross-cultural and demographic aspects. Nevertheless, recognizing facial expressions is a diﬃ-"
        },
        {
          "Abstract": "cult task even for humans.\nIn this paper, we investigate the suitability of state-of-the-art deep"
        },
        {
          "Abstract": "learning architectures based on convolutional neural networks (CNNs) for continuous emotion"
        },
        {
          "Abstract": "recognition using long video sequences captured in-the-wild. This study focuses on deep learn-"
        },
        {
          "Abstract": "ing models that allow encoding spatiotemporal relations in videos considering a complex and"
        },
        {
          "Abstract": "multi-dimensional emotion space, where values of valence and arousal must be predicted. We"
        },
        {
          "Abstract": "have developed and evaluated convolutional\nrecurrent neural networks combining 2D-CNNs"
        },
        {
          "Abstract": "and long short term- memory units, and inﬂated 3D-CNN models, which are built by inﬂat-"
        },
        {
          "Abstract": "ing the weights of a pre-trained 2D-CNN model during ﬁne-tuning, using application-speciﬁc"
        },
        {
          "Abstract": "videos.\nExperimental\nresults on the challenging SEWA-DB dataset have shown that\nthese"
        },
        {
          "Abstract": "architectures can eﬀectively be ﬁne-tuned to encode the spatiotemporal\ninformation from suc-"
        },
        {
          "Abstract": "cessive raw pixel\nimages and achieve state-of-the-art results on such a dataset."
        },
        {
          "Abstract": "Keywords:\nFacial Expression Recognition, Deep Learning, Convolutional Recurrent Neural"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "Memory."
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "1.\nIntroduction"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "Facial expressions are the results of peculiar positions and movements of\nfacial muscles"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "over time. According to previous studies,\nface images and videos provide an important source"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "of\ninformation for representing the emotional state of an individual (Li & Deng, 2018). Facial"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "expression recognition (FER) has attracted a growing interest in recent years. The detection"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "of\nspontaneous\nfacial expressions\nin-the-wild is a very challenging task, where performance"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "depends on several factors such as variations among individuals,\nidentity bias of subjects such"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "as gender, age, culture and ethnicity, and the quality of recordings (illumination, resolution,"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "head pose, capture conditions)."
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "Early research on FER systems was inspired by the fundamentals of human aﬀect theory"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "(Ekman & Friesen, 1971; Ekman, 1994), where discrete models are employed to classify facial"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "images\ninto discrete\ncategories,\nsuch as\nanger, disgust,\nfear, happiness,\nsadness,\nsurprise,"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "that can be recognizable across cultures. The limited generalization capacity of these models"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "has paved the way for multi-dimensional spaces that can improve the representativeness and"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "accuracy for describing emotions (Jack et al., 2012)."
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "There are three levels\nfor describing emotion:\npleasantness, attention and levels of acti-"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "vation.\nSpeciﬁcally, emotion recognition datasets annotate emotional\nstate with two values"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "named valence and arousal, where the former represents the level of pleasantness and the latter"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "represents the level of activation, each of these values lying into [−1; 1] range. With these val-"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "ues, we are able to project an individual emotional state into a 2D space called the circumplex"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "model, which is\nshown in Figure 1. The level of arousal\nis\nrepresented on the vertical axis,"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "whereas the level of valence is represented on the horizontal axis."
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "Early FER systems\nrelied on shallow approaches, which combined handcrafted features,"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "such as eigenfaces (Oliveira et al., 2011), Gabor wavelets (Zavaschi et al., 2011),\nlocal binary"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "patterns (LBP) (Shan et al., 2009; Zavaschi et al., 2013) and its variants such as LBP-TOP"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "Zhao & Pietikainen (2007) and LBP-SIP (Wang et al., 2014), and Weber\nlocal descriptor"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "(Cossetin et al., 2016) with support vector machines (SVM). However, recent advances in deep"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "learning (DL) and computing technologies have led FER systems to learn discriminant repre-"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "sentations directly from face images. Previous studies, ﬁrstly exploited static FER datasets,"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "where subjects are associated to discrete and mutually exclusive categories of emotions. State-"
        },
        {
          "Networks, Inﬂated 3D-CNNs, Dimensional Emotion Representation, Long Short Term-": "of-the-art performance on benchmarking datasets such as FER2013 (Goodfellow et al., 2013),"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "TFD (Susskind et al., 2010), and SFEW (Dhall et al., 2011) were achieved using CNN-based"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "approaches (Georgescu et al., 2019; Kim et al., 2015; Liu et al., 2017; Zhang et al., 2015; Guo"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "et al., 2016; Kim et al., 2016; Pramerdorfer & Kampel, 2016)."
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "Temporal information and dynamic facial components can play a crucial role for describing"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "facial expressions\n(Li & Deng, 2018).\nBy deﬁnition, people express emotions\nin a dynamic"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "process, and learning spatiotemporal structures has become the current trend. Recent studies"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "have proposed deep architectures for FER, which were trained on video data (Fayolle & Droit-"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "Volet, 2014).\nIn the literature, video sequences have been mostly processed using aggregation"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "and concatenation of\nfeatures\nfor each facial\nimage into clips, yet\nthey cannot\nleverage the"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "temporal dependencies over a video (Bargal et al., 2016; Ding et al., 2016). To circumvent this"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "issue, convolutional recurrent neural networks (CRNN) and 3D-CNN architectures have been"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "proposed to encode spatiotemporal relations among video frames (Ayral et al., 2021; Carneiro"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "de Melo et al., 2020)."
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "In this paper, state-of-the-art DL models based on CNNs are investigated and compared for"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "video-based emotion recognition where aﬀective states are represented as continuous values"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "in the bi-dimensional\nspace of valence and arousal.\nThis\nstudy focuses on state-of-the-art"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "architectures developed based on pre-trained 2D-CNN that allow encoding spatiotemporal"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "relations of\nfacial\nfeatures in videos, thereby improving the predictions for continuous values"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "of emotion.\nStarting from pre-trained 2D-CNNs, we ﬁne-tune two types of DL models with"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "videos:\n(i) a 2D-CNN combined with a long short -term memory (LSTM) structure; (ii) a 2D-"
        },
        {
          "Figure 1: The circumplex model (Warr et al., 2014).": "CNN inﬂated into a 3D-CNN. These models predict emotions\nthrough regression of valence"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "each sequence into several clips, not only for augmenting the amount of\ntraining data, but"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "also by isolating unique facial expressions. For proof-of-concept, CNN architectures such as"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "VGG and ResNet have been pre-trained with ImageNet and RAF-DB datasets,\nand ﬁne-"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "tuned with with multiple video sequences from the SEWA-DB dataset to address the problem"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "of predicting valence and arousal.\nExperiments were conducted over diﬀerent clip lengths,"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "overlapping ratios, and strategies for fusing annotations."
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "This paper is organized as follows. Section 2 provides a review of DL models proposed for"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "emotion recognition in videos captured in-the-wild,\nfocusing on models\nthat allow encoding"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "the spatio-temporal relations in facial emotions. Section 3 presents an overview of DL models"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "that are proposed for continuous emotion recognition,\nincluding pre-processing steps, model"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "architectures, pre-training, ﬁne-tuning procedures and post-processing steps.\nSection 4 de-"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "scribes the experimental methodology (e.g., protocol, datasets and performance metrics) used"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "to validate DL models for continuous emotion recognition, as well as the experimental results."
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "Section 5 presents a discussion and a comparison with the state-of-the-art. Conclusions are"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "presented in the last section."
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "2. Related Work"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "A conventional approach for dealing with video frames is to aggregate features extracted"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "from each frame into a clip before ﬁnal emotion prediction Ding et al.\n(2016).\nIn addition"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "to the\nfeature aggregation, Bargal\net al.\n(2016) also aggregated mean, variance, minimum"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "and maximum over a sequence of\nfeatures thus adding some statistical\ninformation. However,"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "since feature aggregation cannot exploit inter-correlations between frames and is not able to"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "depict temporal dependencies, this approach has strong limitations. To circumvent this issue,"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "recurrent neural networks (RNN) such as LSTM or 3D-CNN architectures can integrate data"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "series as\ninput, provided that data are sequentially ordered and transitions have a substan-"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "tial potential of\ninformation. While LSTMs can deal with sequential data of variable length"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "in both directions, 3D-CNNs exploit textured variations from sequence of\nimages by extend-"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "ing convolutional kernels\nto a third dimension. Hence, 3D-CNNs are well\nsuited to encode"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "spatiotemporal\ninformation in video-based FER applications. Tran et al. (2015) proposed a"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "3D-CNN model for action recognition on the UCF101 dataset, which encompasses videos clas-"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "siﬁed over 101 action categories. They have shown that 3D-CNNs can outperform 2D-CNNs"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "on diﬀerent video analysis benchmarks, and could bring eﬃcient and compact features. Several"
        },
        {
          "and arousal values. We assume that long video sequences are captured in-the-wild, and split": "recent studies (Abbasnejad et al., 2017; Fan et al., 2016; Nguyen et al., 2017; Liu et al., 2018;"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "based on 3D-CNNs for FER, nevertheless all of them deal with discrete emotion prediction."
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "A popular approach for dealing with temporal sequences of\nframes is a cascaded network,"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "in which architectures for representation learning and discrimination are stacked on top of each"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "other, thus various levels of\nfeatures are leaned by each block and processed by the following"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "until the ﬁnal prediction. Particularly,\nthe combination of CNNs and LSTM units has been"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "shown eﬀectiveness to learn spatiotemporal representations (Ji et al., 2013; Tran et al., 2015)."
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "For\ninstance, Ouyang et al.\n(2017) used a VGG-16 CNN to extract 16-frame\nsequence of"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "features and fed an LSTM unit to predict six emotion categories. They pre-processed video"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "frames with a multi-task cascade CNN (MTCNN)\nto detect\nfaces and described each video"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "by a single 16-frame window.\nSimilarly, Vielzeuf et al.\n(2017) used a VGG-16 CNN and an"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "LSTM unit as part of an ensemble with a 3D-CNN, and an audio network.\nParticularly,"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "they used a method called multi-instance learning (MIL) method to create bag-of-windows"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "for\neach video with a speciﬁc overlapping ratio.\nEach sequence was described by a single"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "label and contribute to the overall prediction of\nthe matching video clip.\nSince deep neural"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "networks (DNNs) are highly data-dependent, there are strong limitations for designing FER"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "systems based on DNNs,\neven more\nsince FER datasets are often small and task-oriented"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "(Li & Deng, 2018).\nConsidering this\nfact,\ntraining deep models on FER datasets usually"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "leads\nto overﬁtting.\nIn other words,\nend-to-end training is not\nfeasible\nif one may learn"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "representation and a discriminant with deep architectures on images with few pre-processing."
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "In this way,\nsome previous work showed that additional\ntask-oriented data for pre-training"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "networks or ﬁne-tuning on well-known pre-trained models could greatly help on building better"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "FER models\n(Campos\net al., 2015; Xu et al., 2014).\nPre-training deep neural networks\nis"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "then essential\nfor not\nleading DL models to overﬁtting.\nIn this way,\nseveral state-of-the-art"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "models have been developed and shared for\nresearch purposes.\nVGG-Face\n(Parkhi\net al.,"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "2015) is a CNN based on the VGG-16 architecture Simonyan & Zisserman (2015), with the"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "purpose of circumventing the lack of data by building an architecture for\nface identiﬁcation"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "and veriﬁcation and make it available for the research community. This CNN was trained on"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "about three million images of 2,600 diﬀerent subjects, which makes this architecture specially"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "adapted both for face and emotion recognition. Recent works that have performed well in FER"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "challenges such as EmotiW (Dhall et al., 2018) or AVEC (Ringeval et al., 2015) are based on"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "VGG-Face architecture. Wan et al. (2017) combined linear discriminant analysis and weighted"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "principal component analysis with VGG-Face for feature extraction and dimension reduction"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "in a face\nrecognition task.\nKnyazev et al.\n(2017), as part of\nthe EmotiW challenge, ﬁne-"
        },
        {
          "Barros & Wermter, 2016; Zhao et al., 2018; Ouyang et al., 2017) have proposed approaches": "tuned a VGG-Face on the FER2013 dataset (Goodfellow et al., 2013), and aggregated frame"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "(2017) proposed peculiar ﬁne-tuning techniques with VGG-Face. They constrained their own"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "network to act\nlike VGG-Face network by transferring the distribution of outputs from late"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "layers rather than transferring the weights."
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "Recent works used 3D convolutional kernels\nfor\nspatiotemporal description of visual\nin-"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "formation. The ﬁrst 3D-CNN models were developed for action recognition task (Ji et al.,"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "2013; Tran et al., 2015; Liu et al., 2018). 3D-CNNs pre-trained on action recognition datasets"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "were\nthen made available and transferred to aﬀect\ncomputing\nresearch (Fan et\nal., 2016;"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "Nguyen et al., 2017).\nOuyang et al.\n(2017)\ncombined VGG-Face and LSTM among other"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "CNN-RNN and 3D-CNN networks for building an ensemble network for multi-modality fusion"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "(video+audio), which predicts seven categories of emotions. We belied no 3D-CNN model has"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "ever been evaluated for predicting continuous emotions, and only few approaches have been"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "proposed for discrete emotion prediction. This is mainly due to the lack of video datasets for"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "FER, which allow exploiting the temporal dimension. To circumvent\nthis\nissue, Carreira &"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "Zisserman (2017) developed the i3D network, which is able to learn 3D feature representations"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "based on 2D datasets. They inﬂated a 2D Inception CNN to extend learned weights in 2D to"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "a third dimension.\nIn this way, they developed several pre-trained networks based on the same"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "architecture with combination of ImageNet and Kinetics datasets either on images or optical"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "ﬂow inputs. As demonstrated by Carneiro de Melo et al. (2020), convolutional 3D networks"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "(C3D) proposed in previous studies (Fan et al., 2016; Nguyen et al., 2017; Ouyang et al., 2017)"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "for emotion recognition and depression detection, has a lower capacity to produce discrimi-"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "nant spatio-temporal\nfeatures than i3D (Carreira & Zisserman, 2017). This is mainly due to"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "the fact\nthat\nthe i3D-CNN is deeper and it beneﬁts of eﬃcient neural connections\nthrough"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "the inception module.\nIn this way, with inﬂated 2D networks we are able to build eﬃcient"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "3D-CNNs from competitive 2D architectures. Carneiro de Melo et al. (2020) proposed a deep"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "maximization-diﬀerentiation network (MDN) and compared this architecture with i3D-CNN"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "and T-3D-CNN (Diba et al., 2017),\nshowing that\ni3D requires\nfewer parameters\nthan other"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "models and is computationally faster. Finally, Praveen et al. (2020a,b) applied i3D networks"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "in the context of pain intensity estimation with ordinal regression. Their approach achieved"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "state-of-the-art results notably by using deep weakly-supervised domain adaptation based on"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "adversarial\nlearning."
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "Most of the previous studies on FER are based on the categorical representation of emotion"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "but some studies have also dealt with continuous representations, which have been proved to be"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "eﬀective on both image and video datasets. Discrete models are a very simple representation"
        },
        {
          "features\nfor classifying emotions on video sequences with a linear SVM. Finally, Ding et al.": "of emotion and for instance, they do not generalize well across cultures. For instance, smiles"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "On the other hand, dimensional models can distinguish emotions upon a better basis, which"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "are levels of arousal and valence (Cardinal et al., 2015).\nThese two values, widely used in"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "the psychology ﬁeld, can assign a wider\nrange of emotional\nstates. Researchers have shown"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "that\nlow and high-level\nfeatures complement each other and their combination could shrink"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "the aﬀective gap, which is deﬁned as\nthe concordance between signal properties or\nfeatures"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "and the desired output values.\nFor\ninstance, Simonyan & Zisserman (2014) and Kim et al."
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "(2018), built feed-forward networks combining color features, texture features (LBP) and shape"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "features\n(SIFT descriptors). Other works\nfocused on emotion recognition at group level by"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "studying not only facial expressions but also body posture or context (Mou et al., 2015), as well"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "as by exploring various physiological signals such as electrocardiogram and respiration volume"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "(Ben Henia & Lachiri, 2017; Cardinal et al., 2015). Kollias & Zafeiriou (2018) compared and"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "used exhaustive variations of CNN-RNN models\nfor valence and arousal prediction on the"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "Aﬀ-Wild dataset\n(Kollias et al., 2019). Past studies have particularly worked on full-length"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "short video clips\nin order\nto predict a unique categorical\nlabel\n(Dhall et al., 2018; Ringeval"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "et al., 2015). However with current datasets and dimensional models, almost every frame is"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "annotated and several peaks of emotions can be distinguished (Kossaiﬁ et al., 2019). Therefore,"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "a unique label cannot be attributed to a single video clip. The straightforward approach is"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "to split videos\ninto several clips and averaging the predictions on consecutive frames of\nthe"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "sequence to come out at a unique continuous value. Nevertheless, the duration of emotion is"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "not standardized and it is almost totally dependent on random events such as environmental"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "context or subject identity.\nIn this way, windowing video clips is challenging since detecting"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "the most signiﬁcant sequence for a single unity of emotion is not straightforward. Therefore,"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "ﬁxing arbitrary sequence lengths could bring important biases in emotion prediction and can"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "lead to a loss of\ninformation."
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "3. Spatiotemporal Models for Continuous Emotion Recognition"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "We propose a two-step approach for\ncontinuous\nemotion prediction.\nIn the ﬁrst\nstep,"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "to circumvent\nthe\nlack of\nsequences of\ncontinuous\nlabeled videos, we\nrely on three\nsource"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "image datasets:\nImageNet, VGG-Face and RAF-DB. ImageNet and VGG-Face datasets, which"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "contains generic object\nimages and face images,\nrespectively, are used for pre-training three"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "2D-CNN architectures: VGG-11, VGG-16 and ResNet50. The RAF-DB dataset is closer to"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "the target dataset since it contains face images annotated with discrete (categorical) emotions,"
        },
        {
          "can be either attributed to happiness or to fearness or to disgust depending on the context.": "and it is used for ﬁne-tuning the 2D-CNN architectures previously trained on ImageNet and"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "with the target dataset."
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "Figure 2: Overview of pre-training and ﬁne-tuning of 2D-CNN architectures for discrete emotion prediction."
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "In the second step, we adapt such baseline models for spatiotemporal continuous emotion"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "recognition and we ﬁne-tune them on a target dataset. We use two strategies\nto model\nse-"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "quential\ninformation of videos, as shown in Figure 3:\n(i) a cascade approach where an LSTM"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "unit is added after the last convolutional\nlayer of the 2D-CNNs to form a 2D-CNN-LSTM; (ii)"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "inﬂating the 2D convolutional layers of the 2D-CNNs to a third dimension to build a i3D-CNN."
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "This second step also includes pre-processing of the videos frames, as well as post-processing"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "of the predictions."
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "Figure 3: Overview of the two used deep architectures for depicting spatiotemporal features on video sequences:"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "a cascaded network 2D-CNN-LSTM and an i3D-CNN."
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "The rest of this section provides additional\ninformation on the pre-training and ﬁne-tuning"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "of 2D-CNN models,\nthe pre-processing steps used to locate face images within video frames"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "and to build the sequences of\nframes to feed the spatiotemporal models,\nthe DL models for"
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "continuous emotion recognition, and post-processing of the emotion predictions."
        },
        {
          "VGG-Face datasets, as illustrated in Figure 2. Such 2D-CNNs will be used as baseline models": "8"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "Training CNNs on small datasets systematically leads to overﬁtting. To circumvent\nthis"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "issue, CNNs can be pre-trained or ﬁne-tuned on datasets similar or not to the target task (Cam-"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "pos et al., 2015; Xu et al., 2014). Well-known CNN architectures such as AlexNet (Krizhevsky"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "et al., 2017), VGG (Simonyan & Zisserman, 2015), and GoogleNet form an important set of"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "baselines\nfor a large number of\ntasks, particularly pre-training such networks on ImageNet"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "dataset constitutes a powerful tool\nfor representation learning. However, recent FER studies"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "have shown that VGG-Face architectures, which are trained on a very large dataset of face im-"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "ages overwhelms architectures trained on ImageNet for FER applications (Kaya et al., 2017)."
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "Furthermore, Li & Deng (2018) have shown that multi-stage ﬁne-tuning can provide an even"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "better performance. We can particularly mention FER2013 (Goodfellow et al., 2013), TFD"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "(Susskind et al., 2010) or more recently RAF-DB (Li et al., 2017a; Li & Deng, 2019) datasets"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "as good sources of additional data for FER tasks. Besides, Tannugi et al.\n(2019) and Li &"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "Deng (2020) pursued interesting work on cross-dataset generalization task by switching in"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "turn source and target FER datasets and evaluating performance of FER models. Li & Deng"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "(2020) have shown that datasets are\nstrongly biased and they have developed accordingly,"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "novel architecture that can learn domain-invariant and discriminative features."
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "Globally,\nin this\nstudy we have considered using three diﬀerent data sources\nfor double"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "transfer learning (de Matos et al., 2019): VGG-Face,\nImageNet, and RAF-DB. For the ﬁrst"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "two datasets, we already have three pre-trained architectures (VGG-11, VGG-16, ResNet50)."
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "On the other, we had to re-train such architectures on RAF-DB. We have evaluated several"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "conﬁgurations for training and ﬁne-tuning diﬀerent CNN architectures with RAF-DB to ﬁnd"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "out how multi-stage ﬁne-tuning can be well performed.\nIn detail, we ﬁne-tuned CNN ar-"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "chitectures by freezing the weights of certain early layers while optimizing deeper ones. As"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "architectures are divided into convolution blocks, we have frozen weights according to these"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "blocks. The proposed architecture kept convolution blocks but classiﬁcation layers (i.e fully"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "connected layers) were replaced by a stack of two fully connected layers with 512 and 128 units"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "respectively, and an output layer with seven units, since there are seven diﬀerent emotion cat-"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "egories in RAF-DB: surprise,\nfear, disgust, happiness, sadness, anger, and neutral."
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "3.2. Pre-Processing"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "Face images are usually aﬀected by background variations such as illumination, head pose,"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "and face patterns linked to some identity bias.\nIn this way, alignment and normalization are the"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "two most commonly used preprocessing methods in face recognition, which may aid learning"
        },
        {
          "3.1. Pre-Training and Fine-Tuning of 2D-CNNs": "discriminant\nfeatures.\nFor\ninstance,\nthe RAF-DB dataset contains aligned faces, while the"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "is not an important issue for this study. Furthermore, normalization only consists in scaling"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "pixel values between 0 to 1 and to standardize input dimensions,\nfaces have been resized to"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "100×80 pixels, which is the average dimension of\nfaces founded in the target dataset. On the"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "other hand, we detail other essential steps for face expression recognition in video sequences"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "such as frame and face extraction, and window bagging."
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "3.2.1. Frame and Face Extraction"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "The videos of the target dataset (SEWA-DB) have been recorded at 50 frames per second"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "(fps). On the other hand,\nthe valence and arousal annotations are available at each 10 ms,"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "which corresponds to 10 fps. Therefore,\nit is necessary to replicate annotations for non-labeled"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "frames when using 50 fps."
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "For locating and extracting faces from the frames of the SEWA-DB videos, we used a multi-"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "task cascaded CNN (MTCNN) (Zhang et al., 2016), which has shown a great eﬃciency to elect"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "the best bounding box candidates showing a complete face within the image. MTCNN employs"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "three CNNs sequentially to decide which bounding box must be kept according to particular"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "criteria learned by deep learning. The face extractor network outputs box coordinates and"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "ﬁve facial\nlandmarks: both eyes, nose and mouth extremities. Once faces are located, they are"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "cropped using the corresponding bounding box. An overview of MTCNN architecture is shown"
        },
        {
          "subjects in the SEWA-DB dataset are naturally facing a web camera. Then,\nface alignment": "in Figure 4. Only frames showing whole faces are kept, while other frames are discarded."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "get expression intensity variations by pointing peak and non-peak expressions along sequences."
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "However, while whole video sequences\nrepresent multiple annotations at a speciﬁc sampling"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "rate and not a single label, to represent a succession of diverse emotional states, we split the"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "video sequences into several clips of ﬁxed length with a speciﬁc overlapping ratio. This has"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "two main advantages:\n(i) it increases the amount of data for training CNNs; (ii) it allows the"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "investigation of which window settings\nis better\nfor\ntraining spatiotemporal CNNs\nto learn"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "from long sequences of valence and arousal annotations. Based on an exploratory study, we"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "have chosen two sequence lengths (16 and 64 consecutive frames of a single video) and three"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "overlapping ratios for each sequence length (0.2, 0.5 and 0.8). For instance, a window of 16"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "consecutive frames with an overlapping of 0.5 contains the last eight frames of the previous"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "window."
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "It was also important to check the integrity of contiguous video frames.\nIndeed some frames"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "are discarded because no face was detected within them, hence damaging the continuity of"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "the temporal\ninformation of emotion between each frame. The proposed strategy to divide"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "videos\ninto clips may introduce\nimportant\ntemporal gaps between two consecutive\nframes."
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "Therefore, we applied a tolerance (a temporal diﬀerence between close frames) to select clips"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "that give sense to a unique emotion unit. Globally, the MTCNN can detect faces in clips and"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "in average, 90% of the frames are kept, depending on the sequence length, overlapping ratio"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "and frame rate. Figure 5 presents the number of clips available in training, validation and test"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "sets according to such parameters. Finally, the last preprocessing step is to fuse annotations"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "of multiple frames in one clip to get a single emotion label\nfor each window. For such aim we"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "use either the average of labels or the extremum value of the labels to obtain a single label for"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "each continuous emotion (a single value of valence and a single value of arousal."
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "3.3. Spatiotemporal Models"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "We have developed two spatiotemporal models:\n(i) a cascaded network based on a VGG-16"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "network pre-trained on VGG-Face that can be ﬁne-tuned or not on RAF-DB; (ii) an inﬂated"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "network based on either VGG-11, VGG-16, or ResNet50 architectures pre-trained on diﬀerent"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "datasets (VGG-Face, RAF-DB, ImageNet)."
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "3.3.1. Cascaded Networks (2D-CNN-LSTM)"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "Long short\nterm memory units (LSTMs) are a special kind of RNN, capable of\nlearning"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "order dependence as we may ﬁnd in a sequence of\nframes from a video. The core of LSTMs"
        },
        {
          "and it varies for each individual. Several studies have been previously carried out in order to": "is a cell state, which adds or removes information depending on the input, output and forget"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "10 and 50 fps; sequence length of 16, 32 and 64 frames; overlapping ratio of 20%, 50% and 80%."
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "gates. The cell\nstate remembers values over arbitrary time intervals and the gates\nregulate"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "the ﬂow of\ninput and output information of the cell."
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "The architecture of the proposed cascade network combines the 2D convolutional\nlayers of"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "VGG-16 for representation learning with an LSTM to support sequence prediction, as shown in"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "Figure 6. The LSTM has a single layer with 1,024 units, with random and uniform distribution"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "initialization to extract temporal\nfeatures from the face features learned by the 2D-CNN.\nIn"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "order\nto avoid overﬁtting, we added some dropout\n(20%) and recurrent dropout\n(20%) on"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "LSTM units. Besides, there are also three fully connected layers stacked after the LSTM to"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "improve the expressiveness and accuracy of the model."
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "The VGG-16-LSTM architecture\nis pre-trained considering two diﬀerent\nstrategies:\n(i)"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "VGG-16 pre-trained on the VGG-Face dataset;\n(ii) VGG-16 pre-trained on the VGG-Face"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "dataset and ﬁne-tuned on the RAF-DB dataset. The former strategy adds extra information"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "to the models, such as classiﬁcation of discrete emotions with RAF-DB, which could help to"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "improve the performance on the regression task."
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "3.3.2.\nInﬂated 3D-CNN (i3D-CNN)"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "The need to analyze a sequence of frames led us to the use of 3D-CNNs. 3D-CNNs produce"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "activation maps that allow analyzing data where temporal\ninformation is relevant. The main"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "advantage of 3D-CNNs is to learn representation from clips that can strengthen the spatiotem-"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "poral relationship between frames. Diﬀerent from 2D-CNNs, 3D-CNNs are directly trained on"
        },
        {
          "Figure 5: Evolution of the number of sequences for training, validation and test sets for diﬀerent conﬁgurations:": "batches of\nframe sequences rather than batches of\nframes. On the other hand, adding a third"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "its output\nto form a feature vector\nrepresenting one clip. After going through the LSTM unit\nfor modeling"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "temporal\ninformation between frames, three fully connected (FC) layers perform the regression of valence and"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "arousal values. For each convolutional\nlayer a 3×3 kernel\nis used, and the number of ﬁlters are indicated. We"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "also detailed the number of units in LSTM and FC layers."
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "dimension to the CNN architecture increases the number of parameters of the model and that"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "requires much larger training datasets than those required by 2D models. The main downside"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "of using such an architecture for FER tasks is the lack of pre-trained models. Besides that, we"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "cannot consider training 3D-CNN architectures in an end-to-end fashion for continuous emo-"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "tion recognition due to the limited amount of training data. Therefore, a feasible solution is to"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "resort to weight inﬂation of 2D-CNN pre-trained models (Carreira & Zisserman, 2017).\nInﬂat-"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "ing a 2D-CNN minimizes the need for large amounts of data for training properly a 3D-CNN"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "as the inﬂation process reuses the weights of the 2D-CNNs. Figure 7 shows that the weight"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "inﬂation consists of enlarging kernels of each convolution ﬁlter by one dimension. Regarding"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "our target task,\nit means to extend the receptive ﬁeld of each neuron to the time dimension"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "(a.k.a.\na sequence of\nframes).\n2D convolutional kernels are then replicated as many times"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "as necessary to ﬁt the third dimension and form a 3D convolutional kernel. At ﬁrst glance,"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "pre-trained weights are just copied through the time dimension and provide better approxi-"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "mation for initialization than randomness but do not constitute yet an adequate distribution"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "for the time dimension. With this in mind, the next issue is to ﬁnd a method that ﬁts best the"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "transfer\nlearning to time dimension with weight\ninﬂation by varying some parameters,\nsuch"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "as:\ninitialization, masking, multiplier and dilation."
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "Initialization:. When replicating kernels for weight inﬂation,\nit is possible to simply copy the"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "weights n times (n being the dimension of time axis) or to center the weights. Centering means"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "copying once weights of a 2D kernel and initializing the weights of the surrounding kernels that"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "form the 3D ﬁlter either\nrandomly (with a uniform distribution) or with zeros. We assume"
        },
        {
          "Figure 6: The architecture of the VGG-16-LSTM. Video frames are fed to the CNN and then accumulated at": "that pre-trained 2D kernels have a good capacity of generalization for\nimages,\nthen giving"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "replicated along a new dimension (temporal dimension), to obtain 3D convolutional kernels. Basically, n × n"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "kernels are made cubic to obtain n × n × n kernels. This process\nis applied to every convolutional ﬁlter\nto"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "transform 2D convolutional\nlayers into 3D convolutional\nlayers."
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "suﬃciently distant distribution for all but one 2D kernel from the copied 2D kernel could have"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "a positive impact on model convergence."
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "Masking:. Assuming that copied 2D kernels have been pre-trained properly considering a very"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "similar task and that they perform well on images, the idea of masking is to train adequately"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "inﬂated weights on time dimension. Then we consider not modifying centered weights during"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "training in order to disseminate the spatial representation learned from pre-trained weights to"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "inﬂated weights."
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "Multiplier:. The distribution of CNN weights and the range of targeted values for regression"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "are closely related. Since values of valence and arousal range from −1 to 1 and standard values"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "of the weights often take values between 10−3 and 10−1, then rising targeted values by a factor"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "could allow to scale up the distribution space and improve convergence."
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "Dilation:. As suggested by Yu & Koltun (2016), we used dilated convolutions on our models."
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "The dilation was performed only on time dimension. We divided the architectures into four"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "blocks with increasing\nlevels of dilation starting\nfrom level 1\nfor\nconvolutional\nlayers\n(no"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "dilation) then 2, 4 and 8 for top convolutional\nlayers. Dilated convolution consists of receptive"
        },
        {
          "Figure 7: Representation of the inﬂation method for a single convolutional ﬁlter. 2D convolutional kernels are": "ﬁelds larger than conventional ones.\nIn other words, neuron connections of one convolutional"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "shown a good performance for segmentation and object recognition task."
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "Figure 8: The proposed i3D-CNN based on inﬂation of 2D convolutional kernels of a pre-trained VGG-16 CNN."
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "Video clips are fed to the i3D-CNN, then spatiotemporal\nface features are extracted and two fully connected"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "(FC) layers perform the regression of valence and arousal values. For each convolutional\nlayer a 3×3×3 kernel"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "is used, and the number of ﬁlters are indicated. We also show the number of units in FC layers."
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "Figure 8 shows the architecture of the proposed i3D-CNN, which is based on the inﬂation"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "of 2D convolutional kernels of a pre-trained VGG-16 CNN. Such a i3D-CNN is then ﬁne-tuned"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "on a target dataset\nto perform regression of valence and arousal values with a sequence of"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "fully-connected layers."
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "3.4. Post-Processing"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "The post-processing aims to improve the quality of the prediction by using some statistical"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "information of\nthe target dataset\nto reduce variance among datasets\n(Ortega et al., 2019)."
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "Due to data imbalance in the training set, some values of valence and arousal are diﬃcult to"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "reach. For instance,\nin the target dataset, neutral emotions, which imply valence and arousal"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "levels\nclose\nto zero, are much more\nfrequent\nin the\ntraining set\nthan extreme valence and"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "arousal values. We use three post-processing steps:\nscale normalization, mean ﬁltering, and"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "time delay."
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "Scale normalization consists\nin normalizing the predictions according to the distribution"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "of the labels in the training set. Valence and arousal predictions (y(cid:48)) are normalized by the"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "mean (yltr ) and the standard deviation (σltr ) of the labels of the training set as:"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "y(cid:48) − yltr"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "(1)\nysn ="
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "σltr"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "Mean ﬁltering consists in centering predictions around mean values,\nincreasing the linear"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "relationship and correspondence to the labels. Valence and arousal predictions (y(cid:48)) are centered"
        },
        {
          "layer are spread among neurons of previous layers. Notably, this kind of\nimplementation has": "15"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "value of the predictions (y(cid:48)\ntr) on the training set as:"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "(2)"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "tr\nymf = y(cid:48) − yltr + y(cid:48)"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "Finally, time delay is used to compensate some oﬀset between the labels and the predictions"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "due\nto the\nreaction-lag of annotators.\nValence and arousal predictions\n(y(cid:48)(f )) at\nframe f"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "are shifted over\nt\nframes\n(precedent or\nsubsequent)\nin order\nto align predictions and labels"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "temporally as:"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "(3)\nytd = y(cid:48)(f + t)"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "where t is an integer in [−10, 10]."
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "4. Experimental Results"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "In this section we present a brief description of the two FER datasets used in the experi-"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "ments: RAF-DB and SEWA-DB. Next, we present the performance measures and summarize"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "our experimental setting and the results achieved by the proposed 2D-CNN-LSTM and i3D-"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "CNN models."
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "4.1. Facial Expression Datasets"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "Real World Aﬀective Faces Database\n(RAF-DB)\nis a real world dataset\nthat\ncontains"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "29,672 images downloaded from the Internet (Li et al., 2017b). Each image has been labeled"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "by around 40 annotators.\nThe dataset has\ntwo types of annotation:\nseven classes of basic"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "emotions and 12 classes of compound emotions. We have only used the seven basic emotions"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "(face images and labels). Other metadata such as facial landmarks, bounding box and identity"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "bias such as age, gender,\nrace are also provided but they have not been used in any step of"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "the proposed approach. RAF-DB was used to ﬁne-tune the pre-trained 2D-CNNs."
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "SEWA-DB is a large and richly annotated dataset\nconsisting of\nsix groups of\nsubjects"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "(around 30 people per group), from six diﬀerent cultural backgrounds (British, German, Hun-"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "garian, Greek, Serbian, and Chinese) and divided into pairs of subjects (Kossaiﬁ et al., 2019)."
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "Each pair had to discuss their emotional state and sentiment toward four adverts previously"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "watched. The dataset consists of 64 videos (around 1,525 minutes of audio visual data), which"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "are split into three folders (34 training, 14 validation, 16 test). Since the labels are not pro-"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "vided for the test set due to its use in FER challenges, we used the validation set as the test"
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "set and split the training set into a new training set (28 videos) and validation set (6 videos)."
        },
        {
          "by subtracting the mean value of the labels (yltr ) of the training set and by adding the mean": "16"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "arousal annotations\nsince previous\nstudies have indicated that\nthe level of\nliking is not well"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "related with facial expressions."
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "4.2. Performance Metrics"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "The standard performance metrics used in continuous emotion recognition are the mean"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "absolute error (MAE), the mean absolute percentage error (MAPE), Pearson correlation coef-"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "ﬁcient (PCC) and concordance correlation coeﬃcient (CCC). PCC assesses distance between"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "target values and predictions and CCC establishes the strength of a linear relationship between"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "two variables. The range of possible values lies in the interval\n[−1; 1], where −1 or 1 means"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "strong relation and 0 means no relation at all. The MAE for a set of\nlabels y and predictions"
        },
        {
          "Annotations are given for valence, arousal and levels of\nliking. We only used valence and": "y(cid:48)\nis given by:"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 1: , where the suffix BN refers to batch normalization layers added to the original archi-",
      "data": [
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "pre-trained 2D-CNNs on RAF-DB, video frames have been resized to 100×80×3, which is the"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "mean dimension of video frames of\nthe target dataset\n(SEWA-DB). Learning rate has been"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "ﬁxed to 1e−5 and batches of size 16. Optimization has been performed with Adam optimizer."
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "We have assigned diﬀerent weights to each class, according to the number of samples, to deal"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "with the data imbalance found in RAF-DB. This allows that classes with few samples can aﬀect"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "the weights of the model to the same extent as classes with many more samples. Moreover,"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "we have observed that\nlow-level data augmentation such as\nlike rotation, ﬂipping, highlight"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "variations, could help improve the performance. Although data augmentation cannot bring"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "signiﬁcant information for emotion recognition,\nit can prevent overﬁtting on a single sample"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "and improve model distributions."
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "The performance achieved by the 2D-CNNs after ﬁne-tuning on RAF-DB is presented in"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "Table 1, where the suﬃx BN refers to batch normalization layers added to the original archi-"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "tectures after each convolutional\nlayer to improve model convergence and reduce overﬁtting."
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "Furthermore, we indicate for each architecture the dataset used for pre-training as well as the"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "convolution block (2 1 to 5 1) from which we start ﬁne-tuning the architectures.\nIn general,"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "most of the ﬁne-tuned models achieved accuracy higher than the baseline models (Jyoti et al.,"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "2019). Jyoti et al. (2019) developed two 2D-CNNs to analyze action units detection eﬃciency"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "on three datasets including RAF-DB. The ﬁrst CNN was based on residual connections with"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "densely connected blocks (ResNet) and the second architecture was a 2D-CNN consisting of"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "four convolution layers and three fully connected layers. Such baselines achieved 76.54% and"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "78.23% of accuracy on the test set of the RAF-DB dataset, respectively. On the other hand,"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "the proposed VGG-16 CNN model pre-trained with VGG-Face achieved 79.90% of accuracy"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "on the same test set."
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "Other recent works, which employed attention networks have achieved better performances"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "(Wang et al., 2020; Li et al., 2019).\nIn the proposed approach, we did not have considered two"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "common problems we may found in face analysis in real-world scenarios: occlusions and pose"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "variations. On the contrary, Wang et al. (2020) and Li et al. (2019) addressed these problems"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "by using region-based attention networks. Attention modules are used to extract compact face"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "representations based on several\nregions cropped from the face and they adaptively adjusts"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "the importance of\nfacial parts. Therefore,\nthese models\nlearn to discriminate occluded and"
        },
        {
          "three architectures were pre-trained either on VGG-Face or\nImageNet.\nFor ﬁne-tuning the": "non-occluded faces while improving emotion detection in both cases."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "level of ﬁne-tuning of each model. Full denotes that all"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Dataset for"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Reference\nModel\nPre-Training"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-11\nImageNet"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-11-BN\nImageNet"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-16\nVGG-Face"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Proposed"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-16-BN\nVGG-Face"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "ResNet50\nVGG-Face"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "RCNN\nNA"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Jyoti et al. (2019)"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "CNN\nNA"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Li et al. (2019)\nACNN\nImageNet"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Wang et al. (2020)\nCNN+RAN\nMS-Celeb-1M"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "level of ﬁne-tuning of each model. Full denotes that all"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Dataset for"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Reference\nModel\nPre-Training"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-11\nImageNet"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-11-BN\nImageNet"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-16\nVGG-Face"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Proposed"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "VGG-16-BN\nVGG-Face"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "ResNet50\nVGG-Face"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": ""
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "RCNN\nNA"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Jyoti et al. (2019)"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "CNN\nNA"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Li et al. (2019)\nACNN\nImageNet"
        },
        {
          "Table 1: Results of the ﬁne-tuning of VGG and ResNet50 models on RAF-DB. Convolution block denotes the": "Wang et al. (2020)\nCNN+RAN\nMS-Celeb-1M"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 2: shows the results in terms of PCC and CCC considering different frames rates",
      "data": [
        {
          "4.4. 2D-CNN-LSTM Architecture": "After specializing the three pre-trained CNN architectures (VGG-11, VGG-16 and ResNet50)"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "for emotion recognition by ﬁne-tuning them on the RAF-DB dataset, we develop cascaded net-"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "works based on such architectures for spatiotemporal continuous emotion recognition. For such"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "an aim, we have developed two 2D-CNN-LSTM models, one based on the VGG-16 architecture"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "pre-trained on VGG-Face and ﬁne-tuned on RAF-DB because such an architecture achieved"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "the best results on RAF-DB test set, and a second one without ﬁne-tuning on the RAF-DB"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "dataset. Spatial\nfeatures for each input frame are sequentially provided by the 2D-CNN and"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "the LSTM unit models the temporal\ninformation from a single clip. Diﬀerent conﬁgurations"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "were evaluated by varying the length of sequences, the overlapping ratio and the strategy to"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "fuse the labels within a clip.\nThe architectures were ﬁne-tuned on the development\nset of"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "SEWA-DB and the mean squared error (MSE) was used as cost function. Some other works"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "have also considered CCC as cost\nfunction (Ortega et al., 2019)\nsince it provides\ninforma-"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "tion about correspondence and correlation between predictions and annotations. However, we"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "observed a better convergence while using the MSE."
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "Table 2 shows\nthe results\nin terms of PCC and CCC considering diﬀerent\nframes\nrates"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "(fps),\nsequence\nlengths\n(SL), overlapping ratios\n(OR) and fusion modes\n(FM).\nIn general,"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "both extremum and mean fusion performed well and the best\nresults\nfor both valence and"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "arousal were achieved for sequences of 64 frames at 10 fps. The VGG-16 architecture beneﬁted"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "from ﬁne-tuning on the RAF-DB and it achieved CCC values of 0.625 for valence and 0.557"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "for arousal on the validation set of SEWA-DB.\nIn addition to the\ncorrelation metrics,\nthe"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "proposed 2D-CNN-LSTM achieved an overall MAE of 0.06 (amongst 2D-CNN-LSTM models),"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "which also indicates a good correspondence between predictions and annotations.\nSince the"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "best performance has been obtained with post-processing steps,\nthus\nremodeling our\nset of"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "predictions and annotations, we have also computed MAPE to evaluate the error ratio between"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "predictions and annotations."
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "4.5.\ni3D-CNN Architecture"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "Another alternative for spatiotemporal modeling is to use the i3D-CNN. In this way, strong"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "spatiotemporal correlations between frames are directly learned from video clips by a single"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "network. Thanks\nto weight\ninﬂation, we are able to use the pre-trained 2D-CNNs\nto build"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "i3D-CNN architectures. The inﬂation method allows us to transpose learned information from"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "various static tasks to dynamic ones, and therefore to perform the essential transfer learning"
        },
        {
          "4.4. 2D-CNN-LSTM Architecture": "for learning spatiotemporal\nfeatures. With this in mind, we reused the 2D-CNN architectures"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 1: and expand their convolutional layers to build i3D-CNNs considering two",
      "data": [
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": "Dataset for"
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": "Initialization"
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": "VGG-Face"
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": "RAF-DB"
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        },
        {
          "Table 2: Performance of the 2D-CNN-LSTM based on the VGG-16 architecture on the SEWA-DB dataset.": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 1: and expand their convolutional layers to build i3D-CNNs considering two",
      "data": [
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "shown in Table 1 and expand their convolutional\nlayers\nto build i3D-CNNs considering two"
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "conﬁgurations, denoted as C1 and C2 in Table 3."
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "Due to the high number of trainable parameters, i3D-CNNs are particularly time-consuming"
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "to train and therefore we had to ﬁx the value of some basic hyperparameters instead of perform-"
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "ing exploratory experiments to set them. Therefore, we evaluated only the best conﬁguration"
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "found for the 2D-CNN-LSTM, as shown in Table 1, which uses batch of size 8, sequence length"
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "of 64 frames, overlapping ratio of 0.8, and frame rate of 10 fps. This\nis\nthe main downside"
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "of our approach based on i3D CNNs, as the number of trainable parameters of\ni3D-CNNs is"
        },
        {
          "SL: Sequence length (in frames), OR: Overlapping ratio, FM: Fusion mode.": "three times greater than the counterpart 2D-CNNs."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 1: and expand their convolutional layers to build i3D-CNNs considering two",
      "data": [
        {
          "Table 3: Hyperparameters of": "Parameters",
          "i3D-CNN architectures and their possible values.": "C1\nC2"
        },
        {
          "Table 3: Hyperparameters of": "Inﬂation",
          "i3D-CNN architectures and their possible values.": "Centered\nCopied"
        },
        {
          "Table 3: Hyperparameters of": "Weight Initialization",
          "i3D-CNN architectures and their possible values.": "Random\nZero"
        },
        {
          "Table 3: Hyperparameters of": "Masking",
          "i3D-CNN architectures and their possible values.": "No\nYes"
        },
        {
          "Table 3: Hyperparameters of": "",
          "i3D-CNN architectures and their possible values.": "Bloc1: 1\nBloc1: 1"
        },
        {
          "Table 3: Hyperparameters of": "",
          "i3D-CNN architectures and their possible values.": "Bloc2: 1\nBloc2: 2"
        },
        {
          "Table 3: Hyperparameters of": "Dilation",
          "i3D-CNN architectures and their possible values.": ""
        },
        {
          "Table 3: Hyperparameters of": "",
          "i3D-CNN architectures and their possible values.": "Bloc3: 1\nBloc3: 4"
        },
        {
          "Table 3: Hyperparameters of": "",
          "i3D-CNN architectures and their possible values.": "Bloc4: 1\nBloc4: 8"
        },
        {
          "Table 3: Hyperparameters of": "Multiplier",
          "i3D-CNN architectures and their possible values.": "×1\n×100"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table 2: ) better results than i3D-CNN architectures (Table 6). Notably, for",
      "data": [
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "Inﬂated 3D-CNNs for regression seem to be very sensitivity to some conﬁgurations for training"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "regarding the range of\nresults achieved by diﬀerent base models and datasets used in their"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "initialization.\nIn these conditions,\nit\nis diﬃcult\nto state on the eﬀect of a single parameter"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "for\ninﬂation. VGG-16 with batch normalization and ResNet50 achieved the best\nresults\nfor"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "both valence and arousal and have shown a good ability to predict these values compared to"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "other base models. Surprisingly, the VGG-16 pre-trained on ImageNet achieved higher PCC"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "and CCC for both valence and arousal than those base models pre-trained on VGG-Face and"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "RAF-DB, which are source datasets closer to the target one. On the hand, ResNet50 beneﬁted"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "from the initialization with VGG-Face.\nIn summary, the best results range from 0.313 to 0.406"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "for PCC and from 0.253 to 0.326 for CCC. These performances still show a poor correlation"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "between predictions and annotations but are comparable to the performance achieved by other"
        },
        {
          "obtained for\neach architecture\nfor valence and arousal\nin terms of PCC and CCC values.": "studies on continuous emotion prediction that use the SEWA-DB dataset."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 2: ) better results than i3D-CNN architectures (Table 6). Notably, for",
      "data": [
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": ""
        },
        {
          "Table 4:": "Base",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "Dataset for"
        },
        {
          "Table 4:": "Models",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "Initialization"
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "RAF-DB"
        },
        {
          "Table 4:": "VGG-11-BN",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": ""
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "ImageNet"
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "VGG-Face"
        },
        {
          "Table 4:": "VGG-16",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "RAF-DB"
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "ImageNet"
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "VGG-Face"
        },
        {
          "Table 4:": "VGG-16-BN",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "RAF-DB"
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "ImageNet"
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "VGG-Face"
        },
        {
          "Table 4:": "ResNet50",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "RAF-DB"
        },
        {
          "Table 4:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting valence.": "ImageNet"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": ""
        },
        {
          "Table 5:": "Base",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "Dataset for"
        },
        {
          "Table 5:": "Models",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "Initialization"
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "RAF-DB"
        },
        {
          "Table 5:": "VGG-11-BN",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": ""
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "ImageNet"
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "VGG-Face"
        },
        {
          "Table 5:": "VGG-16",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "RAF-DB"
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "ImageNet"
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "VGG-Face"
        },
        {
          "Table 5:": "VGG-16-BN",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "RAF-DB"
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "ImageNet"
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "VGG-Face"
        },
        {
          "Table 5:": "ResNet50",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "RAF-DB"
        },
        {
          "Table 5:": "",
          "i3D-CNN model conﬁgurations according to the best performances for predicting arousal.": "ImageNet"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6: Best performances of": "and MAPE according to diﬀerent models and their initialization.",
          "i3D-CNNs for predicting valence and arousal": "",
          "in terms of PCC and CCC values": ""
        },
        {
          "Table 6: Best performances of": "Base",
          "i3D-CNNs for predicting valence and arousal": "",
          "in terms of PCC and CCC values": "Arousal"
        },
        {
          "Table 6: Best performances of": "Models",
          "i3D-CNNs for predicting valence and arousal": "PCC↑",
          "in terms of PCC and CCC values": "CCC↑"
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.035",
          "in terms of PCC and CCC values": "0.348"
        },
        {
          "Table 6: Best performances of": "VGG-11-BN",
          "i3D-CNNs for predicting valence and arousal": "",
          "in terms of PCC and CCC values": ""
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.040",
          "in terms of PCC and CCC values": "0.203"
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.119",
          "in terms of PCC and CCC values": "0.166"
        },
        {
          "Table 6: Best performances of": "VGG-16",
          "i3D-CNNs for predicting valence and arousal": "0.036",
          "in terms of PCC and CCC values": "0.119"
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.209",
          "in terms of PCC and CCC values": "0.189"
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.203",
          "in terms of PCC and CCC values": "0.304"
        },
        {
          "Table 6: Best performances of": "VGG-16-BN",
          "i3D-CNNs for predicting valence and arousal": "0.123",
          "in terms of PCC and CCC values": "0.165"
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.346",
          "in terms of PCC and CCC values": "0.326"
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.313",
          "in terms of PCC and CCC values": "0.273"
        },
        {
          "Table 6: Best performances of": "ResNet50",
          "i3D-CNNs for predicting valence and arousal": "0.113",
          "in terms of PCC and CCC values": "0.207"
        },
        {
          "Table 6: Best performances of": "",
          "i3D-CNNs for predicting valence and arousal": "0.183",
          "in terms of PCC and CCC values": "0.256"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "information for describing the level of positivity in emotions, hence valence values.\nIn contrast,"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "arousal\nis better predicted with voice frequencies and audio signals. However, our work with"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "inﬂated networks suggests that simultaneous learning spatiotemporal face features beneﬁts the"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "prediction of arousal values."
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "Regarding the complexity of the two proposed approaches for continuous emotion recog-"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "nition, we had to make some trade-oﬀ that certainly have impacted the quality of the results"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "provided by i3D-CNN architectures. We have also observed a high sensitivity in the training"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "of\nthis type of architecture according to various conﬁgurations. This implies that\ni3D-CNN"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "architectures are very ﬂexible and further improvement could lie in better initialization and"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "tuning of the number and quality of parameters regarding the potential of this model. Further-"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "more, inﬂated weights provided good initialization for action recognition tasks which suggested"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "that we could also take advantage of this method for emotion recognition. However, the main"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "diﬀerence is that for action recognition, researchers had hundreds of various short videos for"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "a classiﬁcation task while we have relatively long videos of\nfew subjects for a regression task."
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "Nevertheless, the experimental results have shown a great potential for further improvement if"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "more data is available for ﬁne-tuning the i3D models. On the other hand, the performance of"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "2D-CNN-LSTM architectures were very satisfying and this type of architecture is still a good"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "choice for FER applications."
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "Table 7: Comparison of the best results achieved by the proposed models and the state-of-the-art for SEWA-DB"
        },
        {
          "have raised the fact that intuitively face textures on video sequences are the main source of": "dataset."
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": "CCC↑",
          "Arousal": "CCC↑"
        },
        {
          "Valence": "0.625",
          "Arousal": "0.557"
        },
        {
          "Valence": "",
          "Arousal": ""
        },
        {
          "Valence": "0.304",
          "Arousal": "0.326"
        },
        {
          "Valence": "0.312",
          "Arousal": "0.202"
        },
        {
          "Valence": "0.207",
          "Arousal": "0.123"
        },
        {
          "Valence": "0.281",
          "Arousal": "0.115"
        },
        {
          "Valence": "0.270",
          "Arousal": "0.110"
        },
        {
          "Valence": "0.350",
          "Arousal": "0.290"
        },
        {
          "Valence": "0.540",
          "Arousal": "0.581"
        },
        {
          "Valence": "0.580",
          "Arousal": "0.594"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 7: shows the best results achieved by the proposed 2D-CNN-LSTM and i3D-CNN",
      "data": [
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "models and compare them with the baseline models proposed by Kossaiﬁ et al. (2019). For"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "ResNet18, they have evaluated Root Mean Squared Error (RMSE) and CCC as loss function."
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "The CCCs achieved by the i3D-CNN are slightly higher than those achieved by all models of"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "Kossaiﬁ et al. (2019). On the other hand,\nthe CCC values achieved by the 2D-CNN-LSTM"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "are almost twice than the best results achieved by the best model (ResNet18) of Kossaiﬁ et al."
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "(2019). Table 7 also shows the results achieved by Chen et al. (2019) and Zhao et al. (2019),"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "which are not directly comparable since both used a subset of SEWA-DB encompassing only"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "three cultures. They optimized emotion detection for two cultures (Hungarian, German)\nto"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "perform well on the third one (Chinese). Notably, Chen et al. (2019) proposed a combination"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "of 2D-CNN and 1D-CNN, which has fewer parameters than 3D-CNNs, and a spatiotemporal"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "graph convolution network (ST-GCN)\nto extract appearance features from facial\nlandmarks"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "sequences.\nZhao et al.\n(2019) used a VGG-style CNN and a DenseNet-style CNN to learn"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "cross-culture face features, which were used to predict two adversarial targets: one for emotion"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "prediction, another for culture classiﬁcation.\nIn conclusion, these two methodologies achieved"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "state-of-the-art results on cross-cultural emotion prediction tasks."
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "6. Conclusion"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "In this paper, we have presented two CNN architectures for continuous emotion prediction"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "in-the-wild.\nThe ﬁrst architecture\nis a combination of a ﬁne-tuned VGG-16 CNN and an"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "LSTM unit. Such an architecture achieved state-of-the-art results on the SEWA-DB dataset,"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "producing CCC values of 0.625 and 0.557 for valence and arousal prediction,\nrespectively."
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "The second architecture is based on the concept of\ninﬂation, which transfers knowledge from"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "pre-trained 2D-CNN models into a 3D to model temporal\nfeatures. The best proposed i3D-"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "CNN architecture achieved CCC values of 0.304 and 0.326 for valence and arousal prediction,"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "respectively. These values are far below than those achieved by the 2D-CNN-LSTM. Due to"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "the high number of parameters of\ni3D-CNNs\n(barely 3 times greater\nthan 2D-CNNs), ﬁne-"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "tuning and hyperparameter tuning of such architectures require a huge computational eﬀort"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "as well as huge datasets.\nUnfortunately,\nfacial\nexpression datasets\nfor\ncontinuous\nemotion"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "recognition are relatively small\nfor such a task."
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "We have also shown that a double transfer\nlearning strategy over VGG and ResNet ar-"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "chitectures with ImageNet and RAF-DB datasets can improve the accuracy of\nthe baseline"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "models.\nIt should be noticed that in this work, subjects were mostly facing the camera with"
        },
        {
          "Table 7 shows\nthe best\nresults achieved by the proposed 2D-CNN-LSTM and i3D-CNN": "relative clear view of the whole face. To some extent, this could imply some bias in the results"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "tecture could at this time be a drag for live applications. Finally, to the best of our knowledge,"
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "it was the ﬁrst time that 3D-CNNs were used in regression applications for predicting valence"
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "and arousal values for emotion recognition."
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "There are some promising directions\nto expand the approaches proposed in this paper."
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "One could take advantage of\nthe development of huge and complex cross-cultural datasets"
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "such as the Aﬀ-Wild dataset to exploit occlusion cases, pose variations or even scene breaks."
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "In particular, with i3D-CNN architectures, we believe deep learning algorithms possess\nthe"
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "capacity and robustness to deal with these speciﬁc cases and beneﬁt from an adequate ﬂexibility"
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "to analyze both the\nseparability and combination of discriminant\nspatiotemporal\nfeatures."
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "Finally, we have shown a peculiar and ﬂexible way of ﬁne-tuning inﬂated CNNs and maybe"
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "this strategy could be transferred to other applications such as object and action recognition"
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "on video sequences."
        },
        {
          "when presenting diverse real world scenarios. Moreover, the complexity of the i3D-CNN archi-": "Competing Interests"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "Abbasnejad, I., Sridharan, S., Nguyen, D., Denman, S., Fookes, C., & Lucey, S. (2017). Using"
        },
        {
          "References": "synthetic data to improve facial expression analysis with 3D convolutional networks.\nIn"
        },
        {
          "References": "IEEE Intl Conf on Computer Vision Workshops (ICCVW) (pp. 1609–1618)."
        },
        {
          "References": "Ayral, T., Pedersoli, M., Bacon, S., & Granger, E. (2021). Temporal stochastic softmax for"
        },
        {
          "References": "3d cnns: An application in facial expression recognition. WACV ,\n."
        },
        {
          "References": "Bargal, S. A., Barsoum, E., Ferrer, C. C., & Zhang, C. (2016). Emotion recognition in the wild"
        },
        {
          "References": "from videos using images.\nIn Intl Conf on Multimodal\nInteraction ICMI\n’16 (p. 433–436)."
        },
        {
          "References": "doi:10.1145/2993148.2997627."
        },
        {
          "References": "Barros, P., & Wermter, S. (2016). Developing crossmodal expression recognition based on a"
        },
        {
          "References": "deep neural model. Adaptive Behavior , 24 . doi:10.1177/1059712316664017."
        },
        {
          "References": "Ben Henia, W. M., & Lachiri, Z. (2017). Emotion classiﬁcation in arousal-valence dimension"
        },
        {
          "References": "using discrete aﬀective keywords tagging.\nIn Intl Conf on Engineering MIS (ICEMIS) (pp."
        },
        {
          "References": "1–6)."
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Understanding ﬁne-tuned CNNs for visual sentiment prediction.\nIn 1st\nIntl Workshop on"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Aﬀect and Sentiment\nin Multimedia (p. 57–62)."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Cardinal, P., Dehak, N., Koerich, A. L., Alam, J., & Boucher, P.\n(2015).\nETS system for"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "AV+EC 2015 challenge.\nIn ACM Multimedia Conference (pp. 17–23). ACM."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Carneiro de Melo, W., Granger, E., & Hadid, A.\n(2020). A deep multiscale spatiotemporal"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "network for assessing depression from facial dynamics.\nIEEE Transactions on Aﬀective"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Computing, PP . doi:10.1109/TAFFC.2020.3021755."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Carreira, J., & Zisserman, A. (2017). Quo vadis, action recognition? A new model and the"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "kinetics dataset.\nIn IEEE Conf on Computer Vision and Pattern Recognition, CVPR (pp."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "4724–4733). doi:10.1109/CVPR.2017.502."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Chen, H., Deng, Y., Cheng, S., Wang, Y., Jiang, D., & Sahli, H.\n(2019).\nEﬃcient\nspatial"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "temporal convolutional\nfeatures\nfor audiovisual continuous aﬀect\nrecognition.\nIn 9th Intl"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "on Audio/Visual Emotion Challenge and Workshop AVEC ’19 (p. 19–26). New York, NY,"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "USA. doi:10.1145/3347320.3357690."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Cossetin, M. J., Nievola, J. C., & Koerich, A. L.\n(2016).\nFacial expression recognition us-"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "ing a pairwise feature selection and classiﬁcation approach.\nIn IEEE International Joint"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Conference on Neural Networks (IJCNN) (pp. 5149–5155)."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Dhall, A., Goecke, R., Lucey, S., & Gedeon, T.\n(2011).\nStatic\nfacial\nexpression analysis"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "in tough conditions: Data,\nevaluation protocol and benchmark.\nIn IEEE Intl Conf on"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Computer Vision Workshops (ICCV Workshops) (pp. 2106–2112)."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Dhall, A., Kaur, A., Goecke, R., & Gedeon, T. (2018). Emotiw 2018: Audio-video, student"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "engagement and group-level aﬀect prediction. In Intl Conf on Multimodal Interaction (ICMI)"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "(pp. 653–656). doi:10.1145/3242969.3264993."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Diba, A., Fayyaz, M., Sharma, V., Karami, A. H., Arzani, M. M., Yousefzadeh, R., & Gool,"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "L. V.\n(2017).\nTemporal 3d convnets: New architecture and transfer\nlearning for video"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "classiﬁcation. arXiv:1711.08200."
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Ding, H., Zhou, S. K., & Chellappa, R.\n(2017).\nFacenet2expnet: Regularizing a deep face"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "recognition net\nfor expression recognition.\nIn 12th IEEE Intl Conf on Automatic Face &"
        },
        {
          "Campos, V., Salvador, A., Giro-iNieto, X., & Jou, B.\n(2015). Diving deep into sentiment:": "Gesture Recognition (pp. 118–126). doi:10.1109/FG.2017.23."
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "face video emotion recognition in the wild using deep neural networks and small datasets."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "In 18th ACM Intl Conf on Multimodal\nInteraction ICMI\n’16 (p. 506–513).\ndoi:10.1145/"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "2993148.2997637."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Ekman, P.\n(1994).\nStrong evidence for universals\nin facial expressions:\na reply to russell’s"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "mistaken critique. Psychological bulletin, 115 , 268–287."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Ekman, P., & Friesen, W. V.\n(1971).\nConstants across\ncultures\nin the\nface and emotion."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Journal of personality and social psychology, 17 , 124–129."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Fan, Y., Lu, X., Li, D., & Liu, Y. (2016). Video-based emotion recognition using CNN-RNN"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "and C3D hybrid networks.\nIn Intl Conf on Multimodal Interaction ICMI ’16 (p. 445–450)."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "doi:10.1145/2993148.2997632."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Fayolle, S. L., & Droit-Volet, S. (2014). Time perception and dynamics of\nfacial expressions"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "of emotions. PLOS ONE , 9 , 1–9. doi:10.1371/journal.pone.0097944."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Georgescu, M.-I.,\nIonescu, R. T., & Popescu, M.\n(2019).\nLocal\nlearning with deep and"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "handcrafted\nfeatures\nfor\nfacial\nexpression\nrecognition.\nIEEE Access,\n7 ,\n64827–64836."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "doi:10.1109/access.2019.2917266."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A. C., Mirza, M., Hamner, B., Cukierski,"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "W., Tang, Y., Thaler, D., Lee, D., Zhou, Y., Ramaiah, C., Feng, F., Li, R., Wang, X.,"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Athanasakis, D., Shawe-Taylor, J., Milakov, M., Park, J.,\nIonescu, R. T., Popescu, M.,"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Grozea, C., Bergstra, J., Xie, J., Romaszko, L., Xu, B., Zhang, C., & Bengio, Y.\n(2013)."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Challenges in representation learning: A report on three machine learning contests.\nIn 20th"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Intl Conf Neural\nInformation Processing\n(pp. 117–124).\nSpringer volume 8228 of Lecture"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Notes in Computer Science. doi:10.1007/978-3-642-42051-1\\_16."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Guo, Y., Tao, D., Yu, J., Xiong, H., Li, Y., & D.Tao (2016). Deep neural networks with"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "relativity learning for facial expression recognition.\nIn IEEE Intl Conf on Multimedia Expo"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Workshops (ICMEW) (pp. 1–6)."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Jack, R. E., Garrod, O. G., Yu, H., Caldara, R., & Schyns, P. G. (2012). Facial expressions"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "of emotion are not culturally universal.\nProc of\nthe National Academy of Sciences, 109 ,"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "7241–7244."
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "Ji, S., Xu, W., Yang, M., & Yu, K.\n(2013).\n3D convolutional neural networks\nfor human"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "action recognition.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 35 ,"
        },
        {
          "Ding, W., Xu, M., Huang, D.-Y., Lin, W., Dong, M., Yu, X., & Li, H.\n(2016). Audio and": "221–231."
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "action unit detection.\nIn 14th IEEE Intl Conf on Automatic Face & Gesture Recognition"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "(pp. 1–8). doi:10.1109/FG.2019.8756580."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Kaya, H., G¨urpınar, F., & Salah, A. (2017). Video-based emotion recognition in the wild using"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "deep transfer\nlearning and score fusion.\nImage and Vision Computing,\n. doi:10.1016/j."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "imavis.2017.01.012."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Kim, B., Dong, S., Roh, J., Kim, G., & Lee, S. (2016). Fusing aligned and non-aligned face"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "information for automatic aﬀect recognition in the wild: A deep learning approach. In IEEE"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Conf on Computer Vision and Pattern Recognition Workshops (CVPRW) (pp. 1499–1508)."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Kim, B.-K., Lee, H., Roh, J., & Lee, S.-Y. (2015). Hierarchical committee of deep CNNs with"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "exponentially-weighted decision fusion for static facial expression recognition.\nIn Intl Conf"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "on Multimodal Interaction ICMI ’15 (p. 427–434). doi:10.1145/2818346.2830590."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Kim, H., Kim, Y., Kim, S. J., & Lee,\nI.\n(2018). Building emotional machines: Recognizing"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "image emotions through deep neural networks. IEEE Trans. Multim., 20 , 2980–2992. doi:10."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "1109/TMM.2018.2827782."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Knyazev, B., Shvetsov, R., Efremova, N., & Kuharenko, A.\n(2017).\nConvolutional neural"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "networks pretrained on large face recognition datasets for emotion classiﬁcation from video."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "arXiv e-prints, (p. arXiv:1711.04598). arXiv:1711.04598."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Kollias, D., Tzirakis, P., Nicolaou , M. A., A.Papaioannou, Zhao, G., Schuller, B., Kotsia, I.,"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "& Zafeiriou, S. (2019). Deep aﬀect prediction in-the-wild: Aﬀ-wild database and challenge,"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "deep architectures, and beyond.\nInternational Journal of Computer Vision, 127 , 907–929."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "doi:10.1007/s11263-019-01158-4."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Kollias, D., & Zafeiriou, S. (2018). A multi-component CNN-RNN approach for dimensional"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "emotion recognition in-the-wild. CoRR, abs/1805.01452 ."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Kossaiﬁ, J., Schuller, B. W., Star, K., Hajiyev, E., Pantic, M., Walecki, R., Panagakis, Y.,"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Shen, J., M.Schmitt, Ringeval, F., & et al. (2019). Sewa db: A rich database for audio-visual"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "emotion and sentiment research in the wild.\nIEEE Transactions on Pattern Analysis and"
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Machine Intelligence, (p. 1–1). doi:10.1109/tpami.2019.2944808."
        },
        {
          "Jyoti, S., Sharma, G., & Dhall, A. (2019). Expression empowered residen network for facial": "Krizhevsky, A., Sutskever,\nI., & Hinton, G. E.\n(2017).\nImagenet\nclassiﬁcation with deep"
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": ". doi:1804.08348."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Li, S., & Deng, W. (2019). Reliable crowdsourcing and deep locality-preserving learning for"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "unconstrained facial expression recognition.\nIEEE Transactions on Image Processing, 28 ,"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "356–370."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Li, S., & Deng, W. (2020). A deeper look at facial expression dataset bias. IEEE Transactions"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "on Aﬀective Computing, (p. 1–1). doi:10.1109/taffc.2020.2973158."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Li, S., Deng, W., & Du, J.\n(2017a).\nReliable\ncrowdsourcing and deep locality-preserving"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "learning for expression recognition in the wild.\nIn IEEE Conf on Computer Vision and"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Pattern Recognition (CVPR) (pp. 2584–2593)."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Li, S., Deng, W., & Du, J.\n(2017b).\nReliable\ncrowdsourcing and deep locality-preserving"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "learning for expression recognition in the wild.\nIn IEEE Conf on Computer Vision and"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Pattern Recognition (CVPR) (pp. 2584–2593)."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Li, Y., Zeng, J., Shan, S., & Chen, X. (2019). Occlusion aware facial expression recognition"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "using CNN with attention mechanism.\nIEEE Transactions on Image Processing, 28 , 2439–"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "2450."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Liu, K., Liu, W., Gan, C., Tan, M., & Ma, H.\n(2018). T-C3D:\ntemporal convolutional 3d"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "network for real-time action recognition.\nIn 32nd AAAI Conf on Artiﬁcial Intelligence (pp."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "7138–7145)."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Liu, X., Kumar, B. V. K. V., You, J., & Jia, P.\n(2017). Adaptive deep metric learning for"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "identity-aware facial expression recognition. In IEEE Conf on Computer Vision and Pattern"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Recognition Workshops (CVPRW) (pp. 522–531)."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "de Matos, J., Britto Jr., A. S., Oliveira, L. E. S., & Koerich, A. L. (2019). Double transfer"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "learning for breast cancer histopathologic image classiﬁcation.\nIn IEEE Intl Joint Conf on"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Neural Networks (IJCNN) (pp. 1–6)."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Mou, W., Celiktutan, O., & Gunes, H.\n(2015). Group-level arousal and valence recognition"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "in static images: Face, body and context.\nIn 2015 11th IEEE Intl Conf and Workshops on"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Automatic Face and Gesture Recognition (FG) (pp. 1–6). volume 05."
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "Nguyen, D., Nguyen, K., Sridharan, S., Ghasemi, A., Dean, D., & Fookes, C. (2017). Deep"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "spatio-temporal\nfeatures for multimodal emotion recognition.\nIn 2017 IEEE Winter Conf"
        },
        {
          "Li, S., & Deng, W. (2018). Deep facial expression recognition: A survey. arXiv preprint arXiv:,": "on Applications of Computer Vision (WACV) (pp. 1215–1223)."
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "pal component analysis\nfor\nface and facial-expression recognition.\nComputing in Science"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Engineering, 13 , 9–13. doi:10.1109/MCSE.2010.149."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Ortega, J. D. S., Cardinal, P., & Koerich, A. L. (2019). Emotion recognition using fusion of"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "audio and video features. In IEEE Intl Conf on Systems, Man, and Cybernetics (SMC) (pp."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "3827–3832)."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Ouyang, X., Kawaai, S., Goh, E., Shen, S., Ding, W., Ming, H., & Huang, D.-Y.\n(2017)."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Audio-visual emotion recognition using deep transfer learning and multiple temporal models."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "In 19th ACM Intl Conf on Multimodal\nInteraction (pp. 577–582).\ndoi:10.1145/3136755."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "3143012."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Parkhi, O. M., Vedaldi, A., & Zisserman, A. (2015). Deep face recognition. In British Machine"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Vision Conf (BMVC) (pp. 41.1–41.12). doi:10.5244/C.29.41."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Pramerdorfer, C., & Kampel, M.\n(2016).\nFacial Expression Recognition\nusing Convo-"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "lutional Neural Networks:\nState\nof\nthe Art.\narXiv\ne-prints,\n(p.\narXiv:1612.02903)."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "arXiv:1612.02903."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Praveen, G., Granger, E., & Cardinal, P.\n(2020a).\nDeep da for ordinal\nregression of pain"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "intensity estimation using weakly-labeled videos. arXiv:2010.15675."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Praveen, G., Granger, E., & Cardinal, P. (2020b). Deep weakly-supervised domain adaptation"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "for pain localization in videos. FG,\n."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Ringeval, F., Schuller, B., Valstar, M., Jaiswal, S., Marchi, E., Lalanne, D., Cowie, R., &"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Pantic, M. (2015). Av+ec 2015: The ﬁrst aﬀect recognition challenge bridging across audio,"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "video, and physiological data.\nIn Proc of\nthe 5th Intl Workshop on Audio/Visual Emotion"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Challenge AVEC ’15 (p. 3–8). doi:10.1145/2808196.2811642."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Shan, C., Gong, S., & McOwan, P. W. (2009). Facial expression recognition based on local"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "binary patterns: A comprehensive study.\nImage and Vision Computing, 27 , 803–816."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Simonyan, K., & Zisserman, A. (2014). Two-stream convolutional networks for action recog-"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "nition in videos.\nIn Advances in Neural\nInformation Processing Systems 27: Annual Con-"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "ference on Neural Information Processing Systems\n(pp. 568–576)."
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image"
        },
        {
          "Oliveira, L. E. S., Mansano, M., Koerich, A. L., & de Souza Britto, A.\n(2011).\n2d princi-": "recognition.\nIn 3rd Intl Conf on Learning Representations, (ICLR)."
        }
      ],
      "page": 31
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Report TR 2010-001 U. Toronto."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Tannugi, D. L., Britto Jr., A. S., & Koerich, A. L. (2019). Memory integrity of CNNs for cross-"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "dataset facial expression recognition.\nIn IEEE Intl Conf on Systems, Man, and Cybernetics"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "(SMC) (pp. 3806–3811)."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Tran, D., Bourdev, L. D., Fergus, R., Torresani, L., & Paluri, M. (2015). Learning spatiotem-"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "poral\nfeatures with 3D convolutional networks.\nIn IEEE Intl Conf on Computer Vision,"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "(ICCV) (pp. 4489–4497). doi:10.1109/ICCV.2015.510."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Vielzeuf, V., Pateux, S., & Jurie, F. (2017). Temporal multimodal\nfusion for video emotion"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "classiﬁcation in the wild.\nIn 19th ACM Intl Conf on Multimodal\nInteraction (ICMI)\n(pp."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "569–576). doi:10.1145/3136755.3143011."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Wan, L., Liu, N., Huo, H., & Fang, T.\n(2017).\nFace recognition with convolutional neural"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "networks and subspace learning. In 2nd Intl Conf on Image, Vision and Computing (ICIVC)"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "(pp. 228–233)."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Wang, K., Peng, X., Yang, J., Meng, D., & Qiao, Y. (2020). Region attention networks for"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "pose and occlusion robust facial expression recognition.\nIEEE Trans.\nImage Process., 29 ,"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "4057–4069. doi:10.1109/TIP.2019.2956143."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Wang, Y., See, J., Phan, R. C., & Oh, Y. (2014). LBP with six intersection points: Reducing"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "redundant information in LBP-TOP for micro-expression recognition.\nIn 12th Asian Conf"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "on Computer Vision (pp. 525–537).\nSpringer volume 9003 of Lecture Notes\nin Computer"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Science. doi:10.1007/978-3-319-16865-4\\_34."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Warr, P., Bindl, U., Parker, S., & Inceoglu,\nI.\n(2014).\nFour-quadrant\ninvestigation of\njob-"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "related aﬀects and behaviours. European Journal of Work and Organizational Psychology,"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "23 , 342–363."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Xu, C., Cetintas, S., Lee, K.-C., & Li, L.-J.\n(2014). Visual\nsentiment prediction with deep"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "convolutional neural networks. arXiv:1411.5731."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Yu, F., & Koltun, V. (2016). Multi-scale context aggregation by dilated convolutions.\nIn 4th"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Intl Conf on Learning Representations (ICLR)."
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "Zavaschi, T. H. H., Britto Jr., A. S., Oliveira, L. E. S., & Koerich, A. L. (2013). Fusion of fea-"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "ture sets and classiﬁers for facial expression recognition. Expert Systems with Applications,"
        },
        {
          "Susskind, J. M., Anderson, A. K., & Hinton, G. E. (2010). The Toronto face dataset. Technical": "40 , 646–655."
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "using ensemble of classiﬁers.\nIn 2011 IEEE International Conference on Acoustics, Speech"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "and Signal Processing (ICASSP) (pp. 1489–1492). doi:10.1109/ICASSP.2011.5946775."
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "Zhang, K., Zhang, Z., Li, Z., & Qiao, Y.\n(2016).\nJoint\nface detection and alignment using"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "multitask cascaded convolutional networks. IEEE Signal Processing Letters, 23 , 1499–1503."
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "Zhang, Z., Luo, P., Loy, C. C., & Tang, X. (2015). Learning social relation traits from face"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "images.\nIn IEEE Intl Conf on Computer Vision (ICCV)\n(pp. 3631–3639).\ndoi:10.1109/"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "ICCV.2015.414."
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "Zhao, G., & Pietikainen, M. (2007). Dynamic texture recognition using local binary patterns"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "with an application to facial\nexpressions.\nIEEE Transactions on Pattern Analysis and"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "Machine Intelligence, 29 , 915–928. doi:10.1109/TPAMI.2007.1110."
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "Zhao, J., Li, R., Liang, J., Chen, S., & Jin, Q. (2019). Adversarial domain adaption for multi-"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "cultural dimensional emotion recognition in dyadic interactions. In 9th Intl on Audio/Visual"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "Emotion Challenge and Workshop AVEC ’19 (p. 37–45). New York, NY, USA. doi:10.1145/"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "3347320.3357692."
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "Zhao, J., Mao, X., & Zhang, J.\n(2018).\nLearning deep facial expression features\nfrom im-"
        },
        {
          "Zavaschi, T. H. H., Koerich, A. L., & Oliveira, L. E. S. (2011). Facial expression recognition": "age and optical ﬂow sequences using 3D CNN. The Visual Computer , 34 . doi:10.1007/"
        }
      ],
      "page": 33
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Using synthetic data to improve facial expression analysis with 3D convolutional networks",
      "authors": [
        "I Abbasnejad",
        "S Sridharan",
        "D Nguyen",
        "S Denman",
        "C Fookes",
        "S Lucey"
      ],
      "year": "2017",
      "venue": "IEEE Intl Conf on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "2",
      "title": "Temporal stochastic softmax for 3d cnns: An application in facial expression recognition",
      "authors": [
        "T Ayral",
        "M Pedersoli",
        "S Bacon",
        "E Granger"
      ],
      "year": "2021",
      "venue": "Temporal stochastic softmax for 3d cnns: An application in facial expression recognition"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in the wild from videos using images",
      "authors": [
        "S Bargal",
        "E Barsoum",
        "C Ferrer",
        "C Zhang"
      ],
      "year": "2016",
      "venue": "Intl Conf on Multimodal Interaction ICMI '16",
      "doi": "10.1145/2993148.2997627"
    },
    {
      "citation_id": "4",
      "title": "Developing crossmodal expression recognition based on a deep neural model",
      "authors": [
        "P Barros",
        "S Wermter"
      ],
      "year": "2016",
      "venue": "Adaptive Behavior",
      "doi": "10.1177/1059712316664017"
    },
    {
      "citation_id": "5",
      "title": "Emotion classification in arousal-valence dimension using discrete affective keywords tagging",
      "authors": [
        "W Ben Henia",
        "Z Lachiri"
      ],
      "year": "2017",
      "venue": "Intl Conf on Engineering MIS (ICEMIS)"
    },
    {
      "citation_id": "6",
      "title": "Diving deep into sentiment: Understanding fine-tuned CNNs for visual sentiment prediction",
      "authors": [
        "V Campos",
        "A Salvador",
        "X Giro-Inieto",
        "B Jou"
      ],
      "year": "2015",
      "venue": "1st Intl Workshop on Affect and Sentiment in Multimedia"
    },
    {
      "citation_id": "7",
      "title": "ETS system for AV+EC 2015 challenge",
      "authors": [
        "P Cardinal",
        "N Dehak",
        "A Koerich",
        "J Alam",
        "P Boucher"
      ],
      "year": "2015",
      "venue": "ACM Multimedia Conference"
    },
    {
      "citation_id": "8",
      "title": "A deep multiscale spatiotemporal network for assessing depression from facial dynamics",
      "authors": [
        "W Carneiro De Melo",
        "E Granger",
        "A Hadid"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.3021755"
    },
    {
      "citation_id": "9",
      "title": "Quo vadis, action recognition? A new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "IEEE Conf on Computer Vision and Pattern Recognition, CVPR",
      "doi": "10.1109/CVPR.2017.502"
    },
    {
      "citation_id": "10",
      "title": "Efficient spatial temporal convolutional features for audiovisual continuous affect recognition",
      "authors": [
        "H Chen",
        "Y Deng",
        "S Cheng",
        "Y Wang",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2019",
      "venue": "th Intl on Audio/Visual Emotion Challenge and Workshop AVEC '19",
      "doi": "10.1145/3347320.3357690"
    },
    {
      "citation_id": "11",
      "title": "Facial expression recognition using a pairwise feature selection and classification approach",
      "authors": [
        "M Cossetin",
        "J Nievola",
        "A Koerich"
      ],
      "year": "2016",
      "venue": "IEEE International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "12",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "IEEE Intl Conf on Computer Vision Workshops (ICCV Workshops)"
    },
    {
      "citation_id": "13",
      "title": "Emotiw 2018: Audio-video, student engagement and group-level affect prediction",
      "authors": [
        "A Dhall",
        "A Kaur",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2018",
      "venue": "Intl Conf on Multimodal Interaction (ICMI)",
      "doi": "10.1145/3242969.3264993"
    },
    {
      "citation_id": "14",
      "title": "Temporal 3d convnets: New architecture and transfer learning for video classification",
      "authors": [
        "A Diba",
        "M Fayyaz",
        "V Sharma",
        "A Karami",
        "M Arzani",
        "R Yousefzadeh",
        "L Gool"
      ],
      "year": "2017",
      "venue": "Temporal 3d convnets: New architecture and transfer learning for video classification",
      "arxiv": "arXiv:1711.08200"
    },
    {
      "citation_id": "15",
      "title": "Facenet2expnet: Regularizing a deep face recognition net for expression recognition",
      "authors": [
        "H Ding",
        "S Zhou",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "12th IEEE Intl Conf on Automatic Face & Gesture Recognition",
      "doi": "10.1109/FG.2017.23"
    },
    {
      "citation_id": "16",
      "title": "Audio and face video emotion recognition in the wild using deep neural networks and small datasets",
      "authors": [
        "W Ding",
        "M Xu",
        "D.-Y Huang",
        "W Lin",
        "M Dong",
        "X Yu",
        "H Li"
      ],
      "year": "2016",
      "venue": "18th ACM Intl Conf on Multimodal Interaction ICMI '16",
      "doi": "10.1145/2993148.2997637"
    },
    {
      "citation_id": "17",
      "title": "Strong evidence for universals in facial expressions: a reply to russell's mistaken critique",
      "authors": [
        "P Ekman"
      ],
      "year": "1994",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "18",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "Video-based emotion recognition using CNN-RNN and C3D hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Intl Conf on Multimodal Interaction ICMI '16",
      "doi": "10.1145/2993148.2997632"
    },
    {
      "citation_id": "20",
      "title": "Time perception and dynamics of facial expressions of emotions",
      "authors": [
        "S Fayolle",
        "S Droit-Volet"
      ],
      "year": "2014",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0097944"
    },
    {
      "citation_id": "21",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/access.2019.2917266"
    },
    {
      "citation_id": "22",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "C Zhang",
        "Y Bengio"
      ],
      "year": "2013",
      "venue": "20th Intl Conf Neural Information Processing",
      "doi": "10.1007/978-3-642-42051-1_16"
    },
    {
      "citation_id": "23",
      "title": "Deep neural networks with relativity learning for facial expression recognition",
      "authors": [
        "Y Guo",
        "D Tao",
        "J Yu",
        "H Xiong",
        "Y Li",
        "D Tao"
      ],
      "year": "2016",
      "venue": "IEEE Intl Conf on Multimedia Expo Workshops (ICMEW)"
    },
    {
      "citation_id": "24",
      "title": "Facial expressions of emotion are not culturally universal",
      "authors": [
        "R Jack",
        "O Garrod",
        "H Yu",
        "R Caldara",
        "P Schyns"
      ],
      "year": "2012",
      "venue": "Proc of the National Academy of Sciences"
    },
    {
      "citation_id": "25",
      "title": "3D convolutional neural networks for human action recognition",
      "authors": [
        "S Ji",
        "W Xu",
        "M Yang",
        "K Yu"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Expression empowered residen network for facial action unit detection",
      "authors": [
        "S Jyoti",
        "G Sharma",
        "A Dhall"
      ],
      "year": "2019",
      "venue": "14th IEEE Intl Conf on Automatic Face & Gesture Recognition",
      "doi": "10.1109/FG.2019.8756580"
    },
    {
      "citation_id": "27",
      "title": "Video-based emotion recognition in the wild using deep transfer learning and score fusion",
      "authors": [
        "H Kaya",
        "F Gürpınar",
        "A Salah"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing",
      "doi": "10.1016/j.imavis.2017.01.012"
    },
    {
      "citation_id": "28",
      "title": "Fusing aligned and non-aligned face information for automatic affect recognition in the wild: A deep learning approach",
      "authors": [
        "B Kim",
        "S Dong",
        "J Roh",
        "G Kim",
        "S Lee"
      ],
      "year": "2016",
      "venue": "IEEE Conf on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "29",
      "title": "Hierarchical committee of deep CNNs with exponentially-weighted decision fusion for static facial expression recognition",
      "authors": [
        "B.-K Kim",
        "H Lee",
        "J Roh",
        "S.-Y Lee"
      ],
      "year": "2015",
      "venue": "Intl Conf on Multimodal Interaction ICMI '15",
      "doi": "10.1145/2818346.2830590"
    },
    {
      "citation_id": "30",
      "title": "Building emotional machines: Recognizing image emotions through deep neural networks",
      "authors": [
        "H Kim",
        "Y Kim",
        "S Kim",
        "I Lee"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Multim",
      "doi": "10.1109/TMM.2018.2827782"
    },
    {
      "citation_id": "31",
      "title": "Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video",
      "authors": [
        "B Knyazev",
        "R Shvetsov",
        "N Efremova",
        "A Kuharenko"
      ],
      "year": "2017",
      "venue": "Convolutional neural networks pretrained on large face recognition datasets for emotion classification from video",
      "arxiv": "arXiv:1711.04598"
    },
    {
      "citation_id": "32",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-019-01158-4"
    },
    {
      "citation_id": "33",
      "title": "A multi-component CNN-RNN approach for dimensional emotion recognition in-the-wild",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-component CNN-RNN approach for dimensional emotion recognition in-the-wild"
    },
    {
      "citation_id": "34",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "B Schuller",
        "K Star",
        "E Hajiyev",
        "M Pantic",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/tpami.2019.2944808"
    },
    {
      "citation_id": "35",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Commun. ACM",
      "doi": "10.1145/3065386"
    },
    {
      "citation_id": "36",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2018",
      "venue": "Deep facial expression recognition: A survey"
    },
    {
      "citation_id": "37",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "38",
      "title": "A deeper look at facial expression dataset bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/taffc.2020.2973158"
    },
    {
      "citation_id": "39",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "IEEE Conf on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "40",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "IEEE Conf on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "41",
      "title": "Occlusion aware facial expression recognition using CNN with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "42",
      "title": "T-C3D: temporal convolutional 3d network for real-time action recognition",
      "authors": [
        "K Liu",
        "W Liu",
        "C Gan",
        "M Tan",
        "H Ma"
      ],
      "year": "2018",
      "venue": "32nd AAAI Conf on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Adaptive deep metric learning for identity-aware facial expression recognition",
      "authors": [
        "X Liu",
        "B Kumar",
        "J You",
        "P Jia"
      ],
      "year": "2017",
      "venue": "IEEE Conf on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "44",
      "title": "Double transfer learning for breast cancer histopathologic image classification",
      "authors": [
        "J De Matos",
        "A Britto",
        "L Oliveira",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "IEEE Intl Joint Conf on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "45",
      "title": "Group-level arousal and valence recognition in static images: Face, body and context",
      "authors": [
        "W Mou",
        "O Celiktutan",
        "H Gunes"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE Intl Conf and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "46",
      "title": "Deep spatio-temporal features for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "A Ghasemi",
        "D Dean",
        "C Fookes"
      ],
      "year": "2017",
      "venue": "2017 IEEE Winter Conf on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "47",
      "title": "2d principal component analysis for face and facial-expression recognition",
      "authors": [
        "L Oliveira",
        "M Mansano",
        "A Koerich",
        "De Souza",
        "A Britto"
      ],
      "year": "2011",
      "venue": "Computing in Science Engineering",
      "doi": "10.1109/MCSE.2010.149"
    },
    {
      "citation_id": "48",
      "title": "Emotion recognition using fusion of audio and video features",
      "authors": [
        "J Ortega",
        "P Cardinal",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "IEEE Intl Conf on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "49",
      "title": "Audio-visual emotion recognition using deep transfer learning and multiple temporal models",
      "authors": [
        "X Ouyang",
        "S Kawaai",
        "E Goh",
        "S Shen",
        "W Ding",
        "H Ming",
        "D.-Y Huang"
      ],
      "year": "2017",
      "venue": "19th ACM Intl Conf on Multimodal Interaction",
      "doi": "10.1145/3136755.3143012"
    },
    {
      "citation_id": "50",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conf (BMVC)",
      "doi": "10.5244/C.29.41"
    },
    {
      "citation_id": "51",
      "title": "Facial Expression Recognition using Convolutional Neural Networks: State of the Art",
      "authors": [
        "C Pramerdorfer",
        "M Kampel"
      ],
      "year": "2016",
      "venue": "Facial Expression Recognition using Convolutional Neural Networks: State of the Art",
      "arxiv": "arXiv:1612.02903"
    },
    {
      "citation_id": "52",
      "title": "Deep da for ordinal regression of pain intensity estimation using weakly-labeled videos",
      "authors": [
        "G Praveen",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2020",
      "venue": "Deep da for ordinal regression of pain intensity estimation using weakly-labeled videos",
      "arxiv": "arXiv:2010.15675"
    },
    {
      "citation_id": "53",
      "title": "Deep weakly-supervised domain adaptation for pain localization in videos",
      "authors": [
        "G Praveen",
        "E Granger",
        "P Cardinal"
      ],
      "year": "2020",
      "venue": "FG"
    },
    {
      "citation_id": "54",
      "title": "Av+ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "S Jaiswal",
        "E Marchi",
        "D Lalanne",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "Proc of the 5th Intl Workshop on Audio/Visual Emotion Challenge AVEC '15",
      "doi": "10.1145/2808196.2811642"
    },
    {
      "citation_id": "55",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "56",
      "title": "Two-stream convolutional networks for action recognition in videos",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "3rd Intl Conf on Learning Representations, (ICLR)"
    },
    {
      "citation_id": "58",
      "title": "The Toronto face dataset",
      "authors": [
        "J Susskind",
        "A Anderson",
        "G Hinton"
      ],
      "year": "2010",
      "venue": "The Toronto face dataset"
    },
    {
      "citation_id": "59",
      "title": "Memory integrity of CNNs for crossdataset facial expression recognition",
      "authors": [
        "D Tannugi",
        "A Britto",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "IEEE Intl Conf on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "60",
      "title": "Learning spatiotemporal features with 3D convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "IEEE Intl Conf on Computer Vision, (ICCV)",
      "doi": "10.1109/ICCV.2015.510"
    },
    {
      "citation_id": "61",
      "title": "Temporal multimodal fusion for video emotion classification in the wild",
      "authors": [
        "V Vielzeuf",
        "S Pateux",
        "F Jurie"
      ],
      "year": "2017",
      "venue": "19th ACM Intl Conf on Multimodal Interaction (ICMI)",
      "doi": "10.1145/3136755.3143011"
    },
    {
      "citation_id": "62",
      "title": "Face recognition with convolutional neural networks and subspace learning",
      "authors": [
        "L Wan",
        "N Liu",
        "H Huo",
        "T Fang"
      ],
      "year": "2017",
      "venue": "2nd Intl Conf on Image, Vision and Computing (ICIVC)"
    },
    {
      "citation_id": "63",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Image Process",
      "doi": "10.1109/TIP.2019.2956143"
    },
    {
      "citation_id": "64",
      "title": "LBP with six intersection points: Reducing redundant information in LBP-TOP for micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R Phan",
        "Y Oh"
      ],
      "year": "2014",
      "venue": "12th Asian Conf on Computer Vision",
      "doi": "10.1007/978-3-319-16865-4_34"
    },
    {
      "citation_id": "65",
      "title": "Four-quadrant investigation of jobrelated affects and behaviours",
      "authors": [
        "P Warr",
        "U Bindl",
        "S Parker",
        "I Inceoglu"
      ],
      "year": "2014",
      "venue": "European Journal of Work and Organizational Psychology"
    },
    {
      "citation_id": "66",
      "title": "Visual sentiment prediction with deep convolutional neural networks",
      "authors": [
        "C Xu",
        "S Cetintas",
        "K.-C Lee",
        "L.-J Li"
      ],
      "year": "2014",
      "venue": "Visual sentiment prediction with deep convolutional neural networks",
      "arxiv": "arXiv:1411.5731"
    },
    {
      "citation_id": "67",
      "title": "Multi-scale context aggregation by dilated convolutions",
      "authors": [
        "F Yu",
        "V Koltun"
      ],
      "year": "2016",
      "venue": "4th Intl Conf on Learning Representations (ICLR)"
    },
    {
      "citation_id": "68",
      "title": "Fusion of feature sets and classifiers for facial expression recognition",
      "authors": [
        "T Zavaschi",
        "A Britto",
        "L Oliveira",
        "A Koerich"
      ],
      "year": "2013",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "69",
      "title": "Facial expression recognition using ensemble of classifiers",
      "authors": [
        "T Zavaschi",
        "A Koerich",
        "L Oliveira"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2011.5946775"
    },
    {
      "citation_id": "70",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "71",
      "title": "Learning social relation traits from face images",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2015",
      "venue": "IEEE Intl Conf on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.414"
    },
    {
      "citation_id": "72",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2007.1110"
    },
    {
      "citation_id": "73",
      "title": "Adversarial domain adaption for multicultural dimensional emotion recognition in dyadic interactions",
      "authors": [
        "J Zhao",
        "R Li",
        "J Liang",
        "S Chen",
        "Q Jin"
      ],
      "year": "2019",
      "venue": "th Intl on Audio/Visual Emotion Challenge and Workshop AVEC '19",
      "doi": "10.1145/3347320.3357692"
    },
    {
      "citation_id": "74",
      "title": "Learning deep facial expression features from image and optical flow sequences using 3D CNN. The Visual Computer",
      "authors": [
        "J Zhao",
        "X Mao",
        "J Zhang"
      ],
      "year": "2018",
      "venue": "Learning deep facial expression features from image and optical flow sequences using 3D CNN. The Visual Computer",
      "doi": "10.1007/s00371-018-1477-y"
    }
  ]
}