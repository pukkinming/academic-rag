{
  "paper_id": "2504.18673v1",
  "title": "Can Third-Parties Read Our Emotions?",
  "published": "2025-04-25T19:52:21Z",
  "authors": [
    "Jiayi Li",
    "Yingfan Zhou",
    "Pranav Narayanan Venkit",
    "Halima Binte Islam",
    "Sneha Arya",
    "Shomir Wilson",
    "Sarah Rajtmajer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Natural Language Processing tasks that aim to infer an author's private states, e.g., emotions and opinions, from their written text, typically rely on datasets annotated by thirdparty annotators. However, the assumption that third-party annotators can accurately capture authors' private states remains largely unexamined. In this study, we present human subjects experiments on emotion recognition tasks that directly compare third-party annotations with first-party (author-provided) emotion labels. Our findings reveal significant limitations in third-party annotations-whether provided by human annotators or large language models (LLMs)-in faithfully representing authors' private states. However, LLMs outperform human annotators nearly across the board. We further explore methods to improve third-party annotation quality. We find that demographic similarity between first-party authors and thirdparty human annotators enhances annotation performance. While incorporating first-party demographic information into prompts leads to a marginal but statistically significant improvement in LLMs' performance. We introduce a framework for evaluating the limitations of third-party annotations and call for refined annotation practices to accurately represent and model authors' private states.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recognizing and interpreting subjective language used to express private states-users' internal experiences, e.g., opinions, emotions, evaluations, and speculations  (Quirk et al., 1985) -has been a long-standing area of study in natural language processing (NLP)  (Wiebe et al., 2004; Banfield, 1982) . Examples of private state inference tasks include emotion classification  (Mohammad et al., 2018; Demszky et al., 2020) , emotion intensity detection  (Mohammad and Bravo-Marquez, 2017) , sarcasm detection  (Bamman and Smith, 2015; Oprea and Magdy, 2019) , sentiment analysis  (Nemes and Kiss, 2021) , political ideology detection  (Iyyer et al., 2014) , stance detection  (AlDayel and Magdy, 2021) , and many more. Approaches for these tasks have relied heavily on 'gold standard' datasets annotated by third-party annotators. The standard annotation process typically involves multiple human annotators, often recruited through crowdsourcing platforms or drawn from internal research teams  (Rashtchian et al., 2010; Snow et al., 2008) . More recently, large language models (LLMs) have been explored as scalable and cost-effective alternatives or complements to human annotators  (Li et al., 2023) . Studies have shown that LLMs can match or surpass human annotators on some annotation tasks  (Hasanain et al., 2024; Gilardi et al., 2023) .\n\nWhile the adoption of third-party annotations is appropriate for many NLP tasks such as those with objective ground truth (e.g., named entity recognition  (Nadeau and Sekine, 2009) , part-of-speech tagging  (Brill, 1994) ) or for tasks that benefit from diverse third-party perception (e.g., toxicity detection  (Pavlopoulos et al., 2020) , hate speech detection  (Davidson et al., 2017 )), we suggest that thirdparty annotations (human or machine) have inherent limitations for tasks that seek to model an author's private state. Fundamentally, the use of third-party annotations assumes that an author's private state can be identified from their writing by human or machine annotators. However, subjective language often lacks explicit linguistic cues  (Wiebe et al., 2005; Balahur et al., 2012) , leaving annotators to perform inference on textual cues, which may be implicit, ambiguous, or context-dependent. This challenge is compounded by individual differences in authors' expression of private states  (DeAndrea et al., 2010; Bauer et al., 2003)  as well as annotators' socio-demographic, cultural backgrounds, and personal beliefs which shape how they interpret authors' text  (Shen and Rose, 2021; Ding et al., 2022; Oprea and Magdy, 2019) .\n\nMisalignment between an author's private state and its interpretation by third-party annotators is not merely a labeling error-it can propagate through learned models and compromise the reliability of downstream applications. However, little research has systematically examined this gap.\n\nTo this end, we conduct a series of studies to investigate the alignment between third-party annotations and first-party labels, i.e., authors' selfreported private states, in the context of emotion recognition tasks. Emotion recognition has a wide range of applications, especially in high-stakes contexts such as moderating online content, detecting deception, and powering therapeutic chatbots  (Shaw and Lyons, 2017; Chiril et al., 2022; Zygadło et al., 2021) . In these contexts, misinterpretations of users' emotions not only render the technology deficient but also make it socially pernicious. Moreover, emotion recognition is closely related to other tasks that aim to infer private states, such as mental health detection and sentiment analysis  (Zhang et al., 2023; Venkit et al., 2023 ) so that we anticipate lessons learned in this domain will inform the next steps in others. Our work is guided by the research questions: RQ1: How do third-party human annotators and LLMs align with first-party labels, i.e., authors' self-reports, on emotion recognition tasks? RQ2: Does demographic similarity between human annotators and authors improve alignment between third-party annotations and first-party labels? RQ3: Does including authors' demographic information within prompts improve LLM annotations?\n\nTo explore these questions, we recruit social media users to share their own social media posts and label them with their own emotion labels (first party). We then collect annotations of these same posts from third-party human annotators and LLMs and compare their performance. We investigate the impact of demographic similarity between thirdparty human annotators and first-party authors; similarly, we explore whether including first-party demographic information within prompts improves LLM annotations.\n\nOur results reveal notable misalignment between first-party labels and third-party annotations, as both human annotators and LLMs achieve low to fair Cohen's kappa scores and F1 scores across emotions. LLMs perform better than human annotators universally (RQ1). In-group human annotators' performance is significantly better than out-group annotators, suggesting that demographic similarity between third-party human annotators and first-party individuals improves annotation performance (RQ2). We further observe that adding first-party information in prompts marginally improves LLMs' annotations (RQ3).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "2.1 Private State Annotations for NLP Emotions, sentiment, opinions, evaluations, and speculations are fundamental aspects of an individual's internal world, collectively referred to as private states  (Quirk et al., 1985) . Many NLP tasks aim to identify an author's private states from their texts. Labeled datasets are needed to train and test models to perform these tasks. Human-annotated datasets. For emotion recognition,  (Mohammad et al., 2018)  introduce a set of subtasks, such as emotion classification and emotion intensity classification, that aim to infer the effectual state of a person from their tweets and provide a human-annotated dataset for each of the subtasks.  (Aman and Szpakowicz, 2007)  introduce a corpus that consists of blog posts with human annotations of emotion on sentence level.  (Demszky et al., 2020)  build a dataset of Reddit posts labeled by third-party human annotators based on a fine-grained emotion taxonomy. These datasets have been widely adopted for model building. LLM-annotated datasets. Given LLMs' demonstrated zero-shot and few-shot capabilities for various NLP tasks  (Brown et al., 2020) , researchers have increasingly explored their use to augment or replace human annotators  (Gilardi et al., 2023; Kim et al., 2024) . Studies have highlighted the potential of LLMs as annotators even for subjective tasks that model authors' private states, such as emotion recognition  (Niu et al., 2024)  and stance detection  (Li and Conrad, 2024) . Limitations of third-party annotations. Limited research has systematically investigated the limitations of third-party annotations for private states.  Oprea et al. (Oprea and Magdy, 2019 ) examined the differences between intended sarcasm (as labeled by the author) and perceived sarcasm (as labeled by third-party annotators). While sarcasm detection can benefit from explicit linguistic cues, such as irony and exaggeration, understanding private states often relies on implicit signals and is subject to individual interpretation.\n\nAnother relevant study  (Joseph et al., 2021)  explored the misalignment between third-party in-ferences of users' political stances and users' responses in public opinion surveys. Rather than directly comparing third-party judgments to an author's self-reported private state, authors focused on the discrepancy between the publicly expressed stance-as inferred from social media posts-and the stance measured by public opinion polls.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Emotion recognition, equivalently emotion classification, is a widely studied task in NLP that aims to identify the emotions expressed by an author based on their written content  (Alswaidan and Menai, 2020) . Several third-party-annotated datasets have been developed for emotion classification across different domains and applications  (Liu et al., 2019; Li et al., 2017; Buechel and Hahn, 2017) . Among these, datasets such as those by  (Mohammad et al., 2018)  and  (Demszky et al., 2020)  are specifically based on social media posts.\n\nTraditional approaches to emotion classification often rely on taxonomies such as Ekman's six basic emotions  (Ekman, 1992)  or Plutchik's wheel of emotions  (Plutchik, 2001) . Recently, more nuanced and comprehensive representations have emerged. For example,  (Demszky et al., 2020)  introduced a fine-grained emotion taxonomy that includes 27 emotions, along with a neutral category. We adopt this taxonomy in our study.\n\nPrior psychological studies have shown that the expression and interpretation of emotions can be influenced by cultural and social factors in different ways  (Lim, 2016; Gendron et al., 2014; Mill et al., 2009; Barrett, 2004) . Individuals from the same demographic background-such as age, gender, or cultural identity-tend to exhibit greater alignment in emotion expression and interpretation  (Elfenbein and Ambady, 2002b; Sauter et al., 2010; Elfenbein and Ambady, 2002a) . Building on these findings, our study investigates whether human annotators who share demographic traits with an author perform better in emotion recognition tasks compared to those who do not.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Incorporating Social Factors In Llms",
      "text": "The importance of modeling and incorporating social factors in NLP tasks has been increasingly emphasized.  (Hovy and Yang, 2021)  have argued that understanding factors such as the speaker's and receiver's background information will ultimately be necessary if NLP models are to ever achieve humanlevel performance. Several studies have examined LLMs' ability to capture and align with the communication styles, perspectives, and preferences of specific demographic groups or even individuals  (Hwang et al., 2023; Mukherjee et al., 2024) .  (Beck et al., 2024)  examined socio-demographic prompting, where socio-demographic information is incorporated into prompts to guide an LLM into adopting the perspective of a certain group or user, and found potential in this technique. In contrast,  (Sun et al., 2023)  observed that demographic-infused prompts fail to mitigate gender and racial biases in LLMs' annotations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "To investigate the alignment between first-party labels and third-party annotations in emotion recognition, we adopt the emotion recognition task from  (Demszky et al., 2020)  and conduct human subject experiments. Data collection occurred in two stages: (1) collection of first-party social media posts with self-reported emotion labels; and (2) collection of third-party annotations for these posts.\n\nWe analyze annotation performance using Cohen's kappa, F1 score, Recall, and precision. We apply statistical analyses, including mixed linear models and the Wilcoxon signed-rank test, to further assess the impact of demographic similarity on third-party annotation performance. Additionally, we analyze patterns of misalignment to understand the role of linguistic cues in emotion recognition. All human subjects research was approved by an Institutional Review Board.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Taxonomy",
      "text": "We adopt the fine-grained emotion taxonomy introduced by  Demszky et al. (2020) , which includes 27 distinct emotions plus \"neutral\". The taxonomy provides definitions for each emotion, along with an associated emoji where applicable. The full taxonomy is provided in Appendix A. To evaluate the alignment between third-party annotations and first-party labels at a coarser level, we group the 28 emotion categories into seven groups: joy, love, anger, surprise, fear, sadness, and neutral. As there is no universally agreed-upon set of basic emotions, our grouping is based on the union of basic emotion categories proposed by  Ekman (1992)    (anger, disgust, fear, joy, neutral, sadness, surprise)  and  Shaver et al. (1987) (anger, love, fear, joy, sadness, and surprise) . Analysis based on this aggregated emotion taxonomy is presented the Appendix H.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "First-Party Data Collection",
      "text": "We recruited social media users through Connect 1  , a crowdsourcing platform designed to facilitate research participation by connecting researchers with diverse and high-quality participant pools  (Douglas et al., 2023; Eyal et al., 2021) . We recruited participants in the United States from three age groups  (18-27, 28-43, and 44-59) , two gender categories (female and male), and three racial groups (Black, White, and Asian). We attempted to include additional groups, e.g., non-binary and Hispanic individuals, but they had insufficient representation on Connect. Our intersectional recruitment strategy aimed to achieve balanced representation across demographics. Specifically, we aimed to obtain approximately 10 responses for each combination of age, gender, and race. While we successfully reached most groups, some intersectional groups-such as Asian participants aged 44-59-proved challenging due to their underrepresentation on the platform. A full breakdown of the number of participants by age, gender, and race is offered in Table  4  of Appendix B.\n\nVia online survey, participants were asked to provide basic demographic information and to name one or two social media platforms where they post most frequently. They were then asked to upload a minimum of 5 (maximum of 15) posts they had authored on the selected social media platform(s) in the past 12 months. Posts were submitted in the form of screenshots. For each uploaded post, participants were asked to review the content and select all emotions they believed were expressed in the post, using the list of 28 emotion categories, along with definitions. To maintain data quality and authenticity, posts were required to adhere to a number of criteria, related to language, multimedia, identifying information, and date of publication. These are detailed in Appendix C. Each participant was compensated $2.50 for completing the task, which took approximately 10-15 minutes.\n\nTable  4  in Appendix B presents the number of participants who provided valid responses and the total number of valid posts for each intersectional group. In total, we collected 729 posts from 123 participants; 44% of these contained only text and the remaining posts contained both text and images. A quality check was manually performed on these posts and their associated labels; details of this process are provided in Appendix D.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Third-Party Emotion Labels",
      "text": "Human annotations. We recruited human annotators through Connect. For each post, we assigned six annotators: three in-group annotators, who shared all three demographic traits (age group, gender, and racial group) with the author; and, three out-group annotators, who differed from the author on at least two of these traits.  2  The demographic distribution of in-group annotators mirrored that of first-party participants by design. We aimed to ensure a diverse and balanced sample of out-group annotators across different age groups, genders, and racial backgrounds. Figure  3  in Appendix B details the demographic distribution of out-group annotators.\n\nEach annotation task consisted of 5 to 10 posts, which were randomly assigned to annotators. Each annotator was compensated $2.00 for completing the task, which took approximately 8-10 minutes. Annotators were presented with the original screenshots of posts provided by their authors and asked to identify the any of emotions expressed by the author of the post (first-party) using the 27 emotion categories plus \"neutral\" (if none). The list was accompanied by clear definitions of each category. In cases where annotators could not confidently assign any labels from the list, they were asked to select \"unrecognizable\". We disregarded annotations that were self-contradictory, such as those which selected both \"neutral\" and one or more emotions from the list of 27 categories. We interpreted such self-contradictory annotations as indicative of a lack of attention. Furthermore, annotations from annotators who frequently made such errors were removed. Summary statistics of human annotations is presented in Appendix B Table  5 . LLM annotations. To complement human annotations, we used LLMs to generate third-party emotion labels. Specifically, we used GPT-4 Turbo (OpenAI, 2023),  GPT-4o (OpenAI, 2023) , Gemini 1.5 Pro  (Reid et al., 2024) , Gemini 1.5 Flash  (Reid et al., 2024) , and Claude 3.5 Sonnet  (Anthropic, 2024) . These models were chosen based on their strong performance on other NLP tasks and their ability to process multimodal content.\n\nEach post was independently annotated by each LLM. LLMs were provided the screenshots of social media posts submitted by study participants, along with instructions defining the 27 emotion categories plus \"neutral\" and \"unrecognizable\". The prompt mirrored the instructions given to human annotators to maintain consistency.\n\n4 Third-party Annotators' Performance",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Third-Party Annotators' Agreement",
      "text": "We estimated interrater agreement amongst human annotators and amongst LLMs, following the method used in  (Demszky et al., 2020) , i.e., calculating the Spearman correlation between each annotator's judgments and the mean judgments of other annotators, averaged across all posts labeled by the annotator. LLMs generally exhibit higher and more consistent interrater agreement compared to human annotators, but there is some variability in this with respect to positive vs. negative emotions. These results are further detailed in Appendix F.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "First-And Third-Party Alignment (Rq1)",
      "text": "We analyze the alignment between first-party labels and third-party annotations. First-party labels are treated as the gold standard. As mentioned earlier, for each post, we collected 6 annotations from human annotators (3 in-group; 3 out-group) and one annotation from each of 5 LLMs. Each annotation provided by a third-party annotator for a post consists of a binary label for each of the 27 emotions plus \"neutral\". We employed two sets of metrics:\n\n(1) Cohen's kappa and (2) classification metrics: F1, recall, and precision.\n\nTo assess alignment, we compute Cohen's kappa for both human and LLM annotations relative to the first-party labels. We aggregate annotations for each post using majority voting-applied separately to human annotators and to LLMs. For human annotations, ties can occur after removal of low-quality annotations. Both human annotators and LLMs exhibit low to fair alignment with firstparty labels; their Cohen's kappa scores range from 0 to 0.45. Details are provided in Appendix G.\n\nWe further evaluated first-and third-part alignment using F1 score, recall, and precision-standard metrics for multi-label classification tasks. The macro-average precision, recall, and F1-score for in-group annotators are (0.38, 0.29, 0.32); for outgroup annotators, they are (0.36, 0.24, 0.28). LLMs achieve (0.38, 0.50, 0.40)(see Figure  1 ). F1 scores for LLMs range from 0.2 to 0.6, while the F1 scores fall between 0.1 and 0.5 for human. Overall, LLMs outperform human annotators across most emotions. However, for grief, sadness, and curiosity, in-group human annotators demonstrate comparable or even superior performance. Realization, relief, and neutral exhibit consistently low F1 scores across all annotator types, suggesting that these emotions are particularly challenging to classify. We further examined the correlation between Cohen's kappa and F1 scores for both LLM and human annotations. In both cases, Cohen's kappa scores show a strong positive correlation with F1 scores (LLM: r = 0.918, p ≈ 5.86e -10 ; human: r = 0.855, p ≈ 6.85e -9 ). These findings high-light that third-party annotations fail to capture firstparty intended emotional expressions. We further present confusion matrices showing the alignment between third-party annotations and first-party labels in Appendix G, along with detailed analyses.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "In-Vs. Out-Group Annotators (Rq2)",
      "text": "To address RQ2, we compare the performance of in-and out-group annotators at both the post and annotator levels. Post-level comparison. Among 91% posts, both groups reached a majority decision for all 28 emotions. For each post, we computed F1, recall, and precision by comparing the majority-voted labels to the corresponding first-party labels for in-group and out-group. We further adopted Cohen's kappa to assess in-group and out-group's annotation performance. For each emotion, we computed Cohen's kappa based on posts where the majority of annotators in both groups reached a decision on that emotion. On average, 99% of posts were included for the calculation. We perform Wilcoxon signedrank tests to assess the statistical significance of difference between in-group and out-group performance measured by F1, recall, precision, and recall. Table  1  presents the results of Wilcoxon signedrank tests. Results indicate that the observed differences in performance of in-group and out-group annotations, measured based on F1, recall, precision, and Cohen's kappa, are statistically significant, allowing us to conclude in-group outperform out-group annotators in recognizing the emotions expressed by first-party authors. Further details are provided in Figure  15  and Figure  16  in Appendix. Annotator-level comparison While post-level analysis provides insights into aggregated group performance, annotator-level evaluation helps identify individual alignment with first-party labels. This approach accounts for individual annotator tendencies, which may not be apparent when only considering majority labels. To compare in-group and out-group performance at annotator-level, we persevered individual third-party annotations. For each annotation, we obtained F1, recall, and precision by comparing it to the corresponding firstparty labels. We averaged the F1 score, precision, and recall across annotations provided within each annotation task for each annotator.\n\nFigure  15  in Appendix presents the distribution of average F1 scores, recall, and precision for individual annotators per task, grouped by in-group and out-group membership. Across all three met-",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Metric",
      "text": "In rics (precision, recall, and F1), the in-group annotators' distributions appear shifted slightly to the right compared to the out-group annotators. The gap is most pronounced in recall and F1, whereas precision exhibits substantial overlap between the two groups. To formally test whether these differences are statistically significant, we employed linear mixed models to account for the nested structure of the data. Specifically, task ID and annotator ID were treated as random effects to account for variability across tasks and individual annotators.\n\nThe results of the mixed linear model, presented in Table  2 , indicate that in-group annotators outperform out-group annotators in terms of recall and F1 score. However, the difference in precision between the two groups is not statistically significant. These findings confirm that shared identity traits (age, gender, and race) enhance annotators' ability to align with first-party expressed emotions, particularly in achieving higher recall and overall performance (as measured by F1).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Human Annotators Vs. Llms",
      "text": "We compare the performance of human annotators (both in-group and out-group) with that of LLMs by analyzing F1, recall, and precision metrics on a perpost basis. We compute performance metrics based on the majority-voted labels for each post for ingroup, out-group, and LLMs. Comparison between LLMs and in-group is based on posts where the majority of in-group annotators reached a decision for all 28 emotions (94% of posts). between LLMs and out-group is based on posts where the majority of out-group annotators reached a decision for all 28 emotions (97% of posts).\n\nFor each emotion, we computed Cohen's kappa for in-group annotations using only posts where a majority of in-group annotators reached a decision (on average, 99% of posts). Similarly, Cohen's kappa for out-group annotations was calculated using posts where a majority of out-group annotators reached a decision (nearly all posts). Although LLMs provide decisions for all emotions on every post (since we aggregate responses from 5 LLMs), we restricted LLM evaluations to the same subsets of posts used for human annotators to allow direct comparison.\n\nWe employed the Wilcoxon signed-rank test to assess whether differences between in-group and out-group in terms of F1, recall, precision, Cohen's kappa are statistically significant. The results, in Table  3 , suggest that LLMs perform significantly better in terms of F1, recall, and Cohen's kappa. However, their advantage does not extend to precision, as indicated by the lack of statistical significance in the in-group comparison.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Demographic Prompting (Rq3)",
      "text": "In this section, we examine the impact of incorporating first-party demographic information-specifically age, gender, and race-into the prompt. By explicitly including demographic details of first-party, we investigate whether the model's predictions align more closely with firstparty emotion labels.\n\nFor each LLM, we generated an annotation for every post using demographic prompting. We applied majority voting across the annotations to derive a majority-voted label for each post. We then compared these majority-voted labels with those obtained without demographic information in the prompt. We adopted the Wilcoxon signed-rank test to evaluate whether incorporating demographic information in the prompt improved alignment be-tween the model's predictions and the first-party labels. The results indicate that demographic prompting results in a statistically significant difference in F1 score (p = 0.0095) and precision (p = 0.0004), while no significant difference is observed in recall (p = 0.9934). However, the similar median (F1 score: 0.4, precision: 0.333, recall: 0.667) and distribution presented in Figure  17  in Appendix, suggesting that while demographic information systematically impacts model predictions, the practical improvement in performance remains minimal.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Understanding Misalignment Between First-Party Labels And Third-Party Annotations",
      "text": "We conducted a qualitative analysis to explore how the linguistic and contextual characteristics of posts may contribute to annotation discrepancies. We examined both cases where third-party annotations closely aligned with first-party labels and cases where they diverged significantly, aiming to identify patterns in post content that may explain these differences. Specifically, we defined highalignment cases as posts where the majority-voted label received an F1-score greater than 0.6 and lowalignment cases as those with an F1-score below 0.2. These thresholds were determined based on the distribution of F1-scores across annotations. Content within these categories were manually reviewed by two authors.\n\nFor posts where third-party annotations (ingroup, out-group, and LLMs) achieved near-perfect F1 scores, emotional expression was conveyed through explicit and unambiguous language. For instance, posts categorized by the first party as expressing \"joy\" explicitly included the word \"happy,\" while expressions of \"gratitude\" featured phrases such as \"thank you,\" and instances of \"confusion\" were marked by direct questions. In posts where third-party annotations achieved moderate to high F1 scores, a lack of clear linguistic cues led to subtle discrepancies rather than complete mismatches between the emotions identified by the author and those recognized by third-party annotators. E.g., third-party annotators selected additional emotions that the author did not specify, while in others, certain emotions identified by the author were not acknowledged by annotators.\n\nAmong posts exhibiting significant misalignment, we observed three notable patterns among posts: (1) posts that lacked strong textual cues for emotion. For instance, a first-party (author) wrote: \"Introducing Toki! :) He's a ringneck dove.\" and labeled the post with love, excitement, and optimism.\n\n(2) A considerable number of posts originated from discussions on Reddit, where the lack of conversational context may have posed a challenge for accurate emotion classification, and (3) many instances involved authors self-reporting \"neutral\" despite the presence of discernible emotional cues, suggesting that the presence of emotional language does not necessarily reflect the author's internal emotional state. In addition, we observed that the majority of posts with first-party labels identified as spurious (illustrated in Appendix D) were found within this subset of misaligned posts, which is expected.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion And Discussion",
      "text": "Our work challenges the assumption that thirdparty annotations (both human and LLM-based) can reliably infer authors' private states from their texts. By demonstrating a significant misalignment between third-party annotations and firstparty (author-provided) labels, we show that thirdparty annotations fail to accurately capture authors' emotional expressions. We further explored methods to improve third-party annotation quality, leveraging first-party demographic information. We find that demographic similarity between first-party and third-party human annotators enhances annotation performance. Prompting first-party demographic traits marginally enhances LLMs' annotation performance.\n\nA simple sentence like \"I got a cup of coffee.\" can express different emotions depending on the speaker and the context, information which may or may not be transparent to a third party. Similarly, a statement like \"I love morning classes.\" could be interpreted as expressing joy (genuine appreciation) or annoyance (sarcasm or frustration). However, the important question here is: Are we modeling a third-party's perception of the emotion expressed by the author, or the actual emotion expressed by author in their written text?\n\nThird-party annotations struggle to differentiate between semantically similar emotions, such as joy and excitement, and to recognize the complexity of emotional expression-such as frustration underlying gratitude or anger coexisting with caring. This challenge highlights the inherent subjectivity in emotion recognition, where annotators bring their own interpretations, biases, and contextual assumptions to the task. Unlike first-party authors who experience the emotion firsthand, third-party annotators rely solely on textual cues, which may lack sufficient context for accurate inference. As a result, subtle emotional nuances, mixed emotions, or sarcasm often go unnoticed or misinterpreted. Moreover, third-party annotations may reflect individually, socially, culturally influenced perceptions of emotion rather than the actual emotional state intended by the author. For instance, an indirect expression of distress might not be recognized as sadness if it does not conform to expected linguistic patterns. This misalignment could undermine the reliability of models trained on third-party annotations and lead to unintended harm, particularly in high-stakes contexts, e.g., chatbots.\n\nWhile our study focuses on emotion recognition, the challenges we identify have implications for other NLP tasks that attempt to infer authors' private states from text. Given these challenges, we argue that it is crucial to critically evaluate the extent to which third-party annotations can serve as a reliable ground truth for modeling authors' private states. Future research should explore methods, such as incorporating first-party first-party feedback and explanation, to achieve a more truthful representation of the emotions actually expressed by authors.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Limitations",
      "text": "The observed limitations of third-party annotations are based on first-party labels and third-party annotations collected from study participants. Even though we implemented strict quality control for first-party posts, we still observed first-party labels that may be spurious. These first-party data were retained in the analysis as whether first-party labels faithfully represent first-party internal emotion can not be externally verifiable. These first-party data were retained in the analysis because whether first-party labels faithfully represent the author's in-ternal emotions cannot be externally verified. In addition, the inclusion of these data points is expected to have minimal impact on the overall findings.\n\nFor third-party annotations, we removed annotations that did not follow annotation instruction, indicating a lack of attention. However, despite these quality control measures, some low-quality annotations may still remain, as inconsistencies in subjective judgment cannot be entirely eliminated.\n\nIn addition, we lack sufficient data points within each demographic subgroup to conduct statistically robust analyses on how specific identity traits influence annotation performance. Furthermore, while this study examines differences between in-group and out-group annotators, our findings may not generalize to broader populations, cultures, or languages.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ethical Consideration",
      "text": "The intention of our work is to highlight critical ethical concerns in inferring authors' private states (specifically emotions) via third-party annotations. However, there are ethical considerations within the work that need to be considered for further research development. The key issues include privacy risks when handling sensitive emotional data, potential bias from demographic mismatches between authors and annotators, and the unreliability of third-party labels (human or LLM) in capturing genuine emotions, risking harmful misjudgments in real-world applications. While our results show that LLMs outperform humans, in certain scenarios, they may perpetuate training data biases and foster overconfidence in flawed systems. Our study also underscores transparency gaps in methodology and LLM decision-making, labor concerns around displacing human annotators, and risks of mishandling demographic data. It calls for diverse annotation teams, participatory design, rigorous consent protocols, and bias audits to ensure ethical practices in modeling private states.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B Demographics And Annotation Statistics",
      "text": "In this section, we present the demographic breakdown of first-party participants in Table  4  and the demographic breakdown of third-party annotators in Figure  3 . Additionally, Table  5  provides summary statistics of third-party annotations.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C Criteria And Verification For First-Party Posts",
      "text": "To maintain data quality and genuine responses, submitted posts were required to adhere to the follow criteria:(1) originally created by the participant  within the past 12 months (excluding shares, reposts, or non-original content); (2) in English and contain at least 5 words, excluding hashtags and URLs;\n\n(3) if multimedia is included, it should contain only images, with emotion conveyed through the images and text captured in the screenshot(s);\n\n(4) published at least 24 hours prior to submission;\n\n(5) fully captured in the screenshots. Additionally, participants were required to submit a screenshot of their social media account page, displaying their profile name, with the option to redact other information. The UI features of the account page helped us verify that participants had provided their own social media posts. We reviewed all submissions and removed posts that failed to meet one or more of the above criteria. We also excluded the entire response of participants whose submission authenticity could not be verified.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D Quality Check On First-Party Labels",
      "text": "Two authors separately reviewed each post and its corresponding first-party label to identify spurious labels-cases where the first-party label could not be reasonably justified by the post content. They labeled only clear mismatches between first-party labels and the emotional expression in the posts, rather than cases of subtler misalignment where there might be room for interpretation. They agreed on 89% of the posts. For posts where disagreements occurred, a senior author conducted a final review to resolve discrepancies. Among the 729 posts, 9.2% of first-party labels were flagged as spurious. , suggesting that the majority of first-party labels are reasonably aligned with the post content, indicating high quality. We retain posts identified as spurious in our analysis as a label flagged by us as spurious does not nec-essarily mean that it fails to accurately reflect the author's internal emotional state. We believe these labels are still valuable, as they highlight the complexity of emotional expression and the challenges of inferring emotions solely from textual content.",
      "page_start": 12,
      "page_end": 14
    },
    {
      "section_name": "E Statistical Overview Of First-Party And",
      "text": "Third-party Labels\n\nFirst-party labels Among all posts, 6.32% are labeled as Neutral, indicating no emotion is expressed. Of the remaining posts, 37.9% have a single emotion label, 25.3% have two labels, 14.6% have three labels, and 22.3% have four or more labels. Figure  5  shows the co-occurrence patterns of emotion labels, where the labels on the x-and y-axes are color-coded to represent distinct semantic categories: positive, negative, and ambiguous emotions. Positive emotions, such as joy, excitement, and admiration, occur more frequently overall compared to negative emotions like anger or disapproval. The heatmap reveals that emotions with similar conceptual or semantic tones often co-occur within the same post. For instance, positive emotions like joy, excitement, and admiration frequently appear together. Similarly, subsets of negative emotions, such as disappointment and annoyance, also exhibit notable co-occurrence. This pattern aligns with the observed tendency of social media users to predominantly share positive sentiments, while negative emotions appear less frequently  (Waterloo et al., 2018) . The co-occurrence of semantically related emotions suggests that users naturally cluster similar emotional tones in their posts, reflecting common patterns in emotional expression.\n\nThird-party annotations We applied majority voting separately to in-group annotations, out-group annotations, and LLM annotations. For in-group annotators, a majority decision was reached for all 28 emotions in 94% of posts, while for out-group annotators, a majority decision was reached for all 28 emotions in 97% of posts. In the case of LLMs, a majority label was assigned for all 28 emotions in every post, as we obtained annotations from 5 LLMs. 4.1% of posts were labeled as neutral by in-group annotators, 4.4% by out-group annotators, and 0.1% by LLMs. For each annotator group, for posts not labeled as neutral, we calculated the difference in the number of emotion labels selected by first-party participants (authors) and by third-party annotators.\n\nFigure  4  shows the distribution. The x-axis represents the difference in the number of emotion labels (calculated as first-party minus third-party), where negative values indicate over-labeling by third-party annotators, and positive values indicate under-labeling. LLMs exhibit a wider distribution with more over-labeling, while out-group annotators tend to align more closely with first-party labels, clustering around zero. In-group annotators show slightly more variation but tend to assign fewer labels than first-party participants.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "F Agreement Among Third-Party Annotators",
      "text": "Figure  6  presents the interrater agreement amongst human annotators and amongst LLMs, evaluated by interrater correlation. LLMs generally exhibit higher and more consistent interrater agreement acorss emotions compared to human annotators. LLMs exhibit the highest agreement when labeling joy, love, and annoyance, while the lowest agreement is observed for neutral, confusion, and approval. In contrast, grief, sadness, and fear yield the highest interrater correlations among human annotators, whereas desire, realization, and relief yield the lowest.\n\nHuman annotators demonstrate higher agreement on negative emotions, while LLMs show higher agreement on positive emotions. Both human annotators and LLMs exhibit higher interrater agreement for emotions that are typically expressed with distinct textual cues and lower agreement for more nuanced and context-dependent emotions such as approval, relief, and realization. These emotions may be harder to infer externally due to their reliance on situational or implicit contextual cues.\n\nWe also computed the Cohen's Kappa by randomly sampling two annotations for each post and calculating the agreement between these two sets of annotations. We computed the correlation between Cohen's kappa values and interrater correlation and found that Cohen's kappa and interrater correlation are highly correlated (LLMs: Pearson r = 0.83, p = 6.57 × 10 -8 ; Human annotators: Pearson r = 0.71, p = 2.07 × 10 -5 for out-group). This strong correlation suggests that both metrics capture similar trends in annotator agreement.\n\nG First-and Third-Party Alignment   third-party annotations and first-party labels vary across different emotions. While LLMs demonstrate more consistent alignment across emotions, human annotators show greater variation, which potentially reflect more subjective interpretation. Alignment is higher for emotions such as anger, love, and sadness for both human and LLM annotators, suggesting these emotions are more explicitly expressed and consistently recognized. Realization, relief, and approval yield the lowest alignment.\n\nIn addition, for each emotion, we calculated the percentage of posts where: (1) all annotators reached a decision; (2) a majority-but not all-of the annotators reached a decision; and (3) annotators failed to reach a decision due to ties. These percentages are reported in Table  6 . For human annotations, on average, 27% of posts for a given emotion received a majority (but not unanimous) decision, 70% received a unanimous decision, and 3% resulted in ties. For LLM annotations, on average, 16% of posts yielded a majority decision and 84% a unanimous decision, with no ties occurring (as each post is annotated by 5 LLMs). The  large proportion of posts where disagreement occurs, which aligns with the relatively low inter-rater agreement among third-party annotators, further highlights the highly subjective nature of emotion recognition.\n\nWe further examined the confusion matrices (Figures  8, 9 , 10) to understand the patterns of alignment and misalignment between first-party labels and third-party annotations. As shown in the figures, annotations frequently align with emotions within the same broad category-for example, positive emotions (e.g., joy, admiration, excitement) and negative emotions (e.g., anger, sadness, disappointment) tend to be more frequently confused with each other, as highlighted by the green and blue clusters. However, emotions that are more ambiguous or context-dependent, such as confusion, surprise, and realization, exhibit greater divergence between first-party and third-party labels, as seen in the red-outlined cluster. Additionally, LLMs show more consistent annotation patterns across emotions, whereas human annotators display greater variability, reflecting possible subjectivity in emotion interpretation. Figure  11 : Alignment between third-party annotations with first-party labels, on coarse-grained emotions, evaluated using Cohen Kappa.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "H Alignment Between Third-Party",
      "text": "Annotations and First-party Labels on Coarse-Grained Emotions",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "H.1 Coarse-Grained Emotion Taxonomy",
      "text": "To evaluate the alignment between third-party annotations and first-party labels at a higher level, we group the 28 emotion categories into seven groups: joy, love, anger, surprise, fear, sadness, and neutral. Since there is no universally agreed-upon set of basic emotions, we construct our grouping based the basic emotion categories proposed by Ekman (1992) (anger, disgust, fear, joy, neutral, sadness, surprise) and  Shaver et al. (1987)  (anger, love, fear, joy, sadness, and surprise).",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "H.2 Third-Party Annotators' Performance",
      "text": "We analyze the alignment between third-party annotations and first-party labels after mapping the original 28 emotion categories into 7 broader emotion groups. By aggregating annotations, we examine whether third-party annotators, while struggling to capture fine-grained emotions, align more closely with first-party labels when emotions are categorized at a higher level. For instance, annotations such as 'Admiration' and 'Approval' are grouped under 'Joy'. To assess alignment, similar to 4.2, we computed Cohen's kappa, F1, recall, and pre-Figure  11  presents Cohen's kappa scores for each emotion for human and LLM annotations. For human annotations, Cohen's kappa scores fall within the range of 0.15 -0.6. For LLM annotations, Cohen's kappa scores fall within the range of 0.2 -0.5. The macro-average scores for LLM annotations are as follows: (F1: 0.54, Recall: 0.61, Precision: 0.57). For out-group human annotators, the scores are (F1: 0.45, Recall: 0.39, Precision: 0.55), while for in-group human annotators, they are (F1: 0.47, Recall: 0.41, Precision: 0.55). Figure  12  presents the F1 scores achieved by third-party human annotators (in-group, out-group) and LLMs across different emotions. These results suggest that thirdparty annotators struggle to identify the emotion expressed by the first party even at a higher level, indicating that this misalignment is not solely due to mistakenly interpreting similar emotions within the same broader category, but also stems from selecting emotions that are substantially different from those expressed by the first party. This highlights the inherent limitations in third-party annotations. However, in-group annotators achieve significantly higher recall than out-group annotators (Median in-group = 1.00, Median out-group = 0.50, p = 3 × 10 -3 ), suggesting that they are better at capturing first-party expressed emotions at a broader level.\n\nAnnotator-level comparison We adopted the same method presented in 4.3 to test the impact of demographic similarity between third-party and firstparty annotators at the annotator level. The results of the mixed linear models indicate that ingroup annotators achieve significantly higher recall (Median in-group = 0.60, Median out-group = 0.56, p = 0.006) and marginally higher F1-score (Median in-group = 0.53, Median out-group = 0.51, p = 0.04) compared to out-group annotators. However, no significant difference is observed in precision (Median in-group = 0.54, Median out-group = 0.52, p = 0.29).These findings suggest that demographic similarity continues to play a role in improving annotation quality, even when evaluating alignment between third-party annotations and first-party label at a coarse-grained level of emotion categorization.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "I Llm Prompts",
      "text": "We introduce the LLM prompts for annotations, as presented in Figure  13  and 14. We also inserted first-party age group, gender, and race into the prompt of Figure  13 . We applied the same prompt for all LLMs.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Alignment between third-party annotations with first-party labels, evaluated using F1 score.",
      "page": 5
    },
    {
      "caption": "Figure 1: ). F1 scores",
      "page": 5
    },
    {
      "caption": "Figure 15: and Figure 16 in Appendix.",
      "page": 6
    },
    {
      "caption": "Figure 15: in Appendix presents the distribution",
      "page": 6
    },
    {
      "caption": "Figure 17: in Appendix,",
      "page": 7
    },
    {
      "caption": "Figure 2: Emotion taxonomy",
      "page": 13
    },
    {
      "caption": "Figure 3: Additionally, Table 5 provides sum-",
      "page": 13
    },
    {
      "caption": "Figure 3: Distribution of third-party annotators by age",
      "page": 13
    },
    {
      "caption": "Figure 4: Distributions of the differences in the number of emotion labels assigned by first-party annotators and",
      "page": 14
    },
    {
      "caption": "Figure 5: Co-occurrence of first-party emotion labels.",
      "page": 14
    },
    {
      "caption": "Figure 5: shows the co-occurrence patterns",
      "page": 14
    },
    {
      "caption": "Figure 6: Interrater correlation among third-party annotators.",
      "page": 15
    },
    {
      "caption": "Figure 4: shows the distribution. The x-axis rep-",
      "page": 15
    },
    {
      "caption": "Figure 6: presents the interrater agreement amongst",
      "page": 15
    },
    {
      "caption": "Figure 7: presents the alignment between first-party",
      "page": 15
    },
    {
      "caption": "Figure 7: Alignment between third-party annotations with first-party labels, evaluated using Cohen’s kappa.",
      "page": 16
    },
    {
      "caption": "Figure 8: Confusion matrix showing the alignment be-",
      "page": 16
    },
    {
      "caption": "Figure 9: Confusion matrix showing the alignment be-",
      "page": 16
    },
    {
      "caption": "Figure 10: Confusion matrix showing the alignment",
      "page": 17
    },
    {
      "caption": "Figure 11: Alignment between third-party annotations",
      "page": 17
    },
    {
      "caption": "Figure 11: presents Cohen’s kappa scores",
      "page": 18
    },
    {
      "caption": "Figure 1: 2 presents",
      "page": 18
    },
    {
      "caption": "Figure 12: Alignment between third-party annotations",
      "page": 18
    },
    {
      "caption": "Figure 13: and 14. We also inserted",
      "page": 18
    },
    {
      "caption": "Figure 13: We applied the same prompt",
      "page": 18
    },
    {
      "caption": "Figure 13: Prompt used to elicit LLM annotations.",
      "page": 19
    },
    {
      "caption": "Figure 14: Prompt with first-party demographic information.",
      "page": 19
    },
    {
      "caption": "Figure 15: Density plots showing the distribution of F1 scores, recall for each post (first row). Density plots showing",
      "page": 20
    },
    {
      "caption": "Figure 16: Performance comparison of in-group annotators and out-group annotators by Cohen’s kappa",
      "page": 20
    },
    {
      "caption": "Figure 17: Density plots showing the distribution of F1 scores, recall, and precision for each.",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 5: provides sum- within the past 12 months (excluding shares, re-",
      "data": [
        {
          "Number of annotators (unique)": "Number of annotators\n- In-group role\n- Out-group role",
          "399": "236\n201"
        },
        {
          "Number of annotators (unique)": "Average number of posts per annota-\ntor\n- In-group role\n- Out-group role",
          "399": "9.1\n10.7"
        },
        {
          "Number of annotators (unique)": "Total annotations\n- By in-group annotators\n- By out-group annotators",
          "399": "2136\n2157"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Stance detection on social media: State of the art and trends",
      "authors": [
        "Abeer Aldayel",
        "Walid Magdy"
      ],
      "year": "2021",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "2",
      "title": "A survey of state-of-the-art approaches for emotion recognition in text",
      "authors": [
        "Nourah Alswaidan",
        "Bachir Menai"
      ],
      "year": "2020",
      "venue": "Knowledge and Information Systems"
    },
    {
      "citation_id": "3",
      "title": "Identifying expressions of emotion in text",
      "authors": [
        "Saima Aman",
        "Stan Szpakowicz"
      ],
      "year": "2007",
      "venue": "International Conference on Text, Speech and Dialogue"
    },
    {
      "citation_id": "4",
      "title": "The claude 3 model family: Opus, sonnet, haiku",
      "authors": [
        "Anthropic"
      ],
      "year": "2024",
      "venue": "The claude 3 model family: Opus, sonnet, haiku"
    },
    {
      "citation_id": "5",
      "title": "Detecting implicit expressions of emotion in text: A comparative analysis. Decision support systems",
      "authors": [
        "Alexandra Balahur",
        "Jesús Hermida",
        "Andrés Montoyo"
      ],
      "year": "2012",
      "venue": "Detecting implicit expressions of emotion in text: A comparative analysis. Decision support systems"
    },
    {
      "citation_id": "6",
      "title": "Contextualized sarcasm detection on twitter",
      "authors": [
        "David Bamman",
        "Noah Smith"
      ],
      "year": "2015",
      "venue": "proceedings of the international AAAI conference on web and social media"
    },
    {
      "citation_id": "7",
      "title": "Unspeakable sentences : Narration and representation in the language of fiction",
      "authors": [
        "Ann Banfield"
      ],
      "year": "1982",
      "venue": "Unspeakable sentences : Narration and representation in the language of fiction"
    },
    {
      "citation_id": "8",
      "title": "Feelings or words? understanding the content in self-report ratings of experienced emotion",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2004",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "9",
      "title": "Representation of the inner self in autobiography: Women's and men's use of internal states language in personal narratives",
      "authors": [
        "Patricia Bauer",
        "Leif Stennes",
        "Jennifer Haight"
      ],
      "year": "2003",
      "venue": "Memory"
    },
    {
      "citation_id": "10",
      "title": "Sensitivity, performance, robustness: Deconstructing the effect of sociodemographic prompting",
      "authors": [
        "Tilman Beck",
        "Hendrik Schuff",
        "Anne Lauscher",
        "Iryna Gurevych"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "Some advances in transformationbased part of speech tagging",
      "authors": [
        "Eric Brill"
      ],
      "year": "1994",
      "venue": "Some advances in transformationbased part of speech tagging"
    },
    {
      "citation_id": "12",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "Readers vs. writers vs. texts: Coping with different perspectives of text understanding in emotion annotation",
      "authors": [
        "Sven Buechel",
        "Udo Hahn"
      ],
      "year": "2017",
      "venue": "Proceedings of the 11th linguistic annotation workshop"
    },
    {
      "citation_id": "14",
      "title": "Emotionally informed hate speech detection: a multitarget perspective",
      "authors": [
        "Patricia Chiril",
        "Endang Pamungkas",
        "Farah Benamara",
        "Véronique Moriceau",
        "Viviana Patti"
      ],
      "year": "2022",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "15",
      "title": "Automated hate speech detection and the problem of offensive language",
      "authors": [
        "Thomas Davidson",
        "Dana Warmsley",
        "Michael Macy",
        "Ingmar Weber"
      ],
      "year": "2017",
      "venue": "Proceedings of the international AAAI conference on web and social media"
    },
    {
      "citation_id": "16",
      "title": "Online language: The role of culture in self-expression and self-construal on facebook",
      "authors": [
        "Allison David C Deandrea",
        "Timothy Shaw",
        "Levine"
      ],
      "year": "2010",
      "venue": "Journal of language and social psychology"
    },
    {
      "citation_id": "17",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.372"
    },
    {
      "citation_id": "18",
      "title": "Impact of annotator demographics on sentiment dataset labeling",
      "authors": [
        "Yi Ding",
        "Jacob You",
        "Tonja-Katrin Machulla",
        "Jennifer Jacobs",
        "Pradeep Sen",
        "Tobias Höllerer"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "19",
      "title": "Data quality in online humansubjects research: Comparisons between mturk, prolific, cloudresearch, qualtrics, and sona",
      "authors": [
        "Patrick Benjamin D Douglas",
        "Markus Ewell",
        "Brauer"
      ],
      "year": "2023",
      "venue": "Plos one"
    },
    {
      "citation_id": "20",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "21",
      "title": "Is there an in-group advantage in emotion recognition? Hillary Anger Elfenbein and Nalini Ambady. 2002b. On the universality and cultural specificity of emotion recognition: a meta-analysis",
      "authors": [
        "Hillary Anger",
        "Nalini Ambady"
      ],
      "year": "2002",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "22",
      "title": "Data quality of platforms and panels for online behavioral research",
      "authors": [
        "Peer Eyal",
        "Rothschild David",
        "Gordon Andrew",
        "Evernden Zak",
        "Damer Ekaterina"
      ],
      "year": "2021",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "23",
      "title": "Perceptions of emotion from facial expressions are not culturally universal: evidence from a remote culture",
      "authors": [
        "Maria Gendron",
        "Debi Roberson",
        "Jacoba Marietta Van Der Vyver",
        "Lisa Barrett"
      ],
      "year": "2014",
      "venue": "Emotion"
    },
    {
      "citation_id": "24",
      "title": "Chatgpt outperforms crowd-workers for textannotation tasks",
      "authors": [
        "Fabrizio Gilardi",
        "Meysam Alizadeh",
        "Maël Kubli"
      ],
      "year": "2023",
      "venue": "Chatgpt outperforms crowd-workers for textannotation tasks"
    },
    {
      "citation_id": "25",
      "title": "Large language models for propaganda span annotation",
      "authors": [
        "Maram Hasanain",
        "Fatema Ahmad",
        "Firoj Alam"
      ],
      "year": "2024",
      "venue": "EMNLP (Findings)"
    },
    {
      "citation_id": "26",
      "title": "The importance of modeling social factors of language: Theory and practice",
      "authors": [
        "Dirk Hovy",
        "Diyi Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.49"
    },
    {
      "citation_id": "27",
      "title": "Aligning language models to user opinions",
      "authors": [
        "Eunjeong Hwang",
        "Bodhisattwa Majumder",
        "Niket Tandon"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
      "doi": "10.18653/v1/2023.findings-emnlp.393"
    },
    {
      "citation_id": "28",
      "title": "Political ideology detection using recursive neural networks",
      "authors": [
        "Mohit Iyyer",
        "Peter Enns",
        "Jordan Boyd-Graber",
        "Philip Resnik"
      ],
      "year": "2014",
      "venue": "Proceedings of the 52nd annual meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "29",
      "title": "2021. (mis)alignment between stance expressed in social media data and public opinion surveys",
      "authors": [
        "Kenneth Joseph",
        "Sarah Shugars",
        "Ryan Gallagher",
        "Jon Green",
        "Alexi Quintana Mathé",
        "Zijian An",
        "David Lazer"
      ],
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2021.emnlp-main.27"
    },
    {
      "citation_id": "30",
      "title": "MEGAnno+: A human-LLM collaborative annotation system",
      "authors": [
        "Hannah Kim",
        "Kushan Mitra",
        "Rafael Li Chen",
        "Sajjadur Rahman",
        "Dan Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "31",
      "title": "Advancing annotation of stance in social media posts: A comparative analysis of large language models and crowd sourcing",
      "authors": [
        "Mao Li",
        "Frederick Conrad"
      ],
      "year": "2024",
      "venue": "Advancing annotation of stance in social media posts: A comparative analysis of large language models and crowd sourcing",
      "arxiv": "arXiv:2406.07483"
    },
    {
      "citation_id": "32",
      "title": "Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation",
      "authors": [
        "Minzhi Li",
        "Taiwei Shi",
        "Caleb Ziems",
        "Min-Yen Kan",
        "Nancy Chen",
        "Zhengyuan Liu",
        "Diyi Yang"
      ],
      "year": "2023",
      "venue": "Coannotating: Uncertainty-guided work allocation between human and large language models for data annotation",
      "arxiv": "arXiv:2310.15638"
    },
    {
      "citation_id": "33",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "arxiv": "arXiv:1710.03957"
    },
    {
      "citation_id": "34",
      "title": "Cultural differences in emotion: differences in emotional arousal level between the east and the west",
      "authors": [
        "Nangyeon Lim"
      ],
      "year": "2016",
      "venue": "Integrative medicine research"
    },
    {
      "citation_id": "35",
      "title": "Dens: A dataset for multi-class emotion analysis",
      "authors": [
        "Chen Liu",
        "Muhammad Osama",
        "Anderson Andrade"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "36",
      "title": "Age-related differences in emotion recognition ability: a cross-sectional study",
      "authors": [
        "Aire Mill",
        "Jüri Allik"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "37",
      "title": "Emotion intensities in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez"
      ],
      "year": "2017",
      "venue": "Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)",
      "doi": "10.18653/v1/S17-1007"
    },
    {
      "citation_id": "38",
      "title": "SemEval-2018 task 1: Affect in tweets",
      "authors": [
        "Saif Mohammad",
        "Felipe Bravo-Marquez",
        "Mohammad Salameh",
        "Svetlana Kiritchenko"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation",
      "doi": "10.18653/v1/S18-1001"
    },
    {
      "citation_id": "39",
      "title": "Cultural conditioning or placebo? on the effectiveness of socio-demographic prompting",
      "authors": [
        "Sagnik Mukherjee",
        "Muhammad Farid Adilazuarda",
        "Sunayana Sitaram",
        "Kalika Bali"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.884"
    },
    {
      "citation_id": "40",
      "title": "A survey of named entity recognition and classification",
      "authors": [
        "David Nadeau",
        "Satoshi Sekine"
      ],
      "year": "2009",
      "venue": "Named Entities: Recognition, classification and use"
    },
    {
      "citation_id": "41",
      "title": "Social media sentiment analysis based on covid-19",
      "authors": [
        "László Nemes",
        "Attila Kiss"
      ],
      "year": "2021",
      "venue": "Journal of Information and Telecommunication"
    },
    {
      "citation_id": "42",
      "title": "From text to emotion: Unveiling the emotion annotation capabilities of llms",
      "authors": [
        "Minxue Niu",
        "Mimansa Jaiswal",
        "Emily Provost"
      ],
      "year": "2024",
      "venue": "From text to emotion: Unveiling the emotion annotation capabilities of llms"
    },
    {
      "citation_id": "43",
      "title": "GPT-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report"
    },
    {
      "citation_id": "44",
      "title": "isarcasm: A dataset of intended sarcasm",
      "authors": [
        "Silviu Oprea",
        "Walid Magdy"
      ],
      "year": "2019",
      "venue": "isarcasm: A dataset of intended sarcasm",
      "arxiv": "arXiv:1911.03123"
    },
    {
      "citation_id": "45",
      "title": "Toxicity detection: Does context really matter?",
      "authors": [
        "John Pavlopoulos",
        "Jeffrey Sorensen",
        "Lucas Dixon"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "46",
      "title": "The nature of emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2001",
      "venue": "American scientist"
    },
    {
      "citation_id": "47",
      "title": "A Comprehensive Grammar of the English Language",
      "authors": [
        "Randolph Quirk",
        "Sidney Greenbaum",
        "Geoffrey Leech",
        "Jan Svartvik"
      ],
      "year": "1985",
      "venue": "A Comprehensive Grammar of the English Language"
    },
    {
      "citation_id": "48",
      "title": "Collecting image annotations using amazon's mechanical turk",
      "authors": [
        "Cyrus Rashtchian",
        "Peter Young",
        "Micah Hodosh",
        "Julia Hockenmaier"
      ],
      "year": "2010",
      "venue": "Proceedings of the NAACL HLT 2010 workshop on creating speech and language data with Amazon's Mechanical Turk"
    },
    {
      "citation_id": "49",
      "title": "",
      "authors": [
        "Machel Reid",
        "Nikolay Savinov",
        "Denis Teplyashin",
        "Dmitry Lepikhin",
        "Timothy Lillicrap",
        "Jean-Baptiste Alayrac",
        "Radu Soricut",
        "Angeliki Lazaridou",
        "Orhan Firat",
        "Julian Schrittwieser",
        "Ioannis Antonoglou",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Andrew Dai",
        "Katie Millican",
        "Ethan Dyer",
        "Mia Glaese",
        "Thibault Sottiaux",
        "Benjamin Lee",
        "Fabio Viola",
        "Malcolm Reynolds",
        "Yuanzhong Xu",
        "James Molloy",
        "Jilin Chen",
        "Michael Isard",
        "Paul Barham",
        "Tom Hennigan",
        "Ross Mcilroy",
        "Melvin Johnson",
        "Johan Schalkwyk",
        "Eli Collins",
        "Eliza Rutherford",
        "Erica Moreira",
        "Kareem Ayoub",
        "Megha Goel",
        "Clemens Meyer",
        "Gregory Thornton",
        "Zhen Yang",
        "Henryk Michalewski",
        "Zaheer Abbas",
        "Nathan Schucher"
      ],
      "venue": ""
    },
    {
      "citation_id": "50",
      "title": "Cross-cultural recognition of basic emotions through nonverbal emotional vocalizations",
      "authors": [
        "A Disa",
        "Frank Sauter",
        "Paul Eisner",
        "Sophie Ekman",
        "Scott"
      ],
      "year": "2010",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "51",
      "title": "Emotion knowledge: further exploration of a prototype approach",
      "authors": [
        "Phillip Shaver",
        "Judith Schwartz",
        "Donald Kirson",
        "Cary O' Connor"
      ],
      "year": "1987",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "52",
      "title": "Lie detection accuracy-the role of age and the use of emotions as a reliable cue",
      "authors": [
        "Hannah Shaw",
        "Minna Lyons"
      ],
      "year": "2017",
      "venue": "Journal of Police and Criminal Psychology"
    },
    {
      "citation_id": "53",
      "title": "What sounds \"right\" to me? experiential factors in the perception of political ideology",
      "authors": [
        "Qinlan Shen",
        "Carolyn Rose"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter",
      "doi": "10.18653/v1/2021.eacl-main.152"
    },
    {
      "citation_id": "54",
      "title": "Cheap and fast-but is it good? evaluating non-expert annotations for natural language tasks",
      "authors": [
        "Rion Snow",
        "Brendan 'connor",
        "Dan Jurafsky",
        "Andrew Ng"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "55",
      "title": "Aligning with whom? large language models have gender and racial biases in subjective nlp tasks",
      "authors": [
        "Huaman Sun",
        "Jiaxin Pei",
        "Minje Choi",
        "David Jurgens"
      ],
      "year": "2023",
      "venue": "Aligning with whom? large language models have gender and racial biases in subjective nlp tasks",
      "arxiv": "arXiv:2311.09730"
    },
    {
      "citation_id": "56",
      "title": "The sentiment problem: A critical survey towards deconstructing sentiment analysis",
      "authors": [
        "Pranav Venkit",
        "Mukund Srinath",
        "Sanjana Gautam",
        "Saranya Venkatraman",
        "Vipul Gupta",
        "Rebecca Passonneau",
        "Shomir Wilson"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2023.emnlp-main.848"
    },
    {
      "citation_id": "57",
      "title": "Norms of online expressions of emotion: Comparing facebook, twitter, instagram, and whatsapp",
      "authors": [
        "Susanne Sophie F Waterloo",
        "Jochen Baumgartner",
        "Patti Peter",
        "Valkenburg"
      ],
      "year": "2018",
      "venue": "New media & society"
    },
    {
      "citation_id": "58",
      "title": "Learning subjective language",
      "authors": [
        "Janyce Wiebe",
        "Theresa Wilson",
        "Rebecca Bruce",
        "Matthew Bell",
        "Melanie Martin"
      ],
      "year": "2004",
      "venue": "Computational linguistics"
    },
    {
      "citation_id": "59",
      "title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation",
      "authors": [
        "Janyce Wiebe",
        "Theresa Wilson",
        "Claire Cardie"
      ],
      "year": "2005",
      "venue": "Annotating expressions of opinions and emotions in language. Language resources and evaluation"
    },
    {
      "citation_id": "60",
      "title": "Emotion fusion for mental illness detection from social media: A survey",
      "authors": [
        "Tianlin Zhang",
        "Kailai Yang",
        "Shaoxiong Ji",
        "Sophia Ananiadou"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "61",
      "title": "Text-based emotion recognition in english and polish for therapeutic chatbot",
      "authors": [
        "Artur Zygadło",
        "Marek Kozłowski",
        "Artur Janicki"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    }
  ]
}