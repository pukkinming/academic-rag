{
  "paper_id": "2306.09486v2",
  "title": "Fedmultimodal: A Benchmark For Multimodal Federated Learning",
  "published": "2023-06-15T20:31:26Z",
  "authors": [
    "Tiantian Feng",
    "Digbalay Bose",
    "Tuo Zhang",
    "Rajat Hebbar",
    "Anil Ramakrishna",
    "Rahul Gupta",
    "Mi Zhang",
    "Salman Avestimehr",
    "Shrikanth Narayanan"
  ],
  "keywords": [
    "Federated Learning",
    "Multimodal Learning",
    "Multimodal Benchmark"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients submit a locally trained model, and the server aggregates these parameters until convergence. Despite significant efforts that have been made to FL in fields like computer vision, audio, and natural language processing, the FL applications utilizing multimodal data streams remain largely unexplored. It is known that multimodal learning has broad real-world applications in emotion recognition, healthcare, multimedia, and social media, while user privacy persists as a critical concern. Specifically, there are no existing FL benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal FL, we introduce FedMultimodal, the first FL benchmark for multimodal learning covering five representative multimodal applications from ten commonly used datasets with a total of eight unique modalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-end modeling framework ranging from data partition and feature extraction to FL benchmark algorithms and model evaluation. Unlike existing FL benchmarks, FedMultimodal provides a standardized approach to assess the robustness of FL against three common data corruptions in real-life multimodal applications: missing modalities, missing labels, and erroneous labels. We hope that FedMultimodal can accelerate numerous future research directions, including designing multimodal FL algorithms toward extreme data heterogeneity, robustness multimodal FL, and efficient multimodal FL. The datasets and benchmark results can be accessed at: https://github.com/usc-sail/fed-multimodal.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "With rapid advances in machine learning (ML)  [39]  in the past decade, modern mobile devices and wearable sensors  [7, 52]  have revolutionized applications and services in industries ranging from entertainment and transportation to healthcare and defense, significantly changing how people live, work, and interact with each other. These intelligent sensing devices, equipped with sensors of multiple modalities, can capture diverse information about a user, including but not limited to physiological, emotional, and rich spatiotemporal contextual information  [4, 6, 17, 22, 54] . These data records are typically transmitted to remote servers for centralized training of the ML models. However, the collection of human-centered data raises significant concerns about compromising user privacy due to association with sensitive environments and contexts, ranging from homes, workplaces, and business meetings to hospitals and schools  [56] . Therefore, it is critical to ensure that modern ML systems can protect user privacy by preventing any unauthorized access to data  [19, 49] .\n\nIn response to this, ML practitioners have developed Federated Learning as an alternative paradigm to build models, without the need to transfer user data from the edge devices  [37] . Unlike centralized training, models are trained locally using locally stored Table  1 : Experimental considerations between FedMultimodal and existing multimodal FL studies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Missing",
      "text": "Erroneous Missing Modailties Labels Labels SSCL  [59]  ‚úó ‚úó ‚úì MMFed  [74]  ‚úó ‚úó ‚úó FedMsplit  [10]  ‚úì ‚úó ‚úó CreamFL  [77]  ‚úì ‚úó ‚úó\n\ndata, and updated parameters are transmitted to the server instead of raw data. FL allows clients to train a model collaboratively without sharing their local data, making it one of the most emerging privacy-enhancing learning algorithms in ML research  [33] .\n\nPrevious works in FL have primarily focused on designing robust and efficient algorithms for federated model training. FedAvg  [46]  was the earliest FL optimization algorithm to train the model in the distribution mechanism. In FedAvg, each client executes local model updates before submitting the updates to the server. Even though FedAvg offers possibilities for deploying FL in the wild, it often encounters slow convergence as a consequence of gradient drifting from data heterogeneity. As such, researchers have proposed algorithms such as stochastic Controlled Averaging Algorithm (SCAFFOLD)  [35]  and FedProx  [40]  to minimize the impact of gradient drift for heterogeneous data. For example, SCAFFOLD accelerates the training speed through control variates which prevent the client gradients from drifting away from the global optima. Similarly,  [57]  introduced adaptive optimization algorithms, FedOpt, that allow server optimization through momentum.\n\nTo facilitate FL research in more diverse problem domains, a number of FL benchmarks have been developed in the past few years. For example, LEAF  [8]  was the earliest FL benchmark which includes multiple FL training tasks on 5 datasets covering various computer vision and NLP tasks. FedML  [28] , besides providing an open-source library and a platform for federated learning deployment, it includes multiple FL benchmarks on computer vision and health  [29] , data mining  [27] , IoT  [80] , and NLP  [43] . More recently,  [38]  announced a multi-domain FL benchmark called FedScale. Fed-Scale included implementations with 20 realistic FL datasets mainly in computer vision and natural language processing applications.  [14]  introduced an FL simulation tool named FLUTE, which covers the application of CV, NLP, and audio tasks. Meanwhile,  [79]  presented an audio-centric federated learning framework, FedAudio, which focused on speech emotion recognition, keyword spotting, and audio event classification. Further, FLamby  [66]  is a recently proposed FL benchmark for a wide range of healthcare applications such as identifying lung nodules and predicting death risks. Feder-atedScope  [73]  also incorporates various benchmarks for federated learning in CV, NLP, and data mining  [71] .\n\nExisting Multimodal Federated Learning Works: While existing FL benchmarks largely focus on unimodal applications such as computer vision (CV), natural language processing (NLP), and speech recognition, a significant number of real-world applications are associated with multimodal data streams. As listed in Table  1 ,  [59]  was one of the earliest to investigate FL using multi-sensory data. They proposed a self-supervised learning approach called Scalogram-signal Correspondence Learning (SSCL) to learn robust multi-modal representations in FL. More recently,  [74]  designed a multi-modal FL framework named MMFed using the cross-attention mechanism. Moreover,  [10]  proposed an FL framework called FedM-Split that targeted the issue of missing modalities in the multimodal setup. CreamFL  [77]  provides a multi-modal FL framework using contrastive representation-level ensemble to learn a larger server model from heterogeneous clients across multi-modalities. However, existing multimodal FL frameworks perform their evaluation using their defined experimental setups, thus making it challenging for researchers to compare their methods with existing state-ofthe-art fairly and effectively.\n\nOur Contributions: In this work, we introduce FedMultimodal, a FL benchmark for multimodal applications. We summarize our key contributions as follows:\n\n‚Ä¢ FedMultimodal includes ten representative datasets covering five diverse application scenarios -emotion recognition, multimodal action recognition, human activity recognition, healthcare, and social media -that are well aligned with FL. We present systematic benchmark results on the above datasets to facilitate researchers to fairly compare their algorithms. ‚Ä¢ To help the community accurately compare performance and ensure reproducibility, FedMultimodal presents an open-source end-to-end FL simulation framework and includes capabilities to perform data partitioning, feature processing, and multimodal training. FedMultimodal offers support for several popular FL optimizers including FedAvg  [46] , FedProx  [40] , Fe-dRS  [41] , SCAFFOLD  [35] , and FedOpt  [57] , and provide flexibility that allows users to customized the trainers on the included datasets. The source codes and user guides are available at https://github.com/usc-sail/fed-multimodal. ‚Ä¢ In addition to ensuring accessibility and reproducibility, the benchmark provides a robustness assessment module that allows researchers to simulate challenges uniquely tied to multimodal FL applications in real-world scenarios. As listed in Table  1 , previous works on multimodal FL provide limited assessments of the robustness under real-world settings. Specifically, Fed-Multimodal emulates missing modalities, missing labels, and erroneous labels on top of the provided datasets to simulate scenarios when deploying FL systems in real-world settings. This is a crucial difference and a unique contribution of FedMultimodal compared to existing FL literature.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Datasets And Tasks",
      "text": "Table  2  provides an overview of the 10 datasets included in FedMultimodal. These 10 multimodal datasets cover five diverse tasks -Emotion Recognition, Multimedia Action Recognition, Human Activity Recognition, Healthcare, and Social Media classification. One important reason we select these 10 datasets is that they are publicly available, thus ensuring ease of accessibility and reproducibility. In this section, we provide a brief overview of each included dataset and the corresponding tasks. MELD is a multiparty dialog dataset  [55]  containing over 9k utterances with audio and transcripts data from the Friends TV series. Due to the imbalanced label distribution in the dataset, we keep 4 emotions with the most samples i.e., neutral, happy, sad, and angry.\n\nCREMA-D has 7,442 audio-visual clips recorded by 91 actors  [9] . Each speaker was instructed to utter 12 sentences emulating 6 emotions: neutral, happy, anger, disgust, fear, and sad.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Action Recognition (Mar)",
      "text": "The task of MAR consists of classifying a video into action categories based on underlying visual and audio modalities. In FedMultimodal, we include two well-known MAR testbeds: UCF101 and Moments in Time (MiT).\n\nUCF101 dataset  [63]  consists of 13,320 web videos with 101 sportbased action labels. However, data associated with only 51 labels are presented with video and audio information, resulting in less than 7,000 videos for the experiments. The duration of the videos ranges from several seconds to over 20 seconds. We subsample the video at the frame rate of 1Hz to reduce the computation overhead.\n\nMoments in Time (MiT) is a large-scale MAR ( 1 million) dataset  [50]  with short (3 seconds) videos with overall list of 339 action labels. It is worth noting that MiT is a challenging dataset, with state-of-the-art top-1 accuracy close to 35%  [58] . Given the inherent difficulty of this task, we tackle the easier classification problem by creating partitions of data with fewer distinct labels. We create two sub-datasets, MiT10 and MiT51, from the original MiT dataset. MiT10 and MiT51 contain videos of the 10 and the 51 most frequent labels. Similar to the UCF101 setting, we subsample the video every 10 frames to accommodate the computing constraints in FL.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Human Activity Recognition (Har)",
      "text": "HAR identifies human actions based on wearable data such as accelerometers and gyroscopes. Due to the nature of its wearablefriendly attribute, it has become a prevalent research topic in FL  [61] . FedMultimodal provides the implementation on two HAR datasets: UCI-HAR and KU-HAR. In our experiments, we treat the accelerometer and gyroscope data as two different modalities.\n\nUCI-HAR dataset  [3]  consists of smartphone sensors (Accelerometer and Gyroscope) data from 30 subjects (19-48 yrs old) performing six daily activities: walking, walking upstairs, walking downstairs, sitting, standing, laying. The participants wear smartphones on their waists during the collection phase. The accelerometer and gyroscope data are sampled at 50Hz.\n\nKU-HAR is a recent human activity recognition dataset  [62]  collected with a group of 90 participants (75 male and 15 female) on 18 different activities. Instead of evaluating the 18 activities, we decided to keep 6 activities existing in the UCI-HAR dataset while adding jumping and running activities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Healthcare",
      "text": "Healthcare ML applications have made immense progress in a range of domains e.g., heart-disease classification over the last decade.\n\nFedMultimodal explores the problem of ECG classification based on the PTB-XL  [69]  dataset. PTB-XL  [69]  includes over 20,000 clinical 12-lead ECG recordings from 18,885 patients for a multi-label classification task. There are 5 classes describing ECG diagnosis, including normal ECG, myocardial infarction, ST/T change, conduction disturbance, and hypertrophy. As suggested by  [64] , we use the ECG data provided at the sampling frequency of 100 Hz. We separate the readings from electrodes I, II, III, AVL, AVR, AVF, and V1-V6 as two modalities suggested by  [2] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Social Media (Sm)",
      "text": "Social media has become an increasingly important tool during disasters and emergencies for people to track the latest updates in the area, especially the impact (e.g., property damage, injury, and death) of the disaster, as well as urgent needs for help. However, the widespread adoption of social media has also drawn significant concerns about spreading misinformation, thus urging the need to identify and mitigate this misleading and harmful content. To accelerate FL research in this domain, FedMultimodal incorporates two social-media-based multimodal datasets related to hateful content and crisis information classification.\n\nHateful Memes dataset was released from the 2020 Hateful Memes Challenge  [36]  that focus on detecting hateful speech in memes. The database includes 10,000 multimodal data with image and text pairs with binary classes.\n\nCrisisMMD  [1]  comprises 18.1k tweets containing both paired visual and textual information. It collects relevant tweets from seven prominent natural disasters such as California Wildfires (2017). One of the purposes of the dataset is to identify the impact of the disaster like utility damage and injured or dead people.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "End-To-End Multimodal Federated Learning Framework",
      "text": "To benchmark the performance of the multimodal datasets described in Section 2 as well as to support future research in the area of multimodal federated learning, we have built an end-to-end multimodal federated learning research framework.   Mainly, the selected feature needs to align with the computation capabilities available on the edge computing devices. For example, it is unrealistic to assume that edge devices could load and run large transformer-based  [68]  models for inference purposes without sacrificing system performance. Hence, we focus on implementing mobile-friendly feature extraction pipelines in FedMultimodal which are listed below, targeting swift computation, efficient storage, and ease of deployment.\n\n‚Ä¢ Visual: For the visual data, our benchmark supports MobileNetV2  [30]  and MobileViT  [47]  as the embedding network to extract latent presentations. The complete MobileNetV2 and MobileViT have 4.3M and 2.7M parameters, respectively, making them practical visual feature backbones in FL. Due to space constraints, we report benchmark results with MobileNetV2 in this paper. ‚Ä¢ Text: FedMultimodal integrates both MobileBERT  [65]  and Dis-tillBERT  [60]  to extract representations from textual data. Mo-bileBERT uses a bottleneck structure to reduce the parameter size from 340M to 25M when compared to BERT  [13] , while Distill-BERT applies a knowledge distillation process that decreases the BERT model to 66M parameters. We decide to benchmark with the MobileBERT feature backbone given the page constraints. ‚Ä¢ Audio: FedMultimodal uses Mel-frequency cepstral coefficients (MFCCs) due to their widespread usage in the state-of-the-art speech recognition models like Wav2Vec 2.0  [11] . ‚Ä¢ Other Modalities: We use the raw data with the remaining modalities in the FedMultimodal. These data streams include accelerometer, gyroscope, and ECG readings.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Models",
      "text": "Model Design Principles. Compared to remote servers, edge computing nodes are more appropriate for lightweight computing tasks due to constraints in computation resources, storage capabilities, battery capacities, and communication bandwidths. When designing ML models for resource-constrained devices, a significant design consideration is to reduce the number of parameters in edge ML models, thus reducing memory and execution latency. Such models can either be the backbone feature extraction models or application-specific prediction models. One major design principle of FedMultimodal is to study lightweight but effective solutions for multimodal FL learning instead of training models with multi-million parameters. Model Architecture. With this design principle in mind, we construct ML models mainly based on the 1D Conv-RNN/MLP architecture. Even though the transformer-based model has achieved SOTA performance in diverse applications, these models typically include millions or even billions of parameters, making them impractical to use in FL settings as a result of massive computations, memory usage, and battery consumption during back-propagation  [70] .\n\nAn example model architecture is presented in Figure  2 . Specifically, the multimodal model in FedMultimodal includes an encoder, a modality-fusion block, and a downstream classifier. The encoder part follows either Conv+RNN architecture or RNN-only architecture. The encoder that adopts Conv+RNN architecture takes the input of audio, accelerometer, gyroscope, and ECG information, otherwise uses RNN-only architecture. Following encoder modules, FedMultimodal uses a late-fusion mechanism to combine modality-specific representations into a multimodal representation. The multimodal representation is then fed through 2 dense layers for downstream predictions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Fusion Schemes",
      "text": "In this work, we present two basic fusion approaches: concatenationbased fusion and attention-based fusion. In the concatenation-based fusion, the average pooling operation is first performed on the GRU output. After that, we concatenate the pooling embeddings to form the multimodal embedding. On the other hand, the attentionbased fusion concatenates the temporal output from each modality without the average pooling step. We apply an attention mechanism similar to hierarchical attention  [76] . Given the concatenated multimodal data ‚Ñé, the attention procedures are as follows:\n\nThe concatenated multimodal data ‚Ñé is first fed through a onelayer MLP to get representation ùë¢. We then use a context vector ùëê to obtain a normalized importance score through a softmax function. After that, we compute the final multimodal embedding ùë£ as a weighted sum of ‚Ñé based on the weights ùëé. Here, we can further implement a multi-head attention mechanism by having multiple ùëê. We would also stress that this attention mechanism is lightweight, thus making it realistic to deploy on a variety of edge devices. In addition, the attention mechanism allows us to mask the missing",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Federated Optimizers",
      "text": "First, most existing FL training algorithms are validated in unimodal settings, and their efficacy on multimodal tasks remains unexplored. As a result, FedMultimodal is suited to several popular FL algorithms, including FedAvg  [46] , FedProx  [40] , FedRS  [41] ,\n\nand FedOpt  [57] . Particularly, FedOpt holds state-of-the-art performance across multiple unimodal applications  [57] . One objective of FedMultimodal is to provide comprehensive evaluations across various FL algorithms.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Real-World Noise Factor Emulator",
      "text": "Prior literature (see Table  1 ) on multimodal FL provides little or no assessment of their robustness in real-life settings. In order to provide a comprehensive evaluation of multimodal FL models toward safe and robust deployment, FedMultimodal enables the emulation of missing modalities, missing labels, and erroneous labels for real-world multimodal FL. Missing Modality. In practice, data sources, whether they are microphones, cameras, mobile hardware, or medical electrodes, are prone to data imperfections or complete data losses (e.g., missing modalities) caused by firmware malfunctions, network disconnections, or sensor damages  [20] . Hence, it is critical to design an emulator module to synthesize the cases of missing modalities for some clients. FedMultimodal provides the simulations of missing modalities as suggested by  [10] , where the availability of each modality follows a Bernoulli distribution. We set an equal missing rate ùëû for each modality in the following experiments.\n\nMissing Labels. Not only can the multimodal FL encounter data imperfection challenges, it can also suffer from missing label problems. Surprisingly, most prior works have made the ideal assumption that the data stored on edge devices are fully annotated with groundtruth labels. However, in a more realistic real-world FL setting, we argue that only a portion of the data can come with labels. As such, FedMultimodal allows the missing label simulation to assess the risk of decreased robustness. Erroneous Labels. In addition to missing labels, real-world FL implementations encounter label noise as a result of bias, skill differences, and labeling errors from the annotators. Inspired by  [79] , we apply a label error generation process described in  [51] . In summary, the erroneous labels are generated using a transition matrix ùëÑ, where ùëÑ ùëñ,ùëó = P(ùë¶ = ùëó |ùë¶ = ùëñ) indicates the chance of ground-truth label ùëñ being erroneous annotated with label ùëó.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Discussion 4.1 Experimental Details",
      "text": "Setup. We adopt the RNN-only model architecture to the video and text modalities while utilizing the Conv-RNN model architecture in other modalities. Specifically, the model with the convolutional module consists of 3 convolution layers with the number of filters in {16, 32, 64} and the filter kernel size of 5 √ó 5. Moreover, we set the hidden layer size of RNN as 128. We choose ReLU as the activation function and the dropout rate as 0.2. The number of attention heads is 6 in all experiments. We fixed the batch size for all datasets to 16 and the local epoch to 1 for all experiments. Additionally, we set the training epochs as 200 for all datasets except the MiT sub-datasets. However, the total training epoch is 300 in MiT10 and MiT51 as these 2 datasets contain more data than the other datasets. Hyperparameter details such as the learning and client sampling rates for each dataset are listed in Table  3 . Due to the limited number of clients in the Hateful Memes dataset and PTB-xl datasets, we apply a higher client sample rate in these 2 datasets. In the FedOpt algorithm, we search the server learning rate in a range from 10 -3 to 2.5 √ó 10 -3 . Meanwhile, the proximal term ranges from 10 -2 to 10 0 in the FedProx algorithm.  Evaluation Metrics. We follow established practices from the literature while conducting evaluations on each dataset. Specifically, evaluation metrics (e.g., F1) and validation protocols (e.g., predefined splits) are two fundamental components to ensure comparability with past (and future) works. With datasets that provide a pre-defined partition for training/validation/testing, we repeat the experiments 5 times using different seeds. We perform 5-fold crossvalidation on datasets without such pre-defined experimenting rules. We provide details about our evaluation methods in Table  2 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Overall Performance",
      "text": "We first report the comparisons between two fusion mechanisms (attention-based fusion and concatenation-based fusion) in Table  4 . From the results, we can find that the attention-based fusion mechanism outperforms concatenation-based fusion in the majority of the datasets. Specifically, the attention-based fusion mechanism leads to better performances in most high data heterogeneity conditions, but it underperforms the concatenation-based fusion in two synthetic datasets with ùõº = 5.0 (low data heterogeneity). Moreover, we observe that the FedOpt algorithm consistently yields better baselines compared to other FL algorithms with a few exceptions in low data heterogeneity conditions. However, we would like to highlight that, in practice, FedOpt requires additional hyperparameter tuning on the server learning rate to reach the best performance. Overall, these results imply that the fusion mechanism is a critical factor impacting multimodal model performance in data heterogeneous FL.\n\nMoreover, HAR tasks are associated with the highest performance scores, suggesting the simplicity of this learning task. In contrast, classification results on the Hateful Memes dataset and CrisisMMD dataset imply that social media classification is a challenging task using FL, with the best model performance on the CrisisMMD dataset below 30%. A plausible explanation is that the pre-trained models that we rely on are not generalized to social media data, generating image and textual features that are unrepresentative of downstream learning models. On the other hand, performances on the MiT51 dataset demonstrate similar findings pointed out from  [50] , validating that MiT is a challenging dataset. However, reducing the number of labels indeed simplifies the predicting task, resulting in moderate model performance on MiT10.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Uni-Modality Vs. Multi-Modalities",
      "text": "One fundamental research question centering around multimodal learning is its performance compared to unimodal models. For example, the previous multimodal benchmark  [42] , with an emphasis on centralized learning setup, demonstrates that unimodal learning could yield similar performance to multimodal models with fewer parameters. Similar to MultiBench, FedMultimodal provides the unimodal FL to compare with multimodal FL baselines. We summarize the benchmark comparisons between unimodal FL and multimodal FL in Figure  3 . The comparisons use datasets with natural partitions or high data heterogeneity partitions. Overall, we observe that unimodal learning provides competitive performance compared with the multimodal FL benchmarks, complying with centralized benchmark results reported in  [42] . Nevertheless, in most scenarios, multimodal learning still outperforms unimodal learning, whereas the performance gap between multimodal and unimodal FL is within 5% in the majority of the datasets.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Missing Modalities",
      "text": "As described in earlier sections, a unique challenge associated with multimodal learning is dealing with scenarios of missing modalities  [10, 77] . In this section, we benchmark our selected datasets with different rates of missing modalities. In this experiment, we assume that the availability of each modality follows a Bernoulli distribution with a missing rate of ùëû. Following the experiment protocol presented by  [10] , we set a uniform missing rate of ùëû for each modality, where ùëû ‚àà {0.1, 0.2, 0.3, 0.4, 0.5}. As described in the multimodal model section, attention-based fusion allows us to train the model even with missing data through masking. To train the model with the missing entries, we fill the missing data with 0  [53]  while masking out the corresponding data points in calculating attention scores.\n\nWe present the relative model performance changes at different missing modality rates in Figure  4 . From the graph, we find that the relative performance changes with missing rates below 30% are not substantial, suggesting that a small amount of missing modalities in deployment does not impact the final model performance in multimodal FL. Furthermore, we observe that the model performance starts to decline substantially at the missing rate of 50%. Surprisingly, we observe that half of the models have relative performance decreases that are under 10%, suggesting that the provided baseline models that use attention-based fusion still learn useful information in these cases. However, we find that the missing modality introduces a significantly larger impact on CrisisMMD data and the HAR applications compared to the other multimodal applications we evaluated. This suggests that future FL HAR research should carefully consider missing modalities as a part of the evaluation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Missing Labels",
      "text": "Missing labels is a widely presented challenge in FL. In this section, we perform evaluations of missing label conditions using the Fed-Multimodal benchmark. Similar to the missing modality experiment, we assume that the availability of each label follows a Bernoulli distribution with a missing rate ùëô. We apply the FedMultimodal benchmark to emulate the missing label rate ùëô ‚àà {0.1, 0.2, 0.3, 0.4, 0.5}. The goal of this experiment is to quantify the impact of missing labels on the overall performance of benchmarks, hence we do not integrate any mitigation methodologies, such as semi-supervised learning or self-supervised learning in our experiments.\n\nResults on relative model performance changes at different label missing ratios using the FedMultimodal framework are presented in Figure  5 . Overall, we can observe that missing labels have a reduced impact on the model performance when compared to the missing modality scenario. For instance, the model performance suffers less than 10% relative performance decreases in the majority of the datasets with the exception of KU-HAR datasets. When the missing label ratio is below 50%, we observe minor performance decreases in all datasets. Surprisingly, we identify that CrisisMMD yields worse performance at 30% than at 10% missing label ratio. We conjecture that the reason behind this result might be attributed to the data labeling quality of the Hateful-Memes dataset. For example, determining whether the content is hateful or not can be very subjective, and such subjectiveness could hurt labeling quality.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Impact Of Erroneous Labels",
      "text": "Besides missing modalities and missing labels, erroneous labels frequently exist in FL  [75] . In this section, we report our benchmark performance with erroneous labels. Similar to previous experiments, we search the erroneous label ratio ùëô ‚àà {0.1, 0.2, 0.3, 0.4, 0.5}, where ùëô represents the amount of data with erroneous labels. Similar to the experiment setup in  [79] , our benchmark defines the sparsity of erroneous label transition matrix Q as 0.4. The error sparsity specifies the possible number of unique labels ùëò that one label can be wrongly annotated with, with a small sparsity error rate corresponding to a larger ùëò.\n\nThe complete results of the relative model performance changes at different levels of erroneous label ratios are shown in Figure  6 . Compared to the missing modalities experiment, the erroneous label condition leads to substantially larger performance decreases. For example, more than half of the datasets have the relative performance decreases above 10% at the erroneous label ratio of 30%. Moreover, a 20% performance drop can be identified from these datasets when the erroneous label ratio reaches 50%. To compare the impact of data corruption conditions in FL, we plot the relative performance changes with different data corruption conditions at the data corrupted ratio of 30% in Figure  7 . We can observe that performances of multimodal FL are more susceptible to label noises than missing modalities or missing labels. Based on these observations, our future benchmark directions also include implementations of backdoor attacks and mitigation in FedMultimodal.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "Scale of Datasets and Models. The dataset selection criteria of FedMultimodal ensures that the chosen datasets are representative and diversified across different dimensions such as application scenarios, data size, and number of clients. In addition, FedMultimodal only includes ML models that align with the use cases of FL, taking into account the computational limitations of edge devices. We acknowledge that FedMultimodal currently does not cover several promising multimodal applications, such as medical image analysis, autonomous driving, and virtual reality, and the range of the supported models is limited. We will continuously update FedMultimodal to support new tasks such as Ego4D  [26] , as well as newer feature extraction models. Scale of Modality Fusion Schemes. Currently, FedMultimodal includes two basic approaches for modality fusion: concatenation and attention. Modality fusion under FL remains an open problem, and our objective is to draw attention to the need for developing more advanced modality fusion schemes under FL  [32, 45] . Data Heterogeneity. As discussed in previous sections, addressing the data heterogeneity challenge is critical in FL. While many FL studies focus their experiments on the unimodal setup, there is a lack of extensive research on tackling data heterogeneity in multimodal FL. To address this gap, the FedMulitmodal benchmark provides opportunities to facilitate fundamental research in this direction. In the future, it is of further interest to explore knowledgetransfer learning approaches as suggested in  [12, 31, 44] , within the context of multimodal FL. Label Scarcity. One major challenge for FL is the lack of qualitative labels. FedMultimodal enables researchers to efficiently perform experiments on multimodal FL with missing labels by providing the ability to emulate experimental conditions with missing labels. We hope FedMultimodal brings unique benefits for ML practitioners to develop self-supervised learning  [15, 78, 84]  and semi-supervised learning  [34, 81, 82]  algorithms under multimodal FL. Privacy Leakage. While sharing model updates is considered to be more private than sharing raw data, recent works have revealed that FL can still be susceptible to privacy attacks. These attacks include (but are not limited to) membership inference attacks  [48] , reconstruction attacks  [25, 83] , attribute inference attacks  [18]  and label inference attacks  [24] . Consequently, an emerging research direction for expanding FedMultimodal is to explore the privacy leakages in multimodal FL. Apart from identifying privacy risks associated with multimodal FL, it is also crucial to investigate privacyenhancing techniques, such as differential privacy  [16, 23, 72]  and secure aggregation  [5]  as promising areas of research within the scope of FedMultimodal to mitigate privacy attacks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented a new framework for multimodal federated learning, named FedMultimodal, which enables federated learning in multimodal applications. We further established a reproducible benchmark of results for 5 multimodal FL applications covering 10 datasets for future comparisons. We also benchmarked results on model robustness to missing modalities, missing labels, and noisy labels in each of these tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Acknowledgement",
      "text": "This work is in part supported by USC Amazon Center, as well as research gift awards from Intel, Meta, and Konica Minolta.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of the end-to-end multimodal federated learning framework included in FedMultimodal.",
      "page": 4
    },
    {
      "caption": "Figure 1: uses image sources from https://openmoji.org/",
      "page": 4
    },
    {
      "caption": "Figure 2: The architecture of the basic model.",
      "page": 5
    },
    {
      "caption": "Figure 2: . Specifi-",
      "page": 5
    },
    {
      "caption": "Figure 3: Performance comparisons between multimodal and unimodal learning under FL settings.",
      "page": 7
    },
    {
      "caption": "Figure 4: Relative performance changes under different missing modality rates.",
      "page": 7
    },
    {
      "caption": "Figure 3: The comparisons use datasets with nat-",
      "page": 7
    },
    {
      "caption": "Figure 5: Relative performance changes under different label missing rates.",
      "page": 8
    },
    {
      "caption": "Figure 6: Relative performance changes under different erroneous label rates.",
      "page": 8
    },
    {
      "caption": "Figure 4: From the graph, we find that the",
      "page": 8
    },
    {
      "caption": "Figure 5: Overall, we can observe that missing labels have a",
      "page": 8
    },
    {
      "caption": "Figure 7: Relative performance changes with 30% data corrupted (missing modalities vs. missing labels vs. erroneous labels).",
      "page": 9
    },
    {
      "caption": "Figure 6: Compared to the missing modalities experiment, the erroneous",
      "page": 9
    },
    {
      "caption": "Figure 7: We can observe that perfor-",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Multimedia Activity\nRecognition",
          "Multimodal Applications": "",
          "Column_3": ""
        },
        {
          "Column_1": "",
          "Multimodal Applications": "",
          "Column_3": "Human Activity\nRecognition"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data\nPartition": "Synthetic\nPartition\nNatural\nPartition"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Feature\nProcessing": "Raw\nAudio\n(MFCC)\nImage/Video\n(MobileNet/\nMobileViT)\nText\n(MobileBert/\nDistilBERT)",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Multimodal\nModels": "Y\nClassifier\nFusion\nGRUs GRUs\n1D Conv\nAudio/Acc/ Video/\nGyro/ECG Text",
          "Column_7": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fusion\nSchemes": "Concatenation\nbased"
        },
        {
          "Fusion\nSchemes": "Attention\nbased"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FL\nOptimizers": "FedAvg\nScaffold\nFedProx\nFedOpt\nFedRS\n‚Ä¶",
          "Column_2": "FedAvg",
          "Column_3": ""
        },
        {
          "FL\nOptimizers": "",
          "Column_2": "FedProx",
          "Column_3": ""
        },
        {
          "FL\nOptimizers": "",
          "Column_2": "FedOpt",
          "Column_3": ""
        },
        {
          "FL\nOptimizers": "",
          "Column_2": "FedRS",
          "Column_3": ""
        },
        {
          "FL\nOptimizers": "",
          "Column_2": "",
          "Column_3": "‚Ä¶"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Noise Factor\nEmulator": "Missing\nModalities\nMissing\nLabels\nErroneous\nLabels"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Health Status"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Content Inference Attack\nBob lives\nhere.\n_ lives\nName, Content\nhere.\naddress, etc.": "",
          "Column_2": "",
          "Content Inference Attack": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "Bob lives\nhere.",
          "Column_11": ""
        },
        {
          "Content Inference Attack\nBob lives\nhere.\n_ lives\nName, Content\nhere.\naddress, etc.": "Name,\naddress, etc.",
          "Column_2": "",
          "Content Inference Attack": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "_ li\nhe",
          "Column_7": "ves\nre.",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Sensitive Emotion": "Sensitive Activity"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Crisismmd: Multimodal twitter datasets from natural disasters",
      "authors": [
        "Firoj Alam",
        "Ferda Ofli",
        "Muhammad Imran"
      ],
      "year": "2018",
      "venue": "Twelfth international AAAI conference on web and social media"
    },
    {
      "citation_id": "2",
      "title": "Classification of 12-lead ecgs: the physionet/computing in cardiology challenge",
      "authors": [
        "Erick A Perez Alday",
        "Annie Gu",
        "J Amit",
        "Chad Shah",
        "An Robichaux",
        "Ian Kwok",
        "Chengyu Wong",
        "Feifei Liu",
        "Ali Liu",
        "Andoni Bahrami Rad",
        "Salman Elola",
        "Seyedi"
      ],
      "year": "2020",
      "venue": "Physiological measurement"
    },
    {
      "citation_id": "3",
      "title": "A public domain dataset for human activity recognition using smartphones",
      "authors": [
        "Davide Anguita",
        "Alessandro Ghio",
        "Luca Oneto",
        "Xavier Perez",
        "Jorge Luis",
        "Reyes Ortiz"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21th international European symposium on artificial neural networks, computational intelligence and machine learning"
    },
    {
      "citation_id": "4",
      "title": "The field of human building interaction for convergent research and innovation for intelligent built environments",
      "authors": [
        "Bur√ßin Becerik-Gerber",
        "Gale Lucas",
        "Ashrant Aryal",
        "Mohamad Awada",
        "Mario Berg√©s",
        "Sarah Billington",
        "Olga Boric-Lubecke",
        "Ali Ghahramani",
        "Arsalan Heydarian",
        "Christoph H√∂elscher",
        "Farrokh Jazizadeh",
        "Azam Khan",
        "Jared Langevin",
        "Ruying Liu",
        "Frederick Marks",
        "Matthew Louis Mauriello",
        "Elizabeth Murnane",
        "Haeyoung Noh",
        "Marco Pritoni",
        "Shawn Roll",
        "Davide Schaumann",
        "Mir Hasan Seyedrezaei",
        "John Taylor",
        "Jie Zhao",
        "Runhe Zhu"
      ],
      "year": "2022",
      "venue": "The field of human building interaction for convergent research and innovation for intelligent built environments"
    },
    {
      "citation_id": "5",
      "title": "Practical secure aggregation for privacy-preserving machine learning",
      "authors": [
        "Keith Bonawitz",
        "Vladimir Ivanov",
        "Ben Kreuter",
        "Antonio Marcedone",
        "Brendan Mcmahan",
        "Sarvar Patel",
        "Daniel Ramage",
        "Aaron Segal",
        "Karn Seth"
      ],
      "year": "2017",
      "venue": "proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security"
    },
    {
      "citation_id": "6",
      "title": "Toward robust interpretable human movement pattern analysis in a workplace setting",
      "authors": [
        "M Brandon",
        "Tiantian Booth",
        "Abhishek Feng",
        "Jangalwa",
        "Shrikanth S Narayanan"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Multimodal human and environmental sensing for longitudinal behavioral studies in naturalistic settings: Framework for sensor selection, deployment, and management",
      "authors": [
        "M Brandon",
        "Karel Booth",
        "Tiantian Mundnich",
        "Amrutha Feng",
        "Nadarajan",
        "H Tiago",
        "Jennifer Falk",
        "Emilio Villatte",
        "Shrikanth Ferrara",
        "Narayanan"
      ],
      "year": "2019",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "8",
      "title": "Leaf: A benchmark for federated settings",
      "authors": [
        "Sebastian Caldas",
        "Sai Meher",
        "Karthik Duddu",
        "Peter Wu",
        "Tian Li",
        "Jakub Koneƒçn·ª≥",
        "Brendan Mcmahan",
        "Virginia Smith",
        "Ameet Talwalkar"
      ],
      "year": "2018",
      "venue": "Leaf: A benchmark for federated settings",
      "arxiv": "arXiv:1812.01097"
    },
    {
      "citation_id": "9",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "10",
      "title": "FedMSplit: Correlation-Adaptive Federated Multi-Task Learning across Multimodal Split Networks",
      "authors": [
        "Jiayi Chen",
        "Aidong Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "11",
      "title": "Exploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2021",
      "venue": "Exploring Wav2vec 2.0 fine-tuning for improved speech emotion recognition",
      "arxiv": "arXiv:2110.06309"
    },
    {
      "citation_id": "12",
      "title": "Heterogeneous ensemble knowledge transfer for training large models in federated learning",
      "authors": [
        "Yae Jee Cho",
        "Andre Manoel",
        "Gauri Joshi",
        "Robert Sim",
        "Dimitrios Dimitriadis"
      ],
      "year": "2022",
      "venue": "Heterogeneous ensemble knowledge transfer for training large models in federated learning",
      "arxiv": "arXiv:2204.12703"
    },
    {
      "citation_id": "13",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
    },
    {
      "citation_id": "14",
      "title": "Flute: A scalable, extensible framework for highperformance federated learning simulations",
      "authors": [
        "Dimitrios Dimitriadis",
        "Mirian Garcia",
        "Daniel Diaz",
        "Andre Manoel",
        "Robert Sim"
      ],
      "year": "2022",
      "venue": "Flute: A scalable, extensible framework for highperformance federated learning simulations",
      "arxiv": "arXiv:2203.13789"
    },
    {
      "citation_id": "15",
      "title": "Federated contrastive learning for decentralized unlabeled medical images",
      "authors": [
        "Nanqing Dong",
        "Irina Voiculescu"
      ],
      "year": "2021",
      "venue": "Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference"
    },
    {
      "citation_id": "16",
      "title": "Differential privacy. In Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006",
      "authors": [
        "Cynthia Dwork"
      ],
      "year": "2006",
      "venue": "Differential privacy. In Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006"
    },
    {
      "citation_id": "17",
      "title": "A multimodal analysis of physical activity, sleep, and work shift in nurses with wearable sensor data",
      "authors": [
        "Tiantian Feng",
        "Brandon Booth",
        "Brooke Baldwin-Rodr√≠guez",
        "Felipe Osorno",
        "Shrikanth Narayanan"
      ],
      "year": "2021",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "18",
      "title": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "authors": [
        "Tiantian Feng",
        "Hanieh Hashemi",
        "Rajat Hebbar",
        "Murali Annavaram",
        "Shrikanth S Narayanan"
      ],
      "year": "2021",
      "venue": "Attribute inference attack of speech emotion recognition in federated learning settings",
      "arxiv": "arXiv:2112.13416"
    },
    {
      "citation_id": "19",
      "title": "A Review of Speech-centric Trustworthy Machine Learning: Privacy, Safety, and Fairness",
      "authors": [
        "Tiantian Feng",
        "Rajat Hebbar",
        "Nicholas Mehlman",
        "Xuan Shi",
        "Aditya Kommineni",
        "Shrikanth Narayanan"
      ],
      "year": "2023",
      "venue": "APSIPA Transactions on Signal and Information Processing",
      "doi": "10.1561/116.00000084"
    },
    {
      "citation_id": "20",
      "title": "Imputing missing data in largescale multivariate biomedical wearable recordings using bidirectional recurrent neural networks with temporal activation regularization",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": "2019",
      "venue": "2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "21",
      "title": "Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling",
      "authors": [
        "Tiantian Feng",
        "Shrikanth Narayanan"
      ],
      "year": "2022",
      "venue": "Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling",
      "arxiv": "arXiv:2203.08810"
    },
    {
      "citation_id": "22",
      "title": "Discovering optimal variablelength time series motifs in large-scale wearable recordings of human biobehavioral signals",
      "authors": [
        "Tiantian Feng",
        "Shrikanth S Narayanan"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "User-Level Differential Privacy against Attribute Inference Attack of Speech Emotion Recognition on Federated Learning",
      "authors": [
        "Tiantian Feng",
        "Raghuveer Peri",
        "Shrikanth Narayanan"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-10060"
    },
    {
      "citation_id": "24",
      "title": "Label inference attacks against vertical federated learning",
      "authors": [
        "Chong Fu",
        "Xuhong Zhang",
        "Shouling Ji",
        "Jinyin Chen",
        "Jingzheng Wu",
        "Shanqing Guo",
        "Jun Zhou",
        "Alex Liu",
        "Ting Wang"
      ],
      "year": "2022",
      "venue": "31st USENIX Security Symposium"
    },
    {
      "citation_id": "25",
      "title": "Towards General Deep Leakage in Federated Learning",
      "authors": [
        "Jiahui Geng",
        "Yongli Mou",
        "Feifei Li",
        "Qing Li",
        "Oya Beyan",
        "Stefan Decker",
        "Chunming Rong"
      ],
      "year": "2021",
      "venue": "Towards General Deep Leakage in Federated Learning",
      "arxiv": "arXiv:2110.09074"
    },
    {
      "citation_id": "26",
      "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
      "authors": [
        "Kristen Grauman",
        "Andrew Westbury",
        "Eugene Byrne",
        "Zachary Chavis",
        "Antonino Furnari",
        "Rohit Girdhar",
        "Jackson Hamburger",
        "Hao Jiang",
        "Miao Liu",
        "Xingyu Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks",
      "authors": [
        "Chaoyang He",
        "Keshav Balasubramanian",
        "Emir Ceyani",
        "Yu Rong",
        "Peilin Zhao",
        "Junzhou Huang",
        "Murali Annavaram",
        "Salman Avestimehr"
      ],
      "year": "2021",
      "venue": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks"
    },
    {
      "citation_id": "28",
      "title": "FedML: A Research Library and Benchmark for Federated Machine Learning",
      "authors": [
        "Chaoyang He",
        "Songze Li",
        "Jinhyun So",
        "Mi Zhang",
        "Hongyi Wang",
        "Xiaoyang Wang",
        "Praneeth Vepakomma",
        "Abhishek Singh",
        "Hang Qiu",
        "Li Shen",
        "Peilin Zhao",
        "Yan Kang",
        "Yang Liu",
        "Ramesh Raskar",
        "Qiang Yang",
        "Murali Annavaram",
        "Salman Avestimehr"
      ],
      "year": "2020",
      "venue": "FedML: A Research Library and Benchmark for Federated Machine Learning",
      "arxiv": "arXiv:2007.13518"
    },
    {
      "citation_id": "29",
      "title": "Fedcv: a federated learning framework for diverse computer vision tasks",
      "authors": [
        "Chaoyang He",
        "Dilipbhai Alay",
        "Zhenheng Shah",
        "Tang ; Keerti",
        "Mita Bhogaraju",
        "Li Shimpi",
        "Xiaowen Shen",
        "Mahdi Chu",
        "Salman Soltanolkotabi",
        "Avestimehr"
      ],
      "year": "2021",
      "venue": "Fedcv: a federated learning framework for diverse computer vision tasks",
      "arxiv": "arXiv:2111.11066"
    },
    {
      "citation_id": "30",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "Menglong Andrew G Howard",
        "Bo Zhu",
        "Dmitry Chen",
        "Weijun Kalenichenko",
        "Tobias Wang",
        "Marco Weyand",
        "Hartwig Andreetto",
        "Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "31",
      "title": "Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data",
      "authors": [
        "Sohei Itahara",
        "Takayuki Nishio",
        "Yusuke Koda",
        "Masahiro Morikura",
        "Koji Yamamoto"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Mobile Computing"
    },
    {
      "citation_id": "32",
      "title": "Perceiver: General perception with iterative attention",
      "authors": [
        "Andrew Jaegle",
        "Felix Gimeno",
        "Andy Brock",
        "Oriol Vinyals",
        "Andrew Zisserman",
        "Joao Carreira"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "33",
      "title": "Rachel Cummings, et al. 2021. Advances and open problems in federated learning",
      "authors": [
        "Peter Kairouz",
        "Brendan Mcmahan",
        "Brendan Avent",
        "Aur√©lien Bellet",
        "Mehdi Bennis",
        "Nitin Arjun",
        "Kallista Bhagoji",
        "Zachary Bonawitz",
        "Graham Charles",
        "Cormode"
      ],
      "year": "2021",
      "venue": "Foundations and Trends¬Æ in Machine Learning"
    },
    {
      "citation_id": "34",
      "title": "Fedcvt: Semi-supervised vertical federated learning with cross-view training",
      "authors": [
        "Yan Kang",
        "Yang Liu",
        "Xinle Liang"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "35",
      "title": "Scaffold: Stochastic controlled averaging for federated learning",
      "authors": [
        "Praneeth Sai",
        "Satyen Karimireddy",
        "Mehryar Kale",
        "Sashank Mohri",
        "Reddi"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "36",
      "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "authors": [
        "Douwe Kiela",
        "Hamed Firooz",
        "Aravind Mohan",
        "Vedanuj Goswami",
        "Amanpreet Singh",
        "Pratik Ringshia",
        "Davide Testuggine"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "37",
      "title": "Federated learning: Strategies for improving communication efficiency",
      "authors": [
        "Jakub Koneƒçn·ª≥",
        "Brendan Mcmahan",
        "Felix Yu",
        "Peter Richt√°rik",
        "Ananda Theertha Suresh",
        "Dave Bacon"
      ],
      "year": "2016",
      "venue": "Federated learning: Strategies for improving communication efficiency",
      "arxiv": "arXiv:1610.05492"
    },
    {
      "citation_id": "38",
      "title": "FedScale: Benchmarking model and system performance of federated learning",
      "authors": [
        "Fan Lai",
        "Yinwei Dai",
        "Xiangfeng Zhu",
        "Mosharaf Harsha V Madhyastha",
        "Chowdhury"
      ],
      "year": "2021",
      "venue": "Proceedings of the First Workshop on Systems Challenges in Reliable and Secure Federated Learning"
    },
    {
      "citation_id": "39",
      "title": "Deep learning",
      "authors": [
        "Yann Lecun",
        "Yoshua Bengio",
        "Geoffrey Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "40",
      "title": "Federated optimization in heterogeneous networks",
      "authors": [
        "Tian Li",
        "Anit Kumar Sahu",
        "Manzil Zaheer",
        "Maziar Sanjabi",
        "Ameet Talwalkar",
        "Virginia Smith"
      ],
      "year": "2020",
      "venue": "Proceedings of Machine Learning and Systems"
    },
    {
      "citation_id": "41",
      "title": "Fedrs: Federated learning with restricted softmax for label distribution non-iid data",
      "authors": [
        "Xin-Chun Li",
        "De-Chuan Zhan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "42",
      "title": "Multibench: Multiscale benchmarks for multimodal representation learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Zetian Wu",
        "Yun Cheng",
        "Jason Wu",
        "Leslie Chen",
        "Peter Wu",
        "Michelle Lee",
        "Yuke Zhu"
      ],
      "year": "2021",
      "venue": "Multibench: Multiscale benchmarks for multimodal representation learning",
      "arxiv": "arXiv:2107.07502"
    },
    {
      "citation_id": "43",
      "title": "Fednlp: Benchmarking federated learning methods for natural language processing tasks",
      "authors": [
        "Chaoyang Bill Yuchen Lin",
        "Zihang He",
        "Hulin Zeng",
        "Yufen Wang",
        "Mahdi Huang",
        "Xiang Soltanolkotabi",
        "Salman Ren",
        "Avestimehr"
      ],
      "year": "2021",
      "venue": "Fednlp: Benchmarking federated learning methods for natural language processing tasks",
      "arxiv": "arXiv:2104.08815"
    },
    {
      "citation_id": "44",
      "title": "Ensemble distillation for robust model fusion in federated learning",
      "authors": [
        "Tao Lin",
        "Lingjing Kong",
        "Sebastian Stich",
        "Martin Jaggi"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Hierarchical question-image co-attention for visual question answering",
      "authors": [
        "Jiasen Lu",
        "Jianwei Yang",
        "Dhruv Batra",
        "Devi Parikh"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "46",
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "Brendan Mcmahan",
        "Eider Moore",
        "Daniel Ramage",
        "Seth Hampson",
        "Blaise Aguera Y Arcas"
      ],
      "year": "2017",
      "venue": "Artificial intelligence and statistics"
    },
    {
      "citation_id": "47",
      "title": "Mobilevit: light-weight, generalpurpose, and mobile-friendly vision transformer",
      "authors": [
        "Sachin Mehta",
        "Mohammad Rastegari"
      ],
      "year": "2021",
      "venue": "Mobilevit: light-weight, generalpurpose, and mobile-friendly vision transformer",
      "arxiv": "arXiv:2110.02178"
    },
    {
      "citation_id": "48",
      "title": "Exploiting unintended feature leakage in collaborative learning",
      "authors": [
        "Luca Melis",
        "Congzheng Song",
        "Emiliano De Cristofaro",
        "Vitaly Shmatikov"
      ],
      "year": "2019",
      "venue": "2019 IEEE Symposium on Security and Privacy (SP)"
    },
    {
      "citation_id": "49",
      "title": "Privacy in deep learning: A survey",
      "authors": [
        "Fatemehsadat Mireshghallah",
        "Mohammadkazem Taram",
        "Praneeth Vepakomma",
        "Abhishek Singh",
        "Ramesh Raskar",
        "Hadi Esmaeilzadeh"
      ],
      "year": "2020",
      "venue": "Privacy in deep learning: A survey",
      "arxiv": "arXiv:2004.12254"
    },
    {
      "citation_id": "50",
      "title": "Moments in time dataset: one million videos for event understanding",
      "authors": [
        "Mathew Monfort",
        "Alex Andonian",
        "Bolei Zhou",
        "Kandan Ramakrishnan",
        "Sarah Bargal",
        "Tom Yan",
        "Lisa Brown",
        "Quanfu Fan",
        "Dan Gutfreund",
        "Carl Vondrick"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "51",
      "title": "Confident learning: Estimating uncertainty in dataset labels",
      "authors": [
        "Curtis Northcutt",
        "Lu Jiang",
        "Isaac Chuang"
      ],
      "year": "2021",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "52",
      "title": "A survey on wearable sensor-based systems for health monitoring and prognosis",
      "authors": [
        "Alexandros Pantelopoulos",
        "G Nikolaos",
        "Bourbakis"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)"
    },
    {
      "citation_id": "53",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "54",
      "title": "A review of wearable sensors and systems with application in rehabilitation",
      "authors": [
        "Shyamal Patel",
        "Hyung Park",
        "Paolo Bonato",
        "Leighton Chan",
        "Mary Rodgers"
      ],
      "year": "2012",
      "venue": "Journal of neuroengineering and rehabilitation"
    },
    {
      "citation_id": "55",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "56",
      "title": "Privacy risks emerging from the adoption of innocuous wearable sensors in the mobile environment",
      "authors": [
        "Andrew Raij",
        "Animikh Ghosh",
        "Santosh Kumar",
        "Mani Srivastava"
      ],
      "year": "2011",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "57",
      "title": "Adaptive federated optimization",
      "authors": [
        "Sashank Reddi",
        "Zachary Charles",
        "Manzil Zaheer",
        "Zachary Garrett",
        "Keith Rush",
        "Jakub Koneƒçn·ª≥",
        "Sanjiv Kumar",
        "Brendan Mcmahan"
      ],
      "year": "2020",
      "venue": "Adaptive federated optimization",
      "arxiv": "arXiv:2003.00295"
    },
    {
      "citation_id": "58",
      "title": "Assemblenet: Searching for multi-stream neural connectivity in video architectures",
      "authors": [
        "Michael S Ryoo",
        "Mingxing Piergiovanni",
        "Anelia Tan",
        "Angelova"
      ],
      "year": "2019",
      "venue": "Assemblenet: Searching for multi-stream neural connectivity in video architectures",
      "arxiv": "arXiv:1905.13209"
    },
    {
      "citation_id": "59",
      "title": "Federated self-supervised learning of multisensor representations for embedded intelligence",
      "authors": [
        "Aaqib Saeed",
        "D Flora",
        "Tanir Salim",
        "Johan Ozcelebi",
        "Lukkien"
      ],
      "year": "2020",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "60",
      "title": "Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "authors": [
        "Victor Sanh",
        "Lysandre Debut",
        "Julien Chaumond",
        "Thomas Wolf"
      ],
      "year": "2019",
      "venue": "Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "arxiv": "arXiv:1910.01108"
    },
    {
      "citation_id": "61",
      "title": "A federated learning aggregation algorithm for pervasive computing: Evaluation and comparison",
      "authors": [
        "Francois Ek Sannara",
        "Philippe Portet",
        "Lalanda",
        "Vega German"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Pervasive Computing and Communications (PerCom)"
    },
    {
      "citation_id": "62",
      "title": "KU-HAR: An open dataset for heterogeneous human activity recognition",
      "authors": [
        "Niloy Sikder",
        "Abdullah-Al Nahid"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "63",
      "title": "A dataset of 101 human actions classes from videos in the wild",
      "authors": [
        "Khurram Soomro",
        "Mubarak Amir Roshan Zamir",
        "Shah"
      ],
      "year": "2012",
      "venue": "A dataset of 101 human actions classes from videos in the wild",
      "arxiv": "arXiv:1212.0402"
    },
    {
      "citation_id": "64",
      "title": "Deep learning for ECG analysis: Benchmarks and insights from PTB-XL",
      "authors": [
        "Nils Strodthoff",
        "Patrick Wagner",
        "Tobias Schaeffter",
        "Wojciech Samek"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "65",
      "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
      "authors": [
        "Zhiqing Sun",
        "Hongkun Yu",
        "Xiaodan Song",
        "Renjie Liu",
        "Yiming Yang",
        "Denny Zhou"
      ],
      "year": "2020",
      "venue": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
      "arxiv": "arXiv:2004.02984"
    },
    {
      "citation_id": "66",
      "title": "FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings",
      "authors": [
        "Jean Ogier Du Terrail",
        "Samy-Safwan Ayed",
        "Edwige Cyffers",
        "Felix Grimberg",
        "Chaoyang He",
        "Regis Loeb",
        "Paul Mangold",
        "Tanguy Marchand",
        "Othmane Marfoq",
        "Erum Mushtaq"
      ],
      "year": "2022",
      "venue": "FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings",
      "arxiv": "arXiv:2210.04620"
    },
    {
      "citation_id": "67",
      "title": "Privacypreserving Speech Emotion Recognition through Semi-Supervised Federated Learning",
      "authors": [
        "Vasileios Tsouvalas",
        "Tanir Ozcelebi",
        "Nirvana Meratnia"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)"
    },
    {
      "citation_id": "68",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Å Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "69",
      "title": "PTB-XL, a large publicly available electrocardiography dataset",
      "authors": [
        "Patrick Wagner",
        "Nils Strodthoff",
        "Ralf-Dieter Bousseljot",
        "Dieter Kreiseler",
        "Fatima Lunze",
        "Wojciech Samek",
        "Tobias Schaeffter"
      ],
      "year": "2020",
      "venue": "Scientific data"
    },
    {
      "citation_id": "70",
      "title": "A survey on large-scale machine learning",
      "authors": [
        "Meng Wang",
        "Weijie Fu",
        "Xiangnan He",
        "Shijie Hao",
        "Xindong Wu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "71",
      "title": "FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",
      "authors": [
        "Zhen Wang",
        "Weirui Kuang",
        "Yuexiang Xie",
        "Liuyi Yao",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "year": "2022",
      "venue": "Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "72",
      "title": "Federated learning with differential privacy: Algorithms and performance analysis",
      "authors": [
        "Kang Wei",
        "Jun Li",
        "Ming Ding",
        "Chuan Ma",
        "Howard Yang",
        "Farhad Farokhi",
        "Jin Shi",
        "Tony Quek",
        "H Vincent Poor"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "73",
      "title": "FederatedScope: A Comprehensive and Flexible Federated Learning Platform via Message Passing",
      "authors": [
        "Yuexiang Xie",
        "Zhen Wang",
        "Daoyuan Chen",
        "Dawei Gao",
        "Liuyi Yao",
        "Weirui Kuang",
        "Yaliang Li",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "year": "2022",
      "venue": "FederatedScope: A Comprehensive and Flexible Federated Learning Platform via Message Passing"
    },
    {
      "citation_id": "74",
      "title": "A unified framework for multi-modal federated learning",
      "authors": [
        "Baochen Xiong",
        "Xiaoshan Yang",
        "Fan Qi",
        "Changsheng Xu"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "75",
      "title": "Secure Federated Learning against Model Poisoning Attacks via Client Filtering",
      "authors": [
        "Duygu Yaldiz",
        "Tuo Zhang",
        "Salman Avestimehr"
      ],
      "year": "2023",
      "venue": "Secure Federated Learning against Model Poisoning Attacks via Client Filtering"
    },
    {
      "citation_id": "76",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Zichao Yang",
        "Diyi Yang",
        "Chris Dyer",
        "Xiaodong He",
        "Alex Smola",
        "Eduard Hovy"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies"
    },
    {
      "citation_id": "77",
      "title": "Multimodal Federated Learning via Contrastive Representation Ensemble",
      "authors": [
        "Qiying Yu",
        "Yimu Wang",
        "Ke Xu",
        "Yang Liu",
        "Jingjing Liu"
      ],
      "year": "2023",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "78",
      "title": "Federated unsupervised representation learning",
      "authors": [
        "Fengda Zhang",
        "Kun Kuang",
        "Zhaoyang You",
        "Tao Shen",
        "Jun Xiao",
        "Yin Zhang",
        "Chao Wu",
        "Yueting Zhuang",
        "Xiaolin Li"
      ],
      "year": "2020",
      "venue": "Federated unsupervised representation learning",
      "arxiv": "arXiv:2010.08982"
    },
    {
      "citation_id": "79",
      "title": "FedAudio: A Federated Learning Benchmark for Audio Tasks",
      "authors": [
        "Tuo Zhang",
        "Tiantian Feng",
        "Samiul Alam",
        "Sunwoo Lee",
        "Mi Zhang",
        "Salman Shrikanth S Narayanan",
        "Avestimehr"
      ],
      "year": "2022",
      "venue": "FedAudio: A Federated Learning Benchmark for Audio Tasks",
      "arxiv": "arXiv:2210.15707"
    },
    {
      "citation_id": "80",
      "title": "Federated Learning for the Internet of Things: Applications, Challenges, and Opportunities",
      "authors": [
        "Tuo Zhang",
        "Lei Gao",
        "Chaoyang He",
        "Mi Zhang",
        "Bhaskar Krishnamachari",
        "Salman Avestimehr"
      ],
      "year": "2021",
      "venue": "IEEE Internet of Things Magazine"
    },
    {
      "citation_id": "81",
      "title": "Improving semisupervised federated learning by reducing the gradient diversity of models",
      "authors": [
        "Zhengming Zhang",
        "Yaoqing Yang",
        "Zhewei Yao",
        "Yujun Yan",
        "Joseph Gonzalez",
        "Kannan Ramchandran",
        "Michael Mahoney"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "82",
      "title": "Semi-supervised federated learning for activity recognition",
      "authors": [
        "Yuchen Zhao",
        "Hanyang Liu",
        "Honglin Li",
        "Payam Barnaghi",
        "Hamed Haddadi"
      ],
      "year": "2020",
      "venue": "Semi-supervised federated learning for activity recognition",
      "arxiv": "arXiv:2011.00851"
    },
    {
      "citation_id": "83",
      "title": "Deep leakage from gradients",
      "authors": [
        "Ligeng Zhu",
        "Song Han"
      ],
      "year": "2020",
      "venue": "Deep leakage from gradients"
    },
    {
      "citation_id": "84",
      "title": "Collaborative unsupervised visual representation learning from decentralized data",
      "authors": [
        "Weiming Zhuang",
        "Xin Gan",
        "Yonggang Wen",
        "Shuai Zhang",
        "Shuai Yi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    }
  ]
}