{
  "paper_id": "2307.12221v1",
  "title": "Fatrer: Full-Attention Topic Regularizer For Accurate And Robust Conversational Emotion Recognition",
  "published": "2023-07-23T04:01:24Z",
  "authors": [
    "Yuzhao Mao",
    "Di Lu",
    "Xiaojie Wang",
    "Yang Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper concentrates on the understanding of interlocutors' emotions evoked in conversational utterances. Previous studies in this literature mainly focus on more accurate emotional predictions, while ignoring model robustness when the local context is corrupted by adversarial attacks. To maintain robustness while ensuring accuracy, we propose an emotion recognizer augmented by a full-attention topic regularizer, which enables an emotion-related global view when modeling the local context in a conversation. A joint topic modeling strategy is introduced to implement regularization from both representation and loss perspectives. To avoid overregularization, we drop the constraints on prior distributions that exist in traditional topic modeling and perform probabilistic approximations based entirely on attention alignment. Experiments show that our models obtain more favorable results than state-of-the-art models, and gain convincing robustness under three types of adversarial attacks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "ERC (Emotion Recognition in Conversations)  [27, 34]  aims to identify hidden emotion states by mining utterance expressions in conversations, and each utterance conveys one type of emotion, such as happiness, anger, sadness, etc. ERC is undeniably essential for enabling a humanoid robot to be empathetic with human emotions  [24, 26] . In addition, ERC technologies can be easily transferred to other dialogue understanding tasks  [10, 45]  for further research.\n\nEmotion dynamics modeling is an important criterion for the ERC task  [20] . Specifically, emotion dynamics assumes that the interlocutors' emotions are influenced by two factors in a conversation: self and interpersonal dependencies  [29] . Studies  [28, 36, 51, 22]  formulate the two factors in a way of hierarchical context modeling for more accurate emotional predictions. However, model robustness  [44]  is rarely touched, especially robustness against adversarial examples. Adversarial examples  [18, 35]  shown in Figure  1  are small and intentional worst-case perturbations to test samples that have high confidence in fooling a target classifier. Compared to speech recognition errors, robustness to adversarial examples can explain the reliability and safety of a target model.\n\nAdversarial attacks arise mainly from local perturbations, and the fundamental issue with weak robustness stems from an excessive dependence on local context. Therefore, a global view is needed to withstand the impact of adversarial attacks. In addition, the global [speaker0]: This is the final last boarding call for Flight 664 to Yemen.\n\nOriginal label: neutral(81%) Adversarial label: neutral(66%)\n\n[speaker1]: Well, I guess assume I gotta have to go.\n\nOriginal label: sadness(46%) Adversarial label: neutral(38%)\n\n[speaker2]: Oh, my Bing-a-ling. I'll wait for you Y0u. Do Could you even know how long you're going to be gone go? Original label: sadness(77%) Adversarial label: anger(47%)\n\n[speaker1]: Well, just until we find discover an energy source to replace fuel fue l.\n\nOriginal label: neutral(81%) Adversarial label: surprise(66%)\n\n[speaker2]: Oh. Well Weil, I'll right you everyday daily.  15  Yemen Road,Yemen.\n\nOriginal label: neutral(56%) Adversarial label: disgust(38%)\n\n[speaker1]: Okay Oaky, good-bye. Good-bye.\n\nOriginal label: sadness(95%) Adversarial label: sadness(91%) Figure  1 : An example of adversarial attacks in ERC view should correlate with hidden emotional states so that accuracy is also ensured. To this end, we try to employ topic models  [4]  to help achieve accurate and robust emotion recognition in conversations. The topic is an important latent semantic factor related to hidden emotional states and is reported to be effective for more accurate emotional predictions in previous studies  [42, 41, 51] . However, these topic models represent documents as topic variables, which only indicate the importance of topics, lacking a global view to ensure robustness. Therefore, the key problem lies in how to enable an emotion-related global view through topic modeling, and how to conduct proper joint modeling to fully exploit the advantages of the emotion-related global view to strike a balance between accuracy and robustness. In this paper, we propose FATRER (Full-Attention Topic Regularizer augmented conversational Emotion Recognizer) for the ERC task and share our insights on the above issues. First, our topic modeling interprets document generation as a twostage process, which is\n\nHere, based on the Law of Total Probability, the conditional probability of words given a document (utterance), p(w|d), is decomposed into topic-word conditional probabilities, p(w|z), and documenttopic conditional probabilities, p(z|d). Traditional topic models only use p(z|d) to represent a document, while ignoring to exploit p(w|z) that is associated with the global vocabulary. The vocabulary is nat-urally robust to local perturbations. Thus, based on the generation process in equation (  1 ), an utterance can be represented as a combination of topic embedding by p(z|d), and p(w|z) is used to combine the whole word embedding into the topic embedding. Through such a two-stage weighted linear combination, we obtain a topicalized representation of an utterance, which is tightly coupled with the entire word embedding and thus has the global views to ensure robustness. Secondly, the global view obtained by topic modeling should not only guarantee robustness but also enhance accuracy. To achieve this, the topics discovered should be emotion-related, i.e. the top words of a topic should relate to a specific emotion, which can lead to more accurate emotional predictions. Thus, we design a training loss including a classification loss term and a topic-oriented regularization term to optimize a full-attention model. The classification loss term is used to model emotion labels. This term drives the attention model to assign more weight to words that are related to the target emotion labels. The topic-oriented regularization term guides p(w|d) to be as similar as possible to the observed word distribution of a given utterance. As we use the attention mechanism to approximate p(z|d) and p(w|z), regularization of p(w|d) will propagate to attention alignment for p(z|d) and p(w|z) and increase sparsity of the two distributions. To sum up, the full-attention framework and the two terms in the training loss help the topics discovered to be discriminative and emotion-related, which are properties towards more accurate emotional predictions.\n\nFinally, we use our FATR (Full-Attention Topic Regularizer) to enable a global view when modeling the local context for the ERC task. Regularization from both representation and loss is conducted. To avoid over-regularization, we drop the constraints on prior distributions that exist in traditional topic modeling. Specifically, we perform our topic modeling to represent an utterance from a global view, obtaining the topicalized representation. On the other hand, we conduct context modeling for representing an utterance from the local view, generating contextualized representation. Such multi-view modeling for representing interlocutor-specific utterances constitutes our approach to self-dependency 1 modeling. Then, the multi-view representations of interlocutors at each conversation turn form a timeseries sequence. Through deep layers of sequence interactions for modeling interpersonal dependency 2 , the multi-view representations are fully balanced and mixed in the final representation. Together with our training loss, the full-attention model is guided toward accurate and robust emotional predictions.\n\nExperimental results show that our models achieve new SOTA (State-Of-The-Art) on four benchmark datasets in terms of Micro F1, and obtain convincing robustness under three types of adversarial attacks. Analysis and visualizations are conducted to better understand our models.\n\nOur contributions can be summarized as:\n\n• We propose a novel full-attention topic regularizer to enable an emotion-related global view when modeling local context for recognizing emotions in conversations. • Our joint topic modeling strategy provides regularization from both representation and loss perspectives for accurate and robust emotional predictions. • We conduct a series of experiments in terms of not only generalization but also robustness to evaluate the performance of our proposed models and baselines on the ERC task.\n\n1 Self dependency is known as emotion inertia manifested as the interlocutor's tendency to maintain their emotional state during the conversation. 2 Interpersonal dependency is an emotional factor from other interlocutors in a conversation that tries to change the emotion of the current interlocutor.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Emotion Recognition in Conversations. Prior research on ERC mostly focuses on exploring different context models, such as Bidirectional LSTM  [32] , memory network  [14] , Transformer  [28] , etc. Later, emotion dynamics  [20]  is summarized to guide the context modeling in ERC. Following this criterion, many hierarchical structures  [27, 11, 28]  are putting forward to capture the self and inter-personal dependencies in conversations. Inspired by the success of the pre-training paradigm  [7] , approaches  [13, 36]  try to implicitly memorize external knowledge into a large number of parameters via the language model pre-training. Other studies  [50, 9]  explicitly extract commonsense knowledge via the ConceptNet  [37] . Very recently,  [51]  proposes a two-stage learned topic-driven knowledgeaware Transformer whose topic module is removed in the fine-tuning stage. Differently, we perform a joint topic modeling strategy with consistent topic guidance. Our strategy not only boosts accuracy but also guarantees robustness. Topic Model. Typically, a statistical topic model  [15, 4]  captures topics in the form of latent variables with probability distributions over the entire vocabulary and performs approximate inference over document-topic and topic-word distributions through Variational Bayes  [3] . However, such a learning paradigm requires an expensive iterative inference step performed on every document in a corpus  [30] . The efficiency is boosted after the introduction of VAE-based (Variational AutoEncoder) neural topic model  [2, 49]  because variational inference can be performed through a single forward pass  [19] . The VAE-based topic models often hypothesize a prior distribution  [38, 8]  that is used to approximate the posterior for latent document-topic distribution by maximizing the Evidence Lower Bound (ELBO) using the reparametrization trick. Differently, our strategy of topic modeling does not have prior constraints. Recently, pre-training models are used to augment neural topic models via enhanced text representation  [2] , knowledge distilling  [16] , or embedding clustering  [39] . Besides VAEs, there are other frameworks of neural topic models including autoregressive models  [12] , and graph neural networks  [47] . Our topic modeling is fully based on attention alignments.\n\nNeural Topic Enhanced Supervised Model. Fitting unsupervised topic representations is sub-optimal for the supervised task. Research has been conducted to build a joint learning framework that integrates neural topic models with supervised learning models  [6, 31, 42, 43, 41] .  [6]  proposes a simple feed-forward net with a max-margin loss to approximate topic-word distribution and a classification loss for the downstream task.  [31]  proposes an attention-based topic module without a regularization term.  [41]  uses document-topic distribution as just a vector to guide the attentive pooling of classification vectors (hidden layers of recurrent nets).  [42]  enables the classification model to be aware of topic information through different kinds of topic inference in auxiliary tasks, but there are no connections to link topic and classification models.  [43]  links a VAE-based topic model to a recurrent net for better representations. Our joint learning framework produces regularization from both representation and loss perspectives and can lead to more accurate and robust predictions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Topicalized Representation",
      "text": "The topicalized representation of an utterance is obtained via the fullattention topic modeling depicted in the bottom right of Figure  2 . We first represent documents (utterances), topics, and words in the embedding space. Then, we use attention alignments to approximate parameters α and β for document-topic and topic-word multinomial distributions, respectively. Finally, we deploy α and β as the weights to construct a topicalized representation of an utterance. A document is represented as a linear combination of topics, and a topic is a linear combination of words. Specifically, let K be the topic number, V be the vocabulary size, and H be the embedding dimension. The topicalized representation of a document is calculated as K k=1 α k z k , or αZ ⊤ in matrix form, where α={α1,\n\nwhere E is initialized from the embedding layer of BERT and is finetuned during training. In the following, z denotes the topic variable, and bold z or Z indicates the topic embedding or embedding matrix.\n\nLet p(z|d) ∼ Multinomial(α), p(w|z) ∼ Multinomial(β), our topic modeling output the document-word posterior distributions via probability inference in equation (  1 ), which is\n\nHere, B = {β k } K k=1 is the topic-word probability matrix. Note that, the attention alignment is natural conditional probabilities of the values given the queries. Thus, α can be approximated via attention alignments between a document as the query and topics as the values. Similarly, β is from alignments between topics as the query and words as the values. The process is defined as,\n\nHere, r bow = γE ⊤ is the bag-of-word combination of local word embedding. γ ∈ R V is the observed word distribution of a document. 1 √ H is the scaling factor  [40] . Through topic modeling, the representation of a document evolves from a combination of local terms, γiE ⊤ , to a combination of global topics, αiZ ⊤ . Topic Embedding Chain Rule. Notice that, computing z needs β and computing β also needs z. Thus, we deploy a chain rule that connects z and β along the training steps. Let T be the number of total training steps, and the chain rule can be formulated as\n\nHere, the computation of z t at the t-th training step relies on β t , and β t at the t-th training step is computed based on z t-1 from previous training step. β can be randomly initiated at the 0-th step, just like the attention alignments that are (almost) equally distributed on all keys at the early training stage. z cannot be random initialized first since computing z depends on E as well. We formulate the learning process in a recursive form,\n\nHere, the symbol ˜indicates that the variable comes from the previous training step, e.g., Z = {z k } K k=1 denotes the previously preserved topic embedding matrix.\n\nRepresentation Underlying Different Topic Hypotheses. We offer two hypotheses, multi-and single-topic, to explain the generation process of a document. The two hypotheses correspond to the two versions of our models.\n\nMulti-Topic: Each document describes a mixture of topics from an underlying topic set. In the multi-topic hypothesis, a document is represented as a weighted sum of the entire topic embedding. Since we have two topic embedding, Z and Z, the model outputs two kinds of document-topic distribution, which is α, by attention alignments between r bow and Z, and α, by attention alignments between r bow and Z. To keep the learned representation stable, we define the following process to represent a document,\n\nHere, Z can be understood as the topic embedding learned during the training steps (long-term info), Z is the updated topic embedding that affected by the current document (short-term info). The topicalized representation r topic is learned in the form of additive gate unit  [1]  that balances the long-term and short-term info. σ and ξ are sigmoid and Leaky ReLU  [46] , respectively. ⊙ denotes the elementwise product.\n\nSingle-Topic: Each document describes a single topic out of an underlying topic set. Under the single-topic hypothesis, a document is represented as one single topic embedding sampled from documenttopic distribution. Let α∼Multinomial(α), α∼Multinomial( α), the topicalized representation is calculated as:\n\nHere, αi, αi ∈ R K are one-hot vectors in which the position of the sampled topic is set to one during training. For inference, we always choose the topic having the highest probability. Finally, our topic modeling process is shortly denoted as:\n\nwhere d is the utterance and Ω is the entire vocabulary. The topicalized representation is the result of full attention on the overall word embedding and thus is less sensitive to local perturbations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Contextualized Representation",
      "text": "The contextualized representation is obtained in the process of hierarchical emotion dynamics modeling. Let {Dn} N n=1 be a corpus of N conversations. Each conversation Dn = {(d λ i i , yi)} Mn i=1 contains Mn utterance-emotion pair, where d λ i i is the i-th utterance spoken by interlocutor λi, and yi is the corresponding emotion. The self dependency modeling captures emotional influence within an interlocutor. We use f self (d λ i i , c λ i i ) to denote the self dependency modeling where c λ i i = {d λτ τ |τ ∈ [1, i), λτ = λi} is λi's historical utterances. The interpersonal dependency modeling, finter, combines the emotional influence across interlocutors. The complete emotion dynamics modeling forms a hierarchical structure, which is\n\nSelf dependency modeling Here, we use BERT  [7]  as the branches of the hierarchical structure to model f self , then the branches are connected to a Transformer  [40]  backbone to model finter. Given an utterance of L words, d = w1 • • • wL, and its historical context with L ′ words, c = ω1 • • • ω L ′ , the input of the BERT can be denoted as,\n\nwhere [CLS] and [SEP] are two special words with specific purposes in BERT. After feeding BERT with X, it outputs the contextualized representations from the last hidden layer at [CLS] position, which is\n\nThe BERT encodes a Mn-turn conversation into a sequence of contextualized representations R={r context i } Mn i=1 . By feeding R to a TRM (TRansFormer), the interpersonal dependency is modeled as,\n\nHere, we use the last hidden layer of TRM at the i-th position as the classification representation for the i-th turn utterance. ηi masks the future information.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Joint Modeling",
      "text": "Given the i-th utterance di, corresponding context ci of the same interlocutor, and a vocabulary set Ω, the process of topic modeling links di and Ω such that the topicalized representation has a global view, and the process of context modeling connects di with ci to enable the contextualized representation with a local view. The regularization from the representation perspective is enforced by simply concatenating the two representations. Specifically, the modeling of emotion dynamics can be reformulated as,\n\nwhere f ′ self is for self-dependency modeling that concatenates output of ftopic and the original f self in equation (  13 ) to form representations with both global and local views. Let r concat be the concatenated representation output by f ′ self , the interpersonal dependency is modeled via feeding the TRM with a representation sequence, R = {r concat i } Mn i=1 . After interpersonal dependency modeling, the global and local views can be fully balanced and mixed through deep layers of multi-head attention within the TRM. {hi} Mn i=1 are the output representation sequence for emotional predictions.\n\nOur joint learning objective contains two terms, a classification loss term ℓCLS and a topic-oriented loss regularization term δREC . Let Y = {1, • • • , |Y|} be the emotion set. The classification loss is to minimize the cross entropy between a sequence of the predicted emotion probabilities {P CLS i ∈ R |Y| } Mn i=1 and a sequence of the ground truth {yi ∈ Y} Mn i=1 :\n\nThe topic-oriented loss regularization term is to minimize the KL (Kullback-Leibler) divergence between the document-word posterior distribution Multinomial(P REC i ) and the observed distribution Multinomial(γi):\n\nγi,j log( γi,j\n\nwhere P REC i is our approximated word probabilities conditioned on the i-th utterance in a conversation.\n\nThe regularization from the loss perspective is enforced by the product between the two loss terms, which is\n\nThe values of these two terms serve as the learning rates for each other, which dynamically and interactively adjust the learning weights during the training process. µ is the global learning rate to control the convergent speed of the model.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets. The benchmark of ERC involves four datasets, including DailyDialog  [23] , IEMOCAP  [5] , MELD  [33] , and EmoryNLP  [48] . The first two are dyadic conversational datasets, and the last two comprise multi-party conversations. IEMOCAP annotates each utterance with one of 6 emotion labels, and the emotion labels are 7 in the other datasets. We follow the standard split  [51]  for training, validation, and testing in the benchmark. The 'neutral' label is not included in DailyDialog's evaluation to avoid category imbalance.  [9]  Adversarial Attacks. Three types of adversarial attacks 3 , including PWWS  [35] , TextFooler  [18] , and TextBugger  [21] , are employed for robustness evaluation. PWWS offers a word-level attack according to the word saliency and the classification probability. TextFooler is reported to have effectiveness to attack pre-training models. TextBugger can execute both character-level and word-level attacks. We set all the attackers to perturb up to 25% of words per input. Implementation Details. The single-topic and multi-topic hypotheses yield the FATRER-single and FATRER-multi versions of our model, respectively. The model described in section 3.2 is named as Baseline, in which the BERT is initialized from the BERT Base and the TRM is a random initialized, 6-layer, 12-head-attention, and 768-hidden-unit Transformer encoder. Following the spirit of ProdLDA  [38] , we implement VAE-based topic regularizers underlying different priors. By replacing our topicalized representation with topic variables obtained via variational inference, we have VAEbased variants, including VAE (Laplace), VAE (LogNormal), VAE (Dirichlet), and VAE (Gamma). Based on the same Baseline, we can perform a fair comparison between FATR-based and VAE-based 3 https://github.com/QData/TextAttack topic regularizers. We use off-the-shelf implementations of TodKat 4  and CoGBART 5  as well as their trained models to perform robustness evaluations in our experimental settings.\n\n5 Results, Analysis, and Visualization",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Generalization On Benchmark Datasets",
      "text": "The generalization results are presented in Table  1 . The listed methods can be divided into two groups according to whether or not latent topics are used. The methods using latent topics can be further categorized as 1) merely pre-trained topics, i.e., TodKat, 2) VAE-based joint learned topics, i.e., VAE (Dirichlet), etc., and 3) our FATR-based topics. We employ Micro F1 to measure the accuracy of the listed models. The results show that FATRER-multi achieves the best performance on the benchmark in terms of Micro F1. Compared with TodKat which applies VAE-based topic modeling only in the pretraining stage, we understand the importance of performing joint topic modeling which provides consistent topic guidance for accurate predictions. Compared to adding VAE-based topic regularizers, we can see that our FATR can obtain better generalization results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Robustness To Adversarial Examples",
      "text": "Based on the fully converged models, the robustness results are presented in   2  and 3 present the robustness results on IEMOCAP and MELD, respectively. For a fair comparison, the ANQ (Average Number of attack Queries) for each model is tuned to be as equal as possible in this experiment. Our models achieve the best results under the three types of adversarial attacks on the two datasets. We conclude four points from the results: 1) Topic-oriented models, e.g., the VAE-based and our FATRERs, get better robustness performance, which means the joint modeling of topic and context already provides a certain degree of robustness. 2) Our FATRERs are superior to VAE-based models, which indicates that our topicalized representation improves robustness beyond the simple use of topic variables.\n\n3) The robustness of our FATRERs is more evident when the conversational context is disrupted, further demonstrating the benefits of  In a word, the superior robustness of our FATRERs mainly comes from the topicalized representation guided by a global view and the full-attention-based joint modeling framework.\n\nIn Figure  3 , we track the trend of after-attack accuracy, attacksuccess rate, and original accuracy (accuracy before the attack) over 340 training epochs. The best performed VAE (Dirichlet) is taken as the representative of VAE-based variants. In this experiment, to exert maximum pressure on the models, we let the attacker PWWS determine the number of attack queries and use U+C attack strategy. From the trend in Figure  3a , we understand that the generalization ability converges quickly within 10 epochs. By contrast, the model robustness in Figure  3b  and 3c keeps getting better until 300 epochs, which means the model robustness needs a long period of training. From the 200th to 300th training epoch, the ANQ of our models is around 800 which is 1.09 to 1.26 times that of the other models. Higher ANQ means the model is more difficult to be successfully attacked. The results show that our models obtain convincing robustness with greater pressures at every epoch of testing.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Analysis",
      "text": "Ablation Study. To better understand our models, we yield several variants of our model by removing some key components. Firstly, FATRER-muti and FATRER-single are two basic variants of our model. -rep regularizer indicates removing the topicalized representation and thus is identical to the Baseline. -loss regularizer means removing the topic-oriented regularization term in the training loss while preserving the attention network for topic modeling, and the topic relative distributions are driven by the classification loss. From the results in Table  4 , we understand that removing either representation or loss regularizer leads to a drop in accuracy and robustness. The model robustness is more sensitive to the removal of the two regularizers than the accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization",
      "text": "Topic words visualization. We evaluate our topics' quality by comparison with unsupervised topics learned from LDA  [4] . By inspecting the top 15 keywords, we depict three topics that are most relative to a kind of emotion in Figure  5a . We mark the emotional words in red. Benefiting from the supervision of emotion labels, our topic model can discover more emotional words than LDA. Such emotionrelated topics can improve accuracy. The top words of our topics have perceptible probabilities while LDA's topics have less",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We propose a topic-augmented conversational emotion recognizer, namely FATRER, for the task of ERC. To cope with the impact of local perturbations, the discovered topic is enabled with an emotionrelated global view based on a full-attention framework. By jointly performing topic and context modeling, our full-attention topic regularizer augmented models can achieve accurate and robust conversational emotion recognition. Experimental results on standard benchmark datasets and adversarial examples have shown the generalization and robustness of our models. Ablation studies are conducted to help understand the effectiveness of our topic regularizers. We also provide visualization to give an intuitive explanation of our models.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: are small",
      "page": 1
    },
    {
      "caption": "Figure 1: An example of adversarial attacks in ERC",
      "page": 1
    },
    {
      "caption": "Figure 2: We first represent documents (utterances), topics, and words in the",
      "page": 2
    },
    {
      "caption": "Figure 2: Joint modeling framework",
      "page": 4
    },
    {
      "caption": "Figure 3: The evaluation uses AAA (After-",
      "page": 5
    },
    {
      "caption": "Figure 3: Results over training epochs on IEMOCAP",
      "page": 6
    },
    {
      "caption": "Figure 4: Results over the number of topics on IEMOCAP",
      "page": 6
    },
    {
      "caption": "Figure 3: , we track the trend of after-attack accuracy, attack-",
      "page": 6
    },
    {
      "caption": "Figure 3: a, we understand that the generalization ability",
      "page": 6
    },
    {
      "caption": "Figure 3: b and 3c keeps getting better until 300 epochs, which",
      "page": 6
    },
    {
      "caption": "Figure 4: , we can see that our models have better robustness per-",
      "page": 7
    },
    {
      "caption": "Figure 5: a. We mark the emotional words",
      "page": 7
    },
    {
      "caption": "Figure 5: Topic visualization.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "HiGRU [17]\nDialGCN [11]\nBERT [7]\nRoBERTa [25]\nDialTRM [28]\nCoGBART [22]": "TodKat* [51]\nVAE (Laplace)\nVAE (LogNormal)\nVAE (Dirichlet)\nVAE (Gamma)",
          "×\n×\n×\n×\n×\n×\n√": "√\n√\n√\n√",
          "58.28%\n54.52%\n51.90%\n33.54%\n60.63%\n56.17%\n53.73%\n33.13%\n-\n63.49%\n54.85%\n41.11%\n-\n63.75%\n54.33%\n40.81%\n68.41%\n64.60%\n56.44%\n37.13%\n64.10%\n63.66%\n54.71%\n37.57%": "57.38%\n61.11%\n53.44%\n32.62%\n68.98%\n65.36%\n55.11%\n39.43%\n68.95%\n65.40%\n56.27%\n37.80%\n69.19%\n65.82%\n56.53%\n40.35%\n68.48%\n64.37%\n56.34%\n37.91%"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gated multimodal networks",
      "authors": [
        "John Arevalo",
        "Thamar Solorio",
        "Manuel Montes-Y Gomez",
        "Fabio González"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "2",
      "title": "Pre-training is a hot topic: Contextualized document embeddings improve topic coherence",
      "authors": [
        "Federico Bianchi",
        "Silvia Terragni",
        "Dirk Hovy"
      ],
      "year": "2020",
      "venue": "Pre-training is a hot topic: Contextualized document embeddings improve topic coherence",
      "arxiv": "arXiv:2004.03974"
    },
    {
      "citation_id": "3",
      "title": "Supervised topic models",
      "authors": [
        "M David",
        "Jon Blei",
        "Mcauliffe"
      ],
      "year": "2008",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "4",
      "title": "Latent dirichlet allocation",
      "authors": [
        "M David",
        "Andrew Y Et Blei",
        "Ng"
      ],
      "year": "2003",
      "venue": "the Journal of machine Learning research"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "A novel neural topic model and its supervised extension",
      "authors": [
        "Ziqiang Cao",
        "Sujian Li",
        "Yang Liu",
        "Wenjie Li",
        "Heng Ji"
      ],
      "year": "2015",
      "venue": "AAAI"
    },
    {
      "citation_id": "7",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "8",
      "title": "Coherence-aware neural topic modeling",
      "authors": [
        "Ran Ding",
        "Ramesh Nallapati",
        "Bing Xiang"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "9",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Findings of EMNLP"
    },
    {
      "citation_id": "10",
      "title": "Utterance-level dialogue understanding: An empirical study",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Utterance-level dialogue understanding: An empirical study",
      "arxiv": "arXiv:2009.13902"
    },
    {
      "citation_id": "11",
      "title": "Dia-logueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "12",
      "title": "Document informed neural autoregressive topic models with distributional prior",
      "authors": [
        "Pankaj Gupta",
        "Yatin Chaudhary",
        "Florian Buettner",
        "Hinrich Schütze"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversations with transfer learning from generative conversation modeling",
      "arxiv": "arXiv:1910.04980"
    },
    {
      "citation_id": "14",
      "title": "Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea"
      ],
      "year": "2018",
      "venue": "EMNLP"
    },
    {
      "citation_id": "15",
      "title": "Probabilistic latent semantic indexing",
      "authors": [
        "Thomas Hofmann"
      ],
      "year": "1999",
      "venue": "ACM SIGIR"
    },
    {
      "citation_id": "16",
      "title": "Improving neural topic models using knowledge distillation",
      "authors": [
        "Alexander Hoyle",
        "Pranav Goel",
        "Philip Resnik"
      ],
      "year": "2020",
      "venue": "Improving neural topic models using knowledge distillation",
      "arxiv": "arXiv:2010.02377"
    },
    {
      "citation_id": "17",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Irwin Michael R Lyu",
        "King"
      ],
      "year": "2019",
      "venue": "Real-time emotion recognition via attention gated hierarchical memory network",
      "arxiv": "arXiv:1911.09075"
    },
    {
      "citation_id": "18",
      "title": "Is bert really robust? natural language attack on text classification and entailment",
      "authors": [
        "Di Jin",
        "Zhijing Jin",
        "Joey Zhou",
        "Peter Szolovits"
      ],
      "year": "2019",
      "venue": "Is bert really robust? natural language attack on text classification and entailment",
      "arxiv": "arXiv:1907.11932"
    },
    {
      "citation_id": "19",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Kingma",
        "Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "20",
      "title": "Emotion dynamics",
      "authors": [
        "Peter Kuppens",
        "Philippe Verduyn"
      ],
      "year": "2017",
      "venue": "Current Opinion in Psychology"
    },
    {
      "citation_id": "21",
      "title": "Textbugger: Generating adversarial text against real-world applications",
      "authors": [
        "Jinfeng Li",
        "Shouling Ji",
        "Tianyu Du",
        "Bo Li",
        "Ting Wang"
      ],
      "year": "2018",
      "venue": "Textbugger: Generating adversarial text against real-world applications",
      "arxiv": "arXiv:1812.05271"
    },
    {
      "citation_id": "22",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "AAAI"
    },
    {
      "citation_id": "23",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "IJCAI"
    },
    {
      "citation_id": "24",
      "title": "Caire: An end-to-end empathetic chatbot",
      "authors": [
        "Zhaojiang Lin",
        "Peng Xu"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "25",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "26",
      "title": "A survey on empathetic dialogue systems",
      "authors": [
        "Yukun Ma",
        "Linh Khanh",
        "Frank Nguyen",
        "Xing"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "27",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "28",
      "title": "Dialoguetrm: Exploring multi-modal emotional dynamics in a conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "Findings of EMNLP"
    },
    {
      "citation_id": "29",
      "title": "How emotions work: The social functions of emotional expression in negotiations",
      "authors": [
        "W Michael",
        "Dacher Morris",
        "Keltner"
      ],
      "year": "2000",
      "venue": "Research in organizational behavior"
    },
    {
      "citation_id": "30",
      "title": "Tan-ntm: Topic attention networks for neural topic mod-eling",
      "authors": [
        "Madhur Panwar",
        "Shashank Shailabh",
        "Milan Aggarwal",
        "Balaji Krishnamurthy"
      ],
      "year": "2020",
      "venue": "ACL"
    },
    {
      "citation_id": "31",
      "title": "Tdam: A topic-dependent attention model for sentiment analysis",
      "authors": [
        "Gabriele Pergola",
        "Lin Gui",
        "Yulan He"
      ],
      "year": "2019",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "32",
      "title": "Contextdependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika"
      ],
      "year": "2017",
      "venue": "ACL"
    },
    {
      "citation_id": "33",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "35",
      "title": "Generating natural language adversarial examples through probability weighted word saliency",
      "authors": [
        "Yihe Shuhuai Ren",
        "Kun Deng",
        "Wanxiang He",
        "Che"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "36",
      "title": "Dialogxl: All-inone xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "AAAI"
    },
    {
      "citation_id": "37",
      "title": "An open multilingual graph of general knowledge/r. speer",
      "authors": [
        "Conceptnet Speer"
      ],
      "year": "2017",
      "venue": "AAAI"
    },
    {
      "citation_id": "38",
      "title": "Autoencoding variational inference for topic models",
      "authors": [
        "Akash Srivastava",
        "Charles Sutton"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "39",
      "title": "Topic modeling with contextualized word representation clusters",
      "authors": [
        "Laure Thompson",
        "David Mimno"
      ],
      "year": "2020",
      "venue": "Topic modeling with contextualized word representation clusters",
      "arxiv": "arXiv:2010.12626"
    },
    {
      "citation_id": "40",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "41",
      "title": "An end-to-end topic-enhanced selfattention network for social emotion classification",
      "authors": [
        "Chang Wang",
        "Bang Wang"
      ],
      "year": "2020",
      "venue": "Proceedings of The Web Conference"
    },
    {
      "citation_id": "42",
      "title": "Sentiment classification in customer service dialogue with topic-aware multi-task learning",
      "authors": [
        "Jiancheng Wang",
        "Jingjing Wang",
        "Changlong Sun"
      ],
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "43",
      "title": "Neural topic model with attention for supervised learning",
      "authors": [
        "Xinyi Wang",
        "Yi Yang"
      ],
      "year": "2020",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "44",
      "title": "Measure and improve robustness in nlp models: A survey",
      "authors": [
        "Xuezhi Wang",
        "Haohan Wang",
        "Diyi Yang"
      ],
      "year": "2022",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "45",
      "title": "Topicka: Generating commonsense knowledge-aware dialogue responses towards the recommended topic fact",
      "authors": [
        "Sixing Wu",
        "Ying Li",
        "Dawei Zhang"
      ],
      "year": "2021",
      "venue": "IJCAI"
    },
    {
      "citation_id": "46",
      "title": "Empirical evaluation of rectified activations in convolutional network",
      "authors": [
        "Bing Xu",
        "Naiyan Wang"
      ],
      "year": "2015",
      "venue": "Empirical evaluation of rectified activations in convolutional network",
      "arxiv": "arXiv:1505.00853"
    },
    {
      "citation_id": "47",
      "title": "Graph attention topic modeling network",
      "authors": [
        "Liang Yang",
        "Fan Wu",
        "Junhua Gu",
        "Chuan Wang",
        "Xiaochun Cao",
        "Di Jin",
        "Yuanfang Guo"
      ],
      "year": "2020",
      "venue": "Proceedings of The Web Conference"
    },
    {
      "citation_id": "48",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "AAAI Workshops"
    },
    {
      "citation_id": "49",
      "title": "Neural topic model via optimal transport",
      "authors": [
        "He Zhao",
        "Dinh Phung",
        "Viet Huynh",
        "Trung Le",
        "Wray Buntine"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "50",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP"
    },
    {
      "citation_id": "51",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "ACL"
    }
  ]
}