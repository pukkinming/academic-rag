{
  "paper_id": "2411.19803v1",
  "title": "A Cross-Corpus Speech Emotion Recognition Method Based On Supervised Contrastive Learning",
  "published": "2024-11-25T07:03:31Z",
  "authors": [
    "Xiang minjie"
  ],
  "keywords": [
    "Speech emotion recognition",
    "Contrastive learning",
    "Self-supervised features"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Research on Speech Emotion Recognition (SER) often faces challenges such as the lack of largescale public datasets and limited generalization capability when dealing with data from different distributions. To solve this problem, this paper proposes a cross-corpus speech emotion recognition method based on supervised contrast learning. The method employs a two-stage fine-tuning process: first, the self-supervised speech representation model is fine-tuned using supervised contrastive learning on multiple speech emotion datasets; then, the classifier is fine-tuned on the target dataset. The experimental results show that the WavLM-based model achieved unweighted accuracy (UA) of 77.41% on the IEMOCAP dataset and 96.49% on the CASIA dataset, outperforming the state-of-the-art results on the two datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recognition of emotion in speech is a key technology in human-computer interaction. With the increasing application of dialogue systems, the demand for SER tasks is also increasing  [1] . General deep learning models often require a large amount of data to achieve good results and strong generalization ability, while in the field of SER, there is a lack of large-scale public datasets, and SER tasks also face the problem of weak generalization ability due to the gap between different languages and speakers  [2] . The emergence of self-supervised learning speech representation models, which are often trained on hundreds of hours of speech recognition datasets and can be used for numerous downstream tasks related to speech, provides a viable solution to these problems. Many studies  [3] [4]  [5]  design fine-tuning algorithms for SER tasks based on pre-trained speech representation models.\n\nBut these studies tend to finetune only a single dataset, ignoring the gains in model performance from using information from SER datasets in different languages.\n\nContrast learning has been used in self-supervised learning in image field in recent years. Positive and negative pairs are important concepts in contrast learning. The goal of contrast learning is to minimize the gap between positive pairs and maximize the gap between negative pairs. In order to make effective use of the data, this paper designs a supervised contrast learning algorithm to further fine-tune the speech representation model, taking speech samples from the same or different languages but with the same emotion as a set of positive pairs, and samples from different languages or different emotions as negative pairs, optimizing the speech representation model by minimizing contrast loss and cosine interval loss. After fine-tuning the speech representation model, a second finetuning is performed on different datasets to obtain the classifier. In order to simplify the experiment, only two SER datasets are considered in this paper, namely the English SER dataset IEMOCAP  [6]  and the Chinese SER dataset CASIA  [7] . The experimental results show that the UA of the model based on the speech representation model Hubert  [8]  on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of the classification model based on WavLM  [9]  on the two datasets are 77.41% and 96.49%, respectively.\n\nThe follow-up arrangement of this paper is as follows: the second chapter introduces the work related to SER task of fine-tuning speech representation model and contrast learning, the third chapter discusses the proposed algorithm in detail, the fourth chapter introduces the experimental setting and analyzes the experimental results, and the fifth chapter summarizes the research results and briefly looks forward to the future research direction.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In recent years, many studies have applied self-supervised learning to speech representation, and speech representation models such as Wav2Vec2.0  [10] , Hubert and WavLM have successively appeared, and these models are often fine-tuned for SER tasks. The simplest method of fine-tuning is to perform supervised training on the SER dataset after freezing some parameters. The features of all time steps need to be aggregated before classification, and many studies use different aggregation methods when fine-tuning. For example, Morais et al.  [11]  used ECAPA-TDNN  [12] , which was originally used to extract speech embedding features, to aggregate features, and Kakouros et al.  [13]  used Attentive Correlation Pooling (ACP) to aggregate time step features. Some studies have proposed new fine-tuning algorithms. For example, Chen et al.  [3]  used Pseudo-label task adaptive pretraining (P-TAPT) to fine-tune speech representation models.\n\nContrast learning is based on the simple idea of learning deep features of objects by asking the model to compare the similarities between similar objects (positive pairs) and the differences between different objects (negative pairs). In the field of speech, in addition to generating positive pairs using image-like enhancement methods, data from different time steps can also be used as positive and negative pairs. For example, Li et al.  [14]  used contrast prediction coding to perform SER, allowing the model to predict the data of several future time steps only through the current and past information, taking the real data of several future time steps as a positive pair and the randomly sampled data as a negative pair. Ulgen et al.  [5]  assumed that emotional information exists in speaker features, and used k-means to cluster speaker features. Positive pairs were samples sampled in the same cluster for the same speaker, while negative pairs were samples sampled in different clusters for the same speaker. Considering the continuity of phonetic emotion, Xu et al.  [15]  took the emotion of the current pronunciation and the previous pronunciation of the same speaker in a dialogue as positive pairs, and the other pairs as negative pairs. In order to extract features from speech, many contrast learning methods also use pre-trained speech representation models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Two-Stage Fine Tuning",
      "text": "The two-stage fine-tuning framework is shown in Figure  1 . The same basic architecture is used in the implementation of the speech representation model. The original speech is first encoded into high-dimensional feature representation by a multi-layer convolution-based speech encoder, and then dimension is enhanced by a feature projection built at the full connection layer, and features are further extracted by the feature encoder of the Transformer architecture. In the first stage of fine tuning, the parameters of the speech encoder and the feature projection are fixed, and only the parameters of the feature encoder are optimized. The self-attention pooling layer refers to the design in Safari et al.  [16] , which uses a simple attention mechanism to aggregate the features of all time steps, assuming that the output of the feature encoder is\n\n, , ,\n\nWhere d c W ∈  is trainable parameters. The structure of the classifier is two fully connected layers, with the ReLU activation function in the middle. The classifier is only used when fine-tuning the classifier in the second stage, and the loss function in the second stage is cross entropy loss. x is the feature of the i-th positive sample, and i n x is the feature of the k-th negative sample. One positive sample in a batch forms a positive pair with other positive samples and a negative pair with all negative samples. The formula for calculating the contrast loss c L is\n\nWhere sim( , ) T x y x y y x = ⋅ ，τ is the hyperparameter, which is 0.07.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cosine Margin Loss",
      "text": "To further narrow the distance between similar samples and avoid the difference between different samples, cosine margin loss is introduced. When calculating the loss, the positive sample equal quantity is split into two parts, called x and y. The loss formula is as follows\n\nWhere α and m are hyperparameters, α is 0.5 and m is 0.4. Finally, the total model loss is the sum of contrast loss and cosine margin loss.\n\n4 Experiment and Result Analysis",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "The two datasets used in this experiment are IEMOCAP and CASIA. IEMOCAP is a common dataset used for SER tasks, and because of the uneven distribution of emotions across the various categories of the IEMOCAP dataset, only four of them are considered: neutral, sad, angry, and happy. CASIA is one of the most commonly used datasets in Chinese speech emotion recognition, containing a total of six emotions, which cover the four emotions in the IEMOCAP dataset , in addition to the two emotions of fear and surprise.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Settings",
      "text": "The speech representation model Hubert 1 and WavLM 2 used in this experiment are both base versions downloaded from huggingface, and the number of parameters is about 95M. When fine-tuning the speech representation model, the pre-trained model parameters are loaded first, the parameters of the speech encoder and feature projection are fixed, and only the parameters of the feature encoder are optimized. The initial learning rate for the first stage of fine tuning is 1e-4, using the Adam optimizer. A total of 50 epochs are trained, starting from the 25th epoch, the learning rate is reduced by half every 5 epochs, the learning rate of the feature encoder is 0.4 times the learning rate, and the batch size is 32. When fine-tuning the classifier in the second stage, the parameters of the speech representation model and the self-attention pooling layer will be loaded, and the parameters of the whole speech representation model will be fixed at the same time, and only the parameters of the self-attention pooling layer and the classifier will be fine-tuned. The learning rate is 5e-4. Adam optimizer will be used to train 10 epochs, and the learning rate will be reduced to 0.1 times of the original after 5 epochs. The batch size is 32. The output dimension of the encoder is 768, and the output dimension of the two fully connected layers in the classifier is 256 and the number of affective categories.\n\n1 https://huggingface.co/facebook/hubert-base-ls960 2 https://huggingface.co/microsoft/wavlm-base-plus\n\nIn order to evaluate the performance of supervised contrast learning, an experiment was also conducted to directly fine-tune the speech representation model (FT). During the experiment, the initial learning rate was 1e-3, and the Adam optimizer was used to train 50 epochs in total. The learning rate became 0.2 times of the original every 20 epochs, and the batch size was 32. For the speech representation model, the parameters of the speech encoder and the feature projection are also fixed, and the parameters of the feature encoder are optimized with only 0.4 times the learning rate.\n\nDuring the training, the 5-fold cross-validation method was adopted, and the two datasets were divided into 5\n\nparts respectively, leaving one part for testing. In the first stage of fine-tuning, only the data used for training was sampled. The final evaluation of the model is the average of the 5 experimental results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Compared With Methods For Fine-Tuning Speech Representation Models",
      "text": "Table  1  shows the performance comparison between the supervised contrast learning fine-tuning method proposed in this paper and other fine-tuning methods in the IEMOCAP dataset. Input features of all models are original speech, excluding multimodal features such as text and video.   It can be seen from the figure that for the original WavLM, the similarity of the negative pair is even higher than that of the positive pair, rising first and then leveling off as the number of feature encoder layers increases.\n\nAfter the fine-tuning of the first stage of WavLM, when the number of layers of the feature encoder is more than 7 layers, the similarity of the positive pair exceeds the similarity of the negative pair, and by the last layer tends to 1, while the similarity of the negative pair decreases and fluctuates around 0.4.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduce a two-stage fine-tuning method based on supervised contrast learning. The first stage fine-tuning is performed on multiple SER datasets, and then the second stage fine-tuning is performed on a single dataset to train the classifier, and finally the optimal results can be obtained on multiple datasets simultaneously. However, the model parameters obtained by this method are relatively large, and the cost of deployment in real application scenarios is relatively high. In the future, the model distillation method can be considered to train the model with smaller parameters. In addition, fine-tuning more datasets to allow the model to learn more can enhance the model's generalization ability. In the future, human-computer interaction may use multimodal information for emotion analysis, and use contrast learning for multi-modal alignment, which can improve the intelligence, emotional understanding ability and user experience of human-computer interaction system.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The same basic architecture is used in the",
      "page": 2
    },
    {
      "caption": "Figure 1: Two-stage fine-tuning frame",
      "page": 3
    },
    {
      "caption": "Figure 2: and figure 3. Firstly, an emotion category e",
      "page": 3
    },
    {
      "caption": "Figure 2: Sampling process of two datasets has e",
      "page": 3
    },
    {
      "caption": "Figure 3: Sampling process when only dataset 1 has e",
      "page": 3
    },
    {
      "caption": "Figure 4: shows the original WavLM (left) and fine-tuned WavLM (right) of the positive and negative samples",
      "page": 6
    },
    {
      "caption": "Figure 4: Feature similarity of the feature encoder output positive and negative pairs",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Minjie Xiang": "School of Information Science and Engineering, Southeast University；Nanjing China；210089"
        },
        {
          "Minjie Xiang": ""
        },
        {
          "Minjie Xiang": ""
        },
        {
          "Minjie Xiang": ""
        },
        {
          "Minjie Xiang": "two-stage"
        },
        {
          "Minjie Xiang": ""
        },
        {
          "Minjie Xiang": ""
        },
        {
          "Minjie Xiang": ""
        },
        {
          "Minjie Xiang": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "the classification model based on WavLM[9] on the two datasets are 77.41% and 96.49%, respectively."
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "task  of  fine-tuning  speech  representation  model  and  contrast  learning,  the  third  chapter  discusses  the  proposed"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "algorithm in detail, the fourth chapter introduces the experimental setting and analyzes the experimental results, and"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "the fifth chapter summarizes the research results and briefly looks forward to the future research direction."
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "2   Related Work"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "dataset"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "and many studies use different aggregation methods when fine-tuning. For example, Morais et al. [11] used ECAPA-"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "fine-tune speech representation models."
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "extract features from speech, many contrast learning methods also use pre-trained speech representation models."
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "3   Methods"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "3.1 Two-stage fine tuning"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "time steps, assuming that the output of the feature encoder is"
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": ""
        },
        {
          "Hubert[8] on the IEMOCAP dataset and the CASIA dataset are 76.92% and 97.19%, respectively, and the UA of": "self-attention pooling layer is"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "T": "=\n \n \n(1) \nC\nW H\nH\nsoftmax\nc\n(\n)"
        },
        {
          "T": "Where \ncW ∈    is trainable parameters. The structure of the classifier is two fully connected layers, with the"
        },
        {
          "T": "ReLU activation function in the middle. The classifier is only used when fine-tuning the classifier in the second"
        },
        {
          "T": "stage, and the loss function in the second stage is cross entropy loss."
        },
        {
          "T": "Calculate loss\nSpeech representation model"
        },
        {
          "T": "Finetuning speech"
        },
        {
          "T": "Freeze parameters\nrepresentation model"
        },
        {
          "T": "Speech\nFeature \nFeature\nSelf-attention"
        },
        {
          "T": "emotion\nClassifier"
        },
        {
          "T": "Encoder\nEncoder\nPooling\nProjection\nFinetuning"
        },
        {
          "T": "Classifier"
        },
        {
          "T": "Figure 1 Two-stage fine-tuning frame"
        },
        {
          "T": "In  the  first  stage  of  fine-tuning,  the  supervised  contrast  learning  method  was  used  to  optimize  the  model."
        },
        {
          "T": "Samples  of  one  batch  were  sampled  from  two  datasets  in  each  iteration. Assuming  the  batch  size  was  N ,  the"
        },
        {
          "T": "process of sampling positive and negative pairs was shown in figure 2 and figure 3. Firstly, an emotion category  e"
        },
        {
          "T": "is  randomly  determined  from  all  the  emotion  categories  as  the  emotion  category  when  extracting  the  positive"
        },
        {
          "T": "N\n/ 4\n  samples  with  e    are  randomly \nemotion  category.  If  emotion  category  e    exists  in  both  datasets,  then"
        },
        {
          "T": "N\n/ 2\nselected from each dataset. If  e   appears in only one dataset, then randomly sample \n  samples of  e   from"
        },
        {
          "T": "positive  samples.  Negative  samples  are  randomly  selected  from  two  datasets \nthis  dataset,  for  a  total  of \nN\n/ 2"
        },
        {
          "T": "respectively, \n \n \n  negative \nN\n/ 4\nN\n/ 2\n  samples  whose  emotion  category  does  not  belong  to  e  ,  and  a  total  of"
        },
        {
          "T": "samples are collected."
        },
        {
          "T": "N/4 positive"
        },
        {
          "T": "samples"
        },
        {
          "T": "N/2 positive"
        },
        {
          "T": "samples"
        },
        {
          "T": "Dataset 1"
        },
        {
          "T": "Dataset 1"
        },
        {
          "T": "N/4 negative"
        },
        {
          "T": "samples"
        },
        {
          "T": "N/4 negative"
        },
        {
          "T": "samples"
        },
        {
          "T": "N/4 positive"
        },
        {
          "T": "samples"
        },
        {
          "T": "Dataset 2"
        },
        {
          "T": "Dataset 2"
        },
        {
          "T": "N/4 negative \nN/4 negative"
        },
        {
          "T": "samples\nsamples"
        },
        {
          "T": "Figure 2 Sampling process of two datasets has  e  \nFigure 3 Sampling process when only dataset 1 has  e"
        },
        {
          "T": "2.2 Contrast loss"
        },
        {
          "T": "The  contrast  loss  is  InfoNCE  loss,  suppose  x   is  the  feature  obtained  by  aggregation  of  the  self-attention"
        },
        {
          "T": "i p\ni n\n  is the feature of the i-th positive sample, and \npooling layer, \nx\nx   is the feature of the k-th negative sample. One"
        },
        {
          "T": "positive sample in a batch forms a positive pair with other positive samples and a negative pair with all negative"
        },
        {
          "T": "samples. The formula for calculating the contrast loss \ncL   is"
        },
        {
          "T": "i\nj"
        },
        {
          "T": "τ\nx\nx\nexp sim\n,\n/\np\np\n(\n)\n(\n)"
        },
        {
          "T": "=\n \n(2) \n \nl"
        },
        {
          "T": "ij\nN\n/2"
        },
        {
          "T": "i\nj\ni\nk"
        },
        {
          "T": "τ\nτ\nx\nx\nx\nx\nexp sim\n,\n/\nexp sim\n,\n/\np\np\np\nn\n(\n)\n(\n)\n+ ∑\n(\n)\n(\n)"
        },
        {
          "T": "=\nk\n1"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "∑ ∑",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "=\n1\ni",
          "N": "= +\nj",
          "/2": "1"
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "max",
          "/2": "("
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "+",
          "N": "L",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "m",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        },
        {
          "N\n/2": "",
          "N": "",
          "/2": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: shows the performance comparison between the supervised contrast learning fine-tuning method",
      "data": [
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "directly fine-tune the speech representation model (FT). During the experiment, the initial learning rate was 1e-3,"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "and the Adam optimizer was used to train 50 epochs in total. The learning rate became 0.2 times of the original"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "every  20  epochs,  and  the  batch  size  was  32.  For  the  speech  representation  model,  the  parameters  of  the  speech"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "encoder and the feature projection are also fixed, and the parameters of the feature encoder are optimized with only"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "0.4 times the learning rate."
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "During the training, the 5-fold cross-validation method was adopted, and the two datasets were divided into 5"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "parts respectively, leaving one part for testing. In the first stage of fine-tuning, only the data used for training was"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "sampled. The final evaluation of the model is the average of the 5 experimental results."
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "3.3 Compared with methods for fine-tuning speech representation models"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "Table  1  shows  the  performance  comparison  between  the  supervised  contrast  learning  fine-tuning  method"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "proposed in this paper and other fine-tuning methods in the IEMOCAP dataset. Input features of all models are"
        },
        {
          "In  order  to  evaluate  the  performance  of  supervised contrast  learning,  an experiment was  also conducted  to": "original speech, excluding multimodal features such as text and video."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: shows the performance comparison between the supervised contrast learning fine-tuning method",
      "data": [
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "Speech representation model"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "Hubert-base(FT)"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "WavLM-base(FT)"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "Wav2vec2.0(P-TAPT)[3]"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "Hubert(ACP)[13]"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "WavLM(ACP)[13]"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "Hubert(ECAPA-TDNN)[11]"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "Wav2vec2[5]*"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "Hubert-base*"
        },
        {
          "Table 1 Unweighted accuracy UA (%) on the IEMOCAP dataset": "WavLM-base*"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Unweighted accuracy UA (%) on IEMOCAP and CASIA datasets",
      "data": [
        {
          "of  both  datasets  at  the  same  time.  The  fine-tuned  speech  representation  model  is  more  generalized  by  using": ""
        },
        {
          "of  both  datasets  at  the  same  time.  The  fine-tuned  speech  representation  model  is  more  generalized  by  using": "Model"
        },
        {
          "of  both  datasets  at  the  same  time.  The  fine-tuned  speech  representation  model  is  more  generalized  by  using": "Dual-TBNet[17]"
        },
        {
          "of  both  datasets  at  the  same  time.  The  fine-tuned  speech  representation  model  is  more  generalized  by  using": "TIM-Net[18]"
        },
        {
          "of  both  datasets  at  the  same  time.  The  fine-tuned  speech  representation  model  is  more  generalized  by  using": "EmoDARTS[19]"
        },
        {
          "of  both  datasets  at  the  same  time.  The  fine-tuned  speech  representation  model  is  more  generalized  by  using": "Hubert-base"
        },
        {
          "of  both  datasets  at  the  same  time.  The  fine-tuned  speech  representation  model  is  more  generalized  by  using": "WavLM-base"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "deployment  in  real  application  scenarios  is  relatively  high.  In  the  future,  the  model  distillation  method  can  be": "considered to train the model with smaller parameters. In addition, fine-tuning more datasets to allow the model to"
        },
        {
          "deployment  in  real  application  scenarios  is  relatively  high.  In  the  future,  the  model  distillation  method  can  be": "learn more can enhance the model's generalization ability. In the future, human-computer interaction may use multi-"
        },
        {
          "deployment  in  real  application  scenarios  is  relatively  high.  In  the  future,  the  model  distillation  method  can  be": "modal information for emotion analysis, and use contrast learning for multi-modal alignment, which can improve"
        },
        {
          "deployment  in  real  application  scenarios  is  relatively  high.  In  the  future,  the  model  distillation  method  can  be": "the intelligence, emotional understanding ability and user experience of human-computer interaction system."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reference": "Sakurai M, Kosaka T. Emotion recognition combining acoustic and linguistic features based on speech recognition results[C]."
        },
        {
          "Reference": "2021 IEEE 10th Global Conference on Consumer Electronics (GCCE). IEEE, 2021: 824-827."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "Recognition[C]. 2023 6th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)."
        },
        {
          "Reference": "IEEE, 2023: 295-300."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "Speech Emotion Recognition[C]. ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "Reference": "Processing (ICASSP). IEEE, 2024: 12081-12085."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "evaluation, 2008, 42: 335-359."
        },
        {
          "Reference": "Pan S, Tao J, Li Y. The CASIA audio emotion recognition method for audio/visual emotion challenge 2011[C]. Affective"
        },
        {
          "Reference": "Computing and Intelligent Interaction: Fourth International Conference, ACII 2011, Memphis, TN, USA, October 9–12, 2011,"
        },
        {
          "Reference": "Proceedings, Part II. Springer Berlin Heidelberg, 2011: 388-395."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "units[J]. IEEE/ACM transactions on audio, speech, and language processing, 2021, 29: 3451-3460."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "Journal of Selected Topics in Signal Processing, 2022, 16(6): 1505-1518."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "Advances in neural information processing systems, 2020, 33: 12449-12460."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022: 6922-6926."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "based speaker verification[J]. arXiv preprint arXiv:2005.07143, 2020."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "channel-wise correlations and label smoothing[C]. ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and"
        },
        {
          "Reference": "Signal Processing (ICASSP). IEEE, 2023: 1-5."
        },
        {
          "Reference": ""
        },
        {
          "Reference": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021: 6329-6333."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "in Real-Time Conversation[C]. ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "(ICASSP). IEEE, 2024: 11956-11960."
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "[16] Safari P, India M, Hernando J. Self-attention encoding and pooling for speaker recognition[J]. arXiv preprint arXiv:2008.01077,"
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "2020."
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "[17] Liu Z, Kang X, Ren F. Dual-tbnet: Improving the robustness of speech features via dual-transformer-bilstm for speech emotion"
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "recognition[J]. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023, 31: 2193-2203."
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "[18] Ye J, Wen X C, Wei Y, et al. Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion"
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "recognition[C]. ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "IEEE, 2023: 1-5."
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "[19] Rajapakshe T, Rana R, Khalifa S, et al. emodarts: Joint optimisation of cnn & sequential neural network architectures for"
        },
        {
          "[15] Xu Y, Yang M. MCM-CSD: Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition": "superior speech emotion recognition[J]. IEEE Access, 2024."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition combining acoustic and linguistic features based on speech recognition results[C]. 2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)",
      "authors": [
        "M Sakurai",
        "T Kosaka"
      ],
      "year": "2021",
      "venue": "Emotion recognition combining acoustic and linguistic features based on speech recognition results[C]. 2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)"
    },
    {
      "citation_id": "2",
      "title": "Deep implicit distribution alignment networks for cross-corpus speech emotion recognition",
      "authors": [
        "Y Zhao",
        "J Wang",
        "Y Zong"
      ],
      "venue": "Deep implicit distribution alignment networks for cross-corpus speech emotion recognition"
    },
    {
      "citation_id": "3",
      "title": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition[C]",
      "authors": [
        "L Chen",
        "A Rudnicky"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Comprehensive Exploration of Fine-Tuning WavLM for Enhancing Speech Emotion Recognition[C]. 2023 6th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)",
      "authors": [
        "F Ali",
        "A Arymurthy",
        "R Prasojo"
      ],
      "year": "2023",
      "venue": "Comprehensive Exploration of Fine-Tuning WavLM for Enhancing Speech Emotion Recognition[C]. 2023 6th International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)"
    },
    {
      "citation_id": "6",
      "title": "Revealing Emotional Clusters in Speaker Embeddings: A Contrastive Learning Strategy for Speech Emotion Recognition[C]",
      "authors": [
        "Ulgen I R",
        "Z Du",
        "C Busso"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database[J]. Language resources and evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C C Lee"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database[J]. Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "The CASIA audio emotion recognition method for audio/visual emotion challenge 2011",
      "authors": [
        "S Pan",
        "J Tao",
        "Y Li"
      ],
      "venue": "The CASIA audio emotion recognition method for audio/visual emotion challenge 2011"
    },
    {
      "citation_id": "9",
      "title": "Affective Computing and Intelligent Interaction: Fourth International Conference, ACII 2011",
      "year": "2011",
      "venue": "Affective Computing and Intelligent Interaction: Fourth International Conference, ACII 2011"
    },
    {
      "citation_id": "10",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units[J]",
      "authors": [
        "W N Hsu",
        "B Bolte",
        "Y H H Tsai"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "11",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing[J]",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations[J]. Advances in neural information processing systems",
      "authors": [
        "A Baevski",
        "Y Zhou"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations[J]. Advances in neural information processing systems"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using self-supervised features[C]",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
      "arxiv": "arXiv:2005.07143"
    },
    {
      "citation_id": "15",
      "title": "Speech-based emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing[C]",
      "authors": [
        "S Kakouros",
        "T Stafylakis",
        "L Mošner"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Contrastive unsupervised learning for speech emotion recognition[C]",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Multi-Granularity Context Modeling with Contrastive Speaker Detection for Emotion Recognition in Real-Time Conversation[C]",
      "authors": [
        "Y Xu",
        "M Yang",
        "Mcm-Csd"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Self-attention encoding and pooling for speaker recognition",
      "authors": [
        "P Safari",
        "M India",
        "J Hernando"
      ],
      "year": "2020",
      "venue": "Self-attention encoding and pooling for speaker recognition",
      "arxiv": "arXiv:2008.01077"
    },
    {
      "citation_id": "19",
      "title": "Dual-tbnet: Improving the robustness of speech features via dual-transformer-bilstm for speech emotion recognition",
      "authors": [
        "Z Liu",
        "X Kang",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition[C]",
      "authors": [
        "J Ye",
        "X C Wen",
        "Y Wei"
      ],
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "emodarts: Joint optimisation of cnn & sequential neural network architectures for superior speech emotion recognition[J]",
      "authors": [
        "T Rajapakshe",
        "R Rana",
        "S Khalifa"
      ],
      "year": "2024",
      "venue": "emodarts: Joint optimisation of cnn & sequential neural network architectures for superior speech emotion recognition[J]"
    }
  ]
}