{
  "paper_id": "2101.09231v1",
  "title": "Expression Recognition Analysis In The Wild",
  "published": "2021-01-22T17:28:31Z",
  "authors": [
    "Donato Cafarelli",
    "Fabio Valerio Massoli",
    "Fabrizio Falchi",
    "Claudio Gennaro",
    "Giuseppe Amato"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Introduction",
      "text": "Facial Expression Recognition(FER) is one of the most important topic in Human-Computer interactions(HCI)  [3] . In this work we report details and experimental results about a facial expression recognition method based on state-of-the-art methods. We fine-tuned a SeNet deep learning architecture pre-trained on the well-known VGGFace2  [5]  dataset, on the AffWild2  [12]  facial expression recognition dataset. The main goal of this work is to define a baseline for a novel method we are going to propose in the near future. This paper is also required by the Affective Behavior Analysis in-the-wild (ABAW) competition in order to evaluate on the test set this approach. The results reported here are on the validation set and are related on the Expression Challenge part (seven basic emotion recognition) of the competition. We will update them as soon as the actual results on the test set will be published on the leaderboard.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Since 2010, deep learning algorithms have become the most popular and used approach to affect recognition problems  [18] . Among the several deep-learning models available, the Convolutional Neural Network (CNN) is the most popular network model. Kahou et al.  [8]  proposed an Hybrid RNN-CNN framework for propagating information over a sequence using temporal averaging for aggregation in order to detect seven emptions. Jung et al.  [7]  used two different types of CNN to detect seven emotions on CK+  [15]  and MMI  [2]  datasets. The first model extracts temporal appearance features from the image sequences, whereas the second extracts temporal geometry features from temporal facial landmark points. These two models are combined using a new integration method to boost the performance of facial expression recognition. Breuer and Kimmel  [4]  employed CNN visualization techniques to understand a model learned using various FER datasets (CK+  [15] , NovaEmotions  [4] ), and demonstrated the capability of networks trained on emotion detection, across both datasets and various FER-related tasks. D.Kollias et al.  [9]  [20]  [11]  [13] built a large-scale dataset Aff-Wild and proposed AffWildNet to explain why CNN-RNN architectures yielded to the best result. In an additional work Kollias et al.  [14]  presented the extended AffWild database called AffWild2 and proposed a multi-task CNN combined with a recurrent neural network (RNN) for VA and EX recognition.\n\nFinally, the model used for our experiments was obtained from the work provided byu Hu et al  [6] . They proposed an architectural block called Squeeze and Excitation(SE), designed to improve the representational power of a network by enabling it to perform dynamic channel-wise feature re-calibration.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experimental Settings",
      "text": "In this paper we propose a network designed to perform Expression Recognition task, i.e. a network able to detect the seven basic emotion: Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "The Aff-Wild2 dataset  [12]  [14] is the first ever database annotated for all three main behavior tasks: valence-arousal estimation, action unit detection and basic expression classification  [10] . For the purpose of the last task, the dataset consists of 539 videos (collected from YouTube) for a total of 2, 595, 572 frames with 431 subjects, 265 of which are male and 166 female. The annotation was made frame-by-frame by a team of seven experts. Aff-Wild2 is split into three subsets: training, validation and test. Regarding our training set, in order to extend it and to have more data at our dispostal, we merged the Aff-Wild2 training set with the ExpW Dataset  [1]  , which consists in 91,793, manually annotated, faces. So, in the end, our training set consists of 1,004,523 faces.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Procedure",
      "text": "In this section we introduce our method and training procedure for the emotion recognition task.\n\nData Processing As input to our network we used the cropped aligned frames provided by the competition, so the frames for the AffWild2 have all costant 112x112 resolution, while the ExpW images present different resolution faces. As shown in Figure  1 , the train set is highly imbalanced. To address this problem we assigned a weight to each of the classes through the Cross-Entropy Loss function. We used the following formula to calculate the weights: (Number of samples in most common classes)/(Number of samples in every single class). The resulting weights are  (1, 22.92, 37.5, 50.66, 3.47, 5.79, 13.55) , respectively to (Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise). Data augmentation for the data are random horizontal flip and a small changes in brightness (0.4), contrast (0.3), saturation (0.25), hue (0.5).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation And Setup",
      "text": "As mentioned in the introduction we fine-tuned a SE-Net50  [6]  pre-trained on the VGGFace2 dataset. Our framework is implemented with PyTorch  [17] . We setted the mini-batch size to 256 that means that there are 256 samples for each iteration. In order to achieve this in an environment with only one GPU, we divided the mini-batch into 4 parts and we accumulated the gradients. We reshaped the last Fully-Connected layer of the pre-trained model with a new output of size 7, and trained our model with the SGD optimizer  [19] . We used a weight decay of 0.005, a momentum of 0.9 and two different learning rate: one to 0.001 for the last layer and the second to 1e-6 for the rest of the network. Our loss function was the Cross-Entropy loss. Furthermore, we validated our model every 3920 iterations. Finally, we stopped the training on the best validation performance. The results we report have been obtained on the validation set as shown in Table  1 . We used the same evaluation criterion presented in  [10] . Classification of the seven basic expressions is measured by 0.67 × F 1 Score + 0.33 × Total Accuracy (Expression Criterion).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Validation",
      "text": "Our proposed model outperforms the baseline result provided in  [10] , with an F 1 Score of 0.33 and a Total Accuracy of 0.63, for a total of 0.43. We will update this paper as soon as our method will be evaluated by the competition organizers 3  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Future Work",
      "text": "In this work we reported details and experimental results about a facial expression recognition method that we will use as a baseline in a future work we are preparing.\n\nIn fact, our main goal is to propose a multi-resolution approach based on  [16]  to facial expression recognition that will be the focus of a paper we are finalizing.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Training Set Distribution",
      "page": 2
    },
    {
      "caption": "Figure 2: Validation Set Distribution",
      "page": 3
    },
    {
      "caption": "Figure 1: , the train set is highly imbalanced. To address this problem we",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results on Validation Set",
      "data": [
        {
          "Validation Set": "Model",
          "Column_2": "Expression Challenge"
        },
        {
          "Validation Set": "Baseline\nSENet-50",
          "Column_2": "0.36\n0.43"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: We used the same evaluation criterion presented in [10]. Classification",
      "data": [
        {
          "Neutral": "0.75",
          "Anger": "0.09",
          "Disgust": "0.02",
          "Fear": "0.22",
          "Happiness": "0.58",
          "Sadness": "0.27",
          "Surprise": "0.41"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial Expression in-the-Wild (ExpW) Dataset",
      "venue": "Facial Expression in-the-Wild (ExpW) Dataset"
    },
    {
      "citation_id": "2",
      "title": "MMI Facial Expression Database",
      "venue": "MMI Facial Expression Database"
    },
    {
      "citation_id": "3",
      "title": "Face expression recognition and analysis: The state of the art",
      "authors": [
        "V Bettadapura"
      ],
      "year": "2012",
      "venue": "Face expression recognition and analysis: The state of the art"
    },
    {
      "citation_id": "4",
      "title": "A deep learning perspective on the origin of facial expressions",
      "authors": [
        "R Breuer",
        "R Kimmel"
      ],
      "year": "2017",
      "venue": "A deep learning perspective on the origin of facial expressions"
    },
    {
      "citation_id": "5",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "13th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "J Hu",
        "L Shen",
        "S Albanie",
        "G Sun",
        "E Wu"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2015.341"
    },
    {
      "citation_id": "8",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "S Kahou",
        "V Michalski",
        "K Konda",
        "R Memisevic",
        "C Pal"
      ],
      "year": "2015",
      "venue": "https",
      "doi": "10.1145/2818346.2830596"
    },
    {
      "citation_id": "9",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "10",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first abaw 2020 competition"
    },
    {
      "citation_id": "11",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "13",
      "title": "A multi-task learning & generation framework: Valencearousal, action units & primary expressions",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "A multi-task learning & generation framework: Valencearousal, action units & primary expressions",
      "arxiv": "arXiv:1811.07771"
    },
    {
      "citation_id": "14",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface"
    },
    {
      "citation_id": "15",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "16",
      "title": "Cross-resolution learning for face recognition",
      "authors": [
        "F Massoli",
        "G Amato",
        "F Falchi"
      ],
      "year": "2020",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "17",
      "title": "",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z Devito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "18",
      "title": "Deep learning for human affect recognition: Insights and new developments",
      "authors": [
        "P Rouast",
        "M Adam",
        "R Chiong"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing p",
      "doi": "10.1109/TAFFC.2018.2890471"
    },
    {
      "citation_id": "19",
      "title": "An overview of gradient descent optimization algorithms",
      "authors": [
        "S Ruder"
      ],
      "year": "2016",
      "venue": "An overview of gradient descent optimization algorithms"
    },
    {
      "citation_id": "20",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops"
    }
  ]
}