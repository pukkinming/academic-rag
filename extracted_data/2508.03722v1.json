{
  "paper_id": "2508.03722v1",
  "title": "Multimodal Video Emotion Recognition With Reliable Reasoning Priors",
  "published": "2025-07-29T15:55:23Z",
  "authors": [
    "Zhepeng Wang",
    "Yingjian Zhu",
    "Guanghao Dong",
    "Hongzhu Yi",
    "Feng Chen",
    "Xinming Wang",
    "Jun Xie"
  ],
  "keywords": [
    "Video Emotion Recognition",
    "Multimodal Fusion",
    "Relaible Reasoning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study investigates the integration of trustworthy prior reasoning knowledge from MLLMs into multimodal emotion recognition. We employ Gemini to generate fine-grained, modality-separable reasoning traces, which are injected as priors during the fusion stage to enrich cross-modal interactions. To mitigate the pronounced classimbalance in multimodal emotion recognition, we introduce Balanced Dual-Contrastive Learning, a loss formulation that jointly balances interclass and intra-class distributions. Applied to the MER2024 benchmark, our prior-enhanced framework yields substantial performance gains, demonstrating that the reliability of MLLM-derived reasoning can be synergistically combined with the domain adaptability of lightweight fusion networks for robust, scalable emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recent advances in reinforcement learning  [1]  with verified reward (RLVR)  [2]  have accelerated the adoption of reinforcement-learning-based post-training and chain-of-thought  [3]  procedures that follow a Reflection-Answer paradigm across multimodal large language models (MLLMs), gaining great traction. These substantially expand the generalization frontier beyond what standard in-context learning or supervised fine-tuning (SFT)  [4]  can achieve.\n\nEmpirical studies consistently show that embedding a CoT paradigm within MLLMs markedly improves generalization. Nevertheless, this test-time scaling  [5]  strategy also introduces considerable computational overhead during both training and inference, exceeding that of conventional multimodal fusion pipelines. Moreover, identifying the appropriate level of reasoning granularity for different task scenarios remains an open question. Focusing on multimodal video emotion recognition, this work advances two central inquiries: (i) For a well-trained reasoning model, can its strong, generalized reasoning priors be integrated into an LLM-agnostic, multimodal recognition framework? (ii) In multimodal video analysis, how can we conduct reliable and coherent reasoning over the emotional cues conveyed by both the human subjects and their surrounding context?\n\nTo address the aforementioned challenges, we propose a modified multimodal fusion framework. This approach elicits reliable reasoning priors from MLLMs and then fuses these priors into a task-specific multimodal architecture to guide subsequent training. Conceptually, this can be viewed as targeted distillation, where the MLLMs acts as a teacher model at the instructional level and the traditional lightweight networks serve as a student model at the implementation level. Such distillation elevates the performance ceiling of the student model.\n\nOur contributions are as follows:\n\n-We distill high-level reasoning priors from MLLMs into a lightweight multimodal recognition model, enhancing generalization with minimal computational cost. -We introduce a balanced contrastive strategy to address label imbalance and improve emotion class separability in multimodal feature space.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Video Emotion Recognition Development of multimodal fusion and LLMs brings new prospects for video emotion recognition. transformer audio-visual fusion method like AVT-CA  [6]  and TACFN  [7]  which synchronises and filters cross-modal cues for new highs on CMU-MOSEI  [8] , RAVDESS  [9] . Visionlanguage prompting method like EmoVCLIP  [10]  and SoVTP  [11]  utilise prompt tuning and modality dropout in the multimodal fusion process. Also, Video-Emotion-OVR  [12]  and AffectGPT  [13]  shift from closed-set labels to free-form textual emotion descriptions, enriching emotion expression.\n\nVisual Reasoning Model As the CoT progressively propagates the notion of \"System-2\" reasoning  [14] , the research community has begun to focus more intently on inference mechanisms for large multimodal models. Visual-RFT  [15]  has empirically validated the efficacy of RLVR in purely visual settings, while VCTP leverages visual chain-of-thought prompts to enhance zero-shot visual reasoning.  [16] [17] [18]  have further sought to improve the interpretability and trustworthiness of model outputs  [19] . Nevertheless, whether such reasoning traces can be distilled into reliable prior knowledge that meaningfully guides conventional models remains an open and intriguing research question.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Recognition With Reliable Reasoning",
      "text": "Although MLLMs exhibit impressive zero-shot capabilities and strong generalisability on general-purpose benchmarks, they often struggle to achieve high dis-\n\nWhen performing emotion recognition, describing facial expressions through AU patterns as I V := {AU i ∈ A} offers empirical support for labelling decisions, where I generated from MLLMs with Instructions.\n\nAs Gemini model family demonstrates strong multimodal reasoning and generalisation, we employ the Gemini-2.0-exp  [23]  model to analyse representative video samples along three complementary dimensions:\n\n-AU combinations extracted from facial frames I V -Prosodic cues derived from the audio track I A ; -Semantic content from subtitle transcripts I T .\n\nThe model further quantifies the relative contribution of video, audio, and textual channels, enabling an integrated affective judgement.\n\nThe whole reasoning priors can be expressed as\n\nPlease use the acoustic, visual and lexical information to recognize the emotion in the video.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Angry.",
      "text": "Please analyze the character action units in the video, combine the video, audio and dialogs to comprehensively judge the character's emotions, and give the contribution of the features. Then give the final emotion prediction.\n\nIn subsequent training stages, these trustworthy reasoning traces can be incorporated as priors guided dataset, guiding the multimodal fusion network and enhancing its integrative performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Trustworthy Multimodal Fusion Paradigm",
      "text": "Annotating multimodal video-based emotion-recognition corpora is intrinsically difficult, as automated labeling pipelines typically yield sub-par accuracy, whereas human verification is prohibitively expensive. As a result, these datasets characteristically contain a substantial proportion of unlabeled samples.\n\nEach video segment can be decomposed into three complementary information streams-visual frame sequences, acoustic waveforms, and transcript-level dialogue text. After passing each stream through its dedicated modality-specific encoder, we obtain three heterogeneous feature vectors, which are subsequently projected and harmonised within a common latent space by a unified feature projector.\n\nOur multimodal fusion framework is optimised through a two-stage training regimen as Fig  3 :\n\nThe first stage is Large-scale Semi-supervised Pre-training, which leverages both labeled and unlabeled data to learn robust cross-modal representation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dialogue",
      "text": "Exp.\n\nAction Units Audio Exp.  And the second stage is Reliable Prior Guided Tuning, wherein domainspecific priors are incorporated to refine the model for downstream emotionrecognition tasks with enhanced reliability.\n\nDuring the first stage of large-scale pre-training, our objective is to enforce intra-class compactness while simultaneously maximizing inter-class separability across all emotion categories. Therefore, we need to pull the feature vectors of the same category as close as possible, and pull the feature vectors of different categories as far as possible. The contrastive learning strategy of infoNCE can effectively achieve this separability. However, since the labels of the original expression samples are not balanced, we introduce\n\nTo that end, we employ a Balanced Dual Contrastive Learning (BDCL) strategy. In BDCL, we construct two parallel contrastive objectives: the First is inter-modality contrast, which draws together representations of the same video segment across different modalities, and the second is intra-modality contrast, which pushes apart representations originating from different emotion classes within the same modality. Formally, let\n\ndenote the projected features for the visual, audio, and textual streams of a given sample. For a mini-batch B containing N labeled examples, we define the dual contrastive loss as\n\nwhere the weighting hyper-parameters λ inter and λ intra balance the two components.\n\nThe intra-modality term applies an emotion-aware InfoNCE  [24]  loss independently within each modality, using labels to regard features with the same emotion as positives and all others as negatives. For sample i with modality m with same-label positive index set P intra m,i = {j ̸ = i|y i = y j }, the L intra (B) is The inter-modality term is a modified symmetric InfoNCE loss that treats each pair as (m, n) ∈ P inter i = {(z v , z a ), (z v , z t ), (z a , z t )} of the same sample as positives, while all remaining modality pairs in the batch serve as negatives.\n\nThe class-balanced denominator\n\nworks because it implicitly replaces the usual \"pick any negative from the batch\"  [25]  rule with a two-step procedure that first selects a class uniformly and then samples a representation uniformly from that class. This yields a balanced negative distribution in which every category, no matter how rare in the real data, contributes exactly 1/C of the probability mass, as head classes no longer exert a disproportionately large repulsive force, and tail-class embeddings are prevented from collapsing toward the origin.\n\nGiven the scarcity of annotated examples, the training corpus comprises a large proportion of unlabeled instances. After an initial cold-start phase trained solely on the labeled subset, the model iteratively assigns pseudo-labels to the unlabeled data at the conclusion of each epoch. These pseudo-labels are then incorporated as additional supervisory signals, enabling a semi-supervised optimization of the network. Upon completion of stage 1, the resulting model demonstrates a robust capacity for emotion recognition.\n\nWe use reliable emotion priors data generated by Gemini in 3.1 as our stage 2 tuning dataset to distill high-level reasoning priors from MLLMs into a lightweight multi-modal recognition architecture.\n\nIn this stage, we refine reasoning priors into text, audio, and video modalities as e a = Proj(Embeddings(I A ))\n\nThen we can fuse them with corresponding modal information\n\nIn the process, we freeze the modality fusion and linear layer as well as the encoder of each modality, and only train the encoder and fusion used by the pre-introduction prior. They are composed of linear and transformer modules, and at the same time, the modality contribution ratio provided by the prior information is used as the modality fusion weight.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments Setup",
      "text": "The main experiments were conducted on the MER2024 dataset. We use MER2024  [26]  labeled samples and partially MER2023  [27]  test dataset provided by Emotion-Llama  [28]  as our supervised training dataset. All training was done on a single NVIDIA A100. we use clip-vit-large-patch14, chinese-hubert-large, bloom-7b as the feature embedding for visual, audio, and text, respectively. We set λ inter and λ intra both to 0.2 in BDCL, and we follow the infoNCE conventional setting τ to 0.1.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Main Result",
      "text": "A comprehensive empirical comparison between the LLM-based method and the modality-fusion method is conducted on the MER2024-SEMI benchmark's test partition, with the quantitative outcomes summarized in Table  1 .\n\nAlthough LLMs benefit from extensive pre-training on broad, general-domain corpora, they still exhibit marked domain shift when confronted with specialised tasks such as fine-grained emotion recognition. Closed-source systems mismatch to a degree and generally outperform open-source counterparts; however, a nontrivial performance gap remains relative to traditional, task-specific multimodal fusion architectures.\n\n\"Attention\" reports the baseline scores provided by the MER2024 benchmark. And \"Contrastive\" presents the performance obtained after augmenting the architecture with a contrastive-learning objective. When robust domain priors are subsequently infused into this contrastive framework, the model exhibits a pronounced uplift across all evaluation metrics, indicating a substantial enhancement in overall performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablations",
      "text": "Modality Ablations Table  2  presents an ablation analysis that quantifies the individual contributions of each modality to the emotion-recognition task. When the model is restricted to a single input channel, the acoustic pathway consistently yields the highest performance. We attribute this superiority to the pronounced discriminative boundaries embedded in speech-spectral cues, which map affective states onto well-separated regions of the feature space. Moreover, the incorporation of our enhanced, reliability-weighted priors further amplifies this advantage, delivering additional gains in recognition accuracy.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we explored the integration of reliable reasoning priors from MLLMs into lightweight multimodal video emotion recognition frameworks. Reliable priors not only enhance multimodal integration but also serve as a powerful signal for modality-specific feature refinement. To further address the challenge of label imbalance, we incorporate a balanced dual-contrastive learning",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Mapping between Action Units and emotions",
      "page": 3
    },
    {
      "caption": "Figure 2: Reliable Reasoning Process",
      "page": 4
    },
    {
      "caption": "Figure 3: The first stage is Large-scale Semi-supervised Pre-training, which lever-",
      "page": 4
    },
    {
      "caption": "Figure 3: The whole framework of the multimodal fusion paradigm.",
      "page": 5
    },
    {
      "caption": "Figure 4: depicts the sample distributions in",
      "page": 9
    },
    {
      "caption": "Figure 4: Latent feature space comparison.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 2: presents an ablation analysis that quantifies the",
      "data": [
        {
          "Method": "LLM-based Method\nQwen-Audio [29]\nLLaVA-NEXT [30]\nMiniGPT-v2 [31]\nVideo-LLaVA(image) [32] 31.10\nVideo-LLaVA(video) [32]\nVideo-Llama [33]\nGPT-4o\nGemini-2.0-flash",
          "Avg": "31.74\n33.75\n34.47\n35.24\n35.75\n58.43\n71.81",
          "Neutral Angry Happy Sad": "29.20\n0.00\n20.69\n58.85\n58.85\n5.29\n56.44\n82.46",
          "Worried Surprise": "6.12\n0.00\n2.04\n12.97\n0.00\n4.76\n35.23\n61.02"
        },
        {
          "Method": "Modality-fusion Method\nAttention\nContrastive\nOurs",
          "Avg": "77.53\n79.72\n84.68 88.72",
          "Neutral Angry Happy Sad": "76.11\n82.74 87.73\n78.65",
          "Worried Surprise": "32.43\n43.24\n70.83"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: presents an ablation analysis that quantifies the",
      "data": [
        {
          "Modality": "AVT+ Prior 84.68\nAVT\nA\nV\nT",
          "Accuracy F1-Score": "77.53\n74.16\n66.30\n51.89"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Results of different data samples.",
      "data": [
        {
          "Dataset": "MER2024\nMER2024 +4K, all",
          "Size Accuracy F1-score": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "Dejian Daya Guo",
        "Haowei Yang",
        "Junxiao Zhang",
        "Ruoyu Song",
        "Runxin Zhang",
        "Qihao Xu",
        "Shirong Zhu",
        "Peiyi Ma",
        "Xiao Wang",
        "Bi"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "arxiv": "arXiv:2501.12948"
    },
    {
      "citation_id": "2",
      "title": "Pushing frontiers in open language model post-training",
      "authors": [
        "Nathan Lambert",
        "Jacob Morrison",
        "Valentina Pyatkin",
        "Shengyi Huang",
        "Hamish Ivison",
        "Faeze Brahman",
        "Lester James",
        "V Miranda",
        "Alisa Liu",
        "Nouha Dziri",
        "Shane Lyu",
        "Yuling Gu",
        "Saumya Malik",
        "Victoria Graf",
        "Jena Hwang",
        "Jiangjiang Yang",
        "Ronan Le Bras",
        "Oyvind Tafjord",
        "Chris Wilhelm",
        "Luca Soldaini",
        "Noah Smith",
        "Yizhong Wang",
        "Pradeep Dasigi",
        "Hannaneh Hajishirzi"
      ],
      "year": "2024",
      "venue": "Pushing frontiers in open language model post-training",
      "arxiv": "arXiv:2411.15124"
    },
    {
      "citation_id": "3",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "Jason Wei",
        "Xuezhi Wang",
        "Dale Schuurmans",
        "Maarten Bosma",
        "Fei Xia",
        "Ed Chi",
        "V Quoc",
        "Denny Le",
        "Zhou"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "4",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "authors": [
        "Zhe Chen",
        "Weiyun Wang",
        "Yue Cao",
        "Yangzhou Liu",
        "Zhangwei Gao",
        "Erfei Cui",
        "Jinguo Zhu",
        "Shenglong Ye",
        "Zhaoyang Hao Tian",
        "Liu"
      ],
      "year": "2024",
      "venue": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "arxiv": "arXiv:2412.05271"
    },
    {
      "citation_id": "6",
      "title": "Multimodal emotion recognition using audio-video transformer fusion with cross attention",
      "authors": [
        "Joe Dhanith",
        "P Shravan Venkatraman",
        "Vigya Sharma",
        "Santhosh Malarvannan",
        "Modigari Narendra"
      ],
      "year": "2025",
      "venue": "Multimodal emotion recognition using audio-video transformer fusion with cross attention",
      "arxiv": "arXiv:2407.18552"
    },
    {
      "citation_id": "7",
      "title": "Tacfn: Transformer-based adaptive cross-modal fusion network for multimodal emotion recognition",
      "authors": [
        "Feng Liu",
        "Ziwang Fu",
        "Yunlong Wang",
        "Qijian Zheng"
      ],
      "year": "2023",
      "venue": "Artificial Intelligence Research"
    },
    {
      "citation_id": "8",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018)"
    },
    {
      "citation_id": "9",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "10",
      "title": "Multimodal emotion recognition with vision-language prompting and modality dropout",
      "authors": [
        "Anbin Qi",
        "Zhongliang Liu",
        "Xinyong Zhou",
        "Jinba Xiao",
        "Fengrun Zhang",
        "Qi Gan",
        "Ming Tao",
        "Gaozheng Zhang",
        "Lu Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Visual and textual prompts for enhancing emotion recognition in video",
      "authors": [
        "Zhifeng Wang",
        "Qixuan Zhang",
        "Peter Zhang",
        "Wenjia Niu",
        "Kaihao Zhang",
        "Ramesh Sankaranarayana",
        "Sabrina Caldwell",
        "Tom Gedeon"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "12",
      "title": "Ov-mer: Towards open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Haoyu Chen",
        "Lan Chen",
        "Hao Gu",
        "Zhuofan Wen",
        "Shun Chen",
        "Siyuan Zhang",
        "Hailiang Yao",
        "Bin Liu",
        "Rui Liu",
        "Shan Liang",
        "Ya Li",
        "Jiangyan Yi",
        "Jianhua Tao"
      ],
      "year": "2025",
      "venue": "Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)"
    },
    {
      "citation_id": "13",
      "title": "Affectgpt: A new dataset, model, and benchmark for emotion understanding with multimodal large language models",
      "authors": [
        "Zheng Lian",
        "Haoyu Chen",
        "Lan Chen",
        "Haiyang Sun",
        "Licai Sun",
        "Yong Ren",
        "Zebang Cheng",
        "Bin Liu",
        "Rui Liu",
        "Xiaojiang Peng",
        "Jiangyan Yi",
        "Jianhua Tao"
      ],
      "year": "2025",
      "venue": "Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)"
    },
    {
      "citation_id": "14",
      "title": "From system 1 to system 2: A survey of reasoning large language models",
      "authors": [
        "Zhong-Zhi Li",
        "Duzhen Zhang",
        "Ming-Liang Zhang",
        "Jiaxin Zhang",
        "Zengyan Liu",
        "Yuxuan Yao",
        "Haotian Xu",
        "Junhao Zheng",
        "Pei-Jie Wang",
        "Xiuyi Chen"
      ],
      "year": "2025",
      "venue": "From system 1 to system 2: A survey of reasoning large language models",
      "arxiv": "arXiv:2502.17419"
    },
    {
      "citation_id": "15",
      "title": "Visual-rft: Visual reinforcement fine-tuning",
      "authors": [
        "Ziyu Liu",
        "Zeyi Sun",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "year": "2025",
      "venue": "Visual-rft: Visual reinforcement fine-tuning",
      "arxiv": "arXiv:2503.01785"
    },
    {
      "citation_id": "16",
      "title": "Visual programming: Compositional visual reasoning without training",
      "authors": [
        "Tanmay Gupta",
        "Aniruddha Kembhavi"
      ],
      "year": "2022",
      "venue": "Visual programming: Compositional visual reasoning without training",
      "arxiv": "arXiv:2211.11559"
    },
    {
      "citation_id": "17",
      "title": "Visual chain-of-thought prompting for knowledgebased visual reasoning",
      "authors": [
        "Zhenfang Chen",
        "Qinhong Zhou",
        "Yikang Shen",
        "Yining Hong",
        "Zhiqing Sun",
        "Dan Gutfreund",
        "Chuang Gan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 38th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Improving visual commonsense in language models via multiple image generation",
      "authors": [
        "Guy Yariv",
        "Idan Schwartz",
        "Yossi Adi",
        "Sagie Benaim"
      ],
      "year": "2024",
      "venue": "Improving visual commonsense in language models via multiple image generation",
      "arxiv": "arXiv:2406.13621"
    },
    {
      "citation_id": "19",
      "title": "An explainable vision question answer model via diffusion chain-of-thought",
      "authors": [
        "Chunhao Lu",
        "Qiang Lu",
        "Jake Luo"
      ],
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "20",
      "title": "The relation between valence and arousal in subjective experience",
      "authors": [
        "Peter Kuppens",
        "Francis Tuerlinckx",
        "James Russell",
        "Lisa Barrett"
      ],
      "year": "2013",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "21",
      "title": "Arousal, valence, and memory for detail",
      "authors": [
        "Terry Libkuman",
        "Charles Stabler",
        "Hajime Otani"
      ],
      "year": "2004",
      "venue": "Memory"
    },
    {
      "citation_id": "22",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "23",
      "title": "a family of highly capable multimodal models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew Dai",
        "Anja Hauth",
        "Katie Millican"
      ],
      "year": "2023",
      "venue": "a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "24",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "Aaron Van Den Oord",
        "Yazhe Li",
        "Oriol Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "25",
      "title": "Balanced contrastive learning for long-tailed visual recognition",
      "authors": [
        "Jianggang Zhu",
        "Zheng Wang",
        "Jingjing Chen",
        "Yi-Ping Phoebe Chen",
        "Yu-Gang Jiang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "26",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM international conference on multimedia"
    },
    {
      "citation_id": "28",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "29",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "30",
      "title": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models",
      "authors": [
        "Feng Li",
        "Renrui Zhang",
        "Hao Zhang",
        "Yuanhan Zhang",
        "Bo Li",
        "Wei Li",
        "Zejun Ma",
        "Chunyuan Li"
      ],
      "year": "2024",
      "venue": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models",
      "arxiv": "arXiv:2407.07895"
    },
    {
      "citation_id": "31",
      "title": "Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: Large language model as a unified interface for vision-language multi-task learning",
      "authors": [
        "Jun Chen",
        "Deyao Zhu",
        "Xiaoqian Shen",
        "Xiang Li",
        "Zechun Liu",
        "Pengchuan Zhang",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra"
      ],
      "year": "2023",
      "venue": "Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: Large language model as a unified interface for vision-language multi-task learning",
      "arxiv": "arXiv:2310.09478"
    },
    {
      "citation_id": "32",
      "title": "Videollava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Yang Ye",
        "Bin Zhu",
        "Jiaxi Cui",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Videollava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "33",
      "title": "Video-llama: An instruction-tuned audiovisual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audiovisual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    }
  ]
}