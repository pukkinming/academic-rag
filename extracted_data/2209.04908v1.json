{
  "paper_id": "2209.04908v1",
  "title": "Automatic Detection Of Sentimentality From Facial Expressions",
  "published": "2022-09-11T17:36:41Z",
  "authors": [
    "Mina Bishay",
    "Jay Turcot",
    "Graham Page",
    "Mohammad Mavadati"
  ],
  "keywords": [
    "Sentimentality",
    "Facial expressions",
    "AU detection",
    "Ad-level KPIs"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition has received considerable attention from the Computer Vision community in the last 20 years. However, most of the research focused on analyzing the six basic emotions (e.g. joy, anger, surprise), with a limited work directed to other affective states. In this paper, we tackle sentimentality (strong feeling of heartwarming or nostalgia), a new emotional state that has few works in the literature, and no guideline defining its facial markers. To this end, we first collect a dataset of 4.9K videos of participants watching some sentimental and non-sentimental ads, and then we label the moments evoking sentimentality in the ads. Second, we use the ad-level labels and the facial Action Units (AUs) activation across different frames for defining some weak framelevel sentimentality labels. Third, we train a Multilayer Perceptron (MLP) using the AUs activation for sentimentality detection. Finally, we define two new ad-level metrics for evaluating our model performance. Quantitative and qualitative results show promising results for sentimentality detection. To the best of our knowledge this is the first work to address the problem of sentimentality detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Understanding facial expressions are quite important to analyze humans' emotions and non-verbal communications. Automatic Facial Expression Analysis (AFEA) has been an active research area in Computer Vision, as it has gained popularity in several applications like ad testing  [1, 2, 3] , driver state monitoring  [4, 5] , and health care  [6, 7, 8] ). In ad testing, analyzing customers' facial responses gives traders insights about customers engagement, liking, and purchase intent  [1] . However, AFEA is quite limited to the detection of AUs and basic emotions, as collecting and labelling real-world data for other emotional states are quite challenging.\n\nEvoking emotions like sentimentality (an emotion with heartwarming or nostalgic feelings) in commercial ads is an emerging trend in advertising  [9] . Subsequently, there has been a growing interest in studying sentimentality  [10, 11] . In  [10] , McDuff highlighted the prominent AUs in sentimental responses, while in  [11]  McDuff classified the ad media content into 5 classes (informed, inspired, sentimental, amused, persuaded) based on the participants facial responses and some media features. The facial markers of sentimentality has not been defined in the literature, in contrast to the basic emotions that were interpreted from AUs through the Emotional Facial Action Coding System (EMFACS)  [12] . Therefore, it was difficult to directly label and detect sentimentality.\n\nIn this paper, we present a novel methodology for detecting sentimentality. Specifically, we first collect real-world facial responses for participants watching a group of sentimental and non-sentimental ads. Second, we label the moments evoking sentimentality in the ads. Third, using the ad-level labels and an AU detector (predicting 20 different AUs), we filter and categorize the participants' frames in the training set into positive and negative examples. Specifically, frames with active AUs shown during sentimental moments are considered positive sentimentality examples, while other frames negative examples. Finally, we use the frame-level labels and AU predictions for training a MLP, that extracts high-level features on the top of the AU predictions (low-level features) for sentimentality detection. Fig.  1  shows an overview of our architecture. We believe that the proposed methodology can be replicated to other untackled emotions without the need for the exhaustive frame-level labelling, nor predefined facial markers for the target emotion.\n\nFor evaluating our architecture, we define two new adlevel Key Performance Indicators (KPIs), that are based on the ad-level aggregated sentimentality. The first KPI measures how separable are the sentimental and non-sentimental ads in terms of the aggregated sentimentality, while the second measures if the aggregated sentimentality is firing high at the right sentimental moments. Our architecture shows promising qualitative and quantitative results for sentimentality detection. We base our emotion/sentimentality analysis on the recommendations given by Barrett et al. in  [13]  for studying facial movements in real life, sampling across different cultures, and using multiple facial stimuli.\n\nTo the best of our knowledge this is the first work to directly address the problem of sentimentality detection. It is worth noting that in this paper we detect \"sentimentality\", a kind of emotion with heartwarming or nostalgic feelings, from an image capturing participant's facial expression -this is completely different from \"sentiment analysis\" that has a  quite good amount of work in the literature  [14, 15] , and basically detects if a piece of writing (e.g. reviews, survey responses) was positive, negative, or neutral.\n\nThe rest of the paper is organized as follows: In Section 2 and Section 3, we present the sentimentality dataset and the ad-level KPIs used in our analysis, respectively. In Section 4, we introduce the proposed methodology for sentimentality detection. Finally, we draw our conclusions in Section 5.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Sentimentality Dataset",
      "text": "Affectiva/SmartEye in collaboration with global market agencies have collected and analyzed thousands of commercial ads across different markets. For each ad, participants were hired to watch the ad, and then fill a survey about how they feel about the ad. A consent was given by the participants to get video recorded while they were watching the ad. The participants' facial responses in the videos were detected and analyzed to get insights about their level of engagement, liking, and purchase intent  [1] .\n\nFor our analysis, experienced ad testers have selected 33 ads (18 sentimental and 15 non-sentimental), and 4.9K participants' videos to form a dataset for sentimentality. Sentimental ads are evoking sentimentality at some moments, while non-sentimental ads are typically informative, funny, or musical ads. The selected sentimental and non-sentimental ads span different markets (USA, UK, East and South Asia and Latin America), and subsequently the participants' videos used in our analysis have diverse demographics (gender, age band and ethnicity). Note that different participants were recruited for watching the different ads, and those participants were not informed about the emotion the ad is trying to evoke.\n\nThe start and the end of the sentimental moments in the sentimental ads were labelled by 3 labellers, who were labelling by just watching the ads.\n\nWe use 3 sentimental ads for training our model, and 15 sentimental and 15 non-sentimental ads for testing. More ads/samples are needed in the testing as the proposed KPIs are calculated on the top of the aggregated sentimentality across different ads. For the participants' videos, we have around 2.1K videos for the 18 sentimental ads (250 videos for the training ads and 1.85K videos for the testing ads), and 2.8K videos for the 15 non-sentimental ads.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Ad-Level Kpis",
      "text": "Our dataset does not have frame-level ground truth labels for sentimentality, so it is challenging to use the typical framelevel KPIs in our analysis. As the dataset has mainly ad-level labels defining the sentimental and non-sentimental ads, and the sentimental moments in the sentimental ads, we define new KPIs based on the available ad-level labels. Specifically, we aggregate (i.e. average) the participants' predicted sentimentality across each sentimental and non-sentimental ad, in order to get a single sentimentality curve for each ad (Fig.  2  shows the aggregated sentimentality across four ads). Then, we calculate two KPIs that compare the ad-level predicted sentimentality to the ad-level labels.\n\nThe first KPI, named ROC-Ad, uses the area under the ROC curve (ROC-AUC) for measuring how separable are the sentimental and non-sentimental ads. To do so, we first calculate the maximum sentimentality score across the aggregate curve of each ad -this leads to 15 scores across sentimental ads (considered the positive predictions), and 15 scores across non-sentimental ones (the negative predictions). Then, the ROC-AUC is calculated between the positive and negative predictions.\n\nThe second KPI, named ROC-Sent, measures if the model is firing high at the right sentimental moments. Specifically, ROC-Sent uses ROC-AUC for measuring how separable are the sentimental and non-sentimental moments in the sentimental ads. Similar to the ROC-Ad, we calculate the maximum sentimentality score across the 15 sentimental moments (positive predictions), and the 15 non-sentimental moments (negative predictions). Then, the ROC-AUC is calculated between the positive and negative predictions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "In this section we present the proposed methodology for detecting sentimentality. Our model takes as input a frame depicting a face and gives as output a binary label indicating if the face is showing markers of sentimentality or not. The analysis is performed in 3 stages; preprocessing, AU detection, and sentimentality detection. Fig.  1  shows an overview of the whole architecture.\n\nAfter collecting a dataset of diverse participants watching some sentimental and non-sentimental ads, and labeling the moments evoking sentimentality. We first detect and align the participant face. Then, we build an AU detector for analyzing the participant facial expression. The AU predictions are used for defining some weak frame-level sentimentality labels, as well as extracting low-level features from the face image. Finally, we train a MLP using the AU predictions for extracting high-level features for sentimentality detection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing",
      "text": "In order to process each video in our dataset, we first extract the region of interest (i.e. the participant face) at each frame, by using a face detector trained in the wild. To ensure we are including participants who are not distracted from the ad or away from the screen, we only include participants with face coverage (i.e. the percentage of the video frames with a face got detected) ≥ 90% for the next steps. Second, we extract 4 facial landmarks (outer eye corners, nose tip and chin) from the face region. These landmarks are used for aligning the face horizontally to have a zero roll angle. Finally, the aligned faces are scaled to a fixed resolution, and passed as an input to the AU detection architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Au Detection",
      "text": "In this section we describe the dataset, CNN architecture, and experimental settings used for building and training our AU detector. Most of the settings in our AU detection architecture are chosen based on the recommendations given in  [17, 18] . AU Dataset. In the literature, several datasets (e.g. DISFA  [19] , UNBC  [20] ) have been used for training different architectures. However, many of these datasets have relatively limited number of participants, recording conditions, and/or diversity in demographics. In our analysis, we use a large-scale dataset consisting of ∼ 55K videos, that were captured in the wild. The participants in our dataset have diverse age, gender and ethnicity. For the experiments, we divide the dataset into 40.9K videos for training, 5.9K for validation, and 8.2K for testing.\n\nOur large-scale dataset was collected using the web-based approach described in  [21, 22] . The videos were collected worldwide (from 90+ countries) for participants watching commercial ads. The videos were manually annotated for the presence of 20 AUs using trained FACS coders. A part of this dataset was made available to the research community through AM-FED  [21]  and AM-FED+  [22] . Note that the videos in the AU dataset are different from the ones in the sentimentality dataset.\n\nCNN architecture. We treat the AU detection problem as a multi-label classification problem where a single CNN is jointly trained for detecting 20 AUs simultaneously (AUs are given in Table  1 ). The CNN consists of 5 convolutional and 1 fully-connected layers. A max-pooling layer is used after each convolutional layer. The fully-connected layer has 20 sigmoid units representing the predictions of the 20 AUs.\n\nExperimental settings. As we are using a naturalistic dataset, most of the AUs in our dataset are severely imbalanced, having a high ratio of negative to positive examples. In order to avoid biasing the classifier to the most frequent class, we use oversampling to balance the data. The training batches are augmented by flipping, shifting, rotation, etc. The Binary Cross Entropy function is used for calculating the loss.\n\nWe compare the developed AU detector to a widely-used facial analysis toolkit (AFFDEX-SDK  [16] ) on our large testing set. ROC-AUC is used for evaluating the AU detection performance. Table  1  shows the performance across the different AUs for both models. Our AU detector achieves better performance than AFFDEX-SDK for most of the AUs (by ∼4% on average).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Sentimentality Detection",
      "text": "We train a MLP on the top of the AU predictions for detecting sentimentality. For training the MLP, we need to have a set of positive and negative examples for sentimentality. There is no guideline in the literature that has defined the markers of sentimentality -this is in contrast to basic emotions that are described by AUs through EMFACS  [12] . Subsequently, it is challenging to directly label a face dataset for sentimentality. In this paper we use the ad-level labels defining the sentimental moments for extracting positive and negative examples for sentimentality. Specifically, we use the expressive faces shown during sentimental moments as positive exam-  In order to make the positive examples cleaner, we first discard all the frames with no active AUs. Second, we evaluate using the ad-level KPIs how indicative are the 20 detected AUs for sentimentality (i.e. test if the activation of a single AU can be used as a marker for sentimentality). Table  2  shows the results across the different AUs. Results show that most of the AUs are achieving relatively low performance on one or two of the KPIs. Subsequently, sentimentality is potentially expressed by more complex combination of AUs. Based on that, we discard all the positive frames with only one active AU. Eventually, positive examples include frames with ≥2 active AUs during sentimental moments, while the negative examples include any other frame (with or without active AUs) in the non-sentimental moments.\n\nThe MLP consists of 2 Fully-Connected (FC) layers, the first FC has 8 neurons for extracting high-level features on the top of the AU predictions, while the second has 1 neuron for detecting sentimentality. We train the MLP using the positive and negative examples extracted from the participants watching the 3 sentimental ads in the training set. We train the MLP for 100 epochs using the Adam optimizer. For testing, we use 15 sentimental and 15 non-sentimental ads. The detected sentimentality for different participants is aggregated for each ad in the testing set, and then the ad-level KPIs are calculated.\n\nTable  2  shows the ROC-Ad and ROC-Sent achieved by the proposed model. On average our model has better performance than the chance level and the 20 AUs. The MLP combines different AUs to get better representation for sentimentality. Fig.  2  shows the aggregated sentimentality across 2 sentimental and 2 non-sentimental ads from the testing set, as well as the sentimental moments in the ads. The aggregate curves show that our model has relatively higher activation at the sentimental ads than the non-sentimental ones. In addition, the model is firing more accurately at the sentimental moments. Fig.  3  shows some of the faces with positive sentimentality detection (faces belong to Smart Eye employees who have been recorded while watching a sentimental ad). Reviewing the detected faces shows that sentimentality has different combinations of AUs, and is not expressed in the same way across different people.\n\nWe believe that the proposed methodology can be replicated for other emotional states that has no predefined facial markers. In addition, collecting and labelling some stimuli (e.g. ads) evoking the target emotional state can replace the exhaustive frame-level labelling.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work we present a novel methodology for detecting sentimentality. Our architecture consists of 3 steps; a) detecting and aligning the participants' faces, b) detecting 20 different AUs for each frame (low-level features), and c) training a MLP for detecting sentimentality by extracting highlevel features on the top of the AU predictions. For training our model, we first collect a dataset of participants watching some sentimental and non-sentimental ads, and then we label the moments evoking sentimentality in the ads. The ad-level labels along with the frame-level AU activations are used for defining a group of positive and negative examples for training the MLP. We define two new ad-level KPIs for evaluating our model performance, by measuring how separable are the sentimental and non-sentimental ads, and the sentimental and non-sentimental moments. Qualitative and quantitative results show promising results for sentimentality detection.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows an overview of our",
      "page": 1
    },
    {
      "caption": "Figure 1: The proposed architecture for sentimentality detection.",
      "page": 2
    },
    {
      "caption": "Figure 2: The aggregated sentimentality across different sentimental and non-sentimental ads.",
      "page": 2
    },
    {
      "caption": "Figure 2: shows the aggregated sentimentality across four ads). Then,",
      "page": 2
    },
    {
      "caption": "Figure 1: shows an overview",
      "page": 3
    },
    {
      "caption": "Figure 3: The detected positive moments of sentimentality using the proposed model.",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the aggregated sentimentality across",
      "page": 4
    },
    {
      "caption": "Figure 3: shows some of the faces with positive sen-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: shows the ROC-Ad and ROC-Sent achieved by labelsalongwiththeframe-levelAUactivationsareusedfor",
      "data": [
        {
          "Facial\nExpressions": "AFFDEX[16]",
          "AU1": "0.76",
          "AU2": "0.79",
          "AU4": "0.86",
          "AU5": "0.87",
          "AU6": "0.92",
          "AU7": "0.75",
          "AU9": "0.91",
          "AU10": "0.86",
          "AU14": "0.86",
          "AU15": "0.78",
          "AU17": "0.79",
          "AU18": "0.91",
          "AU20": "0.86",
          "AU24": "0.76",
          "AU25": "0.86",
          "AU26": "0.63",
          "AU28": "0.91",
          "Eye\nclosure": "0.92",
          "Smile": "0.94",
          "Smirk": "0.82",
          "Avg": "0.84"
        },
        {
          "Facial\nExpressions": "Ours",
          "AU1": "0.79",
          "AU2": "0.84",
          "AU4": "0.92",
          "AU5": "0.85",
          "AU6": "0.94",
          "AU7": "0.83",
          "AU9": "0.89",
          "AU10": "0.93",
          "AU14": "0.80",
          "AU15": "0.88",
          "AU17": "0.88",
          "AU18": "0.92",
          "AU20": "0.93",
          "AU24": "0.87",
          "AU25": "0.89",
          "AU26": "0.71",
          "AU28": "0.96",
          "Eye\nclosure": "0.91",
          "Smile": "0.97",
          "Smirk": "0.84",
          "Avg": "0.88"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: shows the ROC-Ad and ROC-Sent achieved by labelsalongwiththeframe-levelAUactivationsareusedfor",
      "data": [
        {
          "KPIs": "ROC-Ad",
          "Chance\nlevel": "0.50",
          "AU1": "0.61",
          "AU2": "0.73",
          "AU4": "0.55",
          "AU5": "0.74",
          "AU6": "0.84",
          "AU7": "0.54",
          "AU9": "0.52",
          "AU10": "0.61",
          "AU14": "0.70",
          "AU15": "0.69",
          "AU17": "0.72",
          "AU18": "0.73",
          "AU20": "0.67",
          "AU24": "0.73",
          "AU25": "0.60",
          "AU26": "0.75",
          "AU28": "0.74",
          "Eye\nClosure": "0.43",
          "Smile": "0.66",
          "Smirk": "0.72",
          "Proposed": "0.79"
        },
        {
          "KPIs": "ROC-Sent",
          "Chance\nlevel": "0.50",
          "AU1": "0.44",
          "AU2": "0.58",
          "AU4": "0.41",
          "AU5": "0.36",
          "AU6": "0.36",
          "AU7": "0.46",
          "AU9": "0.32",
          "AU10": "0.52",
          "AU14": "0.48",
          "AU15": "0.47",
          "AU17": "0.36",
          "AU18": "0.40",
          "AU20": "0.51",
          "AU24": "0.40",
          "AU25": "0.35",
          "AU26": "0.54",
          "AU28": "0.48",
          "Eye\nClosure": "0.27",
          "Smile": "0.60",
          "Smirk": "0.50",
          "Proposed": "0.61"
        },
        {
          "KPIs": "Avg",
          "Chance\nlevel": "0.50",
          "AU1": "0.52",
          "AU2": "0.65",
          "AU4": "0.48",
          "AU5": "0.55",
          "AU6": "0.60",
          "AU7": "0.50",
          "AU9": "0.42",
          "AU10": "0.56",
          "AU14": "0.59",
          "AU15": "0.58",
          "AU17": "0.54",
          "AU18": "0.56",
          "AU20": "0.59",
          "AU24": "0.56",
          "AU25": "0.47",
          "AU26": "0.64",
          "AU28": "0.61",
          "Eye\nClosure": "0.35",
          "Smile": "0.63",
          "Smirk": "0.61",
          "Proposed": "0.70"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Predicting ad liking and purchase intent: Large-scale analysis of facial responses to ads",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Applications of automated facial coding in media measurement",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby"
      ],
      "year": "2016",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Understanding consumer attention on mobile devices",
      "authors": [
        "Natalia Efremova",
        "Navid Hajimirza"
      ],
      "venue": "Understanding consumer attention on mobile devices"
    },
    {
      "citation_id": "5",
      "title": "Improving in-car emotion classification by nir database augmentation",
      "authors": [
        "Alexandru Mȃlȃescu",
        "Liviu Cristian Dut ¸u"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "6",
      "title": "Towards facial expression analysis in a driver assistance system",
      "authors": [
        "Torsten Wilhelm"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "7",
      "title": "Automatic detection of adhd and asd from expressive behaviour in rgbd data",
      "authors": [
        "Shashank Jaiswal",
        "F Michel",
        "Valstar"
      ],
      "year": "2017",
      "venue": "Automatic detection of adhd and asd from expressive behaviour in rgbd data"
    },
    {
      "citation_id": "8",
      "title": "Schinet: Automatic estimation of symptoms of schizophrenia from facial behaviour analysis",
      "authors": [
        "Mina Bishay",
        "Petar Palasek",
        "Stefan Priebe",
        "Ioannis Patras"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Can automatic facial expression analysis be used for treatment outcome estimation in schizophrenia?",
      "authors": [
        "Mina Bishay",
        "Stefan Priebe",
        "Ioannis Patras"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "The fine line of sentimental advertising",
      "authors": [
        "Daniel Mcduff",
        "May Amr",
        "Rana Kaliouby"
      ],
      "year": "2015",
      "venue": "The fine line of sentimental advertising"
    },
    {
      "citation_id": "11",
      "title": "Discovering facial expressions for states of amused, persuaded, informed, sentimental and inspired",
      "authors": [
        "Daniel Mcduff"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "12",
      "title": "Largescale affective content analysis: Combining media content features and facial reactions",
      "authors": [
        "Daniel Mcduff",
        "Mohammad Soleymani"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "13",
      "title": "Emfacs-7: Emotional facial action coding system",
      "authors": [
        "Paul Wallace V Friesen",
        "Ekman"
      ],
      "year": "1983",
      "venue": "Emfacs-7: Emotional facial action coding system"
    },
    {
      "citation_id": "14",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "Lisa Feldman",
        "Ralph Adolphs"
      ],
      "year": "2019",
      "venue": "Psychological science in the public interest"
    },
    {
      "citation_id": "15",
      "title": "Sentiment analysis algorithms and applications: A survey",
      "authors": [
        "Walaa Medhat",
        "Ahmed Hassan",
        "Hoda Korashy"
      ],
      "year": "2014",
      "venue": "Ain Shams engineering journal"
    },
    {
      "citation_id": "16",
      "title": "Deep learning for sentiment analysis: A survey",
      "authors": [
        "Lei Zhang",
        "Shuai Wang",
        "Bing Liu"
      ],
      "year": "1253",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "17",
      "title": "Affdex sdk: a cross-platform real-time multi-face expression recognition toolkit",
      "authors": [
        "Daniel Mcduff",
        "Abdelrahman Mahmoud"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 CHI conference extended abstracts on human factors in computing systems"
    },
    {
      "citation_id": "18",
      "title": "Choose settings carefully: Comparing action unit detection at different settings using a large-scale dataset",
      "authors": [
        "Mina Bishay",
        "Ahmed Ghoneim"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "19",
      "title": "Which cnns and training settings to choose for action unit detection? a study based on a large-scale dataset",
      "authors": [
        "Mina Bishay",
        "Ahmed Ghoneim"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "20",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Mahoor"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Painful data: The unbc-mcmaster shoulder pain expression archive database",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn"
      ],
      "year": "2011",
      "venue": "FG 2011. IEEE"
    },
    {
      "citation_id": "22",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "23",
      "title": "Am-fed+: An extended dataset of naturalistic facial expressions collected in everyday settings",
      "authors": [
        "Daniel Mcduff",
        "May Amr",
        "Rana Kaliouby"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}