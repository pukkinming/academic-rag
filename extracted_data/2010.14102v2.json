{
  "paper_id": "2010.14102v2",
  "title": "Emotion Recognition By Fusing Time Synchronous And Time Asynchronous Representations",
  "published": "2020-10-27T07:14:31Z",
  "authors": [
    "Wen Wu",
    "Chao Zhang",
    "Philip C. Woodland"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, a novel two-branch neural network model structure is proposed for multimodal emotion recognition, which consists of a time synchronous branch (TSB) and a time asynchronous branch (TAB). To capture correlations between each word and its acoustic realisation, the TSB combines speech and text modalities at each input window frame and then uses pooling across time to form a single embedding vector. The TAB, by contrast, provides cross-utterance information by integrating sentence text embeddings from a number of context utterances into another embedding vector. The final emotion classification uses both the TSB and the TAB embeddings. Experimental results on the IEMOCAP dataset demonstrate that the two-branch structure achieves state-of-the-art results in 4-way classification with all common test setups. When using automatic speech recognition (ASR) output instead of manually transcribed reference text, it is shown that the cross-utterance information considerably improves robustness against ASR errors. Furthermore, by incorporating an extra class for all the other emotions, the final 5-way classification system with ASR hypotheses can be viewed as a prototype for more realistic emotion recognition systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic emotion recognition (AER) is an essential capability for machines to understand and interact with humans and has attracted much attention due to its wide range of potential applications in e.g. driver monitoring, mental health analysis, spoken dialogue systems and chatbots. Although significant progress has been made  [1] [2] [3] , AER is still a challenging research problem since human emotions are inherently complex, ambiguous, and highly personal. Humans often express their emotions using multiple simultaneous approaches, such as voice characteristics, linguistic content, facial expressions, and body actions, which makes AER by nature a complex multimodal task  [4] [5] [6] . Furthermore, due to the difficulties in data collection, publicly available datasets often do not have enough speakers to properly cover personal variations in emotion expression. Consequently, current research efforts include the use of transfer learning with speech or speaker recognition data  [7, 8] , multi-task learning with gender or speaker classification to model the personal aspects of emotions  [9] , features embedded in multiple modalities, and more powerful model architectures. For instance, various types of acoustic features can be fused with text features derived either from pre-trained word embeddings  [10, 11]  or from a jointly trained neural network component  [12, 13] . Context-dependent hierarchical fusion  [14, 15] , multi-head attention mechanisms  [13] , and multiplicative fusion  [6]  have been applied to emotion recognition. In this paper, we propose a novel deep neural network architecture for AER, which consists of a time synchronous branch (TSB) and a time asynchronous branch (TAB). In the TSB, audio features are fused with the corresponding text at each frame in the input window. This can model not only the speech content and its acoustic realisation, but also their alignments and can help capture prosody. Note that video features could also be optionally included in the TSB. Along with traditional acoustic features, we propose using long-term acoustic features with a 250 millisecond (ms) frame length as an extra type of audio feature. In contrast to the TSB that focuses on modelling the correlations of multimodal features across time, the TAB leverages the transcriptions of the spoken utterances as well as other potential global or utterance-level features. Sentence embeddings relevant to a number of consecutive utterances in the dialogue are derived from a pre-trained model with bidirectional encoder representations from Transformers (BERT)  [16] , and are used as the cross-utterance input to the TAB. Two self-attentive layers  [17]  are used as the separate pooling functions for the TSB and TAB, and the resulting vectors fused for final emotion classification. Experimental results on the widely used IEMOCAP dataset  [18]  show that the proposed structure achieves state-of-the-art results in the usual 4-way classification setup when evaluated with all usual test configurations. Rather than assuming perfect automatic speech recognition (ASR), the use of ASR outputs was also studied, and it was found that using cross-utterance input considerably improves robustness against ASR errors. Finally, a 5-way classification setup is studied, which has an extra class to represent \"frustration\" and all the other emotions in IEMOCAP that are not considered in the 4-way setup.\n\nThe rest of the paper is organised as follows. Section 2 introduces our proposed two-branch structure. Section 3 presents the experimental setup and results related to the model architecture. Further analysis and experiments towards implementing a more realistic AER system is given in Section 4, followed by conclusions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Feature Representation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Features",
      "text": "The audio representation for speech-based AER often includes log Mel filterbank features (FBKs)  [19] . In this paper, 40-dimensional (-d) FBKs with a 10 ms frame duration and 25 ms frame length are used, which is denoted FBK25. FBK features have information about the short-term spectrum but do not contain pitch information that can be important in describing emotional speech  [20]  and is often complementary to FBKs  [21, 22] . The log pitch frequency features with probability-of-voicing-weighted mean subtraction over a 1.5 second window are used along with FBKs  [23] . In addition, we propose using long-term FBK features extracted in the same way as FBK25 apart from a long 250ms frame length, which is denoted as FBK250.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Text Embeddings",
      "text": "The use of vector representations of words and sentences has become a widely-used approach in natural language processing. The global vector (GloVe) is a commonly used method that estimates word embeddings using a global log-bilinear regression model in order to combine the advantages of both global matrix factorization and local context window methods  [24] . In this paper, pre-trained 50-d GloVe embeddings are used in the TSB to encode word-level transcriptions. Recently, pre-trained sentence-level embeddings derived from a large pre-trained language model, such as BERT  [16] , have drawn much attention. We use the pre-trained BERT-base model without fine-tuning to encode the transcription of each single utterance into a 768-d vector, which is used as the input to the TAB.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Structure",
      "text": "The proposed model structure is shown in Fig.  1 , which consists of a TSB that fuses the audio features with the corresponding text information at each time step, as well as a TAB that captures the text information embedded across the transcriptions of a number of consecutive utterances. The TSB structure is similar to that which is often used for speaker embedding extraction  [25] . The TSB uses a fivehead self-attentive layer  [17]  to pool the frame-level vectors across time in the input window, and a time delay neural network with residual connections  [26]  is used as the encoder to derive the framelevel vectors. A modified penalty term is used for the five-head selfattentive layer, with three heads set to obtain more \"spiky\" attention weight distributions and the other two set to obtain \"smoother\" distributions  [25] . In the TSB, the audio features and the corresponding GloVe-based word embeddings are combined at each time step with a simple concatenation operation. This structure exploits the alignment between each speech unit and its acoustic realisation. This could be particularly useful for modelling prosody, including the duration for each speech unit and pitch information, which is important for emotional expressions in spoken language  [27] . Moreover, video information can also be included in the TSB, by simply concatenating the visual features of each video frame with the corresponding audio and text features, and ensuring that the frame rate of the audio features matches the video frame rate. Since we did not observe any performance improvement by including the visual features, possibly due to the quality and style of the videos in IEMOCAP, no videorelated experiments are presented in this paper. More detail of these video-related experiments can be found in  [28] .\n\nWhile the TSB includes modelling the temporal correlations between different modalities, the TAB focuses on capturing text information including meaning from the speech transcriptions. The BERT-derived sentence embeddings of the utterance transcriptions are used as the input vectors to TAB. The embeddings for a number of consecutive utterances were used as the TAB input since the emotion of each utterance is often strongly related to its context in a spoken dialogue  [29] . The following text snippets illustrate the importance of context in determining emotional content: \"Did you pass the exam?\" \"Yes, exactly.\" (happy) \"Did you fail the exam?\" \"Yes, exactly.\" (frustrated)\n\nIt is clear that the two different examples of \"Yes, exactly.\" convey very different emotions, and hence determining the emotion from the text requires the context. Furthermore, since cross-utterance information provides extra information for the emotional content, it is possible to improve the AER system robustness when using erroneous transcriptions derived from an ASR system. In the TAB, a shared fully-connected (FC) layer is used to reduce the dimension of each input BERT embedding, and the resulting vectors are then integrated by another five-head self-attentive layer, whose attentionweight distribution reflects the extent to which sentences in the context affect the current emotion.\n\nFinally, the output vectors from both branches are fused using an FC layer for emotion classification. The hidden and output activation functions are ReLU and softmax respectively, and a large-margin softmax loss function is used to avoid over-fitting  [30] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setup",
      "text": "The IEMOCAP  [18]  corpus used in this paper is a multimodal dyadic conversational dataset. It consists of approximately 12 hours of multimodal data, including speech, text transcriptions and facial recordings. IEMOCAP contains a total of 5 sessions and 10 different speakers, with a session being a conversation of two exclusive speakers. To be consistent and to be able to compare with previous studies, only utterances with ground truth labels belonging to \"angry\", \"happy\", \"excited\", \"sad\", and \"neutral\" were used. The \"excited\" class was merged with \"happy\" to better balance the size of each emotion class, which results in a total of 5,531 utterances (happy 1,636, angry 1,103, sad 1,084, neutral 1,708).\n\nUnless otherwise stated, leave-one-session-out 5-fold cross validation (CV) is used and the average result reported. At each fold of the 5-fold CV set-up, 8 speakers are used for training while the other two are used for testing. Since the test sets are slightly imbalanced between different emotion categories, both the weighted accuracy (WA) and unweighted accuracy (UA) are reported. Models were implemented using HTK  [31]  in combination with PyTorch. The newbob learning rate scheduler with an initial learning rate of 5 × 10 -5 was used throughout training.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Tsb Only Results",
      "text": "Dialogue-level variance normalization and utterance-level mean normalization were performed on audio features. Pitch and the first differentials were appended to the 40-d FBK25. The released reference transcripts of IEMOCAP were used for the text modality as most previous work on IEMOCAP takes this approach. Although  [10, 11, 13]  used 300-d GloVe in their experiments, 50-d GloVe was found to be most effective in our framework. As shown in Table  1 , attaching GloVe to the audio features improves the accuracy by ∼5% absolute. The standard deviation across the five folds also decreases when these two features are combined, which indicates that the system is more robust to speaker variation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results With Tab",
      "text": "Here we present the results with both the TSB and TAB branches. Table  2  shows the results vary with the number of context utterances from the dialogue. Comparing systems with no context ([0]) to 4 utterances before and 4 after ([-4,4]), shows that increased context leads to improved accuracy. The best UA and WA results were obtained with context windows of [-3,3] and  [-4,4]  respectively. Since emotion is a long-term attribute that may last for several utterances, context utterances from the same speaker help to recognise the current emotion. As a dyadic dialogue, utterances spoken by the other speaker carry reaction information, which is also useful to infer the current speaker's emotion. The systems with [-2,0] and [-3,0] contexts do not use information from future utterances and simulate an online streaming AER system and demonstrates the importance of the future utterance information. A [-3,+3] context window is used for all future systems thereafter, unless otherwise stated.   2 . Results of the two-branch systems with different TAB contexts, which were trained on Session 1-4 and tested on Session 5.\n\nTable  3  shows the results of the TAB only system with no context utterances (BERT[0]) and a [-3,3] context window (BERT[-3,3]), along with the two-branch system with a [-3,3] context window (Audio25+GloVe+BERT[-3,3]). It can be seen that the TAB only system with [-3,3] produces both a higher accuracy and a lower standard deviation. Compared to only using the TSB (Audio25+GloVe in Table  1 ), incorporating cross-utterance information from the TAB improves the accuracy by ∼10%.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study With Different Features",
      "text": "The results of an ablation study are shown in system can reduce the accuracy while adding GloVe to Audio25+BERT system can increase accuracy, which shows the importance of the prosody provided through correlations between audio and GloVe. Furthermore, by comparing the Audio25+BERT and GloVe+BERT systems, it can be seen that although GloVe itself is more useful than audio features, audio features perform better when combined with BERT, which indicates that the audio features provide complementary information to the BERT embeddings.\n\nNext, we study the use of our proposed long-term acoustic feature FBK250. With added FBK250 features, there is a ∼0.65% increase in the classification accuracy. The best results achieve 76.12% WA and 77.36% UA for 5-fold CV. As shown in Table  4 , GloVe is still useful even if the long-term acoustic features are used. Here we also found the use of long-term acoustic features requires more regularization. Dropout with a dropout rate of 0.5 was added to the output of each residual block and each self-attentive layer. Adding the dropout increases the overall performance by ∼1%, resulting in the 5-fold CV averages of 77.57% WA and 78.41% UA.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross Comparisons",
      "text": "To compare to the best previous results published on IEMOCAP, both a single fold test with the model trained on Session 1-4 and tested on Session 5, as well as the leave-one-speaker-out test with 10fold CV are provided for the final system, as shown in Table  5 . Our systems only used emotional audio data from IEMOCAP and two pre-trained text embeddings, without any audio data augmentation. The results and modalities used in the related work are summarised in Table  5 . It is worth noting that although  [32]  produces higher 10-CV UA, their cross validation setup is not speaker exclusive.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Use Of Asr Transcriptions",
      "text": "Although the released reference transcriptions of the IEMOCAP dataset were used in all of the previous experiments, in practice, reference transcriptions are usually not available. Therefore, here Table  5 . Summary of results in the literature. \"A\", \"T\", and \"V\" refer to the audio, text, and video modalities respectively. Results from  [6]  are not included since the test setting was not clear. Results from  [13]  and  [32]  were not obtained using leave-one-speaker-out 10-fold CV and thus not directly comparable.\n\nwe study the use of the transcriptions generated by a real ASR system. The Google Cloud Speech API was used to generate the ASR transcriptions. Among all 5,531 utterances, 9.7% did not receive any valid recognition output. Since the Google API does not provide the word-to-frame alignments, the GloVe feature was not used in the experiments in this section. The results are shown in Table  6 . The use of ASR leads to an accuracy decrease of ∼12.6% and ∼6.6% for BERT[0] and BERT  [-3,3] , respectively. The decrease is reduced to ∼3.7% when the audio features were included. Comparing the last two rows in Table  6 , the system that is both trained and tested with ASR outputs performed better than the system trained on reference transcriptions but tested on the ASR outputs. This may reflect the fact that the system can learn to better account for the errorful ASR transcipts when they are also used in training. A major advantage of using multimodal features is that different modalities can augment or complement each other, especially when certain modalities are susceptible to noise.\n\nFurther from the results shown in Table  6 , the performance loss caused by using ASR transcriptions is smaller when crossutterance information was added, which matches the expected findings discussed in Sec. 2.2. The addition of context can help in two ways. First, it provides more information, and second, it can partly compensate for the case that ASR system gives no valid output. Analysing the 9.7% of utterances without valid ASR output and looking at AER results for only those utterances, the BERT[0] system gave 25.00% UA, which is the same as a random guess.\n\nHowever, BERT[-3,3] produced a much better UA of 57.34%, which shows the information lost due to ASR failures can be partly recovered with the context utterances, and hence makes the system more robust.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "5-Way Classification Results",
      "text": "One of the main goals of this paper is to evaluate the performance of an emotion recognition system without making any unrealistic assumptions. Such unrealistic assumptions include assuming perfect ASR results, and that only a subset of emotion types will be seen in real applications. Papers that investigate performance on IEMOCAP normally only use a 4-way classification task based on the emotions: happy, sad, angry, neutral. However, in reality, people express many other emotions, such as fear, surprise, and disgust. In fact, IEMO-CAP itself contains 10 different emotion labels, and when only 4 emotions are considered, nearly half of the utterances in the dataset are normally discarded. Therefore, in this section we investigate an alternative 5-way classification setup which has an extra class \"others\" to represent all the other emotions that exist in IEMOCAP. The data used for the \"others\" class includes utterances labelled as \"frustration\", \"fear\", \"surprise\", \"disgust\", and \"other\". Note that \"frustration\" accounts for 92.4% of the data for the \"others\" category and is also the largest emotion group in IEMOCAP. In the 5-way classification setup, all of the 7,532 utterances with ground truth labels were used for training and testing. The classification accuracy of the 5-way system on the previous four emotions (happy, sad, neutral, angry) is 77.67% WA and 77.74% UA, which is similar to the results of the 4-way system given in Table  5 . However, the reverse would not be the case since the 4-way system cannot correctly classify examples from the other class and the overall classification accuracy of the 4-way system drops dramatically to 57.02% WA and 62.72% UA when tested on the 5-way data. The results show that when encountering emotions that don't belong to the target four emotions, while the 4-way system will obviously result in errors, our 5-way system can classify these other emotion classes with more than 75% accuracy.\n\nThe final 5-fold CV results for the 5-class system with ASR transcriptions are shown in Table  7 . Our 5-way system, combined with real ASR output transcriptions, can serve as a prototype for a more realistic emotion classification system.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Feature",
      "text": "Text WA (%) UA (%)   7 . 5-fold CV results of the 5-way system with the 5-way training and test data with different sources for the text modality.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, a two-branch structure fusing time synchronous and time asynchronous features has been proposed for multimodal emotion recognition. The TSB captures the correlations between each word and its acoustic realisations at each time step, while the TAB integrates the sentence embeddings from context utterances. The two-branch system achieves state-of-the-art 4-way classification accuracy of 77.76% WA and 78.30% UA for 10-fold CV on IEMO-CAP. By investigating the use of transcriptions produced by an ASR system and the alternative 5-way classification setup with an extra class representing all the other emotions, the performance of a more realistic AER system has also been demonstrated.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which consists of a",
      "page": 2
    },
    {
      "caption": "Figure 1: Proposed two-branch model structure.",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "In this paper, we propose a novel deep neural network architec-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "ture for AER, which consists of a time synchronous branch (TSB)"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "and a time asynchronous branch (TAB). In the TSB, audio features"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "are fused with the corresponding text at each frame in the input win-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "dow. This can model not only the speech content and its acoustic"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "realisation, but also their alignments and can help capture prosody."
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "Note that video features could also be optionally included in the"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "TSB. Along with traditional acoustic features, we propose using"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "long-term acoustic features with a 250 millisecond (ms) frame length"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "as an extra type of audio feature. In contrast to the TSB that focuses"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "on modelling the correlations of multimodal features across time, the"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "TAB leverages the transcriptions of the spoken utterances as well as"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "other potential global or utterance-level features. Sentence embed-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "dings relevant to a number of consecutive utterances in the dialogue"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "are derived from a pre-trained model with bidirectional encoder rep-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "resentations from Transformers (BERT)\n[16], and are used as the"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "cross-utterance input\nto the TAB. Two self-attentive layers [17] are"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "used as the separate pooling functions for the TSB and TAB, and the"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "resulting vectors fused for ﬁnal emotion classiﬁcation. Experimental"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "results on the widely used IEMOCAP dataset [18] show that the pro-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "posed structure achieves state-of-the-art\nresults in the usual 4-way"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "classiﬁcation setup when evaluated with all usual test conﬁgurations."
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "Rather than assuming perfect automatic speech recognition (ASR),"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "the use of ASR outputs was also studied, and it was found that using"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "cross-utterance input considerably improves robustness against ASR"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "errors. Finally, a 5-way classiﬁcation setup is studied, which has an"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "extra class to represent “frustration” and all\nthe other emotions in"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "IEMOCAP that are not considered in the 4-way setup."
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "The rest of\nthe paper\nis organised as follows.\nSection 2 intro-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "duces our proposed two-branch structure. Section 3 presents the ex-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "perimental setup and results related to the model architecture. Fur-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "ther analysis and experiments towards implementing a more realistic"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "AER system is given in Section 4, followed by conclusions."
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "2. PROPOSED APPROACH"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "2.1. Feature representation"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "2.1.1. Audio features"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": ""
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "The audio representation for speech-based AER often includes log"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "Mel ﬁlterbank features (FBKs) [19].\nIn this paper, 40-dimensional"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "(-d) FBKs with a 10 ms frame duration and 25 ms frame length are"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "used, which is denoted FBK25. FBK features have information about"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "the short-term spectrum but do not contain pitch information that can"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "be important\nin describing emotional speech [20] and is often com-"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "plementary to FBKs [21, 22]. The log pitch frequency features with"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "probability-of-voicing-weighted mean subtraction over a 1.5 second"
        },
        {
          "{ww368,cz277,pcw}@eng.cam.ac.uk": "window are used along with FBKs [23].\nIn addition, we propose"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "using long-term FBK features extracted in the same way as FBK25": "apart from a long 250ms frame length, which is denoted as FBK250.",
          "information can also be included in the TSB, by simply concatenat-": "ing the visual features of each video frame with the corresponding"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "audio and text features, and ensuring that the frame rate of the audio"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "features matches the video frame rate. Since we did not observe any"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "2.1.2.\nText embeddings",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "performance improvement by including the visual features, possibly"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "due to the quality and style of\nthe videos in IEMOCAP, no video-"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "The use of vector representations of words and sentences has become",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "related experiments are presented in this paper. More detail of these"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "a widely-used approach in natural\nlanguage processing. The global",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "video-related experiments can be found in [28]."
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "vector (GloVe) is a commonly used method that estimates word em-",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "While the TSB includes modelling the temporal correlations be-"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "beddings using a global\nlog-bilinear\nregression model\nin order\nto",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "tween different modalities,\nthe TAB focuses on capturing text\nin-"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "combine the advantages of both global matrix factorization and lo-",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "formation including meaning from the speech transcriptions.\nThe"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "cal context window methods [24].\nIn this paper, pre-trained 50-d",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "BERT-derived sentence embeddings of\nthe utterance transcriptions"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "GloVe embeddings are used in the TSB to encode word-level\ntran-",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "are used as the input vectors to TAB. The embeddings for a num-"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "scriptions. Recently, pre-trained sentence-level embeddings derived",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "ber of consecutive utterances were used as the TAB input since the"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "from a large pre-trained language model, such as BERT [16], have",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "emotion of each utterance is often strongly related to its context\nin"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "drawn much attention. We use the pre-trained BERT-base model",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "a spoken dialogue [29].\nThe following text snippets illustrate the"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "without ﬁne-tuning to encode the transcription of each single utter-",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "importance of context in determining emotional content:"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "ance into a 768-d vector, which is used as the input to the TAB.",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "“Did you pass the exam?”"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "“Yes, exactly.” (happy)"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "2.2. Model structure",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "“Did you fail the exam?”"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "The proposed model structure is shown in Fig. 1, which consists of a",
          "information can also be included in the TSB, by simply concatenat-": "“Yes, exactly.” (frustrated)"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "TSB that fuses the audio features with the corresponding text infor-",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "It\nis clear that\nthe two different examples of “Yes, exactly.” convey"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "mation at each time step, as well as a TAB that captures the text in-",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "very different emotions, and hence determining the emotion from"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "formation embedded across the transcriptions of a number of consec-",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "the text requires the context. Furthermore, since cross-utterance in-"
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "utive utterances. The TSB structure is similar to that which is often",
          "information can also be included in the TSB, by simply concatenat-": ""
        },
        {
          "using long-term FBK features extracted in the same way as FBK25": "",
          "information can also be included in the TSB, by simply concatenat-": "formation provides extra information for\nthe emotional content,\nit"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 3: Mean and std. deviation of the 5-fold CV results with",
      "data": [
        {
          "3.2. TSB only results": "Dialogue-level variance normalization and utterance-level mean nor-",
          "Feature\nWA (%)\nUA (%)": "BERT[0]\n58.53±4.41\n59.20±5.57"
        },
        {
          "3.2. TSB only results": "malization were performed on audio features.\nPitch and the ﬁrst",
          "Feature\nWA (%)\nUA (%)": "BERT[-3,3]\n71.22±3.16\n71.88±2.62"
        },
        {
          "3.2. TSB only results": "The released ref-\ndifferentials were appended to the 40-d FBK25.",
          "Feature\nWA (%)\nUA (%)": "75.53±3.79\n76.65±3.67\nAudio25+GloVe+BERT[-3,3]"
        },
        {
          "3.2. TSB only results": "erence transcripts of IEMOCAP were used for the text modality as",
          "Feature\nWA (%)\nUA (%)": ""
        },
        {
          "3.2. TSB only results": "most previous work on IEMOCAP takes this approach. Although",
          "Feature\nWA (%)\nUA (%)": "Table 3. Mean and std.\ndeviation of\nthe 5-fold CV results with"
        },
        {
          "3.2. TSB only results": "[10, 11, 13] used 300-d GloVe in their experiments, 50-d GloVe was",
          "Feature\nWA (%)\nUA (%)": "different input features. “Audio25” denotes FBK25+pitch+∆."
        },
        {
          "3.2. TSB only results": "found to be most effective in our framework. As shown in Table 1,",
          "Feature\nWA (%)\nUA (%)": ""
        },
        {
          "3.2. TSB only results": "attaching GloVe to the audio features improves the accuracy by ∼5%",
          "Feature\nWA (%)\nUA (%)": ""
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "GloVe\nBERT\nWA (%)\nUA (%)\nAudio25\nFBK250"
        },
        {
          "3.2. TSB only results": "absolute. The standard deviation across the ﬁve folds also decreases",
          "Feature\nWA (%)\nUA (%)": ""
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√"
        },
        {
          "3.2. TSB only results": "when these two features are combined, which indicates that the sys-",
          "Feature\nWA (%)\nUA (%)": "60.64\n61.32"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√"
        },
        {
          "3.2. TSB only results": "tem is more robust to speaker variation.",
          "Feature\nWA (%)\nUA (%)": "61.27\n62.67"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "71.22\n71.88"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√\n√"
        },
        {
          "3.2. TSB only results": "Feature\nWA (%)\nUA (%)",
          "Feature\nWA (%)\nUA (%)": "70.40\n71.88"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√\n√"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "65.53\n66.43"
        },
        {
          "3.2. TSB only results": "60.64±1.96\n61.32±2.26\nAudio25",
          "Feature\nWA (%)\nUA (%)": "√\n√"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "74.10\n74.89"
        },
        {
          "3.2. TSB only results": "GloVe\n61.27±3.73\n62.67±3.55",
          "Feature\nWA (%)\nUA (%)": ""
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√\n√\n√"
        },
        {
          "3.2. TSB only results": "65.53±1.83\n66.43±1.33\nAudio25+GloVe",
          "Feature\nWA (%)\nUA (%)": "75.53\n76.65"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√\n√\n√"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "74.74\n75.60"
        },
        {
          "3.2. TSB only results": "",
          "Feature\nWA (%)\nUA (%)": "√\n√\n√\n√"
        },
        {
          "3.2. TSB only results": "Table 1. Mean and std. dev. of the TSB only 5-fold CV results with",
          "Feature\nWA (%)\nUA (%)": "76.12\n77.36"
        },
        {
          "3.2. TSB only results": "different input features. “Audio25” denotes FBK25+pitch+∆.",
          "Feature\nWA (%)\nUA (%)": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 5: Summary of results in the literature. “A”, “T”, and “V” realapplications.PapersthatinvestigateperformanceonIEMOCAP",
      "data": [
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "ASR results, and that only a subset of emotion types will be seen in"
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "real applications. Papers that investigate performance on IEMOCAP"
        },
        {
          "[32]\nA+T": "Table 5.\nSummary of",
          "10-fold CV\n–\n79.2": "results in the literature.\n“A”, “T”, and “V”",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "normally only use a 4-way classiﬁcation task based on the emotions:"
        },
        {
          "[32]\nA+T": "refer\nto the audio,",
          "10-fold CV\n–\n79.2": "text, and video modalities respectively. Results",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "happy, sad, angry, neutral. However, in reality, people express many"
        },
        {
          "[32]\nA+T": "from [6] are not included since the test setting was not clear. Results",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "other emotions, such as fear, surprise, and disgust.\nIn fact, IEMO-"
        },
        {
          "[32]\nA+T": "from [13] and [32] were not obtained using leave-one-speaker-out",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "CAP itself contains 10 different emotion labels, and when only 4"
        },
        {
          "[32]\nA+T": "10-fold CV and thus not directly comparable.",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "emotions are considered, nearly half of the utterances in the dataset"
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "are normally discarded. Therefore,\nin this section we investigate an"
        },
        {
          "[32]\nA+T": "we study the use of the transcriptions generated by a real ASR sys-",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "alternative 5-way classiﬁcation setup which has an extra class “oth-"
        },
        {
          "[32]\nA+T": "tem. The Google Cloud Speech API was used to generate the ASR",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "ers” to represent all the other emotions that exist in IEMOCAP. The"
        },
        {
          "[32]\nA+T": "transcriptions. Among all 5,531 utterances, 9.7% did not receive any",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "data used for the “others” class includes utterances labelled as “frus-"
        },
        {
          "[32]\nA+T": "valid recognition output.",
          "10-fold CV\n–\n79.2": "The ASR system gives an overall WER",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "tration”, “fear”, “surprise”, “disgust”, and “other”. Note that “frus-"
        },
        {
          "[32]\nA+T": "of 40.8%. This indicates that emotional speech recognition is still a",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        },
        {
          "[32]\nA+T": "",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": "tration” accounts for 92.4% of the data for the “others” category and"
        },
        {
          "[32]\nA+T": "difﬁcult task for a standard ASR system.",
          "10-fold CV\n–\n79.2": "",
          "sumptions.\nSuch unrealistic assumptions include assuming perfect": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: Summary of results in the literature. “A”, “T”, and “V” realapplications.PapersthatinvestigateperformanceonIEMOCAP",
      "data": [
        {
          "data used for the “others” class includes utterances labelled as “frus-": ""
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "tration”, “fear”, “surprise”, “disgust”, and “other”. Note that “frus-"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": ""
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "tration” accounts for 92.4% of the data for the “others” category and"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": ""
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "is also the largest emotion group in IEMOCAP. In the 5-way clas-"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "siﬁcation setup, all of the 7,532 utterances with ground truth labels"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "were used for training and testing. The classiﬁcation accuracy of the"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "5-way system on the previous four emotions (happy, sad, neutral, an-"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": ""
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "gry) is 77.67% WA and 77.74% UA, which is similar to the results"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": ""
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "of the 4-way system given in Table 5. However,\nthe reverse would"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": ""
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "not be the case since the 4-way system cannot correctly classify ex-"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "amples from the other class and the overall classiﬁcation accuracy"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "of the 4-way system drops dramatically to 57.02% WA and 62.72%"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "UA when tested on the 5-way data. The results show that when en-"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "countering emotions that don’t belong to the target\nfour emotions,"
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": ""
        },
        {
          "data used for the “others” class includes utterances labelled as “frus-": "while the 4-way system will obviously result\nin errors, our 5-way"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 5: Summary of results in the literature. “A”, “T”, and “V” realapplications.PapersthatinvestigateperformanceonIEMOCAP",
      "data": [
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "which shows the information lost due to ASR failures can be partly"
        },
        {
          "Paper\nModality\nTest Setting": "ours\nA+T\nSession 5",
          "WA (%)": "83.08",
          "UA (%)": "83.22",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "recovered with the context utterances, and hence makes the system"
        },
        {
          "Paper\nModality\nTest Setting": "ours\nA+T\n5-fold CV",
          "WA (%)": "77.57",
          "UA (%)": "78.41",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "more robust."
        },
        {
          "Paper\nModality\nTest Setting": "ours\nA+T\n10-fold CV",
          "WA (%)": "77.76",
          "UA (%)": "78.30",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "[14]\nA+T+V\nSession 5",
          "WA (%)": "76.5",
          "UA (%)": "–",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "4.3.\n5-way classiﬁcation results"
        },
        {
          "Paper\nModality\nTest Setting": "[11]\nA+T\n5-fold CV",
          "WA (%)": "–",
          "UA (%)": "71.8",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "[12]\nA+T+V\n10-fold CV",
          "WA (%)": "76.1",
          "UA (%)": "–",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "One of the main goals of this paper is to evaluate the performance"
        },
        {
          "Paper\nModality\nTest Setting": "[13]\nA+T\n10-fold CV",
          "WA (%)": "76.5",
          "UA (%)": "77.6",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "of an emotion recognition system without making any unrealistic as-"
        },
        {
          "Paper\nModality\nTest Setting": "[32]\nA+T\n10-fold CV",
          "WA (%)": "–",
          "UA (%)": "79.2",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "sumptions.\nSuch unrealistic assumptions include assuming perfect"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "ASR results, and that only a subset of emotion types will be seen in"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "real applications. Papers that investigate performance on IEMOCAP"
        },
        {
          "Paper\nModality\nTest Setting": "Summary of",
          "WA (%)": "results in the literature.",
          "UA (%)": "“A”, “T”, and “V”",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "normally only use a 4-way classiﬁcation task based on the emotions:"
        },
        {
          "Paper\nModality\nTest Setting": "to the audio,",
          "WA (%)": "text, and video modalities respectively. Results",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "happy, sad, angry, neutral. However, in reality, people express many"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "other emotions, such as fear, surprise, and disgust.\nIn fact, IEMO-"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "CAP itself contains 10 different emotion labels, and when only 4"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "emotions are considered, nearly half of the utterances in the dataset"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "are normally discarded. Therefore,\nin this section we investigate an"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "alternative 5-way classiﬁcation setup which has an extra class “oth-"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "ers” to represent all the other emotions that exist in IEMOCAP. The"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "data used for the “others” class includes utterances labelled as “frus-"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "The ASR system gives an overall WER",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "tration”, “fear”, “surprise”, “disgust”, and “other”. Note that “frus-"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "tration” accounts for 92.4% of the data for the “others” category and"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "is also the largest emotion group in IEMOCAP. In the 5-way clas-"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "siﬁcation setup, all of the 7,532 utterances with ground truth labels"
        },
        {
          "Paper\nModality\nTest Setting": "Feature\nText",
          "WA (%)": "WA (%)",
          "UA (%)": "UA (%)",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "were used for training and testing. The classiﬁcation accuracy of the"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "5-way system on the previous four emotions (happy, sad, neutral, an-"
        },
        {
          "Paper\nModality\nTest Setting": "BERT[0]\nRef",
          "WA (%)": "58.53",
          "UA (%)": "59.20",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "gry) is 77.67% WA and 77.74% UA, which is similar to the results"
        },
        {
          "Paper\nModality\nTest Setting": "BERT[-3,3]\nRef",
          "WA (%)": "71.22",
          "UA (%)": "71.88",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "of the 4-way system given in Table 5. However,\nthe reverse would"
        },
        {
          "Paper\nModality\nTest Setting": "Ref\nAudio25,250+BERT[-3,3]",
          "WA (%)": "74.74",
          "UA (%)": "75.60",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "not be the case since the 4-way system cannot correctly classify ex-"
        },
        {
          "Paper\nModality\nTest Setting": "BERT[0]\nASR",
          "WA (%)": "46.61",
          "UA (%)": "45.83",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "amples from the other class and the overall classiﬁcation accuracy"
        },
        {
          "Paper\nModality\nTest Setting": "BERT[-3,3]\nASR",
          "WA (%)": "64.15",
          "UA (%)": "65.73",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "of the 4-way system drops dramatically to 57.02% WA and 62.72%"
        },
        {
          "Paper\nModality\nTest Setting": "ASR\nAudio25,250+BERT[-3,3]",
          "WA (%)": "70.96",
          "UA (%)": "71.90",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "UA when tested on the 5-way data. The results show that when en-"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "countering emotions that don’t belong to the target\nfour emotions,"
        },
        {
          "Paper\nModality\nTest Setting": "Mix\nAudio25,250+BERT[-3,3]",
          "WA (%)": "61.91",
          "UA (%)": "63.47",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "while the 4-way system will obviously result\nin errors, our 5-way"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "system can classify these other emotion classes with more than 75%"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "5-fold CV results with real ASR outputs.",
          "UA (%)": "“Ref” denotes",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "accuracy."
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "The ﬁnal 5-fold CV results for the 5-class system with ASR tran-"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "scriptions are shown in Table 7. Our 5-way system, combined with"
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": ""
        },
        {
          "Paper\nModality\nTest Setting": "",
          "WA (%)": "",
          "UA (%)": "",
          "However, BERT[-3,3]\nproduced\na much\nbetter UA of\n57.34%,": "real ASR output\ntranscriptions, can serve as a prototype for a more"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "S. Kim, J.N. Chang, S. Lee, and S.S. Narayanan, “IEMOCAP:"
        },
        {
          "6. REFERENCES": "[1] Y. Kim, H. Lee, and E. M. Provost,\n“Deep learning for\nro-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "Lan-\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "6. REFERENCES": "bust feature generation in audiovisual emotion recognition,” in",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "guage Resources and Evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "6. REFERENCES": "Proc. ICASSP, Vancouver, 2013.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[19] C. Busso, S. Lee, and S. Narayanan, “Use neutral speech mod-"
        },
        {
          "6. REFERENCES": "[2]\nS. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "els for emotional speech analysis,” in Proc. Interspeech, 2007."
        },
        {
          "6. REFERENCES": "affective computing:\nFrom unimodal analysis to multimodal",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[20] E. Rodero,\n“Intonation and emotion:\nInﬂuence of pitch lev-"
        },
        {
          "6. REFERENCES": "fusion,” Information Fusion, vol. 37, pp. 98–125, 2017.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "Journal of Voice:\nels and contour type on creating emotions,”"
        },
        {
          "6. REFERENCES": "[3]\nP. Tzirakis, G. Trigeorgis, M. Nicolaou, B.W. Schuller, and",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "Ofﬁcial Journal of the Voice Fdn., vol. 25, pp. e25–e34, 2011."
        },
        {
          "6. REFERENCES": "S. Zafeiriou, “End-to-end multimodal emotion recognition us-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[21]\nP. Dumouchel, N. Dehak, Y. Attabi, R. Dehak,\nand N. Bo-"
        },
        {
          "6. REFERENCES": "ing deep neural networks,” IEEE Journal of Selected Topics in",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "ufaden,\n“Cepstral and long-term features for emotion recog-"
        },
        {
          "6. REFERENCES": "Signal Processing, vol. 11, no. 8, pp. 1301–1309, 2017.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "nition,” in Proc. Interspeech, Brighton, 2009."
        },
        {
          "6. REFERENCES": "[4] M. Pantic, N. Sebe, J. Cohn, and T. Huang,\n“Affective multi-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[22] R. Pappagari, T. Wang,\nJ. Villalba, N. Chen, and N. Dehak,"
        },
        {
          "6. REFERENCES": "modal human-computer interaction,”\nin Proc. ACM Multime-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "“X-vectors meet emotions: A study on dependencies between"
        },
        {
          "6. REFERENCES": "dia, Singapore, 2005.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "emotion and speaker recognition,” in Proc. ICASSP, 2020."
        },
        {
          "6. REFERENCES": "[5] M. Soleymani, M. Pantic, and T. Pun,\n“Multimodal emotion",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[23]\nP. Ghahremani, B. BabaAli, D. Povey, K. Riedhammer, J. Tr-"
        },
        {
          "6. REFERENCES": "IEEE Transactions on Af-\nrecognition in response to videos,”",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "mal, and S. Khudanpur,\n“A pitch extraction algorithm tuned"
        },
        {
          "6. REFERENCES": "fective Computing, vol. 3, no. 2, pp. 211–223, 2012.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "for automatic speech recognition,” in Proc. ICASSP, 2014."
        },
        {
          "6. REFERENCES": "[6] T. Mittal,\nU.\nBhattacharya,\nR.\nChandra,\nA.\nBera,\nand",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[24]\nJ. Pennington, R. Socher, and C. Manning,\n“Glove: Global"
        },
        {
          "6. REFERENCES": "D. Manocha,\n“M3ER: Multiplicative multimodal\nemotion",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "vectors for word representation,” in Proc. EMNLP, 2014."
        },
        {
          "6. REFERENCES": "recognition using facial,\ntextual, and speech cues,”\nin Proc.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[25] G. Sun, C. Zhang, and P.C. Woodland,\n“Speaker diarisation"
        },
        {
          "6. REFERENCES": "AAAI, New York, 2020.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "using 2D self-attentive combination of embeddings,”\nin Proc."
        },
        {
          "6. REFERENCES": "[7]\nS. Latif, R. Rana, Shahzad Younis, Junaid Qadir, and J. Epps,",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "ICASSP, Brighton, 2019."
        },
        {
          "6. REFERENCES": "“Transfer learning for improving speech emotion classiﬁcation",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[26]\nF. Kreyssig, C. Zhang, and P.C. Woodland, “Improved TDNNs"
        },
        {
          "6. REFERENCES": "accuracy,” in Proc. Interspeech, Hyderabad, 2018.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "using deep kernels and frequency dependent Grid-RNNs,”\nin"
        },
        {
          "6. REFERENCES": "[8] Z. Lu, L. Cao, Y. Zhang, C. Chiu, and J. Fan,\n“Speech sen-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "Proc. ICASSP, Calgary, 2018."
        },
        {
          "6. REFERENCES": "timent analysis via pre-trained features from end-to-end ASR",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[27] E. Liebenthal, D. Silbersweig, and E. Stern,\n“The language,"
        },
        {
          "6. REFERENCES": "models,” in Proc. ICASSP, Barcelona, 2020.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "tone and prosody of emotions: Neural substrates and dynam-"
        },
        {
          "6. REFERENCES": "[9] A. Nediyanchath, P. Paramasivam, and P. Yenigalla,\n“Multi-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "ics of spoken-word emotion perception,” Frontiers in Neuro-"
        },
        {
          "6. REFERENCES": "head attention for speech emotion recognition with auxiliary",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "science, vol. 10, pp. 506:1–13, 2016."
        },
        {
          "6. REFERENCES": "learning of gender recognition,” in Proc. ICASSP, 2020.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[28] W. Wu, “Multimodal emotion recognition,” MPhil thesis, Uni-"
        },
        {
          "6. REFERENCES": "[10]\nS. Tripathi, S. Tripathi, and H. Beigi,\n“Multi-modal emotion",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "versity of Cambridge, 2020."
        },
        {
          "6. REFERENCES": "recognition on IEMOCAP dataset using deep learning,” arXiv",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[29]\nF. Eyben, S. Buchholz, N. Braunschweiler, J. Latorre, V. Wan,"
        },
        {
          "6. REFERENCES": "preprint 1804.05788, 2018.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "M. Gales, and K. Knill,\n“Unsupervised clustering of emotion"
        },
        {
          "6. REFERENCES": "[11]\nS. Yoon, S. Byun, and K. Jung,\n“Multimodal speech emotion",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "and voice styles for expressive TTS,” in Proc. ICASSP, 2012."
        },
        {
          "6. REFERENCES": "recognition using audio and text,” in Proc. SLT, Athens, 2018.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[30] W. Liu, Y. Wen,\nZ. Yu, M. Li,\nB. Raj,\nand L.\nSong,"
        },
        {
          "6. REFERENCES": "[12]\nS. Poria, N. Majumder, D. Hazarika, E. Cambria, A. Gelbukh,",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "“SphereFace: Deep hypersphere embedding for face recogni-"
        },
        {
          "6. REFERENCES": "and A. Hussain,\n“Multimodal sentiment analysis: Addressing",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "tion,” in Proc. CVPR, Hawaii, 2017."
        },
        {
          "6. REFERENCES": "key issues and setting up the baselines,” IEEE Intelligent Sys-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[31]\nS. Young, Gunnar E., M.J.F. Gales, T. Hain, D. Kershaw,"
        },
        {
          "6. REFERENCES": "tems, vol. 33, no. 6, pp. 17–25, 2018.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, A. Ragni,"
        },
        {
          "6. REFERENCES": "[13]\nS. Yoon, S. Byun, S. Dey, and K. Jung, “Speech emotion recog-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "V\n. Valtchev, P. Woodland, and C. Zhang, The HTK Book (ver-"
        },
        {
          "6. REFERENCES": "nition using multi-hop attention mechanism,” in Proc. ICASSP,",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "sion 3.5a), University of Cambridge, 2015."
        },
        {
          "6. REFERENCES": "Brighton, 2019.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "[32] R. Li, Z. Wu,\nJ.\nJia, Y. Bu, S. Zhao,\nand H. Meng,\n“To-"
        },
        {
          "6. REFERENCES": "[14] N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria,\nand",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "wards discriminative representation learning for speech emo-"
        },
        {
          "6. REFERENCES": "S. Poria, “Multimodal sentiment analysis using hierarchical fu-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": "tion recognition,” in Proc. IJCAI, Macao, China, 2019."
        },
        {
          "6. REFERENCES": "sion with context modeling,” Knowledge-Based Systems, vol.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "161, pp. 124–133, 2018.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "[15] Y. Xu, H. Xu, and J. Zou,\n“HGFM: A hierarchical grained",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "and feature model for acoustic emotion recognition,”\nin Proc.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "ICASSP, Barcelona, 2020.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "[16]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“BERT:",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "Pre-training of deep bidirectional Transformers for\nlanguage",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "understanding,” in Proc. NAACL-HLT, Minneapolis, 2019.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "[17] Z. Lin, M. Feng, C.N. dos Santos, Y. Mo, X. Bing, B. Zhou,",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "and Y. Bengio,\n“A structured self-attentive sentence embed-",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        },
        {
          "6. REFERENCES": "ding,” in Proc. ICLR, Toulon, 2017.",
          "[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E.M. Provost,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "3",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "4",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Affective multimodal human-computer interaction",
      "authors": [
        "M Pantic",
        "N Sebe",
        "J Cohn",
        "T Huang"
      ],
      "year": "2005",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "M3ER: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proc. AAAI"
    },
    {
      "citation_id": "8",
      "title": "Transfer learning for improving speech emotion classification accuracy",
      "authors": [
        "S Latif",
        "R Rana",
        "Shahzad Younis",
        "Junaid Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end ASR models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang",
        "C Chiu",
        "J Fan"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "10",
      "title": "Multihead attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "11",
      "title": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning",
      "authors": [
        "S Tripathi",
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on IEMOCAP dataset using deep learning"
    },
    {
      "citation_id": "12",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Proc. SLT"
    },
    {
      "citation_id": "13",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using multi-hop attention mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "16",
      "title": "HGFM: A hierarchical grained and feature model for acoustic emotion recognition",
      "authors": [
        "Y Xu",
        "H Xu",
        "J Zou"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "17",
      "title": "BERT: Pre-training of deep bidirectional Transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. NAACL-HLT"
    },
    {
      "citation_id": "18",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Z Lin",
        "M Feng",
        "C Santos",
        "Y Mo",
        "X Bing",
        "B Zhou",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "20",
      "title": "Use neutral speech models for emotional speech analysis",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2007",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "21",
      "title": "Intonation and emotion: Influence of pitch levels and contour type on creating emotions",
      "authors": [
        "E Rodero"
      ],
      "year": "2011",
      "venue": "Journal of Voice: Official Journal of the Voice Fdn"
    },
    {
      "citation_id": "22",
      "title": "Cepstral and long-term features for emotion recognition",
      "authors": [
        "P Dumouchel",
        "N Dehak",
        "Y Attabi",
        "R Dehak",
        "N Boufaden"
      ],
      "year": "2009",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "X-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "24",
      "title": "A pitch extraction algorithm tuned for automatic speech recognition",
      "authors": [
        "P Ghahremani",
        "B Babaali",
        "D Povey",
        "K Riedhammer",
        "J Trmal",
        "S Khudanpur"
      ],
      "year": "2014",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "25",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proc. EMNLP"
    },
    {
      "citation_id": "26",
      "title": "Speaker diarisation using 2D self-attentive combination of embeddings",
      "authors": [
        "G Sun",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2019",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "27",
      "title": "Improved TDNNs using deep kernels and frequency dependent Grid-RNNs",
      "authors": [
        "F Kreyssig",
        "C Zhang",
        "P Woodland"
      ],
      "year": "2018",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "28",
      "title": "The language, tone and prosody of emotions: Neural substrates and dynamics of spoken-word emotion perception",
      "authors": [
        "E Liebenthal",
        "D Silbersweig",
        "E Stern"
      ],
      "year": "2016",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition",
      "authors": [
        "W Wu"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised clustering of emotion and voice styles for expressive TTS",
      "authors": [
        "F Eyben",
        "S Buchholz",
        "N Braunschweiler",
        "J Latorre",
        "V Wan",
        "M Gales",
        "K Knill"
      ],
      "year": "2012",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "31",
      "title": "SphereFace: Deep hypersphere embedding for face recognition",
      "authors": [
        "W Liu",
        "Y Wen",
        "Z Yu",
        "M Li",
        "B Raj",
        "L Song"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "32",
      "title": "",
      "authors": [
        "S Young",
        "M Gales",
        "T Hain",
        "D Kershaw",
        "X Liu",
        "G Moore",
        "J Odell",
        "D Ollason",
        "D Povey",
        "A Ragni",
        "V Valtchev",
        "P Woodland",
        "C Zhang"
      ],
      "year": "2015",
      "venue": ""
    },
    {
      "citation_id": "33",
      "title": "Towards discriminative representation learning for speech emotion recognition",
      "authors": [
        "R Li",
        "Z Wu",
        "J Jia",
        "Y Bu",
        "S Zhao",
        "H Meng"
      ],
      "year": "2019",
      "venue": "Proc. IJCAI"
    }
  ]
}