{
  "paper_id": "2207.06410v1",
  "title": "Mdeaw: A Multimodal Dataset For Emotion Analysis Through Eda And Ppg Signals From Wireless Wearable Low-Cost Off-The-Shelf Devices",
  "published": "2022-07-14T07:04:29Z",
  "authors": [
    "Arijit Nandi",
    "Fatos Xhafa",
    "Laia Subirats",
    "Santi Fort"
  ],
  "keywords": [
    "Affective Computing",
    "Dataset for Emotion Recognition",
    "E-learning Dataset for Emotion Analysis",
    "Federated Learning",
    "Data Streaming",
    "Students' Emotion State Analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present MDEAW, a multimodal database consisting of Electrodermal Activity (EDA) and Photoplethysmography (PPG) signals recorded during the exams for the course taught by the teacher at Eurecat Academy, Sabadell, Barcelona in order to elicit the emotional reactions to the students in a class room scenario. Signals from 10 students were recorded along with the students self-assessment of their affective state after each stimuli, in terms 6 basic emotion states. All the signals were captured using portable, wearable, wireless, low-cost, and offthe-shelf equipment that has the potential to allow the use of affective computing methods in everyday applications. A baseline for student-wise affect recognition using EDA and PPG-based features, as well as their fusion, was established through ReMECS, Fed-ReMECS and Fed-ReMECS-U. These results indicate the prospects of using low-cost devices for affective state recognition applications. The proposed database will be made publicly available in order to allow researchers to achieve a more thorough evaluation of the suitability of these capturing devices for emotion state recognition applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Learning Exercise",
      "text": "The stimuli used in this experiment are the exams for the course taught by the teacher at Eurecat Academy, Sabadell, Barcelona in order to elicit the emotional reactions to the students in a class room scenario and record Electrodermal Activity (EDA) and Photoplethysmography (PPG) data. The exam was designed by the tutor of that course and it was divided into smaller exercises (see Annex A for the examination formulation).\n\nEach exercise duration was 5 minute slots and there were a total of 12 sets of exercises, so in total the whole exam session was 84 minutes. In between each of the 12 sets we gave 2 minutes gap to for the students to \"cool down\". Also, students were advised that their annotations would not affect their evaluations to avoid any possible bias. In each of the 12 sets, from exercise Set 1 to Set 7; and 11 and 12 had 4 questions in each. Exercise Set 8 to 10 had one subjective question in each. The exercises sets are added in the additional materials for further reference.\n\nBefore starting the exam we instructed the students the whole exam pattern. In which students were asked to give their current emotional states that they had felt after answering each questions. For the emotion states input from students we used 6 basic discrete emotion model  [1]   1  developed by Paul Ekman, widely accepted theory of basic emotions and their expressions. These emotion labels are as follows:\n\n1) Sadness 2) Happiness 3) Fear 4) Anger 5) Surprise 6) Disgust",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Students",
      "text": "In the exam there were 10 students. The students were all male. Their age varies from 23 to 57 years.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "D. Data Acquisition",
      "text": "The sensors used to record the EDA and PPG data is Consensys Bundle Development kit from Shimmer Sensing. The pictures of the sensor is in  Before starting the exam we put the sensors on the students shown in the following picture (Fig.  3 ) for collecting the EDA and PPG data. The sensors were put on the non active hand of each student (such as if he/she is right handed we put the sensor on the left hand and so on). The whole experiment was carried out in 2 sessions, in each session there were 5 students. The following picture (Fig.  4 ) is from the class while doing the experiment. The data collection from multiple Shimmer sensors were done using the Shimmer's Consensys software, in which all the data streams are recorded and saved. In the picture (Fig.  5 ), the software interface while doing the experiment is presented.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "E. Data Formatting",
      "text": "The data recorded in the Shimmer's Consensys software is exported into two format, such as .csv and .mat. In each file the data is saved in the following format (Fig.  6 ):\n\nEach .mat or .csv file contains these channels which are available in the Shimmer sensor. From the filename shown in Fig.  6 , we can see that Consensys software saves the file in the following format:\n\n[T rial name/number, Sessionnumber, SensorU niqueID, Calibrated/notcalibrated, P C] The sensor recordings can be accessed by providing the channel names as dot(.) operator, e.g. in Fig.  7 .\n\nThe emotion labels were saved in .csv file with the file name which contains [session name + student ID + type of data (EDA/PPG)].",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "F. Data Set",
      "text": "For easy accessibility and better understandability we have extracted only the EDA and PPG data from the data recordings saved in Shimmer's Consensys software. The final datatset with the EDA and PPG recordings with the emotion labels are saved in .csv file. The folder structure of the MDEAW dataset is shown in Fig.  8 .    The MDEAW dataset will be made publicly available. In Fig  9 , the emotion class frequencies per student is presented and in Fig.  10  the overall emotion class frequencies for the MDEAW dataset is presented. This figures shows the class distributions of the MDEAW dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Analysis Of The Data Set A. Feature Extraction",
      "text": "It's important to extract features from different physiological signals (such as EDA, PPG etc.) for retrieving relevant information, which effectively represents emotion states. Based on the extracted features the emotion classifier is tested and trained.\n\nThe most popular and widely used time-frequency analysis of various signals (especially EDA, PPG, etc.) for feature extraction is Wavelet Decomposition (WD)  [2] . Its popularity and wide use are due to its localized analysis approach(i,e. time-frequency), multi-rate filtering, and multi-scale zooming. It is better suited for non-stationary signals (such as EDA, PPG etc.)  [3] . Most frequently used wavelet base functions are Meyer WD, Morlet Mother WD, Haar Mother WD and Daubechies WD  [4] . The most frequently used features extracted from each sub-bands of EDA and PPG are entropy, median, mean, standard deviation, variance, 5th percentile value, 25th percentile value, 75th percentile value, 95th percentile value, root means square value, zero crossing rate, mean crossing rate [5]-  [7] . In our experiment we have extracted and used these features for emotion classification. The wavelet feature extraction technique is used to extract features from multi-modal signal streams (EDA, and PPG signals from the dataset) in this experiment. The wavelet Daubechies 4 (Db4) is the base function for feature extraction. Our experiment decomposes EDA and RB, into three levels, respectively.\n\n1) Fusion of EDA and PPG-based features: The use of features based on multiple modalities has been shown to provide increased classification accuracy compared to approaches based on a single modality. In order to evaluate the performance of the combined EDA and PPG-based features, the two feature vectors F EDA and F P P G are fused as follows: First, the values of each feature vector are normalised in the range [0, 1] in order to compensate for the differences in numerical range. Then, the two normalised feature vectors F EDA and F P P G are concatenated in the final feature vector\n\nIV. EMOTION RECOGNITION RESULTS FOR THE DATA SET For the emotion recognition results, we have used our previously developed Real-time Multimodal Emotion Classification System (ReMECS  [8] ) based on Feed-Forward Neural Network, trained in an online fashion using the Incremental Stochastic Gradient Descent algorithm. The avg. accuracy and F1score along with the standard deviation (STD) are presented in TABLE I. More results can be found in Appendix B.\n\nAlso, the confusion matrix is presented in Fig.  11 .  V. A NEW ROUND OF THE EXPERIMENT We have planned a new round of the experiment, to extend the dataset. The reason for conducting another round of experiment to make the MDEAW dataset more diverse and also to increase number data samples by increasing number participants. The diversity can help each procedure to guarantee a total good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result  [9] . Having more number of diverse data samples in the MDEAW dataset will help ML/DL models to generalize more so that the accuracy and efficiency of those intelligent emotion models increase.\n\nIn the new round of experiment we will follow the same design pattern followed for the first experiment for the EDA and PPG data collection from the students but the course and the subject may be different or same based on the availability. The reason for adding the term \"based on availability\" is because we do the experiment on real students with the real courses studied at our Eurecat Academy.\n\nRight now, the dataset has 10 students data, which is rather small and if we train (offline mode training) any ML/DL model, there might be a chance that the model will be over-fitted easily. So, by increasing the data samples in the MDEAW dataset we can avoid the issue so that ML/DL models can learn and generalize properly and those trained models can be used for emotion classification from new EDA and PPG signals. Also, making the MDEAW dataset diverse, balanced so that the ML/DL model's accuracy can be good for emotion classification.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The classroom where the experiment was conducted (Eurecat Academy, Sabadell, Barcelona).",
      "page": 2
    },
    {
      "caption": "Figure 2: 1https://www.paulekman.com/universal-emotions/",
      "page": 2
    },
    {
      "caption": "Figure 2: Shimmer Sensors",
      "page": 3
    },
    {
      "caption": "Figure 3: Students wearing Shimmer sensors in the classroom while taking the exams.",
      "page": 3
    },
    {
      "caption": "Figure 4: ) is from the class while doing the experiment.",
      "page": 3
    },
    {
      "caption": "Figure 5: ), the software interface while",
      "page": 3
    },
    {
      "caption": "Figure 6: , we can see that Consensys software saves the ﬁle in the following format:",
      "page": 3
    },
    {
      "caption": "Figure 7: The emotion labels were saved in .csv ﬁle with the ﬁle name which contains [session name + student",
      "page": 3
    },
    {
      "caption": "Figure 4: Students taking the exams with the senors wearing.",
      "page": 4
    },
    {
      "caption": "Figure 5: Shimmer’s Consensys software interface while collecting the data.",
      "page": 4
    },
    {
      "caption": "Figure 6: Default ﬁle format in Shimmer’s Consensys software.",
      "page": 5
    },
    {
      "caption": "Figure 7: Reading data from the .mat ﬁle.",
      "page": 5
    },
    {
      "caption": "Figure 9: , the emotion class frequencies per student is presented and in Fig. 10 the overall emotion class",
      "page": 5
    },
    {
      "caption": "Figure 8: MDEAW dataset folder structure and ﬁles inside it.",
      "page": 6
    },
    {
      "caption": "Figure 9: Number of emotion class labels for individual student in MDEAW dataset.",
      "page": 7
    },
    {
      "caption": "Figure 10: Emotion class distribution in MDEAW dataset",
      "page": 8
    },
    {
      "caption": "Figure 11: Confusion matrix showing the individual class accuracy for MDEAW using ReMECS",
      "page": 8
    },
    {
      "caption": "Figure 12: Fig. 12. Global model’s performance in Fed-ReMECS for different client scenario (5 and 10 clients) using MDEAW dataset.",
      "page": 25
    },
    {
      "caption": "Figure 13: Global model’s performance in Fed-ReMECS-U for different client scenario (5 and 10 clients) using MDEAW dataset.",
      "page": 26
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No. of clients": "5",
          "Accuracy": "0.9315 (±0.16)"
        },
        {
          "No. of clients": "10",
          "Accuracy": "0.6195 (±0.25)"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No. of clients": "5",
          "Accuracy": "0.9487 (±0.13)"
        },
        {
          "No. of clients": "10",
          "Accuracy": "0.9410 (±0.15)"
        }
      ],
      "page": 25
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion",
      "doi": "10.1080/02699939208411068"
    },
    {
      "citation_id": "2",
      "title": "Wavelet analysis based classification of emotion from eeg signal",
      "authors": [
        "M Islam",
        "M Ahmad"
      ],
      "year": "2019",
      "venue": "Int'l Conf. on Electrical"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "4",
      "title": "Eeg signal classification using wavelet feature extraction and a mixture of expert model",
      "authors": [
        "A Subasi"
      ],
      "year": "2007",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "5",
      "title": "Emotion assessment using feature fusion and decision fusion classification based on physiological data: Are we there yet?",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Silva"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "E Kamasak"
      ],
      "year": "2020",
      "venue": "J. of Medical and Biological Eng"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition via random forest and galvanic skin response: Comparison of time based feature sets, window sizes and wavelet approaches",
      "authors": [
        "D Ayata",
        "Y Yaslan",
        "M Kamas"
      ],
      "year": "2016",
      "venue": "Medical Technologies National Congress"
    },
    {
      "citation_id": "8",
      "title": "Real-time multimodal emotion classification system in e-learning context",
      "authors": [
        "A Nandi",
        "F Xhafa",
        "L Subirats",
        "S Fort"
      ],
      "year": "2021",
      "venue": "Proceedings of the 22nd Engineering Applications of Neural Networks Conference"
    },
    {
      "citation_id": "9",
      "title": "Diversity in machine learning",
      "authors": [
        "Z Gong",
        "P Zhong",
        "W Hu"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "A federated learning method for real-time emotion state classification from multi-modal streaming",
      "authors": [
        "A Nandi",
        "F Xhafa"
      ],
      "year": "2022",
      "venue": "Methods"
    },
    {
      "citation_id": "11",
      "title": "Federated learning with exponentially weighted moving average for real-time emotion classification",
      "authors": [
        "A Nandi",
        "F Xhafa",
        "L Subirats",
        "S Fort"
      ],
      "year": "2022",
      "venue": "Proceedings of the 13th 13th International Symposium on Ambient Intelligence Conference"
    }
  ]
}