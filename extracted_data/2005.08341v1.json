{
  "paper_id": "2005.08341v1",
  "title": "Impact Of Multiple Modalities On Emotion Recognition: Investigation Into 3D Facial Landmarks, Action Units, And Physiological Data",
  "published": "2020-05-17T18:59:57Z",
  "authors": [
    "Diego Fabiano",
    "Manikandan Jaishanker",
    "Shaun Canavan"
  ],
  "keywords": [
    "Multimodal",
    "emotion recognition",
    "action units",
    "3D landmarks",
    "physiological data"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "To fully understand the complexities of human emotion, the integration of multiple physical features from different modalities can be advantageous. Considering this, we present an analysis of 3D facial data, action units, and physiological data as it relates to their impact on emotion recognition. We analyze each modality independently, as well as the fusion of each for recognizing human emotion. This analysis includes which features are most important for specific emotions (e.g. happy). Our analysis indicates that both 3D facial landmarks and physiological data are encouraging for expression/emotion recognition. On the other hand, while action units can positively impact emotion recognition when fused with other modalities, the results suggest it is difficult to detect emotion using them in a unimodal fashion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Recognizing emotion is considered one of the most important parts of human intelligence  [25]  and it has applications in fields as varied as entertainment, transportation, medicine and health, and psychology. Due to this, there has been a great deal of research into human emotion recognition in the past decades, where many important advances have been made. This is due in part to the new availability of large, varied, and challenging datasets  [6, 13, 20, 23, 27, 28, 30, 33] .\n\nThere is a large and varied body of work into facial expression recognition. Using a Spatio-Temporal Hidden Markov Model (HMM), the intra-and inter-frame information can be used for this task  [29] . It has been shown that using a random forest  [2]  along with a Deformation Vector Field  [8] , to obtain the local deformations of the face over time, can be used to accurately classify expressions. Facial expressions have also been successfully classified using a Support Vector Machine (SVM) with a radial basis function (RBF) kernel with geometrical coordinates, as well as the normal of the coordinates  [15] . Lucey et al  [22] , analyzed videos of patients with shoulder injuries to automatically recognize pain. In this work, an Active Appearance Model  [4]  was used to detect Facial Action Units to distinguish pain on facial expressions. They detail 84.7 for area under the ROC curve on the UNBC-McMaster Shoulder Pain Database  [26] . This study is encouraging as it suggests Action Units can be used to recognize emotions (e.g. pain).\n\nDeep learning has shown recent success in expression recognition. Using a Boosted Deep Belief Network, Liu et al.  [21]  trained feature learning, selection, and classifier construction iteratively in a unified loopy framework; which showed an increase in the classification accuracy. Deexpression Residue Learning  [31]  was also proposed which can generate a corresponding neutral expression given an arbitrary facial expression from an image. Yang et al.  [32]  proposed regenerating expression from input facial images. By using a conditional GAN  [24] , they developed an identity adaptive feature space that can handle variations in subjects.\n\nFacial expression recognition is a popular approach to recognizing emotion, however, there is also a varied body of work that makes use of multimodal data for emotion recognition. Soleymani et al.  [27]  incorporated electroencephalogram, pupillary response, and gaze distance information from 20 videos. They used this data to train an SVM to classify arousal and valence for 24 participants. Kessous also showed an increase of more than 10% when using a multimodal approach  [18] . They used a Bayesian classifier, and fused facial expression with speech data that consisted of multiple languages including Greek, French, German, and Italian.\n\nWhile these works, and others, have had success detecting expressions and emotion with multimodal data, little work has been done on analyzing their impact on recognition. Motivated by this, we present an analysis of multimodal data and the impact each modality has on emotion recognition. Our contributions can be summarized as follows:\n\n1. A detailed analysis of physiological data, 3D landmarks, and facial action units (AU)  [11]  both independently and combined at the feature level (unimodal vs. multimodal), for emotion recognition, is presented. 2. Insight is provided on the impact of physiological data, 3D landmarks, and AUs for positively influencing emotion recognition studies. 3. To the best of our knowledge, this is the first work to conduct this type of analysis on the BP4D+ multimodal dataset  [35] , resulting in a baseline for future analyses.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Selection And Feature Extraction",
      "text": "We propose to use 3D facial data (landmarks), action units and physiological data in our analysis. We chose these 3 modalities based on their complementary nature. First, given movement, and the shape of the face changes (3D landmarks), we can also assume that there will be a change in the occurrence of action units  [10] . We have also chosen the complementary modality, physiological data, as facial expressions can be faked. It has been observed that people smile during negative emotional experiences  [9] . Considering this, physiological data can complement the other two modalities for recognizing emotion.\n\nTo conduct the proposed analysis, a suitably large corpus of emotion data is needed that contains 3D facial data, action units, and physiological data. For our experiments we have chosen the BP4D+ multimodal spontaneous emotion corpus  [35] . In total, there are over 1.5 million frames of multimodal available in the BP4D+. For this study we use 192,452 frames of multimodal data from all 140 subjects. This subset of data contains 4 target emotions that are happiness, embarrassment, fear, and pain. We are using this subset, as it is largest set of frames, in BP4D+, that are encoded with action units.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "3D Facial Data",
      "text": "For our study we used 83 3D facial landmarks (same as seen in BP4D+) to represent the face. Each of the landmarks were detected using a shape index-based statistical shape model (SI-SSM)  [3] , that creates shape index-based patches from global and local features of the face. These global and local features are concatenated into one model, which is then used along with a cross-correlation matching technique to match the training data to an input mesh model. Examples of detected 3D facial landmarks can be seen in Fig.  1 . For our 3D facial data feature vector, we directly use the coordinates (x, y, z) of the 3D tracked facial landmarks as they can accurately represent the induced expression that can be seen in the entire 3D model  [12] , which contains approximately 30k-50k vertices; where our reduced feature vector contains 249 features (83 3D coordinates). Using this reduced feature space (relative to the entire 3D mesh) allows for lower dimensional data, without sacrificing any recognition accuracy.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Action Units",
      "text": "For each of the 4 emotions that have action units coded, a total of 35 action units (AUs) were coded by five different expert FACS coders. For each task of all 140 subjects approximately 20 seconds of the most expressive part of the sequence was annotated, giving us our 192,452 frames of multimodal data that we use for our study. For our AU feature vector, we include the occurrence of all annotated AUs for each frame where 1 corresponds to the AU being present and 0 corresponds to the AU not being present in the current frame. There are some instances in the BP4D+ where the AU occurrence is listed as 9, which is referred to as unknown. For our experiments, 9 is treated as a 0 (i.e. not present).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Physiological Data",
      "text": "For each subject and task, the BP4D+ contains 8 separate measurements of physiological data derived from blood pressure (BP), heart rate (HR), respiration (RESP), and skin conductivity (EDA). All physiological data was sampled at 1000 Hz which required us to synchronize with the available 3D facial data and corresponding action units to have accurate readings for each frame of data. To synchronize this, we first divide the total number of frames of physiological data by the total number of frames of 3D facial data for that task (average sync value). We then use the average value over the average sync value as our new frame. For example, given a task with 1000 frames of 3D facial data, along with 40,000 frames of diastolic BP we would have 40, 000/1000 = 40, resulting in us taking the average diastolic BP for every 40 frames. Calculating the mentioned average over all 40,000 frames, results in 1000 frames of diastolic BP matching to the 1000 frames of corresponding 3D facial data. In this same task, there are 400 frames that include both 3D facial landmarks and AUs (frames labeled with task, subject, and frame number). We then use the corresponding frame number to extract that exact index from the calculated diastolic BP averages. This gives us our resulting 400 frames of synchronized 3D facial data, physiological data, and action units. For our physiological feature vector, we take the average value of each frame over all eight of the data types (i.e. fuse the signals).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Design And Results",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Analysis",
      "text": "A main contribution of this work is analyzing which modality and features are most important for our 4 target emotions. To do this we used principal component analysis (PCA) for fea- ture selection keeping 95% of the original variance. We did this for each of our unimodal feature vectors for all the training data, as well as each individual emotion. This was done to analyze which features are important for emotion recognition in a general sense, and for each targeted emotion resulting in a total of 15 total rankings (3 feature vectors for each: happy, embarrassment, pain, fear, and all emotions). The features were then ranked based on highest eigenvalue.\n\nAction Units. The top ranked action units included the lips, cheeks, nose, and eye/eyebrow regions. Across each of the target emotions, along with all combined emotions the ranked AUs were similar. The difference being their rankings change across different emotion (e.g. AU12 was ranked first for happy, while AU12 was ranked second for embarrassed). Table  1 , second column, shows the top 5 ranked AUs. As can be seen here the top AUs for Happy are 12, 6, 11, and 7. When considering the Emotion Facial Action Coding System  [16] , which only looks at emotion-related facial action, Happy, is 6+12. This shows a correlation between the PCA rankings and the action units associated with the emotion. We also calculated the normalized AU distribution across each target emotion. This showed that while each emotion had similar occurring action units, they varied in distribution, which contributes complimentary information to the other modalities. This can explain the increase in accuracy when a multimodal approach is used (Table  3 ).\n\nPhysiological Data. Most of the top ranked features for physiological data were variations on blood pressure (e.g. diastolic and systolic). Pulse rate was also ranked as a top feature for each of the target emotions, however, when all emotions were included in the training data, pulse rate was replaced by EDA. This suggests that skin conductivity is important for recognizing multiple emotions. It is interesting to note that for each of the 4 target emotions, not only were the top ranked features the same, they were also ranked in the same order. Although each emotion had the same ranked physiological data, they all had large variations in the data between them. This variance in data allows for a high level of recognition accuracy (Table  2 ). Table  1 , third column, shows the top 5 ranked physiological signals.\n\nTable  1 . PCA rankings for each feature for each individual emotion along with all 4 target emotions, shown in ranked order. NOTE: number in parentheses in column four corresponds to total number of landmarks in that region.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Emotion",
      "text": "Action Units Phys 3D Facial Landmarks Lip corner puller  (12)  Mean BP Cheek raiser  (6)  Diastolic BP Happy Upper lip raiser  (10)  Raw BP Right eye (2) Nasolabial deepener  (11)  Pulse Rate Right eyebrow (3) Lid tightener  (7)  Cheek raiser  (6)  Mean BP Lip corner puller  (12)  Diastolic BP Embarrassed Upper lip raiser  (10)  Systolic BP Left face contour (2) Lid tightener  (7)  Raw BP Left eyebrow (3) Nasolabial deepener  (11)  Pulse Rate Lip Corner Puller  (12)  Mean BP Cheek raiser  (6)  Diastolic BP Right eyebrow (2) Pain\n\nUpper lip raiser  (10)  Systolic BP Nose (2) Nasolabial deepener  (11)  Raw BP Left eyebrow(1) Lid tightener  (7)  Pulse Rate Upper lip raiser  (10)  Mean BP Cheek raiser  (6)  Diastolic BP Fear Lid tightener (  7 ) Systolic BP Right eyebrow (5) Lip corner puller  (12)  Raw BP Nasolabial deepener  (11)  Pulse Rate Lip corner puller  (12)  Mean BP Upper lip raiser  (10)  Diastolic BP All Cheek raiser (  6 ) Systolic BP Left eyebrow (5) Lid tightener  (7)  Raw BP Nasolabial deepener  (11)  EDA 3D Facial Data. When analyzing the 3D facial data, each of the target emotions show variance in the regions of the face that were ranked for the top features. For example, happy targeted the right eye and eyebrow, and pain was across the right eyebrow, nose, and left eyebrow. These regions of the face are also consistent with the AUs ranked as the top features (e.g. mouth, face, eyes/eyebrows). See Table  1  for the top 5 ranked 3D facial landmarks (face region for each) and Fig.  2  for examples of these landmarks on corresponding 3D mesh models. It can be seen, in Fig.  2 , that emotional variance is conveyed in different 3D regions of the face for each of the target emotions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition",
      "text": "To conduct our emotion recognition experiments, we created a feature vector for each unimodal and multimodal configuration (Tables  2  and 3 ). We then used each of these feature vectors to train a random forest  [2]  for recognizing the four target emotions. Random forests have successfully been used in a wide variety of classification tasks such as classifying ecological data  [7] , real-time hand gesture recognition  [36] , and head pose estimation  [14] , which makes them a natural fit for our analysis.\n\nUnimodal vs. Multimodal Emotion Recognition. We used 10-fold cross validation for each of our experiments. The results for unimodal and multimodal emotion recognition can be seen in Tables  2  and 3  respectively. When physiological data was used, recognition accuracy was highest for both unimodal and multimodal approaches, achieving an accuracy of 99.94% for the 4 target emotions, with a unimodal approach. This result is intuitive as physiological signals are closely tied to human emotion  [19, 20] . When AUs were combined with physiological data we achieved our highest recognition accuracy of 99.95%. This also agrees with the literature that the fusion of multimodal data, including action units, can provide complimentary information and increase recognition accuracy  [5] . Although emotion recognition from AUs shows promising results, especially when fused other modalities, they exhibit the lowest classification rate of the unimodal feature vectors with a recognition accuracy of 61.94%. The confusion matrices for AUs, physiological data, and AUs combined with physiological data are shown in Tables  4, 5 , and 6 respectively. Fusing multimodal data has been found to increase emotion recognition including pain in infants  [34] . Our results show similar results with pain as well, increasing from 99.92% with physiological data to 99.98% when AUs were fused with physiological data. It is interesting to note, that while the overall recognition accuracy was higher when AUs were combined with physiological data, the recognition rates for both happy and fear decreased to 99.94% and 99.90% respectively. This can be attributed to some redundant action unit patterns between happy and fear.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Discussion",
      "text": "We have presented an analysis on impact of 3D facial landmarks, action units, and physiological data for emotion recognition. We have conducted experiments in both a unimodal and multimodal capacity on four target emotions. Our analysis has shown that 3D facial data shows variations in facial regions allowing for accurate emotion recognition. We have also shown that physiological data can be used for emotion recognition due to the changes across emotion. The occurrence of action units shows differences in distribution over 35 AUs across the four-target emotions, which allows for complimentary information to be used when fusing the AUs with other modalities at the feature level. Although the fusion of AUs is shown to increase the accuracy across the four tested emotions, the results also show that directly using AU occurrences without fusing other modalities, for emotion recognition, is still a challenging problem. These results suggest more research is needed to determine the positive impact of using action units in a unimodal approach for emotion recognition. While these results are encouraging, there are some limitations to the study. First, more multimodal databases need to be investigated, as our study only made use of BP4D+. Secondly, more details are needed as to why the fusion of AU occurrences showed an increase in accuracy, while using them in a unimodal capacity generated a relatively low accuracy. Lastly, our current study only focused on four emotions due to the limited number of available action units. A much larger range of emotions are needed to fully test the efficacy of the proposed approach. Considering this, for our future work, we will detect action units  [1] , across a larger set of data, as well as use deep neural networks and other fusion methods including score level fusion, and the fusion of deep and hand-crafted features  [17] . We will also test on a larger set of multimodal datasets, and we will investigate the impact of both AU occurrences and intensities for emotion recognition. These experiments will be conducted across a larger set of emotions that include, but are not limited to, surprise, sadness, anger, and disgust. Along with these emotions, we will also investigate subject self-reporting of emotion (i.e. perceived emotion).",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: For our 3D",
      "page": 2
    },
    {
      "caption": "Figure 1: 3D facial landmarks on corresponding 3D mesh model",
      "page": 2
    },
    {
      "caption": "Figure 2: Top 5 ranked 3D facial features across the 4 emotions.",
      "page": 3
    },
    {
      "caption": "Figure 2: for examples of these landmarks on corresponding 3D mesh",
      "page": 3
    },
    {
      "caption": "Figure 2: , that emotional variance is",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: PCA rankings for each feature for each individual",
      "data": [
        {
          "Emotion": "Happy",
          "Action Units": "Lip corner puller (12)\nCheek raiser (6)\nUpper lip raiser (10)\nNasolabial deepener (11)\nLid tightener (7)",
          "Phys": "Mean BP\nDiastolic BP\nRaw BP\nPulse Rate",
          "3D Facial\nLandmarks": "Right eye (2)\nRight eyebrow (3)"
        },
        {
          "Emotion": "Embarrassed",
          "Action Units": "Cheek raiser (6)\nLip corner puller (12)\nUpper lip raiser (10)\nLid tightener (7)\nNasolabial deepener (11)",
          "Phys": "Mean BP\nDiastolic BP\nSystolic BP\nRaw BP\nPulse Rate",
          "3D Facial\nLandmarks": "Left face contour (2)\nLeft eyebrow (3)"
        },
        {
          "Emotion": "Pain",
          "Action Units": "Lip Corner Puller (12)\nCheek raiser (6)\nUpper lip raiser (10)\nNasolabial deepener (11)\nLid tightener (7)",
          "Phys": "Mean BP\nDiastolic BP\nSystolic BP\nRaw BP\nPulse Rate",
          "3D Facial\nLandmarks": "Right eyebrow (2)\nNose (2)\nLeft eyebrow(1)"
        },
        {
          "Emotion": "Fear",
          "Action Units": "Upper lip raiser (10)\nCheek raiser (6)\nLid tightener (7)\nLip corner puller (12)\nNasolabial deepener (11)",
          "Phys": "Mean BP\nDiastolic BP\nSystolic BP\nRaw BP\nPulse Rate",
          "3D Facial\nLandmarks": "Right eyebrow (5)"
        },
        {
          "Emotion": "All",
          "Action Units": "Lip corner puller (12)\nUpper lip raiser (10)\nCheek raiser (6)\nLid tightener (7)\nNasolabial deepener (11)",
          "Phys": "Mean BP\nDiastolic BP\nSystolic BP\nRaw BP\nEDA",
          "3D Facial\nLandmarks": "Left eyebrow (5)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happy": "51512",
          "Embarrassment": "10",
          "Fear": "5",
          "Pain": "4"
        },
        {
          "Happy": "21",
          "Embarrassment": "52080",
          "Fear": "4",
          "Pain": "14"
        },
        {
          "Happy": "4",
          "Embarrassment": "7",
          "Fear": "36780",
          "Pain": "3"
        },
        {
          "Happy": "22",
          "Embarrassment": "13",
          "Fear": "6",
          "Pain": "51967"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happy": "51504",
          "Embarrassment": "10",
          "Fear": "5",
          "Pain": "4"
        },
        {
          "Happy": "10",
          "Embarrassment": "52100",
          "Fear": "3",
          "Pain": "6"
        },
        {
          "Happy": "14",
          "Embarrassment": "16",
          "Fear": "36758",
          "Pain": "6"
        },
        {
          "Happy": "3",
          "Embarrassment": "9",
          "Fear": "1",
          "Pain": "51995"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happy": "32511",
          "Embarrassment": "7730",
          "Fear": "3373",
          "Pain": "7917"
        },
        {
          "Happy": "17561",
          "Embarrassment": "26038",
          "Fear": "3238",
          "Pain": "5282"
        },
        {
          "Happy": "8773",
          "Embarrassment": "5206",
          "Fear": "14652",
          "Pain": "8163"
        },
        {
          "Happy": "1983",
          "Embarrassment": "2334",
          "Fear": "1685",
          "Pain": "46006"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltru≈°aitis"
      ],
      "year": "2016",
      "venue": "WACV"
    },
    {
      "citation_id": "3",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine learning"
    },
    {
      "citation_id": "4",
      "title": "Landmark localization on 3d/4d range data using a shape index-based stat shape model with global and local constraints",
      "authors": [
        "S Canavan"
      ],
      "year": "2015",
      "venue": "CVIU"
    },
    {
      "citation_id": "5",
      "title": "Active appearance models",
      "authors": [
        "T Cootes"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on PAMI"
    },
    {
      "citation_id": "6",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu"
      ],
      "year": "2016",
      "venue": "IEEE transactions on PAMI"
    },
    {
      "citation_id": "7",
      "title": "A facs valid 3d dynamic action unit database with applications to 3d dynamic morphable facial modeling",
      "authors": [
        "D Cosker",
        "E Krumhuber",
        "A Hilton"
      ],
      "year": "2011",
      "venue": "ICCV"
    },
    {
      "citation_id": "8",
      "title": "Random forests for classification in ecology",
      "authors": [
        "D Cutler"
      ],
      "year": "2007",
      "venue": "Ecology"
    },
    {
      "citation_id": "9",
      "title": "Drira and others. 3d dynamic expression recognition based on a novel deformation vector field and random forest",
      "year": "2012",
      "venue": "ICPR"
    },
    {
      "citation_id": "10",
      "title": "The argument and evidence about universals in facial expressions. Handbook of social psychophysiology",
      "authors": [
        "P Ekman"
      ],
      "year": "1989",
      "venue": "The argument and evidence about universals in facial expressions. Handbook of social psychophysiology"
    },
    {
      "citation_id": "11",
      "title": "The facial action coding system: A technique for the measurement of facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "The facial action coding system: A technique for the measurement of facial movement"
    },
    {
      "citation_id": "12",
      "title": "What the face reveals: Basic and applied studies of spon exp using the facial action coding system (facs)",
      "authors": [
        "P Ekman",
        "E Rosenberg"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spon exp using the facial action coding system (facs)"
    },
    {
      "citation_id": "13",
      "title": "Spon and non-spontaneous 3d facial expression recognition using a statistical model with global and local constraints",
      "authors": [
        "D Fabiano",
        "S Canavan"
      ],
      "year": "2018",
      "venue": "ICIP"
    },
    {
      "citation_id": "14",
      "title": "A 3-d audio-vis corpus of affect comm",
      "authors": [
        "G Fanelli"
      ],
      "year": "2010",
      "venue": "IEEE Trans on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Real time head pose estimation with random regression forests",
      "authors": [
        "G Fanelli",
        "J Gall",
        "L Van Gool"
      ],
      "year": "2011",
      "venue": "CVPR 2011"
    },
    {
      "citation_id": "16",
      "title": "3d/4d facial expression analysis: An advanced annotated face model approach",
      "authors": [
        "T Fang"
      ],
      "year": "2012",
      "venue": "IVC"
    },
    {
      "citation_id": "17",
      "title": "Emfacs-7: Emotional facial action coding system",
      "authors": [
        "W Friesen",
        "P Ekman"
      ],
      "venue": "Uni of Cali at SF"
    },
    {
      "citation_id": "18",
      "title": "Fusion of hand-crafted and deep features for empathy prediction",
      "authors": [
        "S Hinduja"
      ],
      "year": "2019",
      "venue": "Fusion of hand-crafted and deep features for empathy prediction"
    },
    {
      "citation_id": "19",
      "title": "Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis",
      "authors": [
        "L Kessous"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "20",
      "title": "Phys signals and their use in augmenting emotion recognition for human-machine interaction",
      "authors": [
        "R Knapp"
      ],
      "year": "2011",
      "venue": "Emotion-oriented systems"
    },
    {
      "citation_id": "21",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Facial expression recognition via a boosted deep belief network",
      "authors": [
        "P Liu"
      ],
      "year": "2014",
      "venue": "CVPR"
    },
    {
      "citation_id": "23",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey"
      ],
      "year": "2010",
      "venue": "CVPRW"
    },
    {
      "citation_id": "24",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2014",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "26",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Trans on PAMI"
    },
    {
      "citation_id": "27",
      "title": "The structure, reliability and validity of pain expression: Evidence from patients with shoulder pain",
      "authors": [
        "P Prkachin",
        "K Solomon"
      ],
      "year": "2008",
      "venue": "Pain"
    },
    {
      "citation_id": "28",
      "title": "Multimodal emotion recognition in response to videos",
      "authors": [
        "M Soleymani",
        "M Pantic",
        "T Pun"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "29",
      "title": "Exploring the effect of illumination on automatic expression recognition using the ict-3drfe database",
      "authors": [
        "G Stratou"
      ],
      "year": "2012",
      "venue": "IVC"
    },
    {
      "citation_id": "30",
      "title": "Tracking vertex vlow and model adaptation for 3d spatio-temporal face analysis",
      "authors": [
        "Y Sun"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on SMC-A"
    },
    {
      "citation_id": "31",
      "title": "A natural visible and infrared facial expression database for expression rec and emotion inference",
      "authors": [
        "S Wang"
      ],
      "year": "2010",
      "venue": "IEEE Trans on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Facial expression recognition by deexpression residue learning",
      "authors": [
        "H Yang"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "33",
      "title": "Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks",
      "authors": [
        "H Yang"
      ],
      "year": "2018",
      "venue": "FG"
    },
    {
      "citation_id": "34",
      "title": "A high-resolution 3d dynamic facial expression database",
      "authors": [
        "L Yin"
      ],
      "year": "2008",
      "venue": "FG"
    },
    {
      "citation_id": "35",
      "title": "Machine-based multimodal pain assessment tool for infants: a review",
      "authors": [
        "G Zamzmi"
      ],
      "year": "2016",
      "venue": "Machine-based multimodal pain assessment tool for infants: a review",
      "arxiv": "arXiv:1607.00331"
    },
    {
      "citation_id": "36",
      "title": "Multi spon emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "37",
      "title": "Real-time hand gesture detection and recognition by random forest",
      "authors": [
        "X Zhao"
      ],
      "year": "2012",
      "venue": "Communications and information processing"
    }
  ]
}