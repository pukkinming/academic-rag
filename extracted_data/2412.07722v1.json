{
  "paper_id": "2412.07722v1",
  "title": "Feel My Speech: Automatic Speech Emotion Conversion For Tangible, Haptic, Or Proxemic Interaction Design",
  "published": "2024-12-10T18:14:45Z",
  "authors": [
    "Ilhan Aslan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Innovations in interaction design are increasingly driven by progress in machine learning fields. Automatic speech emotion recognition (SER) is such an example field on the rise, creating well performing models, which typically take as input a speech audio sample and provide as output digital labels or values describing the human emotion(s) embedded in the speech audio sample. Such labels and values are only abstract representations of the felt or expressed emotions, making it challenging to analyse them as experiences and work with them as design material for physical interactions, including tangible, haptic, or proxemic interactions. This paper argues that both the analysis of emotions and their use in interaction designs would benefit from alternative physical representations, which can be directly felt and socially communicated as bodily sensations or spatial behaviours. To this end, a method is described and a starter kit for speech emotion conversion is provided. Furthermore, opportunities of speech emotion conversion for new interaction designs are introduced, such as for interacting with animals or robots.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions in speech can be recognized by analysing both linguistic and paralinguistic features. Arguably, emotion recognition based on paralinguistic features (describing how something is said and not what is being said) is of greater utility and flexibility. One pragmatic reason is that extracting and making use of the linguistic part in speech can be an additional technical challenge and source of error. To transcribe speech into text, an automatic speech recognition (ASR) solution is needed. It is known that the accuracy of ASR can suffer from factors, such as background noise. The resulting (word) errors impact the performance of the subsequent speech emotion recognition task based on linguistic features  [3] . Another issue is that the analysis of linguistic features is typically languagedependent and can not be generalized to different languages as easily as paralinguistic features.\n\nTraditionally, emotion recognition tasks in machine learning (ML) focus on two types of emotion models, predicting (i) emotion labels for discrete emotion categories such as sad, happy, or angry; and predicting values in (ii) continuous emotion dimensions, such as valence and arousal. Arguably, predicting continues dimensions are of greater utility and flexibility. For example, with discrete emotion categories, one is limited to predefined categories. A person who's speech is labelled as happy can be still in very different emotional sates, which fall into the very broad category of happy. Moreover, emotion categories are of limited use if one is interested in studying (continuous) emotion transitions and trajectories  [12] .\n\nConsidering the field and task of speech emotion recognition (SER), important progress has been achieved by utilizing transformer architectures, successfully closing the \"valence gab\"  [31] ; i.e., successfully using paralinguistic features only to predict the valence dimension in speech emotion recognition tasks.\n\nIn general, affective computing solutions are becoming increasingly valuable for the design of novel user interfaces that can adapt to users, stimulate positive affective states, and promote well-being. For example, in a previous art installation at Augsburg's architecture museum, in Germany, we explored a proactive conversational design which perceived features of a person based on ML (e.g., age, hair colour, beard, glasses) to generate personalized compliments and deliver them via text and speech synthesis  [5] . In an other work, we had robots telling jokes and adapting their behaviour and joke choices to individual user's humour and affective responses, such as laughter and smiles  [33] .\n\nClearly ML based affective computing solutions can be useful in building interactive systems that can perceive complex user behaviours, create novel interactive experiences, and impact users' affective states. But, for various reasons, including technical barriers, it is still challenging to exploit state of the art solutions based on ML for interaction design and artistic research.\n\nIn addition to technical barriers, often there is the issue that research in ML doesn't stop where research in interaction design and human-computer interaction starts. Arguably, ML is about machine-centered research and design, which in many cases has the goal to increase machine capabilities and autonomy. Thus, there is a lack in making such rigid ML models ready to be of flexible use for interaction design and human-computer interaction. A pragmatic solution today, is to provide tool-support to allow designers in freely and creatively exploring ML models out of their original scope as new design material.\n\nThe next section focuses on the topic of converting speech emotions into \"physical\" emotions (e.g., tangible, haptic, or proxemic experiences). I describe a method and starter kit for automatic speech emotion conversion, and provide an outlook into future design opportunities. For the purpose of this paper, I use the terms emotions or affective states very colloquially.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "\"Physical Emotion\" Generation From Speech",
      "text": "One automated way to generate physical emotions from speech is by first using ML models to perform a speech emotion recognition (SER) task and then converting the resulting emotion values into arXiv:2412.07722v1 [cs.HC] 10 Dec 2024 tangible or haptic sensation and actions with the help of maker kits and the use of servo motors or vibration motors.\n\nFor this approach, a first implementation in the scope of a starter kit is described to help fellow researchers explore physical interaction design ideas utilizing speech emotion conversion (SEC). I plan to make the starter kit available open source. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition (Ser)",
      "text": "The field of SER has benefitted from various past emotion recognition challenges, with the first official challenge happening at INTERSPEECH 2009. Since then the field has seen tremendous progress developing new and superior methods especially based on deep neural networks to tackle the task of recognizing emotions in speech. Today, SER is increasingly used to predict continuos emotion dimension values, especially for the dimensions valence, arousal, and dominance. Reflecting on past SER challenges Triantafyllopoulos et al.  [28]  describe how SER methods have evolved throughout the last 15 years from using multi-layered perceptrons, long short-term memory recurrent NNs, convolutional NNs, to transformer based models and self-supervised learning approaches. The rapid evolution in SER methods is both impressive and concerning with regards to opportunities for interdisciplinary contributions and application of results in new interaction designs.\n\nThe current state of art in SER methods are transformer based  [31]  which make use of a wave2vec architecture for self-supervised learning of speech representations  [8] , which means that these architectures are typically pre-trained with large amounts of available raw audio data and have already learned to represent audio and speech data in an effective and general way. These pre-trained models can be used as a foundation to create task specific solutions including the task of recognizing emotions in speech taking raw audio as input. Being able to provide SER models speech audio data in raw format without having to preprocess it is really a convenience for anyone who wants to apply SER in interaction designs. 1 https://github.com/islani/SEC_Tool",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Conversion (Sec) Kit",
      "text": "The task of converting speech emotions into physical emotions can be realized by implementing 3 parts, which need to be interconnected: (i) sensing emotion from speech, (ii) generating tangible, haptic, etc. sensations using physical displays, and (iii) applying rules or methods to map speech emotions into physical sensations and actions. Each part is extendable.\n\nFigure  1  presents an overview of the system architecture for the proposed starter kit. Intended users of this kit are designers, researchers, educators, or artists who want to exploit speech emotions as design material in their projects and explore novel ideas and bodily experiences. The starter kit integrates the SER part in a command line tool with a command line interface to be flexible and easy to use. Multiple arguments can be set for customization, such as how the continuous speech signal should be divided into audio junks for the SER task, allowing users to receive emotion labels, for example, for every second of audio or every 10 seconds of audio. Different audio junk sizes can result in different emotion labels and depending on use case one may want to explore different sizes. Technically, the speech emotion sensing part is the most demanding part to realize, since it requires state of the art ML solutions and comes with all the challenges of using a black box solution.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "2.2.1",
      "text": "Realizing affect-sensing. Fortunately, the number of ready to use affect-sensing tools is increasing. Large IT companies provide mainly cloud based solutions and APIs for developers to augment their products with affect-sensitive and \"affect-exploiting\" features.\n\nIn contrast, the AffectToolbox  [18]  is a tool recently provided by researchers of the human-centered AI lab at Augsburg University, which I have had the privilege to be part of in the past. The lab has a long history in providing high quality research on the topic of socially intelligent human-agent interactions and tools to support related research, helping overcome technical tasks, such as signal processing for multimodal human-computer interaction. The AffectToolBox aims to foster a more inclusive and collaborative environment to advance the field of affective computing research, by being (i) easy to use, (ii) comprehensive, (iii) easy to integrate and (iv) open source. The tool predicts values for continuous emotion dimensions valence(pleasure), arousal, and dominance, allowing the fusion of multiple modalities that are of interest in interactions between humans and embodied agents. For the analysis of human affect, the toolbox provides \"out of the box\" analysis of paralinguistic features in speech, sentiment analysis (from speech transcriptions), facial expressions and pose from camera. The toolbox is for online analysis and uses live microphone and camera data streams as input. The results are both displayed for the user on a graphical user interface (GUI) and can be send, for example, via UDP to a UDP server to process the results in an other application.\n\nThe GUI (see Figure  2 ) provides users ease of use for selecting the components in the signal processing pipeline they want to use for their specific use case. Considering SER, the Affectoolbox integrates the SER model described and provided by Wagner et al.  [31]  (another former member of the Human-Centered AI lab in Augsburg) ushering a new era for SER based on transformer architectures and setting a new state of art for SER.\n\nOverall the AffectToolbox is highly useful for anyone interested in prototyping and studying affect-sensitive systems, including interaction designers, artists, and educators who otherwise would face technical barriers. Arguably, its main strength is in providing a ready to use tool for multimodal human affect analysis. As the purpose of this paper is to provide a SEC solution and explore speech emotions as design material, I extracted and modified the SER part from the toolbox and packed it as a command line tool to suit the concrete purpose of creating a (minimal) starter kit for converting abstract speech emotion representations into physical counterparts that can be directly felt, explored, and utilized for new tangibly, haptically, or proxemicly affective interaction designs.\n\n2.2.2 Computational Generation of \"Physical\" Emotions. The command line SER tool accepts as input arguments to set how the emotion recognition results should be send to another device or app for displaying the emotions as physical sensations and integrating them into multisensory interactive experiences. Currently two options are provided for sending the SER results to an external (maker) board (e.g. Arduino 2 ) or/and a separate application (e.g. a 2 https://www.arduino.cc/ Processing 3  application to visualize speech emotions). The starter kit comes with an example for both (i) an Arduino sketch which reads speech emotions over the serial port connection and displays the values physically using a simple vibrotactile motor and (ii) a Processing sketch which receives the SER results through a TCP socket connection and visualizes them.\n\nI chose these two platforms, since they are known to be suitable for entry level programming and tinkering with large communities of users, including multimedia artists. They are also often used in introductory programming courses in interdisciplinary teaching programs. The design space to map speech emotions to physical sensations and behaviours is vast and there is no one solution fits all since emotion communication is typically context-sensitive and hardware to display physical emotions are not standardized (yet). To design desired speech to physical emotion mappings we, as designers, could for example perform participatory and human centred design to elicit and model user preferences. We have in the past applied participatory design techniques to elicit contextual interaction preferences for gestural interaction  [6] , I believe that similar techniques are suitable for designing physical and contextual mappings of speech emotions. The novelty and subjectivity of speech emotions as design material would also support the application of self-reflective and auto-ethnographic research approaches.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Design Opportunities Created By Sec",
      "text": "The SEC research is aimed to complement fellow researchers' efforts in addressing for example multisensory experiences  [20, 30]  and studying the relationship between affect, emotions, empathy, and (mid-air) haptics  [1, 17, 20, 37] . Being able to automatically generate physical emotions from speech has many implications for future interaction designs. To illustrate the potential impact of SEC, I provide next a few initial design ideas and opportunities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sec For Animal-Computer And Animal-Human Interaction",
      "text": "Human-animal communication and bonding can be beneficial for mental health  [10, 32]  and address issues of loneliness in humans. Past research has addressed the creation of empathic spaces and identified the potential for artistic responsible research  [22, 25] . Pet owners (like myself) know the role of emotions in communicating with a pet. For example, it is not unusually for vets to instruct owners to keep speaking with their pets in calm and pleasant tones to keep their dog or cat relaxed during health checks or vaccinations. Owners of such companion animals are known to have strong emotional connections with their animals  [15] , considering them often as part of the family and providing them with affection, including speech-based affection. Quaranta et al.  [23]  have demonstrated that cats use both visual and auditory signals to recognize and (appear to) modulate their behaviour to the valence of the perceived emotions. Similar to human-human interactions, technology has also potential to mediate human-animal communication and bonding. The potential may be even higher since animals may only understand a small set of commands. But, animals like humans can need assistive technology.\n\nIn any case, communication with an animal is in general challenging. However, the tone and sentiment in human voices is something that companion animals seem to be able to recognize and use for adapting their behaviour. Haptic feedback is a modality that is already used for pets, there are for example collars which can provide haptic (or auditive) feedback in an attempt to train animals (e.g., to have a cat stay away from danger or from \"playing\" with the neighbours birds).\n\nA haptic collar could be an additional design option for a (deaf) cat or dog to experience speech emotions. An alternative, which would provide the pets more self-determination could be a pet blanket which can convert speech emotions to physical feedback (haptic, thermal, or smellable). Both design opportunities expand the interaction space and have the potential to fostering (or disrupt if not carefully designed) human-animal bonding. Owner of pets tend to pet their pets while speaking to them, which is of course only possible when owner and pet are co-located. There is some research showing that remote interaction (e.g., video chat) with pets can be reasonable  [13]  and in such situations speech emotion conversion could be used to provide automatic petting like physical sensations. There is also emerging research in recognizing vocal emotions of animals  [27] , which could be used to also assist humans in better understanding the emotional vocal expressions of their companion animals.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Sec For Proxemic Interaction",
      "text": "Proxemic interaction (e.g.,  [9, 14] ) was introduce by Saul Greenberg's research group at the University of Calgary, questioning if it is the new ubicomp. Put differently, if enabling machines to interact with humans in spatio-socially intelligent ways is the future goal of ubiquitous computing research. In this future, as one can partly witness today computing is integrated in environments, comes in different physical forms, and interaction designs are increasingly of implicit nature  [11] . One could argue, while recently big steps in AI have been made, especially regarding language technologies, machines are still non-mobile and very reactive when it comes to spatial interactions with humans. The most relevant and related progress is happening arguably in robots and autonomous cars. Allowing mobile (and autonomous) machines to share the same physical space is also a (safety) critical decision.\n\nIf we want machines to support humans in (collaborative) physical tasks then there is need to realize proxemic interactions where machines can also use proactive proxemics, i.e., make the first move. To do this they require affective computing abilities, to foresee human action tendencies and communicate action tendencies themselves through both verbal and non-verbal behaviour. In the field of human-robot interaction, even simple head and locomotions movements of robots can be used to design emotionally expressive emotions  [29] . Now, how could speech emotions be converted into proxemic behaviour (e.g., change in body orientations and distance)? One could for example, set automatically the acceptable distance and path that a social robot can take while passing by a person or a group of persons by converting their speech emotions in to physical distances. For example, person(s) sounding angry could map to a minimum proximity of 2 meters to always keep between robot and person(s), while if the speech sound changes to happy then the distance could shrink to 1 meter. Similar controls could be applied to change in body orientation or gaze directions. It is worth noting that proxemics is not only about physical space but essentially it is about socio-physical space, including context factors such as (body) heat and smell that are used to organize space. Thus, there is potential for novel designs mapping speech emotions to experiences of smell, taste, or heat.  [19, 21, 35]",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sec For Soemasthetic Interaction Design And Artistic Research",
      "text": "There are clear potentials to use speech emotion conversion for soma-based design or somaesthetic interaction design  [16]  which embraces an experiential stance towards interaction design. One of the main purposes of converting speech emotions is to extract and digitalize the \"emotional essence\" into a malleable material that can easily be mapped through design to various bodily experiences. Somaesthetic interaction design is a rather new field with a limited number of design supporting tools, such as the \"soma bits\"  [36] . The general idea is to externalize biological signals such as we did in our previous work with heartbeats, mapping a person's heartbeats to a tangible design which allows the person to reflect on their heartbeats or share them with others for socially augmented experiences  [4, 7] . Speech emotions can be considered biodata and fit also into the concept of getting from biodata to somadata  [2]  to enable both first-person and collaborative encounters in design explorations. In general ML technologies for perceiving and generating bodily experiences and behaviours has great potential to serve as a foundation for tools supporting somaesthetic design practices. The Affective Bar Piano  [24]  is a good example of \"artistic research\" where speech emotion recognition is used by the piano agent to visual adapt its visual appearance (e.g., smile) and the emotional quality of the music it plays to fit the story and tone of the singer. In addition to alignment, estrangement can also be strategy in artistic expressions and research, which we can also apply in embodied design  [34] . Essentially, with our starter kit we provide a way to feel speech emotions as physical sensations and calibrate the \"strangeness or familiarity\" of experiences by modifying the speech emotion conversion tool's options, and exploring mappings of speech input to outputs of physical displays.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "The future of SER methods is yet undefined. But, one could argue that today we are witnessing a stagnation with models getting ever larger to a degree where less (academic) research groups can contribute to progress in SER due to the lack of available infrastructure. Furthermore, foundation models are disrupting the landscape of affective computing and increasingly positioning themselves as task independent alternatives for many affective computing tasks  [26] . This is both a good and bad development, bad because there is still room for improvement in affective computing tasks but also good, because a slowed down pace of change provides time for interdisciplinary researchers including HCI researcher to explore the current SER solutions and reflect on the new materials that these solutions can produce for interactive system designs.\n\nTo facilitate the explorations of speech emotions as malleable design material requires the reduction of technical barriers and flexible use of digital emotions in new design ideas. To this end, this paper described a method and starter kit to enable speech emotion conversion into \"physical\" emotions that can be expressed through tangilbe, haptic, or proexmic sensations and designs. It was argued that alternative representations of emotions, which can be directly felt and communicated as bodily sensations create new design opportunities for various domains, including animal computer interaction and artistic research. I have described some initial opportunities and implications for design but the domains addressed were far from being comprehensive. For example, there is a clear opportunity to augment media experiences such as gaming, VR, AR, watching TV or listening to the radio, where speech emotion conversions can make content accessible and experiences potentially more enjoyable.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the starter kit. The main part of the kit is provided as a command line tool with a command line interface",
      "page": 2
    },
    {
      "caption": "Figure 1: presents an overview of the system architecture for",
      "page": 2
    },
    {
      "caption": "Figure 2: ) provides users ease of use for selecting",
      "page": 3
    },
    {
      "caption": "Figure 2: Screenshot of the AffectToolbox [18] GUI, with",
      "page": 3
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Reach out and touch me: Effects of four distinct haptic technologies on affective touch in virtual reality",
      "authors": [
        "Imtiaj Ahmed",
        "Ville Harjunen",
        "Giulio Jacucci",
        "Eve Hoggan",
        "Niklas Ravaja",
        "M Michiel",
        "Spapé"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "2",
      "title": "From biodata to somadata",
      "authors": [
        "Miquel Alfaras",
        "Vasiliki Tsaknaki",
        "Pedro Sanches",
        "Charles Windlin",
        "Muhammad Umair",
        "Corina Sas",
        "Kristina Höök"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "3",
      "title": "On the impact of word error rate on acoustic-linguistic speech emotion recognition: An update for the deep learning era",
      "authors": [
        "Artem Shahin Amiriparian",
        "Ilhan Sokolov",
        "Lukas Aslan",
        "Maurice Christ",
        "Tobias Gerczuk",
        "Dmitry Hübner",
        "Manuel Lamanov",
        "Sandra Milling",
        "Ilya Ottl",
        "Poduremennykh"
      ],
      "year": "2021",
      "venue": "On the impact of word error rate on acoustic-linguistic speech emotion recognition: An update for the deep learning era",
      "arxiv": "arXiv:2104.10121"
    },
    {
      "citation_id": "4",
      "title": "Hold my heart and breathe with me: Tangible somaesthetic designs",
      "authors": [
        "Ilhan Aslan",
        "Hadrian Burkhardt",
        "Julian Kraus",
        "Elisabeth André"
      ],
      "year": "2016",
      "venue": "Proceedings of the 9th Nordic Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "5",
      "title": "How to Compliment a Human-Designing Affective and Well-being Promoting Conversational Things",
      "authors": [
        "Ilhan Aslan",
        "Dominik Neu",
        "Daniela Neupert",
        "Stefan Grafberger",
        "Nico Weise",
        "Pascal Pfeil",
        "Maximilian Kuschewski"
      ],
      "year": "2023",
      "venue": "Interaction Design and Architecture (s) Journal"
    },
    {
      "citation_id": "6",
      "title": "Pen+ mid-air gestures: Eliciting contextual gestures",
      "authors": [
        "Ilhan Aslan",
        "Tabea Schmidt",
        "Jens Woehrle",
        "Lukas Vogel",
        "Elisabeth André"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "7",
      "title": "PiHearts: Resonating Experiences of Self and Others Enabled by a Tangible Somaesthetic Design",
      "authors": [
        "Ilhan Aslan",
        "Andreas Seiderer",
        "Chi Dang",
        "Simon Rädler",
        "Elisabeth André"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "8",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "9",
      "title": "Proxemic interaction: designing for a proximity and orientation-aware environment",
      "authors": [
        "Till Ballendat",
        "Nicolai Marquardt",
        "Saul Greenberg"
      ],
      "year": "2010",
      "venue": "ACM International Conference on Interactive Tabletops and Surfaces"
    },
    {
      "citation_id": "10",
      "title": "Future directions in human-animal bond research",
      "authors": [
        "M Alan",
        "Aaron Beck",
        "Katcher"
      ],
      "year": "2003",
      "venue": "American behavioral scientist"
    },
    {
      "citation_id": "11",
      "title": "Of smarthomes, IoT plants, and implicit interaction design",
      "authors": [
        "Björn Bittner",
        "Ilhan Aslan",
        "Chi Dang",
        "Elisabeth André"
      ],
      "year": "2019",
      "venue": "Of smarthomes, IoT plants, and implicit interaction design"
    },
    {
      "citation_id": "12",
      "title": "Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning",
      "authors": [
        "Lukas Christ",
        "Shahin Amiriparian",
        "Manuel Milling",
        "Ilhan Aslan",
        "Björn Schuller"
      ],
      "year": "2024",
      "venue": "Modeling Emotional Trajectories in Written Stories Utilizing Transformers and Weakly-Supervised Learning",
      "arxiv": "arXiv:2406.02251"
    },
    {
      "citation_id": "13",
      "title": "Pet video chat: monitoring and interacting with dogs over distance",
      "authors": [
        "Jennifer Golbeck",
        "Carman Neustaedter"
      ],
      "year": "2012",
      "venue": "CHI'12 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "14",
      "title": "Proxemic interactions: the new ubicomp? interactions",
      "authors": [
        "Saul Greenberg",
        "Nicolai Marquardt",
        "Till Ballendat",
        "Rob Diaz-Marino",
        "Miaosen Wang"
      ],
      "year": "2011",
      "venue": "Proxemic interactions: the new ubicomp? interactions"
    },
    {
      "citation_id": "15",
      "title": "Psychological impact of the animal-human bond in disaster preparedness and response",
      "authors": [
        "Molly J Hall",
        "Anthony Ng",
        "Robert Ursano",
        "Harry Holloway",
        "Carol Fullerton",
        "Jacob Casper"
      ],
      "year": "2004",
      "venue": "Journal of Psychiatric Practice®"
    },
    {
      "citation_id": "16",
      "title": "Designing with the body: Somaesthetic interaction design",
      "year": "2018",
      "venue": "Designing with the body: Somaesthetic interaction design"
    },
    {
      "citation_id": "17",
      "title": "Haptic empathy: Conveying emotional meaning through vibrotactile feedback",
      "authors": [
        "Yulan Ju",
        "Dingding Zheng",
        "Danny Hynds",
        "George Chernyshov",
        "Kai Kunze",
        "Kouta Minamizawa"
      ],
      "year": "2021",
      "venue": "Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "18",
      "title": "The AffectToolbox: Affect Analysis for Everyone",
      "authors": [
        "Silvan Mertes",
        "Dominik Schiller",
        "Michael Dietz",
        "Elisabeth André",
        "Florian Lingenfelser"
      ],
      "year": "2024",
      "venue": "2024 12th International conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "LOLL io: exploring taste as playful modality",
      "authors": [
        "Martin Murer",
        "Ilhan Aslan",
        "Manfred Tscheligi"
      ],
      "year": "2013",
      "venue": "Proceedings of the 7th international conference on tangible"
    },
    {
      "citation_id": "20",
      "title": "Emotions mediated through mid-air haptics",
      "authors": [
        "Marianna Obrist",
        "Sriram Subramanian",
        "Elia Gatti",
        "Benjamin Long",
        "Thomas Carter"
      ],
      "year": "2015",
      "venue": "Proceedings of the 33rd annual ACM conference on human factors in computing systems"
    },
    {
      "citation_id": "21",
      "title": "Opportunities for odor: experiences with smell and implications for technology",
      "authors": [
        "Marianna Obrist",
        "Alexandre Tuch",
        "Kasper Hornbaek"
      ],
      "year": "2014",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "22",
      "title": "From digital media to empathic spaces: A systematic review of empathy research in extended reality environments",
      "authors": [
        "Ville Paananen",
        "Sina Mohammad",
        "Lee Kiarostami",
        "Tristan Lik-Hang",
        "Simo Braud",
        "Hosio"
      ],
      "year": "2023",
      "venue": "Comput. Surveys"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition in cats",
      "authors": [
        "Angelo Quaranta",
        "Rosaria Serenella D'ingeo",
        "Marcello Amoruso",
        "Siniscalchi"
      ],
      "year": "2020",
      "venue": "Animals"
    },
    {
      "citation_id": "24",
      "title": "The Affective Bar Piano",
      "authors": [
        "Hannes Ritschel",
        "Silvan Mertes",
        "Florian Lingenfelser",
        "Thomas Kiderle",
        "Elisabeth André"
      ],
      "year": "2023",
      "venue": "Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "25",
      "title": "TAS for cats: An artist-led exploration of trustworthy autonomous systems for companion animals",
      "authors": [
        "Eike Schneiders",
        "Alan Chamberlain",
        "Joel Fischer",
        "Steve Benford",
        "Simon Castle-Green",
        "Victor Ngo",
        "Ayse Kucukyilmaz",
        "Pepita Barnard",
        "Row Ju",
        "Matt Farr",
        "Adams"
      ],
      "year": "2023",
      "venue": "Proceedings of the First International Symposium on Trustworthy Autonomous Systems"
    },
    {
      "citation_id": "26",
      "title": "Affective Computing Has Changed: The Foundation Model Disruption",
      "authors": [
        "Björn Schuller",
        "Adria Mallol-Ragolta",
        "Alejandro Peña Almansa",
        "Iosif Tsangko",
        "Anastasia Mostafa M Amin",
        "Lukas Semertzidou",
        "Shahin Christ",
        "Amiriparian"
      ],
      "year": "2024",
      "venue": "Affective Computing Has Changed: The Foundation Model Disruption",
      "arxiv": "arXiv:2409.08907"
    },
    {
      "citation_id": "27",
      "title": "Prediction of animal vocal emotions using convolutional neural network",
      "authors": [
        "Varun Totakura",
        "Krishna Mohana",
        "Durganath Janmanchi",
        "Rajesh",
        "Hussan"
      ],
      "year": "2020",
      "venue": "International Journal of Scientific & Technology Research"
    },
    {
      "citation_id": "28",
      "title": "INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition",
      "authors": [
        "Andreas Triantafyllopoulos",
        "Anton Batliner",
        "Simon Rampp",
        "Manuel Milling",
        "Björn Schuller"
      ],
      "year": "2024",
      "venue": "INTERSPEECH 2009 Emotion Challenge Revisited: Benchmarking 15 Years of Progress in Speech Emotion Recognition",
      "arxiv": "arXiv:2406.06401"
    },
    {
      "citation_id": "29",
      "title": "Designing emotionally expressive robots: A comparative study on the perception of communication modalities",
      "authors": [
        "Christiana Tsiourti",
        "Astrid Weiss",
        "Katarzyna Wac",
        "Markus Vincze"
      ],
      "year": "2017",
      "venue": "Proceedings of the 5th international conference on human agent interaction"
    },
    {
      "citation_id": "30",
      "title": "Not just seeing, but also feeling art: Mid-air haptic experiences integrated in a multisensory art exhibition",
      "authors": [
        "Thanh Chi",
        "Damien Vi",
        "Elia Ablart",
        "Carlos Gatti",
        "Marianna Velasco",
        "Obrist"
      ],
      "year": "2017",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "31",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Human-animal bonds I: The relational significance of companion animals",
      "authors": [
        "Froma Walsh"
      ],
      "year": "2009",
      "venue": "Family process"
    },
    {
      "citation_id": "33",
      "title": "How to shape the humor of a robot-social behavior adaptation based on reinforcement learning",
      "authors": [
        "Klaus Weber",
        "Hannes Ritschel",
        "Ilhan Aslan",
        "Florian Lingenfelser",
        "Elisabeth André"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "34",
      "title": "Embodied design ideation methods: Analysing the power of estrangement",
      "authors": [
        "Danielle Wilde",
        "Anna Vallgårda",
        "Oscar Tomico"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "35",
      "title": "Some like it hot: thermal feedback for mobile devices",
      "authors": [
        "Graham Wilson",
        "Martin Halvey",
        "Stephen Brewster",
        "Stephen Hughes"
      ],
      "year": "2011",
      "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "36",
      "title": "Soma Bits-mediating technology to orchestrate bodily experiences",
      "authors": [
        "Charles Windlin",
        "Anna Ståhl",
        "Pedro Sanches",
        "Vasiliki Tsaknaki",
        "Pavel Karpashevich",
        "Madeline Balaam",
        "Kristina Höök"
      ],
      "year": "2019",
      "venue": "RTD 2019-Research through Design Conference 2019, the Science Centre"
    },
    {
      "citation_id": "37",
      "title": "Effects of visual locomotion and tactile stimuli duration on the emotional dimensions of the cutaneous rabbit illusion",
      "authors": [
        "Mounia Ziat",
        "Katherine Chin",
        "Roope Raisamo"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    }
  ]
}