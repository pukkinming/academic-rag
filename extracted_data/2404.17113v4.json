{
  "paper_id": "2404.17113v4",
  "title": "Mer 2024: Semi-Supervised Learning, Noise Robustness, And Open-Vocabulary Multimodal Emotion Recognition",
  "published": "2024-04-26T02:05:20Z",
  "authors": [
    "Zheng Lian",
    "Haiyang Sun",
    "Licai Sun",
    "Zhuofan Wen",
    "Siyuan Zhang",
    "Shun Chen",
    "Hao Gu",
    "Jinming Zhao",
    "Ziyang Ma",
    "Xie Chen",
    "Jiangyan Yi",
    "Rui Liu",
    "Kele Xu",
    "Bin Liu",
    "Erik Cambria",
    "Guoying Zhao",
    "Bj√∂rn W. Schuller",
    "Jianhua Tao"
  ],
  "keywords": [
    "‚Ä¢ Computing methodologies ‚Üí Neural networks",
    "Artifcial intelligence",
    "Computer vision",
    "Natural language processing MER 2024, multimodal emotion recognition, semi-supervised learning, noise robustness, open-vocabulary emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition plays an important role in humancomputer interaction. Recently, researchers have made significant progress in this field. However, this task is still not well solved and its performance still cannot meet the requirements of practical applications  [1] . To this end, last year, we launched MER2023  [2] , focusing on three important topics: multi-label learning, noise robustness, and semi-supervised learning. This year, we continue the latter two tracks and introduce a new track around open-vocabulary emotion recognition, aiming to describe emotional states accurately.\n\nFirst, it is hard to collect samples with emotion labels. On the one hand, the collected samples are often emotionless (i.e. neutral)  [3] . On the other hand, researchers usually hire multiple annotators and use majority voting to improve label consistency  [4] , greatly increasing the annotation cost. To address the sparsity of emotional data, previous works used unlabeled data and focused on unsupervised or semi-supervised learning. Recently, MERBench [1] has conducted a systematic analysis, pointing out the necessity of using unlabeled data from the same domain as labeled data. Therefore, we provide a large number of human-centric unlabeled videos in MER-SEMI and encourage participants to explore more effective unsupervised or semi-supervised strategies.\n\nSecondly, in real scenarios, we cannot ensure that every video is free of audio noise and every frame is in high resolution. To copy with complex environments, emotion recognition systems should have a certain degree of noise robustness. Therefore, we organize MER-NOISE to fairly evaluate the noise robustness of different systems. Although there are many types of noise, we only consider the two most common ones: audio additive noise and image blur noise. In this track, we encourage participants to use data augmentation  [5]  or other effective techniques  [6, 7]  to improve performance under noisy conditions.\n\nThirdly, to improve label consistency, existing datasets usually restrict the label space to a few discrete categories, then employ multiple annotators and use majority voting to select the most likely label. However, this process may cause some correct but non-candidate or non-majority labels to be ignored. Therefore, we introduce MER-OV, centered around open-vocabulary emotion recognition. We encourage participants to generate any number of labels in any category, trying to describe emotional states accurately.\n\nIn summary, MER2024 consists of three tracks: MER-SEMI, MER-NOISE, and MER-OV. In MER-SEMI, we encourage participants to use unlabeled data during training; in MER-NOISE, we focus on noise robustness; in MER-OV, we require participants to describe emotional states as accurately as possible. The MER series of challenges aims to provide participants with a common platform to fairly compare the performance of different techniques. In the rest of this paper, we will introduce the datasets, baselines, evaluation metrics, and experimental results in detail.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Challenge Dataset",
      "text": "MER2024 is an extended version of MER2023  [2]  and its construction process is summarized in Figure  1 . Specifically, MER2023 contains four subsets: Train&Val, MER-MULTI, MER-NOISE, and MER-SEMI. In the last subset, in addition to the labeled data, it also contains a large amount of unlabeled data. In MER2024, we merge all labeled samples and obtain the updated Train&Val. Meanwhile, we collect more unlabeled data and select a subset for annotation, getting MER-SEMI and MER-NOISE. For MER-OV, we select 332 samples from Train&Val and provide open-vocabulary labels. Table  1  summarizes the statistics of these datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "As shown in Figure  1 , MER2024 expands the dataset size. Its data collection process is borrowed from MER2023 and includes two key steps: video cutting and video filtering.\n\nVideo Cutting. The raw data in MER2024 comes from movies and TV series, which are close to real scenarios. However, these videos are usually long and have many characters, so they need to be segmented into video clips. In this process, we require that the content in these video clips is relatively complete. For videos with subtitles, the timestamps in the subtitles provide accurate boundary information, and we use them for video segmentation. For videos without subtitles, we use the voice activity detection toolkit, Silero VAD  4  , to segment the video and the speaker identification toolkit, Deep Speaker 5  , to merge the consecutive clips if they are likely to be from the same speaker.\n\nVideo Filtering. Through the above process, we can generate video clips containing relatively complete content from the same speaker. However, it only guarantees that the audio is from the same person, but cannot guarantee that there is only one person in the visual frames. Therefore, we further filter these video clips. Specifically, we use the face detection toolkit, YuNet  6  , to ensure that most frames contain only one face. Then, we use the face recognition toolkit, face.evoLVe  7  , to ensure that these faces belong to the same person. Meanwhile, the length of video clips is also important. Video clips that are too short may not convey the complete meaning; video clips that are too long may contain emotional changes,",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mer-Semi And Mer-Noise",
      "text": "Annotating all unlabeled data requires a lot of labor costs. To reduce the cost, we only select samples with a high probability of explicit emotions. Specifically, we use the top-16 results in last year's challenge and calculate the proportion of primary labels as the basis for selection. For example, if a sample is predicted as happy 10 times and neutral 6 times, its score is ùë£ = max(6, 10)/16. Then, taking into account the calculated score and class balance, we select a total of 6,000 samples for labeling. During the annotation process, we hire 5 annotators and only select samples in which at least 4 annotators assign the same label, resulting in 2,339 labeled samples. All these samples are equally divided into two parts, one for MER-SEMI and the other for MER-NOISE.\n\nFor MER-NOISE, we additionally add noise to videos. This paper considers two types of noise: audio additive noise and image blur noise. For the audio, we select noise from the MUSAN dataset  [32] , which contains three subsets: music, noise, and speech. The noise in the first two subsets may convey emotions and affect the emotion of the raw audio. For example, rain and thunder may lead to a negative sentiment, while a pleasant breeze may generate a positive sentiment. Therefore, we only use the noise in the last subset. Specifically, we randomly select a speech-to-noise ratio (SNR) between 5dB‚àº10dB and randomly select noise from the speech subset. For the video, image blur is a common noise. To generate blurry images, we downsample the image to lose some details and then upsample the low-resolution image to keep the size unchanged. This paper randomly selects a downsampling factor from ùëü = {1, 2, 4}.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Mer-Ov",
      "text": "Unlike MER-SEMI and MER-NOISE which predict the most likely emotion within a fixed label space, MER-OV needs to predict any number of emotions in any category. Previously, researchers made an initial attempt at this task  [8] . They first annotated emotionrelated acoustic and visual clues. Then, they relied on the reasoning ability of LLMs to disambiguate subtitles using these clues. This process can generate descriptions with rich emotions. After that, they used GPT-3.5 (\"gpt-3.5-turbo-16k-0613\") to extract all labels using the following prompt: Please assume the role of an expert in the field of emotions. We provide clues that may be related to the emotions of the characters. Based on the provided clues, please identify the emotional states of the main characters. Please separate different emotional categories with commas and output only the clearly identifiable emotional categories in a list format. If none are identified, please output an empty list.\n\nFinally, they performed the manual check and got the ground truth ùëå ùëîùë° . Through the above process, each sample can have an average of 3 labels. However, due to the high cost, they only annotated 332 samples  [8] . For MER-OV, participants can borrow some basic ideas from this process. Meanwhile, we encourage participants to use MLLMs or to further conduct instruction fine-tuning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Challenge Protocol",
      "text": "To download the dataset, participants should fill out an End User License Agreement (EULA)  8  . It asks participants to use this dataset only for academic research and not to edit or upload it to the Internet. For MER-SEMI and MER-NOISE, each team should predict the most likely label among 6 categories (i.e., worried, happy, neutral, angry, surprised, and sad). For MER-OV, each team can submit any number of labels in any category. Meanwhile, participants cannot use closed-source models (such as GPT or Claude) in MER-OV. For all tracks, participants should predict results for 115,595 unlabeled data, although we only evaluate a small subset of them. It requires participants to focus on the generalization ability and develop systems that do not require a lot of inference time. Additionally, participants cannot manually annotate samples in MER2024. We will ask them to submit the code for further checking. Finally, each team should submit a paper describing their method. For other requirements, please refer to our official website  9  .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baselines And Evaluation Metrics",
      "text": "In this section, we first introduce the baselines of three tracks. Then, we illustrate implementation details and evaluation metrics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mer-Semi And Mer-Noise",
      "text": "For MER-SEMI and MER-NOISE, we build baselines based on MER-Tools  10  . Feature selection and fusion are crucial for emotion recognition systems. For feature selection, we adopt the recommendations of MERBench [1]. Language compatibility is important for lexical and acoustic features, so we mainly select encoders trained on Chinese corpora. Domain compatibility is important for visual features. Therefore, besides encoders trained on action or caption datasets, we also select encoders trained on human-centric videos. Table  2  lists the model cards of some representative encoders.\n\nRegarding the fusion strategy, MERBench points out that the attention mechanism can achieve relatively good performance  [1] . The reason lies in that the labeled samples are limited in emotion recognition. Complex architectures may cause overfitting problems, which will affect the model's generalization ability on unseen data. Assume that acoustic, lexical, and visual features are ùëì ùëé ‚àà R ùëë ùëé , ùëì ùëô ‚àà R ùëë ùëô , ùëì ùë£ ‚àà R ùëë ùë£ , respectively. During the fusion process, we first map them to a fixed dimension ùëë:   3 : Prompt for generating emotion-related descriptions using MLLMs (drawn from previous works  [8] ).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Models Prompt Audio Llm",
      "text": "As an expert in the field of emotions, please focus on the acoustic information in the audio to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Video Llm",
      "text": "As an expert in the field of emotions, please focus on the facial expressions, body movements, environment, etc., in the video to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual in the video.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio-Video Llm",
      "text": "As an expert in the field of emotions, please focus on the facial expressions, body movements, environment, acoustic information, etc., in the video to discern clues related to the emotions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual in the video.\n\nwhere ùëä ùëö ‚àà R ùëë ùëö √óùëë and ùëè ùëö ‚àà R ùëë are trainable parameters. Then, we calculate the attention score for each modality:\n\nwhere ùëä ùõº ‚àà R ùëë √ó1 and ùëè ùõº ‚àà R 1 are trainable parameters. Here, ‚Ñé ‚àà R ùëë √ó3 and ùõº ‚àà R 3√ó1 . Finally, the fused multimodal features ùëß = ‚Ñéùõº ‚àà R ùëë are used for emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Mer-Ov",
      "text": "OV emotion recognition is a new task and the lack of large-scale datasets makes it difficult to conduct supervised training. Therefore, we choose pre-trained MLLMs as baselines because they can handle various multimodal tasks without further training. To solve OV emotion recognition using MLLMs, we borrow the dataset construction pipeline in Section 2.3. Specifically, we first use MLLMs and the prompts in Table  3  to extract multifaceted and multimodal emotion-related clues. Then, we use these clues to disambiguate the subtitle and generate descriptions with rich emotions. Finally, we extract all emotion labels using the prompt in Section 2.3.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "For MER-SEMI and MER-NOISE, we use the attention mechanism for multimodal fusion. This process involves one hyper-parameter, the dimension of the hidden representation ùëë, and we choose it from {64, 128, 256}. During training, we use the Adam optimizer and choose the learning rate from {10 -3 , 10 -4 }. To alleviate the overfitting problem, we also use Dropout and select the rate from {0.2, 0.3, 0.4, 0.5}. Therefore, a total of 3 hyper-parameters need to be adjusted. To find the optimal hyper-parameters, we randomly select 50 parameter combinations and choose the best-performing combination in five-fold cross-validation. Finally, we report its average result and standard deviation.\n\nFor MER-OV, we directly use pretrained MLLMs. Due to limited GPU memory, we use their 7B weights by default. All models are Table  5 : Multimodal results (%). \"A\", \"V\", and \"T\" represent acoustic, visual, and textual modalities, respectively. \"TopN\" means that we select the top-N features for each modality and their ranking is based on the average WAF in Table  4 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "# Top",
      "text": "Train&Val implemented with PyTorch and all inference processes are carried out with a 32G NVIDIA Tesla V100 GPU.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "For MER-SEMI and MER-NOISE, we choose two widely used metrics in emotion recognition for performance evaluation: accuracy and weighted average f1-score (WAF)  [33] . Considering the inherent class imbalance, we choose WAF for the final ranking.\n\nFor MER-OV, we use set-level accuracy and recall for performance evaluation, consistent with previous works  [8] . Specifically, assume that the true label set is ùëå ùëîùë° = {ùë¶ ùëñ } ùëÄ ùëñ=1 and the predicted label set is ≈∂ùëù = { ≈∑ùëñ } ùëÅ ùëñ=1 , where ùëÄ and ùëÅ are the number of labels. Since we do not fix the label space, there may be synonyms, i.e., labels with different expressions but the same meaning. Therefore, we first group all labels using GPT-3.5: Please assume the role of an expert in the field of emotions. We provide a set of emotions. Please group the emotions, with each group containing emotions with the same meaning. Directly output the results. The output format should be a list containing multiple lists. Next, we use the GPT-based grouping results ùê∫ (‚Ä¢) to map all labels to their group IDs:\n\nWe then calculate set-level accuracy and recall and use their average for the final ranking, consistent with previous works  [8] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results And Discussion",
      "text": "This section reports baseline results for three tracks. For MER-SEMI and MER-NOISE, we report unimodal and multimodal results. For MER-OV, we report the results of various MLLMs.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mer-Semi And Mer-Noise",
      "text": "Table  4  shows the unimodal results. From this table, we observe that models that perform well on Train&Val generally perform well on MER-SEMI and MER-NOISE. These results suggest that the models trained on our dataset have a good generalization ability.\n\nFor the visual modality, unsupervised or semi-supervised models (e.g., CLIP-large) generally outperform supervised models (e.g., SENet-FER2013). This phenomenon suggests that unsupervised or semi-supervised strategies can capture universal representations, which are also helpful for emotion recognition. Meanwhile, previous works have emphasized the importance of domain compatibility Table  6 : Baseline results on MER-OV (taken from previous works  [8] ). The values in the gray column are used for the final ranking. Here, \"L\", \"V\", and \"A\" indicate whether lexical, visual, and acoustic information are used during inference. For the acoustic and lexical modalities, we primarily choose encoders trained on the Chinese corpus, as suggested by MERBench [1]. For the acoustic modality, we observe that unsupervised or semi-supervised models (e.g., HUBERT-large) generally perform better than traditional encoders (e.g., eGeMAPS), which is consistent with the findings in the visual modality. For the lexical modality, we observe that LLMs generally outperform pretrained language models (PLMs). This suggests that by increasing the training data and model size, we can build more powerful lexical encoders.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model",
      "text": "Table  5  shows the multimodal results. In this table, we choose the top-N features for each modality and use the attention mechanism for multimodal fusion. Their ranking is based on the average WAF in Table  4 . We observe that different modality combinations prefer distinct ùëÅ . Therefore, we should adjust it for each combination. Meanwhile, the trimodal results generally perform best, reflecting the effectiveness of multimodal fusion and the necessity of each modality. Overall, in terms of WAF scores, our baseline system can reach 86.73% in MER-SEMI and 79.62% in MER-NOISE.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Mer-Ov",
      "text": "This section discusses the performance of different methods on MER-OV. In addition to MLLMs, we introduce two heuristic baselines: Empty and Random. For the former, we do not assign any label. For the latter, we randomly select a label from six candidate categories (i.e., worried, happy, neutral, angry, surprised, and sad). Experimental results are shown in Table  6 . We observe that MLLMs generally outperform heuristic baselines, indicating that they can solve this task to some extent. However, there is still a significant gap between MLLMs and ground truth, indicating the difficulty of this task. We recommend participants test other MLLMs or use supervised fine-tuning, which may bring performance improvement.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "This paper introduces MER2024, an extension of the MER2023 competition. Besides including more samples, we also introduce a new track called open-vocabulary emotion recognition, requiring participants to predict any number of labels in any category, aiming to achieve more accurate emotion recognition. In this paper, we introduce the datasets, baselines, evaluation metrics, and experimental results. We also open-source the code to ensure reproducibility. We hope that the MER series of challenges can provide researchers with a common platform to fairly evaluate their emotion recognition systems and further promote the development of this field.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Specifically, MER2023 con-",
      "page": 2
    },
    {
      "caption": "Figure 1: Dataset construction pipeline of MER2024.",
      "page": 2
    },
    {
      "caption": "Figure 1: , MER2024 expands the dataset size. Its data",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "Audio LLM",
          "Prompt": "As an expert in the field of emotions, please focus on the acoustic information in the audio to discern\nclues related to the emotions of the individual. Please provide a detailed description and ultimately\npredict the emotional state of the individual."
        },
        {
          "Models": "Video LLM",
          "Prompt": "As an expert in the field of emotions, please focus on the facial expressions, body movements, envi-\nronment, etc., in the video to discern clues related to the emotions of the individual. Please provide a\ndetailed description and ultimately predict the emotional state of the individual in the video."
        },
        {
          "Models": "Audio-Video LLM",
          "Prompt": "As an expert in the field of emotions, please focus on the facial expressions, body movements, environ-\nment, acoustic information, etc., in the video to discern clues related to the emotions of the individual.\nPlease provide a detailed description and ultimately predict the emotional state of the individual in the\nvideo."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Empty\nRandom",
          "L V A": "√ó √ó √ó\n‚àö\n‚àö",
          "Avg\nRecalls\nAccuracys": "0.00¬±0.00\n0.00¬±0.00\n0.00¬±0.00\n√ó √ó √ó 19.13¬±0.06 24.85¬±0.15 13.42¬±0.04"
        },
        {
          "Model": "Qwen-Audio [34]\nOneLLM [35]\nOtter [36]\nVideoChat [37]\nVideo-LLaMA [38]\nPandaGPT [39]\nSALMONN [40]\nVideo-LLaVA [41]\nVideoChat2 [42]\nOneLLM [35]\nLLaMA-VID [43]\nmPLUG-Owl [44]\nVideo-ChatGPT [45]\nChat-UniVi [46]\nGPT-4V [47]",
          "L V A": "√ó\n‚àö\n‚àö\n√ó\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö ‚àö\n‚àö\n‚àö\n√ó\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö\n‚àö ‚àö",
          "Avg\nRecalls\nAccuracys": "40.23¬±0.09 49.42¬±0.18 31.04¬±0.00\n43.04¬±0.06 45.92¬±0.05 40.15¬±0.06\n√ó 44.40¬±0.09 50.71¬±0.10 38.09¬±0.09\n√ó 45.70¬±0.09 42.90¬±0.27 48.49¬±0.10\n√ó 44.74¬±0.14 44.14¬±0.13 45.34¬±0.15\n46.21¬±0.17 50.03¬±0.01 42.38¬±0.33\n48.06¬±0.04 50.20¬±0.04 45.92¬±0.04\n√ó 47.12¬±0.15 48.58¬±0.02 45.66¬±0.29\n√ó 49.60¬±0.28 54.72¬±0.41 44.47¬±0.15\n√ó 50.99¬±0.08 55.93¬±0.09 46.06¬±0.06\n√ó 51.29¬±0.09 52.71¬±0.18 49.87¬±0.00\n√ó 52.79¬±0.13 54.54¬±0.13 51.04¬±0.13\n√ó 50.73¬±0.06 54.03¬±0.04 47.44¬±0.07\n√ó 53.09¬±0.01 53.68¬±0.00 52.50¬±0.02\n√ó 56.69¬±0.04 48.52¬±0.07 64.86¬±0.00"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "Senet-Fer"
      ],
      "year": "2013",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Videomae-Base"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "4",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "7",
      "title": "Analyzing modality robustness in multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Yingting Li",
        "Bo Cheng",
        "Shuai Zhao",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2022",
      "venue": "Proceedings of the North American Chapter"
    },
    {
      "citation_id": "8",
      "title": "Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Gcnet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "11",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Emonets: Multimodal deep learning approaches for emotion recognition in video",
      "authors": [
        "Samira Kahou",
        "Xavier Bouthillier",
        "Pascal Lamblin",
        "Caglar Gulcehre",
        "Vincent Michalski",
        "Kishore Konda",
        "S√©bastien Jean",
        "Pierre Froumenty",
        "Yann Dauphin",
        "Nicolas Boulanger-Lewandowski"
      ],
      "year": "2016",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "13",
      "title": "Learning robust visual features without supervision",
      "authors": [
        "Maxime Oquab",
        "Timoth√©e Darcet",
        "Th√©o Moutakanni",
        "Huy Vo",
        "Marc Szafraniec",
        "Vasil Khalidov",
        "Pierre Fernandez",
        "Daniel Haziza",
        "Francisco Massa",
        "Alaaeldin El-Nouby"
      ],
      "year": "2023",
      "venue": "Learning robust visual features without supervision",
      "arxiv": "arXiv:2304.07193"
    },
    {
      "citation_id": "14",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Learning deep global multiscale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Shanmin Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "17",
      "title": "Eva-02: A visual representation for neon genesis",
      "authors": [
        "Yuxin Fang",
        "Quan Sun",
        "Xinggang Wang",
        "Tiejun Huang",
        "Xinlong Wang",
        "Yue Cao"
      ],
      "year": "2023",
      "venue": "Eva-02: A visual representation for neon genesis",
      "arxiv": "arXiv:2303.11331"
    },
    {
      "citation_id": "18",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "19",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Bj√∂rn Schuller",
        "Johan Sundberg",
        "Elisabeth Andr√©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the International Conference on Machine Learning"
    },
    {
      "citation_id": "22",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "23",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Electra: Pre-training text encoders as discriminators rather than generators",
      "authors": [
        "Kevin Clark",
        "Minh-Thang Luong",
        "Quoc V Le",
        "Christopher Manning"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Learning Representations"
    },
    {
      "citation_id": "27",
      "title": "Pert: pre-training bert with permuted language model",
      "authors": [
        "Yiming Cui",
        "Ziqing Yang",
        "Ting Liu"
      ],
      "year": "2022",
      "venue": "Pert: pre-training bert with permuted language model",
      "arxiv": "arXiv:2203.06906"
    },
    {
      "citation_id": "28",
      "title": "Lert: A linguisticallymotivated pre-trained language model",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Shijin Wang",
        "Ting Liu"
      ],
      "year": "2022",
      "venue": "Lert: A linguisticallymotivated pre-trained language model",
      "arxiv": "arXiv:2211.05344"
    },
    {
      "citation_id": "29",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "30",
      "title": "Revisiting pre-trained models for chinese natural language processing",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Ting Liu",
        "Bing Qin",
        "Shijin Wang",
        "Guoping Hu"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "31",
      "title": "Glm: General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "32",
      "title": "A 176b-parameter open-access multilingual language model",
      "authors": [
        "Bigscience Workshop",
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Iliƒá",
        "Roman Hesslow",
        "Alexandra Castagn√©",
        "Fran√ßois Sasha Luccioni",
        "Yvon"
      ],
      "year": "2022",
      "venue": "A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "33",
      "title": "Open large-scale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open large-scale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "34",
      "title": "A music, speech, and noise corpus",
      "authors": [
        "David Snyder",
        "Guoguo Chen",
        "Daniel Povey",
        "Musan"
      ],
      "year": "2015",
      "venue": "A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "35",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "36",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "37",
      "title": "One framework to align all modalities with language",
      "authors": [
        "Jiaming Han",
        "Kaixiong Gong",
        "Yiyuan Zhang",
        "Jiaqi Wang",
        "Kaipeng Zhang",
        "Dahua Lin",
        "Yu Qiao",
        "Peng Gao",
        "Xiangyu Yue",
        "Onellm"
      ],
      "year": "2023",
      "venue": "One framework to align all modalities with language",
      "arxiv": "arXiv:2312.03700"
    },
    {
      "citation_id": "38",
      "title": "A multi-modal model with in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Jingkang Yang",
        "Ziwei Liu",
        "Otter"
      ],
      "year": "2023",
      "venue": "A multi-modal model with in-context instruction tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "39",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "40",
      "title": "Video-llama: An instruction-tuned audiovisual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audiovisual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "41",
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st Workshop on Taming Large Language Models: Controllability in the era of Interactive Assistants"
    },
    {
      "citation_id": "42",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Chao Ma Zejun",
        "Zhang"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "43",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "44",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Yanwei Li",
        "Chengyao Wang",
        "Jiaya Jia"
      ],
      "year": "2023",
      "venue": "Llama-vid: An image is worth 2 tokens in large language models",
      "arxiv": "arXiv:2311.17043"
    },
    {
      "citation_id": "46",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "47",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "48",
      "title": "Chatunivi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Caiwan Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Chatunivi: Unified visual representation empowers large language models with image and video understanding",
      "arxiv": "arXiv:2311.08046"
    },
    {
      "citation_id": "49",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    }
  ]
}