{
  "paper_id": "2211.07131v1",
  "title": "Ym2413-Mdb: A Multi-Instrumental Fm Video Game Music Dataset With Emotion Annotations",
  "published": "2022-11-14T06:18:25Z",
  "authors": [
    "Eunjin Choi",
    "Yoonjin Chung",
    "Seolhee Lee",
    "JongIk Jeon",
    "Taegyun Kwon",
    "Juhan Nam"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing multi-instrumental datasets tend to be biased toward pop and classical music. In addition, they generally lack high-level annotations such as emotion tags. In this paper, we propose YM2413-MDB, an 80s FM video game music dataset with multi-label emotion annotations. It includes 669 audio and MIDI files of music from Sega and MSX PC games in the 80s using YM2413, a programmable sound generator based on FM. The collected game music is arranged with a subset of 15 monophonic instruments and one drum instrument. They were converted from binary commands of the YM2413 sound chip. Each song was labeled with 19 emotion tags by two annotators and validated by three verifiers to obtain refined tags. We provide the baseline models and results for emotion recognition and emotion-conditioned symbolic music generation using YM2413-MDB.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Music generation has been regarded as an attractive research topic for decades. Due to the revival of neural networks, the trend of music generation research has moved from rule-based to data-driven music approaches  [1, 2] . In the data-driven approaches, the dataset determines music styles of the generated output and thus it is desired to have a wide assortment of datasets that cover diverse music genres and styles. However, the majority of music datasets are limited to a single instrument, particularly the piano  [3] [4] [5] [6] [7] . Multi-instrumental datasets mainly cover popular music  [8, 9]  or classical music  [10] . In this regard, we introduce a new dataset from 80s FM video game music originally played for YM2413, a programmable sound generator based on FM sound synthesis. We name it the YM2413 Music Database or YM2413-MDB.\n\nYM2413-MDB provides not only a unique flavor of retro game music but also addresses two important issues in music generation research: the fixed number of instruments and the lack of high-level annotations. The most widely used multi-instrumental music dataset is the Lakh MIDI dataset (LMD). While it is the only large-scale MIDI dataset so far, the musical quality of MIDI files is not consistent within the dataset because it is gathered from public sources. Therefore, most studies using LMD generated music with a limited number of instruments for pop music  [11] [12] [13] [14] [15] [16] [17] [18] . Other datasets, such as JSB Chorale  [10]  and NES-MDB  [19] , have only four fixed instruments. Furthermore, the majority of these datasets lack high-level annotations such as genre or mood, which can be used for conditional music generation. Although songs in LMD are matched to entries in the Million Song Dataset (MSD), which has high-level tag annotations, the labels are rather noisy. There exist two symbolic emotion music datasets manually labeled  [5, 20] . However, they include only piano music. YM2413-MDB tackles the two issues with the following characteristics.\n\n• Multi-instrumental 80s FM Game Music: The dataset consists of 80's Sega game music played by YM2413, where 9 out of 16 instruments can be played at once, which improved the quality of the played game music at that time. From this advancement, the music played by YM2413 contains rich harmonic and rhythmic information.\n\n• Multi-modal: The dataset contains 669 songs in various formats: binary video game music (VGM) files that contain FM synthesis parameters, converted MIDI files, and rendered audio files which restore the original game music sound. Also, the generated MIDI files can be rendered to audio using an FM-synthesizer plug-in.\n\n• Emotion-annotation: We annotated the entire dataset with 19 emotion tags in the game context. To make a tag vocabulary with high agreement, each song was annotated by two dedicated annotators and verified by three reviewers. We also confirmed that these tags can be classified using the baseline classification approach.\n\nWe first describe the details of dataset collection and statistical analysis of musical features. Then, we provide Table  1 : Comparison with existing multi-instrumental or emotion-labeled symbolic music datasets. When the dataset do not limit the instrument, # Inst is 128. When the dataset has no information of size in hours, it was estimated using the MIDI max tick (denoted as *).\n\nbaseline results of music emotion recognition in both audio and MIDI domains and emotion-conditioned symbolic music generation using a music generation model. We release YM2413-MDB in the online repository 1 . Examples of generated music are accessible in our demo webpage 2 .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Characteristics Of Game Music",
      "text": "Starting with the single sound effect used in the first video game, Pong (1972), the development of game music is in line with the development of computer sound technology  [23] . Today's game music is not much different from music from other media, such as dramas and movies. However, early video game music has a distinct characteristic from other genres of music due to technical constraints at the time. A musicological study on early game music found that some unique composition characteristics are found in the form of arpeggio patterns, compound melodies, and simulated reverb effects using delayed instruments  [24] . Another characteristic of video game music is that it is composed to be played as a loop  [25] . In addition, the transition of playing music occurs when the game context is changed, such as a triggered event: map transition, battle with a boss monster, mission completion, ending, and so on.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Game Music Generation",
      "text": "There are several studies that focus on computer-generated game music. For example, Prechtl used a music generation model based on Markov Chain, which changes the model state in real-time aligned with in-game parameters  [25]  They tested the model in a live game context. Mauthes generated video game music using an RNN-based model by collecting video game music crowd-sourced by the game user community  [21] . Ferreira and Whitehead used piano-arranged video game music to generate music using LSTM and a genetic algorithm with sentiment as a condition  [20] . Donahue et al. generated game music using Transformer-XL  [26]  with the Lakh dataset  [8]  pre-training and NES-MDB fine-tuning  [19, 27] .\n\n1 https://github.com/jech2/YM2413-MDB 2 https://jech2.github.io/YM2413-MDB/",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Instrumental Symbolic Music Dataset",
      "text": "There are a handful of multi-instrumental datasets that cover different music genres, including pop  [8, 22] , classic  [10, 28] , band  [9] , and game  [19, 21]  genres. Table  1  compares the datasets to YM2413-MDB. Among them, the most widely used one is LMD  [8] . Since LMD is a largescale dataset and has a wide variety of instruments, most multi-instrumental research has used LMD with a set of simplified instruments  [11] [12] [13] [14] [15] [16] [17] [18]  so far. Recently, large-scale datasets with a Band  [9]  or Orchestra  [28]  style have been released. In the game music genre, NES-MDB, the early 80's Nintendo Game Music Dataset  [19]  is most similar to our study. All songs in NES-MDB have four monophonic instruments because of the technical constraints of NES's Audio Processing Unit. Songs in NES-MDB have a unique style as retro game music and have expressiveness represented with note timings in the precise time units. However, such expressive characteristics require metric analysis such as beat tracking to extract score-level information.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Symbolic Music Dataset With Emotion Annotation",
      "text": "There are a few symbolic music datasets with emotion annotation  [5, 20, 22] . MOODetector contains audio, MIDI, and lyrics for 193 music files  [22] . The music was annotated according to the emotion category used in the MIREX Mood Classification Task  [29] . VGMIDI contains 200 MIDI files of piano-arranged video game music with arousal-valence annotations  [20] . EMOPIA contains 1087 pop piano music with annotations according to Russell's 4Q  [5] . All of these datasets include both MIDI and audio formats.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Ym2413-Mdb",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Description",
      "text": "The dataset consists of game music played by the YM2413 sound chip. YM2413 is a programmable sound generator based on FM synthesis developed by Yamaha. FM synthesis can generate diverse musical tones with rich harmonics. YM2413 can play a maximum of 9 instruments simultaneously from 16 instrument presets and one user tone register. The names of the instruments are shown in Figure  1 (c). YM2413-MDB contains songs with a short duration for specific game events similar to those in NES-MDB. The statistics are shown in Figure  1 (a). In addition, we observed two unique composition patterns: unison playing and simulated reverb using delay. Unison playing is a pattern of the same melody with single or multiple instruments. This technique renders a rich tone by overlapping the same voices of the FM instrument. The delay pattern is also observed in the NES music to express the reverb by using the delayed same voice  [24] . These patterns show how the composers at that time were concerned about expressing various tones. Therefore, the role and use of each instrument observed in YM2413-MDB seem to be quite different from those in today's composition style.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Collection & Preprocessing",
      "text": "We collected Video Game Music (VGM) files from the game music community websites 3 4 . After data collection, we translated all the game music files to the MIDI format. Since the original VGM files are written as binary command sequences read by the sound chip, the VGM commands are different for all sound chips; for YM2413, there are non-standard commands for time wait, instrument note on/off, program change, volume change, and FM synthesis parameters for one user's custom tone. We conducted the conversion by emulating the YM2413 sound chip following the FM Operator Type-LL (OPLL) Application Manual  [30] . For the VGM disassembly process and sound chip emulation, we referred to the source code of VGMPlay  5  , a software that reads the VGM file and reproduces the sound of hardware sound chips. Also, we exported the VGM files as audio files using this software. The original VGM file format expresses all time-related information as ticks with no tempo-related event. In the case of NES-MDB, they converted all MIDI files with 120 beats per minute (BPM) and 22050 ticks per beat to preserve the temporal resolution of 44100Hz without precise tempo calculation. However, the converted MIDI files were not aligned with the metrical structure that standard MIDI files have. This has hindered the use of bar-based music representation, chord recognition, and further music analysis  [4, 16, 31, 32] . Therefore, we conducted the metrical alignment using the extracted down-beat information of rendered audio files using Madmom  [14] . For each music file, tempo events were added using estimated tempos from all downbeats. Since most of the pieces had a fixed tempo, we used the median value of the beat-wise estimation of tempo.\n\nIn addition, the sound chip synthesizes sounds using frequency values rather than MIDI note numbers; in practice, a frequency is expressed as an F-Number, a discrete value that maps the frequency using 9 bits  [30] . Therefore, some of the songs in YM2413-MDB were intentionally detuned for the song's ambiance, and instruments had pitch bends and vibratos for more expressiveness. These characteristics make it difficult to map the right MIDI pitch without careful consideration. We treated such cases using the MIDI pitch bend event, such that the integers and decimal points of the calculated MIDI note number are mapped to the MIDI pitch and pitch bend value.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Annotation",
      "text": "During the annotation and verification process and analysis, we referred to  [33]  for tag vocabulary refinement, verification procedure, and tag visualization. Two researchers participated in both initial tagging and annotation. Once the dataset was collected, we first listened to the entire game music audio files in the dataset and freely described them with the perceived emotion words. From this process, we found several game-specific emotion words that cannot be directly mapped into both the conventional emotion arousal-valence model and emotion tags used in MIREX music classification  [22] : fluttered (war-inspiring), grand, bizarre, frustrating, comic, faint, and touching. As discussed in  [34] , the categorical approach is suitable for these complex emotions rather than the dimensional approach. Therefore, we decided to annotate the emotions as word tags.\n\nFrom the emotion descriptions of all songs in the dataset, we obtained 35 emotion-related tags. We grouped similar emotion words according to a Korean crowdsource BGM repository  6  and finally obtained 19 emotion tags. After the tag vocabulary was refined, we conducted the labeling. While annotating the dataset, each of the annotators selected one dominant emotion tag for each music which we call \"top tag\" in this paper. Among the top tag annotated by the two annotators, the final top tag was selected as having a higher agreement after verification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Emotion Verification",
      "text": "After the annotation step, each music file was validated by three verifiers; in total, ten people participated as verifier. During verification, verifiers were asked to mark the emotion tag if they disagree with the annotation. We provided verifiers with the annotator's description of each emotion tag to reduce the ambiguity of each emotion tag. After verification, tags with low agreement (lower than two) were excluded from the annotation of each music. When the top tag was excluded after verification (26 songs), the tag with the highest agreement was chosen as the new top tag.\n\nFigure  2 (a) shows the overall emotion tag distribution of the YM2413-MDB after verification. Also, the agreement portion of verification explains the subjectivity of each tag. In particular, 'serious', 'fluttered', 'depressed', and 'cold' tags have less agreement than other tags. The frequency distribution of top tags is shown in Figure  2(b) , in which the imbalance is more noticeable than the frequency distribution of each tag. In Figure  2 (c), we investigated the independence of each tag by calculating the co-occurrence of emotion tags. The words that often appear together(>0.8) are as follows: 'speedy'-'tense', 'creepy'-'tense', 'bizarre'-'tense', 'cute'-'cheerful', 'comic'-'cheerful', and 'boring'-'calm'.\n\nAnnotation and verification for the Korean tags were operated since all participants were native Korean. They are aged from twenty-four to thirty-one, with at least one retro video game-playing experience.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Representation",
      "text": "For the baseline symbolic-domain classification and generation, we used the event-based representation called Multi-Track Music Machine (MMM)  [35] . The MMM representation can handle arbitrary instrument combinations. As discussed in Section 3.1, our dataset contains unison patterns; the same instrument appears in multiple tracks. To apply representations suggested for multi-instrumental music that assume each instrument appears at most once  [16, 27, 32] , we should discard instrument tracks that appear multiple times in a song. However, we did not apply the strategy due to the small dataset size. In our preliminary study, experiments with multi-instrumental representation suggested by  [27]  only generate a drum with a large number of notes. We speculate that it was due to the frequency imbalance of tokens; since the representation suggested by  [27]  combines musical instruments and pitch tokens, the number of tokens increases in proportion to the number of instruments, making the model difficult to learn pitch information. Therefore, we did not use those non-MMM style representations.\n\nTo re-implement MMM, we calculated the note density for each instrument by dividing the total number of note counts by the active bar number, which is the number of bars in which at least one note is played. Also, we quantized the music samples with 48 grids per bar. After removing the bar-fill-related vocabulary of MMM which is not necessary for our tasks, we used a total of 447 tokens: five tokens for the piece, track, and bar; 48 tokens for time delta tokens; 128 tokens for each instrument, note on, and note off. Since the MMM representation can express a fixed number of bars, we used four bars with a maximum of six instruments, which the transformer attention window size of 1024 can handle. Fixing the length of generated music is suitable for game music because the game music is loop-based, as discussed in Section 2. We sampled the instrument when the number of instruments was greater than six. Due to the sparsity issue, we adapted the dataset for training. The instruments were mapped into six categories during training: piano, string, woodwind, brass, guitar, bass, and drum. Tracks that change within the same channel are all adapted to be treated as one instrument.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset Analysis",
      "text": "Emotion is associated with various musical features such as harmony, rhythm, and loudness  [36] . We investigated how the musical elements extracted from YM2413-MDB are correlated with the collected emotion tags. We used note density, note duration, and tonality following the previous work  [5] . Due to the imbalanced distribution of top tags, we focused on songs with five top tags: 'tense', 'cheerful', 'touching', 'bizarre', and 'depressed'. When extracting the features, we excluded drum tracks to consider the pitch information only when estimating tonality and fairly compared songs with and without drums.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Tonality",
      "text": "The major/minor tonality is generally connected to positive/negative emotional states  [37] . We measured tonality using the Krumhansl-Schmuckler key-finding algorithm  [38]  implemented in music21  [39] . Figure  3(a)  shows a clear trend that the major tonality is frequently used for positive emotions (e.g. 'cheerful', 'touching'), and the minor tonality is frequently used for negative emotions (e.g. 'tense', 'bizarre', 'depressed').",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Note Density And Duration",
      "text": "Another musical feature related to emotion is rhythm  [40] . We use note density and note duration as indirect cues to extract the rhythmic characteristics. The note density refers to the number of notes per beat, which was obtained by dividing the total number of notes by the total number of beats. It was normalized by the number of instruments per song. The note duration was calculated by averaging the note duration value in beat units. As shown in Figure  3(b-c ), songs with 'tense', 'cheerful', and 'bizarre' tags showed higher note density than the 'touching' and 'depressed' tags. Most songs with 'tense', 'cheerful', and 'bizarre' tags consisted of notes shorter than one beat, whereas songs with 'touching' and 'depressed' tags were   widely distributed with longer notes. We also measured the note velocity, but all tag groups maintained a similar distribution. It is because songs in YM2413-MDB do not utilize the velocity features well; in most cases, the velocity of notes is changed together only when program change events occur.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Music Emotion Recognition",
      "text": "We provide our baseline symbolic and audio-domain emotion recognition results using YM2413-MDB. For both symbolic-and audio-domain classification, we used the source code and experimental setting (4Q, Arousal, Valence) of  [5]  and adapted it to our dataset configuration. We used the same model for classification (logistic regression, LSTM-Attn  [41] , and short-chunk ResNet  [42] ). For symbolic domain classification, we used a re-implemented MMM representation with 8 bars and 6 instrument chunks. Due to the small volume with imbalanced emotion tags, it was difficult to use the original emotion tags for emotion recognition directly. Instead, we mapped the top tags into Russell's four quadrants and classified a song into one of the four categories; for tags that do not appear in Russell's circumplex model, we manually mapped to one quadrant of the most similar emotion according to the subjectivity of the annotator 7 . We also provide the baseline binary classification results using 'tense' and 'cheerful' top tags. Other tags showed low classification results due to severe imbalance. We split the dataset with an 8:1:1 ratio for train, validation, and test via stratified sampling. The emotion classification results of symbolic-and audio-domain are shown in Table  2  and Table 3 . Both symbolic-and audio-domain classification results of top tags show that these emotions can be recognized by the baseline models. Since the dataset is multi-instrumental and the class imbalance is more severe than EMOPIA, the baseline classification was lower than those reported in  [5] . Also, we note that the symbolic-domain classification performance of the LSTM-Attention model with MMM was poorer than that of the logistic regression. We conjectured that it was difficult to learn the temporal relations between instruments when using MMM representation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Emotion-Conditioned Symbolic Music Generation 6.1 Data Preparation",
      "text": "We provide the baseline emotion-conditioned symbolic music generation results using YM2413-MDB. Due to the emotion tag class imbalance, emotion conditioning was ineffective when using the entire set of emotion top tags. Instead, we selected 'cheerful' and 'depressed' as representative emotion conditions that showed contrasting features in the dataset analysis (Fig  3 ) and conducted conditional generation using the data with the two tags. Similar to  [27] , as our dataset size was small to train the deep learning model from scratch, we used LMD for pre-training the music generation model and fine-tuned it using YM2413-MDB. We filtered too short MIDI files which are less than 3 secs. We also filtered short samples annotated as sound effects. After filtering, we used 10K samples for validation and test sets when we trained the LMD. A total of 286 songs were used for fine-tuning, and four songs containing both 'cheerful' and 'depressed' tags were excluded.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model & Train Setting",
      "text": "Transformer-XL  [26]  is widely used in music generation studies with language modeling approach  [4, 9, 27, 43] . GPT2  [44]  is a language model that showed high finetuning performance in NLP without changing the model structure and was also used in several music generation studies  [35, 45] . In our preliminary study, we observed that GPT2 generated better quality samples than Transformer-XL. In addition, the generation samples from lakh pre-train were better when adding more layers than  [35]  used (six layers). Therefore, we used the 12-layer GPT2  [44]  with an attention window size of 1024 for our baseline generation model. Before passing the main architecture, each token of event sequence input was embedded as 512 dimensions. To maintain the dimension while calculating attention, the number of attention heads and attention head dimensions were set to 8 and 64, respectively. For all layers, the dimension of the feed-forward layer after the attention module was 1024. The initial learning rate was set to 5e-5, and we used the AdamW optimizer with a linear scheduler. To generate music with emotion condition, we fine-tuned the pre-trained GPT2 model using YM2413-MDB. Also, by using the in-attention mechanism proposed in  [18] , we projected the emotion tag embedding to the same space as the hidden states, then summed the projected condition with the hidden states at each self-attention layer. For emotion tag embedding, we used the pre-trained Word2Vec embedding  [46]  of 'cheerful' and 'depressed' tags, which were also updated during fine-tuning.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Generated Results",
      "text": "Although we provided the baseline classification results, the performance was not sufficient to evaluate conditional music generation. Therefore, in this section, we show the overall tendency of the generation model through the objective measure. To see the validity of emotion conditioning in music generation, we compared 100 modelgenerated songs conditioned on 'cheerful' and 'depressed' emotions. We compared the histogram of note density and note duration of the generated results with the model inputs. For model inputs, the first 4 bars of each of the 284 songs were analyzed. As shown in Figure  4 , given 'depressed' conditions, the model generates samples with a longer note duration and lower note density than when using 'cheerful' condition. However, when we listened to the generated samples, we found some of the generated samples struggled with temporal information, such as note onset time. It seems delayed notes in the dataset hindered learning the overall beat structure of the music. Although the model inputs had distinct distributions for major/minor tonalities, the generated results did not reflect these features. We suppose that it is due to the characteristics of game music that we observed in YM2413-MDB; the appearance of whole-tone or chromatic patterns makes it difficult for the model to learn tonality.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced YM2413-MDB, a multiinstrumental FM video game music dataset annotated with emotion tags. Using this dataset, we conducted a basic statistical analysis of musical features and provided the baseline results for emotion recognition and conditional music generation. We plan to study retro game music compositional styles further and improve music emotion classification and emotion-conditioned music generation. Also, we will investigate other uses of YM2413-MDB such as multi-instrumental music transcription, source separation, and structure analysis.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (c). YM2413-MDB contains songs with a short",
      "page": 2
    },
    {
      "caption": "Figure 1: General statistics of YM2413-MDB. (a) Song duration of all songs, (b) Number of instruments per song consid-",
      "page": 3
    },
    {
      "caption": "Figure 1: (a). In addi-",
      "page": 3
    },
    {
      "caption": "Figure 2: Emotion annotation and veriﬁcation results. (a) Frequency distribution of emotion tags where n is the number",
      "page": 4
    },
    {
      "caption": "Figure 2: (a) shows the overall emotion tag distribu-",
      "page": 4
    },
    {
      "caption": "Figure 2: (b), in which the imbalance is more notice-",
      "page": 4
    },
    {
      "caption": "Figure 2: (c), we investigated the independence of each",
      "page": 4
    },
    {
      "caption": "Figure 3: Data distribution for different top tag classes. (a) Normalized frequency of tonality histogram, (b) Note density,",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) shows a",
      "page": 5
    },
    {
      "caption": "Figure 3: (b-c), songs with ‘tense’, ‘cheerful’, and ‘bizarre’",
      "page": 5
    },
    {
      "caption": "Figure 3: ) and conducted condi-",
      "page": 6
    },
    {
      "caption": "Figure 4: The note density and note duration distribution",
      "page": 6
    },
    {
      "caption": "Figure 4: , given ‘de-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Audio-domain classification accuracy. 4Q indi-",
      "data": [
        {
          "Emotion": "4Q Arousal Valence"
        },
        {
          "Emotion": "0.48 0.76 0.56"
        },
        {
          "Emotion": "0.44 0.69 0.59"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Audio-domain classification accuracy. 4Q indi-",
      "data": [
        {
          "Emotion": "4Q Arousal Valence"
        },
        {
          "Emotion": "0.38 0.72 0.54"
        },
        {
          "Emotion": "0.65 0.78 0.60"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Algoritmic music composition based on artificial intelligence: A survey",
      "authors": [
        "O Lopez-Rincon",
        "O Starostenko",
        "G Ayala-San Martín"
      ],
      "year": "2018",
      "venue": "Proceedings of 2018 International Conference on Electronics, Communications and Computers (CONIELECOMP)"
    },
    {
      "citation_id": "3",
      "title": "A comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations, and future directions",
      "authors": [
        "S Ji",
        "J Luo",
        "X Yang"
      ],
      "year": "2020",
      "venue": "A comprehensive survey on deep music generation: Multi-level representations, algorithms, evaluations, and future directions",
      "arxiv": "arXiv:2011.06801"
    },
    {
      "citation_id": "4",
      "title": "POP909: A pop-song dataset for music arrangement generation",
      "authors": [
        "Z Wang",
        "K Chen",
        "J Jiang",
        "Y Zhang",
        "M Xu",
        "S Dai",
        "G Bin",
        "G Xia"
      ],
      "year": "2020",
      "venue": "Proceedings of 21st International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "5",
      "title": "Pop Music Transformer: Beat-based modeling and generation of expressive pop piano compositions",
      "authors": [
        "Y.-S Huang",
        "Y.-H Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation",
      "authors": [
        "H.-T Hung",
        "J Ching",
        "S Doh",
        "N Kim",
        "J Nam",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of 22nd International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "7",
      "title": "Enabling factorized piano music modeling and generation with the MAESTRO dataset",
      "authors": [
        "C Hawthorne",
        "A Stasyuk",
        "A Roberts",
        "I Simon",
        "C.-Z Huang",
        "S Dieleman",
        "E Elsen",
        "J Engel",
        "D Eck"
      ],
      "year": "2019",
      "venue": "Enabling factorized piano music modeling and generation with the MAESTRO dataset"
    },
    {
      "citation_id": "8",
      "title": "GiantMIDI-Piano: A large-scale midi dataset for classical piano music",
      "authors": [
        "Q Kong",
        "B Li",
        "J Chen",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Transactions of the International Society for Music Information Retrieval"
    },
    {
      "citation_id": "9",
      "title": "Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching",
      "authors": [
        "C Raffel"
      ],
      "year": "2016",
      "venue": "Learning-based methods for comparing sequences, with applications to audio-to-midi alignment and matching"
    },
    {
      "citation_id": "10",
      "title": "DadaGP: A dataset of tokenized guitarpro songs for sequence models",
      "authors": [
        "P Sarmento",
        "A Kumar",
        "C Carr",
        "Z Zukowski",
        "M Barthet",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of 22nd International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "11",
      "title": "Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription",
      "authors": [
        "N Boulanger-Lewandowski",
        "Y Bengio",
        "P Vincent"
      ],
      "year": "2012",
      "venue": "Proceedings of the 29th International Conference on Machine Learning(ICML)"
    },
    {
      "citation_id": "12",
      "title": "Musegan: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment",
      "authors": [
        "H.-W Dong",
        "W.-Y Hsiao",
        "L.-C Yang",
        "Y.-H Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 32th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Convolutional generative adversarial networks with binary neurons for polyphonic music generation",
      "authors": [
        "H.-W Dong",
        "Y.-H Yang"
      ],
      "year": "2018",
      "venue": "Proceedings of 19th International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "14",
      "title": "MIDI-Sandwich: Multimodel multi-task hierarchical conditional vae-gan networks for symbolic single-track music generation",
      "authors": [
        "X Liang",
        "J Wu",
        "Y Yin"
      ],
      "year": "2019",
      "venue": "Proceedings of Australian Journal of Intelligent Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "MIDI-Sandwich2: Rnnbased hierarchical multi-modal fusion generation vae networks for multi-track symbolic music generation",
      "authors": [
        "X Liang",
        "J Wu",
        "J Cao"
      ],
      "year": "2019",
      "venue": "MIDI-Sandwich2: Rnnbased hierarchical multi-modal fusion generation vae networks for multi-track symbolic music generation",
      "arxiv": "arXiv:1909.03522"
    },
    {
      "citation_id": "16",
      "title": "Melody2Vec: Distributed representations of melodic phrases based on melody segmentation",
      "authors": [
        "T Hirai",
        "S Sawada"
      ],
      "year": "2019",
      "venue": "Journal of Information Processing"
    },
    {
      "citation_id": "17",
      "title": "PopMAG: Pop music accompaniment generation",
      "authors": [
        "Y Ren",
        "J He",
        "X Tan",
        "T Qin",
        "Z Zhao",
        "T.-Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Pirhdy: Learning pitch-, rhythm-, and dynamics-aware embeddings for symbolic music",
      "authors": [
        "H Liang",
        "W Lei",
        "P Chan",
        "Z Yang",
        "M Sun",
        "T.-S Chua"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "MuseMorphose: Full-song and fine-grained music style transfer with just one transformer VAE",
      "authors": [
        "S.-L Wu",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "MuseMorphose: Full-song and fine-grained music style transfer with just one transformer VAE",
      "arxiv": "arXiv:2105.04090"
    },
    {
      "citation_id": "20",
      "title": "The NES music database: A multi-instrumental dataset with expressive performance attributes",
      "authors": [
        "C Donahue",
        "H Mao",
        "J Mcauley"
      ],
      "year": "2018",
      "venue": "Proceedings of 19th International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "21",
      "title": "Learning to generate music with sentiment",
      "authors": [
        "L Ferreira",
        "J Whitehead"
      ],
      "year": "2019",
      "venue": "Proceedings of 20th International Conference on Music Information Retrieval (IS-MIR)"
    },
    {
      "citation_id": "22",
      "title": "VGM-RNN: Recurrent neural networks for video game music generation",
      "authors": [
        "N Mauthes"
      ],
      "year": "2018",
      "venue": "VGM-RNN: Recurrent neural networks for video game music generation"
    },
    {
      "citation_id": "23",
      "title": "Multi-Modal Music Emotion Recognition: A new dataset, methodology and comparative analysis",
      "authors": [
        "R Panda",
        "R Malheiro",
        "B Rocha",
        "A Oliveira",
        "R Paiva"
      ],
      "year": "2013",
      "venue": "Proceedings of 10th International Symposium on Computer Music Multidisciplinary Research (CMMR)"
    },
    {
      "citation_id": "24",
      "title": "The impact of sound technology on video game music composition in the 1990s",
      "authors": [
        "F Rojas"
      ],
      "year": "2018",
      "venue": "The impact of sound technology on video game music composition in the 1990s"
    },
    {
      "citation_id": "25",
      "title": "Programmed baroque'n'roll: Composition techniques for video game music on the nintendo entertainment system",
      "year": "2016",
      "venue": "Programmed baroque'n'roll: Composition techniques for video game music on the nintendo entertainment system"
    },
    {
      "citation_id": "26",
      "title": "Adaptive music generation for computer games",
      "authors": [
        "A Prechtl"
      ],
      "year": "2016",
      "venue": "Adaptive music generation for computer games"
    },
    {
      "citation_id": "27",
      "title": "Transformer-XL: Attentive language models beyond a fixed-length context",
      "authors": [
        "Z Dai",
        "Z Yang",
        "Y Yang",
        "J Carbonell",
        "Q Le",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Transformer-XL: Attentive language models beyond a fixed-length context"
    },
    {
      "citation_id": "28",
      "title": "LakhNES: Improving multi-instrumental music generation with cross-domain pre-training",
      "authors": [
        "C Donahue",
        "H Mao",
        "Y Li",
        "G Cottrell",
        "J Mcauley"
      ],
      "year": "2019",
      "venue": "Proceedings of 20th International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "29",
      "title": "Symphony generation with permutation invariant language model",
      "authors": [
        "J Liu",
        "Y Dong",
        "Z Cheng",
        "X Zhang",
        "X Li",
        "F Yu",
        "M Sun"
      ],
      "year": "2022",
      "venue": "Symphony generation with permutation invariant language model",
      "arxiv": "arXiv:2205.05448"
    },
    {
      "citation_id": "30",
      "title": "Exploring Mood Metadata: Relationships with genre, artist and usage metadata",
      "authors": [
        "X Hu",
        "J Downie"
      ],
      "year": "2007",
      "venue": "Proceedings of 8th International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "31",
      "title": "YM2413 FM operator Type-LL (OPLL) application manual",
      "authors": [
        "Yamaha"
      ],
      "year": "1987",
      "venue": "YM2413 FM operator Type-LL (OPLL) application manual"
    },
    {
      "citation_id": "32",
      "title": "Compound Word Transformer: Learning to compose full-song music over dynamic directed hypergraphs",
      "authors": [
        "W.-Y Hsiao",
        "J.-Y Liu",
        "Y.-C Yeh",
        "Y.-H Yang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 35th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "MusicBERT: Symbolic music understanding with large-scale pre-training",
      "authors": [
        "M Zeng",
        "X Tan",
        "R Wang",
        "Z Ju",
        "T Qin",
        "T.-Y Liu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "34",
      "title": "Semantic tagging of singing voices in popular music recordings",
      "authors": [
        "K Kim",
        "J Lee",
        "S Kum",
        "C Park",
        "J Nam"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Music emotion recognition: Toward new, robust standards in personalized and context-sensitive applications",
      "authors": [
        "J Gómez-Cañón",
        "E Cano",
        "T Eerola",
        "P Herrera",
        "X Hu",
        "Y.-H Yang",
        "E Gómez"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "36",
      "title": "MMM: Exploring conditional multi-track music generation with the transformer",
      "authors": [
        "J Ens",
        "P Pasquier"
      ],
      "year": "2020",
      "venue": "MMM: Exploring conditional multi-track music generation with the transformer",
      "arxiv": "arXiv:2008.06048"
    },
    {
      "citation_id": "37",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Perception of the major/minor distinction: Iv. emotional connotations in young children",
      "authors": [
        "M Kastner",
        "R Crowder"
      ],
      "year": "1990",
      "venue": "Music Perception"
    },
    {
      "citation_id": "39",
      "title": "What's key for key? the krumhanslschmuckler key-finding algorithm reconsidered",
      "authors": [
        "D Temperley"
      ],
      "year": "1999",
      "venue": "Music Perception"
    },
    {
      "citation_id": "40",
      "title": "music21: A toolkit for computer-aided musicology and symbolic music data",
      "authors": [
        "M Cuthbert",
        "C Ariza"
      ],
      "year": "2010",
      "venue": "music21: A toolkit for computer-aided musicology and symbolic music data"
    },
    {
      "citation_id": "41",
      "title": "Perceiving Emotion in Melody: Interactive effects of pitch and rhythm",
      "authors": [
        "E Schellenberg",
        "A Krysciak",
        "R Campbell"
      ],
      "year": "2000",
      "venue": "Music Perception"
    },
    {
      "citation_id": "42",
      "title": "A structured self-attentive sentence embedding",
      "authors": [
        "Z Lin",
        "M Feng",
        "C Santos",
        "M Yu",
        "B Xiang",
        "B Zhou",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Proceedings of International Conference of Learning Representations (ICLR)"
    },
    {
      "citation_id": "43",
      "title": "Evaluation of cnn-based automatic music tagging models",
      "authors": [
        "M Won",
        "A Ferraro",
        "D Bogdanov",
        "X Serra"
      ],
      "year": "2020",
      "venue": "Proceedings of the 17th Sound and Music Conference (SMC)"
    },
    {
      "citation_id": "44",
      "title": "The jazz transformer on the front line: Exploring the shortcomings of ai-composed music through quantitative measures",
      "authors": [
        "S.-L Wu",
        "Y.-H Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of 21th International Conference on Music Information Retrieval (ISMIR)"
    },
    {
      "citation_id": "45",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "46",
      "title": "MuseNet",
      "authors": [
        "C Payne"
      ],
      "year": "2019",
      "venue": "MuseNet"
    },
    {
      "citation_id": "47",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Proceedings of International Conference on Learning Representations (ICLR), Workshop Track Proceedings"
    }
  ]
}