{
  "paper_id": "2210.01487v1",
  "title": "Swarman: Anthropomorphic Swarm Of Drones Avatar With Body Tracking And Deep Learning-Based Gesture Recognition",
  "published": "2022-10-04T09:31:59Z",
  "authors": [
    "Ahmed Baza",
    "Ayush Gupta",
    "Ekaterina Dorzhieva",
    "Aleksey Fedoseev",
    "Dzmitry Tsetserukou"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "I. Introduction",
      "text": "With the latest development in robotics and telepresence technology, along with the production of robots for manufacturing purposes, more attention is paid to the use of robots in everyday life. Novel research topics are emerging in the field of service robots, suggesting their application as companions to improve the mental state of humans. To achieve the necessary behavior complexity, robots have to accurately determine the state of the user to establish natural communication.\n\nFor example, Muhammad Abdullah et al.  [1]  developed the emotion recognition system that uses the voice features in addition to the facial expressions of a human for the robot assistant functionality.\n\nCompanion robots can play with children and teach them, as proposed in the research by Leite et al.  [2]  in which the developed robots responded empathetically to several of the children's affective states. In addition to the voiced indication of certain emotions and the body language, several papers are focused on robots that can broadcast the emotional state by their eyes, e.g., an eyeball robot developed by Shimizu et al.  [3] .\n\nMobile robots are actively used as agents in teleoperation and telepresence tasks for affective communication. Most of these robots are designed to resemble the human body and to perform various operations similar to a human  [4] . However, telecommunication through the robotic avatar requires delivering the robot to the working area, which often proves challenging either due to the bulkiness of the robot or due to the dangerous environment. The operator stations have been equipped to organize the work of stationary robots, as suggested in the research of Christian Lenz and Sven Behnke  [5] , for telemanipulation by anthropomorphic avatar arms.\n\nThe mentioned above scenarios propose highly sufficient robotic systems. However, the mobility of the mentioned above robots is strictly limited by the workspace of the robot's upper body and the physical dimensions of the mobile platform. Moreover, their implementation may be challenging to the user due to the high mass and relatively slow operation of these systems. Meanwhile, a swarm of drones can serve as an effective remote-control tool. Several researchers explored applications of the robotic swarms in teleoperation, for example, Serpiva et al.  [6]  with the SwarmPaint system that utilizes a swarm of gesture-controlled drones to change formations and paint by the light in the air. Recently, due to the fast developments in telepresence technologies alongside virtual and mixed reality technologies, the teleoperation of drones avatars is suggested by Cordar et al.  [7]  for human telepresence and foster empathy with virtual agents and robots\n\nIn this paper, we propose a novel approach to the task of telepresence, involving a swarm of drones in broadcasting emotions from the operator to the user.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Anthropomorphic robot avatars were extensively investigated and improved in recent years. Such systems as TELESAR VI developed by Tachi et al.  [8]  allow dexterous remote manipulation and communication through an avatar designed to resemble the upper body of the human.\n\nSeveral researchers investigated effective communication through robot avatars. For example, Tsetserukou et al.  [9]  explored remote affective communications and proposed the robotic haptic device iFeel IM to augment the emotional experience during online conversations. Bartneck et al.  [10]  explored the dependence of human emotion perception on the character's embodiment, showing that there is no significant difference in the perceived intensity and recognition accuracy between robotic and screen characters. Chao-gang et al.  [11]  proposed a facial emotion generation model based on random graphs for virtual robots. A fuzzy emotion system that controls the face and the voice modules was developed by Vasquez et al.  [12]  for a tour-guide mobile robot.\n\nThough facial expressions play a major role in emotional recognition, the dynamic body postures could be recognized with relatively high precision. Matsui et al.  [13]  proposed a motion mapping approach to generate natural behavior for humanoid robots by copying human gestures. Cohen et al.  [14]  explored children's reactions to the iCat and NAO robots and achieved to design of well-recognized body postures for NAO. The end-to-end neural network model was developed by Yoon et al.  [15]  to generate sequences of human-like gestures enhancing NAO speech content. The variational auto-encoder framework was implemented by Marmpena et al.  [16]  for generating numerous emotional body language for the anthropomorphic Pepper robot.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. System Architecture",
      "text": "In the developed architecture shown in Fig.  2  the user interacts with the avatar swarm by visual interpretation of the emotion while the avatar operator performs the various body pose gestures to operate the avatar swarm of drones. The tracking and localization of the drones are done through the VICON mocap system which consists of 12 infra-red (IR) cameras. In the remote environment, the operator performs various gestures which showcase different emotions. These poses are captured by a DL-based gesture recognition algorithm which includes the major upper-body nine landmarks which include head, neck, left-shoulder, right-shoulder, left-elbow, rightelbow, right-hand and left-hand. These landmarks are then passed to the decision-making and agent allocation algorithm where along with the localized positions of the swarm the designated positions of the swarm of drones are calculated according to the relative positions of the major nine joints of the human upper body pose. The user interacts with the swarm of drones visually to understand the emotions that the operator was trying to perform. Along with the different poses of the emotions, the light rings on the drones also convey a psychological effect on the user for interpreting the type of emotion which includes green for happy, red for angry, white for neutral, yellow for confusion, and blue for sad.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Trajectory Generation And Swarm Control",
      "text": "For a more immersive experience and intuitive control, the operator of the avatar is controlling the swarm of drones through a camera feed. The operator's body landmarks are collected and then used for trajectory generation and Gesture recognition (V). Based on the calculated trajectory, each drone is assigned a role to fly as in the swarm, e.g., left hand, right shoulder, and head.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Trajectory Generation",
      "text": "The operator's body is being tracked using the MediaPipe Holistic pipeline and the resulting body landmarks are extracted from the camera feed. Since the body landmarks are in the camera frame, both the landmarks' positions and the body scale are dependent on the operator's distance from the camera. Transformation of the axis is needed to represent the swarm in the real remote environment. Let S be the camera coordinate frame; we assume a coordinate frame S with an origin at the head landmark in the camera frame; thus the position p of a body landmark p relative to S is given by:\n\nwhere h is the head position relative to S, p is the body landmark position. Using the resulting points, a tree of vectors is constructed, shown in (Fig.  3 ). The vectors\n\nare the unit vectors pointing from each parent landmark to child landmark, relative to S , and is described as:\n\nwhere p n is the parent landmark position, p m is the child landmark position. An accurately body-scaled formation can be obtained by multiplying each unit vector by the actual length of the correspondent operator's body part, thus eliminating the change in swarm size with the change in the operator-camera distance. Since the list\n\nrepresents the positions of the target formation in the frame S , where the head landmark is the origin, then by choosing the position of head drone in the real environment we can represent each target in the formation using the recursive equation shown in:\n\nwhere P m , P n and L m are the child point target in the real environment, the parent point target in the real environment, and the length of the correspondent operator's body part, respectively. P 0 is equal to the needed head position in the real environment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Agent Assignment",
      "text": "Crazyflie drones are used in the system utilizing the Crazyswarm ROS package, the package offers some reliable solutions for swarm management. integrating with Crazyswarm solutions, dynamic agent assignment helps the system be less complex, easy to set up, and more reliable. The Euclidean distance f is implemented as a cost function, which is calculated as given by:\n\nwhere p i is the target landmark position, q i is the position of the drone of the swarm. The result of the task assignment is shown in Fig.  4 .\n\nThe successful agent assignment is achieved with the developed algorithm by finding the nearest available drone to the formation target.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Collision Avoidance",
      "text": "To avoid the inter-agent collision, Artificial Potential Field (APF) was implemented. The APF algorithm is simply modeling the target way-point as an attraction field and obstacles, other swarm agents, as repulsion filed. A navigation policy is constructed by calculating the resultant virtual force on the agent, using it to steer the drone away from obstacles toward the target. Since the drones are flying in a close formation, the repulsive potential is modeled to only act within a sphere of influence surrounding each drone. The radius r 0 of the implemented sphere of influence, 7, where [0.2, 0.2, 0.4] T as the down-wash of drones were affecting each other, the z component was increased compared to the x and y components. The overall potential U sum , shown in:\n\nwhere U att is the attraction potential and U rep is the repulsion potential. The attraction potential is defined in:\n\nwhere P drone is the drone current position, P target is the drone desired position, and ξ is the scaling factor. The repulsive potential is defined in:\n\n, ρ > r 0  (7)  where ρ(x, y, z) is the distance function and η is the constant scaling factor. The distance function ρ(x, y, z) is implemented as Manhattan distance described as:\n\nwhere p and q are the drone position and the obstacle position, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Gesture Recognition",
      "text": "In the process of interaction between a swarm of drones and a user, we used five simple, most commonly expressed emotions during communication: happiness, sadness, anger, confusion, and neutrality shown in Fig.  5 . To achieve an immersive experience for the operator as well as the user gesture recognition is added based on body tracking. While the user communicates through body tracking, rendering his motion into avatar poses representing his feelings, the system can help the operator convey the mentioned basic emotions through drone illumination utilizing gesture recognition. Thus, when the operator wants to convey to the user happiness, a simple victory hands up is recognized by our model while the swarm of drones repeats the operator's movements, it changes the color of the agents' light ring to green as a hint to the user. The system renders white, blue, red, and yellow to convey neutrality, sadness, anger, and confusion, respectively.\n\nTo automatically change the illumination during broadcasting different emotions, a gesture recognition technique based on deep learning was implemented to run in parallel with body tracking (Fig.  6 ). The network was trained for 100 epochs (Fig.  7 ), achieving accuracy on the test dataset of 97%. The speed of drones within a swarm is limited. Therefore, the operator cannot change position instantly, which necessitates the use of dynamic gestures rather than static ones. The implemented gesture recognition technique utilizes a deep neural network with an architecture that uses Long shortterm memory (LSTM) blocks to store data about the past positions of key points in a single sequence .\n\nTo train the neural network, sequences of body landmarks were used; each sequence consists of 30 video frames. Landmarks were extracted from each frame using the MediaPipe Holistic framework, as discussed in Section IV. A custom dataset was collected from six participants. We gathered body landmarks from 600 collected videos for training on the recognition of our five basic gestures.\n\nReal-time recognition accuracy during user studies was 93%. The user study was performed with a mixed group of participants; part of them didn't take part in recording the training dataset.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vi. Experimental Evaluation",
      "text": "Participants: We invited 10 participants (two females and eight males) aged from 22 to 26 (mean = 23.9, std = 1.22), to experience the visual interpretation of five different emotions from swarm avatar. Eight of the participants had not previously worked with drones, while two participants had interacted with drones several times.\n\nProcedure: The experimental procedure applied for emotion evaluation is based on the methodology suggested by Ajili et al.  [17]  for accessing virtual avatar expressive gestures performed with four basic emotions selected from Russell's Circumplex model of affect (happy, sad, angry, and calm). In this paper, we proposed adding the fifth emotion of confusion to represent knowledge emotions  [18]  on top of the basic emotions mentioned above. Users watched a series of poses performed by the swarm avatar in simulation.\n\nUsers were not being influenced by other external factors such as the sound or color of the drones. After watching the avatar performance, each user rated the degree to which they perceived expressed emotions on a 5-point Likert scale (1 being strongly disagreed, 3 being neutral and 5 being strongly agreed).\n\nExperimental results:\n\nTo validate the internal consistency of user responses, we computed Cronbach's alpha as a metric used to assess the reliability of a set of Likert scale, it is expressed as a number between 0 and 1. The mean of Cronbach's alpha is > 0.8 for all emotions, indicating the high consistency between the users' evaluation of the performed emotions. All emotions were evaluated relatively high by the users with the emotion of happiness being accessed higher than others (mean 4.5 out of 5).\n\nA. User evaluation of the SwarMan by NASA-TLX based Questionnaire\n\nAll participants invited to the emotion recognition experiment were then invited to communicate with each other through the SwarMan interface. After the experiment, each participant was asked to complete a questionnaire based on The NASA Task Load Index (NASA-TLX) and three extra questions which give information such as age, gender of the participant, and previous experience with drones. An additional parameter of Intuitiveness was added to the questionnaire to evaluate how natural was avatar-based communication. Therefore, the participants provided feedback on seven questions:\n\n• Mental Demand: How much mental and perceptual activity was required (e.g. deciding, calculating, etc)? Was the task easy or demanding, simple or complex? (Low -High) • Physical Demand: How much physical activity was required? Was the task easy or demanding, slack or strenuous? (Low -High) • Temporal Demand: How much time pressure did you feel due to the pace at which the tasks or task elements occurred? Was the pace slow or rapid? (Low -High) • Overall Performance: How successful were you in performing the task? How satisfied were you with your performance? (Perfect -Failure) • Effort: How hard did you have to work (mentally and physically) to accomplish your level of performance? (Low -High) • Frustration Level: How irritated, stressed, and annoyed versus content, relaxed, and complacent did you feel during the task? (Low -High)\n\n• Intuitiveness: How natural was the experience? How intuitive did you find the swarm avatar response to your emotions? (Intuitive -Artificial) The results of the NASA-TLX based survey are shown in Fig.  8 . We conducted a chi-square analysis based on the frequency of answers in each category. The chi-square test of independence revealed that the participants' experience with drones does not affect the evaluation of swarm avatar control criteria, such as tiredness ( χ2 = 2.66, p = 0.92), temporal demand ( χ2 = 2.3, p = 0.94) and intuitiveness ( χ2 = 1.33, p = 0.98). In summary, participants did not feel any additional physical effort during the gesture control performance (mean of 1.9 on the 5-point scale). The majority of users did not experience Frustration (mean of 1.4 on the 5-point scale). In addition, the same users rated their overall performance satisfactory. All participants evaluated the system as intuitive (mean of 1.5 on the 5-point scale).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "We present a novel concept of telecommunications operating through an anthropomorphic drone swarm. The presented system utilized body tracking for trajectory generation to intuitively operate an anthropomorphic swarm of drones in a remote environment. Stable Control of the swarm of drones was realized using an artificial potential field algorithm and agent assignment based on minimum distance-based cost. The proposed system allows users to experience an effective human-swarm interaction utilizing the DNN-based gesture recognition technique. We achieved an acceptable gesture recognition accuracy of 93% during our user studies, rendering the recognized gestures into illumination to enhance the user's affective experience. Based on the performed user studies, operators of the system have found it intuitive (1.5 on the Likert scale) with overall low physical and mental demand (1.88, 2.25 on the Likert scale respectively). With the high maneuverability, scalability, low cost, low weight, and small storing size, the usage of the SwarMan swarm-based avatar can potentially provide a solution for a wide scope of problems existing in current applications of anthropomorphic robots in telepresence.\n\nWe plan to develop and integrate larger drones with higher payload in the system to allow operators physical interaction with the remote environment as well as with users through the sense of touch. The integration of more capable agents has the potential to increase the usability of the system and offers more solutions in telepresence applications.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) User interaction with SwarMan avatar. (b) The remote avatar",
      "page": 1
    },
    {
      "caption": "Figure 2: Layout of the SwarMan system.",
      "page": 2
    },
    {
      "caption": "Figure 3: Vector representation of the tracked body landmarks with coordinate",
      "page": 3
    },
    {
      "caption": "Figure 4: The successful agent assignment is achieved with the",
      "page": 3
    },
    {
      "caption": "Figure 4: Agent assignment results: Drones in random starting position",
      "page": 4
    },
    {
      "caption": "Figure 5: To achieve an",
      "page": 4
    },
    {
      "caption": "Figure 6: Dynamic gesture recognition deep learning network architecture.",
      "page": 4
    },
    {
      "caption": "Figure 7: ), achieving",
      "page": 4
    },
    {
      "caption": "Figure 7: Dynamic gesture recognition training. a) Loss. b) The categorical",
      "page": 4
    },
    {
      "caption": "Figure 5: Example of gestures representing emotions. a) Neutral. b) Happy. c) Sad. d) Angry. e) Confused.",
      "page": 5
    },
    {
      "caption": "Figure 8: Fig. 8. Subjective feedback on the 5-point NASA-TLX based Likert scale.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Estimated emotion": "Happy"
        },
        {
          "Estimated emotion": "4.50"
        },
        {
          "Estimated emotion": "1.90"
        },
        {
          "Estimated emotion": "2.40"
        },
        {
          "Estimated emotion": "2.80"
        },
        {
          "Estimated emotion": "2.50"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Hierarchical attention approach in multimodal emotion recognition for human robot interaction",
      "authors": [
        "M Abdullah",
        "M Ahmad",
        "D Han"
      ],
      "year": "2021",
      "venue": "2021 36th International Technical Conference on Circuits/Systems, Computers and Communications"
    },
    {
      "citation_id": "2",
      "title": "Modelling empathic behaviour in a robotic game companion for children: An ethnographic study in real-world settings",
      "authors": [
        "I Leite",
        "G Castellano",
        "A Pereira",
        "C Martinho",
        "A Paiva"
      ],
      "year": "2012",
      "venue": "2012 7th ACM/IEEE International Conference on Human-Robot Interaction"
    },
    {
      "citation_id": "3",
      "title": "Non-verbal communication-based emotion incitation robot",
      "authors": [
        "S Shimizu",
        "K Shimada",
        "R Murakami"
      ],
      "year": "2018",
      "venue": "2018 IEEE 15th International Workshop on Advanced Motion Control (AMC)"
    },
    {
      "citation_id": "4",
      "title": "A neurosymbolic humanlike arm controller for sophia the robot",
      "authors": [
        "D Hanson",
        "A Imran",
        "A Vellanki",
        "S Kanagaraj"
      ],
      "year": "2020",
      "venue": "A neurosymbolic humanlike arm controller for sophia the robot"
    },
    {
      "citation_id": "5",
      "title": "Bimanual telemanipulation with force and haptic feedback and predictive limit avoidance",
      "authors": [
        "C Lenz",
        "S Behnke"
      ],
      "year": "2021",
      "venue": "2021 European Conference on Mobile Robots (ECMR)"
    },
    {
      "citation_id": "6",
      "title": "Swarmpaint: Human-swarm interaction for trajectory generation and formation control by dnn-based gesture interface",
      "authors": [
        "V Serpiva",
        "E Karmanova",
        "A Fedoseev",
        "S Perminov",
        "D Tsetserukou"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Unmanned Aircraft Systems (ICUAS)"
    },
    {
      "citation_id": "7",
      "title": "Building virtual humans with back stories: Training interpersonal communication skills in medical students",
      "authors": [
        "A Cordar",
        "M Borish",
        "A Foster",
        "B Lok"
      ],
      "year": "2014",
      "venue": "Intelligent Virtual Agents"
    },
    {
      "citation_id": "8",
      "title": "Telesar vi: Telexistence surrogate anthropomorphic robot vi",
      "authors": [
        "S Tachi",
        "Y Inoue",
        "F Kato"
      ],
      "year": "2020",
      "venue": "International Journal of Humanoid Robotics",
      "doi": "10.1142/S021984362050019X"
    },
    {
      "citation_id": "9",
      "title": "ifeel im!: Augmenting emotions during online communication",
      "authors": [
        "D Tsetserukou",
        "A Neviarouskaya"
      ],
      "year": "2010",
      "venue": "IEEE Computer Graphics and Applications"
    },
    {
      "citation_id": "10",
      "title": "In your face, robot! the influence of a character's embodiment on how users perceive its emotional expressions",
      "authors": [
        "C Bartneck",
        "J Reichenbach",
        "A Van Breemen"
      ],
      "year": "2004",
      "venue": "In your face, robot! the influence of a character's embodiment on how users perceive its emotional expressions"
    },
    {
      "citation_id": "11",
      "title": "An emotion generation model for interactive virtual robots",
      "authors": [
        "W Chao-Gang",
        "Z Jie-Yu",
        "Z Yuan-Yuan"
      ],
      "year": "2008",
      "venue": "2008 International Symposium on Computational Intelligence and Design"
    },
    {
      "citation_id": "12",
      "title": "A tour-guide robot: Moving towards interaction with humans",
      "authors": [
        "B Alvarado Vásquez",
        "F Matía"
      ],
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Generating natural motion in an android by mapping human motion",
      "authors": [
        "D Matsui",
        "T Minato",
        "K Macdorman",
        "H Ishiguro"
      ],
      "year": "2005",
      "venue": "2005 IEEE/RSJ International Conference on Intelligent Robots and Systems"
    },
    {
      "citation_id": "14",
      "title": "Child's recognition of emotions in robot's face and body",
      "authors": [
        "I Cohen",
        "R Looije",
        "M Neerincx"
      ],
      "year": "2011",
      "venue": "2011 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "15",
      "title": "Robots learn social skills: End-to-end learning of co-speech gesture generation for humanoid robots",
      "authors": [
        "Y Yoon",
        "W.-R Ko",
        "M Jang",
        "J Lee",
        "J Kim",
        "G Lee"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Robotics and Automation (ICRA)"
    },
    {
      "citation_id": "16",
      "title": "Generating robotic emotional body language with variational autoencoders",
      "authors": [
        "M Marmpena",
        "A Lim",
        "T Dahl",
        "N Hemion"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "17",
      "title": "Human motions and emotions recognition inspired by lma qualities",
      "authors": [
        "I Ajili",
        "M Mallem",
        "J.-Y Didier"
      ],
      "year": "2018",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "18",
      "title": "Confusion and interest: The role of knowledge emotions in aesthetic experience",
      "authors": [
        "P Silvia"
      ],
      "year": "2010",
      "venue": "Psychology of Aesthetics, Creativity, and the Arts"
    }
  ]
}