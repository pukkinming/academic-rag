{
  "paper_id": "2403.07260v2",
  "title": "Laerc-S: Improving Llm-Based Emotion Recognition In Conversation With Speaker Characteristics",
  "published": "2024-03-12T02:37:11Z",
  "authors": [
    "Yumeng Fu",
    "Junjie Wu",
    "Zhongjie Wang",
    "Meishan Zhang",
    "Lili Shan",
    "Yulin Wu",
    "Bingquan Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition in conversation (ERC), the task of discerning human emotions for each utterance within a conversation, has garnered significant attention in human-computer interaction systems. Previous ERC studies focus on speaker-specific information that predominantly stems from relationships among utterances, which lacks sufficient information around conversations. Recent research in ERC has sought to exploit pre-trained large language models (LLMs) with speaker modelling to comprehend emotional states. Although these methods have achieved encouraging results, the extracted speaker-specific information struggles to indicate emotional dynamics. In this paper, motivated by the fact that speaker characteristics play a crucial role and LLMs have rich world knowledge, we present LaERC-S, a novel framework that stimulates LLMs to explore speaker characteristics involving the mental state and behavior of interlocutors, for accurate emotion predictions. To endow LLMs with this knowledge information, we adopt the two-stage learning to make the models reason speaker characteristics and track the emotion of the speaker in complex conversation scenarios. Extensive experiments on three benchmark datasets demonstrate the superiority of LaERC-S, reaching the new state-of-the-art. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) is a fundamental task in the community of natural language processing (NLP), which targets to automatically identify the emotion of each utterance within a conversation. With the proliferation of conversation data on social media platforms, likewise Twitter and Facebook, detecting human emotions around conversations  (Tu et al., 2022; Gao et al., 2024)  holds promising potential for a series of real-world applications, such as recommendation  (Song et al., 2024)  and opinion mining  (Kumar et al., 2023) . However, unlike sentence-level emotion recognition  (Deng et al., 2023; Zhang et al., 2024) , conversation involves a process of dynamic interactions, which poses a unique challenge for ERC.\n\nFaced with such a challenge, initial attempts to analyze the content of conversation relied on converstaional context modelling  (Sun et al., 2021; Shen et al., 2021b) , while current sophisticated methods  (Song et al., 2022b; Lee and Lee, 2022; Zhang et al., 2023; Wang et al., 2024a)  start the investigation of speaker-specific information to mitigate emotion ambiguity. However, these methods rely on highly structured paradigms, which make the models overfit to specific data distributions, thereby hampering progress in the realm of ERC.\n\nApart from above studies, another strand of re-search resorts to the reasoning and generation capabilities of large language models (LLMs), such as PaLM  (Chowdhery et al., 2023)  and LLaMA2  (Touvron et al., 2023) , for different conversational datasets. A pioneering work by InstructERC  (Lei et al., 2023)  fine-tunes LLaMA2 by introducing speaker identification. Such paradigm gets significant performance compared to conventional pretrained language models (PLMs) in ERC. Subsequently, BiosERC  (Xue et al., 2024)  integrates the biographical information of speakers to intensify LLMs-based ERC systems. As a result, the exploration of speaker characteristics can bring superior performance to their respective models.\n\nDespite the striking results acquired by above works, they are limited by the following dilemmas:\n\n(1) Speaker identification can not provide sufficient information. (2) Speaker biography lacks clues of emotional dynamics in complex conversations. These static information makes the models tend to generate biased responses for all the utterances uttered by a certain speaker. However, as reported in  (Hwang et al., 2021; Zhao et al., 2022) , speaker characteristics including mental state and behavior of interlocutors can provide deep and rich clues of emotional dynamics  (Ghosal et al., 2020) , thereby triggering the target emotion. Thus, it would be beneficial to exploit such speaker characteristics into LLMs for ERC.\n\nIn this paper, we propose LaERC-S, a novel framework devised to exploit large language models and speaker characteristics for the ERC task. Specifically, we design an efficient instruction template to promote LLMs to generate the mental state, behavior and persona of interlocutors around conversations. Afterwards, to supplement LLMs with this knowledge information, we perform two-stage learning, including speaker characteristic injection and emotion recognition, for the final result. A schematic of LaERC-S is depicted in Figure  1 .\n\nWithout bells and whistles, the proposed LaERC-S surpasses all ERC methods on three benchmark datasets, including IEMOCAP  (Busso et al., 2008) , MELD  (Poria et al., 2019), and EmoryNLP (Zahiri and Choi, 2018) . Moreover, LaERC-S provides a unique perspective to capture speaker characteristics in the realm of LLMs-based ERC, which can be reproduced by a sinlge GPU.\n\nIn summary, our contributions are three-fold:\n\n• We propose a simple and effective framework, namely LaERC-S, which explores large lan-guage models and speaker characteristics for emotion recognition in conversation.\n\n• We design an efficient instruction template to promote LLMs to generate speaker characteristics, and adopt a two-stage learning for capturing emotional dynamics and judging emotional states in conversations.\n\n• Experiments are conducted on three public datasets, including IEMOCAP, MELD, and EmoryNLP, which validates the superiority of LaERC-S over the state-of-the-art methods.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition In Conversation",
      "text": "As an indispensable part of human-interaction systems, the nature of emotion recognition in conversation (ERC) refers to make the models comprehend emotion states of interlocutors within conversations, thereby generating empathy and empathic responses  (Majumder et al., 2020) . In the literature, existing ERC studies  (Poria et al., 2017; Majumder et al., 2019; Ghosal et al., 2019; Li et al., 2021 Li et al., , 2023;; Zhao et al., 2022; Zhang et al., 2023; Tu et al., 2023; Jian et al., 2024)  can be roughly divided into two ideas. One relys on pre-trained language models (PLMs) to model conversational context and speaker for emotion prediction. Typically, DialogXL  (Shen et al., 2021a)  introduces an enhanced memory to store conversational contexts, and further captures intra-and inter-speaker dependencies for multi-party structures. CEPT  (Gao et al., 2024)  devises a mixed prompt template and a label mapping strategy for conversational contexts and comprehensive emotions, respectively. With the advancements of pre-trained large language models (LLMs), another line of research attempts to employ LLMs to the task of ERC. Recently, In-structERC  (Lei et al., 2023)  transforms ERC into a retrieved-based Seq2Seq form for LLMs adaptation. BiosERC  (Xue et al., 2024)  leverages speakers' personalities to enhance LLMs. These methods reveal the statement that speaker characteristics are beneficial for emotion recognition in conversation. However, they lack convincing interpretations for acquiring speaker-specific information, thereby limiting emotional expressions. Therefore, in this paper, we attempt to adopt an explainable way to explore large language models and speaker characteristics for the ERC task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speaker Characteristics",
      "text": "Speaker characteristics involve the mental state, behavior and persona of interlocutors in social interaction  (Bosselut et al., 2019; Sap et al., 2019; Hwang et al., 2021) . It is beneficial for a humancomputer interaction system to comprehend the speaker's intention and purpose, as well as analyze situationally-relevant speaker's reaction and behavior. Motivated by such superiority, a series of works employ speaker characteristics to numerous downstream tasks, such as question answering  (Zhang et al., 2022) , empathic response generation  (Sabour et al., 2022) , and emotional gold mining  (Wang et al., 2024b) . In recent years, scholars have paid attention to making progress in ERC by exploring speaker characteristics. These studies leverage conversational relations expressed by a triplet form, to learn the interaction between speakers. Typically, COSMIC  (Ghosal et al., 2020)  exploits a commonsense knowledge base to learn commonsense features for emotion prediction. SKAIG  (Li et al., 2021)  constructs a graph to capture speaker's psychological states. CauAIN  (Zhao et al., 2022)  regards commonsense knowledge as causal clues to trigger the target emotion.\n\nOur method is different from these methods that achieve speaker characteristics from relationships among utterances. In this paper, we extract rich world knowledge from LLMs by devising an efficient template while making the models reason speaker characteristics and track emotional states. This stimulates the proposed LaERC-S to provide more accurate emotion predictions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Laerc-S Framework",
      "text": "In this section, we present a framework, namely LaERC-S, which introduces speaker characteristics for adapting LLMs to emotion recognition in conversation, as shown in Figure  2 . First, we provide the vanilla model in the task of ERC, followed by the specifics of LaERC-S, including speaker characteristic extraction and injection, emotion recognition. Moreover, LaERC-S can also be extended to any of mainstream large language models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Vanilla Erc Model",
      "text": "A conversation data source as D = {(C i , Y i )} N i=1 , where the symbol C i denotes the i-th conversation, and N is the size of D. Each conversation includes a sequence of utterances U = {u j } S j=1 , where the sign S is the number of all utterances. Each utter-ance in a conversation is assigned with a ground truth label y j ∈ {e 1 , e 2 , ..., e K }, where K is the number of emotion categories.\n\nGenerally, the ERC model M based on LLMs is learned from D to provide a response r over a set of the predefined emotion labels E = {e k } K k=1 . The whole process can be expressed as follows:\n\nwhere, u <j denotes the historical utterances before the target utterance u j in the i-th conversation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speaker Characteristic Extraction",
      "text": "To extract high quality speaker characteristics in conversation, we adopt prompt engineering for extraction due to the beneficial of this technology  (Liu et al., 2023; White et al., 2023; Giray, 2023) .\n\nBesides, considering the fact that pre-trained LLMs serve as a rich world knowledge base, we design a template to query LLMs to capture speaker characteristics. Besides, we manually verified speaker characteristics extracted from the large model. We provide the generation procedure of available information regarding speaker characteristics in conversational scenarios, as depicted in Figure  2  (a). Typically, we investigate previous studies  (Sap et al., 2019; Hwang et al., 2021) , and observe that speaker characteristics cover mental state, behavior and persona. Appendix A.4.1 presents the definitions of the information. Mental state reflects emotional states of interlocutors, containing three relations, i.e., 'oReact', 'xReact' and 'xIntent'. Behavior means a response to an event, including  'xNeed', 'xWant', 'oWant', 'xEffect' and 'oEffect'.  Persona indicates the interlocutor's attribute by 'xAttr'.\n\nThese key elements from different perspectives reveal the interaction between utterances, which is intuitively projected into the query template for retrieving available information regarding speaker characteristics. The templates relevant to all the key elements are presented in Appendix A.4.2.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speaker Characteristic Injection",
      "text": "Speaker characteristic injection is to learn clues of emotional dynamics in conversation scenarios, which endows the model with speaker characteristics for subsequent emotion analysis. Although pre-trained large language models cover speakerspecific information, they have not yet been activated the perception capability about this under In the speaker characteristics injection, the generated speaker-characteristics are employed to make the models perceive emotional dynamics. In the emotion analysis, the conversational contents and predefined emotional labels are converted into a formatted input for the final response. As depicted in the instance, LaERC-S bridges the gap between speaker characteristics and the response of \"sad\". conversational contexts. To this end, we adopt a instruction-tuning strategy tailored to endow LLMs with speaker characteristics at the initial stage, as shown in Figure  2 (b) .\n\nTypically, we design an instruction template with a certain key element and basic elements for knowledge analysis. A key element is one of any relationships provided by above preliminary. The basic elements comprises four aspects, i.e., 'title', 'specific token' and 'objective', 'constraint'. The 'title' indicates that the role of LLMs expert apt in learning emotional clues in conversations. The 'specific token' is to separate conversation contents. The 'objective' refers to a concise elucidation of the task of knowledge analysis, which provides a response based on conversation contexts. The 'constraint' is used to limit the length of the response for avoiding hallucinations. For reference, we construct the input template to align with the instruction-following template of information retrieval at preliminary.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition",
      "text": "After the above stage, we achieve an initial model that is available to perceive clues of emotional dynamics in conversations. However, there is a gap between these clues and emotion states. To reach this, we further conduct an instruction-tuning strategy to learn the interplay between emotional tendencies and clues, as depicted in Figure  2 (c) .\n\nTo aligned with the initial stage, we make adjustments in the initial instruction-following template, i.e., title, objective and constraint. Typically, the \"title\" presents the role of LLMs as assistant skilled in sentiment and emotion analysis. The \"objective\" proposes to give a emotional label for the target utterance in a conversation. The \"constraint\" refers to a set of the predefined emotional labels. Such format can maximize the mutual synergy between multiple tasks, while the generated knowledge information does not need to be added into this template without additional computing resources.\n\nOverall, the objective function for various tasks can be defined as follows:\n\nwhere k indicates a certain stage, and x k is the instruction-following template to the certain stage. µ (k,i ′ ) denotes the generated token. In addition, θ k denotes the trainable parameters in LLMs. Finally, after the second stage, the well-trained model is leveraged for inference purposes. We choose 'oReact' item as the final LaERC-S model for emotion analysis in conversation.",
      "page_start": 4,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we successively present three commonly used conversation datasets, compared baselines and basic experimental settings, and then analyze the experimental results in detail.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We evaluate LaERC-S on three representative datasets which involve IEMOCAP  (Busso et al., 2008) , MELD  (Poria et al., 2019), and EmoryNLP (Zahiri and Choi, 2018) . More details about these datasets can be found in Appendix A.2.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "To demonstrate the superiority of LaERC-S in the task of emotion recognition in conversation, we compare LaERC-S with two kinds of mainstream ERC methods as follows. (i) Conventional ERC methods: COSMIC  (Ghosal et al., 2020) , SKAIG  (Li et al., 2021) , DialogXL  (Shen et al., 2021a) , SPCL  (Song et al., 2022a) , CauAIN  (Zhao et al., 2022) , DualGATs  (Zhang et al., 2023) , MKFM  (Tu et al., 2023) , MFAM  (Hou et al., 2023) , and CEPT  (Gao et al., 2024) . (ii) LLMs-based ERC methods: ChatGPT  (Ouyang et al., 2022) , InstructERC  (Lei et al., 2023) , and BiosERC  (Xue et al., 2024) .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "Following current LLMs-based ERC methods  (Lei et al., 2023; Xue et al., 2024) , we adopt the LLaMA2  (Touvron et al., 2023)  as the foundational model in this paper. Consider the expensive training costs and the issue of catastrophic forgeting, we use a lightweight training technique, i.e., LoRA  (Hu et al., 2022) , to stay the model weights frozen and train a small portion of model parameters for specific subtasks. In detail, we set the learning rate to 2e-4, and the converstaional context window to 12 for all evaluation datasets. In the first stage, the batch size is set to 8. In the second stage, the batch size is set to 16. For the hyper-parameter such as epoch, we tune them on the development dataset. The reported results are an average over five random runs. All the experiments are implemented by using PyTorch  (Paszke et al., 2019)   eters analysis of context window can be found in Appendix A.3.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Main Results",
      "text": "To illustrate the effectiveness of LaERC-S framework in the task of ERC, we report the performance of our proposed method and other baseline methods in Table  1 , where 'Avg.' denote the overall average performance on three benchmark datasets. We can observe that our proposed LaERC-S achieves the best results than other all methods on three public datasets. Such performance demonstrates that LaERC-S has stronger generalization and more accurate predictions for emotion recognition. Typically, compared to previous ERC paradigms, LLMs-based ERC methods have achieved significant results than them. The reason is the thorough understanding capability of pre-trained large language models. Notably, our proposed method LaERC-S achieves an improvement of 1.01% over InstructERC, 3.38% over BiosERC on the IEMO-CAP dataset, respectively. For more complex conversation scenarios, such as MELD and EmoryNLP datasets, LaERC-S still provides meaningful gains in performance. This is due to the efficiency of speaker characteristics explored from the key element 'oReact' in the proposed LaERC-S.\n\nBesides, we notice that the results of ChatGPT in zero-shot scenarios are far from other methods that trained with the full dataset. It is attributed to the purpose of universality rather than specific tasks. Therefore, consistent with LaERC-S, it is essential to fine-tune the models for the task of ERC. In summary, the above comparative results present that LaERC-S outperforms all the ERC methods.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section, we demonstrate the superiority of the proposed method LaERC-S from the impact of speaker characteristics. It is to measure the importance of introducing speaker characteristics, and how to sufficiently exploit it in the task of ERC.\n\nThe experimental results are presented in Table  2 , we can achieve the following findings:\n\n• To understand the importance of introducing information around speaker characteristics in conversational scenarios, we present the results of relevant experiments in Table  2 , where the first two rows are the one-stage learning, and the last two rows are the two-stage learning. For reference, in the first row of this table, we eliminate any of speaker characteristics, and solely implement the stage of emotion analysis, presenting a lowest result. Next, we directly incorporate the generated speaker characteristics into the stage of emotion analysis, resulting in the performance improvements in the most of datasets. This highlights the importance of speaker characteristics in ERC. • On the other hand, we adopt the two-stage learning strategy, and regard speaker identification as the initial stage before the stage of emotion analysis. Such method outperforms the first two methods (i.e., one-stage learning), suggesting the efficiency of two-stage learning in the ERC task.\n\nIn the last row of results on all the datasets. These experiments demonstrate that LaERC-S can achieve accurate emotion predictions through introducing speaker characteristics, and use the two-stage learning to magnify the efficiency of speaker characteristics to enhance the model in performance.\n\n5 In-depth Analysis",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Elements Selection",
      "text": "To investigate the influence of different key elements (Key Ele. for short) within the speaker characteristics extraction and injection stage, we design a more detailed experiment by leveraging just one key elements.\n\nTable  3  shows the results, from which we can observe that apart from 'xAttr', others can efficiently bring performance improvements compared to LaERC-S without the initial stage (the first row of Table  2 ). These phenomena can be attributed to the fact that 'xAttr' only reflects the personal attribute, which struggles to capture dynamic emotional clues in conversation scenarios. And conversely, the extracted information from the mental state and behavior can provide richer and deeper dynamic emotional clues for emotion prediction  (Li et al., 2021; Ghosal et al., 2020) .\n\nNotably, in mental state, 'oReact' describes the reaction of listener that refers to the interlocutor of the target utterance in a conversation. It is manifested as dynamic emotional clues provided by the conversational context, capable of revealing emotional states, leading to a significant improvement in performance. Therefore, we choose 'oReact' as the key element in the initial stage.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Models",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Different Llms Impact On Speaker Characteristic Extraction",
      "text": "To demonstrate the expansibility of LaERC-S, we make a comprehensive comparison of the generated speaker characteristics from different large language models with parameters ranging from 7B to 13B, as shown in Table  Specifically , we employ a series of representative LLMs including Mistral-7B  (Jiang et al., 2023) ,  Mixtral-8×7B (Jiang et al., 2024) , Claude-3-Haiku, Llama2 (13B, 7B)  (Touvron et al., 2023) , for evaluation. In the first row of the table, we present the performance of the baseline to intuitively understanding the impact of speaker characteristics in LaERC-S. We can see that various LLMs generate the speaker characteristics that is beneficial to provide the performance improvements of the proposed method. This emphasize the expansibility of the LaERC-S. Moreover, we intuitively think the reason why Llama3-13B performs worse than Llama2-7b is the inconsistency of the adopted models between extraction and injection. The larger-scale language models have not yet provided significant improvements in performance. However, LaERC-S employs Llama2-7B to generate speaker characteristics and further train it for more accurate emotion predictions.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Different Template Impact On Speaker Characteristics Generation",
      "text": "To explore the impact of various templates in performance, we conduct experiments with four different templates (more details about the templates can be found in Appendix A.4.3 ), as presented in Table  5 . We randomly sampled 100 samples from the training set and generated speaker characteristics for each instance using four different templates.\n\nWe manually validated the quality of the speaker characteristics produced for sample to determine which template to select. Among the 100 samples, we discovered that 80% selected Template 4, 8% selected Template 3, 7% selected Template 2, and 5% selected Template 1. Specifically, Although each template solely exists subtle discrepancies, they present different results. For instance, the word \"potential\" in template 4 is removed in template 2, leading to a 0.97% drop in performance, suggesting the importance of the template in LaERC-S. These experiments proves that LLMs are sensitive to templates, which validates that a good template is important in LaERC-S for emotion recognition task. Therefore, we choose the template 4 in LaERC-S to perform more accurate emotion predictions.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Robustness Analysis",
      "text": "To validate the robustness capability of LaERC-S, we conduct a cross-dataset validation experiment.\n\nSpecifically, we first extract data with the same proportion from the training sets of three datasets, and then merge them into a mixed dataset. Subsequently, we train LaERC-S on the mixed dataset and inference on the test sets of the three original datasets. Finally, we demonstrate the generalization of LaERC-S by comparing its weighted-F1 score to that obtained from training and inference both on the original dataset. Notably, in this experiment, we choose InstructERC as our strong baseline due to its outstanding performance compared to other previous ERC models.\n\nThe results are shown in Figure  3 , from which we can observe that LaERC-S is less affected by the cross-dataset validation compared to Instruc-tERC. More specifically, in the dataset IEMOCAP and EmoryNLP, the 'Avg' of the proposed LaERC-S surpasses the baseline method InstructERC by significant improvements of 0.29% and 0.39%, respectively. Even in the more complex conversation dataset MELD, LaERC-S presents a better Figure  3 : The cross-datasets analysis. 'Single' and 'Mixed Ratio' refer to training on a single and mixed dataset, respectively. We sequentially select data from each dataset in the ratios of 1/8, 1/4, 1/2, and 1. 'Avg' represents the average of the differences between 'Single' W-F1 and 'Ratio mix' W-F1. robustness (a performance improvement of 0.41%). These phenomena exhibits the exceptional robustness of our model. More details about robustness analysis can be found in Appendix A.1.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Case Study",
      "text": "In this section, we present two influence to ERC, including, speaker characteristic categories and emo-tional clues in speaker characteristics. The difference between dynamic speaker characteristics and static speaker characteristics.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose LaERC-S, a novel framework that explores speaker characteristics, such as mental state, behavior and persona, to promote the progress of emotion recognition in conversation (ERC). LaERC-S is well-designed with three imperative parts: speaker characteristics extraction, speaker characteristics injection and emotion analysis, all of which work in harmony to make the model reason emotional dynamics and identify emotional tendencies for each utterance in con-versations. Extensive experiments on three public conversation datasets demonstrate the effectiveness and superiority of our proposed LaERC-S.\n\nIn the future work, we would like to delve into the correlation and discrepancy between speaker characteristics in form of diverse expressions. This reason is that the speaker-specific information under different perspectives presents consistent clues of an identical emotion for the utterance. These properties can make the model possess convincing explanations for emotion analysis.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "While LaERC-S has made a significant progress in adapting the LLMs for the task of emotion recognition in conversation, the current work can still be improved in the following ways. Firstly, it is important to find effective ways to maintain an efficient running cost for such large-scale embedding models. Secondly, speaker characteristics around the mental state and behavior of interlocutors have potential to be extended to other tasks in the realm of natural language processing.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Appendix",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A.1 The Details Of Robustness Experiment",
      "text": "In this section, we will introduce how to construct the custom dataset used in § 5.4. Specifically, since the emotional labels in each original dataset are different, we need to map them to a unified label before the extracting and merging, as shown in Table  6 .\n\nA.2 Details of the Datasets IEMOCAP is a dataset collected from improvisations or scripted scenarios, which contains 12 hours of conversation videos from 10 unique speakers. It has five sessions consisting of 151 conversations and 7,433 utterances. Each utterance is annotated with one of six emotion classes: neutral, happy, sad, excited, angry, and frustrated. MELD is another dataset including more than 13,000 video snippets from the Friends TV series. It comprises 1,433 conversations and 13,708 utterances in total. Each utterance is labeled as one of seven emotion categories: anger, disgust, fear, joy, neutral, sadness, and surprise. EmoryNLP is also based on the Friends TV series, which contains 97 episodes, 897 scenes and 12,606 utterances. Each utterances is annotated as one of seven emotion types: neutral, joyful, peaceful, powerful, scared, mad, and sad.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.3 The Context Window Investigation",
      "text": "To examine the impact of the context window in the performance, we conduct a parametric sensitivity analysis with different context window, as depicted in Table  7 . We can notice that LaERC-S achieves the superior performance over InstructERC under any context window settings. This highlights the efficiency of LaERC-S on the task of ERC. For reference, in the first row of the table, LaERC-S provides a 5.01%, 2.79%, and 1.52% improvements over InstructERC on IEMOCAP, MELD, and EmoryNLP, respectively. With the size increasing of the context window, the performance of both methods presents a tendency of improvement. Compared with MELD and EmoryNLP, the models on the dataset IEMOCAP present a significant improvement with the same context window. This is attributed to the average length of conversation in various datasets. The average length of IEMOCAP is longer than that of other datasets, thereby exploiting the larger window providing the necessary historical context for an improvement in performance. Although the performance discrepancy between them gradually decreases, the proposed LaERC-S still achieves significant superiority on three benchmark datasets. Therefore, we set a context window of 12 in LaERC-S to sufficiently capture the historical cotext in a conversation.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.4 Prompts",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A.4.1 Definitions Of Key Elements",
      "text": "We give the definition of key elements in Table  8 . This key elements include nine categories.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A.4.2 Prompts For Key Elements",
      "text": "The key elements are used in template for speaker characteristics extraction and injection. As illustrated in Table  8  and Table  9 , we design the instruction-following templates for speaker characteristic extraction and injection, respectively. These templates provide precise descriptions for basic elements, such as \"title\", \"specific token\", \"objective\" and \"constraint\", to promote LLMs in performing the ERC task. Such a design is essential to guarantee clarity and accuracy in each stage.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.4.3 Details Of Various Templates Design On Speaker Characteristics Extraction",
      "text": "In the different template design shown as Table  10 , we have designed different textual expressions for each key element of speaker characteristics. For example, the key element \"oReact\" can be expressed as \"the reaction of potential listeners\", \"the reaction of listeners\", \"the oReact of listeners \", and \"the reaction of listeners to the event\". We find that we use template with \"the reaction of potential listeners\" word can better extract accurate speaker characteristics.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.5 The Analysis Of Different Emotion Label' S Performance",
      "text": "Compared with InstructERC, our method achieves improvements in most emotion label, and presents sub-optimal performance in rare cases. (1) As for IEMOCAP, our method is superior to InstructERC across all emotion classes. The highest gain is 6.63% on \"Happy\". (2) As for remaining two datasets, our method still maintains consistent improvements, and achieves sub-optimal results on \"Disgust\" due to its few samples (2.6% of the total dataset).   7 : Parameter analysis of the context window in the proposed method LaERC-S on three widely-used benchmark datasets. The symbol ↑ represents an improvement in performance over the compared method InstructERC.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Key Element Description",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Mental",
      "text": "",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison between existing ERC models",
      "page": 1
    },
    {
      "caption": "Figure 1: Without bells and whistles, the proposed LaERC-",
      "page": 2
    },
    {
      "caption": "Figure 2: First, we provide",
      "page": 3
    },
    {
      "caption": "Figure 2: The overview of LaERC-S. LaERC-S includes speaker characteristics extraction and injection, emotion",
      "page": 4
    },
    {
      "caption": "Figure 3: , from which",
      "page": 7
    },
    {
      "caption": "Figure 3: The cross-datasets analysis. ‘Single’ and ‘Mixed Ratio’ refer to training on a single and mixed dataset,",
      "page": 8
    },
    {
      "caption": "Figure 4: The case study of three samples from IEMO-",
      "page": 8
    },
    {
      "caption": "Figure 4: (c) shows the impact of emo-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance comparison between our pro-",
      "data": [
        {
          "Methods": "COSMIC\nSKAIG\nDialogXL\nSPCL\nCauAIN\nDualGATs\nMKFM\nMFAM\nCEPT",
          "IEMOCAP MELD EmoryNLP Avg.": "63.43\n65.03\n38.49\n55.65\n66.96\n65.18\n38.88\n57.01\n66.20\n62.41\n34.73\n54.45\n69.21\n66.13\n40.25\n58.53\n65.01\n64.89\n37.87\n55.92\n67.68\n66.90\n40.29\n58.29\n68.08\n65.50\n39.76\n57.78\n70.16\n66.65\n41.06\n59.29\n70.53\n67.51\n-\n-"
        },
        {
          "Methods": "ChatGPT\nBiosERC\nInstructERC",
          "IEMOCAP MELD EmoryNLP Avg.": "40.07\n54.37\n37.55\n44.00\n69.02\n68.72\n41.44\n59.73\n71.39\n69.15\n41.37\n60.64"
        },
        {
          "Methods": "LaERC\nLaERC-S",
          "IEMOCAP MELD EmoryNLP Avg.": "69.95\n68.86\n40.87\n59.89\n72.40\n69.27\n42.08\n61.25"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: Analysis of different elements in the initial",
      "data": [
        {
          "Key Ele.": "oReact\nxIntent\nxReact",
          "IEMOCAP MELD EmoryNLP Avg.": "72.40\n69.27\n42.08\n61.25\n71.60\n69.56\n41.39\n60.85\n71.14\n69.17\n39.91\n60.07"
        },
        {
          "Key Ele.": "xEffect\noEffect\noWant\nxWant\nxNeed",
          "IEMOCAP MELD EmoryNLP Avg.": "70.70\n68.54\n41.94\n60.39\n71.27\n68.27\n41.64\n60.39\n70.81\n68.87\n43.24\n60.97\n71.24\n68.65\n42.37\n60.75\n71.94\n68.50\n40.27\n60.24"
        },
        {
          "Key Ele.": "xAttr",
          "IEMOCAP MELD EmoryNLP Avg.": "70.08\n67.82\n40.54\n59.48"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Specifically, we Specifically,wefirstextractdatawiththesame",
      "data": [
        {
          "Models": "Baseline",
          "IEMOCAP MELD EmoryNLP Avg.": "69.95\n68.86\n40.87\n59.89"
        },
        {
          "Models": "Mistral-7B\nMixtral-7B\nClaude",
          "IEMOCAP MELD EmoryNLP Avg.": "70.44\n69.15\n41.25\n60.28\n70.86\n69.32\n40.88\n60.35\n70.88\n69.22\n41.77\n60.62"
        },
        {
          "Models": "Llama2-13B\nLlama2-7B",
          "IEMOCAP MELD EmoryNLP Avg.": "70.31\n69.58\n43.19\n61.03\n72.40\n69.27\n42.08\n61.25"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Context\nWindow": "",
          "IEMOCAP": "InstructERC\nLaERC-S",
          "MELD": "InstructERC\nLaERC-S",
          "EmoryNLP": "InstructERC\nLaERC-S"
        },
        {
          "Context\nWindow": "1\n5\n12",
          "IEMOCAP": "56.12\n61.13 (5.01↑)\n68.65\n69.97 (1.32↑)\n71.39\n72.40 (1.01↑)",
          "MELD": "65.91\n68.70 (2.79↑)\n66.97\n69.21 (2.24↑)\n69.15\n69.27 (0.12↑)",
          "EmoryNLP": "38.32\n39.84 (1.52↑)\n40.48\n41.96 (1.48↑)\n41.37\n42.08 (0.71↑)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Key element": "xIntent\nMental-state\nxReact\noReact",
          "Description": "The reason why the speaker would cause the event\nThe reaction that the speaker would have to the event\nThe reaction of listeners to the event"
        },
        {
          "Key element": "xWant\noWant\nEvent\nxEffect\noEffect\nxNeed",
          "Description": "What the speaker may want to do after the event\nWhat the listener may want to do after the event\nThe effect the event would have on the speaker\nThe effect the event has on the listener\nWhat the speaker might need to do before the event"
        },
        {
          "Key element": "Persona\nxAttr",
          "Description": "How the speaker might be described given their part in the event"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Key element": "xIntent",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the intention of <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Expecting explanation\nor clarification"
        },
        {
          "Key element": "xReact",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the reaction of speaker in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Surprised and curious\nabout the news"
        },
        {
          "Key element": "oReact",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the reaction of potential listeners in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Listener looks surprised\nand excited."
        },
        {
          "Key element": "xEffect",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the effect on speaker in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Speaker 0 looks excited\nabout the news"
        },
        {
          "Key element": "oEffect",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer effect of potential listeners in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Expectation arises;\ncurious minds eagerly\nawait details"
        },
        {
          "Key element": "oWant",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the wanted by listeners in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Exciting development or\nsurprise event"
        },
        {
          "Key element": "xAttr",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the attribute of speaker in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Speaker 0 is a\ncurious person"
        },
        {
          "Key element": "xwant",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the wanted by speaker in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Want to know\nthe big news"
        },
        {
          "Key element": "xneed",
          "Prompt": "Now You are an expert who is good at using common sense for reasoning.\nThe following conversation noted between ’### ###’ involves several speakers.\n### Speaker1:\"Okay, so big news.\" Speaker0:\"What?\" ###\nPlease use common sense to infer the need of speaker in <Speaker0:\"What?\" >:",
          "Speaker characteristics": "Expecting important\ninformation or reaction"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "Antoine Bosselut",
        "Hannah Rashkin",
        "Maarten Sap",
        "Chaitanya Malaviya",
        "Asli Celikyilmaz",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2023",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "4",
      "title": "Llms to the moon? reddit market sentiment analysis with large language models",
      "authors": [
        "Xiang Deng",
        "Vasilisa Bashlovkina",
        "Feng Han",
        "Simon Baumgartner",
        "Michael Bendersky"
      ],
      "year": "2023",
      "venue": "Companion Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "5",
      "title": "Cept: A contrast-enhanced prompttuning framework for emotion recognition in conversation",
      "authors": [
        "Qingqing Gao",
        "Jiuxin Cao",
        "Biwei Cao",
        "Xin Guan",
        "Bo Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "6",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "7",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Prompt engineering with chatgpt: a guide for academic writers",
      "authors": [
        "Louie Giray"
      ],
      "year": "2023",
      "venue": "Annals of biomedical engineering"
    },
    {
      "citation_id": "9",
      "title": "Enhancing emotion recognition in conversation via multi-view feature alignment and memorization",
      "authors": [
        "Guiyang Hou",
        "Yongliang Shen",
        "Wenqi Zhang",
        "Wei Xue",
        "Weiming Lu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "10",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Phillip Hu",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "(comet-) atomic 2020: On symbolic and neural commonsense knowledge graphs",
      "authors": [
        "Jena Hwang",
        "Chandra Bhagavatula",
        "Le Ronan",
        "Jeff Bras",
        "Keisuke Da",
        "Antoine Sakaguchi",
        "Yejin Bosselut",
        "Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "12",
      "title": "Emotrans: Emotional transition-based model for emotion recognition in conversation",
      "authors": [
        "Zhongquan Jian",
        "Ante Wang",
        "Jinsong Su",
        "Junfeng Yao",
        "Meihong Wang",
        "Qingqiang Wu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "13",
      "title": "Mistral 7b",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Arthur Sablayrolles",
        "Chris Mensch",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Florian De Las Casas",
        "Gianna Bressand",
        "Guillaume Lengyel",
        "Lucile Lample",
        "Saulnier"
      ],
      "year": "2023",
      "venue": "Mistral 7b",
      "arxiv": "arXiv:2310.06825"
    },
    {
      "citation_id": "14",
      "title": "Mixtral of experts",
      "authors": [
        "Alexandre Albert Q Jiang",
        "Antoine Sablayrolles",
        "Arthur Roux",
        "Blanche Mensch",
        "Chris Savary",
        "Devendra Bamford",
        "Diego Singh Chaplot",
        "Emma De Las Casas",
        "Florian Hanna",
        "Bressand"
      ],
      "year": "2024",
      "venue": "Mixtral of experts",
      "arxiv": "arXiv:2401.04088"
    },
    {
      "citation_id": "15",
      "title": "Explaining (sarcastic) utterances to enhance affect understanding in multimodal dialogues",
      "authors": [
        "Shivani Kumar",
        "Ishani Mondal",
        "Shad Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Compm: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "17",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "18",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "year": "2021",
      "venue": "Findings of the association for computational linguistics: EMNLP 2021"
    },
    {
      "citation_id": "19",
      "title": "Skier: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "Wei Li",
        "Luyao Zhu",
        "Rui Mao",
        "Erik Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu",
        "Zhengbao Jiang",
        "Hiroaki Hayashi",
        "Graham Neubig"
      ],
      "year": "2023",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "21",
      "title": "Mime: Mimicking emotions for empathetic response generation",
      "authors": [
        "Navonil Majumder",
        "Pengfei Hong",
        "Shanshan Peng",
        "Jiankun Lu",
        "Deepanway Ghosal",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "22",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "23",
      "title": "Training language models to follow instructions with human feedback. Advances in neural information processing systems",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Training language models to follow instructions with human feedback. Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "25",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "26",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Cem: Commonsense-aware empathetic response generation",
      "authors": [
        "Sahand Sabour",
        "Chujie Zheng",
        "Minlie Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
      "authors": [
        "Maarten Sap",
        "Le Ronan",
        "Emily Bras",
        "Chandra Allaway",
        "Nicholas Bhagavatula",
        "Hannah Lourie",
        "Brendan Rashkin",
        "Noah Roof",
        "Yejin Smith",
        "Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "29",
      "title": "Dialogxl: All-in-one xlnet for multiparty conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "2021b. Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "31",
      "title": "2022a. Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "32",
      "title": "2022b. Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "Xiaohui Song",
        "Liangjun Zang",
        "Rong Zhang",
        "Songlin Hu",
        "Longtao Huang"
      ],
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "33",
      "title": "Cagk: Collaborative aspect graph enhanced knowledge-based recommendation",
      "authors": [
        "Xiaotong Song",
        "Huiping Lin",
        "Jiatao Zhu",
        "Xinyi Gong"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "34",
      "title": "A discourseaware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Yang Sun",
        "Nan Yu",
        "Guohong Fu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "35",
      "title": "Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "36",
      "title": "An empirical study on multiple knowledge from chatgpt for emotion recognition in conversations",
      "authors": [
        "Geng Tu",
        "Bin Liang",
        "Bing Qin",
        "Kam-Fai Wong",
        "Ruifeng Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "37",
      "title": "Context-and sentiment-aware networks for emotion recognition in conversation",
      "authors": [
        "Geng Tu",
        "Jintao Wen",
        "Cheng Liu",
        "Dazhi Jiang",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Ruifang He, and Yuexian Hou. 2024a. Emotion recognition in conversation via dynamic personality",
      "authors": [
        "Yan Wang",
        "Bo Wang",
        "Yachao Zhao",
        "Dongming Zhao",
        "Xiaojia Jin",
        "Jijun Zhang"
      ],
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "39",
      "title": "Hang Gao, and Renhong Cheng. 2024b. Ecok: Emotional commonsense knowledge graph for mining emotional gold",
      "authors": [
        "Zhunheng Wang",
        "Xiaoyi Liu",
        "Mengting Hu",
        "Rui Ying",
        "Ming Jiang",
        "Jianfeng Wu",
        "Yalan Xie"
      ],
      "venue": "Findings of the Association for Computational Linguistics ACL 2024"
    },
    {
      "citation_id": "40",
      "title": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "authors": [
        "Jules White",
        "Quchen Fu",
        "Sam Hays",
        "Michael Sandborn",
        "Carlos Olea",
        "Henry Gilbert",
        "Ashraf Elnashar",
        "Jesse Spencer-Smith",
        "Douglas Schmidt"
      ],
      "year": "2023",
      "venue": "A prompt pattern catalog to enhance prompt engineering with chatgpt",
      "arxiv": "arXiv:2302.11382"
    },
    {
      "citation_id": "41",
      "title": "Bioserc: Integrating biography speakers supported by llms for erc tasks",
      "authors": [
        "Jieying Xue",
        "Phuong Nguyen",
        "Blake Matheny",
        "Le Nguyen"
      ],
      "year": "2024",
      "venue": "Bioserc: Integrating biography speakers supported by llms for erc tasks",
      "arxiv": "arXiv:2407.04279"
    },
    {
      "citation_id": "42",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "43",
      "title": "Dualgats: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "44",
      "title": "Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "Wenxuan Zhang",
        "Yue Deng",
        "Bing Liu",
        "Sinno Pan",
        "Lidong Bing"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2024"
    },
    {
      "citation_id": "45",
      "title": "Greaselm: Graph reasoning enhanced language models for question answering",
      "authors": [
        "Xikun Zhang",
        "Antoine Bosselut",
        "Michihiro Yasunaga",
        "Hongyu Ren",
        "Percy Liang",
        "Christopher Manning",
        "Jure Leskovec"
      ],
      "year": "2022",
      "venue": "International Conference on Representation Learning (ICLR)"
    },
    {
      "citation_id": "46",
      "title": "Cauain: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Xin Lu"
      ],
      "year": "2022",
      "venue": "International Joint Conference on Artificial Intelligence"
    }
  ]
}