{
  "paper_id": "2302.08921v1",
  "title": "Deep Implicit Distribution Alignment Networks For Cross-Corpus Speech Emotion Recognition",
  "published": "2023-02-17T14:51:37Z",
  "authors": [
    "Yan Zhao",
    "Jincen Wang",
    "Yuan Zong",
    "Wenming Zheng",
    "Hailun Lian",
    "Li Zhao"
  ],
  "keywords": [
    "Cross-corpus speech emotion recognition",
    "speech emotion recognition",
    "deep transfer learning",
    "transfer learning",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we propose a novel deep transfer learning method called deep implicit distribution alignment networks (DIDAN) to deal with cross-corpus speech emotion recognition (SER) problem, in which the labeled training (source) and unlabeled testing (target) speech signals come from different corpora. Specifically, DIDAN first adopts a simple deep regression network consisting of a set of convolutional and fully connected layers to directly regress the source speech spectrums into the emotional labels such that the proposed DIDAN can own the emotion discriminative ability. Then, such ability is transferred to be also applicable to the target speech samples regardless of corpus variance by resorting to a well-designed regularization term called implicit distribution alignment (IDA). Unlike widely-used maximum mean discrepancy (MMD) and its variants, the proposed IDA absorbs the idea of sample reconstruction to implicitly align the distribution gap, which enables DIDAN to learn both emotion discriminative and corpus invariant features from speech spectrums. To evaluate the proposed DIDAN, extensive cross-corpus SER experiments on widely-used speech emotion corpora are carried out. Experimental results show that the proposed DIDAN can outperform lots of recent stateof-the-art methods in coping with the cross-corpus SER tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As a typical task of affective computing and speech signal processing, research of SER seeks to empower the computers to automatically understand the emotional states, e.g., Happy, Fear, and Disgust, from the speech signals. It has constantly been under the spotlight over past several decades  [1, 2]  and lots of promising SER methods have been proposed  [3, 4, 5] . However, it is noted that numerous interference factors, e.g., language gap, speaker difference, and corpus variance between the training and testing speech signals, still hinder the * indicates the corresponding authors. possibility of existing well-performing SER methods to move from the laboratory to the practical scenes. This is because that these interference factors would leads to a feature distribution mismatch between the training and testing speech signals and hence remarkably degrade the performance of most well-performing SER methods. To overcome this shortcoming, in recent years some researchers have drawn their attention to a more challenging but fascinating SER issue, a.k.a., cross-corpus SER  [6] . Different from the conventional SER, the labeled training and unlabeled testing speech signals in cross-corpus SER belong to different speech emotion corpora. We also refer the training and testing samples/corpora/features/signals as the source and target ones, respectively.\n\nThe earliest contribution to cross-corpus SER can be traced to the work of  [6] , in which Schuller et al. proposed a series of feature normalization methods including corpus normalization, speaker normalization, and corpus-speaker normalization to eliminate the corpus difference between the source and target speech signals. Subsequently, several researchers tried to treat cross-corpus SER as a transfer learning task and proposed lots of well-performing transfer subspace learning and deep transfer learning methods. For example, Liu et al.  [7]  proposed a novel transfer subspace learning method called domain-adaptive subspace learning (DoSL) to learn a common subspace to remove the feature distribution mismatch between the source and target speech samples by minimizing their marginal MMD. In the work of  [8] , Zhao et al. proposed a novel deep regression method called deep transductive transfer regression networks (DTTRN), whose major module designed for adapting source and target speech feature distributions are still based on the variant of MMD, i.e., multi-kernel MMD. Besides MMD based methods, adversarial learning is also widely-used in dealing with the cross-corpus SER problem  [9, 10] . Unlike MMD and its variants, these methods are not straightforward ones. This means they align the feature distribution gap using an implicit way, i.e., leveraging a domain (corpus) discriminator to disable the networks to be aware of corpus variance.\n\nInspired by the success of the adversarial learning based arXiv:2302.08921v1 [cs.SD] 17 Feb 2023 The major contribution of the proposed DIDAN is designing a novel IDA term to calibrate the feature distribution gap caused by the speech corpus variance. Moreover, different from both MMD and adversarial learning based methods, our IDA performs corpus invariant feature learning by enforcing the learned target speech features to be sparsely reconstructed by the source ones. Hence, all target samples gradually become more involved in the source speech corpus and eventually share the same or similar feature distribution with the source ones.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we will address the proposed DIDAN for dealing with the cross-corpus SER problem in detail. Suppose we are given a source speech emotion corpus whose samples are denoted by D s = {(X s i , y s i )} Ns i=1 , where X s i and y s i are the i th source speech spectrum and its corresponding one-hot emotion class label, and N s is the source sample number, respectively. Similarly, the unlabeled speech samples from the target corpus can be denoted by D t = {(X t i , y t i )} Nt i=1 , where X t i and N t represent the i th target speech spectrum and the target sample number, respectively. To make the readers better understand the proposed DIDAN, we draw an overall picture shown in Fig.  1  to illustrate the basic idea and network structure. As Fig.  1  shows, our DIDAN has two major parts including Deep Regression and Implicit Distribution Alignment. In what follows, we will describe them in sequence.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Deep Regression",
      "text": "In our DIDAN, we first build a simple deep regression consisting of a set of convolutional and fully connected layers to bridge the source speech spectrums and their corresponding emotion labels to own the emotion discriminative ability. To achieve this goal, we can optimize the deep regression loss as follows:\n\nwhere J(•), f and g denote the cross-entropy loss function, convolution and full connection operations, respectively. It is clear to see that by feeding the source speech samples to the deep regression network and minimizing the above loss function, the DIDAN can gradually be aware of how to distinguish different emotional speech signals.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implicit Distribution Alignment",
      "text": "Subsequently, we design a novel loss function called IDA to enable DIDAN to be also applicable to recognizing the emotions of target speech signals. Specifically, instead of measuring and narrowing the feature distribution gap like MMD based methods, we would like to make each target speech feature learned by DIDAN possibly look like a source one. To this end, together with deep regression loss function, the following regularization term should be also included in optimization of DIDAN, which can be expressed as:\n\nwhere\n\nis a reconstruction coefficient matrix whose i th column corresponds to the i th target speech sample, and α is a trade-off parameter. It is also noted that W 1 = Nt i=1 w i 1 is a L 1 norm with respect to the reconstruction coefficient matrix. By minimizing such norm, DIDAN would produce a sparse w i , which means only a few source samples are needed to reconstruct i th target one.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Total Loss Function",
      "text": "By combining Eqs.(  3 ) and (2), we will arrive at the final total loss function for learning DIDAN, whose corresponding optimization problem is as follows:\n\nwhere θ f and θ g denote the network parameters corresponding to the convolutional operation f and full connection operation g, respectively, and λ is the trade-off parameter balancing the deep regression and IDA losses.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Corpora And Experimental Setup",
      "text": "To evaluate the proposed DIDAN, three public available speech emotion corpora, i.e., EmoDB (B)  [11] , eNTERFACE (E)  [12] , and CASIA (C)  [13] , are employed to design the cross-corpus SER experiments. EmoDB is a German speech emotion corpus consisting of 535 speech samples from 10 speakers in total. These speakers were requested to perform the seven pre-defined emotional contexts including Happy In the task of cross-corpus SER, one speech corpus is served as the source one and the other different corpus as the target one. Therefore, by alternatively choosing either two of the above three speech emotion corpora and meanwhile extracting the speech samples sharing the same emotion labels, we are able to obtain six cross-corpus SER tasks, which can be denoted by B → E, E → B, B → C, C → B, E → C, and C → E, respectively. Note that the right and left corpora of the arrow correspond to the source and target ones. Table  1  summarizes the statistical information of speech samples associated with these six tasks. Moreover, following the pioneer work in cross-corpus SER  [6] , we adopt unweighted average recall (UAR), which is defined as the average of the prediction accuracy per class, to serve as the performance metric.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Comparison Methods And Implementation Detail",
      "text": "In order to evaluate the effectiveness of the proposed DIDAN, several state-of-the-art transfer subspace learning and deep transfer learning methods are chosen to conduct the comparison experiments. The transfer subspace learning methods include transfer component analysis (TCA)  [14] , geodesic flow kernel (GFK)  [15] , subspace alignment (SA)  [16] , domainadaptive subspace learning (DoSL)  [7] , joint distribution adaptive regression (JDAR)  [17]  and joint distribution implicitly aligned subspace learning (JIASL)  [18] . Note that for these subspace learning methods, we use openSMILE toolkit  [19]  to extract IS09 speech feature sets  [20] , which consists of 32 low-level acoustic descriptors and 12 statistical functions, to describe the speech signals in all three speech corpora. In the experiments, the elements in IS09 feature set are normalized between 0 and 1. As for the deep learning ones, deep adaptation network (DAN)  [21] , domainadversarial neural network (DANN)  [22] , and conditional domain adversarial network (CDAN)  [23] , and deep subdomain adaptation network (DSAN)  [24]  are adopted. Unlike the subspace learning methods, the inputs of the deep neural networks are the speech spectrums converted by applying Fourier Transformation to the original speech signals instead of the hand-crafted IS09 feature set.\n\nIn the experiments, we follow existing cross-corpus SER works by searching the hyper-parameters for all the comparison methods from a preset parameter interval to report their best results. Specifically, for TCA, GFK, and SA, we search its reduced feature dimension from [5 : 5 : d max ]. As for DoSL and JDAR, both of them have two trade-off parameters, i.e., λ controlling the distribution alignment term and µ corresponding to sparsity of projection matrix, whose searching interval is set as  [5 : 5 : 200 ]. For all the deep learning methods, VGG-11 is adopted to serve as the backbone and hence the speech spectrums are resized to 224 × 224 pixels. We also include the original VGG-11 in the comparison. We search the trade-off parameters of deep learning methods from the hyper-parameter set {0.1 : 0.1 : 1, 5, 10, 50, 100}. The minibatch stochastic gradient descent strategy is used for learning the optimal parameters of the deep learning methods. The batch sizes of the source and target speech samples are both fixed at 32.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussions",
      "text": "The detailed experimental results are given in Table  2 . Several interesting observations and conclusions can be obtained.\n\n(1) It is clear to see that the proposed DIDAN method achieved the best average UAR reaching 39.8% among all the methods. Moreover, we also observed that our DIDAN outperformed all the comparison methods including subspace learning and deep learning ones in three of all six cross-corpus SER tasks, i.e., E → B (46.0%), B → C (39.1%), and C → B (54.5%). Nevertheless, it can be found that the performance of our DIDAN is actually very competitive against the bestperforming ones in the rest three tasks, i.e., 36.1% (DIDAN) v.s. 36.9% (JIASL) in B → E, 31.9% (DIDAN) v.s. 32.8% (GFK) in E → C, and 30.9% (DIDAN) v.s. 33.2% (JIASL) in C → E. The above observations demonstrated the effectiveness and superior performance of the proposed DIDAN in dealing with the problem of cross-corpus SER.\n\n(2) Overall speaking, the deep learning methods showed more promising performance in dealing with cross-corpus SER tasks than the subspace learning ones, which can be seen from the comparison between their average UAR. Despite of this, it is noticed that some subspace learning methods can still obtain more satisfactory results compared with the deep learning ones when coping with several cross-corpus SER tasks, e.g., JIASL (36.9%) in B → E and GFK (32.8%) in E → C. This may be because that the hand-crafted speech feature set used in subspace learning methods, i.e., IS09, is more discriminative and corpus invariant than the deep  features directly learned from the speech spectrums by the backbone (VGG-11) used in deep learning ones for these tasks. By further comparing the results of all the methods in these tasks, it is clear that the deep learning methods mostly performed poorer compared with the subspace learning ones, which supports our explanations and analysis.\n\n(3) It can be observed that nearly all the methods cannot well cope with the cross-corpus SER tasks between eNTER-FACE and CASIA. We believe that this may attribute to the large differences between these two speech corpora. According to the works of  [12, 13] , it is well known that eNTER-FACE is an English speech corpus whose samples are elicited by well-designed induced paradigm, while CASIA is a Chinese one, in which all speakers are requested to simulate different emotions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Going Deeper Into Ida Of Didan",
      "text": "As described previously, one of the major contributions in our DIDAN is the IDA loss, which aims to improve the robustness of DIDAN to the corpus variance by enforcing the target speech features to be sparsely reconstructed by the learned source ones. To see whether IDA indeed works, we select three cross-corpus SER tasks, i.e., B → E, E → B, and B → C, as representatives to conduct additional experiments. Besides the original DIDAN (denoted by DIDAN w IDA), another four methods are chosen including VGG-11 (DIDAN w/o IDA), DIDAN without sparsity regularization term (denoted by DIDAN w nonSR-IDA), DAN (MMD), and DANN (Adversarial Learning). Experimental results are depicted in Table  3 . From the results in first three rows of Table  3 , it can be concluded that the proposed implicit distribution alignment method can remarkably improve the corpus invariant ability of the deep regression model. In addition, we can also observe that our DIDAN outperformed DAN and DANN, which adopt MMD and GAN to align the feature distribution gap, respectively. This verifies the superiority of the proposed IDA in DIDAN over these two widely-used strategies for distribution alignment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have presented a novel deep transfer learning method called DIDAN for dealing with the problem of crosscorpus SER. Unlike most of existing deep learning methods, our DIDAN removes the feature distribution mismatch between the source and target speech signals with an implicit manner, i.e., enforcing the target deep features to be sparsely reconstructed by the source ones. Hence, the deep regression model only supervised by the source label information would be able to effectively recognize the emotions of unlabeled target speech signals. Extensive experiments were carried out on three widely-used speech emotion corpora to evaluate the performance of the proposed DIDAN. The results showed that compared with recent state-of-the-art transfer subspace learning and deep transfer learning methods, our DIDAN has more promising performance in dealing with cross-corpus SER tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview Structure and Basic Idea of the Proposed",
      "page": 2
    },
    {
      "caption": "Figure 1: to illustrate the basic idea and network",
      "page": 2
    },
    {
      "caption": "Figure 1: shows, our DIDAN has two major parts",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: (1) It is clear to see that the proposed DIDAN method",
      "data": [
        {
          "B → E\nE → B": "B → C\nC → B",
          "B (AN: 127, SA: 62, FE: 69, HA: 71, DI: 46)\nE (AN: 211, SA: 211, FE: 211, HA: 208, DI: 211)": "B (AN: 127, SA: 62, FE: 69, HA: 71, NE: 79)\nC (AN: 200, SA: 200, FE: 200, HA: 200, NE: 200)",
          "375\n1052": "408\n1000"
        },
        {
          "B → E\nE → B": "E → C\nC → E",
          "B (AN: 127, SA: 62, FE: 69, HA: 71, DI: 46)\nE (AN: 211, SA: 211, FE: 211, HA: 208, DI: 211)": "E (AN: 211, SA: 211, FE: 211, HA: 208, SU: 211)\nC (AN: 200, SA: 200, FE: 200, HA: 200, SU: 200)",
          "375\n1052": "1052\n1000"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: From the results in first three rows of Table 3, it",
      "data": [
        {
          "Subspace Learning": "Deep Learning",
          "SVM\nTCA\nGFK\nSA\nDoSL\nJDAR\nJIASL": "VGG-11\nDAN\nJAN\nDANN\nCDAN\nDSAN",
          "28.9\n23.6\n29.6\n35.0\n26.1\n25.1\n30.5\n44.0\n33.4\n45.1\n31.1\n32.3\n32.8\n32.1\n42.5\n33.1\n48.1\n28.1\n33.5\n43.9\n35.8\n44.0\n32.6\n28.2\n36.1\n39.0\n34.4\n45.8\n30.4\n31.6\n36.3\n40.0\n31.1\n46.3\n32.4\n31.5\n36.9\n33.2\n44.1\n36.5\n49.3\n30.5": "32.8\n38.8\n36.4\n50.0\n27.1\n30.0\n35.2\n39.2\n36.7\n51.6\n28.5\n32.5\n34.9\n39.6\n37.4\n52.1\n27.9\n29.8\n35.0\n43.6\n37.6\n52.3\n28.9\n30.0\n32.9\n40.9\n37.9\n49.5\n30.7\n30.5\n35.6\n44.0\n38.5\n53.4\n30.3\n31.7",
          "28.1\n36.1\n36.1\n36.3\n36.2\n36.3\n38.4": "35.9\n37.3\n37.0\n37.9\n37.1\n38.9"
        },
        {
          "Subspace Learning": "",
          "SVM\nTCA\nGFK\nSA\nDoSL\nJDAR\nJIASL": "DIDAN (Ours)",
          "28.9\n23.6\n29.6\n35.0\n26.1\n25.1\n30.5\n44.0\n33.4\n45.1\n31.1\n32.3\n32.8\n32.1\n42.5\n33.1\n48.1\n28.1\n33.5\n43.9\n35.8\n44.0\n32.6\n28.2\n36.1\n39.0\n34.4\n45.8\n30.4\n31.6\n36.3\n40.0\n31.1\n46.3\n32.4\n31.5\n36.9\n33.2\n44.1\n36.5\n49.3\n30.5": "46.0\n39.1\n54.5\n36.1\n31.9\n30.9",
          "28.1\n36.1\n36.1\n36.3\n36.2\n36.3\n38.4": "39.8"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "4",
      "title": "Double sparse learning model for speech emotion recognition",
      "authors": [
        "Yuan Zong",
        "Wenming Zheng",
        "Zhen Cui",
        "Qiang Li"
      ],
      "year": "2016",
      "venue": "Electronics Letters"
    },
    {
      "citation_id": "5",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "Cheng Lu",
        "Yuan Zong",
        "Wenming Zheng",
        "Yang Li",
        "Chuangao Tang",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "Bjorn Schuller",
        "Bogdan Vlasenko",
        "Florian Eyben",
        "Martin Wöllmer",
        "Andre Stuhlsatz",
        "Andreas Wendemuth",
        "Gerhard Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised cross-corpus speech emotion recognition using domain-adaptive subspace learning",
      "authors": [
        "Na Liu",
        "Yuan Zong",
        "Baofeng Zhang",
        "Li Liu",
        "Jie Chen",
        "Guoying Zhao",
        "Junchao Zhu"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "9",
      "title": "Deep transductive transfer regression network for cross-corpus speech emotion recognition",
      "authors": [
        "Yan Zhao",
        "Jincen Wang",
        "Ru Ye",
        "Yuan Zong",
        "Wenming Zheng",
        "Li Zhao"
      ],
      "year": "2022",
      "venue": "Deep transductive transfer regression network for cross-corpus speech emotion recognition"
    },
    {
      "citation_id": "10",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised cross-corpus speech emotion recognition using a multi-source cycle-gan",
      "authors": [
        "Bo-Hao Su",
        "Chi-Chun Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "M Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "13",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "Olivier Martin",
        "Irene Kotsia",
        "Benoît Macq",
        "Ioannis Pitas"
      ],
      "year": "2006",
      "venue": "ICDE Workshops"
    },
    {
      "citation_id": "14",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "Jianhua Tao",
        "Fangzhou Liu",
        "Meng Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge"
    },
    {
      "citation_id": "15",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "Ivor Sinno Jialin Pan",
        "James Tsang",
        "Qiang Kwok",
        "Yang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "16",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "Boqing Gong",
        "Yuan Shi",
        "Fei Sha",
        "Kristen Grauman"
      ],
      "year": "2012",
      "venue": "CVPR"
    },
    {
      "citation_id": "17",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "Basura Fernando",
        "Amaury Habrard",
        "Marc Sebban",
        "Tinne Tuytelaars"
      ],
      "year": "2013",
      "venue": "ICCV"
    },
    {
      "citation_id": "18",
      "title": "Cross-corpus speech emotion recognition using joint distribution adaptive regression",
      "authors": [
        "Jiacheng Zhang",
        "Lin Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Li Zhao"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Implicitly aligning joint distributions for cross-corpus speech emotion recognition",
      "authors": [
        "Cheng Lu",
        "Yuan Zong",
        "Chuangao Tang",
        "Hailun Lian",
        "Hongli Chang",
        "Jie Zhu",
        "Sunan Li",
        "Yan Zhao"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "20",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "21",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "Björn Schuller",
        "Stefan Steidl",
        "Anton Batliner"
      ],
      "year": "2009",
      "venue": "The interspeech 2009 emotion challenge"
    },
    {
      "citation_id": "22",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "Mingsheng Long",
        "Yue Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2015",
      "venue": "ICML"
    },
    {
      "citation_id": "23",
      "title": "Domain-adversarial neural networks",
      "authors": [
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "Mario Franc ¸ois Laviolette",
        "Marchand"
      ],
      "year": "2014",
      "venue": "Domain-adversarial neural networks",
      "arxiv": "arXiv:1412.4446"
    },
    {
      "citation_id": "24",
      "title": "Conditional adversarial domain adaptation",
      "authors": [
        "Mingsheng Long",
        "Zhangjie Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2018",
      "venue": "NIPS"
    },
    {
      "citation_id": "25",
      "title": "Deep subdomain adaptation network for image classification",
      "authors": [
        "Yongchun Zhu",
        "Fuzhen Zhuang",
        "Jindong Wang",
        "Guolin Ke",
        "Jingwu Chen",
        "Jiang Bian",
        "Hui Xiong",
        "Qing He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    }
  ]
}