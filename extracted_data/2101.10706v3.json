{
  "paper_id": "2101.10706v3",
  "title": "The Pixels And Sounds Of Emotion: General-Purpose Representations Of Arousal In Games",
  "published": "2021-01-26T11:00:44Z",
  "authors": [
    "Konstantinos Makantasis",
    "Antonios Liapis",
    "Georgios N. Yannakakis"
  ],
  "keywords": [
    "General-purpose representation",
    "subject-agnostic",
    "arousal modelling",
    "pixels",
    "audio",
    "games",
    "CNN",
    "classification",
    "preference learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "What if emotion could be captured in a general and subject-agnostic fashion? Is it possible, for instance, to design general-purpose representations that detect affect solely from the pixels and audio of a human-computer interaction video? In this paper we address the above questions by evaluating the capacity of deep learned representations to predict affect by relying only on audiovisual information of videos. We assume that the pixels and audio of an interactive session embed the necessary information required to detect affect. We test our hypothesis in the domain of digital games and evaluate the degree to which deep classifiers and deep preference learning algorithms can learn to predict the arousal of players based only on the video footage of their gameplay. Our results from four dissimilar games suggest that general-purpose representations can be built across games as the arousal models obtain average accuracies as high as 85% using the challenging leave-one-video-out cross-validation scheme. The dissimilar audiovisual characteristics of the tested games showcase the strengths and limitations of the proposed method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "D ESIGNING autonomous agents capable of performing equally well across different tasks is a long term vision of artificial intelligence (AI)  [1] . Towards realizing such a vision, the recent groundbreaking study of Minh et al.  [2]  introduces the idea of general-purpose deep-learned representations for controlling agents capable of performing well across different tasks. These agents, in particular, managed to achieve superhuman performance in playing 2D Atari games by merely observing the pixels of the screen. As impressive as such a result might be, the derived agents are restricted to act in a particular set of deterministic environments and achieve a clearly-and objectively-defined goal: to maximize their score. Arguably, however, several of the most interesting problems that AI is requested to solvesuch as emotion recognition and artificial psychology-have ill-posed and subjectively-defined target functions under non-deterministic contexts.\n\nInspired by the core principles of Mnih et al.  [2] , in this paper we transfer and introduce the idea of generalpurpose representations to the field of affective computing. We thus reframe the user-specific way in which affective detection normally operates and, instead, we investigate the degree to which general-purpose representations can learn to predict emotion. As videos of interaction capture a user's behavior, we base our investigations on the assumption that the audiovisual information contained in such a video can • K. Makantasis, A. Liapis and G. N. Yannakakis are with the Institute of Digital Games, University of Malta, Msida 2080, Malta (e-mail: konstantinos.makantasis@um.edu.mt, antonios.liapis@um.edu.mt, georgios.yannakakis@um.edu.mt).\n\nManuscript received Month Day, Year. hold information about both the interaction context and the elicited affective patterns, and thus it can be a predictor of the user's experience. Our key hypothesis is that we can construct accurate models of affect based only on the audiovisual information of videos of interaction; as in  [2] , we test this hypothesis in the domain of games. In particular we attempt to predict a game's arousal level relying solely on the audiovisual information of game footage. Games provide complex yet well-defined environments, which are designed to elicit increased levels of player engagement and motivation  [3] ,  [4] . Players, during their interaction with games, produce gameplay footage that has the unique property of overlaying the game context onto aspects of playing behavior and affect; this suggests that players' affect is embedded in the gameplay context. That embedding, in turn, renders the explicit fusion of context with affect manifestation unnecessary-a dominant practice within affective computing  [5] ,  [6] ,  [7] ,  [8] . Although we focus on the games domain, our approach is general and potentially applicable to different human-computer interaction domains, since it relies on raw audiovisual information. Such information fuses the interaction context with the affect of the user as manifested during the interaction.\n\nGiven the bimodal (audio and visuals) nature of the affect modeling task, we use a two-stream deep neural network-both as a classifier and as a preference learnerthat considers audiovisual gameplay footage information and predicts the player's annotated arousal. The first stream is a Convolutional Neural Network (CNN) that processes visual information as pixels of video frame sequences. The second stream is a fully connected network that processes the game audio information of the considered sequence of video frames. Via late fusion, we propagate the audiovisual information to a fully connected network that predicts arousal. We test the methodology across four dissimilar 3D games and their corresponding gameplay footage (see Fig.  1 ). All gameplay videos have been annotated by the players themselves (first-person) in terms of arousal using the RankTrace  [9]  continuous annotation tool. Our experimental evaluation validates our hypothesis in most games and suggests that we can derive highly accurate models of affect using general-purpose representations that rely solely on audiovisual information of the interaction. In particular, the two representations (deep classifier and preference learner) predict arousal for three of the games examined with an average classification accuracy that reaches between 82% and 83% on average using the demanding leave-onevideo-out cross-validation scheme. The under-performing models in one of the games lead to an insightful discussion regarding the limitations of the proposed approach and the environments it is best suited for.\n\nOur work is novel in several ways. First, we derive accurate models of affect in different games without relying on any direct manifestation of emotion or modality of user input. Instead, our work is one of the first approaches towards modeling players' affect through general-purpose representations of information embedded solely in the context of interaction. Our methodology, thereby, yields affect models that are general and user-agnostic. Second, to the best of our knowledge, this study is the first attempt to derive a function that maps directly audiovisual gameplay information-such as pixels and audio features-to game experience across different games. Finally, via the employed two-stream deep network, we investigate the degree to which each modality, as well as their fusion, can be used as a predictor for such a mapping in affective computing. The paper builds upon and significantly extends the preliminary results of Makantasis et al.  [10] , which map the visual information of gameplay footage to players' arousal in one game. Specifically, in this paper we explore two different modalities of the game footage: besides the visual information we also exploit audio information in an attempt to yield more accurate representations of arousal. Moreover, we approach the arousal modeling problem using two different learning paradigms-a binary classification and a preference learning task-and we compare their performance quantitatively and their top-performing arousal models qualitatively. Finally, we test the generality of the proposed methodology across four heterogeneous games with regards to both the audiovisual information they offer to the deep learner and the arousal patterns they elicit.\n\nThe remainder of the paper is organized as follows. Section 2 reviews related work regarding affect modeling in games and videos. Section 3 describes the games, the employed datasets and the data pre-processing approach we followed. Section 4 lists the key elements of our methodology including the architectures of the learning models for both binary classification and preference learning. In Section 5 we experimentally validate and analyse our models across the four games. Finally, Section 6 discusses our main findings and Section 7 concludes this study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "This section surveys key literature on affect modelling relevant to the proposed approach of mapping audiovisual data from gameplay videos for predicting affect.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Models Of Affect Based On Audiovisual Information",
      "text": "Audiovisual information has been at the core of interest for both eliciting and modeling emotions in affective computing  [11] ,  [12] ,  [13] . Typically, videos feature the face or the body of one or more humans and their emotions are modeled via non-verbal (visual and vocal) cues  [14] ,  [15]  due to theoretical frameworks and evidence supporting that such modalities can convey emotion  [14] ,  [15] ,  [16] ,  [17] ,  [18] . Visual information is related to the dynamic patterns of human face(s) modeled via facial cues  [19] ,  [20] , body postures  [21] ,  [22] , gestures  [23]  or gait  [24] ,  [25] . Vocal information relies on audio signals which are used to construct acoustic and voice quality cues based on the pitch, the energy, the frequency and the spectrum of the signal  [26] ,  [27] .\n\nA number of earlier studies base the construction of affect models on ad-hoc features of an image. Indicatively, Liu et al.  [28]  combined traditional hand-crafted image features such as SIFT  [29]  and Histogram of Oriented Gradients  [30]  as inputs to machine learning models for emotion recognition in the wild. Yao et al.  [31]  hand-crafted image features based on Local Binary Patterns  [32]  for facial image emotion recognition. Recent advances in deep learning, however, enable the automatic construction of features via convolutional neural networks (CNNs); CNNs were first applied in  [33]  to predict dimensional affective scores from videos, but the small scale datasets challenged the training of deep models of affect. The need for effectively training CNNs triggered the development of medium-and largescale affect datasets such as the Celebrity Face in the Wild  [34] , the Facial Expression Recognition 2013 Dataset  [35]  and the Aff-Wild database  [36] . Based on these datasets, Ng et al.  [37]  used transfer learning and CNNs for emotion recognition through visual cues and Kollias et al.  [38]  combined CNNs with recurrent neural networks to model arousal and valence. Finally, in  [5]  facial expressions were fused with videos of advertisements for predicting whether viewers liked the videos or not.\n\nRegarding emotion recognition via audio data, Eyben et al.  [39]  conducted a detailed study on audio emotion features. The authors constructed GeMAPS, a concise feature set with 62 audio features. Recent studies show that fusing audio and visual information results in more accurate models than those of a single modality  [40] . In  [41]  energy and spectral audio features are fused with visual information for emotion prediction in short video clips, while in  [42]  audiovisual data is used to train a deep neural network for recognizing affect in real-world environments.\n\nThe approach presented in this paper can be seen as unconventional for modeling affect. Following our preliminary study  [10]  on general-purpose pixel-based models of affect, in the current study we use audiovisual information of human-computer interaction as the sole input for modeling the affect of the human across different tasks (i.e. games). The role of the audiovisual interaction footage is thus twofold: the audiovisual information contained in the footage is used to model affect as the context that both elicits and manifests emotion without the need of other external manifestations of affect. The proposed approach is a general method for modeling affect solely via videos containing sound and do not contain either facial/bodily expressions or vocal cues of humans. The experimental validation of the proposed approach-at least within the games domainsuggests that this subject-agnostic perspective is not only possible, but it also yields highly accurate models of affect in games with particular audiovisual characteristics.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Affect Modeling In Games",
      "text": "Affect modeling within the domain of games refers to modeling the behavior and the affective responses of players  [4] ,  [43] . A player model receives as input a modality (or a set of modalities) regarding the player, such as gameplay data and/or physiological measurements, and attempts to predict aspects of the in-game behavior or the player experience. Indicatively Pedersen et al.  [44]  combined gameplay data (e.g., number of deaths) with game level features to predict players' reported affect using Super Mario Bros (Nintendo 1985) as a testbed. Shaker et al.  [45]  used the same testbed to predict players' affect based on players' posture during gameplay. Recently, Melhart et al.  [46]  managed to successfully model the moment-to-moment engagement level of PUBG (PUBG Corporation, 2017) streamed videos by considering the chatting behavior of its viewers. Martinez et al. used various deep learning methodologies to capture player experience via gameplay metrics and physiology  [47] ,  [48] . Finally, Camilleri et al.  [49]  attempted to create arousal models that are general across different games relying solely on gameplay metrics.\n\nThis study advances the state of the art in player modeling as the proposed model of affect is based solely on the audiovisual information contained in gameplay footage.\n\nThe majority of studies that analyze and extract information from gameplay footage rely on contextual information about the game such as structural and game level elements, physics and mechanics of the game (e.g.  [45] ,  [50] ). Moreover, the most common approaches for analyzing player experience heavily rely on direct measurements from players under well-defined experimental settings; the modalities that are usually considered include facial expression and head pose  [45] , speech  [51]  and physiology  [47] ,  [52] .\n\nBuilding upon and significantly extending the preliminary study of Makantasis et al.  [10] , our methodology models players' experience without any a priori contextual knowledge about the game. Instead, it uses general-purpose deep learned representations of gameplay footage (i.e. pixels and audio files) as it ignores functional aspects of the game per se. As a result, our approach does not require any direct in-game feature or manifestation of affect (e.g. via physiology, speech, or facial expression), it is not intrusive, and it allows the rapid collection and processing of vast amounts of data. As gameplay videos are largely available online in massive quantities-e.g. via service such as Twitch 1 and Mixer 2 -the proposed approach is potentially generalizable to any game with available audiovisual content.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Video Affective Content Analysis",
      "text": "Video affective content analysis has been an active research area focusing on classifying and retrieving videos based on their affective content. While conventional content-based video analysis relies on generic semantic content, video affective content analysis tries to identify videos that elicit certain emotions in their viewers  [53] . Recent research adopts either direct or implicit approaches. Direct approaches infer the affective content of videos directly from the related audiovisual features, while implicit approaches detect affective content from videos based on an automatic analysis of a user's spontaneous response while consuming the video  [54] . Below we discuss direct approaches since they are closely related to the present study.\n\nHanjalic and Xu  [55]  proposed one of the first direct approaches for video affective content analysis, using handcrafted features of audio and visual information of video segments to model arousal and valence. Since then, extracting audiovisual features and exploiting machine learning methods to model emotion is the most common practice in video affective content analysis  [56] ,  [57] ,  [58] ,  [59] . More recent work takes advantage of deep learning to automatically generate deep features to describe audiovisual information, such as features of motion and scene cues  [60] . Wang et al.  [61]  use a generative adversarial network to classify emotion of videos, while Mitenkova et al.  [62]  use the output of a pretrained network on face images  [63]  as input to a tensor regression layer for prediction arousal and valence levels. Zhu et al.  [64]  propose a multimodal deep quality embedding network and a deep learning affective classifier to efficiently process noisy affect data.\n\nAlthough our study relates to video affective content analysis studies, it is conceptually different. Video affective content analysis tries to model and predict the emotion elicited by a video to a viewer. In contrast, this paper focuses not on the content creator's side, as we aim to model the emotional state of a player while they are playing the game.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Data Processing",
      "text": "To test the performance and the generality of the proposed approach we used frames and sound from four dissimilar 1. https://www.twitch.tv/ 2. https://mixer.com/microsoft games: Survival Shooter, Maze Ball, Solid Rally and Sonancia. Figure  1  shows a screenshot of each game. In this section, we describe the games, the datasets obtained from these games and the data cleaning process we followed. Participants were recruited via snowball sampling and were primarily university students who are casual gamers and/or follow courses in game design and ICT, with no prior experience in affect annotation. A different set of participants was used for Solid Rally and Sonancia whereas the same set of participants was used for Survival Shooter and Maze Ball  [49] . Prior to annotation, all participants were presented with an introductory screen that describes arousal as \"the intensity of gameplay no matter whether you like the game or not. High arousal can be a feeling of readiness, tension, excitement or exhilaration. Low arousal can be a feeling of fatigue, boredom, calmness or relaxation\".",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Testbed Games",
      "text": "To test how general-purpose input representations can be used for modelling affect, we selected the four games due to their dissimilarities. The games belong to different genres, with different mechanics, camera perspective, pace, visual and audio design. Specifically, Survival Shooter is a fastpaced shooter game that requires accurate aiming and constant movement. Maze Ball is a slow-paced physics game that needs accurate timing of movement. Sonancia is a horror game that elicits negative emotions and disorientation. Finally, Solid Rally is a fast-paced racing game that simulates a multi-player experience with AI drivers. Moreover, the camera perspective is top-down in Survival Shooter, third-person in Maze Ball and first-person in Sonancia and Solid Rally. The dissimilarities between the four games make them ideal for testing the degree to which accurate models of affect can be based on general-purpose input representations. We should also highlight that different sets of players played and annotated three of the four games.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Survival Shooter",
      "text": "Survival Shooter (SS)  [49]  tasks the player to shoot down as many hostile toys as possible while avoiding collisions with them. Hostile toys spawn at predetermined areas of the level and move towards the player's avatar. The avatar is equipped with a laser gun, which can kill a toy with a few shots. Every toy killed adds to the player's score. Background music plays throughout the gameplay of SS; while the player is firing the laser gun, the volume of music lowers, and the dominant sound is the weapon sound. Sound effects play when the avatar collides with hostile toys, when a toy is killed, and when the player runs out of life. The duration of the gameplay is 60 seconds.\n\nThe SS data used in this study was collected from 25 different players (10 females) aged from 19 to 54 (median age 24). Most players considered themselves good or expert players (70%) while the rest considered themselves novice or non-gamers. Each player played the game and then annotated her recorded gameplay footage in terms of arousal; this play-annotation cycle occurred twice. The first-person annotation process was carried out using the RankTrace tool  [9] ,  [65]  which allows the continuous and unbounded annotation of arousal using the Griffin PowerMate wheel interface. Gameplay footage was recorded at 30 frames per second (30Hz), while RankTrace provided 4 annotation samples per second (4Hz).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Maze Ball",
      "text": "Maze Ball (MB)  [66]  (or Space Maze  [67] ) is a 3D game that served as testbed in multiple studies investigating affect detection in games  [4] ,  [47] ,  [48] ,  [49] ,  [66] . The player controls a cyan ball in a maze which contains dark ballshaped enemies and three diamond-shaped tokens of different colors. The player has to avoid colliding with the enemies patrolling the maze, collect all the tokens and move the ball to a predefined goal point (only shown after all three tokens are collected) within 90 seconds. Each collected token adds to the player's score. The game ends either when the player runs out of time or collides with enemies twice. Background music plays during the entirety of gameplay, and sound effects play when the player obtains a token and when the player collides with an enemy.\n\nA total of 25 players provided data for the MB dataset (the same set of players as in SS)  [49] . Similarly to SS, each player conducted a play-annotation cycle twice, using RankTrace and the Griffin PowerMate wheel for annotation. MB game footage was also recorded at 30Hz.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Solid Rally",
      "text": "Solid Rally (SR) tasks the player to drive their car through a closed circuit for two laps. In each race, the player competes against three opponents, and the goal is to finish the race on the highest possible position. Within the track, there are several checkpoints at predetermined locations: passing through a checkpoint adds to the player's score. The car engine makes sounds throughout the gameplay of Solid Rally, and a sound effect plays during car crashes. The game ends either after two laps or after 90 seconds of playing.\n\nSR data was collected from 17 players (7 females) aged from 23 to 55 (median age 32); almost half of them (47%) were novice or non-gamers, 35% considered themselves expert players and the rest played games only occasionally. Each player conducted a play-annotation cycle twice using the RankTrace annotation tool provided by the PAGAN framework  [65] . Game footage was recorded at 60Hz but downsampled to 30Hz to match the sampling rate of the other three games.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sonancia",
      "text": "Sonancia (SON)  [68]  is a horror game taking place in a haunted dungeon divided into rooms. The players' task is to find the old statue while avoiding and outrunning monsters. The level is procedurally generated, including the number of rooms, the positioning of lights and monsters. Background audio plays throughout the game, and changes based on the room the player is in. The only sound effect is a low-volume growl when a monster sees the player.\n\nSON data was collected from 14 players (5 females) aged from 25 to 34; 36% of them played games everyday, 45% played frequently or casually while the rest rarely on never played. Each participant performed a play-annotation cycle twice, using RankTrace and the Griffin PowerMate wheel for annotation. Gameplay footage duration varies from 31 to 173 seconds, and is recorded at 30Hz.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Cleaning",
      "text": "For all datasets we omit short gameplay videos with a duration under 15 seconds, in order to maintain an appropriate balance between sufficient gameplay and a player's cognitive load. This rule yields 43 videos for SS (7 short videos were omitted), 50 videos for MB, 34 videos for SR and 28 videos for SON (no video was omitted).\n\nSince our approach is based on statistical machine learning, we explicitly assume that gameplay frames and sounds can characterize a player's arousal through an unknown mapping that machine learning aims to discover. To preserve the soundness of this assumption, we identified and omitted outlier videos whose annotations are not consistent with the gameplay. For all games, all players play the same level, which has a specific structure, e.g. for the SS game the toys keep spawning at predetermined areas and time instances. Coupled with the fact that the duration of each session is relatively short (60, 90, 90 and maximum 173 seconds for SS, MB, SR and SON respectively), the possible states of the gameplay are restricted. Based on this observation, we assume that arousal annotation traces should, on average, exhibit a specific pattern and we can thus omit outlier videos that deviate substantially from this pattern.\n\nIn particular, we denote a playthrough as an outlier if its annotation trace is dissimilar to an annotation trace that can be considered representative for the whole dataset. Since RankTrace provides continuous and unbounded arousal annotations, initially we normalise all annotation traces to [0, 1]. Then we consider the median of all annotation traces as the representative annotation trace for the whole dataset and compute the distance between the annotation trace of every gameplay footage and the representative trace using the Dynamic Time Warping (DTW)  [69]  algorithm. DTW is widely used for measuring the similarity between two timeseries that may vary in length. The distribution of distances for each game indicates that the density is mainly concentrated on one cluster. Based on that, we omit outliers above a DTW distance threshold. For the SS game we omit videos where the distance to the median (representative) annotation trace is larger than 0.135; for MB, SR and SON the corresponding thresholds are 0.195, 0.4 and 0.2. After removing outliers, the SS, MB, SR and SON datasets contain, respectively, 37, 45, 33 and 25 videos. Figure  2  depicts the average arousal trace of the cleaned dataset for each game.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Learning Audiovisual Models Of Affect",
      "text": "This study investigates the degree to which information coming from footage of the player's interaction with a game-i.e. pixels of frames and sound of a gameplay video-can act as sole predictors of a player's affective state. The RankTrace annotation tool provides continuous values of arousal, and thus it may seem natural to view the arousal estimation problem as a regression task. In this study, however, we wish to develop a user-agnostic and general approach for predicting affect without making any assumptions regarding the value of the output which may, in turn, result in biased and user-specific models  [70] . Instead, we view the challenge of arousal prediction as both a classification and a ranking task. This section first outlines our approach for processing the input and the output of both binary classification and preference learning models of affect (see Section 4.1) and follows with the details of the machine learning models we employ for mapping gameplay frames and sound to players' arousal (see Section 4.2).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Data Preparation",
      "text": "An obvious question that arises when a learning model is faced with video data is how many frames it should consider. The authors in  [71]  and  [72]  argue that a relatively small number of subsequent frames are adequate for representing the core elements and the content of a scene. Following this advice, we train our arousal models and evaluate their performance using small segments of footage with durations ranging from 0.25 to 3.0 seconds. We view the duration of a segment as a hyperparameter of both modeling approaches and we report results regarding the performance of the learning models for varying segment length. We construct those segments by splitting the videos using non-overlapping windows. The frames of those segments represent the visual information of the gameplay. To reduce the computational cost of training and evaluating the learning models, we convert the frames of gameplay videos from RGB colour to grayscale and resize them to 72 × 96 pixels for SS and MB datasets and 72 × 115 pixels for SR and SON datasets; doing so results in a more compact yet general-purpose representation.\n\nAs far as audio data is concerned, we compute the Mel Frequency Cepstral Coefficients (MFCCs)  [73]  corresponding to the sound of each footage segment. MFCCs have been successfully used for audio classification and retrieval schemes  [74] ,  [75]  as they can represent the spectral properties of audio data in a compact fashion.\n\nTo construct models of arousal, independently of the method used, we fix the range of the annotation values of each footage to [0, 1] using min-max normalization. Then, we synchronize the recording frequency of videos (30Hz) with annotations (4Hz) by extrapolating annotation values to each non-annotated frame. Finally, the arousal value associated with each segment is the average of the annotation values of the frames belonging to it.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Deep Learning Models Of Affect",
      "text": "To learn the unknown mapping between gameplay pixels, sounds and arousal we employ and train deep learning models to infer such a function. The deep learning models receive as input both the frames of the footage segments and the computed MFCCs and fuse those two streams of information. The learning architecture that processes and fuses the audiovisual information-for both binary classification and ranking-is depicted in Fig.  3 .\n\nThe video stream feeds a convolutional neural network that contains three convolutional layers with 8, 12, and 16 filters, respectively. The size of the filters for all convolutional layers is 5 × 5 pixels. A max-pooling operation of size 2 × 2 pixels follows each convolutional layer. The output of the convolutional stream (a feature vector of 640 elements for the SS and MB datasets and 1056 elements for the SR and SON datasets) represents the visual information of the input in a compact fashion. We should emphasize that the convolutional stream exploits both spatial and temporal information of the video frames. It exploits the spatial information by learning spatial filters (filters applied along the spatial dimension of the input). Moreover, since the learning model processes sequences of frames that exhibit temporal relations, the learned spatial filters implicitly capture and encode the temporal information of the inputs.\n\nThe audio stream receives the MFCCs as its input and propagates it directly to the fusion layer. The network does not process the MFCCs before fusing the visual and the audio streams since MFCCs are already a compact representation of the sound included in a video segment. The fusion layer, initially, concatenates the MFCCs (330 elements for each second of footage) and the features constructed by the convolutional (video) stream; it then propagates the information to a fully connected layer with 64 nodes. All aforementioned nodes use the ReLU activation function.\n\nAll the hyperparameters of the employed model, i.e., the number and the size of hidden layers, the activation functions and the approach for fusing the two information streams, are empirically selected to balance two different criteria: (a) the computational cost of training and evaluating the model and (b) the sample complexity for avoiding under-/over-fitting. The model described above has approximately 6.5 • 10 4 trainable parameters.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Deep Classifier",
      "text": "The task of arousal classification is formulated as follows; we denote x ∈ R h×w×c and as z ∈ R p the raw visual and audio information of a gameplay footage respectively, where h, w and c stand for height, width and length of the video segment, and p for the length of the video's audio stream. Let ξ(x) and ζ(z) represent the transformations of raw information to informative features. In our case, ξ(•) ∈ Ξ, where Ξ denotes all possible functions that can be modeled by the CNN described in Section 4.\n\nis the negative log-likelihood loss.\n\nIn our case F is the class of functions computed by feedforward fully connected networks with one hidden layer of 64 neurons and 2 output neurons activated by the softmax function (see Fig.  3 ). The fact that we minimize the loss in (1) with respect to both f and ξ indicates that our model is end-to-end trainable, i.e. the weights of the CNN (feature construction of visual input) and the classifier are optimized simultaneously.\n\nFor training the binary classifier we transform the continuous annotation values of the segments into binary values (low and high arousal) by using the mean of the annotation trace of each video as the class splitting criterion (Fig.  4 ). We opt for the mean value of the annotation trace as it is the most intuitive and unbiased way to split a continuous, unbounded annotation trace. Finally, we employ a threshold parameter ( ) to determine a region around the mean within which annotation values are labeled as uncertain and ignored during classification (see the shaded area in Fig.  4 ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Deep Preference Learner",
      "text": "The preference learner indicates, via its output, which one of two input segments is associated with a higher arousal value. By denoting a function\n\nIn our case, g(•) is the softmax function. Based on this formulation, the preference learner employed here-similarly to RankNet  [47] ,  [76] -can be seen as a binary classifier which takes as input a pair of samples and outputs 1 if the first sample in the pair is ranked higher, and 0 otherwise. Again, the output nodes employ the softmax activation.\n\nSimilarly to the parameter of binary classification, in preference learning we employ a threshold δ which determines if the absolute difference between the mean arousal values of two segments is high enough for the segments to be considered as a preference pair (i.e. a datapoint for training). Based on δ we create input pairs by comparing them in both ways; i.e. we use both (a, b) and (b, a) pairs, where a and b represent the audiovisual information of two different segments. This approach gives us a perfectly balanced dataset for deep preference learning.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results",
      "text": "This study aims to test the hypothesis that there is a general-purpose learnable mapping of gameplay footage representation to players' affect. Towards this direction, we use the two-stream neural network (see Section 4.2) for classifying game video and audio segments as high or low arousal, and for ranking them. For all the experiments in this paper we report the average cross-validation accuracy and the 95% confidence following the demanding leave-onevideo-out cross-validation scheme  [10] ,  [77]  which offers a highly conservative estimate for the generalization capacity of the models. To avoid model overfitting we employ early stopping by randomly selecting 4 videos of the training set to form the validation set. Early stopping is activated if the classification accuracy on the validation set does not improve for 30 training epochs. We compare the performance of the model against a baseline model which always outputs the most common class in the training set. The baseline accuracy for preference learning is always 50%, since we have a perfectly balanced dataset (see Section 4.2.2).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Classifying Arousal",
      "text": "To investigate the impact of the two input modalities (video frames and audio) on the performance of the model, we report the classification accuracy of three model types: two single-stream (unimodal) neural networks which are trained on either the visual or the audio information, and the twostream bimodal neural network (see Section 4.2) which is trained on both visual and audio information. Figure  5  reports the average classification accuracy values obtained for different input modalities for all games, across different thresholds for omitting uncertain values near the annotation's mean value. For = 0, all segments of a trace are labelled high or low if their mean arousal value is above or below the mean value of the entire annotation trace (µ), respectively; for > 0, segments with mean arousal values within [µ -, µ + ] are omitted from the data (see Fig.  4 ). Note that preliminary experiments established that splitting the traces into segments of 0.5 seconds (see Section 5.3) led to the highest accuracies across the different modalities and parameters, and as such the exploration of the best in Fig.  5  focuses on a time window of 0.5 seconds, while Table  1  presents the size of the employed datasets.\n\nUnsurprisingly, the deep learning models perform better when both audio and visual inputs are considered. For SS and MB the bimodal classifier reaches accuracies as high as 30% above the baseline classifier, but only 1% to 3% above the visual-only classifier. Similarly, for SR the bimodal classifier reaches accuracies 20% above the baseline classifier and 2% above the unimodal visual-only model. For these three datasets, most of the information regarding arousal is stored within the pixels of the video, while audio seems to play a minor role. For SS, however, audio can also be a good predictor of arousal, since even audio-only classifiers reach accuracies between 10% and 20% above the baseline. The same holds for SR when = 0.2, in which case the audio-only model reaches accuracies nearly 15% above the baseline. On the other hand, audio-only models cannot predict arousal for the MB dataset and the SR dataset (when < 0.2), as accuracies are on par or worse than the baseline. For MB, audio information per se is not a reliable predictor of arousal, most likely because sound effects are rather sparse (see Section 5.5). However, it can contribute to the model's predictive capacity when combined with visual information. For the SR dataset, audio information does not seem to affect the performance of the model when it is combined with visual information. For the SON dataset, both bimodal and unimodal models perform on par or worse than the baseline. In this case, neither visual nor audio information is a reliable predictor of arousal. We believe that this occurs due to the specific nature and design of the game; SON is a horror game with a delayed effect in arousal which may not be captured by the class splitting criterion (one of the limitations of our study discussed in Section 6). As far as the design of the game is concerned, visual information in SON comes in highly vignetted frames with no HUD elements, which makes the vision-based pattern recognition task difficult and ambiguous. The background audio of the game also changes suddenly when the player moves from one room to another. Since these changes do not follow a specific pattern, audio information encoded in MFCCs cannot be easily associated with arousal. In summary, for 3 out of 4 datasets, the high performance obtained by varying the uncertainty bound indicates that the mapping between general-purpose representations of audiovisual gameplay information and arousal can be learned statistically with very high accuracy. Results for the SON dataset indicate that the performance of our models depends on the specific nature of the game, as well as on the underlying assumptions of our approach (see Section 6).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ranking Arousal",
      "text": "Similar to Section 5.1, we investigate how different modalities affect the performance of the preference learner by training three preference learning models with different inputs. The preference learner compares two input segments and outputs which segment has a higher arousal value. Based on preliminary experiments we focus on the best time window for the preference learner, which is 2 seconds.\n\nThe δ parameter (as defined in Section 4) sets the minimum absolute difference between the mean annotation value of two segments that can be considered as a preference. In this section, we investigate the performance of the preference learner in terms of average classification accuracy for 5 different values of δ, i.e., δ = {0.0, 0.2, 0.4, 0.6, 0.75}, and for the different input modalities. For all the experiments presented, we follow the leave-one-video-out validation procedure using segments of 2 seconds. Figure  6  summarises the results of this investigation, and the sizes of the datasets are presented in Table  1 .\n\nThe preference learner achieves up to 32%, 28%, 22% and 11% higher accuracy than the random baseline for the SS, MB, SR and SON dataset, respectively. As with the classifier, the models that exploit both audio and visual input perform better than unimodal models. While high values for δ yield pairs of inputs that have significantly different annotation values, this also results in smaller datasets. According to Fig.  6 , for SS and MB we obtain the highest performance values when δ = 0.6, for SR when δ = 0.4 and for SON when δ = 0.75. These threshold values seem to balance between highly informative and comparable inputs, and adequately large dataset size for training (see Table  1 ).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Impact Of The Time Window",
      "text": "In all experiments presented so far we investigated the performance of the arousal model by keeping the time window of the input signal constant (0.5 seconds for classification and 2 seconds for preference learning). In this section, we vary the time window while retaining the best δ and values found in Sections 5.1 and 5.2 respectively. We assume that the duration of the gameplay videos affects the model performance for three reasons: first, the length of the window determines directly the size of the dataset; second, the duration of footage segments determines the amount and the quality of the audiovisual information contained in a segment (i.e. the longer the segment, the richer the information contained in it); third, the duration of the window affects the ground truth arousal values as those are averaged from the window's annotation trace.\n\nFigure  7  (left) summarizes the impact of window duration on the accuracy of our proposed two-stream (audio and visual) neural network for the classification task. For all results, the uncertainty threshold value is fixed to its best value: = 0.2. For the SS game, accuracy is consistently over 70% for all durations. However, the model achieves the best performance for 0.5 second windows, and accuracies drop for longer windows. It appears that the fast pace of the game does not favor inputs of long duration since their ground truth annotation values are over-smoothed. For the MB game, the accuracies deviate wildly in different time windows. In particular, the performance of the model is over 80% for segments of 0.5 or 1 seconds, and less than 65% for shorter segments (∼0.25 second). Contrary to SS, MB is a slow paced game. Therefore, in this game it seems that short segments do not contain sufficiently rich information for the classification task, and thus do not contribute towards the efficient training of the model. Both games (especially MB) perform worse in segments over 1 second, also due to the fact that the size of the dataset becomes too small for training (for 3 seconds and = 0.2 the datasets for SM and MB are only 345 and 448 segments). For the SR dataset the classification accuracy is ∼80% for all time windows. However, it shows wide confidence intervals due to the small number of training samples (as shown in Table  1 , for 2 or 3 second windows less than 200 samples are retained). Figure  7  (right) similarly visualizes the impact of time window length on the accuracy of the preference learner using audiovisual input, and with the best δ value (δ = 0.6 for SS and MB, δ = 0.4 for SR and δ = 0.75 for SON). As indicated in Section 5.2, the best performance for both datasets is obtained for segments of 2 seconds. For the SS dataset the learner that uses 2 second segments as input performs 5% and 2.7% better that the models that use 1 and 3 second segments, respectively. For the MB and the SR games, all models perform almost the same irrespective of the time window considered, although for short time windows (0.5 seconds) the performance drops in both games. For SON the preference learner performs best for 2 seconds time windows (11% above the baseline), and 3 seconds (6% above the baseline). This suggests that Sonancia, as a horror game, elicits affect in a delayed fashion and thus requires longer segments of gameplay to be considered. Due to the way that preference learning processes the dataset, the number of preferences increases exponentially compared to the number of segments themselves: based on Table  1 , the number of preferences at 0.5 seconds are in the power of 10 4 (up to 50 times the number of preferences at 2 seconds). For segments of 0.25 seconds, the dataset explodes and training becomes problematic due to computational effort.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Classification Vs. Ranking",
      "text": "In this section we compare the preference learner against the binary classifier. While both methods yield high accuracies for three of the four games, such a metric is not appropriate for conducting a fair comparison between the methods as the set of input-output pairs for the two approaches is not the same  [48] . Following the method introduced by Martinez et al.  [48] , we compare the two approaches based on the global orders of arousal they produce when they are fed with inputs that belong to the same gameplay footage. The orders produced by the models are evaluated against the ground truth global order which is derived by the arousal annotation values. Inspired by  [48] ,  [70]  we compare the methods using the Kendall's rank correlation coefficient (τ ), which measures the ordinal association between two rankings  [78] . We calculate τ on the test video in a leaveone-video-out cross-validation process, and report the 95% confidence intervals across all videos in each dataset.\n\nFor both approaches we use the trained models presented in Sections 5.1 and 5.2 which achieve the best classification accuracy: for classification, the best models are with = 0.2 and a time window of 0.5 seconds for all games, and for preference learning the best models are with δ = 0.6 for SS and MB, δ = 0.4 for SR and δ = 0.75 for SON and a time window of 2 seconds. Fig.  8  also shows the τ values for models trained on both time windows for both approaches, for a more holistic comparison. The average Kendall's τ for both datasets indicates-unsurprisinglythat the produced orderings are positively correlated to the ground truth independently of the method used. For the SS and SR games both approaches seem to perform almost the same for their best models (with no significant differences). For MB the classifier yields higher τ values than the preference learner and for SON the best preference learner yields higher τ values than the best classifier (which is also at 2 seconds), but these differences are not statistically significant. As evident from Section 5.3, the classifier performs worse at 2 seconds windows (except for SON) while preference learning performs worse at 0.5 second windows compared to each method's optimal time window. It should be noted that the way classification and preference learning process the data results in a very different treatment (classes versus ranks) which makes a completely fair comparison very difficult. Indicatively, classification with 2 second windows and = 0.2 operates on a dataset of size 483, 700, 180 and 426 for SS, MB, SR and SON respectively, versus 3, 860, 6, 072, 4, 898 and 1, 282 for preference learning (with the best δ values per game). Therefore, using the best models for each approach even if the input is different (specifically, the number of frames used as input to the CNN, and the number of MFCCs for audio) is the most straightforward comparison as the number of samples (with the chosen , δ parameters) are in the same order of magnitude.\n\nBased on the comparison above, we conclude that a binary classifier can reach comparable accuracies to the preference learner, or higher in the case of MB. The accuracy of the binary classifier comes at the cost of the resolution of the output (as there are only two classes). If the problem requires larger output resolution (e.g., high, medium and low arousal), it is not clear how a 3-class classifier could produce such orderings. On the other hand, preference learning models can always produce orderings via pairwise comparisons of inputs and they appear to be more robust across time windows and across all games tested.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Analysis Of Findings",
      "text": "The experiments presented in this paper showed that it is possible to construct accurate models of players' arousal based on general-purpose representations of gameplay footage. The results obtained across different input modalities also indicate that the visual information is key for the efficiency of the models. Moving towards higher degrees of model expressivity and explainability, in this section we attempt to identify the features of the gameplay video that contribute more to the output of the arousal models. One way to achieve this is by visualizing the areas of the frame that have the highest influence on the model's prediction. For that purpose we use the Gradient-weighted Class Activation Mapping (GCAM) method  [79] . For any given input, GCAM computes the gradient of the output neuron with respect to the neurons of a convolutional layer. By multiplying the given input with the computed gradient, we obtain a heatmap that indicates how much each area of the input contributes to the output.\n\nFigure  9  depicts the activation maps for a sample footage segment for different games and learning paradigms; for visualisation purposes, we display the last frame of the segment. We observe that aspects of the heads-up display (HUD) affect the arousal prediction. For SS, the score located at the top centre of the screen-which keeps increasing during the progression of the game as the player kills more and more hostile toys-contributes significantly to arousal\n\nFig.  9 : Activation maps of a sample footage segment for all games. We display the last frame of the segment.\n\nprediction. The pixels of the avatar and the hostile toys, however, seem to have the highest impact on the outcome independently of the method used. Similarly, the time indicator on the HUD of the MB game contributes highly to the arousal prediction regardless of the learning paradigm used. Besides the time indicator, the location of the ball, the enemies and the tokens appear to have a substantial impact on arousal prediction. Interestingly, the HUD element of the player's health was not considered for either game. For SR, the HUD elements and the player's avatar (car) do not seem to be important features for either approach; the focus is instead on level elements immediately in front of the car such as the finish line or the loop in the horizon. For SON, it is obvious that the lack of HUD and dark visuals (only a small part of the screen contains information) confuse the classifier, although both approaches identify the statue (the goal of the game) and the monster as important features.\n\nAs a general comment from our qualitative analysis, there are two key differences in how sound influences the arousal models in different games. On the one hand, sound effects in SS follow shooting and enemy deaths which are common events and information-rich (e.g. killing an enemy means that the player survives longer), while for MB sound effects are rare since they trigger when a token is picked (with three tokens in the game) or when the player dies (which will not be a common event). Sound effects are more common and can thus be exploited better in SS, which explains the low performance of audio channels in arousal detection for MB. SR on the other hand has a persistent sound from the engine, making some frequencies in the MFCC near-constant. Finally, in Sonancia there are almost no sound effects (only if a monster sees the player) and the background audio changes based on the room in a non-diegetic way  [80] . The sound design in SON is thus expected to confuse the models of affect as the soundscape variation is sparse and only captures the gameplay context very indirectly.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "The most common approaches for modelling affect rely on direct observations and measurements of human behaviour. The proposed study, to the best of our knowledge, presents one of the first attempts to model affect via general-purpose representations of information that comes solely from gameplay footage that does not display human behaviour directly. Human behaviour is embedded into the gameplay footage, e.g. as avatar movement and actions, since emotion is manifested through and annotated on the video per se.\n\nWe exploit bimodal audiovisual information to build and test a deep neural network for predicting players' arousal states via two different approaches; binary classification and preference learning. Results show, on the one hand, that building such models is possible, and on the other, that these models can be highly accurate in most cases. Moreover, more robust and accurate models can be constructed when the dataset is pruned from ambiguous data.\n\nIn this study, we make several assumptions which implicitly indicate possible limitations of our approach. First, we use the mean value of each annotation trace to split data into high and low arousal classes. Although this criterion is intuitive and straightforward, it makes sense only for stationary processes. In our study, this criterion results in robust annotations due to the short duration of gameplay. For long playthroughs, however, this assumption will not hold and using the mean as the class splitting criterion may produce misleading classification and preference learning results. Second, we use a representative annotation trace (median trace) to detect and remove outliers. In other words, our data cleaning methodology considers only the distribution of the annotation values. In our study, such a methodology can efficiently detect and remove outliers since the games considered can be played in specific ways, and gameplay duration is short. For sandbox games or long play sessions, a data cleaning methodology that takes into consideration simultaneously the input and the output distributions should be used (i.e., the joint distribution of audiovisual information and annotation values). Finally, the data points used for training the affect models are generated sequentially. Thus the annotation of a data point at a specific time instance might depend on the annotation value of the data point generated before. Our models, however, are not able to exploit this information. To take advantage of this kind of information, models that explicitly take into consideration the temporal ordering of data, such as LSTMs, should be used.\n\nThe differences in performance among the four games also illuminate some concerns regarding the impact of the game environment on the feasibility of general-purpose models. As discussed in Section 3 and Section 5.5, each game is different in terms of what the player sees (camera perspective, color scheme, illumination), hears (background audio, variety and volume of sound effects) and performs (control schemes, actions per minute, degree of immediate feedback, available actions, clarity of game goal). Based on these differences, it is expected that the player also feels (and annotates) differently in each game (see Figure  2 ). While in games such as Survival Shooter highly accurate models of affect could be trained via either approach, in Sonancia specifically the performance of the classifier was not better than the baseline and the preference learner could reach accuracies of ∼ 60% with the best parameter setup. It can be gleaned that arcade games with fast-paced interactions (such as SS and SR), a top-down camera perspective that shows more of the level (SS and MB), distinct forms and colors to distinguish game objects (MB and SR), loud sound effects tied to game events (SS) could help the model predict affect from the audiovisual signals alone. In contrast, Sonancia has none of the above design patterns; moreover, the actions that a player takes (e.g. choosing a room to go into) do not have immediate gameplay (and, one would assume, affective) impact as a monster could be hiding in a remote part of the room she chooses to go. Future work should explore where the limits are in terms of game environments and visual, audio or interaction design for which this method can be applied. While it is expected that high-contrast and fast-paced arcade games such as the ATARI games studied by Mnih et al.  [2]  will work very well for this method, it is unclear whether audiovisual signals in time windows of a few seconds would work well for e.g. role-playing games (which require long interactions), visual novels (where the story consequences are not displayed visually or immediately) or turn-based games (where realtime windows are irrelevant). Exploring how these different design patterns affect the quality of predictive affect models based on audiovisual data alone can be useful not only for affective computing but also for game design, as it can inform designers how to maximize the emotional impact of their content.\n\nWhile this study is one of the first attempts at the challenging task of predicting affect states from general-purpose gameplay footage information, the results are promising and point to a number of extensions in future work. In this paper, our models require training on each particular game; while the method is robust and general-propose, the models themselves have not been tested for their generality. To test for the model's generality, a future direction would be to devise leave-one-game-out validation schemes once our game corpus becomes even larger. Such a cross-validation scheme would allow us to test the degree to which certain characteristics of audiovisual information are general predictors of arousal and transferable to other games. In terms of the model's input, we use grayscale frames to represent the visual information and MFCCs for the audio information. Grayscale frames and MFCCs can compactly represent the audiovisual information of gameplay footage and reduce the computational cost of training the models. These representations, however, can be enhanced without sacrificing the generality of our approach. In terms of sounds, MFCCs can be fused with the concise GeMAPS feature set  [39]  which has been successfully used for voice recognition and affective computing applications. As far as the representation of visual information is concerned, it can be enhanced by using RGB channels or hand-crafted channels that include low-level image information  [81] . For example, exploiting hue and saturation information could better detect the red monsters present in Sonancia. In terms of output (affect labels), we use the mean arousal value within a time window. That is an intuitive approach, which, however, can be further investigated and refined. For example, amplitude and average gradient  [9] ,  [49]  could be used for processing annotations within a time window.\n\nBeyond arousal, the method's robustness needs to be tested for other affective dimensions-including valence and dominance-or continuous affective states such as engagement  [46] . Beyond games, the method appears to be generalizable to any rich human computer interaction domain that interweaves the context of interaction with user behavior and user affect, such as mobile app interaction and web navigation. Additional experiments in datasets of that type, however, need to be performed to validate this hypothesis.",
      "page_start": 11,
      "page_end": 14
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper we introduced a general methodology for predicting affect solely from audiovisual aspects of human computer interaction. Our hypothesis is that arousal embedded in affective interaction can be modeled accurately without considering any user manifestation of affect besides the pixels and the sound of the interaction. The hypothesis was tested in digital games, a domain that interweaves affect with audiovisual content through gameplay interaction. The audiovisual content in games has a dual role: it is both the elicitor of affect and the context of the interaction. We developed two deep learning paradigms for mapping directly from pixels and audio of videos to the annotated arousal of gameplay: a deep classifier and a deep preference learner, both using a combination of CNN and feedforward architectures. Our experimental results across four dissimilar games suggest that arousal can be predicted with very high accuracies via such general-purpose representations (as high as 85%) as long as the audiovisual feed captures the gameplay context accurately (which depends on the game's design). The fusion of the two modalities (gameplay pixels and sounds) unsurprisingly appears to be beneficial for the predictive capacity of the models. Our key findings also show that activation maps can visualize the areas on the screen that lead to high arousal-in our case primarily the score, the avatar and the enemies. The GCAM visualization increases the explainability of the models  [82]  and can be very useful for game designers when adjusting the appearance or in-game function of the game elements to increase or decrease the elicited emotion of certain events.\n\nThis paper defines one of the first steps towards the creation of general representations of affect by studying arousal detection in games. The results showcase that it is possible to detect arousal accurately by only considering low-level contextual information of the interaction. The key findings are relevant to any application area within affective computing and directly applicable to domains of rich human computer interaction that consider user affect.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Screenshots from Survival Shooter (top left), Maze-",
      "page": 1
    },
    {
      "caption": "Figure 1: ). All gameplay videos have been annotated by the",
      "page": 2
    },
    {
      "caption": "Figure 1: shows a screenshot of each game. In this section, we",
      "page": 4
    },
    {
      "caption": "Figure 2: depicts the",
      "page": 5
    },
    {
      "caption": "Figure 2: Arousal annotation per game, averaged from signals",
      "page": 5
    },
    {
      "caption": "Figure 3: The architecture of the proposed deep learning",
      "page": 5
    },
    {
      "caption": "Figure 3: The video stream feeds a convolutional neural network",
      "page": 6
    },
    {
      "caption": "Figure 4: Class splitting procedure. Samples with annotations",
      "page": 6
    },
    {
      "caption": "Figure 3: ). The fact that we minimize the loss in",
      "page": 6
    },
    {
      "caption": "Figure 5: reports the average classiﬁcation accuracy values obtained",
      "page": 7
    },
    {
      "caption": "Figure 5: Average classiﬁcation accuracy (%) on the test set",
      "page": 7
    },
    {
      "caption": "Figure 5: focuses on a time window of 0.5 seconds, while Table",
      "page": 7
    },
    {
      "caption": "Figure 6: summarises the results of this investigation, and the sizes",
      "page": 8
    },
    {
      "caption": "Figure 6: Average accuracy (%) of the preference learner on",
      "page": 8
    },
    {
      "caption": "Figure 6: , for SS and MB we obtain the highest performance",
      "page": 8
    },
    {
      "caption": "Figure 7: (left) summarizes the impact of window du-",
      "page": 8
    },
    {
      "caption": "Figure 7: Average accuracy (%) on test set for the best audiovi-",
      "page": 9
    },
    {
      "caption": "Figure 7: (right) similarly visualizes the impact of time",
      "page": 9
    },
    {
      "caption": "Figure 8: Kendall’s τ across modelling approaches, games, and",
      "page": 9
    },
    {
      "caption": "Figure 8: also shows the τ",
      "page": 9
    },
    {
      "caption": "Figure 9: depicts the activation maps for a sample footage",
      "page": 10
    },
    {
      "caption": "Figure 9: Activation maps of a sample footage segment for all",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Survival Shooter\n1.00\n0.75\nlasuorA\n0.50\n0.25\n0.00\n0 20 40": "Time [sec]",
          "Column_2": "",
          "Maze Ball\n1.00\n0.75\nlasuorA\n0.50\n0.25\n0.00\n0 20 40 60 80": "Time [sec]"
        },
        {
          "Survival Shooter\n1.00\n0.75\nlasuorA\n0.50\n0.25\n0.00\n0 20 40": "Solid Rally\n1.00\n0.75\nlasuorA\n0.50\n0.25\n0.00\n0 20 40 60 80\nTime [sec]",
          "Column_2": "",
          "Maze Ball\n1.00\n0.75\nlasuorA\n0.50\n0.25\n0.00\n0 20 40 60 80": "Sonancia\n1.00\n0.75\nlasuorA\n0.50\n0.25\n0.00\n0 40 80 120\nTime [sec]"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "segments\napoint for\nomparing\n,a) pairs,\nmation of\nperfectly",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": ""
        },
        {
          "Column_1": "",
          "Column_2": "a F\nge a\ne o\nor a\nw\nis\nd\nt\ne-\nt",
          "Column_3": "",
          "Column_4": "ig. 5: Average classification\ncross the two modalities and\nldvalues((cid:15)).Thetimewindo\nreasindicatethe95%confide\nhe traces into segments of 0.5\nothehighestaccuraciesacros",
          "Column_5": "",
          "Column_6": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "0.5 second window 2 seconds\nSurvival\ntneiciff\n0.6 Maze Ba\nSolid Ral": "",
          "Column_3": "",
          "Column_4": "Survival\nMaze Ba\nSolid Ral",
          "Column_5": "Shooter\nll\nly"
        },
        {
          "Column_1": "",
          "0.5 second window 2 seconds\nSurvival\ntneiciff\n0.6 Maze Ba\nSolid Ral": "eoc\nnoitalerroc\nknar\ns'l",
          "Column_3": "",
          "Column_4": "Sonancia",
          "Column_5": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Artificial general intelligence",
      "authors": [
        "B Goertzel",
        "C Pennachin"
      ],
      "year": "2007",
      "venue": "Artificial general intelligence"
    },
    {
      "citation_id": "2",
      "title": "Human-level control through deep reinforcement learning",
      "authors": [
        "V Mnih",
        "K Kavukcuoglu",
        "D Silver",
        "A Rusu",
        "J Veness",
        "M Bellemare",
        "A Graves",
        "M Riedmiller",
        "A Fidjeland",
        "G Ostrovski"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "3",
      "title": "Your gameplay says it all: Modelling motivation in tom clancy's the division",
      "authors": [
        "D Melhart",
        "A Azadvar",
        "A Canossa",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Intl. Conference on Games"
    },
    {
      "citation_id": "4",
      "title": "Artificial Intelligence and Games. Springer Nature",
      "authors": [
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2018",
      "venue": "Artificial Intelligence and Games. Springer Nature"
    },
    {
      "citation_id": "5",
      "title": "Predicting online media effectiveness based on smile responsesgathered over the internet",
      "authors": [
        "D Mcduff",
        "R El Kaliouby",
        "D Demirdjian",
        "R Picard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "6",
      "title": "Deep unsupervised multi-view detection of video game stream highlights",
      "authors": [
        "C Ringer",
        "M Nicolaou"
      ],
      "year": "2018",
      "venue": "Proceedings of the Intl. Conference on the Foundations of Digital Games"
    },
    {
      "citation_id": "7",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Deep analysis of facial behavioral dynamics",
      "authors": [
        "L Zafeiriou",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "9",
      "title": "Ranktrace: Relative and unbounded affect annotation",
      "authors": [
        "P Lopes",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "10",
      "title": "From pixels to affect: A study on games and player experience",
      "authors": [
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "11",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "1995",
      "venue": "MIT, Tech. Rep"
    },
    {
      "citation_id": "12",
      "title": "Vocal emotion recognition across disparate cultures",
      "authors": [
        "G Bryant",
        "H Barrett"
      ],
      "year": "2008",
      "venue": "Journal of Cognition and Culture"
    },
    {
      "citation_id": "13",
      "title": "The Oxford handbook of affective computing",
      "authors": [
        "R Calvo",
        "S D'mello",
        "J Gratch",
        "A Kappas"
      ],
      "year": "2015",
      "venue": "The Oxford handbook of affective computing"
    },
    {
      "citation_id": "14",
      "title": "Modeling multimodal cues in a deep learning-based framework for emotion recognition in the wild",
      "authors": [
        "S Pini",
        "O Ahmed",
        "M Cornia",
        "L Baraldi",
        "R Cucchiara",
        "B Huet"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM Intl. Conference on Multimodal Interaction"
    },
    {
      "citation_id": "15",
      "title": "Affect detection: An interdisciplinary review of models, methods, and their applications",
      "authors": [
        "R Calvo",
        "S Mello"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "16",
      "title": "Deciphering the enigmatic face. the importance of facial dynamics in interpreting subtle facial expressions",
      "authors": [
        "Z Ambadar",
        "J Schooler",
        "J Cohn"
      ],
      "year": "2005",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face",
      "authors": [
        "J Bassili"
      ],
      "year": "1979",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "18",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "19",
      "title": "The computer expression recognition toolbox (CERT)",
      "authors": [
        "G Littlewort",
        "J Whitehill",
        "T Wu",
        "I Fasel",
        "M Frank",
        "J Movellan",
        "M Bartlett"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE Intl. Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "20",
      "title": "Automatic recognition of facial actions in spontaneous expressions",
      "authors": [
        "M Bartlett",
        "G Littlewort",
        "M Frank",
        "C Lainscsek",
        "I Fasel",
        "J Movellan"
      ],
      "year": "2006",
      "venue": "Journal of Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Recognizing affective dimensions from body posture",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2007",
      "venue": "Proceedings of the IEEE Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "22",
      "title": "Automatic recognition of non-acted affective postures",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze",
        "A Steed"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "23",
      "title": "Technique for automatic emotion recognition by body gesture analysis",
      "authors": [
        "D Glowinski",
        "A Camurri",
        "G Volpe",
        "N Dael",
        "K Scherer"
      ],
      "year": "2008",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "24",
      "title": "The identification of emotions from gait information",
      "authors": [
        "J Montepare",
        "S Goldstein",
        "A Clausen"
      ],
      "year": "1987",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition using Kinect motion capture data of human gaits",
      "authors": [
        "S Li",
        "L Cui",
        "C Zhu",
        "B Li",
        "N Zhao",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "26",
      "title": "Vocal communication of emotion: A review of research paradigms",
      "authors": [
        "K Scherer"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "27",
      "title": "An efficient mfcc extraction method in speech recognition",
      "authors": [
        "W Han",
        "C.-F Chan",
        "C.-S Choy",
        "K.-P Pun"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE Intl. symposium on circuits and systems"
    },
    {
      "citation_id": "28",
      "title": "Combining multiple kernel methods on riemannian manifold for emotion recognition in the wild",
      "authors": [
        "M Liu",
        "R Wang",
        "S Li",
        "S Shan",
        "Z Huang",
        "X Chen"
      ],
      "year": "2014",
      "venue": "Proceedings of the Intl. Conference on multimodal interaction"
    },
    {
      "citation_id": "29",
      "title": "Distinctive image features from scale-invariant keypoints",
      "authors": [
        "D Lowe"
      ],
      "year": "2004",
      "venue": "Intl. journal of computer vision"
    },
    {
      "citation_id": "30",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Capturing au-aware facial features and their latent relations for emotion recognition in the wild",
      "authors": [
        "A Yao",
        "J Shao",
        "N Ma",
        "Y Chen"
      ],
      "year": "2015",
      "venue": "Proceedings of the ACM on Intl. Conference on Multimodal Interaction"
    },
    {
      "citation_id": "32",
      "title": "Face recognition with local binary patterns",
      "authors": [
        "T Ahonen",
        "A Hadid",
        "M Pietikäinen"
      ],
      "year": "2004",
      "venue": "Proceedings of the European conference on computer vision"
    },
    {
      "citation_id": "33",
      "title": "Deep learning vs. kernel methods: Performance for emotion prediction in videos",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "34",
      "title": "Finding celebrities in billions of web images",
      "authors": [
        "X Zhang",
        "L Zhang",
        "X.-J Wang",
        "H.-Y Shum"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Proceedings of the Intl. Conference on Neural Information Processing"
    },
    {
      "citation_id": "36",
      "title": "Aff-wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "37",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H.-W Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proceedings of the ACM Intl. conference on multimodal interaction"
    },
    {
      "citation_id": "38",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "39",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Audio and face video emotion recognition in the wild using deep neural networks and small datasets",
      "authors": [
        "W Ding",
        "M Xu",
        "D Huang",
        "W Lin",
        "M Dong",
        "X Yu",
        "H Li"
      ],
      "year": "2016",
      "venue": "Proceedings of the ACM Intl. Conference on Multimodal Interaction"
    },
    {
      "citation_id": "41",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "S Kahou",
        "V Michalski",
        "K Konda",
        "R Memisevic",
        "C Pal"
      ],
      "year": "2015",
      "venue": "Proceedings of the ACM Intl. Conference on Multimodal Interaction"
    },
    {
      "citation_id": "42",
      "title": "Real-world automatic continuous affect recognition from audiovisual signals",
      "authors": [
        "P Tzirakis",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Multimodal Behavior Analysis in the Wild"
    },
    {
      "citation_id": "43",
      "title": "Player modeling",
      "authors": [
        "G Yannakakis",
        "P Spronck",
        "D Loiacono",
        "E André"
      ],
      "year": "2012",
      "venue": "Artificial and Computational Intelligence in Games (Dagstuhl Seminar 12191"
    },
    {
      "citation_id": "44",
      "title": "Modeling player experience in super mario bros",
      "authors": [
        "C Pedersen",
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2009",
      "venue": "Proc. of the Intl. Conf. on Computational Intelligence and Games"
    },
    {
      "citation_id": "45",
      "title": "Fusing visual and behavioral cues for modeling user experience in games",
      "authors": [
        "N Shaker",
        "S Asteriadis",
        "G Yannakakis",
        "K Karpouzis"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on System, Man and Cybernetics"
    },
    {
      "citation_id": "46",
      "title": "Moment-tomoment Engagement Prediction through the Eyes of the Observer: PUBG Streaming on Twitch",
      "authors": [
        "D Melhart",
        "D Gravina",
        "G Yannakakis"
      ],
      "year": "2020",
      "venue": "Proceedings of the Intl. Conference on the Foundations of Digital Games"
    },
    {
      "citation_id": "47",
      "title": "Learning deep physiological models of affect",
      "authors": [
        "H Martinez",
        "Y Bengio",
        "G Yannakakis"
      ],
      "year": "2013",
      "venue": "IEEE Computational intelligence magazine"
    },
    {
      "citation_id": "48",
      "title": "Don't classify ratings of affect; rank them",
      "authors": [
        "H Martinez",
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "49",
      "title": "Towards general models of player affect",
      "authors": [
        "E Camilleri",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "50",
      "title": "Deep static and dynamic level analysis: A study on infinite mario",
      "authors": [
        "M Guzdial",
        "N Sturtevant",
        "B Li"
      ],
      "year": "2016",
      "venue": "Proceedings of the AIIDE workshop on Experimental AI in Games"
    },
    {
      "citation_id": "51",
      "title": "Fantasy, curiosity and challenge as adaptation indicators in multimodal dialogue systems for preschoolers",
      "authors": [
        "T Kannetis",
        "A Potamianos",
        "G Yannakakis"
      ],
      "year": "2009",
      "venue": "Proceedings of the 2nd Workshop on Child"
    },
    {
      "citation_id": "52",
      "title": "Psychophysiology in games",
      "authors": [
        "G Yannakakis",
        "H Martinez",
        "M Garbarino"
      ],
      "year": "2016",
      "venue": "Emotion in games"
    },
    {
      "citation_id": "53",
      "title": "Video affective content analysis: a survey of state-of-the-art methods",
      "authors": [
        "S Wang",
        "Q Ji"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Affective video content analysis: A multidisciplinary insight",
      "authors": [
        "Y Baveye",
        "C Chamaret",
        "E Dellandréa",
        "L Chen"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Affective video content representation and modeling",
      "authors": [
        "A Hanjalic",
        "L.-Q Xu"
      ],
      "year": "2005",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "56",
      "title": "Affective recommendation of movies based on selected connotative features",
      "authors": [
        "L Canini",
        "S Benini",
        "R Leonardi"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "57",
      "title": "Mutual information-based emotion recognition",
      "authors": [
        "Y Cui",
        "S Luo",
        "Q Tian",
        "S Zhang",
        "Y Peng",
        "L Jiang",
        "J Jin"
      ],
      "year": "2013",
      "venue": "The Era of Interactive Media"
    },
    {
      "citation_id": "58",
      "title": "Multimedia content analysis for emotional characterization of music video clips",
      "authors": [
        "A Yazdani",
        "E Skodras",
        "N Fakotakis",
        "T Ebrahimi"
      ],
      "year": "2013",
      "venue": "EURASIP Journal on Image and Video Processing"
    },
    {
      "citation_id": "59",
      "title": "Hierarchical affective content analysis in arousal and valence dimensions",
      "authors": [
        "M Xu",
        "C Xu",
        "X He",
        "J Jin",
        "S Luo",
        "Y Rui"
      ],
      "year": "2013",
      "venue": "Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Multi-modal learning for affective content analysis in movies",
      "authors": [
        "Y Yi",
        "H Wang"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "61",
      "title": "Capturing emotion distribution for multimedia emotion tagging",
      "authors": [
        "S Wang",
        "G Peng",
        "Z Zheng",
        "Z Xu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "62",
      "title": "Valence and arousal estimation in-the-wild with tensor methods",
      "authors": [
        "A Mitenkova",
        "J Kossaifi",
        "Y Panagakis",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Intl. Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "63",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Association"
    },
    {
      "citation_id": "64",
      "title": "Affective video content analysis via multimodal deep quality embedding network",
      "authors": [
        "Y Zhu",
        "Z Chen",
        "F Wu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Pagan: Video affect annotation made easy",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Intl. Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "66",
      "title": "Towards affective camera control in games",
      "authors": [
        "G Yannakakis",
        "H Martínez",
        "A Jhala"
      ],
      "year": "2010",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "67",
      "title": "Space maze: Experience-driven game camera control",
      "authors": [
        "Y Knight",
        "H Martínez",
        "G Yannakakis"
      ],
      "year": "2013",
      "venue": "Proceedings of the Intl. Conference on the Foundations of Digital Games"
    },
    {
      "citation_id": "68",
      "title": "Modelling affect for horror soundscapes",
      "authors": [
        "P Lopes",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2017",
      "venue": "IEEE Transactions of Affective Computing"
    },
    {
      "citation_id": "69",
      "title": "Using dynamic time warping to find patterns in time series",
      "authors": [
        "D Berndt",
        "J Clifford"
      ],
      "year": "1994",
      "venue": "Proceedings of the Intl. Conference on Knowledge Discovery and Data Mining"
    },
    {
      "citation_id": "70",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "Action snippets: How many frames does human action recognition require",
      "authors": [
        "K Schindler",
        "L Van Gool"
      ],
      "year": "2008",
      "venue": "Proceedings of the IEEE Intl. Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "72",
      "title": "Deep learning based human behavior recognition in industrial workflows",
      "authors": [
        "K Makantasis",
        "A Doulamis",
        "N Doulamis",
        "K Psychas"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Intl. Conference on Image Processing"
    },
    {
      "citation_id": "73",
      "title": "Computing melfrequency cepstral coefficients on the power spectrum",
      "authors": [
        "S Molau",
        "M Pitz",
        "R Schluter",
        "H Ney"
      ],
      "year": "2001",
      "venue": "Proceedings of the IEEE Intl. Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "74",
      "title": "Automatic detection of pathological voices using complexity measures, noise parameters, and mel-cepstral coefficients",
      "authors": [
        "J Arias-Londo Ño",
        "J Godino-Llorente",
        "N Sáenz-Lech Ón",
        "V Osma-Ruiz",
        "G Castellanos-Domínguez"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "75",
      "title": "Content based audio retrieval with mfcc feature extraction, clustering and sort-merge techniques",
      "authors": [
        "T Nagavi",
        "S Anusha",
        "P Monisha",
        "S Poornima"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE Intl. Conference on Computing, Communications and Networking Technologies"
    },
    {
      "citation_id": "76",
      "title": "Learning to rank using gradient descent",
      "authors": [
        "C Burges",
        "T Shaked",
        "E Renshaw",
        "A Lazier",
        "M Deeds",
        "N Hamilton",
        "G Hullender"
      ],
      "year": "2005",
      "venue": "Proceedings of the Intl. Conference on Machine learning"
    },
    {
      "citation_id": "77",
      "title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation",
      "authors": [
        "M Kearns",
        "D Ron"
      ],
      "year": "1999",
      "venue": "Neural computation"
    },
    {
      "citation_id": "78",
      "title": "The kendall rank correlation coefficient",
      "authors": [
        "H Abdi"
      ],
      "year": "2007",
      "venue": "Encyclopedia of Measurement and Statistics"
    },
    {
      "citation_id": "79",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Intl"
    },
    {
      "citation_id": "80",
      "title": "Meaningful noise: Understanding sound effects in computer games",
      "authors": [
        "I Ekman"
      ],
      "year": "2005",
      "venue": "Proceedings of Digital Arts and Cultures"
    },
    {
      "citation_id": "81",
      "title": "Deep convolutional neural networks for efficient vision based tunnel inspection",
      "authors": [
        "K Makantasis",
        "E Protopapadakis",
        "A Doulamis",
        "N Doulamis",
        "C Loupos"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Intl. Conference on Intelligent Computer Communication and Processing"
    },
    {
      "citation_id": "82",
      "title": "Explainable ai for designers: A human-centered perspective on mixed-initiative co-creation",
      "authors": [
        "J Zhu",
        "A Liapis",
        "S Risi",
        "R Bidarra",
        "G Youngblood"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computational Intelligence and Games"
    }
  ]
}