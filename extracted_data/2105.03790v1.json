{
  "paper_id": "2105.03790v1",
  "title": "Distribution Matching For Heterogeneous Multi-Task Learning: A Large-Scale Face Study",
  "published": "2021-05-08T22:26:52Z",
  "authors": [
    "Dimitrios Kollias",
    "Viktoriia Sharmanska",
    "Stefanos Zafeiriou"
  ],
  "keywords": [
    "Multi-Task Learning",
    "Heterogeneous Tasks",
    "Weak Supervision",
    "Negative Transfer",
    "Distribution Matching",
    "Co-annotation",
    "Coupling",
    "Distillation",
    "holistic learning",
    "Zero-shot Learning",
    "Few-shot Learning",
    "Affect recognition in-the-wild",
    "emotion and expression classification",
    "detection",
    "valence",
    "arousal",
    "action units",
    "FaceBehaviorNet",
    "attribute detection",
    "face identification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-Task Learning (MTL) has emerged as a methodology in which multiple tasks are jointly learned by a shared learning algorithm, such as a deep neural network. MTL is based on the assumption that the tasks under consideration are related; therefore it exploits shared knowledge for improving performance on each individual task. Tasks are generally considered to be homogeneous, i.e., to refer to the same type of problem, e.g., classification. Moreover, MTL is usually based on ground truth annotations with full, or partial overlap across tasks; i.e., for each input sample, there exist annotations for all or most of the tasks. In this work, we deal with heterogeneous MTL, simultaneously addressing detection, classification and regression problems. We explore task-relatedness as a means for co-training, in a weakly-supervised way, tasks that contain little, or even non-overlapping annotations. Task-relatedness is introduced in MTL, either explicitly through prior expert knowledge, or through data-driven studies. We propose a novel distribution matching approach, in which knowledge exchange is enabled between tasks, via matching of their predictions' distributions. Based on this approach, we build FaceBehaviorNet, the first framework for large-scale face analysis, by jointly learning all facial behavior tasks. We develop case studies for: i) continuous affect estimation, facial action unit detection and basic emotion recognition; ii) facial attribute detection and face identification. We illustrate that co-training via task relatedness alleviates negative transfer, i.e., cases in which MT model's performance is, in some task(s), worse than that of a single-task model. Since FaceBehaviorNet learns features that encapsulate all aspects of facial behavior, we conduct zero-and few-shot learning to perform tasks beyond the ones that it has been trained for, such as compound emotion recognition. By conducting a very large experimental study, utilizing 10 databases, we illustrate that our approach outperforms, by large margins, the state-of-the-art in all tasks and in all databases, even in these which have not been used in its training.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "H OLISTIC frameworks, where several learning tasks are inter- connected and explicable by the reference to the whole, are common in computer vision. A diverse set of examples includes a scene understanding framework that reasons about 3D object detection, semantic segmentation and depth reconstruction  [1] , a face analysis framework that addresses face detection, landmark localization, gender recognition, age estimation  [2] , a universal network for low-, mid-, high-level vision  [3] , a large-scale framework of visual tasks for indoor scenes  [4] . Most if not all prior works rely on building a multi-task framework where learning is done based on the ground truth annotations with full or partial overlap across tasks. During training, all the tasks are optimised simultaneously aiming for representation learning that supports a holistic view of the framework. Our approach for building such framework falls under heterogeneous multi-task learning  [5] ,  [6] . As opposed to standard multi-task learning with a single type of e.g. classification tasks, heterogeneous learning addresses jointly different types of supervised tasks including classification, detection, and regression problems, which poses new challenges for effective knowledge transfer.\n\nWhat makes our work different from the previous holistic approaches is exploring the idea of task-relatedness as means for co-training the heterogeneous tasks. In our work, relatedness between heterogeneous tasks is either provided explicitly in a form of expert knowledge, or is inferred based on empirical studies. Importantly, in co-training, the related tasks exchange their predictions and iteratively teach each other so that predictors of all tasks can excel even if we have limited or no data for some of them. How to perform knowledge exchange and cotraining between heterogeneous tasks that could generalize to different scenarios and across datasets is the key challenge that we address in this work. We propose an effective distribution matching approach based on distillation  [7] , where knowledge exchange between tasks is enabled via distribution matching over their predictions. Based on this approach, we build the first holistic framework for large-scale face analysis, FaceBehaviorNet, with case studies in affective computing and in face recognition. In affective computing, we have heterogeneous tasks such as predicting categorical emotions (e.g. happy, sad, angry, surprised), predicting continuous dimensions of valence and arousal (how positive/negative, active/passive the emotional state is), predicting activations of binary action units  [8]  (activation of facial muscles) to explain the human's affective state. In face recognition, we consider the two interconnected tasks of face identification and facial attribute detection in a single holistic framework.\n\nUp until now training holistic models has been primarily addressed by combining multiple datasets to solve individual tasks  [2] , or by collecting the annotations in terms of all tasks  [3] ,  [4] . As an example, let us consider facial behavior analysis which is one of our case studies. A lot of effort has been made towards collecting datasets of naturalistic facial behavior captured in uncontrolled conditions, in-the-wild  [9] ,  [10] ,  [11] ,  [12] . Among the three heterogeneous tasks in affective computing, collecting annotations of action units is particularly costly, as it requires skilled annotators to perform the task. Nevertheless there has been a lot of effort to collect those annotations and develop automatic toolboxes  [12] ,  [13] . The datasets collected so far have annotations for training some of the heterogeneous tasks and despite significant effort  [14] , there is no dataset that for each image or video has complete annotations of all three tasks. Cotraining via task relatedness is an effective way of aggregating knowledge across datasets and transferring it across heterogeneous tasks, especially with little or non-overlapping annotations.\n\nIn this work we discuss two strategies to infer task relatedness: i) via domain knowledge, ii) via dataset annotation, see Table  1 , Table  2  in our case studies. For example, the three aforementioned tasks of facial behavior analysis are interconnected with known strengths of relatedness from the literature. In  [8] , the facial action coding system (FACS) has been built to indicate for each of the basic expressions its prototypical action units. In  [15] , a dedicated user study has been conducted to study the relationship between AUs activations and emotion expressions for basic types and beyond -see Table  1 . In  [16] , the authors show that neural networks trained for expression recognition implicitly learn action units. Also, in  [17]  the authors have discovered that valence and arousal dimensions could be interpreted by AUs. For example, AU12 (lip corner puller) is related to positive valence. In our second case study on face recognition, we have an example of a dataset such as CelebA  [18] , where annotations for both tasks, identification and attribute prediction, are available for each image. We can infer task relatedness based on the annotations empirically -see examples in Table  2 .\n\nOne of the important challenges in multi-task learning is how to avoid negative transfer, when the performance of the multitask model can be worse than that of a single-task model  [19] ,  [20] . Negative transfer occurs naturally in the multi-task learning scenarios when: i) source data are heterogeneous or less related (as all tasks are diverse to each other, there is no suitable common latent representation and thus multi-task learning produces poor representations); ii) one task dominates the training process (in this scenario, one group of related tasks may dominate the training process and negative transfer may occur simultaneously on tasks outside the dominant group). We demonstrate empirically that the proposed distribution matching approach based on task relatedness can alleviate the problem of negative transfer in FaceBehaviorNet.\n\nOur main contributions are as follows: • We propose a flexible holistic framework that can accommodate heterogeneous tasks with encoding prior knowledge of tasks relatedness. In our experiments we evaluate two effective strategies of task relatedness: a) obtained from domain knowledge, e.g. based on a cognitive study  [15] , and b) inferred empirically from dataset annotations. • We propose an effective weakly-supervised learning approach that couples, via distribution matching and label coannotation, heterogeneous tasks which contain little, or even non-overlapping annotations; we show its effectiveness for face analysis in two case studies: affective computing and face recognition.\n\n• We present the first, to the best of our knowledge, holistic network for facial behavior analysis; this is capable of simultaneously predicting 7 basic expressions, 17 action units and continuous valence-arousal emotion dimensions in-the-wild. For network training and evaluation we utilize publicly available in-the-wild databases. This network will be made publicly available. All available databases are automatically annotated for all tasks by this network; these annotations will also be made publicly available.\n\n• We conduct an extensive experimental study, currently the largest to the best of our knowledge, in which we evaluate FaceBehaviorNet on 10 databases and compare its performance to single-task networks. We demonstrate that FaceBehaviorNet when trained with the proposed coupling losses greatly outperforms single-task networks in all tasks and in all databases, even in ones that have not been used in its training. This validates that the network's capabilities are enhanced when it is jointly trained for all related tasks. It is also shown that the distribution matching approach for knowledge distillation across heterogeneous tasks successfully prevents negative transfer in multi-task learning. We also show that FaceBehaviorNet displays the best performance across all 10 databases outperforming the-stateof-the-art methods in each database. • We further explore the feature representation learned in the joint training and show its generalization abilities on the task of compound expression recognition when no or little training data is available (zero-shot and few-shot learning).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Works exist in literature that use emotion labels to complement missing AU annotations or increase generalization of AU classifiers  [21] ,  [22] ,  [23] . Our work deviates from such methods, as we target joint learning of all three facial behavior tasks via a single holistic framework, whilst these works perform only AU detection (and not emotion recognition and valence-arousal estimation). Multi-task learning (MTL) was first studied in  [24] , where the authors proposed to jointly learn parallel tasks sharing a common representation; they used part of the knowledge learned when solving one task, so as to improve learning of the other related tasks. Since then, several approaches have adopted MTL for solving different problems in computer vision and machine learning. In the face analysis domain, the use of MTL is somewhat limited. In  [25] , Face-SSD was proposed for detecting faces and performing various -separate and independent-face-related tasks, including smile recognition, face attribute prediction and valencearousal estimation; of these tasks just one is implemented at a time and there are no MTL experiments. The authors just mention that MTL can be used in Face-SSD and the network's optimization loss function should be the sum of the independent task losses.\n\nIn  [26] , MTL was tackled through a neural network that jointly handled face recognition and facial attribute prediction tasks. At first the authors trained a network for facial attribute detection on CelebA; then they used it to generate attribute labels for another database (CASIA-WebFace) that only contained facial identification (id) labels. Finally, the authors trained a multi-task network for attribute detection and face identification on that database (that contained the id labels and the generated attribute labels). The loss function of the network was the sum of the independent task losses.\n\nAn approach targeting a problem similar to ours is  [27] , presenting a knowledge augmented deep learning framework for joint AU detection and facial expression recognition. The described framework consisted of a knowledge model -represented by a Bayesian Network -and three neural network based submodels. An image-based FER model performed facial expression classification directly from image data. An AU model performed AU detection from the images. The knowledge model was used to weakly supervise the learning of the AU detector. An AUbased FER model performed expression recognition from AU detection results; it was mainly introduced to assist the model integration process. The three neural network models were initially trained independently and they were then refined jointly until convergence. It should be mentioned that this work does not cover valence-arousal estimation. Moreover, it utilizes only highly controlled databases for training and evaluation; there is no proof that it can be applied effectively to real world in-the-wild data cases. In addition, it does not generate a single network, but three distinct ones; having high time and space requirements and high computational complexity.\n\nAnother work with a goal similar to ours is  [28] , where a unified model performing facial action unit detection, expression classification, and valence-arousal estimation was proposed as part of the ABAW Competition at IEEE FG 2020 1 . The authors used the Aff-Wild2 database  [14] ,  [29]  that contains annotations for all 3 tasks. However not all images were annotated for all tasks. To tackle the incomplete labels' problem, the authors at first trained a teacher multi-task model using only the given complete labels.\n\nThen by testing that network on the database, they generated annotations (soft labels) for the missing labels. Finally they trained a student multi-task network on the union of the original and soft labels; that network outperformed the teacher one. They also applied data balancing techniques and developed ensembles for further boosting the performance. The teacher model did not take into account the fact that the three tasks are interconnected -they simply used an overall loss equal to the sum of the independent task losses. Thus, the student model did not learn this relatedness. It should also be added that this work utilized and was evaluated on only one database, i.e., the Aff-Wild2; the soft labels generated by the teacher model were not reliable.\n\nIn  [30]  a multi-task and multi-modal network was proposed for valence-arousal estimation, action unit detection and seven basic expression classification. The authors utilized a 3D ResNet for processing the image modality (video frames) and a ResNet for processing the audio modality (Mel-spectrograms). The features from these two networks were concatenated and fed to a fully connected layer that provided the final estimates for the three tasks. However, this work was trained with and was evaluated on the Aff-Wild2 database, utilizing only the overlapping and complete annotations.\n\nIn  [31] , a two-level attention with two stage multi-task learning framework was constructed for emotion recognition and valencearousal estimation. In the first attention level, a CNN extracted position-level features and then in the second an RNN with selfattention was proposed to model the relationship between layerlevel features. This work utilized the AffectNet database  [11] , annotated for both tasks and thus only containing overlapping and complete annotations. In addition, this work did not tackle the action unit detection task.\n\n1. https://ibug.doc.ic.ac.uk/resources/fg-2020-competition-affectivebehavior-analysis/",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Approach",
      "text": "Let us consider a set of m tasks {T i } m i=1 . In task T i , the observations are generated by the underlying distribution D i over inputs X and their labels Y associated with the task. For the i-th task T i , the training set D i consists of n i data points (x i j , y i j ), j = 1, . . . , n i with x i j ∈ R d and its corresponding output y i j ∈ R if it is a regression task, or y i j ∈ {0, 1} if it is a binary classification task, or y i j ∈ {0, 1} k (one-hot encoding) if it is a (mutually exclusive) k-class classification task.\n\nThen, the goal of MTL is to find m hypothesis: h 1 , ..., h m over the hypothesis space H to control the average expected error over all tasks: 1 m m i=1 E (x,y)∼Di L(h i (x), y) with L being the loss function. We can also define a weight w i ∈ ∆ m , {w i } m i=1 > 0 to govern the contribution of each task. Then the overall loss is:\n\nIf it is a regression task, the loss L can take a form of MAE, MSE or a correlation-based loss. If it is a binary classification task, the loss L can be binary/sigmoid cross entropy loss. If it is a (mutually exclusive) k-class classification task, the loss L can be softmax cross entropy loss. In the case of a neural network the hypothesis can be expressed as f ({θ}, x) where {θ} denotes the set of weights of the neural network learned during training.\n\nIn the following, we present the proposed framework via two examined case studies. The framework includes inferring the relationship between the tasks (either via domain knowledge or dataset annotation) and using it for coupling them during MTL. The coupling is achieved via the proposed co-annotation and distribution matching losses. These losses can be incorporated and used in any deep neural network that performs MTL.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Case Study I: Affective Computing",
      "text": "We start with the multi-task formulation of the facial behavior model. In this model we have three objectives: (1) learning seven basic emotions, (2) detecting activations of 17 binary facial action units, (3) learning the intensity of the valence and arousal continuous affect dimensions. We train a multi-task neural network model to jointly perform (1)-  (3) . For a given image x ∈ X , we can have label annotations of either one of seven basic emotions y emo ∈ {1, 2, . . . , 7}, or 17 2 binary action units activations y au ∈ {0, 1} 17 , or two continuous affect dimensions, valence and arousal, y va ∈ [-1, 1] 2 . For simplicity of presentation, we use the same notation x for all images leaving the context to be explained by the label notations. We train the multi-task model by minimizing the following objective:\n\nwhere the first term is the cross entropy loss computed over images with a basic emotion label, the second term is the binary cross entropy loss computed over images with TABLE 1: Relatedness between: i) basic emotions and their prototypical and observational AUs from  [15] : the weights w in brackets correspond to the fraction of annotators that observed the AU activation; ii) basic emotions and AUs, inferred from Aff-Wild2: the weights w in brackets correspond to the percentage of images annotated with the specific expression in which the AU was activated.\n\nCognitive-Psychological Study  [15]  Empirical where δ i ∈ {0, 1} indicates whether the image contains annotation for AU i . The third term measures the concordance correlation coefficient between the ground truth valence and arousal y va and the predicted ȳva , CCC(y va , ȳva ) = ρa+ρv",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "2",
      "text": ", where for i ∈ {v, a}, y i is the ground truth, ȳi is the predicted value and",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Task-Relatedness",
      "text": "In the seminal work of  [15] , the authors conduct a study on the relationship between emotions (basic and compound) and facial action unit activations. The summary of the study is a Table of the emotions and their prototypical and observational action units, which we include in Table  1  for completeness. Prototypical action units are ones that are labelled as activated across all annotators' responses, observational are action units that are labelled as activated by a fraction of annotators. For example, in emotion happiness the prototypical are AU12 and AU25, the observational is AU6 with weight 0.51 (observed by 51% of the annotators). Table  1  provides the relatedness between emotion categories and action units obtained from this cognitive and psychological study with human participants.\n\nAlternatively we can infer task relatedness from external dataset annotations. In particular, we use the Aff-Wild2 database, which is the first in-the-wild database that contains annotations for all three behavior tasks to infer task relatedness. The dataset is fully annotated with basic emotions and continual emotions of valence and arousal, and a subset of it is annotated with action units. We first train a network for AU detection on the union of Aff-Wild2 and GFT databases  [32] , and use this network to automatically annotate Aff-Wild2 with AUs. Table  1  shows the distribution of AUs for each basic expression that we use as task relatedness for distribution matching. In parenthesis next to each AU is the percentage of images annotated with the specific expression in which this AU was activated. In the following, we use the domain knowledge, specifically the cognitive and psychological study  [15] , to encode task relatedness and introduce the proposed approach for coupling the tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Coupling Of Basic Emotions And Aus",
      "text": "Via Co-annotation. We propose a simple strategy of coannotation to couple the training of emotions and action unit predictions. Given an image x with the ground truth basic emotion y emo , we enforce the prototypical and observational AUs of this emotion to be activated. We co-annotate the image (x, y emo ) with y au ; this image contributes to both L Emo and L AU 3  in eq. 2. We re-weight the contributions of the observational AUs with the annotators' agreement score (from Table  1 ).\n\nSimilarly, for an image x with the ground truth action units y au , we check whether we can co-annotate it with an emotion label. For an emotion to be present, all its prototypical and observational AUs have to be present. In cases when more than one emotion is possible, we assign the label y emo of the emotion with the largest requirement of prototypical and observational AUs. The image (x, y au ) that is co-annotated with the emotion label y emo contributes to both L AU and L Emo in eq. 2. We call this approach the FaceBehaviorNet with co-annotation.\n\nVia Distribution Matching. The aim here is to align the predictions of the emotions and action units tasks during training. For each sample x we have the predictions of emotions p(y emo |x) as the softmax scores over seven basic emotions and we have the prediction of AUs activations p(y i au |x), i = 1, . . . , 17 as the sigmoid scores over 17 AUs.\n\nThe distribution matching idea is simple: we match the distribution over AU predictions p(y i au |x) with the distribution q(y i au |x), where the AUs are modeled as a mixture over the basic emotion categories:\n\nwhere p(y i au |y emo ) is defined deterministically from Table  1  and is 1 for prototypical/observational action units, or 0 otherwise. For example, AU2 is prototypical for emotion surprise and observational for emotion fear and thus q(y AU2 |x) = p(y surprise |x) + p(y fear |x)  4  .\n\nThis matching aims to make the network's predicted AUs consistent with the prototypical and observational AUs of the network's predicted emotions. So if, e.g., the network predicts the emotion happiness with probability 1, i.e., p(y happiness |x) = 1, then the prototypical and observational AUs of happiness -AUs 12, 25 and 6-need to be activated in the distribution q:\n\nIn spirit of the distillation approach  [7] , we match the distributions p(y i au |x) and q(y i au |x) by minimizing the cross entropy with the soft targets loss term 5 :\n\nwhere all available training samples are used to match the predictions. We call this approach FaceBehaviorNet with distrmatching.\n\nVia Mixing the two strategies, co-annotation and distribution matching. A mix of the two strategies, co-annotation and distribution matching, is also possible. Given an image x with the ground truth annotation of the action units y au , we can first co-annotate it with a soft label in form of the distribution over emotions and then match it with the predictions of emotions p(y emo |x).\n\nMore specifically, at first we compute, for each basic emotion, an indicator score, I(y emo |x) over its prototypical and observational AUs being present:\n\nau is prototypical for y emo (from Table  1 ) w, au is observational for y emo (from Table  1 ) 0, otherwise For example, for emotion happiness, the indicator score\n\nor all weights equal 1 if without reweighting. Then, we convert the indicator scores to probability scores over emotion categories; this soft emotion label, q(y emo |x), is computed as following:\n\ny emo e I(y emo |x) , {y emo , y emo } ∈ {1, . . . , 7}\n\nIn this variant, every single image that has ground truth annotation of AUs will have a soft emotion label assigned. Finally we match the predictions p(y emo |x) and the soft emotion label q(y emo |x) by minimizing the cross entropy with the soft targets loss term:\n\nWe call this approach FaceBehaviorNet with soft co-annotation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Coupling Of Categorical Emotions, Aus With Continuous Affect",
      "text": "In our work, continuous affect (valence and arousal) is implicitly coupled with the basic expressions and action units via a joint training procedure. Also one of the datasets we used has annotations for categorical and continuous emotions (AffectNet  [11] ). Studying an explicit relationship between them is a novel research direction beyond the scope of this work.\n\n5. This can be seen as minimizing the KL-divergence KL(p||q) across the 17 action units.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Compound Expressions: Zero-And Few-Shot Learning",
      "text": "Knowledge Generalization in Zero-shot Learning After Face-BehaviorNet was trained (either without or with any coupling loss or any combination of them), it was able to effectively capture and solve the three behavior tasks of valence-arousal estimation, action unit detection and basic expression classification. Face-BehaviorNet has learned features that encapsulate all aspects of facial behavior. Therefore, exploiting this fact, we describe how FaceBehaviorNet can generalize its knowledge in other emotion recognition contexts, in a zero-shot manner. For this, we use the predictions of FaceBehaviorNet together with the rules from  [15]  to generate compound emotion predictions. We compute a candidate score, C s (y c-emo ), for each class y c-emo :\n\nI au expresses FaceBehaviorNet's predictions of only the prototypical (and observational) AUs that are associated with this compound class according to  [15] . In this manner, every AU acts as an indicator for this particular emotion class. This terms describes the confidence (probability) of AUs that this compound emotion is present.\n\nF emo expresses FaceBehaviorNet's predictions of only the basic expression classes emo1 and emo2 that are mixed and form the compound class (e.g., if the compound class is happily surprised then emo1 is happy and emo2 is surprised).\n\nD va is added only to the happily surprised and happily disgusted classes and is either 0 or 1 depending on whether FaceBe-haviorNet's valence prediction is negative or positive, respectively. The rationale is that, from all compound classes, only happily surprised and happily disgusted classes have positive valence. All other compound classes are expected to have negative valence as they correspond to negative emotions.\n\nThe final prediction is the class that obtained the maximum candidate score.\n\nRobust Prior for Few-Shot Learning By having learned complex and emotionally rich features, FaceBehaviorNet can constitute a robust prior for compound emotion recognition, especially for datasets that are quite small in terms of size. Up to now, there exist only a couple of in-the-wild datasets annotated in terms of compound expressions and contain less than 5K images in total. Therefore FaceBehaviorNet can be used as a pre-trained network and can be further fine-tuned (either with freezing some of its parts or not) to perform compound emotion classification.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Case Study Ii: Face Recognition",
      "text": "We start with the multi-task formulation of the facial behavior model. In this model we have two objectives: (1) detecting activations of 40 binary facial attributes, (2) learning to classify 10, 177 identities (ids). We train a multi-task neural network model to jointly perform (1) and (2). For a given image x ∈ X , we can have label annotations of either one of 10, 177 ids y id ∈ {1, . . . , 10177}, or 40 binary facial attributes y attr ∈ {0, 1} 40 . For simplicity of presentation, we use the same notation x for all images leaving the context to be explained by the label notations. We train the multi-task model by minimizing the following objective:\n\n10177\n\ny i attr log p(y i attr |x)\n\nwhere the first term is the cross entropy loss computed over images with an id label and p(y id |x) is the prediction of the id that corresponds to the positive class; the second term is the binary cross entropy loss computed over images with 40 attribute activations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task-Relatedness",
      "text": "There has been no relatedness study, like the one of Table  1 , between facial attributes and ids. Therefore we inferred it empirically from dataset annotations. In particular, we used the CelebA database, which contains overlapping annotations for facial attributes and identities. In other words, each image in the database is annotated both in terms of facial attributes and identities. We calculated the distribution of attributes for each identity; more precisely, for each identity (id) and for each attribute (attr), we divided the number of images in which the attribute attr existed, by the total amount of images that the specific identity id existed; as a result, we created the corresponding percentages. Table  2  illustrates the relationship between some identities and some of the 40 facial attributes. It can be seen that identities 1 and 5000 are male, whereas 2, 1000, 10000 and 10177 are female. All the images in CelebA where these 4 femals are displayed, show them young; some images in CelebA of the two male indentities show them young and some others show them old. These two male did not wear either lipstick, or necklace, in all of their images. The four female were wearing lipstick and necklace in some of their images, but not in others. Neither of the four female had a 5 o' clock shadow, whereas some images of the male had. All four female were highly attractive, whereas the two male were not that much. Mostly the two male had bags under their eyes in the images; one of them was bald in some images (probably when he was old) and he was wearing a necktie (no female was wearing a necktie).",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Coupling Of Ids And Attributes Via Distribution Matching",
      "text": "Since the database that we utilized contained complete and overlapping annotations, we only propose the distribution matching loss for coupling the facial attributes' and ids' predictions. The aim is similar to the one defined and explained in Section 3.2.2; we want to align the predictions of the ids and attributes tasks during training. For each sample x we have the predictions of ids p(y id |x) as the softmax scores over 10,177 identities; we also have the prediction of attribute activations p(y i attr |x), i = 1, . . . , 40, as sigmoid scores, over 40 AUs.\n\nAt first, we model the attributes as a mixture over the identities, creating distribution q(y i attr |x): q(y i attr |x) = y id ∈{1,...,10177}\n\np(y id |x) p(y i attr |y id ),\n\nwhere p(y i attr |y id ) is defined in Table  2 . For example, for the '5 o' clock' attribute: q(y 1 attr |x) = 0.34483 • p(y id 1 |x) + 0.8 • p(y i 5000 |x), assuming that for all other ids: p(y i attr |y id ) = 0. Next, we match the distributions p(y i attr |x) and q(y i attr |x) by minimizing the cross entropy with the soft targets loss term:\n\nwhere all available training samples are used to match the predictions. We call this approach FaceBehaviorNet with distr-matching.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Facebehaviornet Structure",
      "text": "Fig.  1  shows the structure of the holistic (multi-task, multi-domain and multi-label) FaceBehaviorNet, which is based on residual units. 'bn' stands for batch normalization, the convolution layer is in the format: filter height × filter width 'conv.', number of output feature maps; the stride is equal to 2, everywhere. In the affective computing case study: a (linear) output layer that gives final estimates for valence and arousal; it also gives 7 basic expression logits that are passed through a softmax function to get the final 7 basic expression predictions; lastly, it gives 17 AU logits that are passed through a sigmoid function to get the final 17 AU predictions. One can see that the predictions for all tasks are pooled from the same feature space. Fig.  1  illustrates FaceBe-haviorNet for that case. In the case study of face recognition, the (linear) output layer gives 10, 177 id logits that are passed through a softmax function to get the final id predictions and it also gives 40 attribute logits that are passed through a sigmoid function to get the final attribute predictions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Study",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Databases",
      "text": "Let us first describe the databases that we utilized in all our experiments. In the affective computing case study, we selected to The EmotioNet database  [35]  contains around 1M images and was released for the EmotioNet Challenge in 2017  [36] . 950K images were automatically annotated and the remaining 50K images were manually annotated with 11 AUs  (1, 2, 4, 5, 6, 9, 12, 17, 20, 25, 26) ; around half of the latter constituted the validation and the other half the test set of the Challenge. Additionally, a subset of about 2.5K images was annotated with the 6 basic and 10 compound emotions. The DISFA database  [37]  is a lab controlled database consisting of 27 videos each of which has 4,845 frames, where each frame is coded with the AU intensity on a six-point discrete scale. AU intensities equal or greater than 2 are considered as occurrence, while others are treated as non-occurrence. There are in total 12 AUs  (1, 2, 4, 5, 6, 9, 12, 15, 17, 20, 25, 26) . The GFT database consists of 96 videos of 96 subjects totalling around 130K frames. It is annotated for the occurrence of 10 AUs  (1, 2, 4, 6, 10, 12, 14, 15, 23, 24) . The training set consists of 78 subjects of around 108K frames and the test set of 18 subjects of around 24.5K frames.\n\nThe BP4D-Spontaneous database  [38]  (in the rest of the paper we refer to it as BP4D) contains 61 subjects with 223K frames and is annotated for the occurrence and intensity of  27   Here let us note that for AffectNet, BP4D and BP4D+, no test set is released; thus we use the released validation set to test on and divide the training set into subject independent training and validation subsets.\n\nIn the face recognition case, we utilized the CelebA database. It contains 202,600 images of 10,177K identities (celebrities), each with 40 attribute annotations. It contains around 160K images in the training, 20K in the validation and 20K in the test set. These sets are subject independent. For our experiment, we generated a new split into three subject dependent sets; each set contained images from all 10,177 identities.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Performance Measures",
      "text": "We use: i) the CCC for Aff-Wild and Aff-Wild2 (CCC was the evaluation criterion of the respective Challenges), Affectnet and AFEW-VA, ii) the mean diagonal value of the confusion matrix for RAF-DB (this criterion was selected for evaluating the performance on this database by  [34] ); the accuracy for AffectNet, iii) the F1 score for DISFA, GFT, BP4D and BP4D+ (this metric was the evaluation criterion of the FERA 2015 and 2017 Challenges); for AU detection in EmotioNet the Challenge's metric was the average between: a) the mean (across all AUs) F1 score and b) the mean (across all AUs) accuracy; for the expression classification, it was the average between: a) the average (across all emotions) F1 score and b) the unweighted average recall (UAR) over all emotion categories, iv) the total accuracy and average F1 score for the attributes and ids in CelebA.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Pre-And Post-Processing",
      "text": "Data Cleaning: We performed data cleaning on the AffectNet database that contains overlapping annotations for valence-arousal and 7 basic expressions. We removed images for which there was a mismatch between the values of valence and arousal and the discrete expressions. In more detail: i) images annotated as neutral should have radius of the valence-arousal vector smaller than 0.15; ii) images annotated as sad or disgusted or fearful should have negative valence; iii) images annotated as angry should have negative valence and positive arousal; iv) images annotated as happy should have positive valence.\n\nData Subsampling: Aff-Wild is a video database with consecutive frames having the same (or very similar) values for both valence and arousal. However FaceBehaviorNet is a CNN that does not exploit the temporal dependencies between frames; thus in each video, for each frame that we kept, we skipped the following four.\n\nFace Pre-Processing: We used the SSH detector  [42]  based on ResNet and trained on the WiderFace dataset  [43]  to extract, from all images, face bounding boxes and 5 facial landmarks; the latter were used for face alignment. In the CelebA case we used the aligned data distributed with the database. All cropped and aligned images were resized to 112 × 112 × 3 pixel resolution and their intensity values were normalized to [-1, 1]. Prediction Post-Processing: Because Aff-Wild is a video database and FaceBehaviorNet is a CNN that does not exploit the temporal dependencies between frames, we performed median filtering of the -per frame -predictions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Training Implementation Details",
      "text": "At this point let us describe the strategy that was used, in the affect computing task, for feeding images from different databases to FaceBehaviorNet during training. At first, the training set was split into three different sets, each of which contained images that were annotated in terms of either valence-arousal, or action units, or seven basic expressions; let us denote these sets as VA-Set, AU-Set and EXPR-Set, respectively. During training, at each iteration, three batches, one from each of these sets (as can be seen in Fig.  1 ), were concatenated and fed to FaceBehaviorNet. This step is important for network training, because: i) the network minimizes the objective function of eq. 2; at each iteration, the network has seen images from all categories and thus all loss terms contribute to the objective function, ii) since the network sees an adequate number of images from all categories, the weight updates (during gradient descent) are not based on noisy gradients; this in turn prevents poor convergence behaviors; otherwise, we would need to tackle these problems, e.g. do asynchronous SGD as proposed in  [3]  to make the task parameter updates decoupled, iii) the CCC cost function (defined in Section 3) needs an adequate sequence of predictions.\n\nSince VA-Set, AU-Set and EXPR-Set had different sizes, they needed to be 'aligned'. To do so, we selected the batches of these sets in such a manner, so that after one epoch we will have sampled all images in the sets. In particular, we chose batches of size 200, 124 and 52 for the AU-Set, VA-Set and EXPR-Set, respectively. The training of FaceBehaviorNet was performed in an end-toend manner, using the Momentum optimizer with 0.9 momentum and a learning rate of 10 -4 . Training was performed on a Tesla V100 32GB GPU; training time was about 2 days. The TensorFlow platform has been used.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Case Study I: Affective Computing",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results: Ablation Study",
      "text": "First, we compared the performance of FaceBehaviorNet when trained with the losses of eq. 2 and: i) without using the coupling losses described in Section 3, ii) with co-annotation coupling loss, iii) with soft co-annotation coupling loss, iv) with distr-matching coupling loss and v) with soft co-annotation and distr-matching coupling losses. Table  3  shows the results for all these approaches, when the task relatedness was drawn from the cognitive study, or when it was inferred empirically from Aff-Wild2.\n\nMany deductions can be made. Firstly, when FaceBehaviorNet was trained with a coupling loss, or with any combination of coupling losses, it displayed a better performance than when trained with no coupling loss. This holds on all databases and in both task relatedness scenarios. This validates the fact that the proposed losses helped to couple the three studied tasks regardless of which relatedness scenario was followed. Secondly, the performance in estimation of valence and arousal was improved, although we did not explicitly designed a coupling loss for this; we only coupled emotion categories and action units. We conjecture that when action unit detection and expression classification accuracy improves (due to coupling), valence and arousal performance also improves, because valence and arousal are implicitly coupled with emotions via joint dataset annotations.\n\nFinally, in both scenarios, across all databases, best results have been achieved when FaceBehaviorNet was trained with both soft co-annotation and distr-matching losses. In particular, in both settings, an average performance increase of more than 2.5% has been observed when using both coupling losses, compared to the cases when only one of them was used. One can also observe that when task relatedness was inferred from Aff-Wild2, FaceBehaviorNet -trained with both coupling losses-displayed an average performance gain of 0.5% in expression classification and 0.5% in AU detection compared to the case when task relatedness was inferred from the cognitive study of  [15] .\n\nNext, we utilized three state-of-the-art and broadly used networks, VGG-FACE  [44] , ResNet-50  [45]  and DenseNet-121  [46] . We trained these networks in a multi-task manner (without using any coupling loss) with all the databases described in Section 4.1 and compared their performance to that of FaceBehaviorNet (trained without any coupling loss). As shown in Table  4 , the Face-BehaviorNet has proven to provide the best results, outperforming the MT-VGG-FACE by 1.6%, MT-ResNet by 1.4% and the MT-DenseNet by 4%, on average (across all databases' metrics).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results: Comparison With State-Of-The-Art And Single-Task Methods",
      "text": "Next, we trained a single-task FaceBehaviorNet (ST-FaceBehaviorNet) on all dimensionally annotated databases, so as to predict valence and arousal; we also trained another similar network on all categorically annotated databases, to perform seven basic expression classification; finally we trained a third similar network on all databases annotated with action units, so as to perform AU detection. All these networks were based on residual units and had the same structure as FaceBehaviorNet; their only difference was the output layer. For brevity, these three single-task networks are denoted as '(3 ×) ST-FaceBehaviorNet' in one row of Table  5 .\n\nWe compared these networks' performance with the performance of FaceBehaviorNet when trained with and without the TABLE 3: Performance evaluation of valence-arousal, seven basic expression and action units predictions on all used databases provided by the FaceBehaviorNet when trained with/without the coupling losses, under the two task relatedness scenarios; 'AFA Score' is the average between the F1 Score and the Accuracy  coupling losses. We also compared them with the performance of the state-of-the-art (sota) methodologies for each utilized database: i) the best performing CNN (VGG-FACE)  [47]  [9] on Aff-Wild; ii) the best performing network (AffWildNet)  [47]  [9] on Aff-Wild; iii) the baseline networks (AlexNet)  [11]  on AffectNet (in Table  5  they are denoted as '(2 ×) AlexNet' as they are two different networks: one for VA estimation and another for expression classification); iv) the state-of-the-art VGG-FACE  [48]  for VA estimation on AffectNet; v) the state-of-the-art RAN-ResNet18 +  [49]  for expression classification on AffectNet; vi) the VGG-FACE-mSVM  [34]  on RAF-DB; vii) the best performing network (DLP-CNN)  [34]  on RAF-DB; viii) the baseline network (AlexNet)  [36]  on EmotioNet ; ix) the winner of EmotioNet Challenge and best performing network (ResNet-34)  [50]  on Emo-tioNet; x) the state-of-the-art network (LP-Net)  [52]  on DISFA; xi) the best performing network (LP-Net)  [52]  on DISFA; xii) the winner of FERA 2015, DLE extension  [53]  on BP4D; xiii) the winner of FERA 2017 (VGG-FACE)  [54]  on BP4D+; xiv) the best performing network (ARL)  [55]  on BP4D+. Table  5  displays the performance of all these networks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Non-Coupled Mtl Vs Sota & St:",
      "text": "In Table  5  it can be seen that when no coupling loss is used for training FaceBehaviorNet, the network outperforms the state-of-the-art in BP4D+ by 5%, BP4D by 25% for AU detection and in AffectNet by 2% for arousal estimation. In all other databases it displays either a slightly worse performance (1% in three databases), or worse performance (3%, 5% and 7% in three other databases). Table  5  additionally illustrates a comparison between the three single task ST-FaceBehaviorNet and (multi-task) FaceBehaviorNet trained without any coupling loss. It can be observed that the multi-task network displays a better performance for AU detection and VA estimation, but an inferior one for expression classification. The latter indicates that negative transfer occurs in the case of basic expressions. This negative transfer effect was caused by the fact that some of the related tasks -valence-arousal and action units -dominated the training process. It should be mentioned that these two tasks included more data, i.e., images and corresponding annotations, than the expression recognition task. In particular, there were twice more data in VA and three times more data in AUs compared to basic expressions. Negative transfer largely depends on the size of labeled data per task  [19] . In fact, the amount of labeled data per task has a direct effect on the feasibility and reliability of discovering shared regularities between the joint distributions of the tasks in MTL.\n\nA way of overcoming negative transfer in expression recognition would be to change (i.e., increase) the lambda value in the expression loss (that controls the relative importance of the task), or decrease the lambda values in the VA and AU losses in the total loss function of eq. 2 that is minimized by the multitask model during training. However, this could severely affect the performance of the other tasks. Furthermore, this lambda hyperparameter tuning is a computationally expensive procedure, which lasts many days for each trial. It should be added that this is an ad-hoc methodology which does not guarantee to work on other tasks, or in other databases. In order to balance the performance on many tasks,  [20]  proposed an iterative method which uses the training loss of each task to indicate whether it is well trained or not, and then decreases the relative weights of the well trained tasks. The problem with this approach is that it is based on costly evaluation of performance indicators during each training iteration and that this is performed in a rather task-agnostic way.\n\nNegative transfer may be induced by conflicting gradients among the different tasks  [56] . Searching for Pareto solutions  [57]  could remedy this.  [57]  tackled multi-task learning problems through multi-objective optimization, with decomposition of the problem into a set of constrained sub-problems with different trade-off preferences (among different tasks). However, this approach is rather complex, providing a finite set of solutions that do not always satisfy the MTL requirements and finally need to perform trade-offs among tasks.\n\nThrough the proposed coupling loss, knowledge of the task relationship was infused in network training, thus providing it, in a simple manner, with higher level representation of the relationship between the tasks; it was not based on performance indicators and it did not perform any trade-offs between the different tasks.\n\nThe fact that the proposed coupling losses tackled the negative transfer is illustrated by the performance shown in Tables  5  and 3 , where FaceBehaviorNet trained with the proposed coupling losses outperformed by a large margin both the independently trained single task networks and the multi-task network trained without any coupling loss.\n\nCoupled MTL vs Non-coupled MTL, ST & Sota: In Table  5 , it can be observed that FaceBehaviorNet, when trained with the two coupling losses (in either task relatedness setting), outperformed FaceBehaviorNet trained without any coupling loss by: 9% (average CCC) on Aff-Wild; 8.5% (average CCC) and 6% (accuracy) on AffectNet; 11% on RAF-DB; 5% on EmotioNet; 8% on DISFA; 7% on BP4D and 6% on BP4D+. It further outperformed by a large margin the three ST-FaceBehaviorNet networks (10.3% on average across all databases, with the minimum difference in performance being 5% and the maximum 22%). Finally, in Table  5 , it can be observed that FaceBehaviorNet, when trained with the two coupling losses (in either task relatedness setting), outperformed by a very large margin every state-of-the-art method in all databases in all tasks. More precisely, it outperformed: i. AffWildNet by 8% (average CCC) for VA estimation on Aff-Wild; this is despite the fact that AffWildNet is a CNN-RNN that exploited the fact that Aff-Wild is an audio-visual database and despite the fact that facial landmarks were provided as additional inputs to the network, thus improving its performance; it also outperformed the best performing CNN in Aff-Wild by 16% ii. VGG-FACE by 7.5% (average CCC) for VA estimation on AffectNet, although VGG-FACE was trained with the original images plus thousands of generated images by VA-StarGAN; it also outperformed the AlexNet baseline by 15%\n\niii. RAN-ResNet18 + by 5% for expression classification on Af-fectNet, despite the fact that the network was trained with a region based loss that encouraged a high attention weight for the most important regions in the input images; it also outperformed AlexNet (that used a weighted loss that heavily penalized the network for misclassifying examples from under-represented classes and penalized the network less for misclassifying examples from well-represented classes), by 7% iv. DLP-CNN by 4% for expression classification on RAF-DB, although this network was trained using a joint classical softmax loss -which forced different classes to stay apart -and a newly created loss -that pulled the locally neighboring faces of the same class together; it also outperformed VGG-FACE-mSVM, that used the standard cross entropy loss, by 20% v. ResNet-34 by 4% for AU detection on EmotioNet; it also outperformed AlexNet by 16% vi. both JAA-Net and LP-Net by 6% and 5%, respectively, for AU detection on DISFA, despite the fact that these networks have additionally used facial landmarks as additional inputs, thus improving their performance vii. DLE extension by 34% for AU detection on BP4D viii. ARL by 11% for AU detection on BP4D+; it also outperformed VGG-FACE by 14%\n\nAt this point let us mention that in our approach the loss functions that we utilized (for expression classification and for AU detection) were standard losses (binary and softmax cross entropy, respectively). As was shown above, in most state-ofthe-art approaches, more elaborate and advanced loss functions have been used. We could utilize more elaborate and advanced losses as well, but the focus of this work has been on the coupling between the tasks exploiting their relationship -inferred either empirically from external dataset annotations, or from a cognitive study; therefore we chose not to use such losses whose specific selection could affect the presented analysis and the obtained results. As a future work, we will incorporate more advanced losses in the multi-task learning setting, which will further enhance the achieved results.\n\nIt might be argued that the more data used for network training (i.e., the additional data coming from the multiple tasks that are being solved, even if they contain partial or non-overlapping annotations), the better network performance will be in all tasks. However, as was shown, the three studied tasks are heterogeneous and negative transfer can occur, or sub-optimal models can be produced for some, or even all tasks  [58] . As shown in Table  5 , FaceBehaviorNet, when trained with both proposed coupling losses, achieved a better performance on all databases than the independently trained single-task models. This illustrates that simultaneously training this end-to-end architecture with the heterogeneous databases and coupling the corresponding tasks and annotations, led to improved performance. The fact that the network additionally outperformed the state-of-the-art methods, in both task relatedness settings, verified the generality of the proposed losses; network performance was boosted, independently of the type of task relatedness that was used.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results: Generalisation To Unseen Databases",
      "text": "At this point let us mention that each task and corresponding database contains ambiguous cases: i) there is generally discrepancy in the perception of the disgust, fear, sadness and (negative) surprise emotions across different people (of different ethnicity, race and age) and across databases; emotional displays and their perception are not universal, i.e., facial expressions are displayed and interpreted differently depending on the cultural background of subjects and annotators  [59] ,  [60]  ; ii) the exact valence and arousal value for a particular affect is also not consistent among databases; iii) the AU annotation process is hard to do and error prone, creating incosistency among databases (e.g., regarding the dependencies among AUs, such as AU12 -lip corner puller-and AU15 -lip corner depressor-that cannot co-occur as their corresponding muscle groups -Zgomaticus major and Depressor anguli oris, respectively-are unlikely to be simultaneously activated).\n\nFurthermore each database contains its own bias. The bias is either in terms of the race, ethnicity and ages of the subjects displayed in the database or the race, ethnicity, age and culture of the experts that performed the annotations (having their own bias in judging the depicted affect). When an affect recognition system is trained on one database, then the system inherently learns the bias towards facial displays present in the training data. Therefore, when the system is tested on another, new and unseen database (with other demographics and statistics), its performance is not as good. An example is the AffectNet database that contains images of European Americans and has been annotated mainly by European Americans. Another example is GFT database that mainly contains videos of white and black people.\n\nThe excellent generalization performance of FaceBehaviorNet -across the test sets-of the 7 databases whose training sets have been used for its training, is an indicator that the network and the proposed coupling losses tackled the aforementioned issues. By jointly training on all databases and by coupling the tasks, we overcame these limitations as shown in the extensive experimental study across the 7 databases. In order to further illustrate and validate that FaceBehaviorNet learned good and robust features, we show that it is capable of generalizing its knowledge and capabilities in other new and unseen affect recognition databases that have not been utilized during its training and contain different statistics and contexts.\n\nTable  6  compares the performance of FaceBehaviorNet when trained with both coupling losses with the performance of state-ofthe-art networks on two new databases. It is worth mentioning that these two databases, AFEW-VA and GFT, have not been utilized during FaceBehaviorNet's training and no fine-tuning was performed; FaceBehaviorNet was just tested on the new and unseen databases. It can be observed that FaceBehaviorNet outperformed both AffWildNet (that was further trained on AFEW-VA) and JAA-Net (that was trained on GFT) by 4% (on average) and 5%, respectively. In order to further show and validate that FaceBehaviorNet learned good features encapsulating all aspects of facial behavior, we conducted zero-and few-shot learning experiments for classifying compound expressions. Given that there exist only 2 datasets (EmotioNet and RAF-DB) annotated with compound expressions and that they do not contain a lot of samples (less than 5,000 each), at first, we used the predictions of FaceBehaviorNet together with the rules from  [15]  to generate compound emotion predictions in a zero-shot learning manner, as was described in Section 3.1.4. Additionally, to demonstrate the superiority of FaceBehaviorNet, we used it as a pre-trained network in a few-shot learning experiment. We took advantage of the fact that our network has learned good features and used them as priors for fine-tuning the network to perform compound emotion classification.\n\nRAF-DB database At first, we performed zero-shot experiments on the 11 compound categories of RAF-DB. We computed a candidate score, C s (y emo ), for each class y emo as shown in Section 3.1.4. Table  7  shows the results of this approach when we used the predictions of FaceBehaviorNet trained with and without the soft co-annotation and distr-matching losses. Best results have been obtained when the network was trained with the coupling losses. One can observe, that this approach outperformed by 15.1% the baseline VGG-FACE-mSVM  [34]  that been trained on RAF-DB for compound emotion classification. It also outperformed by 2.1% the state-of-the-art and best performing network, DLP-CNN that has also been trained on RAF-DB for compound emotion classification; DLP-CNN used a loss function designed for this specific task. Next, we targeted few-shot learning. In particular, we finetuned FaceBehaviorNet (trained with and without the soft coannotation and distr-matching losses) on the small training set of RAF-DB. In Table  7 , it can be seen that the fine-tuned FaceBehaviorNet, trained with and without the coupling losses, outperformed by large margins, 23.7% and 10.7%, respectively, the baseline VGG-FACE-mSVM and the state-of-the-art and best performing DLP-CNN.\n\nEmotioNet database Next, we performed zero-shot experiments on the EmotioNet basic and compound set that was released for the related Challenge. This set includes 6 basic plus 10 compound categories, as described at the beginning of this Section. Our zeroshot methodology was similar to the one described above for the RAF-DB database.\n\nThe results of this experiment are shown in Table  7 . Best results were also obtained when the network was trained with the two coupling losses. It can be observed that this approach outperformed by 5.7% and 8.6% in F1 score and Unweighted Average Recall (UAR), respectively, the state-of-the-art and winner of EmotioNet Challenge, NTechLab's  [36]  approach, which used the Emotionet's images with compound annotation.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Case Study Ii: Face Recognition",
      "text": "At first, we trained a single-task FaceBehaviorNet (ST-FaceBehaviorNet) on CelebA to detect the 40 facial attributes; we also trained another similar network on CelebA to perform classification into the 10,177 different identities. The networks were based on residual units and had the same structure as FaceBehaviorNet; their only difference was the output layer. For brevity, these two single-task networks are denoted as '(2 ×) ST-FaceBehaviorNet' in one row of Table  8 . We compared these networks' performance with the performance of FaceBehaviorNet when trained with the distribution matching coupling loss and when trained without the coupling loss. Table  8  shows that the multi-task FaceBehaviorNet, when trained without the coupling loss, outperformed the 2 single-task networks ST-FaceBehaviorNet in both studied tasks and in both metrics (Total Accuracy and average F1 Score). In more detail, it displayed an improved performance by 2.28% and 2.1% in the Accuracy and F1 Score metrics, respectively, for identity classification; it also displayed an improved performance by 1.91% and 0.89% in the Accuracy and F1 Score metrics, respectively, for attribute detection. This shows that the two studied facial heterogeneous tasks were coherently correlated to each other; training the end-to-end multi-task architecture therefore, led to improved performance and no negative transfer occurred.\n\nTable  8  further shows that the multi-task FaceBehaviorNet, when trained with the distribution matching coupling loss, greatly outperformed its counterpart that was trained without that loss, in both studied tasks and in both metrics. More precisely, when training with the coupling loss, performance increased by 4.57% and 5.9% in the Accuracy and F1 Score metrics, respectively, for identity classification; performance also increased by 1.24% and 2.3% in the Accuracy and F1 Score metrics, respectively, for attribute detection. This proves the effectiveness of the proposed distribution matching loss.\n\nNext, we utilized, as an ablation study, three state-of-theart and widely used networks, VGG-FACE, ResNet-50 and DenseNet-121. We trained these networks in a multi-task manner (without using the coupling loss) on CelebA and compared their performance to that of FaceBehaviorNet that was also trained without coupling loss (for a fair comparison). As shown in Table  8 , FaceBehaviorNet has proven to be the best architecture as it provided the best results, outperforming on all tasks and metrics the MT-VGG-FACE by at least 2%, the MT-ResNet by at least 1.8% and the MT-DenseNet by at least 4%.\n\nFinally, let us mention that the achieved performance of FaceBehaviorNet trained with the distribution matching loss for attribute detection was higher than the performance of various state-of-the-art methodologies on the same database. FaceBehav-iorNet reached an accuracy of 93.22%, whereas DMTL  [61]  achieved 92.6% accuracy, MCNN-AUX  [62]  achieved an accuracy of 91.29% and Face-SSD  [25]  achieved an accuracy of 90.29%. These comparisons are provided just as an indication; they are not direct comparisons.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we target heterogeneous MTL, i.e., simultaneously addressing detection, classification and regression problems. We propose co-training task in a weakly-supervised way, by exploring their relatedness. Relatedness between the heterogeneous tasks is either provided explicitly in a form of expert knowledge, or is inferred based on empirical studies. In co-training, the related tasks exchange their predictions and iteratively teach each other so that predictors of all tasks can excel even if we have limited or no data for some of them. We propose an effective distribution matching approach based on distillation, where knowledge exchange between tasks is enabled via distribution matching over their predictions. Based on this approach, we build the first holistic framework for large-scale face analysis, FaceBehaviorNet, with case studies in affective computing and in face recognition.\n\nIn the first case study, FaceBehaviorNet is trained for joint basic expression recognition, action unit detection and valencearousal estimation. All publicly available databases that study facial behavior tasks in-the-wild, have been utilized. In the latter case study, FaceBehaviorNet is trained for joint facial attribute detection and face identification. An extensive experimental study -across 10 databases-is performed that compares the performance of the holistic (multi-task, multi-domain, multi-label) FaceBehav-iorNet (trained with and without taking into account the taskrelatedness) to the performance of the single-task networks, as well as to the performance of the state-of-the-art. FaceBehaviorNet consistently outperformed, by a large margin, all of them ,in all databases, in both case studies, even mitigating bias and tackling negative transfer. Finally, we explored the feature representation learned by FaceBehaviorNet in the joint training and showed its generalization abilities on the task of compound expressions, under zero-shot and few-shot learning settings.",
      "page_start": 12,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The holistic (multi-task, multi-domain, multi-label)",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: provides the relatedness between emotion categories and au",
      "data": [
        {
          "happiness": "sadness",
          "12, 25": "4, 15",
          "6 (0.51)": "1 (0.6), 6 (0.5), 11 (0.26), 17 (0.67)",
          "12 (0.82), 25 (0.7), 6 (0.57), 7 (0.83), 10 (0.63)": "4 (0.53), 15 (0.42), 1 (0.31), 7 (0.13), 17 (0.1)"
        },
        {
          "happiness": "fear",
          "12, 25": "1, 4, 20, 25",
          "6 (0.51)": "2 (0.57), 5 (0.63), 26 (0.33)",
          "12 (0.82), 25 (0.7), 6 (0.57), 7 (0.83), 10 (0.63)": "1 (0.52), 4 (0.4), 25 (0.85), 5 (0.38), 7 (0.57), 10 (0.57)"
        },
        {
          "happiness": "anger",
          "12, 25": "4, 7, 24",
          "6 (0.51)": "10 (0.26), 17 (0.52), 23 (0.29)",
          "12 (0.82), 25 (0.7), 6 (0.57), 7 (0.83), 10 (0.63)": "4 (0.65), 7 (0.45), 25 (0.4), 10 (0.33), 9 (0.15)"
        },
        {
          "happiness": "surprise",
          "12, 25": "1, 2, 25, 26",
          "6 (0.51)": "5 (0.66)",
          "12 (0.82), 25 (0.7), 6 (0.57), 7 (0.83), 10 (0.63)": "1 (0.38), 2 (0.37), 25 (0.85), 26 (0.3), 5 (0.5), 7 (0.2)"
        },
        {
          "happiness": "disgust",
          "12, 25": "9, 10, 17",
          "6 (0.51)": "4 (0.31), 24 (0.26)",
          "12 (0.82), 25 (0.7), 6 (0.57), 7 (0.83), 10 (0.63)": "9 (0.21), 10 (0.85), 17 (0.23), 4 (0.6), 7 (0.75), 25 (0.8)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: For example, for the",
      "data": [
        {
          "# 1": "# 2",
          "0.34483": "0.",
          "0.03449": "0.125",
          "0.27586": "0.875",
          "0.4138": "0.125",
          "0.": "0.",
          "1.": "0.",
          "0.68966": "1."
        },
        {
          "# 1": "# 1000",
          "0.34483": "0.",
          "0.03449": "0.22727",
          "0.27586": "1.",
          "0.4138": "0.",
          "0.": "0.",
          "1.": "0.",
          "0.68966": "1."
        },
        {
          "# 1": "# 5000",
          "0.34483": "0.8",
          "0.03449": "0.1",
          "0.27586": "0.4",
          "0.4138": "0.6",
          "0.": "0.1",
          "1.": "1.",
          "0.68966": "0.5"
        },
        {
          "# 1": "# 10000",
          "0.34483": "0.",
          "0.03449": "0.26667",
          "0.27586": "1.",
          "0.4138": "0.",
          "0.": "0.",
          "1.": "0.",
          "0.68966": "1."
        },
        {
          "# 1": "# 10177",
          "0.34483": "0.",
          "0.03449": "0.07692",
          "0.27586": "0.69231",
          "0.4138": "0.",
          "0.": "0.",
          "1.": "0.",
          "0.68966": "1."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Performance comparison of different widely used",
      "data": [
        {
          "Aff-Wild": "AffectNet",
          "CCC-V\nCCC-A": "CCC-V\nCCC-A\nF1 Score",
          "0.52\n0.38": "0.56\n0.50\n0.59",
          "0.55\n0.36": "0.56\n0.46\n0.53",
          "0.54\n0.38": "0.53\n0.48\n0.54",
          "0.52\n0.35": "0.53\n0.44\n0.52"
        },
        {
          "Aff-Wild": "RAF-DB",
          "CCC-V\nCCC-A": "Mean\ndiag. of\nconf. matrix",
          "0.52\n0.38": "0.67",
          "0.55\n0.36": "0.67",
          "0.54\n0.38": "0.67",
          "0.52\n0.35": "0.64"
        },
        {
          "Aff-Wild": "Emotionet",
          "CCC-V\nCCC-A": "AFA Score",
          "0.52\n0.38": "0.72",
          "0.55\n0.36": "0.72",
          "0.54\n0.38": "0.71",
          "0.52\n0.35": "0.69"
        },
        {
          "Aff-Wild": "DISFA",
          "CCC-V\nCCC-A": "F1 score",
          "0.52\n0.38": "0.54",
          "0.55\n0.36": "0.52",
          "0.54\n0.38": "0.52",
          "0.52\n0.35": "0.49"
        },
        {
          "Aff-Wild": "BP4D",
          "CCC-V\nCCC-A": "F1 score",
          "0.52\n0.38": "0.76",
          "0.55\n0.36": "0.70",
          "0.54\n0.38": "0.73",
          "0.52\n0.35": "0.68"
        },
        {
          "Aff-Wild": "BP4D+",
          "CCC-V\nCCC-A": "F1 score",
          "0.52\n0.38": "0.56",
          "0.55\n0.36": "0.57",
          "0.54\n0.38": "0.56",
          "0.52\n0.35": "0.54"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: Performance evaluation of valence-arousal, seven basic expression and action units predictions on all utilized databases",
      "data": [
        {
          "best performing CNN [47] [9]": "AffWildNet [47] [9]",
          "0.51": "0.57",
          "0.33": "0.43",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "(2 × ) AlexNet [11]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "VGG-FACE [48]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "RAN-ResNet18+ [49]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "VGG-FACE-mSVM [34]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "DLP-CNN [34]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "AlexNet [36]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "ResNet-34 [50]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "JAA-Net [51]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "LP-Net [52]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "DLE extension [53]",
          "0.51": "-",
          "0.33": "-",
          "-": "-"
        },
        {
          "best performing CNN [47] [9]": "VGG-FACE [54]",
          "0.51": "-",
          "0.33": "-",
          "-": "0.48"
        },
        {
          "best performing CNN [47] [9]": "ARL [55]",
          "0.51": "-",
          "0.33": "-",
          "-": "0.51"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Performance evaluation of valence-arousal, seven basic expression and action units predictions on all utilized databases",
      "data": [
        {
          "FaceBehaviorNet, no coupling loss": "FaceBehaviorNet, soft co-annotation\nand distr-matching, [15]",
          "0.56\n(0.52)": "0.67\n(0.63)",
          "0.42\n(0.38)": "0.48\n(0.44)",
          "0.56": "0.60",
          "0.50": "0.58",
          "0.59": "0.65",
          "0.67": "0.77",
          "0.72": "0.77",
          "0.54": "0.60",
          "0.76": "0.85"
        },
        {
          "FaceBehaviorNet, no coupling loss": "FaceBehaviorNet, soft co-annotation\nand distr-matching, Aff-Wild2",
          "0.56\n(0.52)": "0.67\n(0.63)",
          "0.42\n(0.38)": "0.49\n(0.45)",
          "0.56": "0.62",
          "0.50": "0.57",
          "0.59": "0.65",
          "0.67": "0.78",
          "0.72": "0.77",
          "0.54": "0.62",
          "0.76": "0.83"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Holistic 3d scene understanding from a single geo-tagged image",
      "authors": [
        "S Wang",
        "S Fidler",
        "R Urtasun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "An all-in-one convolutional neural network for face analysis",
      "authors": [
        "R Ranjan",
        "S Sankaranarayanan",
        "C Castillo",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "3",
      "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
      "authors": [
        "I Kokkinos"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Taskonomy: Disentangling task transfer learning",
      "authors": [
        "A Zamir",
        "A Sax",
        "W Shen",
        "L Guibas",
        "J Malik",
        "S Savarese"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "6",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "7",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "8",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "9",
      "title": "Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2009",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference"
    },
    {
      "citation_id": "11",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2009",
      "venue": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "arxiv": "arXiv:1708.03985"
    },
    {
      "citation_id": "12",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2002",
      "venue": "Proceedings of IEEE International Conference on Computer Vision & Pattern Recognition (CVPR'16)"
    },
    {
      "citation_id": "13",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "T Baltrušaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "14",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "15",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2009",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "16",
      "title": "Do deep neural networks learn facial action units when doing expression recognition",
      "authors": [
        "P Khorrami",
        "T Paine",
        "T Huang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "17",
      "title": "Emotion categories and dimensions in the facial communication of affect: An integrated approach",
      "authors": [
        "M Mehu",
        "K Scherer"
      ],
      "year": "2015",
      "venue": "Emotion"
    },
    {
      "citation_id": "18",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2002",
      "venue": "Proceedings of International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "19",
      "title": "Characterizing and avoiding negative transfer",
      "authors": [
        "Z Wang",
        "Z Dai",
        "B Póczos",
        "J Carbonell"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Loss-balanced task weighting to reduce negative transfer in multi-task learning",
      "authors": [
        "S Liu",
        "Y Liang",
        "A Gitter"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "From emotions to action units with hidden and semi-hidden-task learning",
      "authors": [
        "A Ruiz",
        "J Van De Weijer",
        "X Binefa"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Multiple facial action unit recognition enhanced by facial expressions",
      "authors": [
        "J Yang",
        "S Wu",
        "S Wang",
        "Q Ji"
      ],
      "year": "2016",
      "venue": "2016 23rd International Conference on Pattern Recognition (ICPR"
    },
    {
      "citation_id": "23",
      "title": "Expression-assisted facial action unit recognition under incomplete au annotation",
      "authors": [
        "S Wang",
        "Q Gan",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1997",
      "venue": "Machine learning"
    },
    {
      "citation_id": "25",
      "title": "Registration-free face-ssd: Single shot analysis of smiles, facial attributes, and affect in the wild",
      "authors": [
        "Y Jang",
        "H Gunes",
        "I Patras"
      ],
      "year": "2019",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "26",
      "title": "Multitask deep neural network for joint face recognition and facial attribute prediction",
      "authors": [
        "Z Wang",
        "K He",
        "Y Fu",
        "R Feng",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "27",
      "title": "Knowledge augmented deep neural networks for joint facial expression and action unit recognition",
      "authors": [
        "Z Cui",
        "T Song",
        "Y Wang",
        "Q Ji"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "D Deng",
        "Z Chen",
        "B Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "29",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "30",
      "title": "Two-stream aural-visual affect analysis in the wild",
      "authors": [
        "F Kuhnke",
        "L Rumberg",
        "J Ostermann"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "31",
      "title": "Two-level attention with two-stage multi-task learning for facial emotion recognition",
      "authors": [
        "X Wang",
        "M Peng",
        "L Pan",
        "M Hu",
        "C Jin",
        "F Ren"
      ],
      "year": "2018",
      "venue": "Two-level attention with two-stage multi-task learning for facial emotion recognition",
      "arxiv": "arXiv:1811.12139"
    },
    {
      "citation_id": "32",
      "title": "Sayette group formation task (gft) spontaneous facial expression database",
      "authors": [
        "J Girard",
        "W.-S Chu",
        "L Jeni",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "33",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "34",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Emotionet challenge: Recognition of facial expressions of emotion in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "Q Feng",
        "Y Wang",
        "A Martinez"
      ],
      "year": "2017",
      "venue": "Emotionet challenge: Recognition of facial expressions of emotion in the wild",
      "arxiv": "arXiv:1703.01210"
    },
    {
      "citation_id": "37",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on"
    },
    {
      "citation_id": "38",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "39",
      "title": "Fera 2015-second facial expression recognition and analysis challenge",
      "authors": [
        "M Valstar",
        "T Almaev",
        "J Girard",
        "G Mckeown",
        "M Mehu",
        "L Yin",
        "M Pantic",
        "J Cohn"
      ],
      "year": "2015",
      "venue": "Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on"
    },
    {
      "citation_id": "40",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Fera 2017-addressing head pose in the third facial expression recognition and analysis challenge",
      "authors": [
        "M Valstar",
        "E Sánchez-Lozano",
        "J Cohn",
        "L Jeni",
        "J Girard",
        "Z Zhang",
        "L Yin",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "42",
      "title": "SSH: Single stage headless face detector",
      "authors": [
        "M Najibi",
        "P Samangouei",
        "R Chellappa",
        "L Davis"
      ],
      "year": "2017",
      "venue": "The IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "43",
      "title": "Wider face: A face detection benchmark",
      "authors": [
        "S Yang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "44",
      "title": "Deep face recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "45",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference"
    },
    {
      "citation_id": "48",
      "title": "Va-stargan: Continuous affect generation",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "49",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "50",
      "title": "Facial action recognition using very deep networks for highly imbalanced class distribution",
      "authors": [
        "W Ding",
        "D.-Y Huang",
        "Z Chen",
        "X Yu",
        "W Lin"
      ],
      "year": "2017",
      "venue": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "51",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "52",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "53",
      "title": "Discriminant multi-label manifold embedding for facial action unit detection",
      "authors": [
        "A Yüce",
        "H Gao",
        "J.-P Thiran"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "54",
      "title": "View-independent facial action unit detection",
      "authors": [
        "C Tang",
        "W Zheng",
        "J Yan",
        "Q Li",
        "Y Li",
        "T Zhang",
        "Z Cui"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "55",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "Y Wu",
        "L Ma"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "56",
      "title": "Gradient surgery for multi-task learning",
      "authors": [
        "T Yu",
        "S Kumar",
        "A Gupta",
        "S Levine",
        "K Hausman",
        "C Finn"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Pareto multi-task learning",
      "authors": [
        "X Lin",
        "H.-L Zhen",
        "Z Li",
        "Q Zhang",
        "S Kwong"
      ],
      "year": "2019",
      "venue": "Thirty-third Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "58",
      "title": "Understanding and improving information transfer in multi-task learning",
      "authors": [
        "S Wu",
        "H Zhang",
        "C Ré"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "59",
      "title": "Universality reconsidered: Diversity in making meaning of facial expressions",
      "authors": [
        "M Gendron",
        "C Crivelli",
        "L Barrett"
      ],
      "year": "2018",
      "venue": "Current directions in psychological science"
    },
    {
      "citation_id": "60",
      "title": "A comparative analysis of emotion-detecting ai systems with respect to algorithm performance and dataset diversity",
      "authors": [
        "D Bryant",
        "A Howard"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "61",
      "title": "Heterogeneous face attribute estimation: A deep multi-task learning approach",
      "authors": [
        "H Han",
        "A Jain",
        "F Wang",
        "S Shan",
        "X Chen"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "62",
      "title": "Attributes for improved attributes: A multi-task network for attribute classification",
      "authors": [
        "E Hand",
        "R Chellappa"
      ],
      "year": "2016",
      "venue": "Attributes for improved attributes: A multi-task network for attribute classification",
      "arxiv": "arXiv:1604.07360"
    }
  ]
}