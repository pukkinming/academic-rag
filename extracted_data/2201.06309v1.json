{
  "paper_id": "2201.06309v1",
  "title": "Group Gated Fusion On Attention-Based Bidirectional Alignment For Multimodal Emotion Recognition",
  "published": "2022-01-17T09:46:59Z",
  "authors": [
    "Pengfei Liu",
    "Kun Li",
    "Helen Meng"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "attention models",
    "information fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a challenging and actively-studied research area that plays a critical role in emotion-aware humancomputer interaction systems. In a multimodal setting, temporal alignment between different modalities has not been well investigated yet. This paper presents a new model named as Gated Bidirectional Alignment Network (GBAN), which consists of an attention-based bidirectional alignment network over LSTM hidden states to explicitly capture the alignment relationship between speech and text, and a novel group gated fusion (GGF) layer to integrate the representations of different modalities. We empirically show that the attention-aligned representations outperform the last-hidden-states of LSTM significantly, and the proposed GBAN model outperforms existing state-ofthe-art multimodal approaches on the IEMOCAP dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a core component in any emotion-aware human-computer interaction system, such as an intelligent virtual assistant, and an affective spoken dialog system. An emotion recognizer typically analyzes speech, text or images. For example, a speech emotion recognizer aims to identify the emotion carried in speech, often in terms of a set of emotion categories such as happy, angry, sad and neutral  [1] [2] [3] [4] [5] . However, this is a nontrivial task because emotions are manifested in various factors such as conversational discourse, linguistic content and prosodic features  [6] [7] [8] . It is difficult to predict the emotion of an utterance based only on the acoustic features from speech, or only on the discrete word sequence in spoken text. Furthermore, there is a lack of large-scale emotive datasets of speech or text, as these are costly and difficult to collect and label. One should therefore explore the use of multiple modalities for emotion recognition.\n\nMultimodal emotion recognition has received a lot of attention in recent years  [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] . Modalities such as speech, text and/or images have been exploited for better recognition performance. Existing approaches to multimodal learning are either early-fusion which fuses low-level feature interactions between different modalities before making a prediction decision; or late-fusion which models each modality independently and combines the decisions from each model  [13] . Recent earlyfusion approaches have focused on different mathematical formulations to fuse acoustic and lexical features, such as multimodal pooling fusion  [13] , modality hierarchical fusion  [16] , conversational neural network  [18] , word-level concatenation of acoustic and lexical features  [8] , etc. To capture inter-modality dynamics in multimodal sentiment analysis, Zadeh et al.  [12]  introduces the Tensor Fusion Network (TFN) model using tensor fusion to explicitly aggregate unimodal, bimodal and trimodal interactions between different features. Arevalo et al.  [19]  presents a Gated Multimodal Unit (GMU) model to find an intermediate representation based on a combination of features from different modalities, where it learns to decide how modalities influence the activation of the unit using multiplicative gates. These methods are generally applicable in information fusion from different sources. However, they do not take into account the particular temporal correlation property of speech and text in an utterance, where a sequence of speech frames are aligned with a sequence of words temporally.\n\nGiven a sequence of speech frames, it is beneficial to know the corresponding words to learn a more discriminative representation for emotion recognition, and vice versa. For example, consider a speaker expressing happiness in the utterance \"That's great!\", and anger in \"That's unfair!\". An emotion recognizer should learn to pay more attention to the words great and unfair and their corresponding speech frames, as there is complementary information across modalities for emotion recognition. This calls for a mechanism to capture the alignment between words and their speech frames. Yoon et al.  [14]  presents a deep dual recurrent neural network to encode information from speech and text sequences, and captures the alignment by a dot product attention to focus on specific words of a text sequence conditioned on speech for emotion classification. Similarly, Xu et al.  [17]  learns the alignment between speech and text using an additive attention, and adopts an LSTM layer fed with the concatenation of the aligned speech representation and the hiddenstate text representation for emotion classification. However, both approaches  [14, 17]  consider only unidirectional alignment, and we will show empirically that bidirectional alignment brings better recognition performance. Furthermore, the concatenation-based fusion methods in  [14, 17]  may not have sufficient expressive power to exploit the complementary information across modalities, which in turn reflects the need for more effective ways to fuse multiple representations.\n\nIn this paper, we propose a model named Gated Bidirectional Alignment Network (GBAN), which consists of a bidirectional attention-based alignment network to capture the alignment information between speech and text, and a novel group gated fusion (GGF) layer to automatically learn the contribution of each modality in determining the final emotion category. We summarize the major contributions of this paper as follows:\n\n(1) Experimental results show that the proposed bidirectional alignment network leads to more discriminative representations of speech and text for emotion recognition;\n\n(2) The proposed GGF method allows integration of multiple representations effectively, and obtains interpretable contribution weights of each modality in emotion recognition; arXiv:2201.06309v1 [cs.CL] 17 Jan 2022\n\n(3) The proposed GBAN model outperforms existing state-ofthe-art multimodal approaches  [14, 17]  using both speech and text on the IEMOCAP dataset  [20] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Gated Bidirectional Alignment Network",
      "text": "We propose an approach named Gated Bidirectional Alignment Network (GBAN) for multimodal emotion recognition. As illustrated in Figure  1 , the GBAN model consists of three major parts:  (1)",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cnn-Lstm Encoders",
      "text": "We adopt two separate CNN-LSTM encoders for speech and text respectively, where CNN layers extract local features and a bidirectional LSTM layer captures the global dependencies over time in either the speech acoustics or the text of an utterance. Speech Representation. The speech signal in an utterance is represented as a sequence of vectors [x1, . . . , xN ], where N is the number of frames in the utterance. First, the sequence is fed into a CNN layer, which consists of convolution and max-pooling operations, to obtain a sequence of pooled vectors [p1, . . . , pK ]. Then, a bidirectional LSTM layer follows the CNN layer to obtain a vector si for each i ∈ {1, . . . , K}, which is concatenated from the forward and backward LSTMs. Thus, the obtained sequence of [s1, . . . , sK ] keeps the temporal ordering for each i ∈ {1, . . . , K}, and will be used as the speech representation for alignment with the text representation. In practice, multiple CNN layers and bidirectional LSTM layers are adopted to encode the low-level speech signals.\n\nText Representation. For text encoding, each sentence is represented as a sequence of vectors [e1, . . . , eM ], where M is the number of words in a sentence, and ej is the word embeddings of the jth word. Similar to the process in speech representation, a CNN layer is first used to obtain a sequence of pooled vectors [q1, . . . , qL]. For each j ∈ {1, . . . , L}, a bidirectional LSTM layer follows to obtain the concatenation vector tj from the forward and backward LSTMs. The sequence of [t1, . . . , tL], which are essentially text features transformed from the sequence of M words, keeps the temporal ordering for each j ∈ {1, . . . , L}, and will be used as the text representation for alignment with the speech representation of [s1, . . . , sK ].",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Bidirectional Alignment Network",
      "text": "An utterance is made up of a speech signal and a text sequence (transcribed by a person or a speech recognizer), where the speech and the text are sequentially correlated over time. To capture their temporal correlations, we propose a bidirectional alignment network to learn the representation of one modality with the help of the other based on attention models. Attention-Aligned Speech Representation. Given the speech representation [s1, . . . , sK ] and the text representation [t1, . . . , tL] obtained from the previous CNN-LSTM encoders for an utterance, we calculate the attention weight between the ith speech vector and the jth text vector as follows:\n\nwhere sj is the weighted summation of the speech vectors, which is considered as an aligned speech vector corresponding to the jth text vector. Finally, we apply an average-pooling operation on the sequence of aligned vectors to obtain the text-aligned speech representation as = average-pooling([ s1, . . . , sL]). Attention-Aligned Text Representation. We apply a similar process to learn the attention-aligned text representation.\n\nGiven the text representation [t1, . . . , tL] and the speech representation [s1, . . . , sK ] from the CNN-LSTM encoders for an utterance, we calculate the attention weight between the jth text vector and the ith speech vector as follows:\n\nbi,j = tanh(t j si)\n\nwhere ti is the weighted summation of the text vectors, which is considered as an aligned text vector corresponding to the ith speech vector. Finally, we place an average-pooling layer to obtain the speech-aligned text representation at = average-pooling([ t1, . . . , tK ]).",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Group Gated Fusion Layer",
      "text": "For the speech modality, we have obtained two representations: as as the text-aligned speech representation and hs as the last hidden state of the BiLSTM layer. For the text modality, we have another two similar representations: at and ht. To exploit these representations, we design a novel group gated fusion (GGF) layer to learn the contribution of each representation automatically. Considering different grouped learning processes, we make as and at as the first group, hs and ht as the second group and design the two separate gates corresponding to each group. The first gate controls contributions of the aligned representations as and at, while the second gate controls contributions of the last hidden states hs and ht. The equations governing the GGF layer are as follows:\n\nHere, W a s , W a t , W h s , W h t are the weights for the non-linear (i.e., tanh) transformations from as, at, hs, ht respectively. W p z , W q z are used to learn the contribution of each modality within each group. σ is the sigmoid function and means element-wise product. h is obtained by summing the gated representations of the two groups, and as the final representation for the following emotion classification layer.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Classification Layer",
      "text": "Given the obtained representation h, we first apply a fullyconnected layer with rectified linear units (ReLUs) for nonlinear transformation g, and use a softmax output layer to get ỹ for emotion classification of an utterance. The training objective L is to minimize the negative log-likelihood, where N is the total samples in training, C is the total number of emotion classes and yi,c = 1 if the ground-truth label is c else 0.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "For model evaluation, we conducted 5-fold cross validation on the Interactive Emotional Dyadic Capture (IEMOCAP) dataset  [20] , which consists of five sessions with one male and one female speaker each. We used 4 sessions as training set 1   1 We randomly select 5% of the utterances as the validation set.\n\nand the remaining session as testing set to ensure speaker independence. To stay consistent with most previous investigations on IEMOCAP, we use the subset covering the four emotional categories of happy, angry, sad and neutral.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech And Text Features",
      "text": "We extract both mel-spectrogram and MFCCs as the acoustic features from the speech modality. Each frame in the speech utterance corresponds to a feature vector consisting of 26-dimensional mel-spectrograms, 13-dimensional MFCCs and their first-order deltas, leading to a 52-dimensional vector. Following  [21] , we set the maximal length of a speech utterance to 7.5s, with longer utterances cut at 7.5s and shorter ones padded with zeros.\n\nFor each utterance in the IEMOCAP dataset, there is a corresponding human transcription, which can also be obtained by an automatic speech recognizer  [14, 17]  in the deployed emotion recognition systems. Both word-level and character-level embeddings can be used to represent the textual transcriptions. We adopt word embeddings to represent each word within an utterance in the IEMOCAP dataset, and initialize the embeddings with the pre-trained 300-dimensional Glove vectors  [22] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Settings And Metrics",
      "text": "We initialize all the network weights in the GBAN model with Xavier normal initializer  [23] , and use the Adam  [24]  optimizer by setting the learning rate as 0.0001. To alleviate overfitting, we put a dropout layer  [25]  with a rate of 0.5 in the GGF layer and before the output softmax layer, and set the coefficient of L2 regularization over the network weights as 0.01.\n\nWe adopt two widely used metrics for evaluation: weighted accuracy (WA), which is the overall classification accuracy and unweighted accuracy (UA), which is the average recall over the emotion categories. We first compute the metrics for each fold and then present the average accuracy over all the folds.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparison Of Representations",
      "text": "Using the CNN-LSTM encoders, the last hidden state of the bidirectional LSTM layer can be used to represent a speech utterance as hs and the corresponding text as ht. Adopting the bidirectional alignment network, we obtained the attentionaligned representations as for speech and at for text. We evaluate their discriminative power in emotion classification using the same classification layer (See Section 2.4). The accuracy (WA) comparison among the representations on the five folds is shown in Table  1 . We observe that the attention-aligned representations outperform the last-hidden-state representations significantly on all folds for both speech and text. Since both as and at take additional information from the other modality, this may explain why they outperform their counterparts. Another interesting observation is that at outperforms all other representations. This verifies our hypothesis that the alignment direction is important for the learned representation. The reason that at outperforms as may be attributed to the different lengths of speech frames and text sequence for the same utterance, where the number of frames can go up to 750 frames while the text sequence may consist of around 20 words. Since the speech sequence is much longer, the attention-aligned speech representation as is not as effective as its counterpart at. A similar observation is also reported in  [21] , where the attention mechanism brings slight improvements for speech emotion recognition on the improvised subset of IEMOCAP.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Comparison Of Information Fusion Methods",
      "text": "Given different representations obtained from speech and text, the next interesting question to ask is: \"How to make use of all the representations to achieve better performance?\" Various information fusion methods can be adopted, such as simple concatenation, tensor fusion network  [12]  (TFL), gated multimodal units  [19]  (GMU) and the proposed group gated fusion (GGF) layer. Performance comparisons among these fusion methods on the IEMOCAP dataset are shown in Table  2 .\n\nOn average, GGF obtains the best accuracy and outperforms all other methods on the folds of 2, 4 and 5. An interesting observation is that both Concat-1 and Concat-2 are strong baselines, although outperformed by TFL and GMU with (as, at) as the input, and further outperformed by GGF given all the four representations (as, at, hs, ht). Since the IEMOCAP dataset is relatively small, the simple concatenation method may even have enough capacity to learn the underlying patterns. GGF computes a sigmoid weight over the non-linear transformation of the representations within each group, which may be the key to its superior performance to all other methods.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison With Existing Approaches",
      "text": "We compare the proposed GBAN model with the existing published approaches in Table  3 . They all perform 5-fold cross validation in a speaker-independent manner by using four sessions as training set and the whole remaining session as test set.\n\nAll the three speech-only models (i.e., CNN-Att  [21] , LSTM-Att  [26]  and Self-Att  [27] ) adopt attention mechanism based on CNN, LSTM and self-attention, respectively. In multimodal emotion recognition, we implement all the models marked with * for fair comparisons using the same experimental settings. We observe that the multimodal approaches using both speech and text generally outperform the speech-only approaches. Note that the Att-LSTM model, proposed by Xu et al.  [17]  is outperformed by all other models based on bidirectional aligned representations (BiAtt), since it adopts only a unidirectional alignment between speech and text. Finally, the pro- posed GBAN model obtains the best performance in terms of both WA and UA on the IEMOCAP dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Weights Analysis",
      "text": "We further investigate the behavior of the proposed GBAN model by analyzing the weights of zp and zq in the GGF layer, which determine the contribution of each representation (i.e., as, at, hs and ht) in emotion classification. Figure  2  shows the average weight of each representation for emotion classification on the 5-fold experiments of the GBAN model. We observe that both at and ht have consistently higher weights within their respective groups (i.e., Group-1 and Group-2) on all the five folds, indicating that the text modality contribute more in emotion classification. This observation is in line with Section 3.3, which shows that both ht and at are more discriminative in emotion classification. Besides, this analysis also illustrates the interpretable benefit of the proposed GGF layer, which allows to present the contribution of a particular modality in multimodal emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presents a model named Gated Bidirectional Alignment Network (GBAN) for multimodal emotion recognition, which consists of a novel attention-based bidirectional alignment network to exploit the alignment information between speech and text explicitly, and a new group gated fusion layer to learn the contribution of each representation from both modalities automatically. We show empirically that the bidirectional alignment network leads to more discriminative representations for emotion classification, and the group gated fusion layer fuses multiple representations effectively in an interpretable manner. GBAN outperforms existing state-of-the-art approaches in emotion classification on the IEMOCAP dataset.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the GBAN model consists of three major",
      "page": 2
    },
    {
      "caption": "Figure 1: The block diagram of the proposed GBAN model.",
      "page": 2
    },
    {
      "caption": "Figure 2: Average weight of each representation in emotion",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Group Gated Fusion Layer\n1-\n1-": "Bidirectional Alignment Network\nAttention Layer\nAttention Layer"
        },
        {
          "Group Gated Fusion Layer\n1-\n1-": "CNN-LSTM Encoder\nBiLSTM\nMax Pooling Layer\nConvolution Layer\nSpeech Features"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold": "1\n2\n3\n4\n5",
          "hs\nht": "0.5860\n0.6285\n0.6384\n0.7060\n0.5920\n0.5860\n0.6936\n0.6494\n0.6030\n0.6497",
          "as\nat": "0.6762\n0.6476\n0.7368\n0.6728\n0.6420\n0.6150\n0.7289\n0.6948\n0.6815\n0.6561"
        },
        {
          "Fold": "Avg",
          "hs\nht": "0.6226\n0.6439",
          "as\nat": "0.6931\n0.6573"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Comparison based on the IEMOCAP dataset",
      "data": [
        {
          "Fold": "",
          "Input (as, at)": "Concat-1\nTFL\nGMU",
          "Input (as, at, hs, ht)": "Concat-2\nGGF"
        },
        {
          "Fold": "1\n2\n3\n4\n5",
          "Input (as, at)": "0.6805\n0.6900\n0.6985\n0.7417\n0.7405\n0.7552\n0.6890\n0.6600\n0.6720\n0.7440\n0.7440\n0.7591\n0.6868\n0.7038\n0.6975",
          "Input (as, at, hs, ht)": "0.6996\n0.6911\n0.7737\n0.7540\n0.6720\n0.6770\n0.7654\n0.7478\n0.7123\n0.7017"
        },
        {
          "Fold": "Avg",
          "Input (as, at)": "0.7026\n0.7101\n0.7199",
          "Input (as, at, hs, ht)": "0.7239\n0.7150"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Steidl",
        "D Seppi"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from variable-length speech segments using deep learning on spectrograms",
      "authors": [
        "X Ma",
        "Z Wu",
        "J Jia",
        "M Xu",
        "H Meng",
        "L Cai"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Towards a small set of robust acoustic features for emotion recognition: challenges",
      "authors": [
        "M Tahon",
        "L Devillers"
      ],
      "year": "2015",
      "venue": "IEEE/ACM transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Which prosodic features contribute to the recognition of dramatic attitudes?",
      "authors": [
        "A Barbulescu",
        "R Ronfard",
        "G Bailly"
      ],
      "year": "2017",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "9",
      "title": "Multimodal and multi-view models for emotion recognition",
      "authors": [
        "G Aguilar",
        "V Rozgic",
        "W Wang",
        "C Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition from expressions in face, and body: the Multimodal Emotion Recognition Test (MERT)",
      "authors": [
        "T Bänziger",
        "D Grandjean",
        "K Scherer"
      ],
      "year": "2009",
      "venue": "Emotion"
    },
    {
      "citation_id": "11",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Bimodal speech emotion recognition using pre-trained language models",
      "authors": [
        "V Heusser",
        "N Freymuth",
        "S Constantin",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Bimodal speech emotion recognition using pre-trained language models",
      "arxiv": "arXiv:1912.02610"
    },
    {
      "citation_id": "13",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "14",
      "title": "Pooling acoustic and lexical features for the prediction of valence",
      "authors": [
        "Z Aldeneh",
        "S Khorram",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "15",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "16",
      "title": "Multimodal sentiment analysis: Addressing key issues and setting up the baselines",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "E Cambria",
        "A Gelbukh",
        "A Hussain"
      ],
      "year": "2018",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "17",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "18",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "20",
      "title": "Gated multimodal units for information fusion",
      "authors": [
        "J Arevalo",
        "T Solorio",
        "M Montes-Y Gómez",
        "F González"
      ],
      "year": "2017",
      "venue": "Gated multimodal units for information fusion",
      "arxiv": "arXiv:1702.01992"
    },
    {
      "citation_id": "21",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "22",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "23",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "24",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "25",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "26",
      "title": "Dropout: A simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "27",
      "title": "Context-aware attention mechanism for speech emotion recognition",
      "authors": [
        "G Ramet",
        "P Garner",
        "M Baeriswyl",
        "A Lazaridis"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "28",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "L Tarantino",
        "P Garner",
        "A Lazaridis"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    }
  ]
}