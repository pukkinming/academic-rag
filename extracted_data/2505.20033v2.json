{
  "paper_id": "2505.20033v2",
  "title": "Emonet-Face: An Expert-Annotated Benchmark For Synthetic Emotion Recognition",
  "published": "2025-05-26T14:19:58Z",
  "authors": [
    "Christoph Schuhmann",
    "Robert Kaczmarczyk",
    "Gollam Rabby",
    "Felix Friedrich",
    "Maurice Kraus",
    "Krishna Kalyan",
    "Kourosh Nadi",
    "Huu Nguyen",
    "Kristian Kersting",
    "Sören Auer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Effective human-AI interaction relies on AI's ability to accurately perceive and interpret human emotions. Current benchmarks for vision and vision-language models are severely limited, offering a narrow emotional spectrum that overlooks nuanced states (e.g., bitterness, intoxication) and fails to distinguish subtle differences between related feelings (e.g., shame vs. embarrassment). Existing datasets also often use uncontrolled imagery with occluded faces and lack demographic diversity, risking significant bias. To address these critical gaps, we introduce EMONET-FACE, a comprehensive benchmark suite. EMONET-FACE features: (1) A novel 40-category emotion taxonomy, meticulously derived from foundational research to capture finer details of human emotional experiences. (2) Three large-scale, AI-generated datasets (EMONET-FACE HQ, EMONET-FACE BINARY, and EMONET-FACE BIG) with explicit, full-face expressions and controlled demographic balance across ethnicity, age, and gender. (3) Rigorous, multi-expert annotations for training and high-fidelity evaluation. (4) We build EMPATHICINSIGHT-FACE, a model achieving human-expert-level performance on our benchmark. The publicly released EMONET-FACE suite-taxonomy, datasets, and model-provides a robust foundation for developing and evaluating AI systems with a deeper understanding of human emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The advent of highly capable Large Language Models (LLMs) such as ChatGPT  [34]  has transformed human-computer interaction, a transformation further accelerated by advances in voice synthesis and multimodal reasoning  [35] . Modern voice interfaces now enable emotionally expressive AI conversations  [44, 25] , leading to heightened expectations for AI systems with genuine emotional intelligence. As AI platforms like Replika  [29] , Character.ai  [9] , and Kajiwoto  [22]  are increasingly used for companionship and support  [10] , and as LLMs are consulted for mental health advice  [47, 23, 16, 46] , it becomes ever more critical for these systems to accurately interpret nonverbal emotional cues, especially facial expressions. Misreading users' emotions can erode trust and have negative psychological repercussions  [8] .\n\nAt the same time, as AI systems become more embodied-through avatars and realistically synthesized faces-another crucial challenge emerges: enabling humans to correctly perceive and interpret the emotional states of these digital agents. For emotionally rich and affective interaction, the expressions generated by machine avatars must be legible and interpretable to human users, ensuring users understand not only the intentionality but also the emotional stance and nuance behind an AI's responses. This reciprocity is critical for building trust, rapport, and effective communication in human-AI partnerships. Truly emotionally aware AI thus requires a bidirectional capability: machines must recognize not only basic emotions  [13]  but also complex and subtle affective states  [36, 12, 39] , while synthetic emotions generated by AI avatars must be expressively rich and clearly understood by human counterparts. However, current benchmarks are limited in size, their spectrum of emotions, the diversity of groups included, and the quality of their annotations, constraining progress toward AI systems capable of authentic, context-sensitive empathy and expressiveness  [11] .\n\nTo address these gaps, we introduce EMONET-FACE, an expert-annotated data suite for fine-grained facial emotion recognition grounded in a novel, expansive 40-category emotion taxonomy. This taxonomy, developed by mining the \"Handbook of Emotions\"  [27]  and refined with psychological expertise, captures a broader and more detailed array of human emotional states than previous efforts. Using state-of-the-art text-to-image models  [6, 32] , we generated a series of structured, demographically balanced, high-quality synthetic datasets, covering a wide range of age, ethnicity, and gender. This includes a pretraining set of over 203,000 images (EMONET-FACE BIG), a finetuning dataset (EMONET-FACE BINARY) with almost 20k images and more than 62,000 human expert binary emotion annotations, and an evaluation benchmark (EMONET-FACE HQ) of 2,500 images meticulously rated by psychology experts using continuous scales across all 40 emotion categories. Alongside these datasets, we present EMPATHICINSIGHT-FACE, two models trained on our suite that achieve human-expert-level performance on the EMONET-FACE HQ benchmark and outperform concurrent models.\n\nOur main contributions are as follows: (i) We introduce a comprehensive 40-category emotion taxonomy, refined through expert consultation, to capture fine-grained human emotional states;\n\n(ii) We construct diverse synthetic expert-annotated image datasets-spanning pretraining, finetuning, and benchmarking test sets-using state-of-the-art text-to-image models; (iii) We develop EMPATHICINSIGHT-FACE models that match human performance on fine-grained emotion recognition; (iv) We openly release our datasets and models to foster research on emotions in AI.  2  .",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Understanding human emotion remains a complex task, grounded in diverse psychological theories and demanding robust data for computational modeling. Emotions are challenging as they are not universal-they are socially constructed, shaped by cultural norms and context (further elaborated in  [3, 4]  and App. A.1). This section outlines key theoretical frameworks, examines existing emotion taxonomies and datasets, and identifies the specific gaps that EMONET-FACE addresses.\n\nExisting Emotion Taxonomies. Theories of emotion span a spectrum from universal, biologically rooted models to constructivist perspectives, with the latter emphasizing that emotions are shaped by context and culture rather than being innate, static programs  [3, 4] . This conceptual diversity has direct consequences for how emotions are categorized and labeled in practice. Traditional taxonomies provide important foundations but often lack granularity. Ekman's model expanded to include emotions like amusement and shame  [14] , but remains limited to discrete, universal states. Parrott's hierarchical structure  [38]  and Plutchik's Wheel  [40]  offer more coverage but overlook distinctions critical for real-world applications. For example, Plutchik's dyadic model emphasizes emotional opposites and blends but may oversimplify ambiguous affective states. Other approaches include Panksepp's neurobiologically rooted systems (e.g., SEEKING, FEAR, LUST)  [37] , and Izard's emphasis on distinct expressive and neural signatures for emotions like joy, guilt, and interest  [20, 21] .\n\nRecent efforts have enriched positive emotion taxonomies-e.g., Shiota et al.'s physiology-based framework  [45]  and Weidman and Tracy's focus on subjective feeling states  [48] . Despite these contributions, existing emotion taxonomies often omit physical states and stances like fatigue, pain, numbness, or sourness, and do not clearly distinguish closely related states such as gratitude, affection, and lust. EMONET-FACE addresses these gaps with a broader, literature-informed taxonomy  [27]  expanding basic emotions with relevant physical states and stances to a 40-category taxonomy, tailored for high-resolution facial emotion recognition.\n\nEmotion Recognition Datasets. Existing emotion recognition datasets vary significantly in terms of realism, diversity, and labeling accuracy. Early posed datasets (e.g., CK+  [28] , JAFFE  [30] ) rely on actor-performed facial expressions, offering clear labels but limited real-world validity. In-the-wild datasets (e.g., AffectNet  [33] , EmotioNet  [5] , EMOTIC  [24] , FindingEmo  [50] ) provide more realistic expressions with contextual richness. However, they suffer from confounders like context-dependent perception effects (e.g., Kuleshov effect  [7] , background assimilation  [49] ), as well as occlusions and inconsistent image quality. In contrast, EMONET-FACE overcomes these limitations by offering high-resolution, synthetic facial images rendered under controlled conditions. To this end, our dataset offers deliberate and precise control over demographic diversity and background context for every emotion, in contrast to previous datasets, which might achieve some diversity incidentally by web scraping. Additionally, it is unique in offering expert annotations across 40 emotion categories. A detailed comparison with previous datasets is presented in Table  1 .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Building Emonet-Face: Fine-Grained Expert Emotion Datasets",
      "text": "Having identified several critical gaps in existing emotion datasets-including limited taxonomic granularity, insufficient demographic and contextual diversity, image quality, and a lack of expert annotations-we turn next to how we constructed EMONET-FACE to address these challenges.\n\nIntroducing a Fine-grained Taxonomy To enable more fine-grained affective understanding in AI, we developed a 40-category emotion taxonomy grounded in contemporary psychology and informed by the Theory of Constructed Emotion (TCE)  [4] . Unlike prior work limited to basic emotions such as joy, anger, fear, sadness, and disgust, our taxonomy intentionally extends to a broad spectrum of fine-grained social, cognitive, and bodily states-including elation, contentment, gratitude, hope, pride, interest, awe, astonishment, relief, longing, teasing, embarrassment, shame, disappointment, contempt, sexual lust, doubt, confusion, envy, jealousy, bitterness, pain, fatigue, numbness, helplessness, and less common categories such as sourness and intoxication. Each category encompasses a cluster of semantically associated descriptive words, systematically extracted from the Handbook of Emotions  [27]  and refined via expert consultation (see App. Table  4  for full taxonomy).\n\nTo construct this taxonomy, we digitized the 946-page Handbook of Emotions via OCR, segmented the text into 500-word blocks, and used GPT-4 to extract candidate emotion nouns. After aggregation and  deduplication, we identified 170 unique terms, which were then clustered through iterative rounds of independent listing, critical review, and expert-guided refinement with psychologists and researchers.\n\nAligned with TCE, we do not claim biological universality; instead, our taxonomy is designed for context-aware, socially informed emotion interpretation in AI. Given the inherent ambiguity in facial emotion perception-e.g., high-arousal expression might plausibly reflect amusement, elation, or excitement-we designed the taxonomy for plausible multi-label annotations over rigid single-label assignments, supporting richer, context-aware emotion representations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Image Generation And Selection Methodology",
      "text": "To construct the EMONET-FACE datasets, we generated synthetic facial images representing all 40 emotion categories using a controlled promptengineering pipeline. Each image was manually screened for quality, ensuring visual consistency and rejecting those with artifacts or ambiguous expressions. Critically, prompt templates were explicitly designed to ensure demographic diversity by balancing gender identity (45% man, 45% woman, 10% non-binary), capturing a wide age range (20-80 years, in 10-year increments), and representing 14 different ethnic backgrounds (e.g., \"Middle Eastern,\" \"South Asian\"). Exemplary generated images are depicted in Figure  1 . In total, we generated more than 200k images. Further details on the specific text-to-image models utilized, prompt design specifics, and measures for transparency and reproducibility, including the release of prompt texts and model identifiers, are provided in App. A.5.\n\nAcquiring Expert Annotations. To ensure precise high-quality emotion labels in EMONET-FACE, we engaged psychology experts recruited via Upwork, selected for their verified academic degrees. Our team of annotators (13 in total), aged 18-44 and spanning diverse ethnic and linguistic backgrounds-e.g., Hispanic, European, South Asian, and Middle Eastern-with fluency in up to four languages, brought a diverse perspective. Annotation was conducted on our open-source platform (see App. C). Annotators were instructed to follow our detailed guidelines, rather than relying solely on personal intuition, following  [41, 42] , promoting coherence. Details on annotator demographics, recruitment, compensation, and adherence to ethical guidelines are provided in App. A.2 and checklist.\n\nFor EMONET-FACE HQ, we used a continuous emotion rating protocol: each expert independently assessed batches of 250 images drawn from the 2,500-image collection, scoring all 40 emotion categories for every image on a 0-7 intensity scale. This multi-rater approach, with four ratings per image, supported fine-grained, multi-label assignments for the ambiguous and overlapping expressions commonly found in facial emotion data. To construct EMONET-FACE BINARY, we implemented a rigorous multi-stage binary annotation protocol. In the affirmative sequence, images initially identified as positive for a target emotion by one annotator were reviewed by a second, and if confirmed again, by a third, resulting in emotion labels with triple positive consensus. Images with any annotator dissent were excluded to ensure high-confidence positives. Additionally, a contrastive batch was included, in which annotators reviewed images with the target emotion deliberately absent, ensuring the inclusion of high-quality true negatives (see Table  2 ). This combination provides robust verification of both the presence and absence of emotion. For EMONET-FACE BIG, we synthetically annotated over 200k images in a two-stage process with Gemini-2.5-Flash. Initially, the model  identified and scored the five most salient emotional dimensions per image; subsequently, to boost detection of underrepresented emotions, we used a \"hinting\" strategy that prompted the model to arbitrarily consider specific emotions. This automated, iterative process continued until we achieved broad coverage across all 40 categories in our taxonomy.\n\nInter-Annotator Agreement. To quantify annotator consistency for EMONET-FACE, we evaluated agreement using both pairwise and group metrics for each emotion. For the continuous 0-7 scale in EMONET-FACE HQ, we used weighted kappa (κ w , with quadratic weights) to capture how closely pairs of annotators rated images, considering only cases where both provided ratings. We also computed Krippendorff's α across all annotators for both EMONET-FACE HQ and EMONET-FACE BINARY. We also compute the conditional agreement for EMONET-FACE BINARY, following the sequential labeling, as described above.\n\nThe average pairwise κ w between human annotators for EMONET-FACE HQ was 0.20 (see Figure  3A , top part), indicating moderate agreement. Furthermore, for EMONET-FACE HQ, we depict Krippendorff's α per emotion in Figure  9 . Across all 40 emotions, the mean α is 0.19 (95% CI [0.16, 0.22]), indicating moderate overall agreement. Per-emotion α varies widely: highest for Elation (α = 0.58), Amusement (0.56) and Anger (0.46); lowest (even negative) for Interest (-0.08), Concentration (-0.02) and Contemplation (-0.02). This spread suggests that stimulus ambiguity, not annotator error, drives most disagreement.\n\nFor EMONET-FACE BINARY, we get overall α = 0.09 (95% CI [0.09, 0.09]), as depicted in Figure  8 , with the top five by binary α are Fatigue/Exhaustion (α = 0.49), Pain (0.45), Malevolence (0.44), Teasing (0.43) and Intoxication (0.40). For conditional binary (presence/absence) agreement, consensus rates are reported in Table  2 . High agreement was achieved across batches, with especially strong consensus in the contrastive (true-negative) setting, reflecting the comparative ease of judging when an emotion is absent, compared to present.\n\nTaken together, these results demonstrate that while human agreement is strong for many emotions, some categories naturally elicit a wider range of interpretations, underscoring the nuanced nature of facial emotion expressions. Rather than indicating weak annotation quality, this pattern highlights the sensitivity of EMONET-FACE to the inherent complexity of affective perception. Our annotations reflect both the challenges and scientific opportunities of capturing emotional diversity.\n\nData Integrity, Safety, and Fairness. All generated images were manually reviewed for obvious stereotypes, artifacts, or harmful content. This way, one inappropriate image was removed from EMONET-FACE BINARY. The EMONET-FACE dataset is released for research under ethical terms of use, supporting safe and fair study of affective AI. Moreover, we focused on a broad and diverse representation across identities. The development of fair and unbiased facial emotion recognition (FER) systems critically depends on diverse training and evaluation datasets. While some existing datasets (see Table  1 ) include varied identities, equitable coverage across groups and emotional categories remains limited to date. In creating our EMONET-FACE datasets, we explicitly addressed this by focusing on diversity across ethnicity, gender, and age groups. Figure  2  illustrates the approximate demographic diversity between average web-scraped datasets, predominantly White/Caucasian  [31] , and our datasets, which are much less Western-centric and more diversified across identities. For additional visualizations, refer to the supplementary files.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "The Emonet-Face Suite",
      "text": "After having established and evaluated its foundation, we release EMONET-FACE as three subsets:\n\n(1) EMONET-FACE HQ: a benchmark test dataset with 2,500 images with multiple continuous expert annotators per image, resulting in 10,000 human expert annotations;\n\n(2) EMONET-FACE BINARY: a fine-tuning dataset with 19,999 images with multiple binary expert annotators per image from a multi-stage process, resulting in 65,686 human expert annotations;\n\n(3) EMONET-FACE BIG: a pre-training dataset with 203,201 images and generated labels.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Benchmarking Facial Emotion Recognition On Emonet-Face",
      "text": "We now use EMONET-FACE to benchmark facial emotion recognition models, including our baselines, proprietary systems, and VLMs, against human expert annotations.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "Developing Baseline Models: EMPATHICINSIGHT-FACE. In addition to establishing new FER datasets for pre-training, fine-tuning, and benchmarking, we leverage these resources to train baseline FER models, which we refer to as EMPATHICINSIGHT-FACE. Specifically, we utilize a SIGLIP2based architecture: images are embedded using SIGLIP2, followed by MLP regression heads to predict continuous emotion scores on a 0-7 scale. We pre-train using EMONET-FACE BIG and finetune on EMONET-FACE BINARY.   3 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Do They See What We See?",
      "text": "We now examine how closely models-both specialized and general-purpose-recognize the same emotions as humans in synthetic faces, and how strongly their judgments align. To provide a clear and concise comparison across annotator groups, Figure  3  aggregates pairwise agreement scores (Weighted Kappa, κ w ) by model type and prompt format. This aggregation highlights several important trends: our models (EMPATHICINSIGHT-FACE) achieve consistently high agreement with human annotators; proprietary models (HumeFace) show only slight agreement; and, as expected, random baselines display negligible agreement. Most notably, VLM performance is highly variable and generally inconsistent-regardless of prompt format, VLMs on average do not offer a dependable improvement over random guessing.\n\nStrikingly, further statistical analyses (Pairwise Mann-Whitney U, bootstrapped 95% confidence intervals) show that EMPATHICINSIGHT-FACE models are statistically indistinguishable from human annotators (∆ = 0.019, p = 0.103), while significantly (p < 0.001) outperforming proprietary models, both multi-shot and zero-shot VLMs, and random baselines. These findings underscore that with focused dataset construction and careful fine-tuning, state-of-the-art models can genuinely approach human-level reliability on synthetic FER tasks. By contrast, general-purpose VLMs continue to lag behind, particularly when faced with nuanced or ambiguous facial expressions, highlighting persistent limitations despite recent advances.\n\nTo provide a more granular perspective, Figure  4  disaggregates results by individual model and prompt setup, offering insight into the underlying variability. Here, Spearman's ρ measures rank correlation between model and human emotion ratings across all 40 categories. Our tailored models (EMPATHICINSIGHT-FACE) again match or exceed the performance of proprietary systems and VLMs, displaying consistency across runs. While a few VLMs (such as Gemini) achieve moderate agreement, many others fall well short, and there is no clear pattern favoring either zero-shot or multi-shot setups. This heterogeneity reinforces the \"hit-or-miss\" nature of current VLM annotators: depending on the model and prompt configuration, performance may range from reasonable to essentially random.\n\nAs further illustrated in Table  3 , prompt handling difficulties, model-internal safety constraints, and sensitivity to format further hinder VLM robustness-only two of nine VLMs consistently produce outputs under both prompt types. These results are also backed by direct comparison in App. Figure  6 , which visually details the pairwise agreement landscape at the annotator level. This unpredictability highlights the need for careful selection and calibration when applying VLMs to FER tasks.\n\nIn summary, systematic evaluation of human, proprietary, and VLM-based annotators demonstrates that our new datasets and models reliably enable near-human performance, while exposing the current limitations of general-purpose AI models. EMONET-FACE thus establishes a rigorous new benchmark for progress in facial emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "Facial Emotion Recognition: Challenges and New Directions. Facial emotion recognition remains an intrinsically complex and nuanced endeavor, rooted in fundamental questions about the nature of emotion itself. Decades of affective science, including the Theory of Constructed Emotion (TCE), emphasize that emotions are not universally \"read out\" from facial muscle configurations, but rather interpreted by observers through a dynamic interplay of visual cues, individual experience, and situational context (see App. A.1). Our analysis of inter-annotator agreement (Section 3) underscores this reality: even among highly trained domain experts, there is marked variability in how emotional expressions are labeled, particularly for subtle or ambiguous categories. Indeed, while there may be a single, underlying emotion in a given situation, there is no single, definitive facial expression signaling that emotion across individuals or contexts. In practice, this means that emotion perception is inherently subjective, and efforts to capture facial affect in a strictly categorical or context-agnostic fashion inevitably face limitations.\n\nThis subjectivity and inter-annotator diversity, should not be seen solely as sources of noise or error; rather, they highlight the deeply constructive and situationally-contingent character of affective perception. Our findings motivate a paradigm shift away from only seeking a single authoritative label for facial images, toward approaches that estimate distributions over plausible emotion categories (see Figure  5 ) or defer to LLMs and VLMs capable of subsequent context-sensitive reasoning.\n\nIn response to these challenges, our study introduces EMONET-FACE -a comprehensive suite designed to systematically benchmark both human and model performance. By providing a largescale, diverse, and fine-grained test bed, EMONET-FACE enables robust examination of inter-annotator variability, the limits of facial expression alone for conveying emotion, and the reliability of both domain-specific and general-purpose AI in this domain. We hope this resource lays the groundwork for future research that not only seeks technical progress, but also more faithfully engages with the rich psychological and contextual complexity of human affect.\n\nSpecialized Models Can Learn to See What Humans See. Despite the inherent challenges of facial emotion recognition, our EMPATHICINSIGHT-FACE models exhibit consistently high agreement with human annotators. This success is partly attributable to our use of SigLIP2, a powerful vision backbone that provides strong image embeddings highly suited for affective analysis. As shown in Table  6  and Figure  6 , our EMPATHICINSIGHT-FACE LARGE model achieves human agreement scores significantly above state-of-the-art zero-shot VLMs, proprietary systems, and even several individual expert annotators. This level of agreement is significant, given our annotators' strong domain expertise, demonstrating that targeted model design and careful data curation can substantially close the gap between machine and human performance on this task. Yet it is important to recognize that such agreement does not necessarily equate to genuine emotional understanding. Current models, including our own, may accurately mimic or imitate human emotion recognition on a surface level by leveraging statistical regularities in the data, without actually grasping the underlying emotional states or meaning. True comprehension of emotion likely requires not only pattern matching in facial cues, but also deeper insight into the context, causes, and subjective experience behind each expression.\n\nGeneral-Purpose VLMs Hold Untapped Potential. Our broader benchmarking of generalpurpose VLMs reveals substantial inconsistencies across models and prompt setups: while a few VLMs, such as Gemini, achieve moderate agreement for certain settings, their performance is generally unreliable and highly sensitive to prompt formatting (see Figures  3  and 4 ). Crucially, there is no current prompt or model configuration that consistently enables VLMs to excel at nuanced FER tasks. Furthermore, we observe a pronounced misalignment problem: different VLMs (or even the same VLM under different prompts) frequently produce divergent or incoherent predictions for the same input image. This lack of alignment and robustness presents a serious challenge for deploying these systems in interactive, real-world settings, as human users require dependable and interpretable model behavior for meaningful interaction  [15, 18] . Robustness and alignment-consistency within a model and agreement across models-are therefore just as important as absolute performance when building trustworthy and user-friendly affective AI. While the foundational capabilities of VLMs are rapidly improving and may eventually surpass specialized approaches like our EMPATHICINSIGHT-FACE, realizing this potential will require addressing both their alignment and robustness shortcomings through new techniques, such as improved prompting strategies, architectural innovations, or alignment-focused training protocols. Future work bridging these gaps-potentially by combining the flexibility of VLMs with task-specific supervision-could unlock even greater advances, and enable robust, reliable, and context-aware emotion understanding at scale.\n\nLimitations. Despite these advances, several limitations should be considered. First, our analyses are restricted to static facial expressions; the absence of broader contextual and multimodal cues, as highlighted by TCE, limits both human and AI performance, especially for subtle or ambiguous emotional states. Second, our 40-category taxonomy (Section 3), while comprehensive and expert-validated, constitutes just one perspective on emotion; its universality-particularly across cultures-remains an open question deserving further research. Third, although expert annotation protocols were standardized, the task remains inherently subjective, potentially introducing individual biases into the \"ground truth\" for complex emotions. Fourth, while synthetic datasets provide crucial ethical and practical advantages, their impact on generalization to unconstrained, real-world images is not fully established; future work should seek to validate our findings on naturalistic datasets. Finally, the current work focuses solely on facial emotion cues, deferring integration of speech, body language, and context to future multimodal research.\n\nIn summary, the EMONET-FACE suite establishes a rigorous and versatile new testbed for emotion recognition research. Our results demonstrate that focused, expert-informed modeling can yield near-human-and in some cases, super-human-consistency on challenging FER tasks. Addressing the outlined limitations, particularly by embracing richer contextual and multimodal information, will be key to the next generation of emotionally intelligent AI systems.\n\nEthical Considerations. This work addresses growing concerns about the unintended consequences of emotionally uncalibrated AI systems. As AI models increasingly generate emotionally evocative content, it is critical to examine how humans interpret and react to these synthetic emotions. Our dataset provides a foundation for research into potential risks, such as miscommunication or emotional manipulation. We acknowledge the ethical risks related to misuse, particularly for manipulative purposes, which stand in contrast to our commitment to transparency and safety. All images in EMONET-FACE are synthetic, generated via T2I models with manual filtering and prompt diversification to mitigate bias and problematic content. While highly unlikely, we cannot exclude the chance that some images may resemble real individuals; however, no personally identifiable data was used.\n\nOur dataset reflects specific demographic profiles, so broader studies are encouraged. We release EMONET-FACE strictly as a research artifact for academic use, urging careful investigation of any downstream biases or ethical concerns. We also invite users to utilize our tools, transparently report unanticipated model or data behaviors, and contribute back to the community, supporting advances in ethical data curation and safer AI research.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We introduced the EMONET-FACE suite-a comprehensive resource featuring a fine-grained 40category taxonomy, richly annotated datasets, and specialized models that establish new stateof-the-art benchmarks for facial emotion recognition. The EMONET-FACE benchmark exposes significant limitations in current tools and models for emotion evaluation, underscoring both the complexity of the task and the need for future innovation. Leveraging this suite, we developed the EMPATHICINSIGHT-FACE models, which deliver leading performance on facial emotion recognition while also illuminating the substantial variability in human judgments and the inherent constraints of using facial expressions alone. We provide a thorough discussion of these challenges and outline promising directions for advancing human-like emotion understanding in AI-impacting not only computer science, but also contributing valuable insights to psychology and the social sciences.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "A Appendices",
      "text": "A.1 Theories of Emotion: From Universal Programs to Constructed Experiences.\n\nEmotion theories span a spectrum from biologically grounded to constructivist accounts. Basic Emotion Theory (BET) posits a universal set of evolved emotions-anger, disgust, fear, happiness, sadness, and surprise-each with distinct physiological and facial expressions  [13] , though it has been critiqued for oversimplifying emotion and overlooking cultural variability  [3] . Cognitive appraisal theories emphasize interpretation, with the Schachter-Singer model combining physiological arousal with contextual labeling  [43] , and Lazarus's framework highlighting evaluations of relevance and coping potential as key drivers of emotional response  [26] . The Theory of Constructed Emotion (TCE) offers a constructivist view, arguing that emotions are not innate but are constructed through predictive brain mechanisms using culturally learned concepts  [4] . TCE holds that emotional experiences emerge from interpreting interoceptive signals (valence and arousal) via culturally shaped categories, rejecting the idea of fixed \"emotion circuits\" and emphasizing variability across instances. This predictive, concept-driven view has major implications for AI, calling for models that interpret affect in context-sensitive, culturally informed ways rather than relying on static facial-emotion mappings. This is further visualized in Figure  5 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.2 Detailed Expert Annotator Recruitment And Process",
      "text": "To ensure the highest quality and precision of emotion labels in EMONET-FACE, we relied on expert annotation by psychology professionals, recruited through Upwork and selected for their verified academic degrees or substantial experience in the field. Our broad team of annotators, aged 18-44 and spanning diverse ethnic and linguistic backgrounds-e.g., Hispanic, European, South Asian, and Middle Eastern-with fluency in up to four languages, brought an inclusive perspective to the annotation process. All annotators (13 in total) participated voluntarily after giving informed consent and were fairly compensated at rates well above local standards, in keeping with the NeurIPS Code of Ethics. Annotators were instructed to follow our detailed, standardized guidelines, rather than relying solely on personal intuition, following  [41, 42] , promoting coherence and reproducibility. Open communication allowed for regular clarification of any ambiguities during annotation.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.3 Emonet-Face Data Preprocessing",
      "text": "The preprocessing pipeline included: (1) Parsing Ratings-extracting structured emotion scores;\n\n(2) Emotion Extraction-identifying all unique emotion labels to define the target space; (3) Long Format Transformation-reshaping the dataset into (image, annotator, emotion) triplets, with missing ratings filled as 0 to reflect the annotation platform's assumption that unrated emotions indicate absence; and (4) Annotator Typing-labeling annotators as Human or VLM based on a binary flag, enabling separate analyses. All ratings were standardized on a 0-7 scale.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A.4 Statistical Analysis And Visualization",
      "text": "The calculated agreement metrics were further analyzed and visualized:\n\n• Summary Statistics: Mean, median, standard deviation, and count were calculated for Weighted Kappa and Spearman's Rho, grouped by the type of annotator pair (Human-Human, Human-Model, Model-Model). Similar summaries were computed for Krippendorff's Alpha, grouped by annotator group (Human, Model, All).\n\n• Visualizations:\n\n-Box plots were generated to show the distribution of pairwise Weighted Kappa and Spearman's Rho across the different pair types. -A heatmap visualized the average Weighted Kappa for every annotator pair across all emotions. Annotators were ordered with human annotators listed first (alphabetically), followed by model annotators (alphabetically), for clarity. -Bar plots displayed Krippendorff's Alpha for the top N emotions (ranked by human annotator agreement) for the Human group, including reference lines indicating common interpretation thresholds (e.g., 0.80 for excellent, 0.67 for acceptable agreement). -Bar plots showed the average pairwise Weighted Kappa per emotion, faceted by pair type. -Statistical comparisons were performed using the Mann-Whitney U test to evaluate significant differences in Weighted Kappa distributions between the different pair types (e.g., Human-Human vs. Human-Model). The results, including the difference in means, bootstrapped 95% confidence intervals for the difference, and p-values, were annotated onto the Kappa distribution box plot using the 'statannotations' library. • Output Files: Detailed results (pairwise scores, Alpha scores, summaries) were saved to CSV files in the 'output/tables' directory, and all generated plots were saved as PNG files in the 'output/figures' directory.\n\nThis comprehensive analysis allows for a detailed understanding of the reliability of the emotion ratings and the degree of alignment between human perception and VLM assessments within the EMO+ dataset.\n\nThe analysis was performed using Python (v3.10.12) with the libraries pandas (v2.2.2), numpy (v1.25.2), scikit-learn (v1.6.1), scipy, krippendorff (v0.8.1), seaborn (v0.13.2), matplotlib (v3.7.2), and statannotations (v0.7.2).   The EMONET-FACE HQ dataset (2,500 images) was generated using a combination of MidJourney v6  [32]  and Flux-Dev and -Pro  [6] . The larger EMONET-FACE BINARY (19,999 images) and EMONET-FACE BIG (over 203,000 images) datasets were produced exclusively with Flux-Dev.\n\nWe deliberately selected multiple state-of-the-art (SOTA) text-to-image (T2I) models to minimize potential confounders and ensure high image quality across the diverse range of emotions and demographic attributes. During this selection process, we intentionally excluded models such as Juggernaut-XL or Kolors from our final pipeline due to their propensity for frequent visual artifacts, including exaggerated wrinkles and unnatural facial features, which were not easily mitigated through prompt engineering. The prompts for all models specified sharp, close-up portraits with neutral backgrounds to maximize emotion clarity and consistently maintain demographic diversity across all generated datasets.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "B Dataset Construction Details B.1 Construction Of Emonet-Face Big",
      "text": "The EMONET-FACE BIG dataset, designed primarily for large-scale model pre-training, was constructed through a multi-stage process. This involved an initial phase of broad-spectrum image generation and annotation, followed by targeted generation and a refined annotation strategy to ensure comprehensive coverage across our 40-emotion taxonomy.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Initial Data Generation And Annotation",
      "text": "The foundation of EMONET-FACE BIG was formed by incorporating all images from the EMONET-FACE BINARY dataset, augmented with an additional 20,000 images generated using the Flux text-to-image model. This combined set of images was then annotated using the Gemini 2.5 Flash model. The annotation prompt directed Gemini 2.5 Flash to identify the five most salient emotional dimensions present in each image and to assign an intensity score ranging from 0 to 7 for each of these selected dimensions. While this initial phase yielded a substantial volume of annotations, a qualitative review revealed that certain emotion categories within our taxonomy-such as infatuation, embarrassment, and sexual lust-received disproportionately few non-zero annotations from Gemini 2.5 Flash. Targeted Generation and Annotation with Hinting Strategy To address the underrepresentation of these specific emotion categories and to enrich the dataset with more diverse examples for these less frequently annotated states, we implemented a revised two-step procedure:\n\n1. Targeted Image Generation: We utilized the Flux-Dev model to generate new images. The prompts for Flux-Dev were specifically designed to elicit facial expressions corresponding to the underrepresented emotions. For instance, prompts targeted the generation of images depicting \"embarrassment\" or \"infatuation.\"\n\n2. Annotation with a Hinting Strategy: These newly generated, targeted images were subsequently annotated using Gemini Flash 2.0. The selection of Gemini Flash 2.0 for this phase was partly influenced by more favorable API rate limits available at the time of data collection, which was crucial for the iterative process. The core annotation instruction remained to score the top five most evident emotional dimensions on the 0-7 scale. However, a key modification was introduced: a \"hint\" was provided to the model. For example, if an image was generated with \"infatuation\" as the target emotion in the prompt, the annotation prompt for Gemini Flash 2.0 would include a suggestion that the image might contain \"infatuation.\" This was intended to encourage the model to consider this specific dimension more readily, without mandating its selection or biasing the intensity score.\n\nThis hinting strategy proved effective. Qualitative inspection of the results indicated that for approximately 50% of the images where a hint was provided for a specific target emotion, Gemini Flash 2.0 assigned a non-zero annotation to that particular dimension. The quality of these hinted annotations was deemed reasonable and sufficient for the purposes of large-scale pre-training.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Iterative Refinement And Final Dataset Composition",
      "text": "We iteratively applied this targeted image generation (with Flux-Dev) and hinted annotation (with Gemini Flash 2.0) process. The primary objective of this iterative refinement was to ensure that each of the 40 emotional dimensions in our taxonomy received approximately 5,000 samples with a non-zero annotation score attributed by Gemini Flash 2.0 (under the described hinting strategy). This iterative process continued until the desired representation was achieved for all emotion categories.\n\nThe final EMONET-FACE BIG dataset comprises a total of 203,201 images, each with synthetic emotion annotations derived through this combined approach of broad-spectrum initial generation/annotation and subsequent targeted generation/hinted-annotation.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "B.2 Training Of Baseline Models",
      "text": "To establish baseline performance for emotion dimension prediction, we developed and trained several Multi-Layer Perceptron (MLP) models. The process began with feature extraction: we computed SIGLIP2-400M image embeddings (1152 dimensions) for all images within the EMoNet-Face Big, and EmoNetFace Binary datasets. These embeddings served as the input features for our models.\n\nOur initial approach involved training 40 distinct MLP models, one for each of the 40 emotion dimensions. Each MLP was designed to take an image's SIGLIP2-400M embedding as input and predict the rating for its specific emotion dimension as a continuous score (ranging from 0 to 7). To calibrate these continuous predictions, we addressed potential biases in the models' interpretation of neutral expressions. For each of the 40 MLP models, we performed inference on 1700 images generated by FluxDev. These images depicted faces with neutral emotional expressions, created using prompts randomly varied across the complete template distribution for different ages, ethnic backgrounds, and genders. We then calculated the average rating assigned by each MLP model to these neutral faces. This average neutral rating, which varied per model, was subsequently subtracted from all predictions made by that specific MLP to correct for its baseline offset on neutral expressions.\n\nWe first developed a \"small model\" configuration. For these small models, training was conducted exclusively on the EMoNet-Face Big dataset. To address class imbalance inherent in the 0-7 rating scale for each emotion dimension, we employed a specific sampling strategy, referred to as the \"stumbling strategy,\" to construct balanced training datasets. For a given emotion dimension, samples were first grouped into eight buckets corresponding to ratings 0 through 7. We then calculated the average number of images across these eight buckets. The training set for that dimension was constructed by sampling up to 25 percent of this average count from each bucket. This approach was adopted because the bucket for rating '0' (emotion not present) typically contained a significantly larger number of samples (e.g., 180,000-190,000) compared to other rating buckets. Consequently, the average number of samples per bucket might be around 20,000, leading to approximately 5,000 samples from the '0' bucket and proportionally fewer (e.g., around 1,000) from each of the other buckets (ratings 1-7). A validation set was created by reserving 5 percent of the samples from each bucket using the same principle. The small MLP architecture consisted of an input layer (1152 features), a hidden layer with 128 neurons (ReLU, Dropout 0.1), a second hidden layer with 32 neurons (ReLU, Dropout 0.1), and an output layer with 1 neuron. This architecture has 151,745 parameters. The checkpoint performing best on the validation set was saved.\n\nFollowing experiments with the small model, a grid search over different model sizes indicated that a larger MLP architecture could yield modest improvements in validation accuracy. This led us to transition our experiments to a \"big model.\" The big MLP architecture is defined as follows: an input layer (1152 features); a first hidden layer with 1024 neurons (ReLU, Dropout 0.2); a second hidden layer with 512 neurons (ReLU, Dropout 0.2); a third hidden layer with 256 neurons (ReLU, Dropout 0.2); and an output layer with 1 neuron. This larger architecture comprises 1,837,057 parameters.\n\nInitially, this big model was trained on EMoNet-Face Big using the same strategy of balancing datasets for each emotion dimension by sampling from each 0-7 rating bucket up to a limit (25 percent of the average samples per bucket) to reduce the dominance of the overrepresented zero-rating for continuous score prediction as the small model. However, after these initial explorations, we shifted the dataset composition and prediction task. For each emotion dimension, we transformed the problem into a binary classification task. Images with a rating of 0 were assigned to a '0' bucket (emotion not present). All images with ratings from 1 to 7 were merged into a single '1' bucket (emotion present, irrespective of intensity). The big model was then trained to predict either 0 or 1 via regression. The EMoNet-Face Big dataset was prepared for this binary task by binarizing its ratings as described. Due to the '0' bucket being substantially larger than the '1' bucket even after binarization, we balanced the EMoNet-Face Big training data by downsampling the '0' bucket to match the number of samples in the '1' bucket. The human-annotated EmoNetFace Binary dataset was also prepared by binarizing its ratings in the same manner.",
      "page_start": 20,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Samples from our EMONET-FACE datasets generated with different sota T2I models.",
      "page": 4
    },
    {
      "caption": "Figure 1: In total, we generated more than 200k images. Further details on the",
      "page": 4
    },
    {
      "caption": "Figure 2: Approximate world map of demographic coverage and diversity in web-scraped datasets",
      "page": 5
    },
    {
      "caption": "Figure 9: Across all 40 emotions, the mean α is 0.19 (95% CI [0.16, 0.22]),",
      "page": 5
    },
    {
      "caption": "Figure 3: Weighted Kappa (κw) agreement scores by annotator group. A (top): Pairwise agreement",
      "page": 6
    },
    {
      "caption": "Figure 2: illustrates the approximate",
      "page": 6
    },
    {
      "caption": "Figure 3: aggregates pairwise agreement scores",
      "page": 7
    },
    {
      "caption": "Figure 4: disaggregates results by individual model and",
      "page": 7
    },
    {
      "caption": "Figure 4: Mean Spearman’s Rho correlation between various model annotators and human annotations.",
      "page": 8
    },
    {
      "caption": "Figure 5: ) or defer to LLMs and VLMs capable of subsequent context-sensitive reasoning.",
      "page": 8
    },
    {
      "caption": "Figure 6: , our EMPATHICINSIGHT-FACE LARGE model achieves human agreement",
      "page": 8
    },
    {
      "caption": "Figure 5: Rather than mapping each face to a single label, we estimate a distribution over plausible",
      "page": 15
    },
    {
      "caption": "Figure 6: Heatmap of Average Pairwise Weighted Kappa (κw, quadratic weights) Across All Emotions.",
      "page": 17
    },
    {
      "caption": "Figure 7: Emotion classification taxonomy showing hierarchical relationships between primary",
      "page": 18
    },
    {
      "caption": "Figure 8: Annotator agreement for Human Annotators (n=4) in EMONET-FACE BINARY. Stacked",
      "page": 21
    },
    {
      "caption": "Figure 9: Krippendorff’s Alpha (α, interval level) for Human Annotators (n=8) in EMONET-FACE",
      "page": 22
    },
    {
      "caption": "Figure 10: Discrepancies between human annotators and zero-shot prompted VLMs.",
      "page": 24
    },
    {
      "caption": "Figure 11: Instructions given to the human annotator for the expert annotation of EMONET-FACE",
      "page": 25
    },
    {
      "caption": "Figure 12: UI of our expert annotation tool for EMONET-FACE HQ.",
      "page": 26
    },
    {
      "caption": "Figure 13: Instructions given to the human annotator for the expert annotation of EMONET-FACE",
      "page": 27
    },
    {
      "caption": "Figure 14: UI of the self-developed expert annotation tool for EMONET-FACE BINARY.",
      "page": 28
    },
    {
      "caption": "Figure 15: Instructions given to the human annotators for the preference annotation of of EMONET-",
      "page": 29
    },
    {
      "caption": "Figure 16: UI of our expert preference annotation tool for EMONET-FACE HQ.",
      "page": 29
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of facial emotion recognition datasets, including synthetic generation and expert",
      "page": 2
    },
    {
      "caption": "Table 4: for full taxonomy).",
      "page": 3
    },
    {
      "caption": "Table 2: ). This combination provides robust",
      "page": 4
    },
    {
      "caption": "Table 2: Annotation protocol and agreement rates for EMONET-FACE BINARY. “Affirmative”",
      "page": 5
    },
    {
      "caption": "Table 2: High agreement was achieved across batches, with especially strong",
      "page": 5
    },
    {
      "caption": "Table 1: ) include varied identities, equitable coverage across groups and emotional cate-",
      "page": 6
    },
    {
      "caption": "Table 3: Support for Zero-shot and Multi-shot Emotion-Analysis Prompts (format only). *Gemini",
      "page": 7
    },
    {
      "caption": "Table 3: , prompt handling difficulties, model-internal safety constraints, and",
      "page": 7
    },
    {
      "caption": "Table 6: and Figure 6, our EMPATHICINSIGHT-FACE LARGE model achieves human agreement",
      "page": 8
    },
    {
      "caption": "Table 4: The EmoNet-Face 40-Category Emotion Taxonomy. Each category represents a cluster of",
      "page": 15
    },
    {
      "caption": "Table 4: – continued from previous page",
      "page": 16
    },
    {
      "caption": "Table 5: Internal Agreement for Repeated Annotations",
      "page": 17
    },
    {
      "caption": "Table 6: Agreement summary (average weighted Kappa κw with quadratic weights) for all annotators:",
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning to generate 3d stylized character expressions from humans",
      "authors": [
        "Deepali Aneja",
        "Bhaswar Chaudhuri",
        "Alex Colburn",
        "Gary Faigin",
        "Linda Shapiro",
        "Barbara Mones"
      ],
      "year": "2018",
      "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "2",
      "title": "Modeling stylized character expressions via deep learning",
      "authors": [
        "Deepali Aneja",
        "Alex Colburn",
        "Gary Faigin",
        "Linda Shapiro",
        "Barbara Mones"
      ],
      "year": "2016",
      "venue": "Asian Conference on Computer Vision"
    },
    {
      "citation_id": "3",
      "title": "Are emotions natural kinds?",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2006",
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "4",
      "title": "How Emotions Are Made: The Secret Life of the Brain",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2017",
      "venue": "How Emotions Are Made: The Secret Life of the Brain"
    },
    {
      "citation_id": "5",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "Ramprakash Srinivasan",
        "Aleix Martinez"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "6",
      "title": "How context influences our perception of emotional faces: A behavioral study on the kuleshov effect",
      "authors": [
        "Marta Calbi",
        "Katrin Heimann",
        "Daniel Barratt",
        "Francesca Siri",
        "Maria Umiltà",
        "Vittorio Gallese"
      ],
      "year": "2019",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "7",
      "title": "Empathy is hard work: People choose to avoid empathy because of its cognitive costs",
      "authors": [
        "Daryl Cameron",
        "Michael Inzlicht"
      ],
      "year": "2019",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "8",
      "title": "Conversational ai agents",
      "authors": [
        "Inc",
        "Character Technologies",
        "Character",
        "Ai"
      ],
      "year": "2023",
      "venue": "Conversational ai agents"
    },
    {
      "citation_id": "9",
      "title": "Social companionship with artificial intelligence: Recent trends and future avenues",
      "authors": [
        "Rijul Chaturvedi",
        "Sanjeev Verma",
        "Ronnie Das",
        "Yogesh Dwivedi"
      ],
      "year": "2023",
      "venue": "Technological Forecasting and Social Change"
    },
    {
      "citation_id": "10",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "11",
      "title": "Affective neuroscience and psychophysiology: Toward a synthesis",
      "authors": [
        "Richard Davidson"
      ],
      "year": "2003",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "12",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "13",
      "title": "Basic emotions. In Handbook of Cognition and Emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. In Handbook of Cognition and Emotion"
    },
    {
      "citation_id": "14",
      "title": "What people think ai should infer from faces",
      "authors": [
        "Chiara Severin Engelmann",
        "Orestis Ullstein",
        "Jens Papakyriakopoulos",
        "Grossklags"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "15",
      "title": "How well does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment",
      "authors": [
        "Aaron Gilson",
        "Chris Safranek",
        "Thomas Huang",
        "V Socrates",
        "Joyce Chi",
        "Richard Taylor",
        "James Yuan"
      ],
      "year": "2023",
      "venue": "How well does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment"
    },
    {
      "citation_id": "16",
      "title": "Multi-pie",
      "authors": [
        "Ralph Gross",
        "Iain Matthews",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Simon Baker"
      ],
      "year": "2010",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "17",
      "title": "Identifying implicit social biases in vision-language models",
      "authors": [
        "Kimia Hamidieh",
        "Haoran Zhang",
        "Walter Gerych",
        "Thomas Hartvigsen",
        "Marzyeh Ghassemi"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "18",
      "title": "A chinese face dataset with dynamic expressions and diverse ages synthesized by deep learning",
      "authors": [
        "Shangfeng Han",
        "Yanliang Guo",
        "Xinyi Zhou",
        "Junlong Huang",
        "Linlin Shen",
        "Yuejia Luo"
      ],
      "year": "2023",
      "venue": "Sci. Data"
    },
    {
      "citation_id": "19",
      "title": "The Face of Emotion",
      "authors": [
        "Carroll Izard"
      ],
      "year": "1971",
      "venue": "The Face of Emotion"
    },
    {
      "citation_id": "20",
      "title": "The Psychology of Emotions",
      "authors": [
        "Carroll Izard"
      ],
      "year": "1991",
      "venue": "The Psychology of Emotions"
    },
    {
      "citation_id": "21",
      "title": "Create custom ai companions",
      "authors": [
        "Kajiwoto",
        "Kajiwoto"
      ],
      "year": "2022",
      "venue": "Create custom ai companions"
    },
    {
      "citation_id": "22",
      "title": "Jannatul Mabia Rahman, Md Rabiul Islam, and Syed Masudur Rahman Dewan. Chatgpt and mental health: Friends or foes? Health Science Reports",
      "authors": [
        "Khondoker Tashya"
      ],
      "year": "2024",
      "venue": "Jannatul Mabia Rahman, Md Rabiul Islam, and Syed Masudur Rahman Dewan. Chatgpt and mental health: Friends or foes? Health Science Reports"
    },
    {
      "citation_id": "23",
      "title": "Emotic: Emotions in context dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "24",
      "title": "Orpheus: Emotionally expressive speech ai",
      "year": "2024",
      "venue": "Orpheus: Emotionally expressive speech ai"
    },
    {
      "citation_id": "25",
      "title": "Emotion and Adaptation",
      "authors": [
        "Richard Lazarus"
      ],
      "year": "1991",
      "venue": "Emotion and Adaptation"
    },
    {
      "citation_id": "26",
      "title": "Handbook of Emotions",
      "authors": [
        "M Lewis",
        "J Haviland-Jones",
        "L Barrett"
      ],
      "year": "2016",
      "venue": "Handbook of Emotions"
    },
    {
      "citation_id": "27",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression",
      "authors": [
        "Patrick Lucey",
        "Jeffrey Cohn",
        "Takeo Kanade",
        "Jason Saragih",
        "Zara Ambadar",
        "Iain Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "28",
      "title": "Replika: Ai companion",
      "authors": [
        "Inc",
        "Luka"
      ],
      "year": "2023",
      "venue": "Replika: Ai companion"
    },
    {
      "citation_id": "29",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "Michael Lyons",
        "Shigeru Akamatsu",
        "Miyuki Kamachi",
        "Jiro Gyoba"
      ],
      "year": "1998",
      "venue": "Proceedings of the Third IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "30",
      "title": "Dataset diversity: Measuring and mitigating geographical bias in image search and retrieval",
      "authors": [
        "Abhishek Mandal",
        "Susan Leavy",
        "Suzanne Little"
      ],
      "year": "2021",
      "venue": "Proceedings of the 1st International Workshop on Trustworthy AI for Multimedia Computing"
    },
    {
      "citation_id": "31",
      "title": "Midjourney v6: Ai image generation",
      "authors": [
        "Inc",
        "Midjourney"
      ],
      "year": "2024",
      "venue": "Midjourney v6: Ai image generation"
    },
    {
      "citation_id": "32",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "Openai",
        "Chatgpt"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "34",
      "title": "Gpt-4o: Advanced multimodal ai",
      "year": "2024",
      "venue": "Gpt-4o: Advanced multimodal ai"
    },
    {
      "citation_id": "35",
      "title": "The cognitive structure of emotions",
      "authors": [
        "Andrew Ortony",
        "Gerald Clore",
        "Allan Collins"
      ],
      "year": "1990",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "36",
      "title": "Affective Neuroscience: The Foundations of Human and Animal Emotions",
      "authors": [
        "Jaak Panksepp"
      ],
      "year": "1998",
      "venue": "Affective Neuroscience: The Foundations of Human and Animal Emotions"
    },
    {
      "citation_id": "37",
      "title": "",
      "authors": [
        "W Parrott"
      ],
      "year": "2001",
      "venue": ""
    },
    {
      "citation_id": "38",
      "title": "Affective Computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "A General Psychoevolutionary Theory of Emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1980",
      "venue": "A General Psychoevolutionary Theory of Emotion"
    },
    {
      "citation_id": "40",
      "title": "Two contrasting data annotation paradigms for subjective nlp tasks",
      "authors": [
        "Paul Röttger",
        "Bertie Vidgen",
        "Dirk Hovy",
        "Janet Pierrehumbert"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "41",
      "title": "Msts: A multimodal safety test suite for vision-language models",
      "authors": [
        "Paul Röttger",
        "Giuseppe Attanasio",
        "Felix Friedrich",
        "Janis Goldzycher",
        "Alicia Parrish",
        "Rishabh Bhardwaj",
        "Chiara Di Bonaventura",
        "Roman Eng",
        "Gaia Khoury Geagea",
        "Sujata Goswami",
        "Jieun Han",
        "Dirk Hovy",
        "Seogyeong Jeong",
        "Paloma Jeretič"
      ],
      "year": "2025",
      "venue": "Msts: A multimodal safety test suite for vision-language models"
    },
    {
      "citation_id": "42",
      "title": "Cognitive, social, and physiological determinants of emotional state",
      "authors": [
        "Stanley Schachter",
        "Jerome Singer"
      ],
      "year": "1962",
      "venue": "Psychological Review"
    },
    {
      "citation_id": "43",
      "title": "Conversational speech model",
      "authors": [
        "Sesame",
        "Sesame"
      ],
      "year": "2024",
      "venue": "Conversational speech model"
    },
    {
      "citation_id": "44",
      "title": "Positive emotion differentiation: A functional approach",
      "authors": [
        "Michelle Shiota",
        "Samantha Neufeld",
        "Alexander Danvers",
        "Elizabeth Osborne",
        "Oliver Sng",
        "Cindy Yee"
      ],
      "year": "2017",
      "venue": "Positive emotion differentiation: A functional approach"
    },
    {
      "citation_id": "45",
      "title": "Ai for mental health: A systematic review of affect recognition techniques",
      "authors": [
        "Michel Valstar",
        "Björn Schuller",
        "Nicholas Cummins",
        "Maja Pantic"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Chatgpt: Opportunities, risks and priorities for psychiatry",
      "authors": [
        "Yaohui Wei",
        "Lei Guo",
        "Cheng Lian",
        "Jue Chen"
      ],
      "year": "2023",
      "venue": "Asian Journal of Psychiatry"
    },
    {
      "citation_id": "47",
      "title": "Picking up good vibrations: Discrete positive emotions and their experiential content",
      "authors": [
        "Aaron Weidman",
        "Jessica Tracy"
      ],
      "year": "2020",
      "venue": "Emotion"
    },
    {
      "citation_id": "48",
      "title": "The influence of background on facial emotion perception: A psychophysical study. i-Perception",
      "authors": [
        "Ying Wu",
        "Wen Chen",
        "Hong Li",
        "Xiaolan Fu"
      ],
      "venue": "The influence of background on facial emotion perception: A psychophysical study. i-Perception"
    },
    {
      "citation_id": "49",
      "title": "Findingemo: An image dataset for emotion recognition in the wild",
      "authors": [
        "Jialin Yang",
        "Ru Li",
        "Shuai Xiao",
        "Wenming Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "50",
      "title": "romantic desire', 'fondness', 'butterflies in the stomach",
      "venue": "romantic desire', 'fondness', 'butterflies in the stomach"
    },
    {
      "citation_id": "51",
      "title": "Encouragement",
      "venue": "Encouragement"
    },
    {
      "citation_id": "52",
      "title": "Triumph 'triumph', 'superiority' 10. Pride 'pride', 'dignity', 'self-confidently', 'honor', 'self-consciousness' 11. Interest 'interest",
      "venue": "Triumph 'triumph', 'superiority' 10. Pride 'pride', 'dignity', 'self-confidently', 'honor', 'self-consciousness' 11. Interest 'interest"
    },
    {
      "citation_id": "53",
      "title": "Craving', 'desire', 'Envy', 'homesickness', 'saudade' 18. Teasing 'teasing', 'bantering', 'mocking playfully', 'ribbing', 'provoking lightly' 19. Impatience and Irritability 'impatience', 'irritability', 'irritation', 'restlessness', 'short-temperedness', 'exasperation' 20. Sexual Lust 'sexual lust",
      "authors": [
        "Awe 'awe"
      ],
      "venue": "Craving', 'desire', 'Envy', 'homesickness', 'saudade' 18. Teasing 'teasing', 'bantering', 'mocking playfully', 'ribbing', 'provoking lightly' 19. Impatience and Irritability 'impatience', 'irritability', 'irritation', 'restlessness', 'short-temperedness', 'exasperation' 20. Sexual Lust 'sexual lust"
    },
    {
      "citation_id": "54",
      "title": "Embarrassment 'embarrassment', 'shyness', 'mortification', 'discomfiture', 'awkwardness",
      "venue": "Embarrassment 'embarrassment', 'shyness', 'mortification', 'discomfiture', 'awkwardness"
    },
    {
      "citation_id": "55",
      "title": "Self-Pity', 'Sullenness', 'heartache', 'mournfulness', 'misery' 29. Bitterness 'resentment', 'acrimony', 'bitterness', 'cynicism', 'rancor' 30. Contempt 'contempt', 'disapproval', 'scorn', 'disdain', 'loathing', 'Detestation' 31. Disgust 'disgust', 'revulsion', 'repulsion', 'abhorrence', 'loathing' 32. Anger 'anger', 'rage', 'fury', 'hate', 'irascibility', 'enragement', 'Vexation', 'Wrath', 'Peevishness', 'Annoyance' 33. Malevolence/Malice 'spite', 'sadism', 'malevolence', 'malice', 'desire to harm', 'schadenfreude' 34. Sourness 'sourness', 'tartness', 'acidity', 'acerbity', 'sharpness' 35. Pain 'physical pain",
      "venue": "Self-Pity', 'Sullenness', 'heartache', 'mournfulness', 'misery' 29. Bitterness 'resentment', 'acrimony', 'bitterness', 'cynicism', 'rancor' 30. Contempt 'contempt', 'disapproval', 'scorn', 'disdain', 'loathing', 'Detestation' 31. Disgust 'disgust', 'revulsion', 'repulsion', 'abhorrence', 'loathing' 32. Anger 'anger', 'rage', 'fury', 'hate', 'irascibility', 'enragement', 'Vexation', 'Wrath', 'Peevishness', 'Annoyance' 33. Malevolence/Malice 'spite', 'sadism', 'malevolence', 'malice', 'desire to harm', 'schadenfreude' 34. Sourness 'sourness', 'tartness', 'acidity', 'acerbity', 'sharpness' 35. Pain 'physical pain"
    },
    {
      "citation_id": "56",
      "title": "Weariness' 38. Emotional Numbness 'numbness', 'detachment', 'insensitivity', 'emotional blunting', 'apathy', 'existential void', 'boredom', 'stoicism', 'indifference' 39. Intoxication/Altered States 'being drunk",
      "venue": "Jealousy & Envy 'jealousy"
    },
    {
      "citation_id": "57",
      "title": "Detailed Image Generation Models and Reproducibility For transparency and reproducibility in the generation of the EMONET-FACE datasets, all prompt texts and specific model identifiers used for image creation will be released alongside the dataset",
      "venue": "Detailed Image Generation Models and Reproducibility For transparency and reproducibility in the generation of the EMONET-FACE datasets, all prompt texts and specific model identifiers used for image creation will be released alongside the dataset"
    }
  ]
}