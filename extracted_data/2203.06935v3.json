{
  "paper_id": "2203.06935v3",
  "title": "A Systematic Review On Affective Computing: Emotion Models, Databases, And Recent Advances",
  "published": "2022-03-14T08:57:12Z",
  "authors": [
    "Yan Wang",
    "Wei Song",
    "Wei Tao",
    "Antonio Liotta",
    "Dawei Yang",
    "Xinlei Li",
    "Shuyong Gao",
    "Yixuan Sun",
    "Weifeng Ge",
    "Wei Zhang",
    "Wenqiang Zhang"
  ],
  "keywords": [
    "affective computing",
    "machine learning",
    "deep learning",
    "feature learning",
    "unimodal affect recognition",
    "multimodal affective analysis Audio Emotion Recognition Physiological-based Emotion Recognition Multi-physical Modalities Fusion Multiphysiological modalities fusion Visual Emotion Recognition Physicalphysiological Modalities Fusion ML-based DL-based Knowledge-based [114-120] Statistical-based [121-125] Hybrid-based [126-130] Adversarial-based [156",
    "157",
    "159",
    "160] ConvNet-RNN [149-153] RNN-based [140-148] ConvNet-based [133-139] ML-based DL-based Acoustic-feature [172",
    "174",
    "175-178] ML-based classifier [179-186]"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing conjoins the research topics of emotion recognition and sentiment analysis, and can be realized with unimodal or multimodal data, consisting primarily of physical information (e.g., text, audio, and visual) and physiological signals (e.g., EEG and ECG). Physical-based affect recognition caters to more researchers due to the availability of multiple public databases, but it is challenging to reveal one's inner emotion hidden purposefully from facial expressions, audio tones, body gestures, etc. Physiological signals can generate more precise and reliable emotional results; yet, the difficulty in acquiring these signals hinders their practical application. Besides, by fusing physical information and physiological signals, useful features of emotional states can be obtained to enhance the performance of affective computing models. While existing reviews focus on one specific aspect of affective computing, we provide a systematical survey of important components: emotion models, databases, and recent advances. Firstly, we introduce two typical emotion models followed by five kinds of commonly used databases for affective computing. Next, we survey and taxonomize state-of-the-art unimodal affect recognition and multimodal affective analysis in terms of their detailed architectures and performances. Finally, we discuss some critical aspects of affective computing and its applications and conclude this review by pointing out some of the most promising future directions, such as the establishment of benchmark database and fusion strategies. The overarching goal of this systematic review is to help academic and industrial researchers understand the recent advances as well as new developments in this fast-paced, high-impact domain.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing is an umbrella term for human emotion, sentiment, and feelings  [1] , emotion recognition, and sentiment analysis. Since the concept of affective computing  [2]  was proposed by Prof. Picard in 1997, it has been guiding computers to identify and express emotions and respond intelligently to human emotions  [3] . In many practical applications, it is desired to build a cognitive, intelligent system  [4]  that can distinguish and understand people's affect, and meanwhile make sensitive and friendly responses promptly  [5, 6] . For example, in an intelligent vehicle system, the real-time monitoring of the driver's emotional state and the necessary response based on the monitored results can effectively reduce the possibility of accidents  [7, 8] . In social media, affective computing can avidly help to understand the opinions being expressed on different platforms  [9] . Thus, many researchers  [3, 10]  believe that affective computing is the key to promoting and advancing the development of human-centric AI and human intelligence.\n\nAffective computing involves two distinct topics: emotion recognition and sentiment analysis  [11] [12] [13] [14] . To understand and compute the emotion or sentiment, psychologists proposed two typical theories to model human emotion: discrete emotion model (or categorical emotion model)  [15]  and dimensional emotion model  [16] . The emotion recognition aims to detect the emotional state of human beings (i.e., discrete emotions or dimensional emotions)  [17] , and mostly focuses on visual emotion recognition (VER)  [18] , audio/speech emotion recognition (AER/SER)  [19] , and physiological emotion recognition (PER)  [20] . In contrast, sentiment analysis mostly concentrates on textual evaluations and opinion mining  [21]  on social events, marketing campaigns, and product preferences. The result of sentiment analysis is typically positive, negative, or neutral  [10, 22] . Considering that one person in a happy mood typically has a positive attitude toward the surrounding environment, emotion recognition and sentiment analysis can be overlapped. For example, a framework based on the context-level inter-modal attention  [23]  was designed to predict the sentiment (positive or negative) and recognize expressed emotions (anger, disgust, fear, happiness, sadness, or surprise) of an utterance.\n\nRecent advances in affective computing have facilitated the release of public benchmark databases, mainly consisting of unimodal databases (i.e., textual, audio, visual, and physiological databases) and multimodal databases. These commonly used databases have, in turn, motivated the development of machine learning (ML)-based and deep learning (DL)-based affective computing.\n\nA study  [24]  revealed that human emotions are expressed mainly through facial expressions (55%), voice (38%), and language (7%) in daily human communication. Throughout this review, textual, audio, and visual signals are collectively referred to as physical data. Since people tend to express their ideas and thoughts freely on social media platforms and websites, a large amount of physical affect data can be easily collected. Based on these data, many researchers pay attention to identifying subtle emotions expressed either explicitly or implicitly  [25] [26] [27] . However, physical-based affect recognition may be ineffective since humans may involuntarily or deliberately conceal their real emotions (so-called social masking)  [20] . In contrast, physiological signals (e.g., EEG and ECG) are not subject to these constraints as spontaneous physiological activities associated with emotions are hardly changed by oneself. Thus, EEG-based or ECG-based emotion recognition can generate more objective predictions in real time and provide reliable features of emotional states  [29, 30] .\n\nHuman affect is a complex psychological and physiological phenomenon  [30] . As human beings naturally communicate and express emotion or sentiment through multimodal information, more researches focus on multi-physical modality fusion for affective analysis  [31] . For example, in a conversation scenario, a person's emotional state can be demonstrated by the words of speech, the tones of voice, facial expressions and emotion-related body gestures  [32] . Textual, auditory and visual information together provide more information than they do individually  [33] , just like the brain validating events relies on several sensory input sources. With the rapid progress of physical-touching mechanisms or intrusive techniques such as low-cost wearable sensors, some emotion recognition methods are based on multimodal physiological signals (e.g., EEG, ECG, EMG and EDA). By integrating physiological modalities with physical modalities, physical-physiological affective computing can detect and recognize subtle sentiments and complex emotions  [34, 35] . It is worth mentioning that the suitable selection of the unimodal emotion data and multimodal fusion strategies  [36]  are the two key components of multimodal affective analysis systems, which often outperform the unimodal emotion recognition systems  [37] . Therefore, this review not only provides a summary of the multimodal affective analysis, but also introduces an overview of unimodal affect recognition.\n\nAlthough there exist many survey papers about affective computing, most of them focus on physicalbased affect recognition. For facial expression recognition (FER), existing studies have provided a brief review of FER  [38] , DL-based FER systems  [39] , facial micro-expressions analysis (FMEA)  [40] , and 3D FER  [41] . Besides, the work  [42]  discussed the research results of the sentiment analysis using transfer learning algorithms, whereas authors  [43]  surveyed the DL-based methods for SER. However, there are just a few review papers related to physiological-based emotion recognition and physical-physiological fusion for affective analysis in recent years. For example, Jiang et al.  [17]  discussed multimodal databases, feature extraction based on physical signals or EEG, and multimodal fusion strategies and recognition methods.\n\nSeveral issues have not been thoroughly addressed in previous reviews: 1) Existing reviews take a specialist view and lack a broader perspective, for instance when classifying the various methods and advances, some reviews do not consider DL-based affect recognition or multimodal affective analysis; 2) Existing reviews do not provide a clear picture about the performance of state-of-the-art methods and the implications of their recognition ability. As the most important contribution of our review, we aim to cover different aspects of affective computing by introducing a series of research methods and results as well as discussions and future works.\n\nTo sum up, the major contributions of this paper are multi-fold:\n\n1) To the best of our knowledge, this is the first review that categorizes affective computing into two broad classes, i.e., unimodal affect recognition and multimodal affective analysis, and further taxonomizes them based on the data modalities. 2) In the retrospect of 20 review papers released between 2017 and 2020, we present a systematic review of more than 380 research papers published in the past 20 years in leading conferences and journals. This leads to a vast body of works, as taxonomized in Fig.  1 , which will help the reader navigate through this complex area. 3) We provide a comprehensive taxonomy of state-of-the-art (SOTA) affective computing methods from the perspective of either ML-based methods or DL-based techniques and consider how the different affective modalities are used to analyze and recognize affect. 4) Benchmark databases for affective computing are categorized by four modalities and videophysiological modalities. The key characteristics and availability of these databases are summarized.\n\nBased on publicly used databases, we provide a comparative summary of the properties and quantitative performance of some representative methods. 5) Finally, we discuss the effects of unimodal, multimodal, models as well as some potential factors on affective computing and some real-life applications of that, and further indicate future researches on emotion recognition and sentiment analysis. The paper is organized as followed. Section 2 introduce the existing review works, pinpointing useful works and better identifying the contributions of this paper. Section 3 surveys two kinds of emotion models, the discrete and the dimensional one. Then Section 4 discusses in detail four kinds of databases, which are commonly used to train and test affective computing algorithms. We then provide an extensive review of the most recent advances in affective computing, including unimodal affect recognition in Section 5 and multimodal affective analysis in Section 6. Finally, discussions are presented in Section 7, while conclusions and new developments are stated in Section 8. Table  1  lists the main acronyms used herein for the reader's reference.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Table 1. Main Acronyms.",
      "text": "Table  2 . Overview of the most relevant reviews related to affective computing published between 2017 and 2020 and our proposed review.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Reviews On Physical-Based Affect Recognition",
      "text": "The existing researches on physical-based affect recognition mainly used visual, textual, and audio modalities  [12] . For visual modality, most studies surveyed FER  [38, 39, 41, 44]  and FMEA  [40] , and 3D FER  [41] . Besides, Noroozi et al.  [45]  reviewed representation learning and emotion recognition from the body gestures followed by multimodal emotion recognition on the basis of speech or face and body gestures. For textual modality, the works on sentiment analysis and emotion recognition  [42, 43, [46] [47] [48]  can be completed according to the implicit emotions in the conversation. Han et al.  [49]  presented DLbased adversarial training using three kinds of physical signals, aiming to address various challenges associated with emotional AI systems. In contrast, Erik et al.  [12]  considered multimodal fusion and provided a critical analysis of potential performance improvements with multimodal affect recognition under different fusion categories, compared to unimodal analysis.\n\nAll reviews on physical-based affect recognition listed in Table  2  have reviewed DL-based methods, which indicates a clear development trend in the application of deep learning for this domain. However, the existing works have not fully involved the latest research advances and achievements in DL-based affective computing, which is one of the objectives of our review.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Reviews On Physiological-Based Emotion Recognition",
      "text": "The emerging development of physiological emotion recognition has been made possible by the utilization of embedded devices for the acquisition of physiological signals. In 2019, Bota et al.  [50]  reviewed ML-based emotion recognition using different physiological signals, along with the key theoretical concepts and backgrounds, methods, and future developments. Garcia-Martinez et al.  [51]  reviewed nonlinear EEG-based emotion recognition and identified some nonlinear indexes in future research. Alarcã o and Fonseca  [29]  also reviewed works about EEG-based emotion recognition from 2009 to 2016, but focused on subjects, feature representation, classification, and their performances.\n\nAll reviews on physiological-based emotion recognition listed in Table  2  have reviewed the ML-based methods in discrete and dimensional emotion space, but only one work involved a few DL-based methods. In this review, we surveyed ML-based and DL-based works which have contributed to the advance of physiological-based emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Reviews On Physical-Physiological Fusion For Affective Analysis",
      "text": "There is a clear trend of using both physical and physiological signals for affective analysis. In 2019, Rouast et al.  [13]  reviewed 233 DL-based human affective recognition methods that use audio-visual and physiological signals to learn spatial, temporal, and joint feature representations. In 2020, Zhang et al.  [20]  introduced different feature extraction, feature reduction, and ML-based classifiers in terms of the standard pipeline for multi-channel EEG emotion recognition, and discussed the recent advances of multimodal emotion recognition based on ML or DL techniques. Jiang et al.  [17]  summarized the current development of multimodal databases, the feature extraction based on EEG, visual, audio and text information, multimodal fusion strategies, and recognition methods according to the pipeline of the realtime emotion health surveillance system. Shoumy et al.  [14]  reviewed different frameworks and lasted techniques using textual, audio, visual and physiological signals, and extensive analysis of their performances. In the end, various applications of affective analysis were discussed followed by their trends and future works.\n\nAll these recent works listed in Table  2  have reviewed general aspects related to affective computing including emotion models, unimodal affect recognition and multimodal fusion for affective analysis as well as ML-based and DL-based models. However, the existing works have not fully elaborated their comparisons in unimodal and multimodal affective analysis, which is one of the objectives of our review.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Models",
      "text": "The definition of the emotion or affect is essential to establish a criterion for affective computing. The basic concept of emotions was first introduced by Ekman  [52]  in the 1970s. Although psychologists attempt to classify emotions in different ways in the multidisciplinary fields of neuroscience, philosophy, and computer science  [53] , there are no unanimously accepted emotion models. However, there are two types of generic emotion models in affective computing, namely discrete emotion model  [52]  and dimensional emotion model (or continuous emotion model)  [16, 54] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discrete Emotion Model",
      "text": "The discrete emotion model, also called as categorical emotion model, defines emotions into limited categories. Two widely used discrete emotion models are Ekman's six basic emotions  [52]  and Plutchik's emotional wheel model  [55] , as shown in Fig.  2  (a) and Fig.  2  (b), respectively.\n\nEkman's basic emotion model and its variants  [56, 57]  are widely accepted by the emotion recognition community  [58, 59] . Six basic emotions typically include anger, disgust, fear, happy, sad, and surprise. They were derived with the following criteria  [52] : 1) Basic emotions must come from human instinct; 2) People can produce the same basic emotions when facing the same situation; 3) People express the same basic emotions under the same semantics; 4) These basic emotions must have the same pattern of expression for all people. The development of Ekman's basic emotion model is based on the hypothesis that human emotions are shared across races and cultures. However, different cultural backgrounds may have different interpretations of basic emotions, and different basic emotions can be mixed to produce complex or compound emotions  [15] .\n\nIn contrast, Plutchik's wheel model  [55]  involves eight basic emotions (i.e., joy, trust, fear, surprise, sadness, anticipation, anger, and disgust) and the way of how these are related to one another (Fig.  2 (b) ). For example, joy and sadness are opposites, and anticipation can easily develop into vigilance. This wheel model is also referred to as the componential model, where the stronger emotions occupy the centre, while the weaker emotions occupy the extremes, depending on their relative intensity levels. These discrete emotions can be generally categorized into three kinds of polarity (positive, negative, and neutral), which are often used for sentiment analysis. To describe fine-grained sentiments, ambivalent sentiment handling  [60]  is proposed to analyze the multi-level sentiment and improve the performance of binary classification.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Dimensional Emotion Model",
      "text": "To overcome the challenges confronted by the discrete emotion models, many researchers have adopted the concept of a continuous multi-dimensional model. One of the most recognized models is the Pleasure-Arousal-Dominance (PAD)  [16] , as shown in Fig.  3    Similar to Mehrabianś three-dimensional space theory of emotion  [61] , the PAD model has threedimensional spaces: 1) Pleasure (Valence) dimension, representing the magnitude of human joy from distress extreme to ecstasies; 2) Arousal (Activation) dimension, measuring physiological activity and psychological alertness level; 3) Dominance (Attention) dimension, expressing the feeling of influencing the surrounding environment and other people, or of being influenced by the surrounding environment and others.\n\nSince two dimensions of Pleasure and Arousal in the PAD model could represent the vast majority of different emotions  [62] , Russell  [54]  proposed a Valence-Arousal based circumplex model to represent complex emotions. This model defines a continuous, bi-dimensional emotion space model with the axes of Valence (the degree of pleasantness or unpleasantness) and Arousal (the degree of activation or deactivation), as shown in Fig.  3 (b ). The circumplex model consists of four quadrants. The first quadrant, activation arousal with positive valence, shows the feelings associated with happy emotions; And the third quadrant, with low arousal and negative valence, is associated with sad emotions. The second quadrant shows angry emotions within high arousal and negative valence; And the fourth quadrant shows calm emotion within low arousal and positive valence  [63] .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Databases For Affective Computing",
      "text": "Databases for affective computing can be classified into textual, speech/audio, visual, physiological, and multimodal databases according to the data modalities. The properties of these databases have a farreaching influence on the model design and network architecture for affective computing.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Textual Databases",
      "text": "Databases for TSA consist of the text data in different granularities (e.g., word, sentence, and document), labelled with emotion or sentiment tags (positive, negative, neutral, emphatic, general, sad, happy, etc). The earliest textual sentiment database is Multi-domain sentiment (MDS)  [64, 65] , which contains more than 100,000 sentences of product reviews acquired from Amazon.com. These sentences are labelled with both two sentiment categories (positive and negative) and five sentiment categories (strong positive, weak positive, neutral, weak negative, strong negative).\n\nAnother widely-used large database for binary sentiment classification is IMDB  [66] . It provides 25,000 highly polar movie reviews for training and 25,000 for testing. Stanford sentiment treebank (SST)  [67]  is the semantic lexical database annotated by Stanford University. It includes fine-grained emotional labels of 215,154 phrases in a parse tree of 11,855 sentences, and it is the first corpus with fully labelled parse trees.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Speech/Audio Databases",
      "text": "Speech databases can be divided into two types: non-spontaneous (simulated and induced) and spontaneous. In the early stage, non-spontaneous speech databases were mainly generated from professional actors' performances. Such performance-based databases are regarded as reliable ones because they can perform well-known emotional characteristics in professional ways. Berlin Database of Emotional Speech (Emo-DB)  [68]  contains about 500 utterances spoken by 10 actors (5 men and 5 women) in a happy, angry, anxious, fearful, bored and disgusting way. However, these non-spontaneous emotions can be exaggerated a little more than the real emotions. To narrow this gap, spontaneous speech databases have been developed recently. The Belfast Induced Natural Emotion (Belfast)  [69]  was recorded from 40 subjects (aged between 18 and 69, 20 men and 20 women) at Queen University in Northern Ireland, UK. Each subject took part in five tests, each of which contains short video recordings (5 to 60 seconds in length) with stereo sound, and related to one of the five emotional tendencies: anger, sadness, happiness, fear, and neutrality.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Visual Databases",
      "text": "Visual databases can also be divided into two categories: facial expression databases and body gesture emotion databases.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "4.3.1",
      "text": "Facial expression databases Table  3  provides an overview of facial expression databases, including the main reference (access), year, samples, subject, and expression category. The early FER databases are derived from the emotions purposely performed by subjects in the laboratory (In-the-Lab). For example, JAFFE  [70]  released in 1998, includes 213 images of 7 facial expressions, posed by 10 Japanese female models. To construct the extended Cohn-Kanade (CK+)  [71] , an extension of CK  [72] , subjects were instructed to perform 7 facial expressions. The facial expression images were recorded and analyzed to provide protocols and baseline results for facial feature tracking, action units (AUs), and emotion recognition. Different from CK+, MMI  [73]  consists of onset-apex-offset sequences. Oulu-CASIA NIR-VIS (Oulu-CASIA)  [74]  released in 2011, includes 2,880 image sequences captured with one of two kinds of imaging systems under three kinds of illumination conditions. In-the-Lab In-the-Wild There are various 3D/4D databases designed for multi-view and multi-pose FER. Binghamton University 3D Facial Expression (BU-3DFE)  [75]  contains 606 facial expression sequences captured from 100 people with one of six facial expressions. BU-4DFE  [76]  is developed based on the dynamic 3D space, which includes 606 high-resolution 3D facial expression sequences. BP4D  [77]  released in 2014, is a well-annotated 3D video database consisting of spontaneous facial expressions, elicited from 41 participants (23 women, 18 men), by well-validated emotion inductions. 4DFAB  [78]  released in 2018, contains at least 1,800,000 dynamic high-resolution 3D faces captured from 180 subjects in four different sessions spanning.\n\nSimilar to MMI  [73] , all micro-expression databases consist of onset-apex-offset sequences. Spontaneous Micro-expression (SMIC)  [79]  contains 164 micro-expression video clips elicited from 16 participants. CASME II  [80]  has videos with relatively high temporal and spatial resolution. The participants' facial expressions have been elicited in a well-controlled laboratory environment and proper illumination. Spontaneous Micro-Facial Movement (SAMM)  [81]  is a currently-public database that contains sequences with the highest resolution, and its participants are from diverse ethnicities and the widest range of ages.\n\nActed facial expression databases are often constructed in a specific environment. Another way to build the databases is collecting facial expression images/videos from the Internet, which we refer to as In-the-Wild. FER2013  [82]  is a firstly public large-scale and unconstrained database that contains 35,887 grey images with 48×48 pixels, collected automatically through the Google image search API. Static Facial Expressions In-the-Wild (SFEW 2.0)  [83]  is divided into three sets, including Train (891 images) and Val (431 images), labelled as one of six basic expressions (anger, disgust, fear, happiness, sadness and surprise), as well as the neutral and Test (372 images) without expression labels. EmotioNet  [84]  consists of one million images with 950,000 automatically annotated AUs and 25,000 manually annotated AUs. Expression in-the-Wild (ExpW)  [85]  contains 91,793 facial images, manually annotated as one of seven basic facial expressions. Non-face images were removed in the annotation process. AffectNet  [86]  contains over 1,000,000 facial images, of which 450,000 images are manually annotated as one of eight discrete expressions (six basic expressions plus neutral and contempt), and the dimensional intensity of valence and arousal. Real-world Affective Face Database (RAF-DB)  [87]  contains 29,672 highly diverse facial images downloaded from the Internet, with manually crowd-sourced annotations (seven basic and eleven compound emotion labels). Dynamic Facial Expression in the Wild (DFEW)  [88]  consists of over 16,000 video clips segmented from thousands of movies with various themes. Professional crowdsourcing is applied to these clips, and 12,059 clips have been selected and labelled with one of 7 expressions (six basic expressions plus neutral).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "4.3.2",
      "text": "Body gesture emotion databases Although studies in emotion recognition focused mainly on facial expressions, a growing number of researchers in affective neuroscience demonstrates the importance of the full body for unconscious emotion recognition. In general, bodily expressive cues are easier to be perceived than subtle changes in the face. To capture natural body movements, body gesture emotion databases contain a corpus of video sequences collected from either real life or movies. Table  4  provides an overview of body gesture emotion databases, as described next.\n\nEmoTV  [89]  contains the interview video sequences from French TV channels. It has multiple types of annotations but is not publicly available. To our best knowledge, FAce and BOdy database (FABO)  [90]  is the first publicly available bimodal database containing both face and body gesture. The recordings for each subject take more than one-hour store rich information of various affect statements. Moreover, THEATER Corpus  [91]  consists of sections from two movie versions which are coded with eight affective states corresponding to the eight corners of PAD space  [92] . GEneva Multimodal Emotion Portrayals (GEMEP)  [93]  is a database of body postures and gestures collected from the perspectives of both an interlocutor and an observer. The GEMEP database is one of the few databases that have frame-by-frame AU labels  [94] . Emotional body expression in daily actions database (EMILYA)  [95]  collected body gestures in daily motions. Participants were trained to be aware of using their bodies to express emotions through actions. EMILYA includes not only videos of facial and bodily emotional expressions, but also 3D data of the whole-body movement. DEAP  [96]  comprises a 32-channel EEG, a 4-channel EOG, a 4-channel EMG, RESP, plethysmograph, Galvanic Skin Response (GSR) and body temperature, collected from 32 subjects. Immediately after watching each video, subjects were required to rate their truly-felt emotion from five dimensions: valence, arousal, dominance, liking and familiarity. SEED  [97, 98]  contains EGG recordings from 15 subjects. In their study, participants were asked to experience three EEG recording sessions, with an interval of two weeks between two successive recording sessions. Within each session, each subject was exposed to the same sequence of fifteen movie excerpts, each one approximately fourminute-long, to induce three kinds of emotions: positive, neutral, and negative. Some databases are task-driven. For example, Detecting Stress during Real-World Driving Tasks (DSdRD)  [7]  is used to determine the relative stress levels of drivers. It contains various signals from 24 volunteers while having a rest for at least 50 minutes after their driving tasks. These volunteers are asked to fill out questionnaires which are used to map their state into low, medium, and high-stress levels. AMIGOS  [99]  was designed to collect participants' emotions in two social contexts: individual and group. AMIGOS was constructed in 2 experimental settings: 1) 40 participants watch 16 short emotional videos; 2) they watch 4 long videos, including a mix of lone and group sessions. These emotions were annotated with self-assessment of affective levels and external assessment of valence and arousal. Wearable devices help to bridge the gap between lab studies and real-life emotions. Wearable Stress and Affect Detection (WESAD)  [100]  is built for stress detection, providing multimodal, high-quality data, including three different affective states (neutral, stress, amusement).",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Multimodal Databases",
      "text": "In our daily life, people express and/or understand emotions through multimodal signals. Multimodal databases can be mainly divided into two types: multi-physical and physical-physiological databases. Table  6  provides an overview of multimodal databases, as described next. Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [101]  is constructed by the Speech Analysis and Interpretation Laboratory. During recording, 10 actors are asked to perform selected emotional scripts and improvised hypothetical scenarios designed to elicit 5 specific types of emotions. The face, head, and hands of actors are marked to provide detailed information about their facial expressions and hand movements while performing. Two famous emotion taxonomies are employed to label the utterance level: discrete categorical-based annotations and continuous attribute-based annotations. Afterwards, CreativeIT  [102, 103]  contains detailed full-body motion visual-audio and text description data collected from 16 actors, during their affective dyadic interactions ranging from 2-10 minutes each. Two kinds of interactions (twosentence and paraphrases exercises) are set as improvised. According to the video frame rate, the annotator gave the values of each actor's emotional state in the three dimensions. Harvesting Opinions from the Web database (HOW)  [104]  contains 13 positive, 12 negative and 22 neutral videos captured from YouTube. Institute for Creative Technologies Multimodal Movie Opinion database (ICT-MMMO)  [105]  contains 308 YouTube videos and 78 movie review videos from ExpoTV. It has five sentiment labels: strongly positive, weakly positive, neutral, strongly negative, and weakly negative. As far as we know, Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)  [106]  is the largest database for sentiment analysis and emotion recognition, consisting of 23,453 sentences and 3,228 videos collected from more than 1,000 online YouTube speakers. Each video contains a manual transcription that aligns audio and phoneme grades.\n\nMAHNOB-HCI  [107]  is a video-physiological database. Using 6 video cameras, a head-worn microphone, an eye gaze tracker, and physiological sensors, it is constructed by monitoring and recording the emotions of 27 participants while watching 20 films. Remote Collaborative and Affective Interactions (RECOLA)  [108]  consists of a multimodal corpus of spontaneous interactions from 46 participants (in French). These participants work in pairs to discuss a disaster scenario escape plan and reach an agreement via remote video conferencing. The recordings of the participants' activities are annotated by 6 annotators with two continuous emotional dimensions: arousal and valence, as well as social behaviour labels on five dimensions. DECAF  [109]  is a Magnetoencephalogram-based database for decoding affective responses of 30 subjects while watching 36 movie clips and 40 one-minute music video clips. DECAF contains a detailed analysis of the correlations between participants' self-assessments and their physiological responses, single-trial classification results for valence, arousal and dominance dimensions, performance evaluation against existing data sets, and time-continuous emotion annotations for movie clips.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Unimodal Affect Recognition",
      "text": "In this section, we systematically summarize the unimodal affect recognition methods from the perspective of affect modalities: physical modalities (e.g., textual, audio, and visual)  [12]  and physiological modalities (e.g., EEG and ECG)  [29] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Textual Sentiment Analysis",
      "text": "With the rapid increase of online social media and e-commerce platforms, where users freely express their ideas, a huge amount of textual data are generated and collected. To identify subtle sentiment or emotions expressed explicitly or implicitly from the user-generated data, textual sentiment analysis (TSA) was introduced  [110] . Traditional approaches of TSA  [111, 112]  often rely on the process known as \"feature engineering\" to find useful features that are related to sentiment. This is a tedious task. DLbased models can realize an end-to-end sentiment analysis from textual data.\n\nTable  7  shows an overview of some representative methods for TSA, where the publication (or preprint) year, analysis granularity, feature representation, classifier, database, and performance are presented. Next, the works of TSA are described in detail.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "5.1.1",
      "text": "ML-based TSA TSA based on the traditional ML methods mainly relies on knowledge-based techniques or statistical methods  [113] . The former requires thesaurus modelling of large emotional vocabularies, and the latter assumes the availability of large databases, labelled with polarity or emotional labels.\n\nKnowledge-based TSA. Knowledge-based TSA is often based on lexicons and linguistic rules. Different lexicons, such as WordNet, WordNet-Affect, SenticNet, MPQA, and SentiWordNet  [114] , often contain a bag of words and their semantic polarities. The lexicon-based approaches can classify a given word into positive or negative, but perform poorly without linguistic rules. Hence, Ding et al.  [115]  adopted a lexiconbased holistic approach that combines external evidence with linguistic conventions in natural language to evaluate the semantic orientation in reviews. Melville et al.  [116]  developed a framework for domaindependent sentiment analysis using lexical association information.\n\nTo better understand the orientation and flow of sentiment in natural language, Poria et al.  [117]  proposed a new framework combining computational intelligence, linguistics and common-sense computing  [118] . They found that the negation played an important role in judging the polarity of the overall sentence. Jia et al.  [119]  also verified the importance of negation on emotional recognition through extensive experiments. Blekanov et al.  [120]  utilized specifics of the Twitter platform in their multi-lingual knowledge-based approach of sentiment analysis. Due to the limitations of knowledge itself, knowledge-based models are limited to understanding only those concepts that are typical and strictly defined.\n\nStatistical-based TSA. Statistical-based TSA relies more on an annotated dataset to train an ML-based classifier by using prior statistics or posterior probability. Compared with lexicon-based approaches  [121, 122] , statistical-based TSA approaches are more suitable for sentiment analysis due to their ability to deal with large amounts of data. Mullen and Collier  [123]  used the semantic orientation of words to create a feature space that is classified by a designed SVM. Pak et al.  [124]  proposed a new sub-graphbased model. It represents a document as a collection of sub-graphs and inputs the features from these sub-graphs into an SVM classifier. Naï ve Bayes (NB) is another powerful and widely-used classifier, which assumes that dataset features are independent. It can be used to filter out the sentences that do not support comparative opinions  [125] .\n\nHybrid-based TSA. By integrating knowledge with statistic models, hybrid-based TSA can take full advantage of both  [126] . For example, Xia et al.  [127]  utilized SenticNet and a Bayesian model for contextual concept polarity disambiguation. Due to the ambiguity and little information of neutral between positive and negative, Valdivia et al.  [128]  proposed consensus vote models and weighted aggregation to detect and filter neutrality by representing the vague boundary between two sentiment polarities. According to experiments, they concluded that detecting neutral can help improve the performance of sentiment analysis. Le et al.  [129]  proposed a novel hybrid method in the combination of word sentiment score calculation, text pre-processing, sentiment feature generation and an ML-based classifier for sentiment analysis. The hybrid method  [129]  performed much better than popular lexiconbased methods in Amazon, IMDb, and Yelp, and achieved an average accuracy of 87.13%. Li et al.  [130]  utilized lexicons with one of the ML-based methods including NB and SVM. This hybrid method is more effective in detecting expressions that are difficult to be polarized into positive-negative categories.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "5.1.2",
      "text": "DL-based TSA DL-based techniques have a strong ability to automatically learn and discover discriminative feature representations from data themselves. DL-based TSA has been proved to be successful with the success of word embeddings  [131]  and the increase of the training data with multi-class classification  [132] . Various DL-based approaches for TSA include deep convolutional neural network (ConvNet) learning, deep RNN learning, deep ConvNet-RNN learning and deep adversarial learning, as detailed next.\n\nDeep ConvNet learning for TSA. CNN-based methods have been applied to different levels of TSA including document-level  [133] , sentence-level  [134] , and aspect-level (or word-level)  [135]  by using different filters to learn local features from the input data. Yin et al.  [136]  proposed a framework of sentence-level sentiment classification based on the semantic lexical-augmented CNN (SCNN) model, which makes full use of word information. Conneau et al.  [137]  applied a very deep CNN (VDCNN), which learns the hierarchical representations of the document and long-range dependencies to text processing. To establish long-range dependencies in documents, Johnson and Zhang  [138]  proposed a word-level deep pyramid CNN (DPCNN) model, which stacked alternately the convolutional layer and the max-pooling downsampling layer to form a pyramid to reduce computing complexity. The DPCNN with 15 weighted layers outperformed the previous best models on six benchmark databases for sentiment classification and topic categorization. For aspect-level sentiment analysis, Huang and Carley also  [139]  proposed a novel aspect-specific CNN by combining parameterized filters and parametrized gates.\n\nDeep RNN learning for TSA. RNN-based TSA is capable of processing long sequence data. For example, Mousa and Schuller  [140]  designed a novel generative approach, contextual Bi-LSTM with a language model (cBi-LSTM LM), which changes the structure of Bi-LSTM to learn the word's contextual information based on its right and left contexts. Moreover, Wang et al.  [141]  proposed a model of recursive neural conditional random field (RNCRF) by integrating the RNN-based dependency tree of the sentence and conditional random fields.\n\nThe attention mechanism contributes to prioritizing relevant parts of the given input sequence according to a weighted representation at a low computational cost. In the paradigm of attention-based LSTM for TSA, LSTM helps to construct the document representation, and then attention-based deep memory layers compute the ratings of each document. Chen et al.  [142]  designed the recurrent attention memory (RAM) for aspect-level sentiment analysis. Specifically, a multiple-attention mechanism was employed to capture sentiment features separated by a long distance, the results of which were non-linearly combined with RNN. Inspired by the capability of human eye-movement behavior, Mishra et al.  [143]  introduced a hierarchical LSTM-based model trained by cognition grounded eye-tracking data, and they used the model to predict the sentiment of the overall review text. More recently, some researchers  [144, 145]  suggested that user preferences and product characteristics should be taken into account.\n\nFor document-level sentiment classification, Dou  [145]  proposed a deep memory network combining LSTM on account of the influence of users who express the sentiment and the products that are evaluated. Chen et al.  [144]  designed a hierarchical LSTM with an attention mechanism to generate sentence and document representations, which incorporates global user and product information to prioritize the most contributing items. Considering the irrationality of encoding user information and product information as one representation, Wu et al.  [146]  designed an attention LSTM-based model, which executed hierarchical user attention and product attention (HUAPA) to realize sentiment classification.\n\nFor multi-task classification (e.g., aspect category and sentiment polarity detection), J et al.  [147]  proposed convolutional stacked Bi-LSTM with a multiplicative attention network concerning global-local information. In contrast, to fully exploit contextual affective knowledge in aspect-level TSA, Liang et al.  [148]  proposed GCN-based SenticNet to enhance graph-based dependencies of sentences. Specifically, LSTM layers were employed to learn contextual representations, and GCN layers were built to capture the relationships between contextual words in specific aspects.\n\nDeep ConvNet-RNN learning for TSA. Although ConvNet-based or RNN-based models have been extensively employed to generate impressive results in TSA, more researchers  [149, 150]  have tried to combine both ConvNets and RNNs to improve the performance of TSA by getting the benefits offered by each model.\n\nAs the CNN is proficient in extracting local features and the BiLSTM is skilled in a long sequence, Li et al.  [150]  combined CNN and BiLSTM in a parallel manner to extract both types of features, improving the performance of sentiment analysis. To decrease the training time and complexity of LSTM and attention mechanism for predicting the sentiment polarity, Xue et al.  [151]  designed a gated convolutional network with aspect embedding (GCAE) for aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). The GCAE uses two parallel CNNs, which output results combined with the gated unit and extended with the third CNN, extracting contextual information of aspect terms.\n\nTo distinguish the importance of different features, Basiri et al.  [152]  proposed an attention-based CNN-RNN deep model (ABCDM), which utilized bidirectional LSTM and GRU layers to capture temporal contexts and apply the attention operations on the discriminative embeddings of outputs generated by two RNN-based networks. In addition, CNNs were employed for feature enhancement (e.g., feature dimensionality reduction and position-invariant feature extraction). All the above works focused on detecting sentiment or emotion, but it is important to predict the intensity or degree of one sentiment in the description of human intimate emotion. To address the problem, Akhtar et al.  [153]  proposed a stacked ensemble method by using an MLP to ensemble the outputs of the CNN, LSTM, GUR, and SVR.\n\nDeep adversarial learning for TSA. Deep adversarial learning with the ability to regularize supervised learning algorithms was introduced to text classification  [154] . Inspired by the domain-adversarial neural network  [155] , Li et al.  [156]  constructed an adversarial memory network model that contains sentiment and domain classifier modules. Both modules were trained together to reduce the sentiment classification error and allowed the domain classifier not to separate both domain samples. The attention mechanism is incorporated into the deep adversarial based model to help the selection of the pivoted words, which are useful for sentiment classification and shared between the source and target domains. Li et al.  [157]  initiated the use of GANs  [158]  in sentiment analysis, reinforcement learning and recurrent neural networks to build a novel model, termed category sentence generative adversarial network (CS-GAN). By combining GANs with RL, the CS-GAN can generate more category sentences to improve the capability of generalization during supervised training. Similarly, to tackle the sentiment classification in low-resource languages without adequate annotated data, Chen et al.  [159]  proposed an adversarial deep averaging network (ADAN) to realize cross-lingual sentiment analysis by transferring the knowledge learned from labelled data on a resource-rich source language to low-resource languages where only unlabeled data existed. Especially, the ADAN was trained with labelled source text data from English and unlabeled target text data from Arabic and Chinese. More recently, Karimi et al.  [160]  fine-tuned the general-purpose BERT and domain-specific post-trained BERT using adversarial training, which showed promising results in TSA.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Audio Emotion Recognition",
      "text": "Audio emotion recognition (also called SER) detects the embedded emotions by processing and understanding speech signals  [161] . Various ML-based and DL-based SER systems have been carried out on the basis of these extracted features for better analysis  [162, 163] . Traditional ML-based SER concentrates on the extraction of the acoustic features and the selection of the classifiers. However DL-based SER constructs an end-to-end CNN architecture to predict the final emotion without considering feature engineering and selection  [164] .\n\nTable  8  shows an overview of representative methods for SER, including the most relevant papers, their publication (or preprint) year, feature representation, classifier, database, and performance (from best or average reported results). These works are next described in detail.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "5.2.1",
      "text": "ML-based SER The ML-based SER systems include two key steps: the strong features representation learning for emotional speech and an appropriate classification for final emotion prediction  [19] . Different kinds of acoustic features can be fused to get the mixed features for a robust SER. Although prosodic features and spectral features are more frequently used in SER systems  [165] , in some cases, voice-quality features and other features are sometimes more important  [166] . OpenSMILE  [167]  is a popular audio feature extraction toolkit that extracts all key features of the speech. The commonly used classifiers for SER systems encompass HMM, GMM, SVM, RF and ANN. In addition to these classifiers, the improved conventional interpretable classifiers and ensemble classifiers are also adopted for SER. In this subsection, we divided the ML-based SER systems into acoustic-feature based SER and interpretableclassifier based SER  [168] ,  [169] . Note that different combinations of models and features result in obvious differences in the performance of SER  [170] .\n\nAcoustic-feature based SER. Prosodic features (e.g. intonation and rhythm) have been discovered to convey the most distinctive properties of emotional content for SER. The prosodic features consist of fundamental frequency (rhythmical and tonal characteristics)  [171] , energy (volume or the intensity), and duration (the total of time to build vowels, words and similar constructs). Voice quality is determined by the physical properties of the vocal tract such as jitter, shimmer, and harmonics to noise ratio. Lugger and Yang  [172]  investigated the effect of prosodic features, voice quality parameters, and different combinations of both types on emotion classification. Spectral features are often obtained by transforming the time-domain speech signal into the frequency-domain speech signal using the Fourier transform  [173] . Bitouk et al.  [174]  introduced a new set of fine-grained spectral features which are statistics of Mel Frequency cepstrum coefficients (MFCC) over three phoneme type classes of interest in the utterance. Compared to prosodic features or utterance level spectral features, the fine-grained spectral features can yield results with higher accuracy. In addition, the combination of these features and prosodic features also improves accuracy. Shen et al.  [175]  utilized SVM to evaluate the performance of using energy and pitch, linear prediction cepstrum coefficients (LPCC), MFCC, and their combination. The experiment demonstrated the superior performance based on the combination of various acoustic features.\n\nIn order to improve the recognition performance of speaker-independent SER, Jin et al.  [176]  designed feature selection with L1-normalization constraint of Multiple kernel learning (MKL) and feature fusion with Gaussian kernels. This work  [176]  achieved an accuracy of 83.10% on the Berlin Database, which is 2.10% higher than the model that used the sequential floating forward selection algorithm, and a GMM  [177] . Wang et al.  [178]  proposed a new Fourier parameter-based model using the perceptual content of voice quality, and the first-order and second-order differences for speaker-independent SER. Experimental results revealed that the combination of Fourier parameters and MFCC could significantly increase the recognition rate of SER. ML-based classifier for SER. The HMM classifier is extensively adopted for SER due to the production mechanism of the speech signals. In 2003, Nwe et al.  [179]  designed an HMM with log frequency power coefficients (LFPC) to detect human stress and emotion. It achieved the best recognition accuracy of 89%, which was higher than 65.8% of human recognition. The GMM and its variants can be considered as a special continuous HMM and are appropriate for global-feature based SER. For example, Navas et al.  [180]  proposed a GMM-based baseline method by using prosodic, voice quality, and MFCC features.\n\nDifferent from HMM and GMM, SVM maps the emotion vector to a higher dimensional space by using a kernel function and establishes the maximum interval hyperplane in the high-dimensional space for optimal classification. Milton et al.  [181]  designed a three-stage hierarchical SVM with linear and RBF kernels to classify seven emotions using MFCC features on the Berlin EmoDB. There are some works  [182, 183]  using different SVM classifiers with various acoustic features and their combinations. Ensemble learning has been proven to give superior performance compared to a single classifier. Yüncü et al.  [184]  designed the SVM with Binary DT by integrating the tree architecture with SVM. Bhavan et al.  [185]  proposed a bagged ensemble comprising of SVM with different Gaussian kernels as a viable algorithm. Considering differences among various categories of human beings, Chen et al.  [186]  proposed the twolayer fuzzy multiple RF by integrating the decision trees with Bootstraps to recognize six emotional states.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "5.2.2",
      "text": "DL-based SER DL-based SER systems can understand and detect contexts and features of emotional speech without designing a tailored feature extractor. The CNNs with auto-encoder  [187]  are regarded as commonly used techniques for DL-based SER  [187] . The RNNs  [188]  and their variants (e.g., Bi-LSTM)  [189]  are widely introduced to capture temporal information. The hybrid deep learning for SER includes ConvNets and RNNs, as well as attention mechanisms  [190, 191] . For the issues of limited data amount and low quality of the databases, adversarial learning can be used for DL-based SER  [192]  by augmenting trained data and eliminating perturbations.\n\nConvNet learning for SER. Huang et al.  [163]  utilized the semi-CNN to extract features of spectrogram images, which are fed into SVM with different parameters for SER. Badshah et al.  [193]  proposed an endto-end deep CNN with three convolutional layers and three fully connected layers to extract discriminative features from spectrogram images and predict seven emotions. Zhang et al.  [194]  designed AlexNet DCNN pre-trained on ImageNet with discriminant temporal pyramid matching (DTPM) strategy to form a global utterance-level feature representation. The deep ConvNet-based models are also used to extract emotion features from raw speech input for multi-task SER  [195] .\n\nRNN learning for SER. RNNs can process a sequence of speech inputs and retain its state while processing the next sequence of inputs. They learn the short-time frame-level acoustic features and aggregate appropriately these features over time into an utterance-level representation. Considering the different emotion states that may exist in a long utterance, RNN and its variants (e.g. LSTM and Bi-LSTM) can tackle the uncertainty of emotional labels. Extreme learning machine (ELM), a single-hidden layer neural network, was regarded as the utterance-level classifier  [196] . Lee and Tashev  [188]  proposed the Bi-LSTM with ELM to capture a high-level representation of temporal dynamic characteristics. Ghosh et al.  [197]  pre-trained a stacked denoising autoencoder to extract low-dimensional distributed feature representation and trained Bi-LSTM-RNN for final emotion classification of speech sequence.\n\nThe attention model is designed to ignore irrelevantly emotional frames and other parts of the utterance. Mirsamadi et al.  [198]  introduced an attention model with a weighted time-pooling strategy into the RNN to more emotionally salient regions. Because there exists some irrelevantly emotional silence in the speech, the silence removal needs to be employed before BLSTMs, incorporated with the attention model used for feature extraction  [190] . Chen et al.  [199]  proposed the attention-based 3D-RNNs to learn discriminative features for SER. Experimental results show that the Attention-3D-RNNs can exceed the performance of SOTA SER on IEMOCAP and Emo-DB in terms of unweighted average recall.\n\nThe attention model is designed to ignore irrelevant emotional frames in the utterance. Mirsamadi et al.  [198]  introduced an attention model with a weighted time-pooling strategy into the RNN to highlight more emotionally salient regions. Since there exists some irrelevant emotional silence in the speech, it is necessary to combine the attention model and silence removal for feature extraction  [190] . Chen et al.  [199]  proposed the attention-based 3D-RNNs to learn discriminative features for SER, which achieved outstanding performances on IEMOCAP and Emo-DB in terms of unweighted average recall.\n\nConvNet-RNN learning for SER. As both ConvNets and RNNs have their advantages and limitations, the combination of CNNs and RNNs enables the SER system to obtain both frequency and temporal dependency  [191] . For example, Trigeorgis et al.  [200]  proposed an end-to-end SER system consisting of CNNs and LSTMs to automatically learn the best representation of the speech signal directly from the raw time representation. In contrast, Tzirakis et al.  [201]  proposed an end-to-end model comprising of CNNs and LSTM networks, which show that a deeper network is consistently more accurate than one shallow structure, with an average improvement of 10.1% and 17.9% of Arousal and Valence (A/V) on RECOLA. Wu et al.  [202]  designed the capsule networks (CapsNets) based SER system by integrating with the recurrent connection. The experiments employed on IEMOCAP demonstrated that CapsNets achieved 72.73% and 59.71% of weighted accuracy (WA) and unweighted accuracy (UA), respectively. Zhao et al.  [203]  designed a spatial CNN and an attention-based BLSTM for deep spectrum feature extraction. These features were concatenated and fed into a DNN to predict the final emotion.\n\nAdversarial learning for SER. In SER systems, the classifiers are often exposed to training data that have a different distribution from the test data. The difference in data distributions between the training and testing data results in severe misclassification. Abdelwahab and Busso  [192]  proposed the domain adversarial neural network to train gradients coming from the domain classifier, which makes the source and target domain representations closer. Sahu et al.  [204]  proposed an adversarial AE for the domain of emotion recognition while maintaining the discriminability between emotion classes. Similarly, Han et al.  [205]  proposed a conditional adversarial training framework to predict dimensional representations of emotion while distinguishing the difference between generated predictions and the ground-truth labels.\n\nGAN and its variants have been demonstrated that the synthetic data generated by generative models can enhance the classification performance of SER  [206] . To augment the emotional speech information, Bao et al.  [207]  utilized Cycle consistent adversarial networks (CycleGAN)  [208]  to generate synthetic features representing the target emotions, by learning feature vectors extracted from unlabeled source data.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Visual Emotion Recognition",
      "text": "Visual emotion recognition  [209, 12]  can be primarily categorized into facial expression recognition (FER) and body gesture emotion recognition (also known as emotional body gesture recognition, or EBGR). The following section looks at FER and EBGR in a great deal of detail, capturing a vast body of research (well-over 100 references).",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "5.3.1",
      "text": "Facial expression recognition FER is implemented using images or videos containing facial emotional cues  [210, 211] . According to whether static images or dynamic videos are to be used for facial expression representation, FER systems can be divided into static-based FER  [212]  and dynamic-based FER  [213] . When it comes to the duration and intensity of facial expression  [214, 215] , FER can be further divided into macro-FER and micro-FER (or FMER)  [216] [217] [218] . Based on the dimensions of the facial images, macro-FER can be further grouped into 2D FER  [219] [220] [221] , and 3D/4D FER  [222] [223] [224] . Note that since facial images or videos suffer from a varied range of backgrounds, illuminations, and head poses, it is essential to employ pre-processing techniques (e.g., face alignment  [225] , face normalization  [226] , and pose normalization  [227] ) to align and normalize semantic information of face region.\n\nIn this sub-section, we distinguish FER methods (shown in Fig.  4 ) via the point of whether the features are hand-crafted features based ML models  [228]  or high-level features based on DL-based models  [229] . Table  9  provides an overview of representative FER methods. The first column gives the publication reference (abbreviated to Pub.), followed by the publication (or preprint) year in the second column. The feature representations and classifiers of the referenced method are given in the third and fourth columns, respectively. The last six columns show the best or average results (%) with the given databases. These works are next described in detail.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Ml-Based Fer",
      "text": "The ML-based FER methods mainly rely on hand-crafted feature extraction and feature post-processing. Generally, hand-crafted facial features can be categorized into geometry-based features explaining the shape of the face and its components, and appearance-based features defining facial texture. The feature post-processing can be divided into two types: feature fusion and feature selection  [18] .\n\nGeometry-based FER. Ghimire and Lee  [230]  (Fig.  4  (a)) utilized 52 facial landmark points (FLP), which represent features of geometric positions and angles, for automatic FER in facial sequences. Sujono and Gunawan  [231]  used the Kinect motion sensor to detect the face region based on depth information and active shape model (AAM)  [232] . The change of key features in AAM and a fuzzy logic model is utilized to recognize facial expression based on prior knowledge derived from the facial action coding system (FACS)  [233] . As the geometry of different local structures with distortions has unstable shape representations, the local prominent directional pattern descriptor (LPDP)  [234]  was proposed by using statistical information of a pixel neighbourhood to encode more meaningful and reliable information.   [230] ; (b) Appearance-based FER adopted from  [235] ; (c) Feature fusion for 3D FER adopted from  [236] ; (d) Feature selection for FMER adopted from  [237] ; (e) ConvNet learning for FMER adopted from  [218] ; (f) ConvNet-RNN learning for FER adopted from  [238] ; (g) Adversarial learning for 3D FER adopted from  [239] .\n\nAppearance-based FER. Appearance-based approaches often extract and analyze spatial information or spatial-temporal information from the whole or specific facial regions  [240] . Tian et al.  [210]  designed an automatic face analysis system that captured fine-grained changes of facial expression into AUs  [241]  of FACS by using the permanent and transient facial features extracted by the Gabor wavelet, SIFT and local binary pattern (LBP)  [242] . Gu et al.  [243]  divided one facial image into several local regions by grids, then applied multi-scale Gabor-filter operations on local blocks, and finally encoded the mean intensity of each feature map. Yan et al.  [235]  (Fig.  4 (b) ) proposed a framework of low-resolution FER based on image filter-based subspace learning (IFSL), including deriving discriminative image filters (DIFs), their combination, and an expression-aware transformation matrix.\n\nThe local binary pattern from three orthogonal planes (LBP-TOP)  [244]  is an extension of LBP  [242]  computes over three orthogonal planes at each bin of a 3D volume formed by stacking the frames. The LBP-TOP and its variants have shown a promising performance on dynamic FER. For example, Wang et al.  [245]  designed two kinds of feature extractors (LBP-six intersection points (LBP-SIP) and supercompact LBP-three mean orthogonal planes (LBP-MOP)) based on the improved LBP-TOP to preserve the essential patterns while reducing the redundancy. Davison et al.  [246]  employed LBP-TOP features and Gaussian derivatives features for FEMR. Similarly, Liong et al.  [247]  designed a framework of FMER based on two kinds of feature extractors, consisting of optical strain flow (OSF) and block-based LBP-TOP.\n\nFeature fusion for FER. It has been proved to fuse different types of geometry-based features and appearance-based features to enhance the robustness of FER  [236] . For example, Majumder et al.  [219]  designed an automatic FER system based on the deep fusion of geometric features and LBP features using autoencoders. For the FMER task, Zhang et al.  [248]  proposed the aggregating local spatiotemporal patterns (ALSTP), which adopts cascaded fusion of local LBP-TOP and LOF extracted from 9 representative local regions of the face. For 3D/4D facial expression images, Zhen et al.  [249]  computed spatial facial deformations using a Riemannian based on dense scalar fields (DSF) and magnified them by a temporal filtering technique. For 2D/3D facial expression images, Yao et al.  [236]  (Fig.  4 (c )) used the MKL fusion strategy to combine 2D texture features, 3D shape features, and their corresponding Fourier transform maps of different face regions.\n\nFeature selection for FER. Although more 3D/4D features  [250]  or dynamic features  [251]  have different effects on FER  [252] , excessive features may break the predictive model. The feature selection is to choose a relevant and useful subset of the given set of features while identifying and removing redundant attributes. To overcome the high-dimensionality problem of 3D facial features, Azazi et al.  [253]  firstly transformed the 3D faces into 2D planes using conformal mapping, and then proposed the differential evolution (DE) to select the optimal facial feature set and SVM classifier parameters, simultaneously. Savran and Sankur  [254]  investigated the model-free 3D FER based on the non-rigid registration by selecting the most discriminative feature points from 3D facial images. As different facial regions contributed different to micro-expressions, Chen et al.  [255]  utilized weighted 3DHOG features and weighted fuzzy classification for FMER. Different from the spatial division with fixed grid, a hierarchical spatial division scheme (HSDS)  [237]  (Fig.  4 (d) ) was proposed to generate multiple types of gradually denser grids and designed kernelized group sparse learning (KGSL) to learn a set of importance weights.",
      "page_start": 17,
      "page_end": 19
    },
    {
      "section_name": "Dl-Based Fer",
      "text": "The backbone networks of DL-based FER are mostly derived from well-known pre-trained ConvNets such as VGG  [256] , VGG-face  [257] , ResNet  [258] , and GoogLeNet  [259] . Thus, we divide DL-based FER into ConvNet learning for FER, ConvNet-RNN learning for FER, and adversarial learning for FER considering the difference of network architectures.\n\nConvNet learning for FER. ConvNet-based FER often design transform learning or loss function  [221]  to overcome overfitting when using relatively small facial expression databases. For example, Yang et al.  [260]  proposed a de-expression residue learning (DeRL) to recognize facial expressions by extracting expressive information from one facial expression image. For FMER, the transform-learning based model is pre-trained on the ImageNet and several popular macro-expression databases with the original residual network  [261] . Su et al.  [218]  (Fig.  4 (e) ) proposed a novel knowledge transfer technique, which comprised a pre-trained deep teacher neural network and a shallow student neural network. Specifically, the AU-based model is trained on the residual network, which is then distilled and transferred for FMER. Liu et al.  [262]  developed an identity-disentangled FER by integrating the hard negative generation with the radial metric learning (RML). Specifically, the RML module combined inception-structured convolutional groups with an expression classification branch for the final emotion recognition by minimizing both the cross-entropy loss and RML loss. To alleviate variations introduced by personal attributes, Meng et al.  [263]  proposed an identity-aware convolutional neural network (IACNN) consisting of two identical sub-CNNs with shared weights. Besides, they designed two losses for identity-invariant FER: expression-sensitive loss and identity-sensitive loss.\n\nTo highlight the most helpful information of facial images, various attention mechanisms  [264, 265]  are proposed to discriminate distinctive features. For example, Fernandez et al.  [266]  proposed an attention network and embedded it into an encoder-decoder architecture for the facial expression representation. Similarly, Xie et al  [267]  proposed an attention-based salient expressional region descriptor (SERD) to locate the most expression-related regions that are beneficial to FER. To reduce the uncertainties of FER, Wang et al.  [268]  proposed the self-cure network consisting of the self-attention importance weighting module, the ranking regularization module, and the relabeling module. Zhu et al.  [269]  proposed a novel discriminative attention-based CNN, where the attention module was used to emphasize the unequal contributions of features for different expressions, and a dimensional distribution loss was designed to model the inter-expression relationship. To provide local-global attention across the channel and spatial location for feature maps, Gera and Balasubramanian  [270]  proposed the spatial-channel attention net (SCAN). Besides, the complementary context information (CCI) branch is proposed to enhance the discriminating ability by integrating with another channel-wise attention.\n\nExcept for the efficient network architectures with attention networks or loss functions  [271] , there is a fast and light manifold convolutional neural network (FLM-CNN) based on the multi-scale encoding strategy  [272]  or the deep fusion convolutional neural network  [273] . Due to the facial expression database biases, conditional probability distributions between source and target databases are often different. To implement a cross-database FER, Li and Deng  [274]  proposed a novel deep emotion-conditional adaption network to learn domain-invariant and discriminative feature representations. Another challenge for FER is the class imbalance of in-the-wild databases. Li et al.  [275]  proposed an adaptive regular loss function named AdaReg loss, which can re-weight category importance coefficients, to learn class-imbalanced expression representations.\n\nThe 3D ConvNet (or C3D)  [276]  has been universally used for dynamic-based FER by learning and representing the spatiotemporal features of videos or sequences. For example, the C3D was first used to learn local spatiotemporal features  [277]  and then these features were cascaded with the multimodal deepbelief networks (DBNs)  [278] . Besides, the C3D can be combined with the global attention module to represent Eulerian motion feature maps generated based on the Eulerian video magnification (EVM)  [279] . Lo et al.  [280]  utilized a 3D CNN to extract AU features and applied a graph convolutional network (GCN) to discover the dependency of AU nodes.\n\nConvNet-RNN learning for FER. For dynamic facial sequences or videos, the temporal correlations of consecutive frames should be considered as important cues. RNNs and their variants (LSTMs) can robustly derive temporal characteristics of the spatial feature representation. In contrast, spatial characteristics of the representative expression-state frames can be learned with CNNs  [281] . Based on the architecture of ConvNet-RNN networks, many studies have proposed cascaded fusion  [224, 282]  or the ensemble strategy  [238]  to capture both spatial and temporal information for FER.\n\nThe standard pipeline of ConvNet-RNN based FER using the cascaded fusion is to cascade outputs of ConvNets into RNNs to extract temporal dynamics. For example, Kim et al.  [283]  proposed an end-to-end FMER framework by cascading the spatial features extracted by the CNN into the LSTM to encode the temporal characteristics. Xia et al.  [284]  proposed a deep spatiotemporal recurrent convolutional network (STRCN) with a balanced loss that can capture the spatiotemporal deformations of the micro-expression sequence. For dimensional emotion recognition, Kollias and Zafeiriou  [285]  proposed RNN subnets to explore the temporal dynamics of low-, mid-and high-level features extracted from the trained CNNs. Liu et al.  [286]  proposed a framework of dynamic FER based on the siamese action-units attention network (SAANet). Specifically, the SAANet is a pairwise sampling strategy, consisting of CNNs with global-AU attention modules, a BiLSTM module and an attentive pooling module. For 4D FER, Behzad et al.  [224]  utilized CNNs to extract deep features of multi-view and augmented images, and then fed these features into the Bi-LSTM to predict 4D facial expression. On the basis of the work  [224] , sparsity-aware deep learning  [282]  was further proposed to compute the sparse representations of multi-view CNN features.\n\nThe standard pipeline of ConvNet-RNN based FER using the ensemble strategy is to fuse outputs of two streams. For example, Zhang et al.  [238]  (Fig.  4  (f)) proposed a deep evolutional spatial-temporal network, which consists of a part-based hierarchical bidirectional recurrent neural network (PHRNN) and a multisignal convolutional neural network (MSCNN), for analyzing temporal facial expression information and still appearance information, respectively.\n\nAdversarial learning for FER. As GANs can generate synthetic facial expression images under different poses and views, GAN-based models are used for pose/view-invariant FER  [287]  or identityinvariant FER  [288] . For pose/view-invariant FER, Zhang et al.  [239]  proposed the GAN using AE structure to generate more facial images with different expressions under arbitrary poses (Fig.  4 (g) ) and  [287]  further took the shape geometry into consideration. Since the slight semantic perturbations of the inputs often affected prediction accuracy, Fu et al.  [289]  proposed a semantic neighbourhood aware (SNA) network, which formulated the semantic perturbation based on the asymmetric AE with additive noise. For identityinvariant FER, Ali and Hughes  [290]  proposed a novel disentangled expression learning GAN (DE-GAN) by untangling the facial expression representation from identity information. Yu et al.  [291]  investigated a framework of facial micro-expression recognition and synthesis based on the identity-aware and capsuleenhanced GAN (ICE-GAN), which consisted of an AE-based generator for identity-aware expression synthesis, and a capsule-enhanced discriminator (CED) for discriminating the real/fake images and recognizing micro-expressions. 1 Leave-one-subject-out (LOSO); 2 Leave-one-video-out (LOVO); 3 BU-3DFE/ Bosphorus; 4 The average accuracy of BU-3DFE under both the protocols, P1 and P2; 5 Unweighted Average Recall (UAR); 6 Col-Cla=Collaborative Classification.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "5.3.2",
      "text": "Body gesture emotion recognition Most studies of visual emotion recognition focus on FER due to the prominent advantages of distinguishing human emotions. However, FER will be unsuitable when the dedicated sensors fail to capture facial images or just capture low-resolution facial images in some environments. EBGR  [45]  aims to expose one's hidden emotional state from full-body visual information (e.g., body postures) and body skeleton movements or upper-body visual information (e.g., hand gestures, head positioning and eye movements)  [292, 293] . The general pipeline of EBGR includes human detection  [294, 258, 295]  (regarded as pre-processing), feature representation, and emotion recognition. In the view of whether the process of feature extraction and emotion recognition is performed in an end-to-end manner, EBGR systems are categorized into ML-based EBGR and DL-based EBGR.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Ml-Based Ebgr",
      "text": "In existing ML-based EBGR systems, the input is an abstraction of the human body gestures (or their dynamics) through an ensemble body parts or a kinematic model  [296] ; and the output emotion is distinguished through ML-based methods or statistical measures to map the input into the emotional feature space, where the emotional state can be recognized by an ML-based classifier  [297] .\n\nStatistic-based or movement-based EBGR. The ways of feature extraction can be grouped into statistic-based analysis  [298] [299] [300]  and movement-based analysis  [301] . For example, Castellano et al.  [298]  proposed an emotional behavior recognition method based on the analysis of body movement and gesture expressivity. Different statistic qualities of dynamic body gestures are used to infer emotions with designed indicators describing the dynamics of expressive motion cues. Similarly, Saha et al.  [299]  and Maret et al.  [300]  utilized low-level features for EBGR, which were calculated based on the statistical analysis of the 3-D human skeleton generated by a Kinect sensor. Besides, Senecal et al.  [301]  proposed a framework of continuous emotional recognition based on the gestures and full-body dynamical motions using the laban movement analysis-based feature descriptors.\n\nFeature fusion for EBGR. Fusing multiple body posture features can enhance the generalization capability and the robustness of EBGR  [296] . By analyzing postural and dynamic expressive gesture features, Glowinski et al.  [302]  proposed a framework of upper-body based EBGR, using the minimal representation of emotional displays with a reduced amount of visual information related to human upperbody movements. Razzaq et al.  [303]  utilized skeletal joint features from a Kinect v2 sensor, to build mesh distance features and mesh angular features for upper-body emotion representation. Santhoshkumar and Geetha successively proposed two EBGR methods, by using histogram of orientation gradient (HOG) and HOG-KLT features from the sequences  [304] , or by calculating and extracting four kinds of geometric body expressive features  [305] . They achieved the best recognition accuracies of 95.9% and 93.1% on GEMEP, respectively.\n\nClassifier-based EBGR. The common classifiers for ML-based EBGR include decision tree, ensemble decision tree, KNN, SVM, etc. For example, Kapur et al.  [306]  proposed gesture-based affective computing by using five different classifiers to analyze full-body skeletal movements captured by the Vicon system. Different from full-body based EBGR, Saha et al.  [299]  focused on the upper-body based EBGR by employing some comparisons, using five classic ML-based classifiers in terms of the average classification accuracy and computation time. The experimental results showed that the ensemble decision tree achieved the highest recognition rate of 90.83%, under an acceptable execution efficiency. Maret et al.  [300]  also utilized five commonly used classifiers, whereby the genetic algorithm was invoked to search the optimal parameters of the recognition process.\n\nNon-acted EBGR. While the above works show good results on acted data, they fail to address the more difficult non-acted scenario due to the exaggerated displays of emotional body motions. Kleinsmith et al.  [307]  used low-level posture descriptions and feature analysis using non-acted body gestures to implement MLP-based emotion and dimension recognition. Volkova et al.  [308]  further investigated whether emotional body expressions could be recognized when they were recorded during natural scenarios. To explore the emotion recognition from daily actions, Fourati et al.  [309]  recorded varieties of emotional body expressions in daily actions and constructed a new database.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Dl-Based Ebgr",
      "text": "Although DL-based EBGR systems do not require to design of a tailored-feature extractor, they often pre-processed the input data based on the commonly-used pose estimation models or low-level feature extractors  [310] . High-level features can be learned in spatial, temporal or spatial-temporal dimensions through the CNN-based network  [311] , the LSTM-based network  [310]  or the CNN-LSTM based network  [312] . Recently, many studies have demonstrated the advantages of effectively combining different DLbased models and the attention mechanism  [313]  to improve the performance of EBGR.\n\nConvNet-RNN learning for EBGR. Ly et al.  [312]  first utilized the hashing model to detect keyframes of upper-body videos, and then applied a CNN-LSTM network to extract sequence information. The model employed on FABO achieved recognition accuracy of 72.5%. Avola et al.  [314]  investigated a framework of non-acted EBGR based on 3D skeleton and DNNs, which consist of MLP and N-stacked LSTMs. Shen et al.  [310]  proposed full-body based EBGR by fusing RGB features of optical flow extracted by the temporal segment network  [315] , and skeleton features extracted by spatial-temporal graph convolutional networks  [316] .\n\nZero-shot based EBGR. Due to the complexity and diversity of human emotion through body gestures, it is difficult to enumerate all emotional body gestures and collect enough samples for each category  [317] . Therefore, the existing methods fail to determine which emotional state a new body gesture belongs to. In order to recognize unknown emotions from seen body gestures or to know emotions from unseen body gestures, Banerjee et al.  [318]  and Wu et al.  [313]  introduced the generalized zero-shot learning framework, including CNN-based feature extraction, autoencoder-based representation learning, and emotion classifier.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Physiological-Based Emotion Recognition",
      "text": "Facial expressions, text, voice, and body gesture from a human being can be easily collected. As the reliability of physical information largely depends on the social environment and cultural background, and personalities of testers, their emotions are easy to be forged  [20] . However, the changes in physiological signals directly reflect the changes in human emotions, which can help humans recognize, interpret and simulate emotional states  [30, 319] . Therefore, it is highly objective to learn human emotions through physiological signals  [320] . Among the above physiological emotion signals, EEG or ECG can provide simple, objective, and reliable data for identifying emotions  [321] , and is most frequently used for sentiment analysis and emotion recognition. Afterwards, we review EEG-based and ECG-based emotion recognition in this subsection. Table  10  shows an overview of representative methods for physiological-based emotion recognition, as detailed next.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "Compared with other peripheral neuro-physiological signals, EEG can directly measure the changes of brain activities, which provides internal features of emotional states  [20] . Besides, EEG with a high temporal resolution makes it possible to monitor a real-time emotional state. Therefore, various EEG-based emotion recognition techniques  [29, 322]  have been developed recently.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Ml-Based Eeg Emotion Recognition",
      "text": "The performance of ML-based EEG-based emotion recognition  [20]  depends on how to properly design feature extraction, feature dimensionality reduction (or feature selection), and classification methods.\n\nThe core objective of feature extraction is to extract important EEG features, which contain time-domain, frequency-domain, and time-frequency-domain features. The fast Fourier transform (FFT) analysis is often used to transform the EEG into the power spectrum  [323] . Feature dimensionality reduction is an important step in EEG-based emotion recognition due to the redundancy of EEG. Yoon and Chung  [323]  used the FFT analysis for feature extraction and the Pearson correlation coefficient (PCC) for feature selection. Yin et al. successively designed a novel transfer recursive feature elimination  [324]  and the dynamical recursive feature elimination  [325]  for EEG feature selection, which determined sets of robust EEG features. Puk et al.  [326]  investigated an alternating direction method of multipliers ADMM-based sparse group lasso (SGL), with hierarchical splitting for recognizing the discrete states of three emotions. He et al.  [327]  designed a firefly integrated optimization algorithm (FIOA) to realize the optimal selection of the features subset and the classifier, without stagnating in the local optimum for the automatic emotion recognition. The FIOA evaluated on DEAP can gradually regulate the balance between ACC and feature number in the whole optimization process, which achieves an accuracy of 95.00%.\n\nThe commonly used ML-based classifier for EEG-based emotion recognition is SVM or its variations. For example, Atkinson and Campos  [328]  combined the mutual information-based EEG feature selection approach and SVM to improve the recognition accuracy. Yin et al. successively designed the linear least square SVM  [324]  and the selected least square SVM  [325]  for EEG-based emotion recognition.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Dl-Based Eeg Emotion Recognition",
      "text": "Different from the pipeline of ML-based EEG-based emotion recognition, Gao et al.  [329]  utilized CNNs and restricted Boltzmann machine (RBM) with three layers, to simultaneously learn the features and classify EEG-based emotions. The CNN-based emotion recognition with subject-tied protocol achieves an accuracy of 68.4%. The affective computing team directed by Prof. Zheng  [330] [331] [332]  from Southeast University, China, has proposed various EEG-based emotion recognition networks such as bi-hemispheres domain adversarial neural network (BiDANN)  [330] , instance-adaptive graph network  [331]  and variational pathway reasoning  [332] .\n\nAs the biological topology among different brain regions can capture both local and global relations among different EEG channels, Zhong et al.  [333]  designed a regularized graph neural network (RGNN), which consists of both regularization operators of node-wise domain adversarial training and emotionaware distribution learning. Gao et al.  [334]  proposed a channel-fused dense convolutional network for EEG-based emotion recognition. Considering the spatial information from adjacent channels and symmetric channels, Cui et al.  [335]  proposed the regional-asymmetric CNN (RACNN), including temporal, regional and asymmetric feature extractors. An asymmetric differential layer is introduced into three feature extractors to capture the discriminative information, by considering the asymmetry property of emotion responses. The RACNN achieves prominent results with average accuracies of 96.88% and 96.28% on DEAP  [96]  and DREAMER  [321] , respectively.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "5.4.2",
      "text": "ECG-based emotion recognition ECG records the physiological changes of the human heart in different situations through the autonomous nervous system activity. With the change of human emotion or sentiment state, ECG will detect the corresponding waveform transformation  [336] , which can provide enough information in emotion recognition. Next, we introduce ECG-based emotion recognition using ML models or DL models.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Ml-Based Ecg Emotion Recognition",
      "text": "Following the diagram of physiological-based emotion recognition, Hsu et al.  [336]  first constructed a music-induced ECG emotion database, and then developed a nine-stage framework for automatic ECGbased emotion recognition, including 1) Signal preprocessing; 2) R-wave detection; 3) Windowing ECG recording; 4) Noisy epoch rejection; 5) Feature extraction based on the time-, and frequency-domain and nonlinear analysis; 6) Feature normalization; 7) Feature selection using sequential forward floating selection-kernel-based class separability; 8) Feature reduction based on the generalized discriminant analysis, and 9) Classifier construction with LS-SVM. Note that not all ML-based ECG emotion recognition methods follow the abovementioned steps, but the steps of feature extraction, feature selection and classifier are indispensable.\n\nThe ECG features can be directly extracted in the time domain. Bong et al.  [337]  extracted three timedomain features: heart rate, mean R peak amplitude, and mean R-R intervals to detect human emotional stress detection. Another common way is to transform time-domain ECG features into those in other domains. For example, Jerritta et al.  [338]  applied FFT, Discrete Wavelet Transform (DWT), and Hilbert Huang Transform (HHT) to transform ECG signals into frequency-domain features, and then utilized PCA and Tabu search to select key features in low-, high-and total (low and high together) frequency range. Note that both  [338]  and  [337]  used the SVM to implement the final emotion classification.\n\nIt may be beneficial to combine different types of ECG features. Cheng et al.  [339]  computed linearderived features, nonlinear-derived features, time-domain features, and time-frequency domain features from ECG and its derived heart rate variability, and then fused them for SVM-based negative emotion detection. The experiments implemented on BioVid Emo DB  [340]  show that  [339]  achieves an accuracy of 79.51%, with a minor time cost of 0.13ms in the classification of positive and negative emotion states.\n\nStatistic ECG features are also useful. Selvaraj et al.  [341]  proposed a non-linear Hurst feature extraction method by combining the rescaled range statistics and the finite variance scaling with higherorder statistics. They further investigated the performances of four conventional classifiers of NB, RT, KNN and fuzzy KNN for emotion classification. A novel Hurst feature and fuzzy KNN achieved recognition accuracy of 92.87%. Ferdinando et al.  [342]  investigated the effect of feature dimensionality reduction in ECG-based emotion recognition. A bivariate empirical mode decomposition was employed to compute features for KNN based on the statistical distribution of dominant frequencies.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Dl-Based Fer Ecg Emotion Recognition",
      "text": "Chen et al.  [343]  proposed a novel EmotionalGAN-based framework to enhance the generalization ability of emotion recognition by incorporating the augmented ECG samples generated by EmotionalGAN. Compared with that using only original data, around 5% improvement of average accuracy shows the significance of GAN-based models on the emotion recognition task. Sarkar and Etemad  [344]  introduced one self-supervised approach to training the signal transformation recognition network (STRN) to learn spatiotemporal features and abstract representations of the ECG. The weights of convolutional layers in the STRN are frozen and then train two dense layers to classify arousal and valence.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Multimodal Affective Analysis",
      "text": "We have reviewed the relevant studies of unimodal feature extraction and emotion classification, in this section we then describe how to integrate multiple unimodal signals to develop a framework of multimodal affective analysis  [345] , which can be regarded as the fusion of different modalities  [346] , aiming to achieve a more accurate result and more comprehensive understanding than unimodal affect recognition  [347, 348] .\n\nNowadays, most reviews of multimodal affective analysis  [12, 14, 17]  focus on multimodal fusion strategies and classify them into feature-level fusion (or early fusion), decision-level fusion (or late fusion), model-level fusion, and hybrid-level fusion. However, the multimodal affective analysis can be also varied with combinations of different modalities. Therefore, we categorize multimodal affective analysis into multi-physical modality fusion for affective analysis, multi-physiological modality fusion for affective analysis, and physical-physiological modality fusion for affective analysis, and further classify them based on four kinds of fusion strategies. Fig.  6  illustrates prominent examples of using different fusion strategies:\n\n1. Feature-level fusion combines features extracted from the multimodal inputs to form one general feature vector, which is then sent into a classifier. Fig.  6  (a), (b) and (c) show examples based on feature-level fusion for visual-audio modalities, text-audio modalities, and visual-audio-text modalities, respectively. 2. Decision-level fusion connects all decision vectors independently generated from each modality into one feature vector. Fig.  6  (d) shows one example based on decision-level fusion for multiphysiological modalities of EGG, ECG and EDA.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Model-Level Fusion Discovers The Correlation Properties Between Features Extracted From Different",
      "text": "modalities and uses or designs a fusion model with relaxed and smooth types such as HMM and two-stage ELM  [349] . Fig.  6  (e) and (f) are two examples based on model-level fusion for physicalphysiological modalities and visual-audio-text modalities, respectively. 4. Hybrid fusion combines feature-level fusion and decision-level fusion. Fig.  6  (g) shows one example based on hybrid fusion for visual-audio-text modalities.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Multi-Physical Modality Fusion For Affective Analysis",
      "text": "In light of common manners of modality combinations, we categorize multi-physical modalities fusion for affective analysis into visual-audio emotion recognition  [350, 31] , text-audio emotion recognition  [351, 352] , and visual-audio-text emotion recognition  [353, 354] . Table  11  shows an overview of representative methods for multi-physical affective analysis, as detailed next.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "6.1.1",
      "text": "Visual-audio emotion recognition Visual and audio signals are the most natural and affective cues to express emotions when people communicate in daily life  [355] . Many research works  [356] [357] [358] [359] 350]  show that visual-audio emotion recognition outperforms visual or audio emotion recognition. Fig.  6 . Taxonomy of multimodal affective analysis. (a) Feature-level fusion for visual-audio emotion recognition adopted from  [360] ; (b) Feature-level fusion for text-audio emotion recognition adopted from  [361] ; (c) Feature-level fusion for visual-audio-text emotion recognition adopted from  [362] ; (d) Decisionlevel fusion for multi-physiological affective analysis adopted from  [363] ; (e) Model-level fusion for physical-physiological affective analysis adopted from  [35] ; (f) Model-level fusion for visual-audio-text emotion recognition adopted from  [23] ; (g) Hybrid-level fusion for visual-audio-text emotion recognition adopted from  [105] .\n\nFeature-level fusion. Chen et al.  [360]  (Fig.  6  (a)) proposed ML-based visual-audio emotion recognition by fusing dynamic HOG-TOP texture features and acoustic/ geometric features. These two kinds of features are then sent into a multiple kernel SVM for FER both under the wild and lab-controlled environments. Tzirakis et al.  [31]  adopted a CNN and a deep residual network to extract audio and visual features, respectively; and then concatenated these visual-audio features to feed into a 2-layer LSTM to predict Arousal-Valence values.\n\nVarious attention mechanisms have been successfully applied for visual-audio emotion recognition  [364, 365] . For example, Zhang et al.  [365]  introduced an embedded attention mechanism to obtain the emotion-related regions from their respective modalities. To deeply fuse video-audio features, the factorized bilinear pooling (FBP) fusion strategy was proposed in consideration of feature differences in expressions of video frames. The FBP achieves a recognition accuracy of 62.48% on AFEW of the audiovideo sub-challenge in EmotiW2018. Zhao et al  [364]  proposed a novel deep visual-audio attention network (VAANet) with specific attention modules and polarity-consistent cross-entropy loss. Specifically, spatial, channel-wise, and temporal attentions are integrated with a 3D CNN  [366]  for video frame segments, and spatial attention is integrated with a 2D CNN (ResNet-18) for audio MFCC segments.\n\nDecision-level fusion. Hao et al.  [367]  proposed an ensemble visual-audio emotion recognition framework based on multi-task and blending learning with multiple features. Specifically, SVM classifiers and CNNs for handcraft-based and DL-based visual-audio features generated four sub-models, which are then fused to predict the final emotion based on the blending ensemble algorithm. The work  [367]  achieved the average accuracies of 81.36% (speaker-independent) and 78.42% (speaker-dependent) on eNTERFACE  [368] .\n\nModel-level fusion. It requires an ML-based model (e.g., HMM, Kalman filters and DBN) to construct the relationships of different modalities to make decisions. Lin et al.  [33]  proposed a semi-coupled HMM (SC-HMM) to align the temporal relations of audio-visual signals, followed by a Bayesian classifier with an error weighted scheme. The SC-HMM with Bayesian achieves prominent average accuracies of 90.59% (four-class emotions) and 78.13% (four quadrants) on the multimedia human-machine communication posed database and public SEMAINE database, respectively. Glodek et al.  [369]  designed Kalman filters based on a Markov model to combine temporally ordered classifier decisions with the reject option to recognize the affective states. For audio-visual data, the unimodal feature extractors and base classifiers are fused based on a Kalman filter and confidence measures.\n\nZhang et al.  [37]  utilized CNNs to extract audio-visual features and developed a deep fusion method (DBNs) for feature fusion. A linear SVM classifier achieved average accuracies of 80.36%, 54.57% and 85.97% on RML, eNTERFACE05 and BAUM-1s, respectively. Similarly, Nguyen et al.  [278]  proposed a score-level fusion approach to compute all likelihoods of DBNs trained on spatiotemporal information of the audio-video streams. Hossain and Muhammad  [349]  used a 2D CNN and a 3D CNN to extract highlevel representations of the pre-processed audio-video signals. A two-stage ELM based fusion model with an SVM classifier was designed to estimate different emotional states.",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "6.1.2",
      "text": "Text-audio emotion recognition Although SER  [26, 163, 178]  and TSA  [156, 370]  have achieved significant progress, performing the two tasks separately makes it hard to achieve compelling results  [361] . Text-audio emotion recognition approaches use linguistic content and speech clues to enhance the performance of the unimodal emotion recognition system  [352, 371] .\n\nFeature-level fusion. Yoon et al.  [372]  proposed a deep dual recurrent neural network for encoding audio-text sequences and then concatenated their outputs to predict the final emotion. It achieves an accuracy of 71.8% (four classes) on IEMOCAP. Afterwards, Cai et al.  [373]  designed an improved CNN and Bi-LSTM to extract spatial features and capture their temporal dynamics. Considering the phonemes effect on emotion recognition, Zhang et al.  [371]  utilized a temporal CNN to investigate the acoustic and lexical properties of phonetic information based on unimodal, multimodal single-stage fusion, and multimodal multi-stage fusion systems. To deeply exploit and fuse text-acoustic features for emotion classification, Priyasad et al.  [361]  (Fig.  6 (b )) designed T-DNN (DCNNs and Bi-RNN followed by other DCNNs) and A-DCNN (SincNet with band-pass filters followed by a DCNN) to extract textual features and learning acoustic features, respectively. Textual-acoustic features are fused with different attention strategies (self-attention or no attention) to predict four emotions on IEMOCAP.\n\nDecision-level fusion. Wu et al.  [374]  fused acoustic-prosodic information based on a meta decision tree (MDT) with multiple base classifiers (e.g. GMM, SVM, and MLP), and employed a maximum entropy model (MaxEnt) to establish the relationship between emotional states and emotion association rules in semantic labels for speech emotion recognition and text emotion recognition, respectively. Using the weighted product fusion strategy, AP-based and SL-based emotion confidences are fused to predict final emotions.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Feature-Level Fusion Versus Decision-Level Fusion.",
      "text": "To verify which feature-level fusion or decisionlevel fusion is more effective in text-audio emotion recognition, Jin et al.  [375]  firstly generated new lexical features and different acoustic features and then utilized two fusion strategies to achieve four-class recognition accuracies of 55.4% and 69.2% on IEMOCAP, respectively. Considering the emotional dialogue composed of sound and spoken content, Pepino et al.  [376]  exploited multiple dual RNNs to encode audio-text sequences. The feature-level fusion and decision-level fusion approaches in 3 different ways are explored to compare their performances.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "6.1.3",
      "text": "Visual-audio-text emotion recognition The vocal modulation, facial expression, and context-based text provide important cues to better identify the true affective states of the opinion holder  [377, 378] . For example, when a lady is proposed and then she says \"I do\" in tears, none of textual-based, audio-based, or visual-based emotion recognition models can predict confident results. While visual-audio-text emotion recognition leads to a better solution.\n\nFeature-level fusion. Veronica et al.  [379]  used BoW, OpenEAR  [380] , and vision software to extract linguistic, audio and visual features, respectively. Compared with three unimodal and three bimodal models, the multimodal (text-audio-visual) model can significantly improve the model performance over the individual use of one modality or fusion of any two modalities. In addition, Poria et al.  [354]  utilized a standard RNN, a deep CNN, and openSMILE to capture temporal dependence of visual data, spatial information of textual data and low-level descriptors of audio data, respectively. The MKL is further designed for feature selection of different modalities to improve the recognition results. Mittal et al.  [362]  (Fig.  6 (c )) proposed the multiplicative multimodal emotion recognition (M3ER): firstly, feature vectors are extracted from the raw three modalities; then, these features are transferred into modality check step to retain the effective features and discard the ineffectual ones which are used to regenerate proxy feature vectors; finally, selected features are fused to predict six emotions based on the multiplicative feature-level fusion combined with attention module. The M3ER achieved better recognition accuracies of 82.7% and 89.0% on IEMOCAP and CMU-MOSEI, respectively. Different from language-independent approaches for English or German sentiment analysis, Chinese sentiment analysis not only understands symbols with explicit meaning, but also captures phonemic orthography (tonal language) with implicit meaning. Based on this assumption, Peng et al.  [381]  proposed reinforcement learning based disambiguate intonation for sentiment analysis (DISA) which consists of policy network, embedding lookup, loss computation, and feature-level fusion of three modalities. By integrating phonetic features with textual and visual representations, multimodal Chinese sentiment analysis significantly outperforms than unimodal models.\n\nFeature-level fusion versus decision-level fusion. Poria et al.  [382]  utilized a CNN to extract textual features, calculated handcrafted features from visual data, and generated audio features by the openSMILE. These three kinds of feature vectors were fused and then used to train a classifier based on the MKL. Feature-level fusion and decision-level fusion with feature selection are conducted to implement unimodal, bimodal and multimodal emotion recognition employed on HOW  [104] . Experimental results show that textual emotion recognition achieves the best performance among three unimodal emotion recognition methods, and in feature-level fusion, visual-audio-text emotion recognition outperforms other unimodal and bimodal emotion recognition. Besides, the accuracy of feature-level fusion is significantly higher than that of decision-level fusion at the expense of computational speed.\n\nModel-level fusion. Considering the interdependencies and relations among the utterances of a video  [378] , Poria et al.  [383]  introduced some variants of the contextual LSTM into the hierarchical architecture to extract context-dependent multimodal utterance features. The visual-audio-text features are concatenated and then fed into a contextual LSTM to predict emotions, reaching 80.3%, 68.1% and 76.1% on MOSI, MOUD and IEMOCAP, respectively. Akhtar et al.  [23]  (Fig.  6 (f) ) proposed an end-to-end DL-based multitask learning for multimodal emotion recognition and sentiment analysis. Due to the unequal importance of three modalities, a context-level inter-modal (CIM) attention module is designed to learn the joint association between textual-audio-visual features of utterances captured by three bi-directional gated recurrent unit (biGRU) networks. These features of three CIMs and three individual modalities are concatenated to generate high-level feature representation to predict sentiments and multi-label emotions.\n\nHybrid-level fusion. To take full advantage of feature-based fusion and decision-based fusion, and overcome the disadvantages of both, Wöllmer et al.  [105]  (Fig.  6  (g)) used BoW or Bag-of-N-Gram (BoNG) features with SVM for linguistic sentiment classification, and fused audio-video features with Bi-LSTM for audio-visual emotion recognition. And then these two prediction results were fused to obtain the final emotion under a strategy of weighted fusion.  4 The recognition accuracy considering the individual personality trait for personalized application; 5 The median values evaluated on IEMOCAP/MSP-PODCAST databases; 6 Mean WA of Arousal, Expectation, Power and Valence.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Multi-Physiological Modality Fusion For Affective Analysis",
      "text": "With the enhancement and refinement of wearable technologies, automatic affective analysis based on multi-physiological modalities has attracted more attention  [384] . However, due to the complexity of emotion and significant individual differences in physiological responses  [348] , it is difficult to achieve satisfactory prediction performance with EEG-based or ECG-based emotion recognition. In this sub-section, we review multi-physiological modality fusion for affective analysis. Table  12  provides an overview of representative methods for multi-physiological affective analysis, as detailed next.\n\nFeature-level fusion. Li et al.  [385]  collected multi-physiological emotion signals induced via music, picture or video, and extracted low-level descriptors and statistical features from the signal waveforms. Then, they fused these features for emotion prediction using ML-based classifiers with a Group-based IRS (Individual Response Specificity) model. The combination of RF with Group-based IRS achieved a higher accuracy of about 90% in the imbalance of the emotion database. Similarly, Nakisa et al.  [386]  preprocessed, extracted and fused time-frequency features of EEG and BVP, and then fed them into an LSTM network, whose hyperparameters were optimized by differential evolution (DE). On their self-collected dataset, its overall accuracy was 77.68% for the four-quadrant dimensional emotions.\n\nUsing ECG, GSR, ST, BVP, RESP, EMG, and EOG selected from DEAP  [96] , Verma and Tiwary  [347]  extracted multi-resolution features based on DWT and used them to estimate the valence-arousaldominance emotions. Hossain et al.  [387]  extracted 9 statistical features, 9 power spectral density features, and 46 DBN features of EDA, Photo plethysmogram, and zEMG. These features were fused to train a fine gaussian SVM to recognize 5 basic emotions. Ma et al.  [388]  designed a multimodal residual LSTM network for learning the dependency of high-level temporal-feature EEG, EOG, and EMG to predict emotions. It achieves the classification accuracies of 92.87% and 92.30% for arousal and valence, respectively.\n\nDecision-level fusion. Wei et al.  [389]  selected EEG, ECG, RESP and GSR from MAHNOB-HCI  [107] , and designed a linear fusing weight matrix to fuse the outputs from multiple SVM classifiers to predict 5 basic emotions. Its highest average accuracy of recognition was 84.6%. To explore the emotional physiological-based temporal features, Li et al.  [390]  transformed EEG, ECG and GSR selected from AMIGOS  [99]  into spectrogram images to represent their time-frequency information. The attention-based Bi-LSTM-RNNs was designed to automatically learn the best temporal features, which were further fed into a DNN to predict the probability of unimodal emotion. The final emotion state was computed based on either an equal weights scheme or a variable weights scheme. As personality-specific human beings often show different physiological reactions after being stimulated by emotional elements, Yang and Lee  [363]  (Fig.  6 (d) ) proposed an attribute-invariance loss embedded variational autoencoder to learn personality-invariant representations. With EEG, ECG and EDA selected from AMIGOS, different features were extracted and then fed into different SVM classifiers to predict unimodal classification results. Dar et al.  [391]  designed a 2D-CNN for EEG and combined LSTM and 1D-CNN for ECG and GSR. By using majority voting based on decisions made by multiple classifiers, the framework achieved the overall highest accuracy of 99.0% and 90.8% for AMIGOS  [99]  and DREAMER  [321] , respectively.\n\nModel-level fusion. Yin et al.  [392]  first pre-processed and extracted 425 salient physiological features of 7 signals selected from DEAP. Separate deep hidden neurons in stacked-AEs were then investigated to extract higher-level abstractions of these salient features. An ensemble of deep classifiers with an adjacent graph-based hierarchical feature fusion network was designed for recognizing emotions based on a Bayesian model.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Physical-Physiological Modality Fusion For Affective Analysis",
      "text": "Since the change of human emotions is a complicated psycho-physiological activity, the research on affective analysis is related to many cues (e.g., behavioral, physical, and psychological signals)  [393, 394] . Researchers have focused on the physical-physiological modality fusion for affective analysis via mining the strong external expression of physical data and undisguisable internal changes of physiological signals  [395] [396] [397] . Table  13  provides an overview of representative methods for physical-physiological affective analysis, as detailed next.\n\nFeature-level fusion. To fully leverage the advantages of the complementary property between EEG and eye movement, Liu et al.  [398]  proposed a bimodal deep AE based on an RBM to recognize emotions on SEED and DEAP. Soleymani et al.  [399]  designed a framework of video-EEG based emotion detection using an LSTM-RNN and continuous conditional random fields. Wu et al.  [400]  proposed a hierarchical LSTM with a self-attention mechanism to fuse the facial features and EEG features to calculate the final emotion. Yin et al.  [397]  proposed an efficient end-to-end framework of EDA-music fused emotion recognition, denominating it as a 1-D residual temporal and channel attention network (RTCAN-1D). Specially, the RTCAN consists of shallow feature extraction, residual feature extraction, the attention module stacked by a signal channel attention module, and a residual non-local temporal attention module. It achieved outstanding performances in AMIGOS, DEAP and PMEmo  [401] .\n\nFeature-level fusion versus decision-level fusion. Huang et al.  [34]  proposed video-EEG based multimodal affective analysis by fusing external facial expression of spatiotemporal local monogenic binary pattern and discriminative spectral power of internal EEG on feature-level and decision-level aspects. According to their experiments, the multimodal affective analysis achieves a better performance than the unimodal emotion recognition; and when it comes to fusion strategies, decision-level fusion outperforms feature-level fusion in the recognition of valence and arousal on MAHNOB-HCI.\n\nModel-level fusion. Wang et al.  [35]  (Fig.  6  (e)) designed a multimodal deep belief network for fusing and optimizing multiple psycho-physiological features, a bimodal DBN (BDBN) for representing discriminative video-based features, and another BDBN to extract high multimodal features of both video and psycho-physiological modalities. The SVM was employed for emotion recognition after the features of all modalities were integrated into a unified dimension.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Discussions",
      "text": "In this review, we have involved emotion models and databases commonly used for affective computing, as well as unimodal affect recognition and multimodal affective analysis. In this section, we mainly discuss the following aspects:\n\n1) Effects of different signals (textual, audio, visual, or physiological) on unimodal affect recognition  [152, 194, 244, 289, 327, 331] ; 2) Effects of modality combinations and fusion strategies on multimodal affective analysis  [37, 371, 375, 383, 388, 400] ; 3) Effects of ML-based techniques  [127, 184, 249, 305, 326]  or DL-based methods  [146, 203, 273, 283, 316, 335]  on affective computing; 4) Effects of some potential factors (e.g., released databases and performance metrics) on affective computing; 5) Applications of affective computing in real-life scenarios.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Effects Of Different Signals On Unimodal Affect Recognition",
      "text": "According to unimodal affect recognition based on the text  [126, 402] , audio  [178, 194, 199] , visual  [253, 281, 403] , EEG  [404] , or ECG  [336] , we can find that the most widely used modality is the visual signal, mainly consisting of facial expressions and body gestures. The number of visual-based emotion recognition systems is comparable to the sum of that of systems based on other modalities since the visual signals are easier to capture than other signals and emotional information in visual signals is more helpful than other signals in recognizing the emotion state of human beings. Visual-based emotion recognition is more effective than audio-based emotion recognition because audio signals are susceptible to noise  [405] . However, a study  [23]  reveals that textual-based affective analysis achieves the highest accuracy in emotion recognition and sentiment analysis. Although the physiological signals collected by wearable sensors are more difficult to obtain than physical signals, numerous EEG-based  [334]  or ECG-based  [336]  emotion recognition methods have been investigated and proposed due to their objective and reliable outcomes.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Effects Of Modality Combinations And Fusion Strategies On Multimodal Affective Analysis",
      "text": "The combination of different modalities and the fusion strategy are two key aspects of the multimodal affective analysis. Multimodal combinations are divided into multi-physical modalities, multiphysiological modalities, and physical-physiological modalities. The fusion strategies consist of featurelevel fusion  [361, 387] , decision-level fusion  [375] , hybrid-level fusion  [105] , and model-level fusion.\n\nIn the multi-physical modalities  [406] , there are three kinds of combinations of different modalities, consisting of visual-audio, text-audio and visual-audio-text. Integrating visual and audio information can enhance performance over unimodal affect recognition  [407] . There are similar results in other combinations of multi-physical modalities  [375] , in which the text modality plays the most vital role in multimodal sentiment analysis  [382, 362] . In studies of multi-physiological modality fusion for affective analysis, in addition to EEG and ECG, other types of physiological signals (e.g., ECG, EOG, BVP, GSR, and EMG) are jointly combined to interpret emotional states  [388, 390] . The visual modality (facial expression, voice, gesture, posture, etc.) may also be integrated with multimodal physiological signals for visual-physiological affective analysis  [397, 400] .\n\nTwo basic fusion strategies for multimodal affective analysis are feature-level fusion  [361, 387]  and decision-level fusion  [375] . The concatenation  [362]  or factorized bilinear pooling  [365]  of feature vectors is commonly used for feature-level fusion. The majority/average voting is often used for decision-level fusion. Linear weighted computing  [374]  can be utilized for both feature-level and decision-level fusion, by employing sum or product operators to fuse features or classification decisions of different modalities. According to the multimodal affective analysis, we find that feature-level fusion  [388]  is strikingly more common than decision-level fusion. The performance of an affect classifier based on feature-level fusion is significantly influenced by the time scales and metric levels of features coming from different modalities. On the other hand, in decision-level fusion, the input coming from each modality is modelled independently, and these results of unimodal affect recognition are combined in the end. Compared with feature-level fusion, decision-level fusion  [362]  is performed easier, but ignores the relevance among features of different modalities. Hybrid-level fusion  [105]  aims to make full use of the advantages of feature-based fusion and decision-based fusion strategies as well as overcome the disadvantages of either one. Unlike the above three fusion strategies, model-level fusion uses HMM  [33]  or Bayesian networks  [392]  to establish the correlation between features of different modalities and one relaxed fusion mode. The selection and establishment of HMM or Bayesian have a fatal effect on the results of the model-level fusion, which is often designed for one specific task.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Effects Of Ml-Based And Dl-Based Models On Affective Computing",
      "text": "The majority of the early works on affective computing have employed ML-based techniques  [19, 10, 18 ]. The ML-based pipeline  [124, 181, 249, 309, 326]  consists of pre-processing of raw signals, hand-crafted feature extractor (feature selection if possible), and well-designed classifiers. Although various types of hand-crafted features have been designed for different modalities, ML-based techniques for affective analysis are hard to be reused across similar problems on account of their task-specific and domain-specific feature descriptors. The commonly used ML-based classifiers are SVM  [182, 253] , HMM  [179] , GMM  [180] , RF  [305] , KNN  [337]  and ANN  [374] , of which the SVM classifier is the most effective one, and is indeed used in most tasks of ML-based affective computing. These ML-based classifiers are also used for final classification when the DL-based model is only designed for unimodal feature extraction  [290]  or multimodal feature analysis  [35, 387] .\n\nNowadays, DL-based models have become hot spots and outperformed ML-based models in most areas of affective computing  [13, 135, 43, 17, 39]  due to their strong ability of feature representation learning. For static information (e.g., facial and spectrogram images), CNNs and their variants are designed to extract important and discriminative features  [137, 194, 289] . For sequence information (e.g., physiological signals and videos), RNNs and their variants are designed for capturing temporal dynamics  [143, 201, 310] . The CNN-LSTM models can perform the deep spatial-temporal feature extraction. Adversarial learning is widely used to improve the robustness of models by augmenting data  [206, 291]  and cross-domain learning  [156, 192] . Besides, different attention mechanisms  [142, 199, 203, 279]  and autoencoders  [363, 289]  are integrated with DL-based techniques to improve the overall performance. It seems that DL-based methods have an advantage in automatically learning the most discriminative features. However, DL-based approaches have not yet had a huge impact on physiological emotion recognition, if compared with MLbased models  [20] .",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "Effects Of Some Potential Factors On Affective Computing",
      "text": "Throughout this review, we have consistently found that advances in affective computing are driven by various database benchmarks. For example, there are few video-physiological emotion recognition methods due to the limitations of physical-physiological emotion databases. In contrast, the rapid development of FER is inseparable from various baseline databases, which are publicly available and can be freely downloaded. Besides, some large-scale visual databases such as BU-4DFE and BP4D can be employed to pre-train the target model to recognize facial expressions  [260]  or micro-expressions  [408] . However, there are significant discrepancies in size, quality, and collection conditions  [274]  across different databases. For example, most body gesture emotion databases contain only several hundred samples with limited gesture categories. What is worse, samples are typically collected in a laboratory environment, which is often far away from real-world conditions. Furthermore, the size and quality of databases have a more obvious effect on DL-based emotion recognition than on ML-based emotion recognition. Many studies have concluded that the reduced size of the available databases is a key factor in the quest for high-performance affective analysis  [409, 207] . To tackle this problem, the pre-trained DL-based models  [410, 411]  may be transferred into task-based models specialized for affective analysis.\n\nAlthough the representation of the natural affective states has no consensus, most affective analyses are trained and evaluated based on two types of emotion models: discrete models and dimensional models. In building the affective databases, either discrete or dimensional labels are typically chosen alternatively to fit the raw signals. For example, emotional images or sequences are typically matched with a discrete affective state (basic emotions or polarity). Affective recognition can be divided into classification (emotions, dimensional or polarity) and continuous dimensional regression (Pleasure, Arousal, Dominance Expectation, or Intensity)  [412, 413] . The metrics of accuracy, precision and recall are generally adopted for categorical or componential emotion classification. When the databases are imbalanced, the F-Measure (or F1-Score) seems to be the best choice out of the existing evaluation metrics of the emotional classification, across 10-fold cross validation and LOSO. The weighted average recall/F1-score (WAR/WF1) and the unweighted average recall/F1-score (UAR/UF1) are best suited for the classification performance of visual, audio or multimodal affective analysis  [291] . On the other hand, MSE and RMSE are commonly used for the evaluation of continuous dimensional emotion prediction  [414] . In order to describe the degree of coincidence, by integrating PCC and MSR, the coefficient concordance correlation coefficient is advised to assess the baseline performance assessment  [415, 416] .",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "Applications Of Affective Computing In Real-Life Scenarios",
      "text": "In recent years, more and more research teams have shifted their focus to applications of affective computing in real-life scenarios  [417, 418] . In order to detect emotions and sentiments from the textual information, the SenticNet directed by Erik Cambria of NTU applied the research outputs of affective computing  [106, 150, 419]  and sentiment analysis  [22, [420] [421] [422] [423] [424]  into many aspects of daily life, including HCI  [425] , finance  [426]  and social media monitoring and forecasting  [132, 427] . TSA is often used for recommender systems, by integrating diverse feedback information  [428]  or microblog texts  [429] . The applications of visual emotion recognition include course teaching  [430] , smarter decision aid  [431] , HCI  [432] , dynamic quality adaption to the players in games  [433] [434] [435] , depression recognition  [436]  and for helping medical rehabilitation children affected by the autism spectrum condition  [437] . In particular, audio and physiological signals are often used for detecting clinical depression and stress  [100, 166, 438]  due to the reliability and stability of audio/speech emotion signals and the accessibility of physiological signals from wearable devices  [439] . As multimodal affective analysis can enhance the robustness and performance of unimodal affect recognition, more researches have begun to transform them into various real-life applications  [440, 441] , making it a promising research avenue.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Conclusion And New Developments",
      "text": "This review has comprehensively surveyed more than 400 papers, including an overview of the recent reviews on affective computing in Section 2, and built the taxonomy of affective computing with representative examples in Section 1. In Section 3, we categorize current emotion models based on psychological theories into discrete models and dimensional models, which determine the category of the output of the affective analysis. These recognition results via either classification or regression are evaluated by a range of corresponding metrics. More importantly, the development of affective computing requires benchmark databases for training and computational models for either DL-based or ML-based affective understanding. In Section 4, we survey five kinds of the commonly adopted baseline databases for affective computing, which are classified into textual, audio, visual, physiological, and multimodal databases. Most methods for affective analysis benefit from these released databases.\n\nIn Section 5 and Section 6, we introduced recent advances of affective computing, which are mainly grouped into unimodal affect recognition and multimodal affective analysis, and further divide them into ML-based techniques and DL-based models. The unimodal affect recognition systems are further divided into textual sentiment analysis, speech emotion recognition, visual emotion recognition (FER and EBGR) and physiological emotion recognition (EEG-based and ECG-based). Traditional ML-based unimodal affect recognition mostly investigates hand-crafted feature extractors or pre-defined rules and interpretable classifiers. In contrast, DL-based unimodal affect recognition further improves the ability of the feature representation and classification by designing deeper network architectures or task-specific network modules and learning objectives. Generally, in addition to employing different strategies (feature-level, decision-level, model-level, or hybrid fusion strategies), the multimodal affective analysis is divided into multi-physical approaches (visual-audio, text-audio and visual-audio-text modalities), multi-physiological approaches, and physical-physiological approaches. The performance of multimodal affective analysis is mainly affected by both modality combination and fusion strategy.\n\nIn Section 7, we discuss some important issues related to affective computing including effects of textual, audio, visual, or physiological signals on unimodal affect recognition, effects of modality combinations and fusion strategies on multimodal affective analysis, the effects of ML-based and DL-based models on affective computing, effects of some potential factors on affective computing, and applications of affective computing in the real-life scenarios.\n\nAlthough affective computing systems using either unimodal or multimodal data have made significant breakthroughs, there are only a few robust and effective algorithms to predict emotion and recognize sentiment under diverse and challenging scenes. Hence, we would like to conclude this review with many important recommendations for future research in affective computing:\n\n1) It will be instrumental to develop new and more extended baseline databases, particularly multimodal affect databases, consisting of different modalities (textual, audio, visual and physiological). Conditions should include both spontaneous and non-spontaneous scenarios, with the provision of annotating data in both discrete and dimensional emotion models. 2) There are some challenging tasks of affective analysis to be solved including FER under partial occlusion or fake emotion expression, physiological emotion recognition based on various complex signals, and a baseline model specifically for both discrete emotion recognition and dimensional emotion prediction. 3) There is significant space for improving fusion strategies, particularly with rule-based or statistic-based knowledge, to implement a mutual fusion of different modalities that can consider the role and importance of each modality in affect recognition.\n\n4) Zero/few-shot learning or unsupervised learning methods (e.g., self-supervised learning) need to be further explored, particularly thanks to their potential to enhance the robustness and stability of affective analysis under limited or biased databases. 5) A prominent application of affective analysis is robotics. Advances presented in this review make it possible to conceive robots equipped with emotional intelligence, which can appropriately imitate and promptly respond to mankind affect and the surrounding environment.",
      "page_start": 33,
      "page_end": 34
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Taxonomy of affective computing with representative examples.",
      "page": 3
    },
    {
      "caption": "Figure 1: , which will help the reader navigate",
      "page": 4
    },
    {
      "caption": "Figure 2: (b), respectively.",
      "page": 6
    },
    {
      "caption": "Figure 2: Two discrete emotion models for affective computing. (a) 6 basic emotion models [15] shown in",
      "page": 7
    },
    {
      "caption": "Figure 3: Dimensional emotion models. (a) Pleasure-Arousal-Dominance (PAD) model reproduced based",
      "page": 7
    },
    {
      "caption": "Figure 3: (b). The circumplex model consists of four quadrants. The first quadrant,",
      "page": 8
    },
    {
      "caption": "Figure 4: ) via the point of whether the features",
      "page": 17
    },
    {
      "caption": "Figure 4: (a)) utilized 52 facial landmark points (FLP),",
      "page": 17
    },
    {
      "caption": "Figure 4: Taxonomy of representative FER methods based on ML techniques or DL models. (a) Geometry-",
      "page": 18
    },
    {
      "caption": "Figure 4: (b)) proposed a framework of low-resolution FER based on image",
      "page": 19
    },
    {
      "caption": "Figure 4: (c)) used the MKL fusion",
      "page": 19
    },
    {
      "caption": "Figure 4: (d)) was proposed to generate multiple types of gradually",
      "page": 19
    },
    {
      "caption": "Figure 4: (e)) proposed a novel knowledge transfer technique, which comprised",
      "page": 19
    },
    {
      "caption": "Figure 4: (f)) proposed a deep evolutional spatial-temporal network,",
      "page": 20
    },
    {
      "caption": "Figure 4: (g)) and [287] further",
      "page": 21
    },
    {
      "caption": "Figure 5: The diagram of emotion recognition via physiological signals.",
      "page": 23
    },
    {
      "caption": "Figure 5: , typically includes the",
      "page": 23
    },
    {
      "caption": "Figure 6: illustrates prominent examples of using different fusion strategies:",
      "page": 25
    },
    {
      "caption": "Figure 6: (a), (b) and (c) show examples based on",
      "page": 25
    },
    {
      "caption": "Figure 6: (d) shows one example based on decision-level fusion for multi-",
      "page": 25
    },
    {
      "caption": "Figure 6: (e) and (f) are two examples based on model-level fusion for physical-",
      "page": 25
    },
    {
      "caption": "Figure 6: (g) shows one example",
      "page": 25
    },
    {
      "caption": "Figure 6: Taxonomy of multimodal affective analysis. (a) Feature-level fusion for visual-audio emotion",
      "page": 26
    },
    {
      "caption": "Figure 6: (a)) proposed ML-based visual-audio emotion recognition",
      "page": 27
    },
    {
      "caption": "Figure 6: (b)) designed T-DNN (DCNNs and Bi-RNN followed by other",
      "page": 27
    },
    {
      "caption": "Figure 6: (c)) proposed the multiplicative multimodal emotion recognition (M3ER): firstly, feature vectors",
      "page": 28
    },
    {
      "caption": "Figure 6: (f)) proposed an end-to-end DL-based multi-",
      "page": 28
    },
    {
      "caption": "Figure 6: (g)) used BoW or Bag-of-N-Gram (BoNG)",
      "page": 28
    },
    {
      "caption": "Figure 6: (d)) proposed an attribute-invariance loss embedded variational autoencoder to learn",
      "page": 30
    },
    {
      "caption": "Figure 6: (e)) designed a multimodal deep belief network for fusing",
      "page": 30
    }
  ],
  "tables": [
    {
      "caption": "Table 1: lists the main acronyms used",
      "data": [
        {
          "Acronym": "SER",
          "Full Form": "Speech Emotion Recognition"
        },
        {
          "Acronym": "FMER",
          "Full Form": "Facial Micro-Expression Recognition"
        },
        {
          "Acronym": "EBGR",
          "Full Form": "Emotional Body Gesture Recognition"
        },
        {
          "Acronym": "ECG",
          "Full Form": "Electrocardiography"
        },
        {
          "Acronym": "EDA",
          "Full Form": "Electro-Dermal Activity"
        },
        {
          "Acronym": "DL",
          "Full Form": "Deep Learning"
        },
        {
          "Acronym": "MLP",
          "Full Form": "Multi-Layer Perceptron"
        },
        {
          "Acronym": "LSTM",
          "Full Form": "Long- Short-Term Memory"
        },
        {
          "Acronym": "DCNN",
          "Full Form": "Deep Convolutional Neural Network"
        },
        {
          "Acronym": "CNN",
          "Full Form": "Convolutional Neural Network"
        },
        {
          "Acronym": "RNN",
          "Full Form": "Recurrent Neural Network"
        },
        {
          "Acronym": "GRU",
          "Full Form": "Gated Recurrent Unit"
        },
        {
          "Acronym": "AE",
          "Full Form": "Auto-encoder"
        },
        {
          "Acronym": "GAN",
          "Full Form": "Generative Adversarial Network"
        },
        {
          "Acronym": "VGG",
          "Full Form": "Visual Geometry Group"
        },
        {
          "Acronym": "DBN",
          "Full Form": "Deep Belief Network"
        },
        {
          "Acronym": "HAN",
          "Full Form": "Hierarchical Attention Network"
        },
        {
          "Acronym": "ResNet",
          "Full Form": "Residual Networks"
        },
        {
          "Acronym": "GAP",
          "Full Form": "Global Average Pooling"
        },
        {
          "Acronym": "AUs",
          "Full Form": "Action Units"
        },
        {
          "Acronym": "AAM",
          "Full Form": "Active Appearance Model"
        },
        {
          "Acronym": "LFPC",
          "Full Form": "Logarithmic Frequency Power Coefficient"
        },
        {
          "Acronym": "ROIs",
          "Full Form": "Regions of Interest"
        },
        {
          "Acronym": "MFCC",
          "Full Form": "MEL Frequency Cepstrum Coefficient"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Defining Affect in Relation to Cognition: A Response to Susan McLeod",
      "authors": [
        "K Fleckenstein"
      ],
      "year": "1991",
      "venue": "J. Adv. Compos"
    },
    {
      "citation_id": "2",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Toward machine emotional intelligence: analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/34.954607"
    },
    {
      "citation_id": "4",
      "title": "Feature vector classification based speech emotion recognition for service robots",
      "authors": [
        "J Park",
        "J Kim",
        "Y Oh"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Consum. Electron",
      "doi": "10.1109/TCE.2009.5278031"
    },
    {
      "citation_id": "5",
      "title": "The Affect Dilemma for Artificial Agents: Should We Develop Affective Artificial Agents?",
      "authors": [
        "M Scheutz"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2012.29"
    },
    {
      "citation_id": "6",
      "title": "A Survey of Autonomous Human Affect Detection Methods for Social Robots Engaged in Natural HRI",
      "authors": [
        "D Mccoll",
        "A Hong",
        "N Hatakeyama",
        "G Nejat",
        "B Benhabib"
      ],
      "year": "2016",
      "venue": "J. Intell. Robot. Syst",
      "doi": "10.1007/s10846-015-0259-2"
    },
    {
      "citation_id": "7",
      "title": "Detecting stress during real-world driving tasks using physiological sensors",
      "authors": [
        "J Healey",
        "R Picard"
      ],
      "year": "2005",
      "venue": "IEEE Trans. Intell. Transp. Syst",
      "doi": "10.1109/TITS.2005.848368"
    },
    {
      "citation_id": "8",
      "title": "Generalized adaptive view-based appearance model: Integrated framework for monocular head pose estimation",
      "authors": [
        "L.-P Morency",
        "J Whitehill",
        "J Movellan"
      ],
      "year": "2008",
      "venue": "2008 8th IEEE Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/AFGR.2008.4813429"
    },
    {
      "citation_id": "9",
      "title": "Velá squez, Opinion Mining and Information Fusion: A survey",
      "authors": [
        "J Balazs"
      ],
      "year": "2016",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2015.06.002"
    },
    {
      "citation_id": "10",
      "title": "Affective Computing and Sentiment Analysis",
      "authors": [
        "E Cambria"
      ],
      "year": "2016",
      "venue": "IEEE Intell. Syst",
      "doi": "10.1109/MIS.2016.31"
    },
    {
      "citation_id": "11",
      "title": "Are They Different? Affect, Feeling, Emotion, Sentiment, and Opinion Detection in Text",
      "authors": [
        "M Munezero",
        "C Montero",
        "E Sutinen",
        "J Pajunen"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2014.2317187"
    },
    {
      "citation_id": "12",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2017.02.003"
    },
    {
      "citation_id": "13",
      "title": "Deep Learning for Human Affect Recognition: Insights and New Developments",
      "authors": [
        "P Rouast",
        "M Adam",
        "R Chiong"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2018.2890471"
    },
    {
      "citation_id": "14",
      "title": "Multimodal big data affective analytics: A comprehensive survey using text, audio, visual and physiological signals",
      "authors": [
        "N Shoumy",
        "L.-M Ang",
        "K Seng",
        "D Rahaman",
        "T Zia"
      ],
      "year": "2020",
      "venue": "J. Netw. Comput. Appl",
      "doi": "10.1016/j.jnca.2019.102447"
    },
    {
      "citation_id": "15",
      "title": "Basic emotions",
      "authors": [
        "E Paul"
      ],
      "year": "1999",
      "venue": "Basic emotions"
    },
    {
      "citation_id": "16",
      "title": "Basic dimensions for a general psychological theory : implications for personality, social, environmental, and developmental studies",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1980",
      "venue": "Basic dimensions for a general psychological theory : implications for personality, social, environmental, and developmental studies"
    },
    {
      "citation_id": "17",
      "title": "A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition",
      "authors": [
        "Y Jiang",
        "W Li",
        "M Hossain",
        "M Chen",
        "A Alelaiwi",
        "M Al-Hammadi"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2019.06.019"
    },
    {
      "citation_id": "18",
      "title": "Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications",
      "authors": [
        "C Corneanu",
        "M Simón",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2016.2515606"
    },
    {
      "citation_id": "19",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2010.09.020"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.01.011"
    },
    {
      "citation_id": "21",
      "title": "Aspect extraction for opinion mining with a deep convolutional neural network",
      "authors": [
        "S Poria",
        "E Cambria",
        "A Gelbukh"
      ],
      "year": "2016",
      "venue": "Knowl.-Based Syst",
      "doi": "10.1016/j.knosys.2016.06.009"
    },
    {
      "citation_id": "22",
      "title": "SenticNet: A Publicly Available Semantic Resource for Opinion Mining, in: AAAI2010",
      "authors": [
        "E Cambria",
        "R Speer",
        "C Havasi",
        "A Hussain"
      ],
      "year": "2010",
      "venue": "SenticNet: A Publicly Available Semantic Resource for Opinion Mining, in: AAAI2010"
    },
    {
      "citation_id": "23",
      "title": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis",
      "authors": [
        "M Akhtar",
        "D Chauhan",
        "D Ghosal",
        "S Poria",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "venue": "Proc. 2019 Conf. North Am. Chapter Assoc"
    },
    {
      "citation_id": "25",
      "title": "Communicating Without Words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1968",
      "venue": "Psychol. Today"
    },
    {
      "citation_id": "26",
      "title": "Emotions from text: machine learning for text-based emotion prediction",
      "authors": [
        "C Alm",
        "D Roth",
        "R Sproat"
      ],
      "year": "2005",
      "venue": "Proc. Conf. Hum. Lang. Technol. Empir. Methods Nat. Lang. Process. -HLT 05",
      "doi": "10.3115/1220575.1220648"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "Y Mei",
        "J.-W Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2018.05.005"
    },
    {
      "citation_id": "28",
      "title": "Raspberry Pi assisted facial expression recognition framework for smart security in law-enforcement services",
      "authors": [
        "M Sajjad",
        "M Nasir",
        "F Ullah",
        "K Muhammad",
        "A Sangaiah",
        "S Baik"
      ],
      "year": "2019",
      "venue": "Inf. Sci",
      "doi": "10.1016/j.ins.2018.07.027"
    },
    {
      "citation_id": "29",
      "title": "Self-supervised ECG Representation Learning for Emotion Recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.3014842"
    },
    {
      "citation_id": "30",
      "title": "Emotions Recognition Using EEG Signals: A Survey",
      "authors": [
        "S Alarcã",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2017.2714671"
    },
    {
      "citation_id": "31",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "J Kim",
        "E Andre"
      ],
      "year": "2008",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2008.26"
    },
    {
      "citation_id": "32",
      "title": "End-to-End Multimodal Emotion Recognition Using Deep Neural Networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE J. Sel. Top. Signal Process",
      "doi": "10.1109/JSTSP.2017.2764438"
    },
    {
      "citation_id": "33",
      "title": "3D Constrained Local Model for rigid and non-rigid facial tracking",
      "authors": [
        "T Baltrusaitis",
        "P Robinson",
        "L Morency"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2012.6247980"
    },
    {
      "citation_id": "34",
      "title": "Error Weighted Semi-Coupled Hidden Markov Model for Audio-Visual Emotion Recognition",
      "authors": [
        "J.-C Lin",
        "C.-H Wu",
        "W.-L Wei"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2011.2171334"
    },
    {
      "citation_id": "35",
      "title": "Pietikä inen, Multi-modal emotion analysis from facial expressions and electroencephalogram",
      "authors": [
        "X Huang",
        "J Kortelainen",
        "G Zhao",
        "X Li",
        "A Moilanen",
        "T Seppä Nen"
      ],
      "year": "2016",
      "venue": "Comput. Vis. Image Underst",
      "doi": "10.1016/j.cviu.2015.09.015"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition using multimodal deep learning in multiple psychophysiological signals and video",
      "authors": [
        "Z Wang",
        "X Zhou",
        "W Wang",
        "C Liang"
      ],
      "year": "2020",
      "venue": "Int. J. Mach. Learn. Cybern",
      "doi": "10.1007/s13042-019-01056-8"
    },
    {
      "citation_id": "37",
      "title": "A survey on machine learning for data fusion",
      "authors": [
        "T Meng",
        "X Jing",
        "Z Yan",
        "W Pedrycz"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2019.12.001"
    },
    {
      "citation_id": "38",
      "title": "Learning Affective Features With a Hybrid Deep Model for Audio-Visual Emotion Recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Circuits Syst. Video Technol",
      "doi": "10.1109/TCSVT.2017.2719043"
    },
    {
      "citation_id": "39",
      "title": "A Brief Review of Facial Emotion Recognition Based on Visual Information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors",
      "doi": "10.3390/s18020401"
    },
    {
      "citation_id": "40",
      "title": "Deep Facial Expression Recognition: A Survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.2981446"
    },
    {
      "citation_id": "41",
      "title": "A Review on Facial Micro-Expressions Analysis: Datasets, Features and Metrics",
      "authors": [
        "W Merghani",
        "A Davison",
        "M Yap"
      ],
      "year": "2018",
      "venue": "A Review on Facial Micro-Expressions Analysis: Datasets, Features and Metrics"
    },
    {
      "citation_id": "42",
      "title": "Systematic review of 3D facial expression recognition methods",
      "authors": [
        "G Alexandre",
        "J Soares",
        "G Pereira Thé"
      ],
      "year": "2020",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2019.107108"
    },
    {
      "citation_id": "43",
      "title": "A Survey of Sentiment Analysis Based on Transfer Learning",
      "authors": [
        "R Liu",
        "Y Shi",
        "C Ji",
        "M Jia"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2925059"
    },
    {
      "citation_id": "44",
      "title": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Using Deep Learning Techniques: A Review",
      "doi": "10.1109/ACCESS.2019.2936124"
    },
    {
      "citation_id": "45",
      "title": "Facial Sentiment Analysis Using AI Techniques: Stateof-the-Art, Taxonomies, and Challenges",
      "authors": [
        "K Patel",
        "D Mehta",
        "C Mistry",
        "R Gupta",
        "S Tanwar",
        "N Kumar",
        "M Alazab"
      ],
      "year": "2020",
      "venue": "Facial Sentiment Analysis Using AI Techniques: Stateof-the-Art, Taxonomies, and Challenges",
      "doi": "10.1109/ACCESS.2020.2993803"
    },
    {
      "citation_id": "46",
      "title": "Survey on Emotional Body Gesture Recognition",
      "authors": [
        "F Noroozi",
        "D Kaminska",
        "C Corneanu",
        "T Sapinski",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2018.2874986"
    },
    {
      "citation_id": "47",
      "title": "Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances",
      "doi": "10.1109/ACCESS.2019.2929050"
    },
    {
      "citation_id": "48",
      "title": "A survey of sentiment analysis in social media",
      "authors": [
        "L Yue",
        "W Chen",
        "X Li",
        "W Zuo",
        "M Yin"
      ],
      "year": "2019",
      "venue": "Knowl. Inf. Syst",
      "doi": "10.1007/s10115-018-1236-4"
    },
    {
      "citation_id": "49",
      "title": "A review of emotion sensing: categorization models and algorithms",
      "authors": [
        "Z Wang",
        "S.-B Ho",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Multimed. Tools Appl",
      "doi": "10.1007/s11042-019-08328-z"
    },
    {
      "citation_id": "50",
      "title": "Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives",
      "authors": [
        "J Han",
        "Z Zhang",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives",
      "doi": "10.1109/MCI.2019.2901088"
    },
    {
      "citation_id": "51",
      "title": "Current Challenges, and Future Possibilities on Emotion Recognition Using Machine Learning and Physiological Signals",
      "authors": [
        "P Bota",
        "C Wang",
        "A Fred",
        "H Placido Da",
        "A Silva",
        "Review"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2944001"
    },
    {
      "citation_id": "52",
      "title": "A Review on Nonlinear Methods Using Electroencephalographic Recordings for Emotion Recognition",
      "authors": [
        "B Garcia-Martinez",
        "A Martinez-Rodrigo",
        "R Alcaraz",
        "A Fernandez-Caballero"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2018.2890636"
    },
    {
      "citation_id": "53",
      "title": "Universals and cultural differences in facial expressions of emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1971",
      "venue": "Nebr. Symp. Motiv"
    },
    {
      "citation_id": "54",
      "title": "Four Models of Basic Emotions: A Review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt",
      "authors": [
        "J Tracy",
        "D Randles"
      ],
      "year": "2011",
      "venue": "Emot. Rev",
      "doi": "10.1177/1754073911410747"
    },
    {
      "citation_id": "55",
      "title": "A Circumplex Model of Affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "J. Pers. Soc. Psychol",
      "doi": "10.1037/h0077714"
    },
    {
      "citation_id": "56",
      "title": "Emotion and Life: perspective from psychology biology and evolution",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "2003",
      "venue": "Am. Physiol. Assoc"
    },
    {
      "citation_id": "57",
      "title": "The Hourglass of Emotions",
      "authors": [
        "E Cambria",
        "A Livingstone",
        "A Hussain"
      ],
      "year": "2012",
      "venue": "Cogn. Behav. Syst",
      "doi": "10.1007/978-3-642-34584-5_11"
    },
    {
      "citation_id": "58",
      "title": "The Hourglass Model Revisited",
      "authors": [
        "Y Susanto",
        "A Livingstone",
        "B Ng",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "IEEE Intell. Syst",
      "doi": "10.1109/MIS.2020.2992799"
    },
    {
      "citation_id": "59",
      "title": "Facial expression recognition with Convolutional Neural Networks: Coping with few data and the training sample order",
      "authors": [
        "A Lopes",
        "E De Aguiar",
        "A De Souza",
        "T Oliveira-Santos"
      ],
      "year": "2017",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2016.07.026"
    },
    {
      "citation_id": "60",
      "title": "Generating and Protecting Against Adversarial Attacks for Deep Speech-Based Emotion Recognition Models",
      "authors": [
        "Z Ren",
        "A Baird",
        "J Han",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP40776.2020.9054087"
    },
    {
      "citation_id": "61",
      "title": "Multi-Level Fine-Scaled Sentiment Sensing with Ambivalence Handling",
      "authors": [
        "Z Wang",
        "S.-B Ho",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Int. J. Uncertain. Fuzziness Knowl.-Based Syst",
      "doi": "10.1142/S0218488520500294"
    },
    {
      "citation_id": "62",
      "title": "Pleasure, Arousal, Dominance: Mehrabian and Russell revisited",
      "authors": [
        "I Bakker",
        "T Van Der Voordt",
        "P Vink",
        "J De Boon"
      ],
      "year": "2014",
      "venue": "Pleasure, Arousal, Dominance: Mehrabian and Russell revisited",
      "doi": "10.1007/s12144-014-9219-4"
    },
    {
      "citation_id": "63",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "J Russell",
        "A Mehrabian"
      ],
      "year": "1977",
      "venue": "J. Res. Personal",
      "doi": "10.1016/0092-6566(77)90037-X"
    },
    {
      "citation_id": "64",
      "title": "Emotion Classification Using EEG Signals",
      "authors": [
        "H Dabas",
        "C Sethi",
        "C Dua",
        "M Dalawat",
        "D Sethia"
      ],
      "year": "2018",
      "venue": "Emotion Classification Using EEG Signals",
      "doi": "10.1145/3297156.3297177"
    },
    {
      "citation_id": "65",
      "title": "Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",
      "authors": [
        "J Blitzer",
        "M Dredze",
        "F Pereira",
        "Biographies",
        "Bollywood"
      ],
      "year": "2007",
      "venue": "ACL"
    },
    {
      "citation_id": "66",
      "title": "Confidence-weighted linear classification",
      "authors": [
        "M Dredze",
        "K Crammer",
        "F Pereira"
      ],
      "year": "2008",
      "venue": "Proc. 25th Int. Conf. Mach. Learn. -ICML 08",
      "doi": "10.1145/1390156.1390190"
    },
    {
      "citation_id": "67",
      "title": "Learning Word Vectors for Sentiment Analysis",
      "authors": [
        "A Maas",
        "R Daly",
        "P Pham",
        "D Huang",
        "A Ng",
        "C Potts"
      ],
      "year": "2011",
      "venue": "Proc. 49th Annu"
    },
    {
      "citation_id": "68",
      "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
      "authors": [
        "R Socher",
        "A Perelygin",
        "J Wu",
        "J Chuang",
        "C Manning",
        "A Ng",
        "C Potts"
      ],
      "venue": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank"
    },
    {
      "citation_id": "69",
      "title": "A Database of German Emotional Speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A Database of German Emotional Speech"
    },
    {
      "citation_id": "70",
      "title": "The Belfast Induced Natural Emotion Database",
      "authors": [
        "I Sneddon",
        "M Mcrorie",
        "G Mckeown",
        "J Hanratty"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.26"
    },
    {
      "citation_id": "71",
      "title": "Coding facial expressions with Gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Proc. Third IEEE Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/AFGR.1998.670949"
    },
    {
      "citation_id": "72",
      "title": "The Extended Cohn-Kanade Dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. -Workshop",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "73",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade",
        "J Cohn",
        "Yingli Tian"
      ],
      "year": "2000",
      "venue": "Proc. Fourth IEEE Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/AFGR.2000.840611"
    },
    {
      "citation_id": "74",
      "title": "Induced Disgust, Happiness and Surprise: an Addition to the MMI Facial Expression Database",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Induced Disgust, Happiness and Surprise: an Addition to the MMI Facial Expression Database"
    },
    {
      "citation_id": "75",
      "title": "Pietikä inen, Facial expression recognition from near-infrared videos",
      "authors": [
        "G Zhao",
        "X Huang",
        "M Taini",
        "S Li"
      ],
      "year": "2011",
      "venue": "Image Vis. Comput",
      "doi": "10.1016/j.imavis.2011.07.002"
    },
    {
      "citation_id": "76",
      "title": "A 3D facial expression database for facial behavior research",
      "authors": [
        "Lijun Yin",
        "Xiaozhou Wei",
        "Yi Sun",
        "Jun Wang",
        "M Rosato"
      ],
      "year": "2006",
      "venue": "th Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/FGR.2006.6"
    },
    {
      "citation_id": "77",
      "title": "A high-resolution 3D dynamic facial expression database",
      "authors": [
        "L Yin",
        "X Chen",
        "Y Sun",
        "T Worm",
        "M Reale"
      ],
      "year": "2008",
      "venue": "2008 8th IEEE Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/AFGR.2008.4813324"
    },
    {
      "citation_id": "78",
      "title": "D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image Vis. Comput",
      "doi": "10.1016/j.imavis.2014.06.002"
    },
    {
      "citation_id": "79",
      "title": "DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications",
      "authors": [
        "S Cheng",
        "I Kotsia",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "2018 IEEECVF Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2018.00537"
    },
    {
      "citation_id": "80",
      "title": "A Spontaneous Micro-expression Database: Inducement, collection and baseline",
      "authors": [
        "X Li",
        "T Pfister",
        "X Huang",
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE Int. Conf. Workshop Autom. Face Gesture Recognit. FG, IEEE",
      "doi": "10.1109/FG.2013.6553717"
    },
    {
      "citation_id": "81",
      "title": "An Improved Spontaneous Micro-Expression Database and the Baseline Evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu",
        "Casme Ii"
      ],
      "year": "2014",
      "venue": "PLoS ONE",
      "doi": "10.1371/journal.pone.0086041"
    },
    {
      "citation_id": "82",
      "title": "SAMM: A Spontaneous Micro-Facial Movement Dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2016.2573832"
    },
    {
      "citation_id": "83",
      "title": "Challenges in Representation Learning: A Report on Three Machine Learning Contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "Z Chuang",
        "Y Bengio"
      ],
      "year": "2013",
      "venue": "Challenges in Representation Learning: A Report on Three Machine Learning Contests",
      "doi": "10.1007/978-3-642-42051-1_16"
    },
    {
      "citation_id": "84",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE Int. Conf. Comput. Vis. Workshop ICCV Workshop",
      "doi": "10.1109/ICCVW.2011.6130508"
    },
    {
      "citation_id": "85",
      "title": "EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR, IEEE",
      "doi": "10.1109/CVPR.2016.600"
    },
    {
      "citation_id": "86",
      "title": "From Facial Expression Recognition to Interpersonal Relation Prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "Int. J. Comput. Vis",
      "doi": "10.1007/s11263-017-1055-1"
    },
    {
      "citation_id": "87",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2017.2740923"
    },
    {
      "citation_id": "88",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR, IEEE",
      "doi": "10.1109/CVPR.2017.277"
    },
    {
      "citation_id": "89",
      "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proc. 28th ACM Int. Conf. Multimed., ACM, Seattle WA USA",
      "doi": "10.1145/3394171.3413620"
    },
    {
      "citation_id": "90",
      "title": "EmoTV1: Annotation of real-life emotions for the specifications of multimodal a ective interfaces",
      "authors": [
        "S Abrilian",
        "L Devillers",
        "S Buisine",
        "J.-C Martin"
      ],
      "year": "2005",
      "venue": "EmoTV1: Annotation of real-life emotions for the specifications of multimodal a ective interfaces"
    },
    {
      "citation_id": "91",
      "title": "A Bimodal Face and Body Gesture Database for Automatic Analysis of Human Nonverbal Affective Behavior",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2006",
      "venue": "18th Int. Conf. Pattern Recognit. ICPR06",
      "doi": "10.1109/ICPR.2006.39"
    },
    {
      "citation_id": "92",
      "title": "Gesture and Emotion: Can basic gestural form features discriminate emotions?",
      "authors": [
        "M Kipp",
        "J.-C Martin"
      ],
      "year": "2009",
      "venue": "Gesture and Emotion: Can basic gestural form features discriminate emotions?",
      "doi": "10.1109/ACII.2009.5349544"
    },
    {
      "citation_id": "93",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in Temperament",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1996",
      "venue": "Curr. Psychol",
      "doi": "10.1007/BF02686918"
    },
    {
      "citation_id": "94",
      "title": "Introducing the Geneva Multimodal Emotion Portrayal (GEMEP) corpus",
      "authors": [
        "T Bä Nziger",
        "K Scherer"
      ],
      "year": "2010",
      "venue": "Bluepr. Affect. Comput. Sourceb"
    },
    {
      "citation_id": "95",
      "title": "Meta-Analysis of the First Facial Expression Recognition Challenge",
      "authors": [
        "M Valstar",
        "M Mehu",
        "B Jiang",
        "M Pantic",
        "K Scherer"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern",
      "doi": "10.1109/TSMCB.2012.2200675"
    },
    {
      "citation_id": "96",
      "title": "Emotional body expression in daily actions database",
      "authors": [
        "N Fourati",
        "C Pelachaud",
        "Emilya"
      ],
      "year": "2014",
      "venue": "Emotional body expression in daily actions database"
    },
    {
      "citation_id": "97",
      "title": "DEAP: A Database for Emotion Analysis ;Using Physiological Signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "98",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th Int. IEEEEMBS Conf. Neural Eng. NER",
      "doi": "10.1109/NER.2013.6695876"
    },
    {
      "citation_id": "99",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Auton. Ment. Dev",
      "doi": "10.1109/TAMD.2015.2431497"
    },
    {
      "citation_id": "100",
      "title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
      "authors": [
        "J Miranda Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2018.2884461"
    },
    {
      "citation_id": "101",
      "title": "Introducing WESAD, a Multimodal Dataset for Wearable Stress and Affect Detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proc. 20th ACM Int. Conf. Multimodal Interact",
      "doi": "10.1145/3242969.3242985"
    },
    {
      "citation_id": "102",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Eval",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "103",
      "title": "The USC CreativeIT Database: A Multimodal Database of Theatrical Improvisation",
      "authors": [
        "A Metallinou",
        "C.-C Lee",
        "C Busso",
        "S Carnicke",
        "S Narayanan"
      ],
      "venue": "The USC CreativeIT Database: A Multimodal Database of Theatrical Improvisation"
    },
    {
      "citation_id": "104",
      "title": "The USC CreativeIT database of multimodal dyadic interactions: from speech and full body motion capture to continuous emotional annotations",
      "authors": [
        "A Metallinou",
        "Z Yang",
        "C Lee",
        "C Busso",
        "S Carnicke",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Lang. Resour. Eval",
      "doi": "10.1007/s10579-015-9300-0"
    },
    {
      "citation_id": "105",
      "title": "Towards multimodal sentiment analysis: harvesting opinions from the web",
      "authors": [
        "L.-P Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proc. 13th Int. Conf. Multimodal Interfaces",
      "doi": "10.1145/2070481.2070509"
    },
    {
      "citation_id": "106",
      "title": "YouTube Movie Reviews: Sentiment Analysis in an Audio-Visual Context",
      "authors": [
        "M Wollmer",
        "F Weninger",
        "T Knaup",
        "B Schuller",
        "C Sun",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intell. Syst",
      "doi": "10.1109/MIS.2013.34"
    },
    {
      "citation_id": "107",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proc. 56th Annu",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "108",
      "title": "A Multimodal Database for Affect Recognition and Implicit Tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.25"
    },
    {
      "citation_id": "109",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE Int. Conf. Workshop Autom",
      "doi": "10.1109/FG.2013.6553805"
    },
    {
      "citation_id": "110",
      "title": "DECAF: MEG-Based Multimodal Database for Decoding Affective Physiological Responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2015.2392932"
    },
    {
      "citation_id": "111",
      "title": "Chapter 1 -Challenges of Sentiment Analysis in Social Networks: An Overview",
      "authors": [
        "F Pozzi",
        "E Fersini",
        "E Messina",
        "B Liu"
      ],
      "year": "2017",
      "venue": "Sentim. Anal. Soc. Netw",
      "doi": "10.1016/B978-0-12-804412-4.00001-2"
    },
    {
      "citation_id": "112",
      "title": "A computer approach to content analysis: studies using the General Inquirer system",
      "authors": [
        "P Stone",
        "E Hunt"
      ],
      "year": "1963",
      "venue": "Proc"
    },
    {
      "citation_id": "113",
      "title": "",
      "authors": [
        "Spring Jt",
        "Comput"
      ],
      "year": "1963",
      "venue": "",
      "doi": "10.1145/1461551.1461583"
    },
    {
      "citation_id": "114",
      "title": "Thumbs up?: sentiment classification using machine learning techniques",
      "authors": [
        "B Pang",
        "L Lee",
        "S Vaithyanathan"
      ],
      "year": "2002",
      "venue": "Proc. ACL-02 Conf. Empir. Methods Nat. Lang. Process. -EMNLP 02",
      "doi": "10.3115/1118693.1118704"
    },
    {
      "citation_id": "115",
      "title": "Statistical Learning Theory and ELM for Big Social Data Analysis",
      "authors": [
        "L Oneto",
        "F Bisio",
        "E Cambria",
        "D Anguita"
      ],
      "year": "2016",
      "venue": "IEEE Comput. Intell. Mag",
      "doi": "10.1109/MCI.2016.2572540"
    },
    {
      "citation_id": "116",
      "title": "Lexicon-Based Methods for Sentiment Analysis",
      "authors": [
        "M Taboada",
        "J Brooke",
        "M Tofiloski",
        "K Voll",
        "M Stede"
      ],
      "year": "2011",
      "venue": "Comput. Linguist",
      "doi": "10.1162/COLI_a_00049"
    },
    {
      "citation_id": "117",
      "title": "A holistic lexicon-based approach to opinion mining",
      "authors": [
        "X Ding",
        "B Liu",
        "P Yu"
      ],
      "year": "2008",
      "venue": "Proc. Int. Conf. Web Search Web Data Min. -WSDM 08",
      "doi": "10.1145/1341531.1341561"
    },
    {
      "citation_id": "118",
      "title": "Sentiment analysis of blogs by combining lexical knowledge with text classification",
      "authors": [
        "P Melville",
        "W Gryc",
        "R Lawrence"
      ],
      "year": "2009",
      "venue": "Proc. 15th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. -KDD 09",
      "doi": "10.1145/1557019.1557156"
    },
    {
      "citation_id": "119",
      "title": "Sentic patterns: Dependency-based rules for concept-level sentiment analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "G Winterstein",
        "G.-B Huang"
      ],
      "year": "2014",
      "venue": "Knowl.-Based Syst",
      "doi": "10.1016/j.knosys.2014.05.005"
    },
    {
      "citation_id": "120",
      "title": "Common Sense Computing: From the Society of Mind to Digital Intuition and beyond",
      "authors": [
        "E Cambria",
        "A Hussain",
        "C Havasi",
        "C Eckl"
      ],
      "year": "2009",
      "venue": "Biom. ID Manag. Multimodal Commun",
      "doi": "10.1007/978-3-642-04391-8_33"
    },
    {
      "citation_id": "121",
      "title": "The effect of negation on sentiment analysis and retrieval effectiveness",
      "authors": [
        "L Jia",
        "C Yu",
        "W Meng"
      ],
      "year": "2009",
      "venue": "Proceeding 18th ACM Conf. Inf. Knowl. Manag. -CIKM 09",
      "doi": "10.1145/1645953.1646241"
    },
    {
      "citation_id": "122",
      "title": "Sentiment Analysis for Ad Hoc Discussions Using Multilingual Knowledge-Based Approach",
      "authors": [
        "I Blekanov",
        "M Kukarkin",
        "A Maksimov",
        "S Bodrunova"
      ],
      "year": "2018",
      "venue": "Proc. 3rd Int. Conf. Appl. Inf. Technol. -ICAIT2018",
      "doi": "10.1145/3274856.3274880"
    },
    {
      "citation_id": "123",
      "title": "A comprehensive survey for sentiment analysis tasks using machine learning techniques",
      "authors": [
        "M Ebru Aydogan",
        "Akcayol"
      ],
      "year": "2016",
      "venue": "Int. Symp. Innov. Intell. Syst. Appl",
      "doi": "10.1109/INISTA.2016.7571856"
    },
    {
      "citation_id": "124",
      "title": "Machine Learning Techniques for Sentiment Analysis: A Review",
      "authors": [
        "M Ahmad",
        "S Aftab",
        "S Muhammad",
        "S Ahmad"
      ],
      "year": "2017",
      "venue": "Int. J. Multidiscip. Sci. Eng"
    },
    {
      "citation_id": "125",
      "title": "Sentiment Analysis using Support Vector Machines with Diverse Information Sources",
      "authors": [
        "T Mullen",
        "N Collier"
      ],
      "year": "2004",
      "venue": "Proc. 2004 Conf. Empir"
    },
    {
      "citation_id": "126",
      "title": "Text Representation Using Dependency Tree Subgraphs for Sentiment Analysis",
      "authors": [
        "A Pak",
        "P Paroubek"
      ],
      "year": "2011",
      "venue": "Database Syst. Adanced Appl.",
      "doi": "10.1007/978-3-642-20244-5_31"
    },
    {
      "citation_id": "127",
      "title": "Feature selection for text classification with Naï ve Bayes",
      "authors": [
        "J Chen",
        "H Huang",
        "S Tian",
        "Y Qu"
      ],
      "year": "2009",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2008.06.054"
    },
    {
      "citation_id": "128",
      "title": "Sentiment Analysis on Product Reviews Using Machine Learning Techniques",
      "authors": [
        "R Jagdale",
        "V Shirsat",
        "S Deshmukh"
      ],
      "year": "2019",
      "venue": "Sentiment Analysis on Product Reviews Using Machine Learning Techniques",
      "doi": "10.1007/978-981-13-0617-4_61"
    },
    {
      "citation_id": "129",
      "title": "Word Polarity Disambiguation Using Bayesian Model and Opinion-Level Features",
      "authors": [
        "Y Xia",
        "E Cambria",
        "A Hussain",
        "H Zhao"
      ],
      "year": "2015",
      "venue": "Word Polarity Disambiguation Using Bayesian Model and Opinion-Level Features",
      "doi": "10.1007/s12559-014-9298-4"
    },
    {
      "citation_id": "130",
      "title": "Consensus vote models for detecting and filtering neutrality in sentiment analysis",
      "authors": [
        "A Valdivia",
        "M Luzón",
        "E Cambria",
        "F Herrera"
      ],
      "year": "2018",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2018.03.007"
    },
    {
      "citation_id": "131",
      "title": "Method for Text-Based Sentiment Analysis",
      "authors": [
        "T Le",
        "Hybrid"
      ],
      "year": "2019",
      "venue": "Comput. Sci. Comput. Intell. CSCI",
      "doi": "10.1109/CSCI49370.2019.00260"
    },
    {
      "citation_id": "132",
      "title": "A Novel Machine Learning-based Sentiment Analysis Method for Chinese Social Media Considering Chinese Slang Lexicon and Emoticons",
      "authors": [
        "D Li",
        "R Rzepka",
        "M Ptaszynski",
        "K Araki"
      ],
      "year": "2019",
      "venue": "A Novel Machine Learning-based Sentiment Analysis Method for Chinese Social Media Considering Chinese Slang Lexicon and Emoticons"
    },
    {
      "citation_id": "133",
      "title": "Recurrent neural network based language model",
      "authors": [
        "T Mikolov",
        "M Karafiá",
        "L Burget",
        "J Cernocký",
        "S Khudanpur"
      ],
      "year": "2010",
      "venue": "Recurrent neural network based language model"
    },
    {
      "citation_id": "134",
      "title": "Predicting political sentiments of voters from Twitter in multi-party contexts",
      "authors": [
        "A Khatua",
        "A Khatua",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/j.asoc.2020.106743"
    },
    {
      "citation_id": "135",
      "title": "HieNN-DWE: A hierarchical neural network with dynamic word embeddings for document level sentiment classification",
      "authors": [
        "F Liu",
        "L Zheng",
        "J Zheng"
      ],
      "year": "2020",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.04.084"
    },
    {
      "citation_id": "136",
      "title": "Convolutional Neural Networks for Sentence Classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Convolutional Neural Networks for Sentence Classification",
      "doi": "10.3115/v1/D14-1181"
    },
    {
      "citation_id": "137",
      "title": "Deep Learning for Aspect-Based Sentiment Analysis: A Comparative Review",
      "authors": [
        "H Do",
        "P Prasad",
        "A Maag",
        "A Alsadoon"
      ],
      "year": "2019",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2018.10.003"
    },
    {
      "citation_id": "138",
      "title": "Sentiment Lexical-Augmented Convolutional Neural Networks for Sentiment Analysis",
      "authors": [
        "R Yin",
        "P Li",
        "B Wang"
      ],
      "year": "2017",
      "venue": "2017 IEEE Second Int. Conf. Data Sci",
      "doi": "10.1109/DSC.2017.82"
    },
    {
      "citation_id": "139",
      "title": "Very Deep Convolutional Networks for Text Classification",
      "authors": [
        "A Conneau",
        "H Schwenk",
        "L Barrault",
        "Y Lecun"
      ],
      "year": "2017",
      "venue": "Proc. 15th Conf"
    },
    {
      "citation_id": "140",
      "title": "Deep Pyramid Convolutional Neural Networks for Text Categorization",
      "authors": [
        "R Johnson",
        "T Zhang"
      ],
      "year": "2017",
      "venue": "Proc. 55th Annu",
      "doi": "10.18653/v1/P17-1052"
    },
    {
      "citation_id": "141",
      "title": "Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification",
      "authors": [
        "B Huang",
        "K Carley"
      ],
      "year": "2018",
      "venue": "Proc. 2018 Conf. Empir",
      "doi": "10.18653/v1/D18-1136"
    },
    {
      "citation_id": "142",
      "title": "Contextual Bidirectional Long Short-Term Memory Recurrent Neural Network Language Models: A Generative Approach to Sentiment Analysis",
      "authors": [
        "A Mousa",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. 15th Conf"
    },
    {
      "citation_id": "143",
      "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis",
      "authors": [
        "W Wang",
        "S Pan",
        "D Dahlmeier",
        "X Xiao"
      ],
      "year": "2016",
      "venue": "Proc. 2016 Conf. Empir. Methods Nat. Lang. Process",
      "doi": "10.18653/v1/D16-1059"
    },
    {
      "citation_id": "144",
      "title": "Recurrent Attention Network on Memory for Aspect Sentiment Analysis",
      "authors": [
        "P Chen",
        "Z Sun",
        "L Bing",
        "W Yang"
      ],
      "year": "2017",
      "venue": "Proc. 2017 Conf. Empir. Methods Nat. Lang. Process",
      "doi": "10.18653/v1/D17-1047"
    },
    {
      "citation_id": "145",
      "title": "Cognition-Cognizant Sentiment Analysis with Multitask Subjectivity Summarization based on Annotators' Gaze Behavior",
      "authors": [
        "A Mishra",
        "S Tamilselvam",
        "R Dasgupta",
        "S Nagar",
        "K Dey"
      ],
      "year": "2018",
      "venue": "Cognition-Cognizant Sentiment Analysis with Multitask Subjectivity Summarization based on Annotators' Gaze Behavior"
    },
    {
      "citation_id": "146",
      "title": "Neural Sentiment Classification with User and Product Attention",
      "authors": [
        "H Chen",
        "M Sun",
        "C Tu",
        "Y Lin",
        "Z Liu"
      ],
      "year": "2016",
      "venue": "Neural Sentiment Classification with User and Product Attention",
      "doi": "10.18653/v1/D16-1171"
    },
    {
      "citation_id": "147",
      "title": "Capturing User and Product Information for Document Level Sentiment Analysis with Deep Memory Network",
      "authors": [
        "Z.-Y Dou"
      ],
      "year": "2017",
      "venue": "Proc. 2017 Conf. Empir. Methods Nat. Lang. Process",
      "doi": "10.18653/v1/D17-1054"
    },
    {
      "citation_id": "148",
      "title": "Improving Review Representations with User Attention and Product Attention for Sentiment Classification",
      "authors": [
        "Z Wu",
        "X.-Y Dai",
        "C Yin",
        "S Huang",
        "J Chen"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conf. Artif. Intell. AAAI-18"
    },
    {
      "citation_id": "149",
      "title": "A Convolutional Stacked Bidirectional LSTM with a Multiplicative Attention Mechanism for Aspect Category and Sentiment Detection",
      "authors": [
        "T Trueman",
        "E Cambria"
      ],
      "year": "2021",
      "venue": "Cogn. Comput",
      "doi": "10.1007/s12559-021-09948-0"
    },
    {
      "citation_id": "150",
      "title": "Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks",
      "authors": [
        "B Liang",
        "H Su",
        "L Gui",
        "E Cambria",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Knowl.-Based Syst",
      "doi": "10.1016/j.knosys.2021.107643"
    },
    {
      "citation_id": "151",
      "title": "Transformation Networks for Target-Oriented Sentiment Classification",
      "authors": [
        "X Li",
        "L Bing",
        "W Lam",
        "B Shi"
      ],
      "year": "2018",
      "venue": "Proc. 56th Annu",
      "doi": "10.18653/v1/P18-1087"
    },
    {
      "citation_id": "152",
      "title": "User reviews: Sentiment analysis using lexicon integrated two-channel CNN-LSTM family models",
      "authors": [
        "W Li",
        "L Zhu",
        "Y Shi",
        "K Guo",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/j.asoc.2020.106435"
    },
    {
      "citation_id": "153",
      "title": "Aspect Based Sentiment Analysis with Gated Convolutional Networks",
      "authors": [
        "W Xue",
        "T Li"
      ],
      "year": "2018",
      "venue": "Proc. 56th Annu",
      "doi": "10.18653/v1/P18-1234"
    },
    {
      "citation_id": "154",
      "title": "ABCDM: An Attention-based Bidirectional CNN-RNN Deep Model for sentiment analysis",
      "authors": [
        "M Basiri",
        "S Nemati",
        "M Abdar",
        "E Cambria",
        "U Acharya"
      ],
      "year": "2021",
      "venue": "Future Gener. Comput. Syst",
      "doi": "10.1016/j.future.2020.08.005"
    },
    {
      "citation_id": "155",
      "title": "How Intense Are You? Predicting Intensities of Emotions and Sentiments using Stacked Ensemble",
      "authors": [
        "M Akhtar",
        "A Ekbal",
        "E Cambria"
      ],
      "year": "2020",
      "venue": "IEEE Comput. Intell. Mag",
      "doi": "10.1109/MCI.2019.2954667"
    },
    {
      "citation_id": "156",
      "title": "Int. Conf. Learn. Represent",
      "authors": [
        "T Miyato",
        "A Dai",
        "I Goodfellow"
      ],
      "year": "2017",
      "venue": "Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "157",
      "title": "Domain-Adversarial Training of Neural Networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M March",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "158",
      "title": "End-to-End Adversarial Memory Network for Cross-domain Sentiment Classification",
      "authors": [
        "Q Yang",
        "Z Li",
        "Y Zhang",
        "Y Wei",
        "Y Wu"
      ],
      "year": "2017",
      "venue": "IJCAI"
    },
    {
      "citation_id": "159",
      "title": "A Generative Model for category text generation",
      "authors": [
        "Y Li",
        "Q Pan",
        "S Wang",
        "T Yang",
        "E Cambria"
      ],
      "year": "2018",
      "venue": "Inf. Sci",
      "doi": "10.1016/j.ins.2018.03.050"
    },
    {
      "citation_id": "160",
      "title": "Generative Adversarial Nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "161",
      "title": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification",
      "authors": [
        "X Chen",
        "Y Sun",
        "B Athiwaratkun",
        "C Cardie",
        "K Weinberger"
      ],
      "year": "2018",
      "venue": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification",
      "doi": "10.1162/tacl_a_00039"
    },
    {
      "citation_id": "162",
      "title": "Adversarial Training for Aspect-Based Sentiment Analysis with BERT",
      "authors": [
        "A Karimi",
        "L Rossi",
        "A Prati"
      ],
      "year": "2020",
      "venue": "25th Int. Conf. Pattern Recognit. ICPR, IEEE",
      "doi": "10.1109/ICPR48806.2021.9412167"
    },
    {
      "citation_id": "163",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "S Chul Min Lee",
        "Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE Trans. Speech Audio Process",
      "doi": "10.1109/TSA.2004.838534"
    },
    {
      "citation_id": "164",
      "title": "Spectral and Cepstral Audio Noise Reduction Techniques in Speech Emotion Recognition",
      "authors": [
        "J Pohjalainen",
        "F Fabien Ringeval",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proc. 2016 ACM Multimed. Conf. -MM 16",
      "doi": "10.1145/2964284.2967306"
    },
    {
      "citation_id": "165",
      "title": "Speech Emotion Recognition Using CNN",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "Proc. ACM Int. Conf. Multimed. -MM 14",
      "doi": "10.1145/2647868.2654984"
    },
    {
      "citation_id": "166",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Netw",
      "doi": "10.1016/j.neunet.2017.02.013"
    },
    {
      "citation_id": "167",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akçay",
        "K Oğuz"
      ],
      "year": "2020",
      "venue": "Speech Commun",
      "doi": "10.1016/j.specom.2019.12.001"
    },
    {
      "citation_id": "168",
      "title": "Detection of Clinical Depression in Adolescents' Speech During Family Interactions",
      "authors": [
        "L Low",
        "N Maddage",
        "M Lech",
        "L Sheeber",
        "N Allen"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Biomed. Eng",
      "doi": "10.1109/TBME.2010.2091640"
    },
    {
      "citation_id": "169",
      "title": "OpenEAR -Introducing the munich open-source emotion and affect recognition toolkit",
      "authors": [
        "F Eyben",
        "M Wollmer",
        "B Schuller"
      ],
      "year": "2009",
      "venue": "2009 3rd Int. Conf. Affect",
      "doi": "10.1109/ACII.2009.5349350"
    },
    {
      "citation_id": "170",
      "title": "Modeling the Temporal Evolution of Acoustic Parameters for Speech Emotion Recognition",
      "authors": [
        "S Ntalampiras",
        "N Fakotakis"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.31"
    },
    {
      "citation_id": "171",
      "title": "Feature selection for fast speech emotion recognition",
      "authors": [
        "L Zhang",
        "M Song",
        "N Li",
        "J Bu",
        "C Chen"
      ],
      "year": "2009",
      "venue": "Proc. 17th ACM Int. Conf. Multimed",
      "doi": "10.1145/1631272.1631405"
    },
    {
      "citation_id": "172",
      "title": "Exploiting the Potentialities of Features for Speech Emotion Recognition",
      "authors": [
        "D Li",
        "Y Zhou",
        "Z Wang",
        "D Gao"
      ],
      "year": "2020",
      "venue": "Inf. Sci",
      "doi": "10.1016/j.ins.2020.09.047"
    },
    {
      "citation_id": "173",
      "title": "Analysis of Emotionally Salient Aspects of Fundamental Frequency for Emotion Detection",
      "authors": [
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASL.2008.2009578"
    },
    {
      "citation_id": "174",
      "title": "The Relevance of Voice Quality Features in Speaker Independent Emotion Recognition",
      "authors": [
        "M Lugger",
        "B Yang"
      ],
      "year": "2007",
      "venue": "2007 IEEE Int. Conf. Acoust. Speech Signal Process. -ICASSP 07",
      "doi": "10.1109/ICASSP.2007.367152"
    },
    {
      "citation_id": "175",
      "title": "Speech based human emotion recognition using MFCC",
      "authors": [
        "M Likitha",
        "S Gupta",
        "K Hasitha",
        "A Raju"
      ],
      "year": "2017",
      "venue": "Commun. Signal Process. Netw. WiSPNET",
      "doi": "10.1109/WiSPNET.2017.8300161"
    },
    {
      "citation_id": "176",
      "title": "Class-level spectral features for emotion recognition",
      "authors": [
        "D Bitouk",
        "R Verma",
        "A Nenkova"
      ],
      "year": "2010",
      "venue": "Speech Commun",
      "doi": "10.1016/j.specom.2010.02.010"
    },
    {
      "citation_id": "177",
      "title": "Automatic Speech Emotion Recognition using Support Vector Machine",
      "authors": [
        "P Shen",
        "Z Changjun",
        "X Chen"
      ],
      "year": "2011",
      "venue": "Proc. 2011 Int. Conf. Electron",
      "doi": "10.1109/EMEIT.2011.6023178"
    },
    {
      "citation_id": "178",
      "title": "A feature selection and feature fusion combination method for speaker-independent speech emotion recognition",
      "authors": [
        "Y Jin",
        "P Song",
        "W Zheng",
        "L Zhao"
      ],
      "year": "2014",
      "venue": "2014 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2014.6854515"
    },
    {
      "citation_id": "179",
      "title": "A Speaker Independent Approach to the Classification of Emotional Vocal Expressions",
      "authors": [
        "H Atassi",
        "A Esposito"
      ],
      "year": "2008",
      "venue": "th IEEE Int. Conf. Tools Artif. Intell",
      "doi": "10.1109/ICTAI.2008.158"
    },
    {
      "citation_id": "180",
      "title": "Speech Emotion Recognition Using Fourier Parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2015.2392101"
    },
    {
      "citation_id": "181",
      "title": "Detection of stress and emotion in speech using traditional and FFT based log energy features",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Fourth Int. Conf. Inf. Commun. Signal Process. 2003 Fourth Pac. Rim Conf. Multimed. Proc. 2003 Jt",
      "doi": "10.1109/ICICS.2003.1292741"
    },
    {
      "citation_id": "182",
      "title": "An objective and subjective study of the role of semantics and prosodic features in building corpora for emotional TTS",
      "authors": [
        "E Navas",
        "I Hernaez",
        "Iker Luengo"
      ],
      "year": "2006",
      "venue": "IEEE Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASL.2006.876121"
    },
    {
      "citation_id": "183",
      "title": "SVM Scheme for Speech Emotion Recognition using MFCC Feature",
      "authors": [
        "A Milton",
        "S Sharmy",
        "S Roy",
        "Selvi"
      ],
      "year": "2013",
      "venue": "Int. J. Comput. Appl",
      "doi": "10.5120/11872-7667"
    },
    {
      "citation_id": "184",
      "title": "Speech Emotion Recognition Using Support Vector Machine",
      "authors": [
        "Y Pan",
        "P Shen",
        "L Shen"
      ],
      "year": "2012",
      "venue": "Int. J. Smart Home"
    },
    {
      "citation_id": "185",
      "title": "Speech emotion recognition using Support Vector Machines",
      "authors": [
        "T Seehapoch",
        "S Wongthanavasu"
      ],
      "year": "2013",
      "venue": "5th Int. Conf. Knowl. Smart Technol. KST",
      "doi": "10.1109/KST.2013.6512793"
    },
    {
      "citation_id": "186",
      "title": "Automatic Speech Emotion Recognition Using Auditory Models with Binary Decision Tree and SVM",
      "authors": [
        "E Yüncü",
        "H Hacihabiboglu",
        "C Bozsahin"
      ],
      "year": "2014",
      "venue": "nd Int. Conf. Pattern Recognit",
      "doi": "10.1109/ICPR.2014.143"
    },
    {
      "citation_id": "187",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Hitkul",
        "Shah"
      ],
      "year": "2019",
      "venue": "Knowl.-Based Syst",
      "doi": "10.1016/j.knosys.2019.104886"
    },
    {
      "citation_id": "188",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Inf. Sci",
      "doi": "10.1016/j.ins.2019.09.005"
    },
    {
      "citation_id": "189",
      "title": "Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2014.2360798"
    },
    {
      "citation_id": "190",
      "title": "High-Level Feature Representation Using Recurrent Neural Network for Speech Emotion Recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-Level Feature Representation Using Recurrent Neural Network for Speech Emotion Recognition"
    },
    {
      "citation_id": "191",
      "title": "On-line emotion recognition in a 3-D activationvalence-time continuum using acoustic and linguistic cues",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "A Graves",
        "B Schuller",
        "E Douglas-Cowie",
        "R Cowie"
      ],
      "year": "2010",
      "venue": "J. Multimodal User Interfaces",
      "doi": "10.1007/s12193-009-0032-6"
    },
    {
      "citation_id": "192",
      "title": "Speech Emotion Recognition Based on Speech Segment Using LSTM with Attention Model",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Based on Speech Segment Using LSTM with Attention Model",
      "doi": "10.1109/ICSIGSYS.2019.8811080"
    },
    {
      "citation_id": "193",
      "title": "Improving Speech Emotion Recognition with Unsupervised Representation Learning on Unlabeled Speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2019.8682541"
    },
    {
      "citation_id": "194",
      "title": "Domain Adversarial for Acoustic Emotion Recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEEACM Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASLP.2018.2867099"
    },
    {
      "citation_id": "195",
      "title": "Speech Emotion Recognition from Spectrograms with Deep Convolutional Neural Network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "Platf. Technol. Serv. PlatCon",
      "doi": "10.1109/PlatCon.2017.7883728"
    },
    {
      "citation_id": "196",
      "title": "Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2017.2766843"
    },
    {
      "citation_id": "197",
      "title": "Real-Time Speech Emotion and Sentiment Recognition for Interactive Dialogue Systems",
      "authors": [
        "D Bertero",
        "F Siddique",
        "C.-S Wu",
        "Y Wan",
        "R Chan",
        "P Fung"
      ],
      "year": "2016",
      "venue": "Proc. 2016 Conf. Empir. Methods Nat. Lang. Process",
      "doi": "10.18653/v1/D16-1110"
    },
    {
      "citation_id": "198",
      "title": "Extreme learning machine: Theory and applications",
      "authors": [
        "G.-B Huang",
        "Q.-Y Zhu",
        "C.-K Siew"
      ],
      "year": "2006",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2005.12.126"
    },
    {
      "citation_id": "199",
      "title": "Representation Learning for Speech Emotion Recognition",
      "authors": [
        "S Ghosh",
        "E Laksana",
        "L.-P Morency",
        "S Scherer"
      ],
      "year": "2016",
      "venue": "Representation Learning for Speech Emotion Recognition",
      "doi": "10.21437/Interspeech.2016-692"
    },
    {
      "citation_id": "200",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2017.7952552"
    },
    {
      "citation_id": "201",
      "title": "3-D Convolutional Recurrent Neural Networks With Attention Model for Speech Emotion Recognition",
      "authors": [
        "M Chen",
        "X He",
        "J Yang",
        "H Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Process. Lett",
      "doi": "10.1109/LSP.2018.2860246"
    },
    {
      "citation_id": "202",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2016.7472669"
    },
    {
      "citation_id": "203",
      "title": "End-to-End Speech Emotion Recognition Using Deep Neural Networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2018.8462677"
    },
    {
      "citation_id": "204",
      "title": "Speech Emotion Recognition Using Capsule Networks, in: ICASSP 2019 -2019 IEEE Int. Conf. Acoust. Speech Signal Process",
      "authors": [
        "X Wu",
        "S Liu",
        "Y Cao",
        "X Li",
        "J Yu",
        "D Dai",
        "X Ma",
        "S Hu",
        "Z Wu",
        "X Liu",
        "H Meng"
      ],
      "year": "2019",
      "venue": "Speech Emotion Recognition Using Capsule Networks, in: ICASSP 2019 -2019 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2019.8683163"
    },
    {
      "citation_id": "205",
      "title": "Deep Spectrum Feature Representations for Speech Emotion Recognition, in: ASMMC-MMAC18",
      "authors": [
        "Z Zhao",
        "Y Zhao",
        "Z Bao",
        "H Wang",
        "Z Zhang",
        "C Li"
      ],
      "year": "2018",
      "venue": "Deep Spectrum Feature Representations for Speech Emotion Recognition, in: ASMMC-MMAC18",
      "doi": "10.1145/3267935.3267948"
    },
    {
      "citation_id": "206",
      "title": "Adversarial Auto-Encoders for Speech Based Emotion Recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "W Abdalmageed",
        "C Espy-Wilson"
      ],
      "year": "2017",
      "venue": "Adversarial Auto-Encoders for Speech Based Emotion Recognition",
      "doi": "10.21437/Interspeech.2017-1421"
    },
    {
      "citation_id": "207",
      "title": "Towards Conditional Adversarial Training for Predicting Emotions from Speech",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2018.8462579"
    },
    {
      "citation_id": "208",
      "title": "Modeling Feature Representations for Affective Speech using Generative Adversarial Networks",
      "authors": [
        "S Sahu",
        "R Gupta",
        "C Espy-Wilson"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.2998118"
    },
    {
      "citation_id": "209",
      "title": "CycleGAN-Based Emotion Style Transfer as Data Augmentation for Speech Emotion Recognition",
      "authors": [
        "F Bao",
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ISCA",
      "doi": "10.21437/Interspeech.2019-2293"
    },
    {
      "citation_id": "210",
      "title": "Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks",
      "authors": [
        "J Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "2017 IEEE Int. Conf. Comput. Vis. ICCV",
      "doi": "10.1109/ICCV.2017.244"
    },
    {
      "citation_id": "211",
      "title": "A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2008.52"
    },
    {
      "citation_id": "213",
      "title": "Recognizing action units for facial expression analysis",
      "authors": [
        "T Tian",
        "J Kanade",
        "Cohn"
      ],
      "year": "2001",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/34.908962"
    },
    {
      "citation_id": "214",
      "title": "Automatic Analysis of Facial Affect: A Survey of Registration, Representation, and Recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2014.2366127"
    },
    {
      "citation_id": "215",
      "title": "Facial Expression Recognition via a Boosted Deep Belief Network",
      "authors": [
        "P Liu",
        "S Han",
        "Z Meng",
        "Y Tong"
      ],
      "year": "2014",
      "venue": "2014 IEEE Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2014.233"
    },
    {
      "citation_id": "216",
      "title": "Joint Fine-Tuning in Deep Neural Networks for Facial Expression Recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "2015 IEEE Int. Conf. Comput. Vis. ICCV, IEEE",
      "doi": "10.1109/ICCV.2015.341"
    },
    {
      "citation_id": "217",
      "title": "Learning from Macro-expression: a Micro-expression Recognition Framework",
      "authors": [
        "B Xia",
        "W Wang",
        "S Wang",
        "E Chen"
      ],
      "year": "2020",
      "venue": "Proc. 28th ACM Int. Conf. Multimed., ACM, Seattle WA USA",
      "doi": "10.1145/3394171.3413774"
    },
    {
      "citation_id": "218",
      "title": "Video-based Facial Micro-Expression Analysis: A Survey of Datasets, Features and Algorithms",
      "authors": [
        "X Ben",
        "Y Ren",
        "J Zhang",
        "S.-J Wang",
        "K Kpalma",
        "W Meng",
        "Y.-J Liu"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2021.3067464"
    },
    {
      "citation_id": "219",
      "title": "A Main Directional Mean Optical Flow Feature for Spontaneous Micro-Expression Recognition",
      "authors": [
        "Y.-J Liu",
        "J.-K Zhang",
        "W.-J Yan",
        "S.-J Wang",
        "G Zhao",
        "X Fu"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2015.2485205"
    },
    {
      "citation_id": "220",
      "title": "Effective micro-expression recognition using relaxed K-SVD algorithm",
      "authors": [
        "H Zheng",
        "J Zhu",
        "Z Yang",
        "Z Jin"
      ],
      "year": "2017",
      "venue": "Int. J. Mach. Learn. Cybern",
      "doi": "10.1007/s13042-017-0684-6"
    },
    {
      "citation_id": "221",
      "title": "Dynamic Micro-Expression Recognition Using Knowledge Distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.2986962"
    },
    {
      "citation_id": "222",
      "title": "Automatic Facial Expression Recognition System Using Deep Network-Based Data Fusion",
      "authors": [
        "A Majumder",
        "L Behera",
        "V Subramanian"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Cybern",
      "doi": "10.1109/TCYB.2016.2625419"
    },
    {
      "citation_id": "223",
      "title": "Facial expression recognition using distance and texture signature relevant features",
      "authors": [
        "A Barman",
        "P Dutta"
      ],
      "year": "2019",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/j.asoc.2019.01.011"
    },
    {
      "citation_id": "224",
      "title": "Dynamic Objectives Learning for Facial Expression Recognition",
      "authors": [
        "G Wen",
        "T Chang",
        "H Li",
        "L Jiang"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2020.2966858"
    },
    {
      "citation_id": "225",
      "title": "Feature selection for improved 3D facial expression recognition",
      "authors": [
        "K Yurtkan",
        "H Demirel"
      ],
      "year": "2014",
      "venue": "Pattern Recognit. Lett",
      "doi": "10.1016/j.patrec.2013.10.026"
    },
    {
      "citation_id": "226",
      "title": "Muscular Movement Model-Based Automatic 3D/4D Facial Expression Recognition",
      "authors": [
        "Q Zhen",
        "D Huang",
        "Y Wang",
        "L Chen"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2016.2557063"
    },
    {
      "citation_id": "227",
      "title": "Automatic 4D Facial Expression Recognition via Collaborative Cross-domain Dynamic Image Network",
      "authors": [
        "M Behzad",
        "N Vo",
        "X Li",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Proc. Br. Mach. Vis. Conf. BMVC"
    },
    {
      "citation_id": "228",
      "title": "Image based Static Facial Expression Recognition with Multiple Deep Network Learning",
      "authors": [
        "Z Yu",
        "C Zhang"
      ],
      "year": "2015",
      "venue": "Proc. 2015 ACM Int. Conf. Multimodal Interact",
      "doi": "10.1145/2818346.2830595"
    },
    {
      "citation_id": "229",
      "title": "Extended deep neural network for facial emotion recognition",
      "authors": [
        "D Jain",
        "P Shamsolmoali",
        "P Sehdev"
      ],
      "year": "2019",
      "venue": "Pattern Recognit. Lett",
      "doi": "10.1016/j.patrec.2019.01.008"
    },
    {
      "citation_id": "230",
      "title": "HoloNet: towards robust emotion recognition in the wild",
      "authors": [
        "A Yao",
        "D Cai",
        "P Hu",
        "S Wang",
        "L Sha",
        "Y Chen"
      ],
      "year": "2016",
      "venue": "Proc. 18th ACM Int. Conf. Multimodal Interact",
      "doi": "10.1145/2993148.2997639"
    },
    {
      "citation_id": "231",
      "title": "Feature-based facial expression recognition: sensitivity analysis and experiments with a multilayer perceptron",
      "authors": [
        "Z Zhang"
      ],
      "year": "1999",
      "venue": "Int. J. Pattern Recognit. Artif. Intell",
      "doi": "10.1142/S0218001499000495"
    },
    {
      "citation_id": "232",
      "title": "Deep spatial-temporal feature fusion for facial expression recognition in static images",
      "authors": [
        "N Sun",
        "Q Li",
        "R Huan",
        "J Liu",
        "G Han"
      ],
      "year": "2019",
      "venue": "Pattern Recognit. Lett",
      "doi": "10.1016/j.patrec.2017.10.022"
    },
    {
      "citation_id": "233",
      "title": "Geometric Feature-Based Facial Expression Recognition in Image Sequences Using Multi-Class AdaBoost and Support Vector Machines",
      "authors": [
        "D Ghimire",
        "J Lee"
      ],
      "year": "2013",
      "venue": "Sensors",
      "doi": "10.3390/s130607714"
    },
    {
      "citation_id": "234",
      "title": "Face Expression Detection on Kinect Using Active Appearance Model and Fuzzy Logic",
      "authors": [
        "A Sujono",
        "Gunawan"
      ],
      "year": "2015",
      "venue": "Face Expression Detection on Kinect Using Active Appearance Model and Fuzzy Logic",
      "doi": "10.1016/j.procs.2015.07.558"
    },
    {
      "citation_id": "235",
      "title": "Active appearance models",
      "authors": [
        "T Cootes",
        "G Edwards",
        "C Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/34.927467"
    },
    {
      "citation_id": "236",
      "title": "Facial Action Coding System: The Manual on CD ROM",
      "authors": [
        "P Ekman",
        "J Hager",
        "W Friesen"
      ],
      "year": "2002",
      "venue": "Facial Action Coding System: The Manual on CD ROM"
    },
    {
      "citation_id": "237",
      "title": "Facial expression recognition with local prominent directional pattern, Signal Process",
      "authors": [
        "F Makhmudkhujaev",
        "M Abdullah-Al-Wadud",
        "M Iqbal",
        "B Ryu",
        "O Chae"
      ],
      "year": "2019",
      "venue": "Image Commun",
      "doi": "10.1016/j.image.2019.01.002"
    },
    {
      "citation_id": "238",
      "title": "Low-resolution facial expression recognition: A filter learning perspective, Signal Process",
      "authors": [
        "Y Yan",
        "Z Zhang",
        "S Chen",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Low-resolution facial expression recognition: A filter learning perspective, Signal Process",
      "doi": "10.1016/j.sigpro.2019.107370"
    },
    {
      "citation_id": "239",
      "title": "Texture and Geometry Scattering Representation-Based Facial Expression Recognition in 2D+3D Videos",
      "authors": [
        "Y Yao",
        "D Huang",
        "X Yang",
        "Y Wang",
        "L Chen"
      ],
      "year": "2018",
      "venue": "ACM Trans. Multimed. Comput. Commun. Appl",
      "doi": "10.1145/3131345"
    },
    {
      "citation_id": "240",
      "title": "Learning from Hierarchical Spatiotemporal Descriptors for Micro-Expression Recognition",
      "authors": [
        "Y Zong",
        "X Huang",
        "W Zheng",
        "Z Cui",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2018.2820321"
    },
    {
      "citation_id": "241",
      "title": "Facial Expression Recognition Based on Deep Evolutional Spatial-Temporal Networks",
      "authors": [
        "K Zhang",
        "Y Huang",
        "Y Du",
        "L Wang"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Image Process",
      "doi": "10.1109/TIP.2017.2689999"
    },
    {
      "citation_id": "242",
      "title": "Joint Pose and Expression Modeling for Facial Expression Recognition, in: 2018 IEEECVF Conf. Comput. Vis. Pattern Recognit",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "C Xu"
      ],
      "year": "2018",
      "venue": "Joint Pose and Expression Modeling for Facial Expression Recognition, in: 2018 IEEECVF Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2018.00354"
    },
    {
      "citation_id": "243",
      "title": "Automatic facial expression analysis: a survey",
      "authors": [
        "B Fasel",
        "J Luettin"
      ],
      "year": "2003",
      "venue": "Pattern Recognit",
      "doi": "10.1016/S0031-3203(02)00052-3"
    },
    {
      "citation_id": "244",
      "title": "Automated Facial Action Coding System for dynamic analysis of facial expressions in neuropsychiatric disorders",
      "authors": [
        "J Hamm",
        "C Kohler",
        "R Gur",
        "R Verma"
      ],
      "year": "2011",
      "venue": "J. Neurosci. Methods",
      "doi": "10.1016/j.jneumeth.2011.06.023"
    },
    {
      "citation_id": "245",
      "title": "Facial expression recognition based on Local Binary Patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image Vis. Comput",
      "doi": "10.1016/j.imavis.2008.08.005"
    },
    {
      "citation_id": "246",
      "title": "Facial expression recognition using radial encoding of local Gabor features and classifier synthesis",
      "authors": [
        "W Gu",
        "C Xiang",
        "Y Venkatesh",
        "D Huang",
        "H Lin"
      ],
      "year": "2012",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2011.05.006"
    },
    {
      "citation_id": "247",
      "title": "Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expressions",
      "authors": [
        "G Zhao",
        "M Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2007.1110"
    },
    {
      "citation_id": "248",
      "title": "Efficient Spatio-Temporal Local Binary Patterns for Spontaneous Facial Micro-Expression Recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R -W. Phan",
        "Y.-H Oh"
      ],
      "year": "2015",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0124674"
    },
    {
      "citation_id": "249",
      "title": "Micro-Facial Movements: An Investigation on Spatio-Temporal Descriptors",
      "authors": [
        "A Davison",
        "M Yap",
        "N Costen",
        "K Tan",
        "C Lansley",
        "D Leightley"
      ],
      "year": "2015",
      "venue": "Micro-Facial Movements: An Investigation on Spatio-Temporal Descriptors",
      "doi": "10.1007/978-3-319-16181-5_8"
    },
    {
      "citation_id": "250",
      "title": "Hybrid Facial Regions Extraction for Micro-expression Recognition System",
      "authors": [
        "S.-T Liong",
        "J See",
        "R -W. Phan",
        "K Wong",
        "S.-W Tan"
      ],
      "year": "2018",
      "venue": "J. Signal Process. Syst",
      "doi": "10.1007/s11265-017-1276-0"
    },
    {
      "citation_id": "251",
      "title": "Micro-Expression Recognition by Aggregating Local Spatio-Temporal Patterns",
      "authors": [
        "S Zhang",
        "B Feng",
        "Z Chen",
        "X Huang"
      ],
      "year": "2017",
      "venue": "Micro-Expression Recognition by Aggregating Local Spatio-Temporal Patterns",
      "doi": "10.1007/978-3-319-51811-4_52"
    },
    {
      "citation_id": "252",
      "title": "Magnifying Subtle Facial Motions for Effective 4D Expression Recognition",
      "authors": [
        "Q Zhen",
        "D Huang",
        "H Drira",
        "B Amor",
        "Y Wang",
        "M Daoudi"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2017.2747553"
    },
    {
      "citation_id": "253",
      "title": "2D facial expression recognition via 3D reconstruction and feature fusion",
      "authors": [
        "A Moeini",
        "K Faez",
        "H Sadeghi",
        "H Moeini"
      ],
      "year": "2016",
      "venue": "J. Vis. Commun. Image Represent",
      "doi": "10.1016/j.jvcir.2015.11.006"
    },
    {
      "citation_id": "254",
      "title": "Are subtle expressions too sparse to recognize?",
      "authors": [
        "A Le Ngo",
        "S.-T Liong",
        "J See",
        "-W Phan"
      ],
      "year": "2015",
      "venue": "2015 IEEE Int. Conf. Digit. Signal Process",
      "doi": "10.1109/ICDSP.2015.7252080"
    },
    {
      "citation_id": "255",
      "title": "Multi-View Facial Expression Recognition Based on Group Sparse Reduced-Rank Regression",
      "authors": [
        "W Zheng"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2014.2304712"
    },
    {
      "citation_id": "256",
      "title": "Ferná ndez-Martí nez, Towards a robust affect recognition: Automatic facial expression recognition in 3D faces",
      "authors": [
        "A Azazi",
        "S Lebai",
        "I Lutfi",
        "F Venkat"
      ],
      "year": "2015",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2014.10.042"
    },
    {
      "citation_id": "257",
      "title": "Non-rigid registration based model-free 3D facial expression recognition",
      "authors": [
        "A Savran",
        "B Sankur"
      ],
      "year": "2017",
      "venue": "Comput. Vis. Image Underst",
      "doi": "10.1016/j.cviu.2017.07.005"
    },
    {
      "citation_id": "258",
      "title": "Emotion recognition using fixed length micro-expressions sequence and weighting method",
      "authors": [
        "M Chen",
        "H Ma",
        "J Li",
        "H Wang"
      ],
      "year": "2016",
      "venue": "IEEE Int. Conf. Real-Time Comput. Robot. RCAR",
      "doi": "10.1109/RCAR.2016.7784067"
    },
    {
      "citation_id": "259",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Int. Conf. Learn. Represent"
    },
    {
      "citation_id": "260",
      "title": "Procedings Br. Mach. Vis. Conf. 2015, British Machine Vision Association",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Procedings Br. Mach. Vis. Conf. 2015, British Machine Vision Association",
      "doi": "10.5244/C.29.41"
    },
    {
      "citation_id": "261",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "262",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "2015 IEEE Conf. Comput. Vis. Pattern Recognit. CVPR",
      "doi": "10.1109/CVPR.2015.7298594"
    },
    {
      "citation_id": "263",
      "title": "Facial Expression Recognition by De-expression Residue Learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "2018 IEEECVF Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2018.00231"
    },
    {
      "citation_id": "264",
      "title": "Micro-attention for micro-expression recognition",
      "authors": [
        "C Wang",
        "M Peng",
        "T Bi",
        "T Chen"
      ],
      "year": "2020",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.06.005"
    },
    {
      "citation_id": "265",
      "title": "Hard negative generation for identity-disentangled facial expression recognition",
      "authors": [
        "X Liu",
        "B Vijaya Kumar",
        "P Jia",
        "J You"
      ],
      "year": "2019",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2018.11.001"
    },
    {
      "citation_id": "266",
      "title": "Identity-Aware Convolutional Neural Network for Facial Expression Recognition",
      "authors": [
        "Z Meng",
        "P Liu",
        "J Cai",
        "S Han",
        "Y Tong"
      ],
      "year": "2017",
      "venue": "th IEEE Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/FG.2017.140"
    },
    {
      "citation_id": "267",
      "title": "OAENet: Oriented attention ensemble for accurate facial expression recognition",
      "authors": [
        "Z Wang",
        "F Zeng",
        "S Liu",
        "B Zeng"
      ],
      "year": "2021",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2020.107694"
    },
    {
      "citation_id": "268",
      "title": "LBAN-IL: A novel method of high discriminative representation for facial expression recognition",
      "authors": [
        "H Li",
        "N Wang",
        "Y Yu",
        "X Yang",
        "X Gao"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.12.076"
    },
    {
      "citation_id": "269",
      "title": "FERAtt: Facial Expression Recognition With Attention Net",
      "authors": [
        "P Fernandez",
        "F Pena",
        "T Ren",
        "A Cunha"
      ],
      "year": "2019",
      "venue": "2019 IEEECVF Conf. Comput. Vis. Pattern Recognit. Workshop CVPRW",
      "doi": "10.1109/CVPRW.2019.00112"
    },
    {
      "citation_id": "270",
      "title": "Deep multi-path convolutional neural network joint with salient region attention for facial expression recognition",
      "authors": [
        "S Xie",
        "H Hu",
        "Y Wu"
      ],
      "year": "2019",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2019.03.019"
    },
    {
      "citation_id": "271",
      "title": "Suppressing Uncertainties for Large-Scale Facial Expression Recognition, in: 2020 IEEECVF Conf. Comput. Vis. Pattern Recognit. CVPR, IEEE",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Suppressing Uncertainties for Large-Scale Facial Expression Recognition, in: 2020 IEEECVF Conf. Comput. Vis. Pattern Recognit. CVPR, IEEE",
      "doi": "10.1109/CVPR42600.2020.00693"
    },
    {
      "citation_id": "272",
      "title": "Discriminative Attention-based Convolutional Neural Network for 3D Facial Expression Recognition",
      "authors": [
        "K Zhu",
        "Z Du",
        "W Li",
        "D Huang",
        "Y Wang",
        "L Chen"
      ],
      "year": "2019",
      "venue": "th IEEE Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/FG.2019.8756524"
    },
    {
      "citation_id": "273",
      "title": "Landmark guidance independent spatio-channel attention and complementary context information based facial expression recognition",
      "authors": [
        "D Gera",
        "S Balasubramanian"
      ],
      "year": "2021",
      "venue": "Pattern Recognit. Lett",
      "doi": "10.1016/j.patrec.2021.01.029"
    },
    {
      "citation_id": "274",
      "title": "Softmax regression based deep sparse autoencoder network for facial emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "M Zhou",
        "W Su",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2018",
      "venue": "Inf. Sci",
      "doi": "10.1016/j.ins.2017.10.044"
    },
    {
      "citation_id": "275",
      "title": "Fast and Light Manifold CNN based 3D Facial Expression Recognition across Pose Variations",
      "authors": [
        "Z Chen",
        "D Huang",
        "Y Wang",
        "L Chen"
      ],
      "year": "2018",
      "venue": "Proc. 26th ACM Int. Conf. Multimed",
      "doi": "10.1145/3240508.3240568"
    },
    {
      "citation_id": "276",
      "title": "Multimodal 2D+3D Facial Expression Recognition With Deep Fusion Convolutional Neural Network",
      "authors": [
        "Huibin Li",
        "Jian Sun",
        "Zongben Xu"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2017.2713408"
    },
    {
      "citation_id": "277",
      "title": "A Deeper Look at Facial Expression Dataset Bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.2973158"
    },
    {
      "citation_id": "278",
      "title": "Adaptively Learning Facial Expression Representation via C-F Labels and Distillation",
      "authors": [
        "H Li",
        "N Wang",
        "X Ding",
        "X Yang",
        "X Gao"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Image Process",
      "doi": "10.1109/TIP.2021.3049955"
    },
    {
      "citation_id": "279",
      "title": "Learning Spatiotemporal Features with 3D Convolutional Networks, in: 2015 IEEE Int. Conf. Comput. Vis. ICCV",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Learning Spatiotemporal Features with 3D Convolutional Networks, in: 2015 IEEE Int. Conf. Comput. Vis. ICCV",
      "doi": "10.1109/ICCV.2015.510"
    },
    {
      "citation_id": "280",
      "title": "Deep Learning for Spatio-Temporal Modeling of Dynamic Spontaneous Emotions",
      "authors": [
        "D Chanti",
        "A Caplier"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2018.2873600"
    },
    {
      "citation_id": "281",
      "title": "Deep Spatio-Temporal Features for Multimodal Emotion Recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "A Ghasemi",
        "D Dean",
        "C Fookes"
      ],
      "year": "2017",
      "venue": "IEEE Winter Conf. Appl. Comput. Vis. WACV",
      "doi": "10.1109/WACV.2017.140"
    },
    {
      "citation_id": "282",
      "title": "Eulerian Motion Based 3DCNN Architecture for Facial Micro-Expression Recognition",
      "authors": [
        "Y Wang",
        "H Ma",
        "X Xing",
        "Z Pan"
      ],
      "year": "2020",
      "venue": "Multimed. Model",
      "doi": "10.1007/978-3-030-37731-1_22"
    },
    {
      "citation_id": "283",
      "title": "MER-GCN: Micro-Expression Recognition Based on Relation Modeling with Graph Convolutional Networks",
      "authors": [
        "L Lo",
        "H.-X Xie",
        "H.-H Shuai",
        "W.-H Cheng"
      ],
      "year": "2020",
      "venue": "Inf. Process. Retr. MIPR",
      "doi": "10.1109/MIPR49039.2020.00023"
    },
    {
      "citation_id": "284",
      "title": "Multi-Objective Based Spatio-Temporal Feature Representation Learning Robust to Expression Intensity Variations for Facial Expression Recognition",
      "authors": [
        "D Kim",
        "W Baddar",
        "J Jang",
        "Y Ro"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2017.2695999"
    },
    {
      "citation_id": "285",
      "title": "Towards Reading Beyond Faces for Sparsity-aware 3D/4D Affect Recognition",
      "authors": [
        "M Behzad",
        "N Vo",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Towards Reading Beyond Faces for Sparsity-aware 3D/4D Affect Recognition",
      "doi": "10.1016/j.neucom.2021.06.023"
    },
    {
      "citation_id": "286",
      "title": "Micro-Expression Recognition with Expression-State Constrained Spatio-Temporal Feature Representations",
      "authors": [
        "D Kim",
        "W Baddar",
        "Y Ro"
      ],
      "year": "2016",
      "venue": "Proc. 24th ACM Int. Conf. Multimed",
      "doi": "10.1145/2964284.2967247"
    },
    {
      "citation_id": "287",
      "title": "Spatiotemporal Recurrent Convolutional Networks for Recognizing Spontaneous Micro-Expressions",
      "authors": [
        "Z Xia",
        "X Hong",
        "X Gao",
        "X Feng",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2019.2931351"
    },
    {
      "citation_id": "288",
      "title": "Exploiting multi-CNN features in CNN-RNN based Dimensional Emotion Recognition on the OMG inthe-wild Dataset",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.3014171"
    },
    {
      "citation_id": "289",
      "title": "SAANet: Siamese action-units attention network for improving dynamic facial expression recognition",
      "authors": [
        "D Liu",
        "X Ouyang",
        "S Xu",
        "P Zhou",
        "K He",
        "S Wen"
      ],
      "year": "2020",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.06.062"
    },
    {
      "citation_id": "290",
      "title": "Geometry Guided Pose-Invariant Facial Expression Recognition",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "C Xu"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Image Process",
      "doi": "10.1109/TIP.2020.2972114"
    },
    {
      "citation_id": "291",
      "title": "Identity-Adaptive Facial Expression Recognition through Expression Regeneration Using Conditional Generative Adversarial Networks",
      "authors": [
        "H Yang",
        "Z Zhang",
        "L Yin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE Int. Conf. Autom. Face Gesture Recognit",
      "doi": "10.1109/FG.2018.00050"
    },
    {
      "citation_id": "292",
      "title": "Semantic Neighborhood-Aware Deep Facial Expression Recognition",
      "authors": [
        "Y Fu",
        "X Wu",
        "X Li",
        "Z Pan",
        "D Luo"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Image Process",
      "doi": "10.1109/TIP.2020.2991510"
    },
    {
      "citation_id": "293",
      "title": "Facial Expression Recognition Using Disentangled Adversarial Learning, ArXiv190913135 Cs",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2019",
      "venue": "Facial Expression Recognition Using Disentangled Adversarial Learning, ArXiv190913135 Cs"
    },
    {
      "citation_id": "294",
      "title": "ICE-GAN: Identity-Aware and Capsule-Enhanced GAN with Graph-Based Reasoning for Micro-Expression Recognition and Synthesis",
      "authors": [
        "J Yu",
        "C Zhang",
        "Y Song",
        "W Cai"
      ],
      "year": "2021",
      "venue": "Neural Netw. IJCNN",
      "doi": "10.1109/IJCNN52387.2021.9533988"
    },
    {
      "citation_id": "295",
      "title": "Adaptive Body Gesture Representation for Automatic Emotion Recognition",
      "authors": [
        "S Piana",
        "A Staglianò",
        "F Odone",
        "A Camurri"
      ],
      "year": "2016",
      "venue": "ACM Trans. Interact. Intell. Syst",
      "doi": "10.1145/2818740"
    },
    {
      "citation_id": "296",
      "title": "Body Language in Affective Human-Robot Interaction",
      "authors": [
        "D Stoeva",
        "M Gelautz"
      ],
      "year": "2020",
      "venue": "Companion 2020 ACMIEEE Int. Conf. Hum.-Robot Interact., ACM, Cambridge United Kingdom",
      "doi": "10.1145/3371382.3377432"
    },
    {
      "citation_id": "297",
      "title": "Articulated Human Detection with Flexible Mixtures of Parts",
      "authors": [
        "Y Yang",
        "D Ramanan"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2012.261"
    },
    {
      "citation_id": "298",
      "title": "Towards Real-Time Object Detection with Region Proposal Networks",
      "authors": [
        "S Ren",
        "K He",
        "R Girshick",
        "J Sun",
        "R-Cnn Faster"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2016.2577031"
    },
    {
      "citation_id": "299",
      "title": "Adaptive Real-Time Emotion Recognition from Body Movements",
      "authors": [
        "W Weiyi",
        "E Valentin",
        "S Hichem"
      ],
      "year": "2015",
      "venue": "ACM Trans. Interact. Intell. Syst. TiiS",
      "doi": "10.1145/2738221"
    },
    {
      "citation_id": "300",
      "title": "Recognizing Affective Dimensions from Body Posture",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2007",
      "venue": "Recognizing Affective Dimensions from Body Posture",
      "doi": "10.1007/978-3-540-74889-2_5"
    },
    {
      "citation_id": "301",
      "title": "Recognising Human Emotions from Body Movement and Gesture Dynamics",
      "authors": [
        "G Castellano",
        "S Villalba"
      ],
      "year": "2007",
      "venue": "Affect. Comput. Intell. Interact",
      "doi": "10.1007/978-3-540-74889-2_7"
    },
    {
      "citation_id": "302",
      "title": "A study on emotion recognition from body gestures using Kinect sensor",
      "authors": [
        "S Saha",
        "S Datta",
        "A Konar",
        "R Janarthanan"
      ],
      "year": "2014",
      "venue": "Int. Conf. Commun. Signal Process",
      "doi": "10.1109/ICCSP.2014.6949798"
    },
    {
      "citation_id": "303",
      "title": "Artif. Intell. Soft Comput",
      "authors": [
        "Y Maret",
        "D Oberson",
        "M Gavrilova"
      ],
      "year": "2018",
      "venue": "Artif. Intell. Soft Comput"
    },
    {
      "citation_id": "304",
      "title": "Continuous body emotion recognition system during theater performances",
      "authors": [
        "S Senecal",
        "L Cuel",
        "A Aristidou",
        "N Magnenat-Thalmann"
      ],
      "year": "2016",
      "venue": "Comput. Animat. Virtual Worlds",
      "doi": "10.1002/cav.1714"
    },
    {
      "citation_id": "305",
      "title": "Toward a Minimal Representation of Affective Gestures",
      "authors": [
        "D Glowinski",
        "N Dael",
        "A Camurri",
        "G Volpe",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.7"
    },
    {
      "citation_id": "306",
      "title": "UnSkEm: Unobtrusive Skeletal-based Emotion Recognition for User Experience",
      "authors": [
        "M Razzaq",
        "J Bang",
        "S Kang",
        "S Lee"
      ],
      "year": "2020",
      "venue": "Inf. Netw. ICOIN",
      "doi": "10.1109/ICOIN48656.2020.9016601"
    },
    {
      "citation_id": "307",
      "title": "Vision-Based Human Emotion Recognition Using HOG-KLT Feature",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2020",
      "venue": "Proc. First Int. Conf. Comput. Commun. Cyber-Secur. IC4S 2019",
      "doi": "10.1007/978-981-15-3369-3_20"
    },
    {
      "citation_id": "308",
      "title": "Human Emotion Recognition Using Body Expressive Feature",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2020",
      "venue": "Microservices Big Data Anal",
      "doi": "10.1007/978-981-15-0128-9_13"
    },
    {
      "citation_id": "309",
      "title": "Gesture-Based Affective Computing on Motion Capture Data",
      "authors": [
        "A Kapur",
        "A Kapur",
        "N Virji-Babul",
        "G Tzanetakis",
        "P Driessen"
      ],
      "year": "2005",
      "venue": "Affect. Comput. Intell. Interact",
      "doi": "10.1007/11573548_1"
    },
    {
      "citation_id": "310",
      "title": "Automatic Recognition of Non-Acted Affective Postures",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze",
        "A Steed"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Syst. Man Cybern. Part B Cybern",
      "doi": "10.1109/TSMCB.2010.2103557"
    },
    {
      "citation_id": "311",
      "title": "Emotion categorization of body expressions in narrative scenarios",
      "authors": [
        "E Volkova",
        "B Mohler",
        "T Dodds",
        "J Tesch",
        "H Bülthoff"
      ],
      "year": "2014",
      "venue": "Front. Psychol",
      "doi": "10.3389/fpsyg.2014.00623"
    },
    {
      "citation_id": "312",
      "title": "Multi-level classification of emotional body expression",
      "authors": [
        "N Fourati",
        "C Pelachaud"
      ],
      "year": "2015",
      "venue": "th IEEE Int. Conf. Workshop Autom. Face Gesture Recognit",
      "doi": "10.1109/FG.2015.7163145"
    },
    {
      "citation_id": "313",
      "title": "Emotion Recognition Based on Multi-View Body Gestures",
      "authors": [
        "Z Shen",
        "J Cheng",
        "X Hu",
        "Q Dong"
      ],
      "year": "2019",
      "venue": "2019 IEEE Int. Conf. Image Process",
      "doi": "10.1109/ICIP.2019.8803460"
    },
    {
      "citation_id": "314",
      "title": "Deep Learning Approach for Emotion Recognition from Human Body Movements with Feedforward Deep Convolution Neural Networks",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2019",
      "venue": "Procedia Comput. Sci",
      "doi": "10.1016/j.procs.2019.05.038"
    },
    {
      "citation_id": "315",
      "title": "Emotion Recognition via Body Gesture: Deep Learning Model Coupled with Keyframe Selection",
      "authors": [
        "S Ly",
        "G.-S Lee",
        "S.-H Kim",
        "H.-J Yang"
      ],
      "year": "2018",
      "venue": "Proc. 2018 Int. Conf",
      "doi": "10.1145/3278312.3278313"
    },
    {
      "citation_id": "316",
      "title": "Generalized zero-shot emotion recognition from body gestures",
      "authors": [
        "J Wu",
        "Y Zhang",
        "S Sun",
        "Q Li",
        "X Zhao"
      ],
      "year": "2021",
      "venue": "Appl. Intell",
      "doi": "10.1007/s10489-021-02927-w"
    },
    {
      "citation_id": "317",
      "title": "Deep Temporal Analysis for Non-Acted Body Affect Recognition",
      "authors": [
        "D Avola",
        "L Cinque",
        "A Fagioli",
        "G Foresti",
        "C Massaroni"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.3003816"
    },
    {
      "citation_id": "318",
      "title": "Temporal Segment Networks for Action Recognition in Videos",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "Y Qiao",
        "D Lin",
        "X Tang",
        "L Gool"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2018.2868668"
    },
    {
      "citation_id": "319",
      "title": "Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Proc. Thirty-Second AAAI Conf"
    },
    {
      "citation_id": "320",
      "title": "Learning to detect unseen object classes by between-class attribute transfer",
      "authors": [
        "C Lampert",
        "H Nickisch",
        "S Harmeling"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2009.5206594"
    },
    {
      "citation_id": "321",
      "title": "Learning Unseen Emotions from Gestures via Semantically-Conditioned Zero-Shot Perception with Adversarial Autoencoders",
      "authors": [
        "A Banerjee",
        "U Bhattacharya",
        "A Bera"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "322",
      "title": "Emotion Recognition from Physiological Signal Analysis: A Review",
      "authors": [
        "M Egger",
        "M Ley",
        "S Hanke"
      ],
      "year": "2019",
      "venue": "Electron. Notes Theor. Comput. Sci",
      "doi": "10.1016/j.entcs.2019.04.009"
    },
    {
      "citation_id": "323",
      "title": "Emotion Recognition Based on Physiological Sensor Data Using Codebook Approach",
      "authors": [
        "K Shirahama",
        "M Grzegorzek"
      ],
      "year": "2016",
      "venue": "Inf. Technol. Med",
      "doi": "10.1007/978-3-319-39904-1_3"
    },
    {
      "citation_id": "324",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Lowcost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE J. Biomed. Health Inform",
      "doi": "10.1109/JBHI.2017.2688239"
    },
    {
      "citation_id": "325",
      "title": "EEG-based Emotion Recognition via Neural Architecture Search",
      "authors": [
        "C Li",
        "Z Zhang",
        "R Song",
        "J Cheng",
        "Y Liu",
        "X Chen"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2021.3130387"
    },
    {
      "citation_id": "326",
      "title": "EEG-based emotion estimation using Bayesian weighted-log-posterior function and perceptron convergence algorithm",
      "authors": [
        "H Yoon",
        "S Chung"
      ],
      "year": "2013",
      "venue": "Comput. Biol. Med",
      "doi": "10.1016/j.compbiomed.2013.10.017"
    },
    {
      "citation_id": "327",
      "title": "Cross-Subject EEG Feature Selection for Emotion Recognition Using Transfer Recursive Feature Elimination",
      "authors": [
        "Z Yin",
        "Y Wang",
        "L Liu",
        "W Zhang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Front. Neurorobotics",
      "doi": "10.3389/fnbot.2017.00019"
    },
    {
      "citation_id": "328",
      "title": "Dynamical recursive feature elimination technique for neurophysiological signal-based emotion recognition",
      "authors": [
        "Z Yin",
        "L Liu",
        "L Liu",
        "J Zhang",
        "Y Wang"
      ],
      "year": "2017",
      "venue": "Cogn. Technol. Work",
      "doi": "10.1007/s10111-017-0450-2"
    },
    {
      "citation_id": "329",
      "title": "Emotion Recognition and Analysis Using ADMM-Based Sparse Group Lasso",
      "authors": [
        "K Puk",
        "S Wang",
        "J Rosenberger",
        "K Gandy",
        "H Harris",
        "Y Peng",
        "A Nordberg",
        "P Lehmann",
        "J Tommerdahl",
        "J.-C Chiao"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2019.2943551"
    },
    {
      "citation_id": "330",
      "title": "Strengthen EEG-based emotion recognition using firefly integrated optimization algorithm",
      "authors": [
        "H He",
        "Y Tan",
        "J Ying",
        "W Zhang"
      ],
      "year": "2020",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/j.asoc.2020.106426"
    },
    {
      "citation_id": "331",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "J Atkinson",
        "D Campos"
      ],
      "year": "2016",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2015.10.049"
    },
    {
      "citation_id": "332",
      "title": "Deep learninig of EEG signals for emotion recognition",
      "authors": [
        "Y Gao",
        "H Lee",
        "R Mehmood"
      ],
      "year": "2015",
      "venue": "2015 IEEE Int. Conf. Multimed. Expo Workshop ICMEW",
      "doi": "10.1109/ICMEW.2015.7169796"
    },
    {
      "citation_id": "333",
      "title": "A Novel Neural Network Model based on Cerebral Hemispheric Asymmetry for EEG Emotion Recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "Proc. Twenty-Seventh Int. Jt. Conf",
      "doi": "10.24963/ijcai.2018/216"
    },
    {
      "citation_id": "334",
      "title": "Instance-Adaptive Graph for EEG Emotion Recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Proc. AAAI Conf",
      "doi": "10.1609/aaai.v34i03.5656"
    },
    {
      "citation_id": "335",
      "title": "Variational Pathway Reasoning for EEG Emotion Recognition",
      "authors": [
        "T Zhang",
        "Z Cui",
        "C Xu",
        "W Zheng",
        "J Yang"
      ],
      "year": "2020",
      "venue": "Proc. AAAI Conf",
      "doi": "10.1609/aaai.v34i03.5657"
    },
    {
      "citation_id": "336",
      "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2020.2994159"
    },
    {
      "citation_id": "337",
      "title": "A Channel-fused Dense Convolutional Network for EEG-based Emotion Recognition",
      "authors": [
        "Z Gao",
        "X Wang",
        "Y Yang",
        "Y Li",
        "K Ma",
        "G Chen"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Cogn. Dev. Syst",
      "doi": "10.1109/TCDS.2020.2976112"
    },
    {
      "citation_id": "338",
      "title": "EEG-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowl.-Based Syst",
      "doi": "10.1016/j.knosys.2020.106243"
    },
    {
      "citation_id": "339",
      "title": "Automatic ECG-Based Emotion Recognition in Music Listening",
      "authors": [
        "Y.-L Hsu",
        "J.-S Wang",
        "W.-C Chiang",
        "C.-H Hung"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2017.2781732"
    },
    {
      "citation_id": "340",
      "title": "Analysis of Electrocardiogram (ECG) Signals for Human Emotional Stress Classification",
      "authors": [
        "S Bong",
        "M Murugappan",
        "S Yaacob"
      ],
      "year": "2012",
      "venue": "Trends Intell. Robot. Autom. Manuf",
      "doi": "10.1007/978-3-642-35197-6_22"
    },
    {
      "citation_id": "341",
      "title": "Emotion recognition from electrocardiogram signals using Hilbert Huang Transform",
      "authors": [
        "S Jerritta",
        "M Murugappan",
        "K Wan",
        "S Yaacob"
      ],
      "year": "2012",
      "venue": "IEEE Conf. Sustain. Util. Dev. Eng. Technol. Stud",
      "doi": "10.1109/STUDENT.2012.6408370"
    },
    {
      "citation_id": "342",
      "title": "A novel ECG-based real-time detection method of negative emotions in wearable applications",
      "authors": [
        "Z Cheng",
        "L Shu",
        "J Xie",
        "C Chen"
      ],
      "year": "2017",
      "venue": "Int. Conf. Secur. Pattern Anal. Cybern. SPAC",
      "doi": "10.1109/SPAC.2017.8304293"
    },
    {
      "citation_id": "343",
      "title": "BioVid Emo DB\": A multimodal database for emotion analyses validated by subjective ratings",
      "authors": [
        "L Zhang",
        "S Walter",
        "X Ma",
        "P Werner",
        "A Al-Hamadi",
        "H Traue",
        "S Gruss"
      ],
      "year": "2016",
      "venue": "IEEE Symp. Ser. Comput. Intell. SSCI",
      "doi": "10.1109/SSCI.2016.7849931"
    },
    {
      "citation_id": "344",
      "title": "Classification of emotional states from electrocardiogram signals: a non-linear approach based on hurst",
      "authors": [
        "J Selvaraj",
        "M Murugappan",
        "K Wan",
        "S Yaacob"
      ],
      "year": "2013",
      "venue": "Biomed. Eng. OnLine",
      "doi": "10.1186/1475-925X-12-44"
    },
    {
      "citation_id": "345",
      "title": "Enhancing Emotion Recognition from ECG Signals using Supervised Dimensionality Reduction",
      "authors": [
        "H Ferdinando",
        "T Seppä Nen",
        "E Alasaarela"
      ],
      "year": "2017",
      "venue": "Proc. 6th Int. Conf. Pattern Recognit. Appl. Methods, SCITEPRESS -Science and Technology Publications",
      "doi": "10.5220/0006147801120118"
    },
    {
      "citation_id": "346",
      "title": "Generating ECG to Enhance Emotion State Classification",
      "authors": [
        "G Chen",
        "Y Zhu",
        "Z Hong",
        "Z Yang"
      ],
      "year": "2019",
      "venue": "Proc. 2019 Int. Conf",
      "doi": "10.1145/3349341.3349422"
    },
    {
      "citation_id": "347",
      "title": "Self-Supervised Learning for ECG-Based Emotion Recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP40776.2020.9053985"
    },
    {
      "citation_id": "348",
      "title": "Multimodal emotion recognition from expressive faces, body gestures and speech",
      "authors": [
        "G Caridakis",
        "G Castellano",
        "L Kessous",
        "A Raouzaiou",
        "L Malatesta",
        "S Asteriadis",
        "K Karpouzis"
      ],
      "year": "2007",
      "venue": "Artif. Intell. Innov"
    },
    {
      "citation_id": "349",
      "title": "",
      "authors": [
        "Theory Appl",
        "U Springer",
        "M Boston"
      ],
      "year": "2007",
      "venue": "",
      "doi": "10.1007/978-0-387-74161-1_41"
    },
    {
      "citation_id": "350",
      "title": "Feature Analysis for Computational Personality Recognition Using YouTube Personality Data set",
      "authors": [
        "C Sarkar",
        "S Bhatia",
        "A Agarwal",
        "J Li"
      ],
      "year": "2014",
      "venue": "Proc. 2014 ACM Multi Media Workshop Comput",
      "doi": "10.1145/2659522.2659528"
    },
    {
      "citation_id": "351",
      "title": "Multimodal fusion framework: A multiresolution approach for emotion classification and recognition from physiological signals",
      "authors": [
        "G Verma",
        "U Tiwary"
      ],
      "year": "2014",
      "venue": "NeuroImage",
      "doi": "10.1016/j.neuroimage.2013.11.007"
    },
    {
      "citation_id": "352",
      "title": "Emotion Recognition From Multimodal Physiological Signals Using a Regularized Deep Fusion of Kernel Machine",
      "authors": [
        "X Zhang",
        "J Liu",
        "J Shen",
        "S Li",
        "K Hou",
        "B Hu",
        "J Gao",
        "T Zhang",
        "B Hu"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Cybern",
      "doi": "10.1109/TCYB.2020.2987575"
    },
    {
      "citation_id": "353",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2019",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2018.09.008"
    },
    {
      "citation_id": "354",
      "title": "Emotion Recognition Based on Joint Visual and Audio Cues",
      "authors": [
        "N Sebe",
        "I Cohen",
        "T Gevers",
        "T Huang"
      ],
      "year": "2006",
      "venue": "18th Int. Conf. Pattern Recognit. ICPR06",
      "doi": "10.1109/ICPR.2006.489"
    },
    {
      "citation_id": "355",
      "title": "Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2004",
      "venue": "2004 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2004.1326051"
    },
    {
      "citation_id": "356",
      "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "authors": [
        "J Sebastian",
        "P Pierucci"
      ],
      "year": "2019",
      "venue": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "doi": "10.21437/Interspeech.2019-3201"
    },
    {
      "citation_id": "357",
      "title": "Towards an intelligent framework for multimodal affective data analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "A Hussain",
        "G.-B Huang"
      ],
      "year": "2015",
      "venue": "Neural Netw",
      "doi": "10.1016/j.neunet.2014.10.005"
    },
    {
      "citation_id": "358",
      "title": "Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "IEEE 16th Int. Conf. Data Min. ICDM",
      "doi": "10.1109/ICDM.2016.0055"
    },
    {
      "citation_id": "359",
      "title": "Noise-Resilient Training Method for Face Landmark Generation From Speech",
      "authors": [
        "S Eskimez",
        "R Maddox",
        "C Xu",
        "Z Duan"
      ],
      "year": "2020",
      "venue": "IEEEACM Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASLP.2019.2947741"
    },
    {
      "citation_id": "360",
      "title": "Audio-visual based emotion recognition-a new approach",
      "authors": [
        "Mingli Song",
        "Jiajun Bu",
        "Chun Chen",
        "Nan Li"
      ],
      "year": "2004",
      "venue": "Proc. 2004 IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2004.1315276"
    },
    {
      "citation_id": "361",
      "title": "A joint particle filter for audio-visual speaker tracking",
      "authors": [
        "K Nickel",
        "T Gehrig",
        "R Stiefelhagen",
        "J Mcdonough"
      ],
      "year": "2005",
      "venue": "Proc. 7th Int. Conf. Multimodal Interfaces -ICMI 05",
      "doi": "10.1145/1088463.1088477"
    },
    {
      "citation_id": "362",
      "title": "Training combination strategy of multi-stream fused hidden Markov model for audiovisual affect recognition",
      "authors": [
        "Z Zeng",
        "Y Hu",
        "M Liu",
        "Y Fu",
        "T Huang"
      ],
      "year": "2006",
      "venue": "Proc. 14th Annu. ACM Int. Conf. Multimed. -Multimed. 06",
      "doi": "10.1145/1180639.1180661"
    },
    {
      "citation_id": "363",
      "title": "Modeling naturalistic affective states via facial and vocal expressions recognition",
      "authors": [
        "G Caridakis",
        "L Malatesta",
        "L Kessous",
        "N Amir",
        "A Raouzaiou",
        "K Karpouzis"
      ],
      "year": "2006",
      "venue": "Proc. 8th Int. Conf. Multimodal Interfaces -ICMI 06",
      "doi": "10.1145/1180995.1181029"
    },
    {
      "citation_id": "364",
      "title": "Facial Expression Recognition in Video with Multiple Feature Fusion",
      "authors": [
        "J Chen",
        "Z Chen",
        "Z Chi",
        "H Fu"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2016.2593719"
    },
    {
      "citation_id": "365",
      "title": "Attention Driven Fusion for Multi-Modal Emotion Recognition",
      "authors": [
        "D Priyasad",
        "T Fernando",
        "S Denman",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP40776.2020.9054441"
    },
    {
      "citation_id": "366",
      "title": "Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues",
      "authors": [
        "T Mittal",
        "U Bhattacharya",
        "R Chandra",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proc. AAAI Conf",
      "doi": "10.1609/aaai.v34i02.5492"
    },
    {
      "citation_id": "367",
      "title": "An Attribute-invariant Variational Learning for Emotion Recognition Using Physiology",
      "authors": [
        "H.-C Yang",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2019.8683290"
    },
    {
      "citation_id": "368",
      "title": "An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "Proc. AAAI Conf",
      "doi": "10.1609/aaai.v34i01.5364"
    },
    {
      "citation_id": "369",
      "title": "Deep Fusion: An Attention Guided Factorized Bilinear Pooling for Audio-video Emotion Recognition",
      "authors": [
        "Y Zhang",
        "Z.-R Wang",
        "J Du"
      ],
      "year": "2019",
      "venue": "Int. Jt. Conf. Neural Netw",
      "doi": "10.1109/IJCNN.2019.8851942"
    },
    {
      "citation_id": "370",
      "title": "Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2018",
      "venue": "2018 IEEECVF Conf. Comput. Vis. Pattern Recognit",
      "doi": "10.1109/CVPR.2018.00685"
    },
    {
      "citation_id": "371",
      "title": "Visual-audio emotion recognition based on multi-task and ensemble learning with multiple features",
      "authors": [
        "M Hao",
        "W.-H Cao",
        "Z.-T Liu",
        "M Wu",
        "P Xiao"
      ],
      "year": "2020",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2020.01.048"
    },
    {
      "citation_id": "372",
      "title": "The eNTERFACE'05 Audio-Visual Emotion Database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proc. 22nd Int. Conf. Data Eng. Workshop",
      "doi": "10.1109/ICDEW.2006.145"
    },
    {
      "citation_id": "373",
      "title": "Kalman Filter Based Classifier Fusion for Affective State Recognition",
      "authors": [
        "M Glodek",
        "S Reuter",
        "M Schels",
        "K Dietmayer",
        "F Schwenker"
      ],
      "year": "2013",
      "venue": "Mult. Classif. Syst",
      "doi": "10.1007/978-3-642-38067-9_8"
    },
    {
      "citation_id": "374",
      "title": "MTNA: A Neural Multi-task Model for Aspect Category Classification and Aspect Term Extraction On Restaurant Reviews",
      "authors": [
        "W Xue",
        "W Zhou",
        "T Li",
        "Q Wang"
      ],
      "year": "2017",
      "venue": "Asian Federation of Natural Language Processing"
    },
    {
      "citation_id": "375",
      "title": "Exploiting Acoustic and Lexical Properties of Phonemes to Recognize Valence from Speech",
      "authors": [
        "B Zhang",
        "S Khorram",
        "E Provost"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2019.8683190"
    },
    {
      "citation_id": "376",
      "title": "Multimodal Speech Emotion Recognition Using Audio and Text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spok. Lang. Technol. Workshop SLT",
      "doi": "10.1109/SLT.2018.8639583"
    },
    {
      "citation_id": "377",
      "title": "Audio-Textual Emotion Recognition Based on Improved Neural Networks",
      "authors": [
        "L Cai",
        "Y Hu",
        "J Dong",
        "S Zhou"
      ],
      "year": "2019",
      "venue": "Math. Probl. Eng",
      "doi": "10.1155/2019/2593036"
    },
    {
      "citation_id": "378",
      "title": "Emotion Recognition of Affective Speech Based on Multiple Classifiers Using Acoustic-Prosodic Information and Semantic Labels",
      "authors": [
        "C.-H Wu",
        "W.-B Liang"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2010.16"
    },
    {
      "citation_id": "379",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2015.7178872"
    },
    {
      "citation_id": "380",
      "title": "Fusion Approaches for Emotion Recognition from Speech Using Acoustic and Text-Based Features, in: ICASSP 2020 -2020 IEEE Int. Conf. Acoust. Speech Signal Process",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer",
        "A Gravano"
      ],
      "year": "2020",
      "venue": "Fusion Approaches for Emotion Recognition from Speech Using Acoustic and Text-Based Features, in: ICASSP 2020 -2020 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP40776.2020.9054709"
    },
    {
      "citation_id": "381",
      "title": "Context-Sensitive Learning for Enhanced Audiovisual Emotion Classification",
      "authors": [
        "A Metallinou",
        "M Wollmer",
        "A Katsamanis",
        "F Eyben",
        "B Schuller",
        "S Narayanan"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2011.40"
    },
    {
      "citation_id": "382",
      "title": "Benchmarking Multimodal Sentiment Analysis",
      "authors": [
        "E Cambria",
        "D Hazarika",
        "S Poria",
        "A Hussain",
        "R Subramanyam"
      ],
      "year": "2018",
      "venue": "Benchmarking Multimodal Sentiment Analysis",
      "doi": "10.1007/978-3-319-77116-8_13"
    },
    {
      "citation_id": "383",
      "title": "Multimodal Sentiment Analysis of Spanish Online Videos",
      "authors": [
        "V Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intell. Syst",
      "doi": "10.1109/MIS.2013.9"
    },
    {
      "citation_id": "384",
      "title": "Topic-Segmentation of Dialogue",
      "authors": [
        "J Arguello",
        "C Rosé"
      ],
      "year": "2006",
      "venue": "Proc. Anal. Conversat. Text Speech"
    },
    {
      "citation_id": "385",
      "title": "Phonetic-enriched text representation for Chinese sentiment analysis with reinforcement learning",
      "authors": [
        "H Peng",
        "Y Ma",
        "S Poria",
        "Y Li",
        "E Cambria"
      ],
      "year": "2021",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2021.01.005"
    },
    {
      "citation_id": "386",
      "title": "Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterancelevel Multimodal Sentiment Analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "A Gelbukh"
      ],
      "year": "2015",
      "venue": "Proc. 2015 Conf. Empir. Methods Nat. Lang. Process",
      "doi": "10.18653/v1/D15-1303"
    },
    {
      "citation_id": "387",
      "title": "Context-Dependent Sentiment Analysis in User-Generated Videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proc. 55th Annu",
      "doi": "10.18653/v1/P17-1081"
    },
    {
      "citation_id": "388",
      "title": "Wearable-Based Affect Recognition-A Review",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Dürichen",
        "K Van Laerhoven"
      ],
      "year": "2019",
      "venue": "Sensors",
      "doi": "10.3390/s19194079"
    },
    {
      "citation_id": "389",
      "title": "Analysis of physiological for emotion recognition with the IRS model",
      "authors": [
        "C Li",
        "C Xu",
        "Z Feng"
      ],
      "year": "2016",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2015.07.112"
    },
    {
      "citation_id": "390",
      "title": "Long Short Term Memory Hyperparameter Optimization for a Neural Network Based Emotion Recognition Framework",
      "authors": [
        "B Nakisa",
        "M Rastgoo",
        "A Rakotonirainy",
        "F Maire",
        "V Chandran"
      ],
      "year": "2018",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2018.2868361"
    },
    {
      "citation_id": "391",
      "title": "Human emotion recognition using deep belief network architecture",
      "authors": [
        "M Hassan",
        "Md Alam",
        "Md Uddin",
        "S Huda",
        "A Almogren",
        "G Fortino"
      ],
      "year": "2019",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2018.10.009"
    },
    {
      "citation_id": "392",
      "title": "Emotion Recognition using Multimodal Residual LSTM Network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proc. 27th ACM Int. Conf. Multimed., ACM, Nice France",
      "doi": "10.1145/3343031.3350871"
    },
    {
      "citation_id": "393",
      "title": "Emotion Recognition Based on Weighted Fusion Strategy of Multichannel Physiological Signals",
      "authors": [
        "W Wei",
        "Q Jia",
        "Y Feng",
        "G Chen"
      ],
      "year": "2018",
      "venue": "Comput. Intell. Neurosci",
      "doi": "10.1155/2018/5296523"
    },
    {
      "citation_id": "394",
      "title": "Exploring temporal representations by leveraging attention-based bidirectional LSTM-RNNs for multimodal emotion recognition",
      "authors": [
        "C Li",
        "Z Bao",
        "L Li",
        "Z Zhao"
      ],
      "year": "2020",
      "venue": "Inf. Process. Manag",
      "doi": "10.1016/j.ipm.2019.102185"
    },
    {
      "citation_id": "395",
      "title": "CNN and LSTM-Based Emotion Charting Using Physiological Signals",
      "authors": [
        "M Dar",
        "M Akram",
        "S Khawaja",
        "A Pujari"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20164551"
    },
    {
      "citation_id": "396",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Z Yin",
        "M Zhao",
        "Y Wang",
        "J Yang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Comput. Methods Programs Biomed",
      "doi": "10.1016/j.cmpb.2016.12.005"
    },
    {
      "citation_id": "397",
      "title": "Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks",
      "authors": [
        "L He",
        "D Jiang",
        "L Yang",
        "E Pei",
        "P Wu",
        "H Sahli"
      ],
      "year": "2015",
      "venue": "Proc. 5th Int",
      "doi": "10.1145/2808196.2811641"
    },
    {
      "citation_id": "398",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "H Ranganathan",
        "S Chakraborty",
        "S Panchanathan"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conf. Appl. Comput. Vis. WACV",
      "doi": "10.1109/WACV.2016.7477679"
    },
    {
      "citation_id": "399",
      "title": "Combining Eye Movements and EEG to Enhance Emotion Recognition",
      "authors": [
        "Y Lu",
        "W.-L Zheng",
        "B Li",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "Proc. Twenty-Fourth Int. Jt. Conf. Artif. Intell. IJCAI"
    },
    {
      "citation_id": "400",
      "title": "Exploiting EEG Signals and Audiovisual Feature Fusion for Video Emotion Recognition",
      "authors": [
        "B Xing",
        "H Zhang",
        "K Zhang",
        "L Zhang",
        "X Wu",
        "X Shi",
        "S Yu",
        "S Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2914872"
    },
    {
      "citation_id": "401",
      "title": "A Efficient Multimodal Framework for Large Scale Emotion Recognition by Fusing Music and Electrodermal Activity Signals",
      "authors": [
        "G Yin",
        "S Sun",
        "D Yu",
        "D Li",
        "K Zhang"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "402",
      "title": "Emotion Recognition Using Multimodal Deep Learning",
      "authors": [
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proc. 23rd Int. Conf. Neural Inf. Process",
      "doi": "10.1007/978-3-319-46672-9_58"
    },
    {
      "citation_id": "403",
      "title": "Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2015.2436926"
    },
    {
      "citation_id": "404",
      "title": "Multimodal Fused Emotion Recognition About Expression-EEG Interaction and Collaboration Using Deep Learning",
      "authors": [
        "D Wu",
        "J Zhang",
        "Q Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.3010311"
    },
    {
      "citation_id": "405",
      "title": "The PMEmo Dataset for Music Emotion Recognition",
      "authors": [
        "K Zhang",
        "H Zhang",
        "S Li",
        "C Yang",
        "L Sun"
      ],
      "year": "2018",
      "venue": "Proc. 2018 ACM Int. Conf. Multimed. Retr",
      "doi": "10.1145/3206025.3206037"
    },
    {
      "citation_id": "406",
      "title": "Feature distillation network for aspect-based sentiment analysis",
      "authors": [
        "K Shuang",
        "Q Yang",
        "J Loo",
        "R Li",
        "M Gu"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.03.003"
    },
    {
      "citation_id": "407",
      "title": "Facial expression recognition in dynamic sequences: An integrated approach",
      "authors": [
        "H Fang",
        "N Parthalá In",
        "A Aubrey",
        "G Tam",
        "R Borgo",
        "P Rosin",
        "P Grant",
        "D Marshall",
        "M Chen"
      ],
      "year": "2014",
      "venue": "Pattern Recognit",
      "doi": "10.1016/j.patcog.2013.09.023"
    },
    {
      "citation_id": "408",
      "title": "Electroencephalogram-based emotion assessment system using ontology and data mining techniques",
      "authors": [
        "J Chen",
        "B Hu",
        "P Moore",
        "X Zhang",
        "X Ma"
      ],
      "year": "2015",
      "venue": "Electroencephalogram-based emotion assessment system using ontology and data mining techniques",
      "doi": "10.1016/j.asoc.2015.01.007"
    },
    {
      "citation_id": "409",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. 57th Annu",
      "doi": "10.18653/v1/P19-1050"
    },
    {
      "citation_id": "410",
      "title": "Multimodal Video Sentiment Analysis Using Deep Learning Approaches, a Survey",
      "authors": [
        "S Abdu",
        "A Yousef",
        "A Salem"
      ],
      "year": "2021",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2021.06.003"
    },
    {
      "citation_id": "411",
      "title": "Audiovisual Discrimination Between Speech and Laughter: Why and When Visual Information Might Help",
      "authors": [
        "S Petridis",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Multimed",
      "doi": "10.1109/TMM.2010.2101586"
    },
    {
      "citation_id": "412",
      "title": "Selective deep features for micro-expression recognition",
      "authors": [
        "Devangini Patel",
        "X Hong",
        "G Zhao"
      ],
      "year": "2016",
      "venue": "rd Int. Conf. Pattern Recognit. ICPR",
      "doi": "10.1109/ICPR.2016.7899972"
    },
    {
      "citation_id": "413",
      "title": "Spontaneous Subtle Expression Recognition: Imbalanced Databases and Solutions",
      "authors": [
        "A Ngo",
        "C Phan",
        "J See"
      ],
      "year": "2014",
      "venue": "Asian Conf. Comput. Vis",
      "doi": "10.1007/978-3-319-16817-3_3"
    },
    {
      "citation_id": "414",
      "title": "Macro-to-micro transformation model for micro-expression recognition",
      "authors": [
        "X Jia",
        "X Ben",
        "H Yuan",
        "K Kpalma",
        "W Meng"
      ],
      "year": "2018",
      "venue": "J. Comput. Sci",
      "doi": "10.1016/j.jocs.2017.03.016"
    },
    {
      "citation_id": "415",
      "title": "Weakly Supervised Local-Global Relation Network for Facial Expression Recognition",
      "authors": [
        "H Zhang",
        "W Su",
        "J Yu",
        "Z Wang"
      ],
      "year": "2020",
      "venue": "Proc. Twenty-Ninth Int",
      "doi": "10.24963/ijcai.2020/145"
    },
    {
      "citation_id": "416",
      "title": "The First International Audio/Visual Emotion Challenge",
      "authors": [
        "B Schuller",
        "M Valstar",
        "F Eyben",
        "G Mckeown",
        "R Cowie",
        "M Pantic",
        "Avec"
      ],
      "year": "2011",
      "venue": "The First International Audio/Visual Emotion Challenge",
      "doi": "10.1007/978-3-642-24571-8_53"
    },
    {
      "citation_id": "417",
      "title": "AVEC 2018 Workshop and Challenge: Bipolar Disorder and Cross-Cultural Affect Recognition",
      "authors": [
        "F Ringeval",
        "A Michaud",
        "E Ciftç I",
        "H Güleç",
        "A Salah",
        "M Pantic",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne"
      ],
      "year": "2018",
      "venue": "AVEC 2018 Workshop and Challenge: Bipolar Disorder and Cross-Cultural Affect Recognition",
      "doi": "10.1145/3266302.3266316"
    },
    {
      "citation_id": "418",
      "title": "Deep neural network and switching Kalman filter based continuous affect recognition",
      "authors": [
        "Ercheng Pei",
        "Xiaohan Xia",
        "Le Yang",
        "Dongmei Jiang",
        "H Sahli"
      ],
      "year": "2016",
      "venue": "2016 IEEE Int. Conf. Multimed. Expo Workshop ICMEW",
      "doi": "10.1109/ICMEW.2016.7574729"
    },
    {
      "citation_id": "419",
      "title": "Learning a sparse codebook of facial and body microexpressions for emotion recognition",
      "authors": [
        "Y Song",
        "L.-P Morency",
        "R Davis"
      ],
      "year": "2013",
      "venue": "Proc. 15th ACM Int. Conf. Multimodal Interact",
      "doi": "10.1145/2522848.2522851"
    },
    {
      "citation_id": "420",
      "title": "AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "AVEC 2016: Depression, Mood, and Emotion Recognition Workshop and Challenge",
      "doi": "10.1145/2988257.2988258"
    },
    {
      "citation_id": "421",
      "title": "A Quantum-Like multimodal network framework for modeling interaction dynamics in multiparty conversational sentiment analysis",
      "authors": [
        "Y Zhang",
        "D Song",
        "X Li",
        "P Zhang",
        "P Wang",
        "L Rong",
        "G Yu",
        "B Wang"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.04.003"
    },
    {
      "citation_id": "422",
      "title": "End-to-end multimodal affect recognition in real-world environments",
      "authors": [
        "P Tzirakis",
        "J Chen",
        "S Zafeiriou",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.10.011"
    },
    {
      "citation_id": "423",
      "title": "New trends and applications in social media analytics",
      "authors": [
        "D Camacho",
        "M Luzón",
        "E Cambria"
      ],
      "year": "2021",
      "venue": "New trends and applications in social media analytics",
      "doi": "10.1016/j.future.2020.08.007"
    },
    {
      "citation_id": "424",
      "title": "SenticNet 2: A Semantic and Affective Resource for Opinion Mining and Sentiment Analysis",
      "authors": [
        "E Cambria",
        "C Havasi",
        "A Hussain"
      ],
      "year": "2012",
      "venue": "Artif. Intell. Res. Soc. Conf"
    },
    {
      "citation_id": "425",
      "title": "SenticNet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis",
      "authors": [
        "E Cambria",
        "D Olsher"
      ],
      "year": "2014",
      "venue": "Proc. Twenty-Eighth AAAI Conf"
    },
    {
      "citation_id": "426",
      "title": "SenticNet 4: A Semantic Resource for Sentiment Analysis Based on Conceptual Primitives",
      "authors": [
        "E Cambria",
        "S Poria",
        "R Bajpai",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "SenticNet 4: A Semantic Resource for Sentiment Analysis Based on Conceptual Primitives"
    },
    {
      "citation_id": "427",
      "title": "Discovering Conceptual Primitives for Sentiment Analysis by Means of Context Embeddings",
      "authors": [
        "E Cambria",
        "S Poria",
        "D Hazarika",
        "K Kwok"
      ],
      "year": "2018",
      "venue": "Proc. Twenty-Eighth AAAI Conf"
    },
    {
      "citation_id": "428",
      "title": "SenticNet 6: Ensemble Application of Symbolic and Subsymbolic AI for Sentiment Analysis",
      "authors": [
        "E Cambria",
        "Y Li",
        "F Xing",
        "S Poria",
        "K Kwok"
      ],
      "year": "2020",
      "venue": "Proc. 29th ACM Int. Conf. Inf. Knowl. Manag., ACM, Virtual Event Ireland",
      "doi": "10.1145/3340531.3412003"
    },
    {
      "citation_id": "429",
      "title": "Targeted Aspect-Based Sentiment Analysis via Embedding Commonsense Knowledge into an Attentive LSTM",
      "authors": [
        "Y Ma",
        "H Peng",
        "E Cambria"
      ],
      "year": "2018",
      "venue": "Proc. AAAI Conf"
    },
    {
      "citation_id": "430",
      "title": "Intelligent Asset Allocation via Market Sentiment Views",
      "authors": [
        "F Xing",
        "E Cambria",
        "R Welsch"
      ],
      "year": "2018",
      "venue": "IEEE Comput. Intell. Mag",
      "doi": "10.1109/MCI.2018.2866727"
    },
    {
      "citation_id": "431",
      "title": "Technical analysis and sentiment embeddings for market trend prediction",
      "authors": [
        "A Picasso",
        "S Merello",
        "Y Ma",
        "L Oneto",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2019.06.014"
    },
    {
      "citation_id": "432",
      "title": "EARS: Emotion-aware recommender system based on hybrid information fusion",
      "authors": [
        "Y Qian",
        "Y Zhang",
        "X Ma",
        "H Yu",
        "L Peng"
      ],
      "year": "2019",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2018.06.004"
    },
    {
      "citation_id": "433",
      "title": "Deep learning based emotion analysis of microblog texts",
      "authors": [
        "D Xu",
        "Z Tian",
        "R Lai",
        "X Kong",
        "Z Tan",
        "W Shi"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.06.002"
    },
    {
      "citation_id": "434",
      "title": "An Emotion Recognition Model Based on Facial Recognition in Virtual Learning Environment",
      "authors": [
        "D Yang",
        "A Alsadoon",
        "P Prasad",
        "A Singh",
        "A Elchouemi"
      ],
      "year": "2018",
      "venue": "Procedia Comput. Sci",
      "doi": "10.1016/j.procs.2017.12.003"
    },
    {
      "citation_id": "435",
      "title": "Sentiment Analysis based Multi-Person Multi-criteria Decision Making methodology using natural language processing and deep learning for smarter decision aid. Case study of restaurant choice using TripAdvisor reviews",
      "authors": [
        "C Zuheros",
        "E Martí Nez-Cá Mara",
        "E Herrera-Viedma",
        "F Herrera"
      ],
      "year": "2021",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.10.019"
    },
    {
      "citation_id": "436",
      "title": "Facial Expression Analysis under Partial Occlusion: A Survey",
      "authors": [
        "L Zhang",
        "B Verma",
        "D Tjondronegoro",
        "V Chandran"
      ],
      "year": "2018",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3158369"
    },
    {
      "citation_id": "437",
      "title": "Continuous Recognition of Player's Affective Body Expression as Dynamic Quality of Aesthetic Experience",
      "authors": [
        "N Savva",
        "A Scarinzi",
        "N Bianchi-Berthouze"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Comput. Intell. AI Games",
      "doi": "10.1109/TCIAIG.2012.2202663"
    },
    {
      "citation_id": "438",
      "title": "Body Motion Analysis for Emotion Recognition in Serious Games",
      "authors": [
        "K Kaza",
        "A Psaltis",
        "K Stefanidis",
        "K Apostolakis",
        "S Thermos",
        "K Dimitropoulos",
        "P Daras"
      ],
      "year": "2016",
      "venue": "Univers. Access Hum.-Comput. Interact. Interact. Tech. Environ",
      "doi": "10.1007/978-3-319-40244-4_4"
    },
    {
      "citation_id": "439",
      "title": "ANFIS fusion algorithm for eye movement recognition via soft multi-functional electronic skin",
      "authors": [
        "W Dong",
        "L Yang",
        "R Gravina",
        "G Fortino"
      ],
      "year": "2021",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2021.02.003"
    },
    {
      "citation_id": "440",
      "title": "Feature-level fusion approaches based on multimodal EEG data for depression recognition",
      "authors": [
        "H Cai",
        "Z Qu",
        "Z Li",
        "Y Zhang",
        "X Hu",
        "B Hu"
      ],
      "year": "2020",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2020.01.008"
    },
    {
      "citation_id": "441",
      "title": "A set of Full-Body Movement Features for Emotion Recognition to Help Children affected by Autism Spectrum Condition",
      "authors": [
        "S Piana",
        "A Staglianò",
        "A Camurri"
      ],
      "year": "2013",
      "venue": "A set of Full-Body Movement Features for Emotion Recognition to Help Children affected by Autism Spectrum Condition"
    },
    {
      "citation_id": "442",
      "title": "A module-based framework to emotion recognition by speech: a case study in clinical simulation",
      "authors": [
        "L Sawada",
        "L Mano",
        "J Torres Neto",
        "J Ueyama"
      ],
      "year": "2019",
      "venue": "J. Ambient Intell. Humaniz. Comput",
      "doi": "10.1007/s12652-019-01280-8"
    },
    {
      "citation_id": "443",
      "title": "Deep learning analysis of mobile physiological, environmental and location sensor data for emotion detection",
      "authors": [
        "E Kanjo",
        "E Younis",
        "C Ang"
      ],
      "year": "2019",
      "venue": "Inf. Fusion",
      "doi": "10.1016/j.inffus.2018.09.001"
    },
    {
      "citation_id": "444",
      "title": "Multi-modal emotion recognition using semi-supervised learning and multiple neural networks in the wild",
      "authors": [
        "D Kim",
        "M Lee",
        "D Choi",
        "B Song"
      ],
      "year": "2017",
      "venue": "Proc. 19th ACM Int. Conf. Multimodal Interact",
      "doi": "10.1145/3136755.3143005"
    },
    {
      "citation_id": "445",
      "title": "Multi-Sensory Perception System for Multi-Modal Child Interaction with Multiple Robots",
      "authors": [
        "A Tsiami",
        "P Koutras",
        "N Efthymiou",
        "P Filntisis",
        "G Potamianos",
        "P Maragos",
        "Multi"
      ],
      "year": "2018",
      "venue": "IEEE Int. Conf. Robot. Autom. ICRA",
      "doi": "10.1109/ICRA.2018.8461210"
    }
  ]
}