{
  "paper_id": "2409.10687v3",
  "title": "Personalized Speech Emotion Recognition In Human-Robot Interaction Using Vision Transformers",
  "published": "2024-09-16T19:34:34Z",
  "authors": [
    "Ruchik Mishra",
    "Andrew Frye",
    "Madan Mohan Rayguru",
    "Dan O. Popa"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Vision Transformers",
    "Human-Robot Interaction"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions are an essential element in human verbal communication, therefore it is important to understand individuals' affect during human-robot interaction (HRI). This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (Bidirectional Encoder Representations from Pre-Training of Image Transformers) pipelines for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from several human subjects having pseudo-naturalistic conversations with the NAO social robot. We then fine-tuned our ViT and BEiTbased models and tested these models on unseen speech samples from the participants in order to dentify four primary emotions from speech: neutral, happy, sad, and angry. The results show that fine-tuning vision transformers on benchmark datasets and then using either these already fine-tuned models or ensembling ViT/BEiT models results in higher classification accuracies than fine-tuning vanilla-ViTs or BEiTs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "T HE increasing integration of social robots across various sectors, from healthcare to customer service, underscores their potential to revolutionize human-machine interaction  [1] -  [4] . A crucial factor in their application success is the ability to perceive and respond appropriately to human emotions, facilitating meaningful and engaging interactions  [2] ,  [5] -  [7] . In this context, Speech Emotion Recognition (SER) emerges as a critical field within human-computer interaction  [8] . By enabling machines to understand and respond to the emotional nuances embedded in human speech (affective speech), SER can transform our interactions with technology, fostering more natural and empathetic communication  [8] . When social robots can accurately interpret affective speech, they can adapt their behavior and responses, leading to more personalized and impactful human interactions  [2] . This emotional connection ability holds tremendous potential for enhancing the effectiveness and acceptance of social robots in various real-world applications. The importance of affective speech in human-robot interaction (HRI) lies in its ability to enhance the robot's social intelligence and facilitate natural communication  [8] ,  [9] . Emotions play a fundamental role in human interactions. By understanding and responding to affective cues, robots can build trust, rapport, and cooperation with their human counterparts  [10] . Affective speech recognition capability enables social robots to accurately perceive the emotional state of the user, allowing them to tailor their responses and provide appropriate support or feedback  [11] . The area of Speech Emotion Recognition (SER) has witnessed significant advancements over time, driven by the exploration of diverse feature extraction methods and suitable machine learning techniques. Early research focused on traditional approaches like Mel-frequency spectral coefficients (MFCCs) and prosodic features, laying the groundwork for subsequent extensions and improvements  [12] -  [14] . The emergence of deep learning further matured the area, with models like DNNs, RNNs, and CNNs demonstrating improved capabilities in capturing emotional nuances from speech  [15] -  [17] .\n\nRecent advancements in computer vision, particularly with the emergence of Vision Transformers (ViTs), have opened up new possibilities for leveraging visual data in SER  [18] . This is evident from their superior performance as compared to other deep learning based approaches as shown in  [19] .\n\nIn this work, we evaluate vision transformer based models for speech emotion recognition. To the best of our knowledge, this work is one of the earliest in the literature to evaluate vision transformer based models for speech emotion recognition in pseudo-naturalistic verbal communications in HRI. This evaluation of ViT based models has been done for modeling the individual characteristics in SER. This means, given a set of audio clips from an individual with labelled emotions (here neutral, happy, sad, and angry), we can predict the speech emotion of that individual for a different set of sentences spoken during a one-to-one HRI. To support our claim, we collect data from human participants an engage them in a pseudo-naturalistic conversation with the robot (explained more in section III-A). This paper makes the following contributions:\n\n• This work is among the first to investigate vision transformer-based models (both ViT and BEiT) for SER in the context of pseudo-naturalistic verbal HRI. • We show that personalization of SER models can be done by fine-tuning ViT and BEiT models on benchmark datasets and then further fine-tuning these on participant data and through ensembling the models.  fine-tuning of the vision transformer-based models. This paper has been arranged in the following manner: Section II outlines the background literature supporting this work. Section III descibes the methodology, which includes the data acquisition (Section III-A), description about melspectrograms (Section III-B), datasets used (Section III-C), and problem formulation (Section III-D). This is followed by Section IV, which discusses the results we obtained, and followed by the conclusion in Section VI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "The evolution of Speech Emotion Recognition (SER) has been marked by a continuous exploration of increasingly sophisticated techniques, each building upon the foundations laid by its predecessors. Early research in SER relied heavily on traditional approaches, such as Mel-frequency cepstral coefficients (MFCCs) and prosodic features  [12] -  [14] . MFCCs, derived from the human auditory system's response to sound, capture spectral characteristics crucial for distinguishing various speech sounds, while prosodic features like pitch, intensity, and duration provide insights into the emotional tone of speech. These handcrafted features, though valuable, often struggled to capture the subtle and complex interplay of acoustic cues that contribute to emotional expression.\n\nThe advent of deep learning revolutionized the field of SER, offering a powerful framework for automatically learning intricate patterns and representations from raw speech data. Deep Neural Networks (DNNs), with their multiple layers of interconnected nodes, enabled the extraction of high-level features that better captured the subtle nuances of emotional speech  [15] . Recurrent Neural Networks (RNNs), particularly Long Short-Term Memory (LSTM) networks, proved adept at modeling the temporal dynamics of speech, crucial for understanding the evolution of emotions over time  [16] . Convolutional Neural Networks (CNNs), originally designed for image processing, demonstrated their effectiveness in capturing local patterns and spatial dependencies in spectrograms, further enhancing SER performance  [17] .\n\nThe authors in  [20]  proposed to use CNN and RNN pipelines along with data augmentation techniques to improve the robustness of these models. This robustness was crucial for a human-robot interaction scenario with robot's ego noise. The authors in  [21]  also used a CNN plus BiLSTM hybrid model for the SER task using SAVEE and TESS datasets. Further, the authors in  [22]  proposed a machine learning pipeline for SER. Their approach involves using personalized and nonpersonalized features for SER. However, neither of these papers contributes to evaluating transformer-based architectures, which are currently SOTA in numerous fields of study  [23] .\n\nA number of benchmark datasets have been developed for SER that capture speaker characteristics owing to the number of actors involved for generating the data. More information about these datasets have been discussed in Section III-C. Owing to this large number of datasets, numerous approaches have been proposed in the literature. Even with transformerbased architectures, limited work has been shown in the SER literature. The authors in  [24]  show the highest performance on the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [25]  (described more in Section III-C), using a pre-trained xlsr-Wav2Vec2.0 transformer. A more recent transformer-based approach includes the work by the authors in  [26]  where they used a Whisper-based speech emotion recognition. Other attention mechanism-based approaches for the RAVDESS dataset include  [27] . For the Toronto emotional speech set (TESS)  [28] , authors in  [19]  tested the accuracies for SER tasks using a vision-transformerbased architecture. These transformers-based approaches have also been evaluated on the Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)  [29] ,  [30] . The authors in  [30]  tested their approach called the improVed emotionspecific pre-trained encoder (Vesper) on benchmark datasets like Multimodal EmotionLines Dataset (MELD) and Interactive Emotional Dyadic Motion Capture (IEMOCAP) database in addition to the CREMA-D. Further, the authors in  [31]  approach to use Acoustic Word Embeddings (AWEs) to push the classification accuracies on the Emotional Speech Database (ESD) and IEMOCAP.\n\nFor transformer based SER models, some recent works have made attempts to model personalised features of users like the authors in  [32] . Other approaches specific to vision transformers based approached for SER include the work by the authors in  [33]  where they have used the strengths of Multi-Axis Vision Transformer (MaxViT) and the Improved Multiscale Vision Transformer (MViTv2).\n\nHowever, the literature on SER and the datasets available have not been extensively leveraged to model speaker characteristics in a one-to-one human-robot situation using these SOTA transformer architectures.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Acquisition",
      "text": "Twelve neurotypical participants were recruited to participate in a human-robot interaction study to classify their speech into four primary emotions. Six of these participants were native English speakers. The other six were non-native English speakers. This was done to include more diverse demographics to examine SER using vision transformers based models. Among the participants, five were male and the rest were female. All the participants were either students or staff from the university aged between 18-59 years of age. Each participant asks pre-defined questions as shown in Figure  1a . These questions had been used for our previous studies during HRI  [34] . The following are the questions we asked the participants to ask the robot:\n\n• Hi. What's your name?\n\n• How are you doing?\n\n• Did you do anything fun yesterday?\n\n• What do you like doing?\n\n• Any plans for the weekend? The robot responds with appropriate answers to those questions and asks those questions back to the participant. The participants' replies are not pre-defined. They were asked to reply to the robot's questions with short answers. For each of these question-and-answer pairs, each participant was asked to speak in an emotional tone depicting one of the four primary emotions, i.e., neutral, happy, sad, and angry. The voices of the participants were recorded during this pseudo-natural humanrobot interaction where the questions that the participant asks were pre-defined but their answers weren't. More information on personalization is shared in Algorithm 1.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Mel Spectrogram",
      "text": "In this paper, since we are using vision based models, we convert the sound signals to 2D images. This is where we leverage the use of mel spectrograms. The mel spectrogram is used for better perception of sounds by humans. Considering f as the normal frequency, the frequency on the mel scale (m) will be given by  [35] -  [37] :\n\nAs can be seen form equation 1, the mel scale is a logarithmic scale to convert the frequency of the sounds from Hz to mels. The audio signal first goes through a fast Fourier transform performed on overlapping signal segments. These frequencies are converted to the log scale and the amplitude is converted to decibels to make the color dimension as shown in Figure  1a . For fine-tuning our vision transformer-based models, we use four benchmark datasets from the literature.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Datasets",
      "text": "• RAVDESS  [25] : This dataset has 1440 files containing data from 24 actors making sixty trials each. These actors cover seven emotions: calm, happy, sad, angry, fearful, surprise, and disgust. All of these emotions are deliberately displayed in the speech characteristics of each of the actors by speaking two sets of sentences, each with these seven emotional traits.\n\n• TESS  [28] : TESS contains data from two actresses aged 26 and 64 years. Each of the actresses speak pre-defined sentences in different ways so as to create a total of 2800 stimuli. These cover seven emotions: happiness, sadness, fear, pleasant surprise, anger, disgust, and neutral. • CREMA-D  [29] : This dataset captures six different emotions: happy, sad, neutral, anger, disgust, and fear. These stimuli were created by 91 actors generating a total of 7442 clips.\n\n• ESD  [38]  : This dataset captures the speakers' emotions for five emotional classes: neutral, happiness, anger, sadness, and surprise. These emotional stimuli were recorded by 20 speakers, 10 of whom were native English speakers.\n\n• MELD  [39]  : It is a multiparty multimodal dataset that captures speakers' emotions from the TV-series Friends. This dataset captures emotions in both continuous and discrete ways. Among the discrete emotions, it captures seven emotions: anger, disgust, sadness, joy, neutral, surprise, and fear. For all of these datasets, we have used only four emotion classes that are common between these four datasets, i.e., neutral, happiness, sadness, and anger. In addition to it, we used only ten actors for the ESD dataset who were native English speakers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Problem Formulation And Proposed Pipeline",
      "text": "For each of the datasets used, we generate mel-spectrograms of the speech data. Given a set of mel-spectrograms extracted from the speech data, the task is to classify each spectrogram into one of four emotion categories: neutral, happy, sad, and angry. Each spectrogram, x d i , where x ∈ R H×W ×C , d ∈ {RAVDESS, TESS, CREMA-D, ESD, MELD}, and i is the index of the datapoint, is passed through two pipelines (see Figure  1b , both ViT and BEiT encoders) to evaluate the performance of vision transformers for speech emotion recognition tasks. Here H = 224, W = 224, C = 3, represent the height, width, and the number of channels of the image respectively.\n\nThe formulation of both of these pipelines remains the same with the only difference of using a pre-trained base ViT encoder (vit-base-patch16-224) for the first pipeline (ViT encoder as the transformer encoder in Figure  1b ) whereas using a base BEiT encoder (microsoft/beit-base-patch16-224-pt22k-ft22k) for pipeline 2 (BEiT as the transformer encoder in Figure  1b )  [40] ,  [41] . Each image x d i is first divided into patches, x p ∈ R P ×P ×C , where P = 16 is the dimension of the image patch. So the output of the linear projection layer, x ′ ∈ R N ×(P 2 C) , where N is the number of patches. The patch and position embedding is then done using:\n\nwhere LN(.) is the layer normalization layer and pos embed is the position embedding added to each vector at the end of the linear projection layer. Then the values in the sequence are weighted through learnable matrices: query (q), key (k), and value (v) to calculate self-attention given by the authors in  [23] ,  [40] :\n\nwhere, U qkv ∈ R D×3D h are learnable matrices. Then the selfattention is calculated as:\n\nSo, the multihead attention, which is the multiple self attention operations in parallel heads can be expressed as  [23] ,  [40] :\n\nwhere, U msa ∈ R k.D h ×D , D h is the dimension of each head, k is the number of attention heads, and D is the dimension of the transformer model. The output of the transformers encoder is given by:\n\nwhere MLP(.) is the multilayer perception.\n\nAlgorithm 1 Personalization Process for Speech Emotion Recognition Require: Set of pre-defined questions Q = {q 1 , q 2 , ..., q 5 }, Emotions E = {neutral, happy, sad, angry} Ensure: y: Accuracy, Precision, Recall, F1 Score, FLOPs, Average Inference Time 1: Initialization: Prepare robot for interaction. 2: for each emotion e ∈ E do 3:\n\nfor each question q ∈ Q do 4:\n\nPerson asks the robot question q.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "5:",
      "text": "Robot responds to question q.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "6:",
      "text": "Robot asks the same question q back to the person.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "7:",
      "text": "Person gives an open-ended reply to question q.   (8)  and then fine-tune a ViT mix and a BEiT mix model on this mix training set X train,mix , Y train,mix . We perform full fine-tuning of our models on two A5000 GPUs, using K-Fold-Cross validation (5-fold-cross-validation in our case) with a constant learning rate of 2.00e -05. Further, we evaluate the performance of both pipelines for both Approach 1 and 2 using accuracy, precision, recall, and f-1 scores.\n\nTable  II    ResNeT-50 mix ) models. For the RAVDESS dataset, we currently achieve SOTA using the vanilla-ViT model, with the highest performance of 97.49% accuracy as compared to the current SOTA, which has a classification accuracy of 86.70% using multimodal data  [25] . Vanilla-ViT model also outperforms OpenAI/Whisper-base and ResNet-50 models. For the TESS dataset, we again achieve SOTA using vanilla-ViTs and vanilla-BEiTs, which is very similar to the ones obtained by the authors in  [19] , openai/whisper-base model, and ResNet-50 model. Among our vision transformer based approaches, the classification accuracy for the CREMA-D dataset was the highest for the mixed dataset approach (Approach 2) with vanilla-ViTs, which is better than the performance of comparable transformer architectures presented by the authors in  [42]  and other non-transformer-based approaches  [43] ,  [44] . However, among all the approaches, openai/whisper-base performed the best (80.82%) for the CREMA-D dataset when it was fine-tuned on only the CREMA-D training set. For the ESD dataset, our peak classification accuracy (96.25%) was obtained by a vanilla-BEiT model fine-tuned only on (X ESD,train , Y ESD,train ), which is again comparable to the current SOTA (93.20%) as presented by the authors in  [31] .\n\nIt also outperforms openai/whisper-base and ResNet-50 based approach we examined. Since MELD dataset has numerous speakers, it covers a wide-range of speaker characteristics (see Figure  2 ). This can be see in the low classification accuracy of the MELD dataset from the Table  II . Among our ViT and BEiT models, we obtained peak accuracy when the BEiT model fine-tuned over (X train,mix , Y train,mix ). However, our results with the MELD come close to the classification accuracies presented by the authors in  [26] . In addition to it, based on our experiments, we observed the highest classification accuracies for MELD mix with openai/whisper-base model.\n\nRemark 1: For a conversational dataset like MELD, methods like meta-learning for few-shot learning, and parameter efficient fine-tuning (PEFT) methods can help learn natural emotions in speech in addition to acted ones  [45] ,  [46]  for better domain adaptation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Human Subjects' Study",
      "text": "We evaluated our speech emotion recognition in a pseudonaturalistic human-robot interaction scenario using our finetuned ViTs and BEiTs. Since each participant asked five questions to the robot and responded to those five questions asked by the robot, we have 40 audio clips from each participant. We divided them into train and test datasets such that two sets of questions and answers each participant gave were separated for the test set. So, each participant had three questions and answer sets for train data. Each of those questions and answers was spoken in a way that depicts each of the four primary emotions of the individual. The split of the train and test data for each participant is shown in Table  III . Once the audio has been recorded from the participants, we convert the WAV files into spectrograms as shown in Figure  1a .\n\nAs described in Section III-A, each question-answer set was spoken in the four primary emotions. Hence, each participant had six audio clips for each emotion for the train set and four for the test set. Owing to the performance of Vision transformers-based approaches from Table  II , we used similar approaches to evaluate the use of vision transformers for speech emotion recognition in pseudo-naturalistic humanrobot interaction.  and of each BEiT i are c i,vit and c i,BEiT respectively, then the ensemble of models is:\n\nBEiT ensemble = 1 5\n\n• Model 5 and 6-ViT ensemble,d and BEiT ensemble,d : In this approach, we use the ViT d and BEiT d models trained in Approach 1 on each of the benchmark datasets. So the ensemble works as follows:\n\nBEiT ensemble,d = 1 5\n\nTable  IV  shows the model performance of all the above proposed models. It becomes evident that the best performance is obtained when we use ViT or BEiT based approaches as compared to OpenAi/Whisper-base and ResNet-50. As can be seen from Figure  2a  and 2b, the participant data has an overlap in the feature space of the datasets used in this paper. The overlap between the speech characteristics of speakers from these benchmark datasets and the participants for our humanrobot interaction study helped better classify speech emotion compared to vanilla ViTs or vanilla-BEiTs. This contributes to the participants having better classification accuracies for the mix models and the ViT ensemble,d /BEiT ensemble,d (see Table  IV ) for participants 1, 2, 3, 7, 8, 11, and 12. For some participants, the ensemble models (ViT ensemble and BEiT ensemble ) worked better since their speech characteristics didn't exactly overlap with the benchmark datasets used in this paper. For both native and non-native English speakers, ViT and BEiT based models performed better than other models compared.\n\nFor time complexity and inference times of our models, we analysed Floating Point Operations (FLOPs) and also recorded the average time it takes each of our models to classify one input test sample. As can be seen from Table  IV , all of the participants had the best classification accuracies with either a ViT or BEiT based model except for participant 11, who had the same accuracy for the openai/whisper-base model too. However, the inference time for the openai/whisper-base was significantly higher (197.141 ms/sample) than the BEiT mix model (3.339 ms/sample). Note that, real-time deployment of SER systems for HRI also depends on the system-specific requirements.\n\nV. ETHICS STATEMENT Since this paper includes a human subjects' study, we took consent of all the participants on a consent form approved by the Institute Review Board (IRB Number: 18.0726). The participants had the opportunity to discontinue at any point of the study if they wanted to.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vi. Conclusion And Future Works",
      "text": "In this work, we address the gap in speech emotion recognition for pseudo-naturalistic and personalized verbal HRI. We evaluate the use of vision transformer based models for identifying four primary emotions: neutral, happy, sad, and angry from the speech characteristics of our participants' data. We do this by first fine-tuning the vision transformer-based models on benchmark datasets. We then use these fine-tuned models to fine-tune them again on participants' speech data and/or perform ensembling of these models. This helps us choose the best model for each participant, hence contributing towards understanding the emotional speech characteristics of each individual instead of proposing a group model. In addition to creating these personalized speech emotion recognition models, we also evaluate vanilla-ViT and vanilla-BEiTs on benchmark datasets like RAVDESS, TESS, CREMA-D, ESD,",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The two pipelines evaluated in this paper for speech emotion recognition.",
      "page": 2
    },
    {
      "caption": "Figure 1: a. These questions had been used for our previous studies",
      "page": 3
    },
    {
      "caption": "Figure 1: b, both ViT and BEiT encoders) to evaluate",
      "page": 4
    },
    {
      "caption": "Figure 1: b) whereas",
      "page": 4
    },
    {
      "caption": "Figure 1: b) [40], [41]. Each image xd",
      "page": 4
    },
    {
      "caption": "Figure 2: ). This can be see in the low classiﬁcation accuracy of",
      "page": 5
    },
    {
      "caption": "Figure 2: T-SNE plots of ViT and BEiT embeddings for each emotion of all datasets and our collected participants’ data. The feature space of the emotional",
      "page": 6
    },
    {
      "caption": "Figure 2: a and 2b, the participant data has an overlap",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets": "Emotion\nRAVDESS\nTESS\nCREMA-D\nESD\nMELD"
        },
        {
          "Datasets": "Neutral\n96\n359\n1087\n3500\n6527"
        },
        {
          "Datasets": "Happy\n192\n350\n1271\n3500\n2416"
        },
        {
          "Datasets": "Sad\n192\n352\n1271\n3500\n917"
        },
        {
          "Datasets": "Angry\n192\n370\n1271\n3510\n1560"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID": "M1\nM2\nM3\nM4\nM5",
          "RAVDESS\nAcc\nP\nR\nF1": "97.49\n0.9749\n0.9749\n0.9749\n94.62\n0.9486\n0.9462\n0.9463\n84.40\n0.8876\n0.7905\n0.8059\n65.67\n0.7002\n0.6567\n0.6651\n–\n–\n–\n–",
          "TESS\nAcc\nP\nR\nF1": "100.0\n1.0000\n1.0000\n1.0000\n100.0\n1.0000\n1.0000\n1.0000\n100.0\n1.0000\n1.0000\n1.0000\n100.0\n1.0000\n1.0000\n1.0000\n–\n–\n–\n–",
          "CREMA-D\nAcc\nP\nR\nF1": "72.06\n0.7237\n0.7206\n0.7213\n71.85\n0.7200\n0.7185\n0.7173\n80.82\n0.8103\n0.8036\n0.8051\n70.41\n0.7026\n0.7041\n0.6999\n80.60\n–\n–\n–",
          "ESD\nAcc\nP\nR\nF1": "95.84\n0.9585\n0.9584\n0.9584\n96.25\n0.9626\n0.9625\n0.9625\n97.14\n0.9716\n0.9714\n0.9715\n90.65\n0.9081\n0.9065\n0.9067\n–\n–\n–\n–",
          "MELD\nAcc\nP\nR\nF1": "49.83\n0.4402\n0.4983\n0.4601\n43.32\n0.4304\n0.4332\n0.4317\n55.97\n0.4168\n0.3822\n0.3925\n51.12\n0.4533\n0.5112\n0.4747\n53.00\n–\n–\n–"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ID": "M1\nM2\nM3\nM4\nM5",
          "RAVDESS mix\nAcc\nP\nR\nF1": "95.70\n0.9572\n0.9570\n0.9570\n94.98\n0.9498\n0.9498\n0.9497\n72.59\n0.7873\n0.7152\n0.7083\n74.63\n0.7517\n0.7463\n0.7474\n–\n–\n–\n–",
          "TESS mix\nAcc\nP\nR\nF1": "100.0\n1.0000\n1.0000\n1.0000\n100.0\n1.0000\n1.0000\n1.0000\n100.0\n1.0000\n1.0000\n1.0000\n99.34\n0.9936\n0.9934\n0.9934\n–\n–\n–\n–",
          "CREMA-D mix\nAcc\nP\nR\nF1": "74.51\n0.7522\n0.7451\n0.7467\n72.36\n0.7281\n0.7236\n0.7217\n80.00\n0.8068\n0.7994\n0.7993\n74.49\n0.7425\n0.7449\n0.7406\n–\n–\n–\n–",
          "ESD mix\nAcc\nP\nR\nF1": "95.13\n0.9513\n0.9513\n0.9513\n95.28\n0.9533\n0.9528\n0.9528\n96.07\n0.9612\n0.9607\n0.9606\n89.44\n0.8959\n0.8944\n0.8946\n–\n–\n–\n–",
          "MELD mix\nAcc\nP\nR\nF1": "49.48\n0.4413\n0.4948\n0.4594\n50.22\n0.4480\n0.5022\n0.4638\n56.70\n0.4458\n0.4021\n0.4162\n50.33\n0.4747\n0.5033\n0.4861\n–\n–\n–\n–"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Participant": "1\n1",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "56.25\n0.5625\n0.5625\n0.5486",
          "Model 2": "62.5\n0.6458\n0.625\n0.6208",
          "Model 3": "68.75\n0.7125\n0.6875\n0.6935",
          "Model 4": "75.00\n0.8\n0.75\n0.75",
          "Model 5": "68.75\n0.7738\n0.6875\n0.6959",
          "Model 6": "56.25\n0.5089\n0.5625\n0.5284",
          "Model 7": "68.75\n0.7946\n0.6875\n0.79",
          "Model 8": "75.00\n0.8304\n0.75\n0.7193",
          "Model 9": "68.75\n0.725\n0.6875\n0.700",
          "Model 10": "75.00\n0.8166\n0.75\n0.7166",
          "Model 11": "50.00\n0.40\n0.5\n0.4166",
          "Model 12": "68.75\n0.725\n0.6875\n0.70"
        },
        {
          "Participant": "2\n2",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "37.5\n0.2905\n0.375\n0.3189",
          "Model 2": "43.75\n0.333\n0.4375\n0.3631",
          "Model 3": "56.25\n0.5833\n0.5625\n0.5607",
          "Model 4": "37.5\n0.3854\n0.375\n0.3512",
          "Model 5": "37.5\n0.2708\n0.375\n0.3006",
          "Model 6": "43.75\n0.25\n0.4375\n0.3154",
          "Model 7": "43.75\n0.35\n0.4375\n0.3611",
          "Model 8": "56.25\n0.6625\n0.5625\n0.5486",
          "Model 9": "43.75\n0.45\n0.4375\n0.4365",
          "Model 10": "43.75\n0.677\n0.4375\n0.425",
          "Model 11": "6.25\n0.0416\n0.0625\n0.05",
          "Model 12": "25.00\n0.175\n0.25\n0.2055"
        },
        {
          "Participant": "3\n3",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "43.75\n0.3571\n0.4375\n0.3697",
          "Model 2": "43.75\n0.5769\n0.4375\n0.3843",
          "Model 3": "25.00\n0.3125\n0.25\n0.1938",
          "Model 4": "56.00\n0.7875\n0.5625\n0.5304",
          "Model 5": "50.00\n0.6111\n0.5\n0.4622",
          "Model 6": "43.75\n0.4405\n0.4375\n0.4161",
          "Model 7": "50.00\n0.6417\n0.5\n0.469",
          "Model 8": "43.75\n0.475\n0.4375\n0.3679",
          "Model 9": "25.00\n0.196\n0.25\n0.2123",
          "Model 10": "18.75\n0.125\n0.1875\n0.1468",
          "Model 11": "25.00\n0.22\n0.25\n0.2197",
          "Model 12": "31.25\n0.2499\n0.3125\n0.275"
        },
        {
          "Participant": "4\n4",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "50.00\n0.375\n0.5\n0.4278",
          "Model 2": "50.00\n0.583\n0.5\n0.4393",
          "Model 3": "50.00\n0.4583\n0.5\n0.4679",
          "Model 4": "37.5\n0.3854\n0.375\n0.3512",
          "Model 5": "75.00\n0.7875\n0.75\n0.7431",
          "Model 6": "62.5\n0.7986\n0.625\n0.608",
          "Model 7": "56.25\n0.525\n0.5625\n0.5214",
          "Model 8": "62.5\n0.6667\n0.625\n0.6071",
          "Model 9": "37.5\n.2986\n0.375\n0.2996",
          "Model 10": "50.00\n0.5821\n0.5\n0.4974",
          "Model 11": "25.00\n0.076\n0.25\n0.1176",
          "Model 12": "50.00\n0.4875\n0.5\n0.479"
        },
        {
          "Participant": "5\n5",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "37.5\n0.211\n0.375\n0.265",
          "Model 2": "25.00\n0.3527\n0.25\n0.23236",
          "Model 3": "37.5\n0.3708\n0.375\n0.37",
          "Model 4": "37.5\n0.425\n0.375\n0.3631",
          "Model 5": "43.75\n0.333\n0.4375\n0.375",
          "Model 6": "50.00\n0.5833\n0.5\n0.4631",
          "Model 7": "31.25\n0.375\n0.3125\n0.3167",
          "Model 8": "43.75\n0.4571\n0.4375\n0.4141",
          "Model 9": "37.5\n0.6375\n0.375\n0.3696",
          "Model 10": "25.00\n0.1916\n0.25\n0.2166",
          "Model 11": "25.00\n0.0625\n0.25\n0.1",
          "Model 12": "37.5\n0.333\n0.375\n0.29166"
        },
        {
          "Participant": "6\n6",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "56.25\n0.6\n0.5625\n0.5754",
          "Model 2": "43.75\n0.4146\n0.4375\n0.4206",
          "Model 3": "37.50\n0.4015\n0.375\n0.3381",
          "Model 4": "43.75\n0.3917\n0.4375\n0.3944",
          "Model 5": "62.50\n0.70\n0.625\n0.5972",
          "Model 6": "50.00\n0.5\n0.5\n0.4667",
          "Model 7": "56.25\n0.75\n0.5625\n0.5916",
          "Model 8": "43.75\n0.4208\n0.4375\n0.4256",
          "Model 9": "50.00\n0.5833\n0.500\n0.4714",
          "Model 10": "50.00\n0.6071\n0.500\n0.4864",
          "Model 11": "31.25\n0.1905\n0.3125\n0.2364",
          "Model 12": "43.75\n0.4687\n0.4375\n0.4226"
        },
        {
          "Participant": "7\n7",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "46.67\n0.44\n0.4667\n0.4487",
          "Model 2": "33.33\n0.422\n0.33\n0.3022",
          "Model 3": "53.33\n0.4778\n0.5333\n0.4857",
          "Model 4": "13.33\n0.0667\n0.133\n0.0889",
          "Model 5": "46.67\n0.5889\n0.4667\n0.4610",
          "Model 6": "40.00\n0.544\n0.400\n0.3859",
          "Model 7": "53.33\n0.6267\n0.533\n0.511",
          "Model 8": "46.67\n0.4667\n0.4667\n0.4222",
          "Model 9": "18.75\n0.1548\n0.1875\n0.1623",
          "Model 10": "18.75\n0.1458\n0.1875\n0.1625",
          "Model 11": "31.25\n0.2019\n0.3125\n0.201",
          "Model 12": "43.75\n0.4688\n0.4375\n0.4167"
        },
        {
          "Participant": "8\n8",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "37.50\n0.4196\n0.3750\n0.3197",
          "Model 2": "25.00\n0.1500\n0.250\n0.1825",
          "Model 3": "37.50\n0.3750\n0.3750\n0.3416",
          "Model 4": "37.50\n0.3155\n0.3750\n0.3292",
          "Model 5": "37.50\n0.4196\n0.3750\n0.3197",
          "Model 6": "37.50\n0.200\n0.3750\n0.2540",
          "Model 7": "56.25\n0.5792\n0.5625\n0.565",
          "Model 8": "31.25\n0.2458\n0.3125\n0.2736",
          "Model 9": "25.00\n0.0769\n0.250\n0.1176",
          "Model 10": "6.25\n0.0321\n0.0625\n0.0417",
          "Model 11": "25.00\n0.0625\n0.2500\n0.10",
          "Model 12": "18.75\n0.206\n0.1875\n0.1806"
        },
        {
          "Participant": "9\n9",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "43.75\n0.3792\n0.4375\n0.4042",
          "Model 2": "56.25\n0.4304\n0.5625\n0.4804",
          "Model 3": "25.00\n0.2167\n0.2500\n0.2306",
          "Model 4": "31.25\n0.1562\n0.3125\n0.2083",
          "Model 5": "43.75\n0.4107\n0.4375\n0.4205",
          "Model 6": "37.50\n0.5089\n0.3750\n0.3784",
          "Model 7": "31.25\n0.3125\n0.3125\n0.3054",
          "Model 8": "37.50\n0.3875\n0.3750\n0.3681",
          "Model 9": "31.25\n0.3083\n0.3125\n0.2944",
          "Model 10": "37.50\n0.3214\n0.3750\n0.3409",
          "Model 11": "18.75\n0.1056\n0.1875\n0.1325",
          "Model 12": "37.50\n0.4167\n0.3750\n0.3667"
        },
        {
          "Participant": "10\n10",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "37.50\n0.300\n0.3750\n0.3056",
          "Model 2": "37.50\n0.2798\n0.3750\n0.3123",
          "Model 3": "56.25\n0.5458\n0.5625\n0.5506",
          "Model 4": "43.75\n0.4405\n0.4375\n0.4161",
          "Model 5": "50.00\n0.500\n0.500\n0.4921",
          "Model 6": "62.50\n0.6875\n0.6250\n0.6446",
          "Model 7": "37.50\n0.3583\n0.3750\n0.3631",
          "Model 8": "56.25\n0.5833\n0.5625\n0.5357",
          "Model 9": "18.75\n0.1458\n0.1875\n0.1548",
          "Model 10": "56.25\n0.4446\n0.5625\n0.4905",
          "Model 11": "25.00\n0.0667\n0.25\n0.1053",
          "Model 12": "31.25\n0.400\n0,3125\n0.333"
        },
        {
          "Participant": "11\n11",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "60.00\n0.6457\n0.600\n0.5606",
          "Model 2": "46.67\n.2519\n0.4667\n0.3241",
          "Model 3": "60.00\n0.7852\n0.600\n0.5708",
          "Model 4": "73.33\n0.7467\n0.7333\n0.7304",
          "Model 5": "53.33\n0.5733\n0.533\n0.4590",
          "Model 6": "40.00\n0.3556\n0.400\n0.3111",
          "Model 7": "60.00\n0.5778\n0.6\n0.5511",
          "Model 8": "66.67\n0.7968\n0.6667\n0.6139",
          "Model 9": "60.00\n0.6250\n0.5833\n0.5893",
          "Model 10": "73.33\n0.7875\n0.7292\n0.7411",
          "Model 11": "40.00\n0.2067\n0.400\n0.2667",
          "Model 12": "40.00\n0.5378\n0.400\n0.4394"
        },
        {
          "Participant": "12\n12",
          "Metric": "Accuracy (%)\nPrecision\nRecall\nF1 Score",
          "Model 1": "37.50\n0.433\n0.3750\n0.3265",
          "Model 2": "43.75\n0.6458\n0.4375\n0.4446",
          "Model 3": "50.00\n0.6071\n0.500\n0.4697",
          "Model 4": "31.25\n0.2979\n0.3125\n0.2956",
          "Model 5": "62.50\n0.7778\n0.6250\n0.6300",
          "Model 6": "50.00\n0.4970\n0.500\n0.4705",
          "Model 7": "62.50\n0.6417\n0.6250\n0.6290",
          "Model 8": "43.75\n0.3611\n0.4375\n0.3681",
          "Model 9": "31.25\n0.3006\n0.3125\n0.2963",
          "Model 10": "37.50\n0.3083\n0.3750\n0.3361",
          "Model 11": "31.25\n0.3167\n0.3125\n0.2053",
          "Model 12": "37.50\n0.4437\n0.3750\n0.3361"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Improving interactions with healthcare robots: a review of communication behaviours in social and healthcare contexts",
      "authors": [
        "D Johanson",
        "H Ahn",
        "E Broadbent"
      ],
      "year": "2021",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "2",
      "title": "Towards adaptive and personalized robotic therapy for children with autism spectrum disorder",
      "authors": [
        "R Mishra"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "3",
      "title": "Towards forecasting engagement in children with autism spectrum disorder using social robots and deep learning",
      "authors": [
        "R Mishra",
        "K Welch"
      ],
      "year": "2023",
      "venue": "SoutheastCon 2023"
    },
    {
      "citation_id": "4",
      "title": "Continuous hospitality with social robots at a hotel",
      "authors": [
        "J Nakanishi",
        "I Kuramoto",
        "J Baba",
        "K Ogawa",
        "Y Yoshikawa",
        "H Ishiguro"
      ],
      "year": "2020",
      "venue": "SN Applied Sciences"
    },
    {
      "citation_id": "5",
      "title": "Affective social robots",
      "authors": [
        "R Kirby",
        "J Forlizzi",
        "R Simmons"
      ],
      "year": "2010",
      "venue": "Robotics and Autonomous Systems"
    },
    {
      "citation_id": "6",
      "title": "Negative emotion management using a smart shirt and a robot assistant",
      "authors": [
        "M Pham",
        "H Do",
        "Z Su",
        "A Bishop",
        "W Sheng"
      ],
      "year": "2021",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "7",
      "title": "Enabling robots to distinguish between aggressive and joking attitudes",
      "authors": [
        "K Maehama",
        "J Even",
        "C Ishi",
        "T Kanda"
      ],
      "year": "2021",
      "venue": "IEEE Robotics and Automation Letters"
    },
    {
      "citation_id": "8",
      "title": "What does it take to generalize ser model across datasets? a comprehensive benchmark",
      "authors": [
        "A Ibrahim",
        "S Shehata",
        "A Kulkarni",
        "M Mohamed",
        "M Abdul-Mageed"
      ],
      "venue": "What does it take to generalize ser model across datasets? a comprehensive benchmark",
      "arxiv": "arXiv:2406.09933v1"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502v1[cs.SD"
    },
    {
      "citation_id": "10",
      "title": "Controlling emotion in text-to-speech with natural language prompts",
      "authors": [
        "T Bott",
        "F Lux",
        "N Vu"
      ],
      "venue": "Controlling emotion in text-to-speech with natural language prompts",
      "arxiv": "arXiv:2406.06406v2[cs.CL"
    },
    {
      "citation_id": "11",
      "title": "Speechprompt: An exploration of prompt tuning on generative spoken language model for speech processing tasks",
      "authors": [
        "K.-W Chang",
        "W.-C Tseng",
        "S.-W Li",
        "H.-Y Lee"
      ],
      "venue": "Speechprompt: An exploration of prompt tuning on generative spoken language model for speech processing tasks",
      "arxiv": "arXiv:2203.16773"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition based on mixed mfcc",
      "authors": [
        "P Zhou",
        "X.-P Li",
        "J Li",
        "X.-X Jing"
      ],
      "year": "2013",
      "venue": "Applied Mechanics and Mechanical Engineering III"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using dwt",
      "authors": [
        "S Lalitha",
        "A Mudupu",
        "B Nandyala",
        "R Munagala"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)"
    },
    {
      "citation_id": "14",
      "title": "Emotion recognition from speech using global and local prosodic features",
      "authors": [
        "K Rao",
        "S Koolagudi",
        "R Vempada"
      ],
      "year": "2013",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Vision transformer for music genre classification using mel-frequency cepstrum coefficient",
      "authors": [
        "Y Khasgiwala",
        "J Tailor"
      ],
      "year": "2021",
      "venue": "2021 IEEE 4th International Conference on Computing, Power and Communication Technologies (GUCON)"
    },
    {
      "citation_id": "19",
      "title": "An enhanced speech emotion recognition using vision transformer",
      "authors": [
        "S Akinpelu",
        "S Viriri",
        "A Adegun"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "20",
      "title": "On the robustness of speech emotion recognition for human-robot interaction with deep neural networks",
      "authors": [
        "E Lakomkin",
        "M Zamani",
        "C Weber",
        "S Magg",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition and classification using hybrid deep cnn and bilstm model",
      "authors": [
        "S Mishra",
        "N Bhatnagar"
      ],
      "year": "2024",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "22",
      "title": "Two-layer fuzzy multiple random forest for speech emotion recognition in human-robot interaction",
      "authors": [
        "L Chen",
        "W Su",
        "Y Feng",
        "M Wu",
        "J She",
        "K Hirota"
      ],
      "year": "2020",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "23",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "24",
      "title": "A proposal for multimodal emotion recognition using aural transformers and action units on ravdess dataset",
      "authors": [
        "C Luna-Jiménez",
        "R Kleinlein",
        "D Griol",
        "Z Callejas",
        "J Montero",
        "F Fernández-Martínez"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "25",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "26",
      "title": "What does it take to generalize ser model across datasets? a comprehensive benchmark",
      "authors": [
        "A Ibrahim",
        "S Shehata",
        "A Kulkarni",
        "M Mohamed",
        "M Abdul-Mageed"
      ],
      "year": "2024",
      "venue": "What does it take to generalize ser model across datasets? a comprehensive benchmark",
      "arxiv": "arXiv:2406.09933"
    },
    {
      "citation_id": "27",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "K Chumachenko",
        "A Iosifidis",
        "M Gabbouj"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "28",
      "title": "Toronto emotional speech set (tess)-younger talker happy",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Psychology Department, Tech. Rep"
    },
    {
      "citation_id": "29",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "30",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "P Chen",
        "X Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition",
      "authors": [
        "A Saliba",
        "Y Li",
        "R Sanabria",
        "C Lai"
      ],
      "year": "2024",
      "venue": "Layer-wise analysis of self-supervised acoustic word embeddings: A study on speech emotion recognition",
      "arxiv": "arXiv:2402.02617"
    },
    {
      "citation_id": "32",
      "title": "Personalized emotion analysis based on fuzzy multi-modal transformer model",
      "authors": [
        "J Liu",
        "M Ang",
        "J Chaw",
        "K Ng",
        "A.-L Kor"
      ],
      "year": "2025",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Maxmvit-mlp: Multiaxis and multiscale vision transformers fusion network for speech emotion recognition",
      "authors": [
        "K Ong",
        "C Lee",
        "H Lim",
        "K Lim",
        "A Alqahtani"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "34",
      "title": "Social impressions of the nao robot and its impact on physiology",
      "authors": [
        "R Mishra",
        "K Welch"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "35",
      "title": "Improved speech emotion recognition with mel frequency magnitude coefficient",
      "authors": [
        "J Ancilin",
        "A Milton"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "36",
      "title": "Mel frequency cepstral coefficient and its applications: A review",
      "authors": [
        "Z Abdul",
        "A Al-Talabani"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "37",
      "title": "Audio recognition using mel spectrograms and convolution neural networks",
      "authors": [
        "B Zhang",
        "J Leitner",
        "S Thornton"
      ],
      "year": "2019",
      "venue": "Noiselab University of California"
    },
    {
      "citation_id": "38",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Tech. Rep",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "40",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "41",
      "title": "Beit: Bert pre-training of image transformers",
      "authors": [
        "H Bao",
        "L Dong",
        "S Piao",
        "F Wei"
      ],
      "year": "2021",
      "venue": "Beit: Bert pre-training of image transformers",
      "arxiv": "arXiv:2106.08254"
    },
    {
      "citation_id": "42",
      "title": "Septr: Separable transformer for audio spectrogram processing",
      "authors": [
        "N.-C Ristea",
        "R Ionescu",
        "F Khan"
      ],
      "year": "2022",
      "venue": "Septr: Separable transformer for audio spectrogram processing",
      "arxiv": "arXiv:2203.09581"
    },
    {
      "citation_id": "43",
      "title": "Self-paced ensemble learning for speech and audio classification",
      "authors": [
        "N.-C Ristea",
        "R Ionescu"
      ],
      "year": "2021",
      "venue": "Self-paced ensemble learning for speech and audio classification",
      "arxiv": "arXiv:2103.11988"
    },
    {
      "citation_id": "44",
      "title": "Nonlinear neurons with human-like apical dendrite activations",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu",
        "N.-C Ristea",
        "N Sebe"
      ],
      "year": "2023",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "45",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "46",
      "title": "Parameter efficient finetuning for speech emotion recognition and domain adaptation",
      "authors": [
        "N Lashkarashvili",
        "W Wu",
        "G Sun",
        "P Woodland"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Prototypical networks for few-shot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    }
  ]
}