{
  "paper_id": "2401.02344v1",
  "title": "Multi-Source Domain Adaptation With Transformer-Based Feature Generation For Subject-Independent Eeg-Based Emotion Recognition",
  "published": "2024-01-04T16:38:47Z",
  "authors": [
    "Shadi Sartipi",
    "Mujdat Cetin"
  ],
  "keywords": [
    "Brain-computer interface",
    "Domain adaptation",
    "Emotion recognition",
    "Moment matching",
    "Transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Although deep learning-based algorithms have demonstrated excellent performance in automated emotion recognition via electroencephalogram (EEG) signals, variations across brain signal patterns of individuals can diminish the model's effectiveness when applied across different subjects. While transfer learning techniques have exhibited promising outcomes, they still encounter challenges related to inadequate feature representations and may overlook the fact that source subjects themselves can possess distinct characteristics. In this work, we propose a multi-source domain adaptation approach with a transformer-based feature generator (MSDA-TF) designed to leverage information from multiple sources. The proposed feature generator retains convolutional layers to capture shallow spatial, temporal, and spectral EEG data representations, while self-attention mechanisms extract global dependencies within these features. During the adaptation process, we group the source subjects based on correlation values and aim to align the moments of the target subject with each source as well as within the sources. MSDA-TF is validated on the SEED dataset and is shown to yield promising results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing intends to process, handle, identify, and react to individuals' emotional states. It holds great potential across various application areas ranging from healthcare and education to brain-computer interfaces (BCIs)  [1] . EEGbased emotion recognition has gained great attention due to the high temporal resolution, data adequacy, and clear response to emotional stimuli  [2] . While various studies try to capture the time and frequency features from the EEG data for emotion recognition, the high subject dependency of the EEG data prevents getting the desired performance  [3] . This variability across different subjects could be due to head shape, This work has been partially supported by the National Science Foundation (NSF) under grants  CCF-1934962 and DGE-1922591 , and by the BRAIN Initiative of the National Institutes of Health through grant U19NS128613.\n\nmental states, noise, etc  [3] .\n\nDeep learning approaches have been applied widely in this domain to find the features that can discriminate the emotional states  [4] . EEGNet  [5]  and ConvNet  [4]  are two convolutional neural networks (CNN) based architectures that showed great performance. Alongside the spatial information, the temporal dependencies can also boost the model's performance. One approach is using CNN and long-shortterm memory (LSTM) networks to capture the spatial and temporal features  [6] . Transformers (TF) are also utilized to capture the long-term dependencies  [7] . However, there is still room to find a network that can extract discriminative features across different subjects.\n\nThe traditional approach to addressing the mentioned limitation involves using a sufficient amount of labeled target domain data (i.e., training data from the subject of interest) to calibrate the learned model, which can be time-consuming. Transfer learning, as discussed in  [8] , is a common strategy applied to tackle this issue. Domain adaptation (DA) is a branch of transfer learning that employs specific metrics to enhance the performance of the target domain by minimizing domain shifts between the target and source domains. Maximum mean discrepancy (MMD)  [9]  is a widely used metric that reduces the distance between two distributions  [10] . Adversarial discriminative domain adaptation (ADDA)  [11]  learns a discriminative representation from source domain labels and then maps the target data to the same space through an asymmetric mapping using a domain-adversarial loss. Yet, differences among subjects in the source domain can challenge the model's learning process.\n\nIn this paper, we propose a multi-source domain adaptation approach with a transformer-based feature generator (MSDA-TF). The proposed feature generator uses CNN blocks to initially capture the local spatial, temporal, and spectral characteristics of the EEG data. Since CNN can only capture local information, the TF is applied to extract global features to compensate for the limitation of CNN. To enhance the model's performance across different subjects, we aim to align the moments of the feature distribution of multiple subjects (labeled source domains) with the test subject (unlabeled target domain). The main contributions of this paper can be summarized as follows: • A novel feature generator is proposed to capture local features via CNN, followed by the application of TF to extract global dependencies.\n\n• Training subjects are grouped based on a correlation metric to form multiple source domains, and moment matching MSDA is applied to improve target domain performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "In this section, we describe the proposed feature generator and the domain adaptation process. The feature generator aims to learn the discriminative features, while domain adaptation d Figure  1  illustrates the overall structure of the proposed method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Generator",
      "text": "The main step in EEG emotion recognition studies is finding the features that can discriminate between different emotional states. Our feature generator consists of two main parts: the CNN module and the transformer encoder. The CNN module adopts multiple 2D convolutions. The CNN module has been utilized for its capability to extract effective features  [12]  and reduce the dimensionality of the EEG data. This module considers the spatial, temporal, and spectral features of the EEG data. As shown in Table  1 , the CNN module comprises two blocks, C1 and C2, each containing three CNN layers, one max-pooling layer, and one dropout layer. Following the CNN module, the extracted features are directed into the TF encoder. The architecture of TF is drawn from  [13] , renowned for its efficacy in natural language processing. Let H, W , and C represent the height, width, and channels of the CNN-module output, respectively. Initially, the TF divides the data into n patches with a lower dimensionality of size p, where n = H p × W p . Subsequently, a linear layer is applied to each patch, projecting it into a Ddimensional space. To uphold the inherent spatial arrangement among these patches, positional embeddings are introduced to the patch embeddings. In this study, we set p and D to 3 and 64, respectively.\n\nThe TF encoder consists of l consecutive blocks of multihead self-attention (MHA) and multi-layer perception (MLP). Each MHA block incorporates h self-attention heads, with each head producing an n × d sequence. To perform the attention mechanism, the input vector is multiplied by three distinct weight matrices, resulting in the derivation of the query vector (Q), key vector (K), and value vector (V), where Q, K, and V ∈ n×d . For the attention mechanism, each query vector is compared to a set of key vectors. The outcome is normalized using a softmax function and then multiplied by a set of value vectors as follows  [13] :\n\nTo obtain the MHA, the resulting sequences from each block are concatenated into an n×dh sequence. In this study, we set l and h to 8. Besides the MHA, the encoder also contains two MLP blocks with the number of units set to 2048 and 1024, respectively. The outputs of the encoder are then fed into the Softmax classifier for classification.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Domain Adaptation",
      "text": "Variations across subjects in a broad training set can reduce the effectiveness of transfer learning based on that set. Splitting that data into more homogeneous multiple source domains enables better domain adaptation between these source domains and the target domain. Let D s = {(Z s1 , Y s1 ), (Z s2 , Y s2 ), . . . , (Z sK , Y sK )} be the k sets of source domain data and their labels, and D T = {Z t } n i=1 represent the target data without labels. Since we are dealing with more than one source domain, during the adaptation process the model aims to align the source domains with the target domain while concurrently aligning the source domains with each other by using the paradigm presented in  [14, 15] .\n\nx p i the p-order moment of X with the total number of m samples, to calculate the distribution differences among domains, the moment distance (MD) based on  [15]  is defined as follows.\n\nAs shown in Figure  1 , the model consists of the feature generator, F, and K classifiers, C K for K source domains. Thus, the objective function would be a combination of the Softmax cross-entropy loss, L (Xsi,Ysi) for training the K classifiers and the feature generator cost function, i.e., (2), as follows  [15] .\n\nwhere λ is the hyperparameter setting the relative weighting of the two different loss functions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Learning Process",
      "text": "As mentioned previously, the brain responses to the same emotional state vary across different subjects. Thus, instead of considering all training subjects as a single source domain, we consider K different source domains. To quantify the degree of similarity among brain responses, we apply the Pearson correlation across all training subjects without considering the labels of the data. Then, we group them into K groups based on the correlation values. In this work, we set K to 4.\n\nDuring the domain adaptation process, inspired by  [14]  we follow the steps presented in Algorithm 1. For each source domain, let C = {(C i , C ′ i )} K i=1 be the pair of classifiers. The goal of the paired classifiers is to get the target samples away from the support of the source. First, we train the feature generator and the classifier, F and C , to classify the source domain samples. Second, with fixed F, C i and C ′ i are trained to maximize the target domain differences in each classifier pair. Third, with fixed C , F is trained to minimize the target domain difference on each classifier pair. Fourth, for the target domain classification, the output would be the average of the K classifiers driven from K multiple sources.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Study",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "In this study, we utilized the publicly available SEED dataset  [16] . This dataset comprises 15 movie clips that elicit happiness, sadness, and neutral emotional states. The dataset consists of 15 participants, comprising 8 females and 7 males. During the experiments, participants were instructed to fully immerse themselves in the movie clips to evoke the corresponding emotions. EEG signals were recorded using a total of 62 channels and each trial adhered to a predefined sequence: a 5-second introductory hint, followed by 4 minutes of the clip serving as the emotional stimulus, then 45 seconds allocated for self-assessment, and finally a 15-second break. EEG data were downsampled from 1000 Hz to 200 Hz, and a band-pass filter with a frequency range of 0.5-70 Hz was applied. We calculate the differential entropy (DE) features at 1-second intervals with no overlap in delta: 1 -4 Hz, theta: 4 -8 Hz, alpha: 8 -13 Hz, beta: 13 -30 Hz, and gamma: 30 -50 Hz frequency subbands.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "In this section, we present the performance of the proposed approach. The input data are normalized by subtracting the mean and dividing by the standard deviation. To conduct the evaluation, we adhere to the leave-one-subject-out crossvalidation scheme, comprising 14 source subjects and 1 target subject in each validation round. Accuracy and F1-scores are calculated for each validation, and the average performance is reported. To group the source subjects, Pearson correlation is calculated among the source subjects and sorted in descending order. Based on the correlation scores, the subjects with the highest correlation values are divided into 4 groups. Two groups of three subjects and two groups of four. The optimizer, learning rate, and number of epochs are set to Adam optimizer, 0.0001, and 350, respectively. Table  2  presents the performance results for the proposed MSDA-TF compared to three baseline methods, namely, Source only, Target only, and Single source, using the metrics of Accuracy and F1-score, along with their respective standard deviations. Training exclusively on the source domain (no domain adaptation) leads to an average accuracy of 0.86 ± 0.06 and an F1-score of 0.85 ± 0.06. Notably, when exclusively trained on the target domain, the model attains an average accuracy of 0.95 ± 0.07, representing the highest performance achieved by the proposed feature gen- 0.87 ± 0.05 She et al.  [18]  0.86 ± 0.07 Zhao et al.  [19]  0.86 ± 0.07 Du et al.  [20]  0.90 ± 0.01\n\nerator. Moreover, we include the results of considering all source subjects as a single source domain. As presented, the proposed MSDA-TF results in an accuracy and F1-score of 0.92 ± 0.04 which highlights the positive effect of using domain adaptation with multiple source domains. Furthermore, Figure  3  displays the predictions for each emotion class as a confusion matrix. Comparing the results of using the source domain data with no adaptation with our proposed MSDA-TF approach, we observe that the proposed approach aids the model in detecting negative emotions significantly. Also, Table  3  presents a comparison of the proposed method with several methods from recent literature and demonstrates the superiority of our proposed approach.\n\nTo verify the MSDA process, we visualize the t-SNE  [21]  of the learned representations corresponding to two source domains and a target domain before the classification step, as shown in Figure  2 . While Figure  2  (a) displays the scatter plot of the two source domains and the target domain, Figures  2  (b-d ) present the alignment of the target with each source and the sources with each other. This visualization suggests the proposed adaptation process works properly.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we proposed a novel multi-source domain adaptation approach called MSDA-TF for subject-independent EEG-based emotion classification. Our method extracts feature representations from spectral, temporal, and spatial EEG characteristics and aligns the moments of the unlabeled target domain with each of the labeled source domains and the source domains with each other as well. The results demonstrate that MSDA-TF performs domain adaptation successfully and outperforms state-of-the-art algorithms.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed MSDA-TF.",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the overall structure of the pro-",
      "page": 2
    },
    {
      "caption": "Figure 1: , the model consists of the feature",
      "page": 3
    },
    {
      "caption": "Figure 2: t-SNE visualization of the proposed method (a) before adaptation, (b) alignment of source one and target, (c) alignment",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices for source-domain training",
      "page": 4
    },
    {
      "caption": "Figure 3: displays the predictions for each",
      "page": 4
    },
    {
      "caption": "Figure 2: While Figure 2 (a) displays the scatter plot",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "†Goergen Institute for Data Science, University of Rochester, Rochester, NY, USA"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "mental states, noise, etc [3]."
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "Deep learning approaches have been applied widely in"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "this domain to find the features that can discriminate the emo-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "tional states [4]. EEGNet [5] and ConvNet [4] are two con-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "volutional neural networks\n(CNN) based architectures\nthat"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "showed great performance.\nAlongside the spatial\ninforma-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "tion,\nthe temporal dependencies can also boost\nthe model’s"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "performance. One approach is using CNN and long-short-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "term memory (LSTM) networks\nto capture the spatial and"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "temporal\nfeatures\n[6].\nTransformers\n(TF) are also utilized"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "to capture the long-term dependencies [7]. However,\nthere"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "is still room to find a network that can extract discriminative"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "features across different subjects."
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "The traditional approach to addressing the mentioned lim-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "itation involves using a sufficient amount of labeled target do-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "main data (i.e.,\ntraining data from the subject of interest) to"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "calibrate the learned model, which can be time-consuming."
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "Transfer learning, as discussed in [8],\nis a common strategy"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "applied to tackle this\nissue.\nDomain adaptation (DA)\nis a"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "branch of\ntransfer\nlearning that employs specific metrics to"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "enhance the performance of the target domain by minimizing"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "domain shifts between the target and source domains. Max-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "imum mean discrepancy (MMD)\n[9]\nis a widely used met-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "ric that reduces the distance between two distributions [10]."
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "Adversarial discriminative domain adaptation (ADDA)\n[11]"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "learns a discriminative representation from source domain la-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "bels and then maps the target data to the same space through"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "an asymmetric mapping using a domain-adversarial loss. Yet,"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "differences among subjects\nin the source domain can chal-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "lenge the model’s learning process."
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "In this paper, we propose a multi-source domain adap-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "tation\napproach with\na\ntransformer-based\nfeature\ngenera-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "tor\n(MSDA-TF). The proposed feature generator uses CNN"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "blocks\nto initially capture\nthe\nlocal\nspatial,\ntemporal,\nand"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "spectral characteristics of the EEG data. Since CNN can only"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "capture local\ninformation,\nthe TF is applied to extract global"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "features to compensate for the limitation of CNN. To enhance"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "the model’s performance across different\nsubjects, we aim"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "to align the moments of\nthe feature distribution of multiple"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "subjects (labeled source domains) with the test subject\n(un-"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "labeled target domain). The main contributions of this paper"
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": ""
        },
        {
          "⋆Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA": "can be summarized as follows:"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: , the CNN module comprises two",
      "data": [
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": ""
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "ment among these patches, positional embeddings are intro-"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "duced to the patch embeddings. In this study, we set p and D"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "to 3 and 64, respectively."
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": ""
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "The TF encoder consists of l consecutive blocks of multi-"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "head self-attention (MHA) and multi-layer perception (MLP)."
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "Each MHA block incorporates h self-attention heads, with"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": ""
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "each head producing an n × d sequence. To perform the at-"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": ""
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "tention mechanism, the input vector is multiplied by three dis-"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": ""
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "tinct weight matrices, resulting in the derivation of the query"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": ""
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "vector (Q), key vector (K), and value vector (V), where Q, K,"
        },
        {
          "dimensional space.\nTo uphold the inherent spatial arrange-": "and V ∈n×d. For the attention mechanism, each query vector"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: , the CNN module comprises two",
      "data": [
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "is compared to a set of key vectors. The outcome is normal-"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": ""
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "ized using a softmax function and then multiplied by a set of"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "value vectors as follows [13]:"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": ""
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": ""
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "(cid:19)\n(cid:18) QKT"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "√"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "Attention(Q, K, V) = Softmax\n(1)"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "d"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": ""
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "To obtain the MHA, the resulting sequences from each block"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "are concatenated into an n×dh sequence. In this study, we set"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": ""
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "l and h to 8. Besides the MHA, the encoder also contains two"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "MLP blocks with the number of units set\nto 2048 and 1024,"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "respectively. The outputs of the encoder are then fed into the"
        },
        {
          "and V ∈n×d. For the attention mechanism, each query vector": "Softmax classifier for classification."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "of the two different loss functions."
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "2.3. Learning Process"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "As mentioned previously,\nthe brain responses\nto the same"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "emotional state vary across different subjects. Thus,\ninstead"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "of considering all training subjects as a single source domain,"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "we consider K different source domains. To quantify the de-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "gree of similarity among brain responses, we apply the Pear-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "son correlation across all\ntraining subjects without consider-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "ing the labels of the data. Then, we group them into K groups"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "based on the correlation values. In this work, we set K to 4."
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "During the domain adaptation process,\ninspired by [14]"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "we follow the steps presented in Algorithm 1. For each source"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "let C = {(Ci, C′"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "domain,\ni)}K\ni=1 be the pair of classifiers. The"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "goal of the paired classifiers is to get the target samples away"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "from the support of\nthe source.\nFirst, we train the feature"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "generator and the classifier, F and C ,\nto classify the source"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "domain samples. Second, with fixed F, Ci and C′\ni are trained"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "to maximize the target domain differences in each classifier"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "pair. Third, with fixed C , F is trained to minimize the target"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "domain difference on each classifier pair. Fourth, for the tar-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "get domain classification, the output would be the average of"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "the K classifiers driven from K multiple sources."
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "3. EXPERIMENTAL STUDY"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "3.1. Dataset"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "In this study, we utilized the publicly available SEED dataset"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "[16]. This dataset comprises 15 movie clips that elicit happi-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "ness, sadness, and neutral emotional states. The dataset con-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "sists of 15 participants, comprising 8 females and 7 males."
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "During the experiments, participants were instructed to fully"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "immerse themselves\nin the movie clips\nto evoke the corre-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "sponding emotions. EEG signals were recorded using a to-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "tal of 62 channels and each trial adhered to a predefined se-"
        },
        {
          "where λ is the hyperparameter setting the relative weighting": ""
        },
        {
          "where λ is the hyperparameter setting the relative weighting": "quence: a 5-second introductory hint, followed by 4 minutes"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: presents a comparison of the pro-",
      "data": [
        {
          "10": "5\n10"
        },
        {
          "10": "20"
        },
        {
          "10": "10\n5\n0\n5\n10\n15\n20\n0\n20\n10\n0\n10\n20\n20\n10\n0\n10"
        },
        {
          "10": "(a)\n(b)\n(c)\n(d)"
        },
        {
          "10": "Fig. 2.\nt-SNE visualization of the proposed method (a) before adaptation, (b) alignment of source one and target, (c) alignment"
        },
        {
          "10": "of source two and target, and (d) alignment of both sources."
        },
        {
          "10": "Table 2. Mean of the performance for the proposed method.\nTable 3. Comparison with previous works."
        },
        {
          "10": "Study\nAccuracy\nThe “Source only” and “Target only” rows are the results on"
        },
        {
          "10": "Proposed MSDA-TF\n0.92 ± 0.04\nthe target domain when using no domain adaptation and train-"
        },
        {
          "10": "0.87 ± 0.05\nPan et al. [17]\ning only on the source or the target domain respectively."
        },
        {
          "10": "Method\nAccuracy\nF1-score\n0.86 ± 0.07\nShe et al. [18]"
        },
        {
          "10": "0.86 ± 0.06\n0.85 ± 0.07\n0.86 ± 0.07\nSource only\nZhao et al. [19]"
        },
        {
          "10": "0.90 ± 0.01\nDu et al. [20]\n0.88 ± 0.06\n0.88 ± 0.06\nSingle Source"
        },
        {
          "10": "0.92 ± 0.04\n0.92 ± 0.05\nMSDA-TF"
        },
        {
          "10": "0.95 ± 0.07\n0.95 ± 0.06\nTarget only"
        },
        {
          "10": "erator. Moreover, we include the results of considering all"
        },
        {
          "10": "source subjects as a single source domain.\nAs presented,"
        },
        {
          "10": "the proposed MSDA-TF results in an accuracy and F1-score"
        },
        {
          "10": "negative\nnegative\n0.8\n0.67\n0.18\n0.16\n0.89\n0.058\n0.053"
        },
        {
          "10": "0.8"
        },
        {
          "10": "of 0.92 ± 0.04 which highlights the positive effect of using"
        },
        {
          "10": "0.6\n0.6\ndomain adaptation with multiple source domains."
        },
        {
          "10": "neutral\nneutral"
        },
        {
          "10": "0.0079\n0.97\n0.021\n0.053\n0.93\n0.018\nFurthermore, Figure 3 displays the predictions for each"
        },
        {
          "10": "0.4\n0.4"
        },
        {
          "10": "emotion class as a confusion matrix. Comparing the results"
        },
        {
          "10": "positive\npositive\n0.2\n0.2\nof using the source domain data with no adaptation with our"
        },
        {
          "10": "0.036\n0.036\n0.93\n0.036\n0.027\n0.94"
        },
        {
          "10": "proposed MSDA-TF approach, we observe that the proposed"
        },
        {
          "10": "negative\nneutral\npositive\nnegative\nneutral\npositive\napproach aids the model\nin detecting negative emotions sig-"
        },
        {
          "10": "nificantly. Also, Table 3 presents a comparison of\nthe pro-"
        },
        {
          "10": "Fig.\n3.\nConfusion matrices\nfor\nsource-domain\ntraining"
        },
        {
          "10": "posed method with several methods from recent literature and"
        },
        {
          "10": "with no adaptation (left), and proposed MSDA-TF approach"
        },
        {
          "10": "demonstrates the superiority of our proposed approach."
        },
        {
          "10": "(right)."
        },
        {
          "10": "To verify the MSDA process, we visualize the t-SNE [21]"
        },
        {
          "10": "of\nthe learned representations corresponding to two source"
        },
        {
          "10": "domains and a target domain before the classification step, as"
        },
        {
          "10": "subject in each validation round. Accuracy and F1-scores are"
        },
        {
          "10": "shown in Figure 2. While Figure 2 (a) displays the scatter plot"
        },
        {
          "10": "calculated for each validation, and the average performance is"
        },
        {
          "10": "of the two source domains and the target domain, Figures 2"
        },
        {
          "10": "reported. To group the source subjects, Pearson correlation is"
        },
        {
          "10": "(b-d) present the alignment of the target with each source and"
        },
        {
          "10": "calculated among the source subjects and sorted in descend-"
        },
        {
          "10": "the sources with each other. This visualization suggests the"
        },
        {
          "10": "ing order. Based on the correlation scores,\nthe subjects with"
        },
        {
          "10": "proposed adaptation process works properly."
        },
        {
          "10": "the highest correlation values are divided into 4 groups. Two"
        },
        {
          "10": "groups of\nthree subjects and two groups of\nfour.\nThe opti-"
        },
        {
          "10": "mizer,\nlearning rate, and number of epochs are set\nto Adam\n4. CONCLUSION"
        },
        {
          "10": "optimizer, 0.0001, and 350, respectively."
        },
        {
          "10": "In this work, we proposed a novel multi-source domain adap-"
        },
        {
          "10": "Table 2 presents the performance results for the proposed"
        },
        {
          "10": "tation\napproach\ncalled MSDA-TF for\nsubject-independent"
        },
        {
          "10": "MSDA-TF compared\nto\nthree\nbaseline methods,\nnamely,"
        },
        {
          "10": "EEG-based emotion classification. Our method extracts fea-"
        },
        {
          "10": "Source only, Target only, and Single source, using the met-"
        },
        {
          "10": "ture representations from spectral, temporal, and spatial EEG"
        },
        {
          "10": "rics of Accuracy and F1-score,\nalong with their\nrespective"
        },
        {
          "10": "characteristics and aligns the moments of\nthe unlabeled tar-"
        },
        {
          "10": "standard deviations. Training exclusively on the source do-"
        },
        {
          "10": "get domain with each of\nthe\nlabeled source domains\nand"
        },
        {
          "10": "main (no domain adaptation)\nleads\nto an average accuracy"
        },
        {
          "10": "the\nsource domains with each other\nas well.\nThe\nresults"
        },
        {
          "10": "of 0.86 ± 0.06 and an F1-score of 0.85 ± 0.06.\nNotably,"
        },
        {
          "10": "demonstrate\nthat MSDA-TF\nperforms\ndomain\nadaptation"
        },
        {
          "10": "when exclusively trained on the target domain,\nthe model"
        },
        {
          "10": "successfully and outperforms state-of-the-art algorithms."
        },
        {
          "10": "attains an average accuracy of 0.95 ± 0.07, representing the"
        },
        {
          "10": "highest performance achieved by the proposed feature gen-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "sivan Puthusserypady, “An end-to-end deep learning ap-"
        },
        {
          "5. REFERENCES": "[1]\nJerry J Shih, Dean J Krusienski, and Jonathan R Wol-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "proach to MI-EEG signal classification for BCIs,” Ex-"
        },
        {
          "5. REFERENCES": "paw, “Brain-computer interfaces in medicine,” in Mayo",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "pert Syst. Appl., vol. 114, pp. 532–542, 2018."
        },
        {
          "5. REFERENCES": "clinic proceedings. Elsevier, 2012, vol. 87, pp. 268–279.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[13] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "5. REFERENCES": "[2] Wei-Long Zheng, Jia-Yi Zhu, and Bao-Liang Lu, “Iden-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,"
        },
        {
          "5. REFERENCES": "tifying stable patterns over time for emotion recognition",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "and Illia Polosukhin,\n“Attention is all you need,” Adv."
        },
        {
          "5. REFERENCES": "from EEG,” IEEE Trans. Affect. Comput., vol. 10, no. 3,",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Neural Inf. Process. Syst., vol. 30, 2017."
        },
        {
          "5. REFERENCES": "pp. 417–429, 2017.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[14] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and"
        },
        {
          "5. REFERENCES": "[3] Wojciech Samek, Frank C Meinecke, and Klaus-Robert",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Tatsuya Harada,\n“Maximum classifier discrepancy for"
        },
        {
          "5. REFERENCES": "M¨uller,\n“Transferring subspaces between subjects\nin",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "unsupervised domain adaptation,”\nin IEEE Comput."
        },
        {
          "5. REFERENCES": "IEEE Trans. Biomed.\nbrain–computer\ninterfacing,”",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Soc. Conf. Comput. Vision\nand Pattern Recognition"
        },
        {
          "5. REFERENCES": "Eng., vol. 60, no. 8, pp. 2289–2298, 2013.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "(CVPR), 2018, pp. 3723–3732."
        },
        {
          "5. REFERENCES": "[4] Robin Tibor Schirrmeister and et al.,\n“Deep learning",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[15] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang,"
        },
        {
          "5. REFERENCES": "with convolutional neural networks for EEG decoding",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Kate Saenko, and Bo Wang,\n“Moment matching for"
        },
        {
          "5. REFERENCES": "and visualization,” Human brain mapping, vol. 38, no.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "multi-source domain adaptation,” in Proceedings of the"
        },
        {
          "5. REFERENCES": "11, pp. 5391–5420, 2017.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "IEEE/CVF international conference on computer vision,"
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "2019, pp. 1406–1415."
        },
        {
          "5. REFERENCES": "[5] Vernon J Lawhern, Amelia J Solon, Nicholas R Way-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "towich, Stephen M Gordon, Chou P Hung, and Brent J",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[16] Wei-Long Zheng and Bao-Liang Lu,\n“Investigating"
        },
        {
          "5. REFERENCES": "Lance,\n“EEGNet: a compact convolutional neural net-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "critical\nfrequency bands and channels\nfor EEG-based"
        },
        {
          "5. REFERENCES": "J.\nwork for EEG-based brain–computer\ninterfaces,”",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "IEEE\nemotion recognition with deep neural networks,”"
        },
        {
          "5. REFERENCES": "Neural Eng., vol. 15, no. 5, pp. 056013, 2018.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Trans. Auton. Ment. Dev., vol. 7, no. 3, pp. 162–175,"
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "2015."
        },
        {
          "5. REFERENCES": "[6] Shadi Sartipi, Mastaneh Torkamani-Azar, and Mujdat",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "Cetin,\n“A hybrid\nend-to-end\nspatio-temporal\natten-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[17] Deng Pan, Haohao Zheng, Feifan Xu, Yu Ouyang, Zhe"
        },
        {
          "5. REFERENCES": "tion neural network with graph-smooth signals for EEG",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Jia, Chu Wang,\nand Hong Zeng,\n“MSFR-GCN: A"
        },
        {
          "5. REFERENCES": "IEEE Trans. Cogn. Develop.\nemotion recognition,”",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "multi-scale feature reconstruction graph convolutional"
        },
        {
          "5. REFERENCES": "Syst., 2023.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "network for EEG emotion and cognition recognition,”"
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "IEEE Trans. Neural Syst. Rehabil. Eng., 2023."
        },
        {
          "5. REFERENCES": "[7] Ze\nLiu,\nYutong\nLin,\nYue Cao,\nHan Hu,\nYixuan",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "Wei, Zheng Zhang,\nStephen Lin,\nand Baining Guo,",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[18] Qingshan\nShe, Chenqi Zhang,\nFeng\nFang, Yuliang"
        },
        {
          "5. REFERENCES": "“Swin transformer: Hierarchical vision transformer us-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Ma,\nand Yingchun Zhang,\n“Multisource\nassociate"
        },
        {
          "5. REFERENCES": "ing shifted windows,” in Proceedings of the IEEE/CVF",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "domain adaptation for cross-subject and cross-session"
        },
        {
          "5. REFERENCES": "international conference on computer vision, 2021, pp.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "EEG emotion recognition,” IEEE Trans. Instrum. Meas.,"
        },
        {
          "5. REFERENCES": "10012–10022.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "2023."
        },
        {
          "5. REFERENCES": "[8] Sinno Jialin Pan and Qiang Yang, “A survey on transfer",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[19] Li-Ming Zhao, Xu Yan, and Bao-Liang Lu, “Plug-and-"
        },
        {
          "5. REFERENCES": "learning,”\nIEEE Trans. Knowl. Data Eng., vol. 22, no.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "play domain adaptation for\ncross-subject EEG-based"
        },
        {
          "5. REFERENCES": "10, pp. 1345–1359, 2009.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "emotion recognition,” in Proceedings of the AAAI Con-"
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "ference on Artificial Intelligence, 2021, vol. 35, pp. 863–"
        },
        {
          "5. REFERENCES": "[9] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bern-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "870."
        },
        {
          "5. REFERENCES": "hard Sch¨olkopf,\nand Alex Smola,\n“A kernel method",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "for the two-sample-problem,” Adv. Neural Inf. Process.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[20] Xiaobing Du, Cuixia Ma, Guanhua Zhang,\nJinyao Li,"
        },
        {
          "5. REFERENCES": "Syst., vol. 19, 2006.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Yu-Kun Lai, Guozhen Zhao, Xiaoming Deng, Yong-"
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Jin Liu, and Hongan Wang,\n“An efficient LSTM net-"
        },
        {
          "5. REFERENCES": "[10] Alireza Amirshahi, Anthony Thomas, Amir Aminifar,",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "work for emotion recognition from multichannel EEG"
        },
        {
          "5. REFERENCES": "Tajana Rosing, and David Atienza, “M2d2: Maximum-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "signals,”\nIEEE Trans. Affect. Comput., vol. 13, no. 3,"
        },
        {
          "5. REFERENCES": "mean-discrepancy decoder for temporal\nlocalization of",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "pp. 1528–1540, 2020."
        },
        {
          "5. REFERENCES": "IEEE J. Biomed. Health In-\nepileptic brain activities,”",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "form., vol. 27, no. 1, pp. 202–214, 2022.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "[21] David Vazquez, Antonio M Lopez, Javier Marin, Daniel"
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "Ponsa, and David Geronimo,\n“Virtual and real world"
        },
        {
          "5. REFERENCES": "[11] Eric Tzeng,\nJudy Hoffman, Kate Saenko,\nand Trevor",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "IEEE Trans. Pat-\nadaptation for pedestrian detection,”"
        },
        {
          "5. REFERENCES": "Darrell,\n“Adversarial discriminative domain adapta-",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "tern Anal. Mach.\nIntell., vol. 36, no. 4, pp. 797–809,"
        },
        {
          "5. REFERENCES": "tion,”\nin IEEE Comput. Soc. Conf. Comput. Vision and",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        },
        {
          "5. REFERENCES": "",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": "2013."
        },
        {
          "5. REFERENCES": "Pattern Recognition (CVPR), 2017, pp. 7167–7176.",
          "[12] Hauke Dose, Jakob S Møller, Helle K Iversen, and Sada-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Brain-computer interfaces in medicine",
      "authors": [
        "Jerry Shih",
        "Dean Krusienski",
        "Jonathan Wolpaw"
      ],
      "year": "2012",
      "venue": "Mayo clinic proceedings"
    },
    {
      "citation_id": "3",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "Wei-Long Zheng",
        "Jia-Yi Zhu",
        "Bao-Liang Lu"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "4",
      "title": "Transferring subspaces between subjects in brain-computer interfacing",
      "authors": [
        "Wojciech Samek",
        "Klaus-Robert Frank C Meinecke",
        "Müller"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Biomed. Eng"
    },
    {
      "citation_id": "5",
      "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
      "authors": [
        "Robin Tibor"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "6",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "J. Neural Eng"
    },
    {
      "citation_id": "7",
      "title": "A hybrid end-to-end spatio-temporal attention neural network with graph-smooth signals for EEG emotion recognition",
      "authors": [
        "Shadi Sartipi",
        "Mastaneh Torkamani-Azar",
        "Mujdat Cetin"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Cogn. Develop. Syst"
    },
    {
      "citation_id": "8",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "9",
      "title": "A survey on transfer learning",
      "authors": [
        "Jialin Sinno",
        "Qiang Pan",
        "Yang"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Knowl. Data Eng"
    },
    {
      "citation_id": "10",
      "title": "A kernel method for the two-sample-problem",
      "authors": [
        "Arthur Gretton",
        "Karsten Borgwardt",
        "Malte Rasch"
      ],
      "year": "2006",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "11",
      "title": "M2d2: Maximummean-discrepancy decoder for temporal localization of epileptic brain activities",
      "authors": [
        "Alireza Amirshahi",
        "Anthony Thomas",
        "Amir Aminifar",
        "Tajana Rosing",
        "David Atienza"
      ],
      "year": "2022",
      "venue": "IEEE J. Biomed. Health Inform"
    },
    {
      "citation_id": "12",
      "title": "Adversarial discriminative domain adaptation",
      "authors": [
        "Eric Tzeng",
        "Judy Hoffman",
        "Kate Saenko",
        "Trevor Darrell"
      ],
      "year": "2017",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "An end-to-end deep learning approach to MI-EEG signal classification for BCIs",
      "authors": [
        "Hauke Dose",
        "Jakob Møller",
        "Sadasivan Helle K Iversen",
        "Puthusserypady"
      ],
      "year": "2018",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "14",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Adv. Neural Inf. Process. Syst"
    },
    {
      "citation_id": "15",
      "title": "Maximum classifier discrepancy for unsupervised domain adaptation",
      "authors": [
        "Kuniaki Saito",
        "Kohei Watanabe",
        "Yoshitaka Ushiku",
        "Tatsuya Harada"
      ],
      "year": "2018",
      "venue": "IEEE Comput. Soc. Conf. Comput. Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "16",
      "title": "Moment matching for multi-source domain adaptation",
      "authors": [
        "Xingchao Peng",
        "Qinxun Bai",
        "Xide Xia",
        "Zijun Huang",
        "Kate Saenko",
        "Bo Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "17",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Auton. Ment. Dev"
    },
    {
      "citation_id": "18",
      "title": "MSFR-GCN: A multi-scale feature reconstruction graph convolutional network for EEG emotion and cognition recognition",
      "authors": [
        "Haohao Deng Pan",
        "Feifan Zheng",
        "Yu Xu",
        "Zhe Ouyang",
        "Chu Jia",
        "Hong Wang",
        "Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Neural Syst. Rehabil. Eng"
    },
    {
      "citation_id": "19",
      "title": "Multisource associate domain adaptation for cross-subject and cross-session EEG emotion recognition",
      "authors": [
        "Qingshan She",
        "Chenqi Zhang",
        "Feng Fang",
        "Yuliang Ma",
        "Yingchun Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "20",
      "title": "Plug-andplay domain adaptation for cross-subject EEG-based emotion recognition",
      "authors": [
        "Li-Ming Zhao",
        "Xu Yan",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "An efficient LSTM network for emotion recognition from multichannel EEG signals",
      "authors": [
        "Xiaobing Du",
        "Cuixia Ma",
        "Guanhua Zhang",
        "Jinyao Li",
        "Yu-Kun Lai",
        "Guozhen Zhao",
        "Xiaoming Deng",
        "Yong-Jin Liu",
        "Hongan Wang"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "22",
      "title": "Virtual and real world adaptation for pedestrian detection",
      "authors": [
        "David Vazquez",
        "Antonio Lopez",
        "Javier Marin",
        "Daniel Ponsa",
        "David Geronimo"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    }
  ]
}