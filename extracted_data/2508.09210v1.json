{
  "paper_id": "2508.09210v1",
  "title": "Mme-Emotion: A Holistic Evaluation Benchmark For Emotional Intelligence In Mul-Timodal Large Language Models",
  "published": "2025-08-11T03:14:55Z",
  "authors": [
    "Fan Zhang",
    "Zebang Cheng",
    "Chong Deng",
    "Haoxuan Li",
    "Zheng Lian",
    "Qian Chen",
    "Huadai Liu",
    "Wen Wang",
    "Yi-Fan Zhang",
    "Renrui Zhang",
    "Ziyu Guo",
    "Zhihong Zhu",
    "Hao Wu",
    "Haixin Wang",
    "Yefeng Zheng",
    "Xiaojiang Peng",
    "Xian Wu",
    "Kun Wang",
    "Xiangang Li",
    "Jieping Ye",
    "Pheng-Ann Heng"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advances in multimodal large language models (MLLMs) have catalyzed transformative progress in affective computing, enabling models to exhibit emergent emotional intelligence. Despite substantial methodological progress, current emotional benchmarks remain limited, as it is still unknown: (a) the generalization abilities of MLLMs across distinct scenarios, and (b) their reasoning capabilities to identify the triggering factors behind emotional states. To bridge these gaps, we present MME-Emotion, a systematic benchmark that assesses both emotional understanding and reasoning capabilities of MLLMs, enjoying scalable capacity, diverse settings, and unified protocols. As the largest emotional intelligence benchmark for MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific questioning-answering (QA) pairs, spanning broad scenarios to formulate eight emotional tasks. It further incorporates a holistic evaluation suite with hybrid metrics for emotion recognition and reasoning, analyzed through a multi-agent system framework. Through a rigorous evaluation of 20 advanced MLLMs, we uncover both their strengths and limitations, yielding several key insights: ❶ Current MLLMs exhibit unsatisfactory emotional intelligence, with the best-performing model achieving only 39.3% recognition score and 56.0% Chain-of-Thought (CoT) score on our benchmark. ❷ Generalist models (e.g., Gemini-2.5-Pro) derive emotional intelligence from generalized multimodal understanding capabilities, while specialist models (e.g., R1-Omni) can achieve comparable performance through domain-specific post-training adaptation. By introducing MME-Emotion, we hope that it can serve as a foundation for advancing MLLMs' emotional intelligence in the future. Project Page: https://mme-emotion.github.io/.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing  (Picard, 2000; Tao & Tan, 2005 ) is a fascinating interdisciplinary field seeking to bridge the gap between human emotional intelligence and machine capabilities, with significant applications in education  (Wu et al., 2016; Yadegaridehkordi et al., 2019) , healthcare  (Yannakakis, 2018; Liu et al., 2024b) , and human-robot interaction  (Filippini et al., 2020; Gervasi et al., 2023) . Catalyzed by emergent multimodal large language models (MLLMs)  (Wu et al., 2023; Zhang et al., 2024b) , research focus has increasingly shifted from exploring emotion recognition capability  (Kosti et al., 2017)  to investigating the interpretability of clues behind emotional states  (Lian et al., 2024c) .\n\nWhile a growing body of methodological research  (Cheng et al., 2024a; Zhao et al., 2025b; Lian et al., 2024c)  explores the use of multimodal  (audio, visual, and textual)  data to boost emotional Going beyond this, we provide a holistic evaluation suite for assessing the capabilities of MLLMs in emotion recognition and reasoning using unified protocols across all sub-tasks within MME-Emotion. For each question, we employ a multi-agent system framework to enable automated evaluation of MLLMs' responses with an MLLM-as-judge strategy. The visual clues, extracted audio clues, groundtruth emotion labels, and partitioned answer steps of a specific MLLM are fed into a GPT-based judge agent to evaluate the performance using three metrics: recognition score, reasoning score, and Chain-of-Thought (CoT) score. To further validate our evaluation approach, we also ask five human experts to cross-evaluate the performance of MLLMs on sampled data and manually annotated scores at each answer step. The comparison between GPT and expert scores demonstrates high consistency across multiple statistical metrics, confirming the effectiveness of our automated evaluation strategy.\n\nApplying our evaluation suite to 20 state-of-the-art MLLMs, we uncover both their strengths and limitations, yielding the following key insights: ❶ The overall emotional intelligence of current MLLMs remains far from satisfactory (Figure  2 ). Even the top-performing model (Gemini-2.5-Pro) achieves merely 39.3% recognition score and 56.0% CoT score on our benchmark, respectively. The average performance across all evaluated MLLMs (29.4% recognition score, 49.5% reasoning score, and 39.5% CoT score) indicates there is still substantial room for improvement. ❷ While generalist models (e.g., Gemini-2.5-Pro  (Google, 2025c)  and  GPT-4o (OpenAI, 2024) ) derive emotional intelligence from generalized multimodal understanding capabilities, specialist models (e.g., R1-Omni  (Zhao et al., 2025a)  and Audio-Reasoner  (Xie et al., 2025) ) can achieve comparable performance through emotion-specific post-training adaptation techniques, such as supervised fine-tuning (SFT)  (Zhang et al., 2023b; Liu et al., 2023b)  and human preference alignment  (Guo et al., 2025; Liu et al., 2025) . ❸ Generally, response step count positively correlates with model performance, underscoring the necessity for equipping MLLMs with emotion reasoning capabilities in future development.\n\nThe main contributions of this paper can be summarized as follows:\n\n• Comprehensive Benchmark: We introduce MME-Emotion, the largest benchmark for emotional intelligence in MLLMs that encompasses eight emotional tasks and 27 distinct scenarios, enabling a comprehensive evaluation of MLLMs' generalization capabilities across diverse settings.\n\n• Holistic Evaluation Suite: We provide a multi-agent system framework that can automatically assess the emotion recognition and reasoning abilities of MLLMs using three unified metrics across all tasks. The validity of our evaluation strategy is also fully verified by five human experts.\n\n• Empirical Analysis: Through a rigorous evaluation of 20 advanced open-source and closed-source MLLMs on MME-Emotion, along with in-depth analysis, we reveal their strengths and limitations, paving the way for future research efforts to advance emotional intelligence in MLLMs.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotional Intelligence In Multimodal Large Language Models",
      "text": "The success of MLLMs  (Liu et al., 2023a; Zhu et al., 2023)  has advanced affective computing, prompting growing interest in equipping these models with emotional intelligence  (Cheng et al., 2024a; Zhao et al., 2025b) . On the one hand, general MLLMs (such as the Gemini  (Team et al., 2023) , GPT  (Achiam et al., 2023) , and Qwen-VL  (Bai et al., 2023)  series) have demonstrated strong multimodal understanding capabilities. Thanks to their powerful LLM backbones and extensive pretraining on diverse datasets, emotional intelligence emerges as a byproduct of these general models.\n\nOn the other hand, MLLMs with relatively small parameter sizes (ranging from 0.5B to 7B) have not yet shown strong general multimodal understanding capabilities, primarily due to limitations in data scale and model capacity. However, with post-training adaptation strategies, it is still possible to elicit strong performance on emotion-related tasks. For example, Emotion-LLaMA  (Cheng et al., 2024a)  employs emotion-specific encoders to project video, audio, and text modalities into a unified LLM space, followed by instruction tuning to activate emotional intelligence. R1-Omni  (Zhao et al., 2025a)  enhances emotional reasoning capabilities in MLLMs through reinforcement learning with verified feedback (RLVR). AffectGPT  (Lian et al., 2024c) , by training on a large-scale emotional dataset and introducing a pre-fusion projector to integrate multimodal emotional signals, builds a powerful emotion-specialized model. However, these models are often evaluated only on specific tasks and limited scenarios, leaving it unclear how they perform under open and fair evaluation settings.   (Sabour et al., 2024)  1 Text ✓ ✗ ✓ ✗ ✗ MOSABench  (Song et al., 2024)  1 Image ✓ ✗ ✓ ✓ ✗ MM-InstructEval  (Yang et al., 2025)  6 Image ✓ ✗ ✓ ✗ ✗ IEMOCAP  (Busso et al., 2008)  1 Video ✓ ✗ ✗ ✗ ✗ MC-EIU  (Liu et al., 2024a)  2 Video ✓ ✗ ✗ ✗ ✗ OV-MER  (Lian et al., 2024a)  1 Video ✓ ✗ ✓ ✗ ✓ EmoBench-M  (Hu et al., 2025)  3 Video ✓ ✗ ✓ ✓ ✗ MER-UniBench  (Lian et al., 2024c)  3",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Benchmarks For Emotional Intelligence",
      "text": "Numerous efforts  (Busso et al., 2008; Li et al., 2017)  have been made to establish an open and fair environment for evaluating emotional intelligence in artificial intelligence (AI) models. However, most existing datasets and benchmarks were developed before the emergence of LLMs and MLLMs, primarily aimed at assessing the performance of traditional machine learning and deep learning models. Although some recent studies  (Lian et al., 2024c; Hu et al., 2025)  have attempted to adapt these datasets for MLLMs using prompt templates, a common limitation still remains: they focus solely on evaluating emotion recognition capabilities, while overlooking the models' abilities to reason about emotional cues. To address this gap, we propose an automated evaluation strategy, along with a newly introduced metric termed reasoning score, to quantify the reasoning abilities of MLLMs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mme-Emotion",
      "text": "In this section, we first introduce the data curation process, followed by an elaboration on our evaluation suite, including a multi-agent system framework and three unified metrics. As shown in Table  1 , compared with existing emotion-related benchmarks, MME-Emotion stands out as the only one that simultaneously accounts for both recognition accuracy and reasoning quality.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Benchmark Construction",
      "text": "To curate data and construct our benchmark while reducing annotation costs, we collect videos and their corresponding emotion labels from publicly available resources. For longer videos that exhibit emotional shifts over time, we segment them into shorter video clips based on timestamps and the consistent emotion labels within specific intervals. We then convert various tasks into a QA format using prompt templates. Given that current MLLMs still lack sufficient emotional intelligence to handle open-ended tasks, we include all candidate emotion labels as a predefined label set within the prompt, enabling the model to make predictions in a closed-set setting. By aggregating and resampling data from multiple public datasets  (Busso et al., 2008; Poria et al., 2018; Jiang et al., 2020; Liu et al., 2022b; Lee et al., 2019; Liu et al., 2024a; Feng et al., 2024; Luo et al., 2020; Lian et al., 2024b; Zadeh et al., 2018; 2016; Liu et al., 2022a) , we compile a total of 6,500 QA pairs along with their corresponding video clips. To prevent potential data leakage, all involved samples are exclusively drawn from the test sets. These samples span eight emotion-related tasks across 27 distinct scenarios, including emotion recognition in the lab (ER-Lab), emotion recognition in the wild (ER-Wild), emotion recognition under noise (Noise-ER), fine-grained emotion recognition (FG-ER), multi-label emotion recognition (ML-ER), sentiment analysis (SA), fine-grained sentiment analysis (FG-SA), and intent recognition (IR). By constructing the MME-Emotion benchmark, we provide a comprehensive and systematic environment for assessing emotional intelligence in MLLMs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Suite",
      "text": "Evaluation Strategy. Evaluating the reasoning capabilities of MLLMs' emotional intelligence is a challenging task. To address this, we propose an annotation-free strategy based on a multi-agent",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluated Mllms",
      "text": "Video-LLaVA AffectGPT ......",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "User:",
      "text": "Please analyze which features in the audio reflect that the speaker is feeling {Emotion Label}. Pay special attention to emotional features such as crying sounds, laughter, changes in tone, speech rate, pauses, emphasis and stress, voice trembling, and other emotional characteristics.\n\n{Emotion Label}: Anger",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Audio Agent:",
      "text": "The speaker exhibits intense anger through their aggressive tone, loud voicing, and the use of forceful language indicating they are upset or agitated about the situation involving Rachel.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Extract Audio Clues",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Judge Agent",
      "text": "Human Verification",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sampling",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluating",
      "text": "Figure  3 : Illustration of Our Evaluation Strategy. We leverage a multi-agent system framework to assess the recognition and reasoning capabilities of MLLMs across different tasks with three unified metrics. To validate the effectiveness of our MLLM-as-judge strategy, we further compare the results of the judge agent on sampled data against results cross-evaluated by five human experts.\n\nsystem (Figure  3 ). This approach allows us to assess the reasoning performance of MLLMs without the need for manually annotated ground-truth reasoning steps. Specifically, we first obtain the MLLM's response to a given question, and then employ a unimodal step agent to automatically extract key reasoning steps from the model's answer. This process can be formulated as:\n\nwhere Q, V , and A are short for question, video, and answer, respectively. P s and S denote the prompt for the step agent and the corresponding answer steps. Here, we opt for  GPT-4.1 (OpenAI, 2025)  as our step agent. Note that we refrain from incorporating additional elements-such as the original question or the video content-during the step partition process, as we aim to avoid introducing external biases that could influence the identification of reasoning steps. Afterward, we employ another powerful multimodal agent as the judge to rate the performance of the extracted answer steps. To ensure an accurate and comprehensive assessment, it is essential to provide the judge agent with complete multimodal information, thereby minimizing the risk of misjudgment due to missing context. However, mainstream multimodal agents such as  GPT-4o (OpenAI, 2024)  are currently unable to process all modalities simultaneously. To address this limitation, we propose a divide-and-conquer perspective. Specifically, for visual clues, we can directly convert the original video into frames and feed them into the judge agent. For audio clues, we utilize a separate, powerful audio-language model as an audio agent to extract relevant audio information. Once we obtain the visual and audio clues, we combine them with the answer steps and the ground-truth emotion labels and input them into the judge agent for final evaluation. This process can be formulated as:\n\nwhere C v , C a , and Y denote visual clues, audio clues, and emotion labels, respectively. P a and P j are the prompts for extracting audio clues and evaluating the performance. Here, we employ Qwen2-Audio  (Chu et al., 2024)  as the audio agent and  GPT-4o (OpenAI, 2024)  as the judge agent. We adopt three unified evaluation metrics across all tasks: recognition score (Rec-S), reasoning score (Rea-S), and Chain-of-Thought score (CoT-S), each of which will be detailed in the following section. Evaluation Metric. Next, we provide a detailed introduction to the evaluation metrics employed in this paper. During the earlier step extraction process from MLLM answers, we consistently place the task-specific prediction in the final step, while treating all other steps as the reasoning process. To compute the recognition score, we compare the final prediction step against the ground-truth emotion labels. For tasks involving a single emotion label, we adopt standard accuracy as the recognition score, following conventional evaluation practices  (Li & Deng, 2020; Tzirakis et al., 2017) . For tasks involving multiple emotion labels, the recognition score is defined as the ratio between the number of correctly predicted emotions and the total number of ground-truth emotions. We then derive the reasoning score from the extracted reasoning steps. Each step is treated as a binary classification problem, where the judge agent determines whether the reasoning is correct or not.\n\nTo eliminate potential bias caused by varying step lengths, we compute the reasoning score as the average correctness across all reasoning steps. Finally, we obtain the CoT score by taking a weighted combination of the recognition score and the reasoning score, providing a holistic measure of both recognition accuracy and reasoning quality. The CoT score is defined as:\n\nwhere α ∈ [0, 1] is a hyperparameter that controls the relative contribution of the recognition score and reasoning score in the overall evaluation process. By default, we set α = 0.5.\n\nHuman Verification. To evaluate the reliability of our MLLM-as-judge evaluation strategy, we recruited five experts to perform manual validation. Specifically, the human experts annotated 373 reasoning steps drawn from 100 randomly sampled questions and their corresponding answers from evaluated MLLMs. Then we assessed the consistency between the GPT scores and the expert scores.\n\nAs shown in the bottom-right panel of Figure  3 , the Spearman's rank correlation coefficient  (Spearman, 1961)  between the two sets of scores is 0.9530, indicating a strong positive correlation. In terms of inter-rater reliability, the two sets of scores achieve a Cohen's Kappa coefficient (McHugh, 2012) of 0.8626, which falls into the category of almost perfect agreement. Additionally, the intra-class correlation coefficient (ICC) (Koch, 2004) of 0.9704 further confirms excellent reliability between the two scoring methods. These results collectively demonstrate that our MLLM-as-judge strategy aligns closely with human judgment and serves as a highly reliable evaluation approach.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments 4.1 Experimental Setup",
      "text": "We evaluate the performance on a total of 20 cutting-edge MLLMs, including Qwen2-Audio  (Chu et al., 2024) , Audio-Reasoner  (Xie et al., 2025) , Qwen2-VL-7B/72B  (Wang et al., 2024) , Qwen2.5-VL-7B/72B  (Bai et al., 2025) , QVQ  (Team, 2024) , Video-LLaVA  (Lin et al., 2023) , Video-LLaMA  (Zhang et al., 2023a) , Video-LLaMA2  (Cheng et al., 2024b) , Qwen2.5-Omni  (Xu et al., 2025) , Emotion-LLaMA  (Cheng et al., 2024a) , HumanOmni  (Zhao et al., 2025b) , R1-Omni  (Zhao et al., 2025a) , AffectGPT  (Lian et al., 2024c) ,  GPT-4o (OpenAI, 2024) ,  GPT-4.1 (OpenAI, 2025) , Gemini-2.0-Flash  (Google, 2025a) , Gemini-2.5-Flash  (Google, 2025b) , and Gemini-2.5-Pro  (Google, 2025c) .\n\nFor most of the open-sourced MLLMs, we use the vLLM  (Kwon et al., 2023)  framework to perform inference on our benchmark. For models that are not compatible with the vLLM framework, we conduct experiments using the official open-source code provided by the authors. For closed-sourced MLLMs, inference is conducted directly via their official APIs. All the experiments are conducted under zero-shot settings. To ensure a fair comparison across models, we maintain consistent prompting wherever possible, with only minor modifications when necessary. For example, for audio-language models, the word \"video\" in prompts is replaced with \"audio\"; for vision-language models that do not support video input but accept multiple images, \"video\" is replaced with \"video frames\".",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Main Results",
      "text": "We showcase the overall performance comparison on MME-Emotion, covering three performance metrics: recognition score (Rec-S), reasoning score (Rea-S), and Chain-of-Thought score (CoT-S) as well as two response length metrics: average step count (Avg Step) and average token count (Avg Token), as shown in Table  2 . Notably, even the top-performing models, Gemini-2.5-Pro  (Google, 2025c) , Audio-Reasoner  (Xie et al., 2025) , and  GPT-4o (OpenAI, 2024) , achieve CoT scores of only 56.0%, 54.8%, and 53.8%, respectively. All evaluated MLLMs report recognition scores below 40%, and most closed-source MLLMs also score below 40% in terms of CoT quality. This performance landscape highlights the challenging nature of the MME-Emotion benchmark. It also suggests that even state-of-the-art MLLMs are still in the early stages of developing emotional intelligence. We also observe that current MLLMs have yet to fully and effectively leverage multimodal information.\n\nAs shown in Table  2 , Audio-Reasoner, which processes only audio and textual modalities, achieves a strong CoT score of 54.8%. Similarly, many closed-source vision-language models, utilizing only visual and textual information, also demonstrate competitive performance. In contrast, omnimodal models, which are designed to process audio, visual, and textual information simultaneously, often exhibit noticeable performance drops. This phenomenon points to two potential issues: ❶ Multimodal data often contains redundant or inconsistent emotional clues across different modalities. ❷ Existing omnimodal models still lack effective strategies for robust multimodal emotional clues fusion.\n\nIn addition, Figure  4  presents a fine-grained comparison of 20 MLLMs across eight distinct emotional tasks, evaluated using three performance metrics. The results reveal notable differences in task difficulty. For more challenging tasks such as FG-SA and IR, even the top-performing MLLMs achieve recognition scores of only around 30%. When comparing the results of two related tasks, ER-Wild and ER-Lab, we observe a clear performance gap across different MLLMs. This discrepancy suggests that most existing MLLMs are primarily trained on in-the-wild data and therefore struggle to generalize effectively to controlled, in-the-lab settings, leading to noticeable performance degradation. These findings further underscore the discriminative capability of the MME-Emotion benchmark.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Observations And Insights",
      "text": "Delving into the experimental results, we can derive the following key observations and insights.\n\nObs. 1: Emotional intelligence in MLLMs can be bootstrapped through diverse approaches.\n\nOn the one hand, generalist MLLMs, such as the Gemini and GPT series, are equipped with large model capacities and trained on massive datasets. As a result, they exhibit strong general understanding capabilities and perform well across various tasks  (Li et al., 2024; Fu et al., 2023; Zhang et al., 2024c; Fu et al., 2025; Jiang et al., 2025) . Although these models have not been explicitly adapted to emotion-specific domains  (Fu et al., 2024) , our experimental results show that they can generalize from general intelligence to emotional intelligence, illustrating one viable path forward.\n\nOn the other hand, specialist models, such as AffectGPT and R1-Omni, may not match generalist models in broad understanding domains due to the limitation of scaling laws  (Kaplan et al., 2020; Zhang et al., 2024a) . However, by constructing large-scale emotional data and applying post-training adaptation, they can achieve comparable performance against generalist models. These findings demonstrate that there is no one-size-fits-all solution: emotional intelligence can be bootstrapped through a variety of strategies, opening the door to a broad range of future research directions.\n\nObs. 2: Encouraging deeper reasoning can enhance emotional intelligence in MLLMs. In Figure  5 , there is a general positive correlation between the average number of answer steps and overall performance. While certain non-reasoning MLLMs, such as Emotion-LLaMA, HumanOmni, and Qwen2-VL-72B, can generate direct answers without explicit reasoning through instruction tuning, most reasoning-capable MLLMs demonstrate a strong synergy between reasoning and recognition: improvements in one metric are often accompanied by gains in the other. These findings suggest a promising research direction for advancing the emotional intelligence of MLLMs: encouraging deeper reasoning by incentivizing models to reflect and identify the underlying triggers of emotions. Emotion Labels: Fear.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Examine facial clues in each frame, focusing on eyes, eyebrows, and mouth positions.\n\nStep 2: Identify key features such as wide-open eyes, raised eyebrows, and a dropped jaw across the clearer frames.\n\nStep 3: Compare observed features with typical expressions of candidate emotions (neutral, happy, angry, sad, fear, surprise, disgust).\n\nStep 4: Determine that the combination of wide eyes, raised eyebrows, and an open mouth best matches the expression of surprise.\n\nStep 5: The predicted emotion is surprise.\n\nRating:\n\nVideo-LLaMA2:\n\nStep 1: Observe the young man's behavior and facial expressions in various situations within the video.\n\nStep 2: Note that despite being in potentially stressful scenarios (e.g., a spider crawling on his face in a dark cave), he does not display any strong emotional reactions.\n\nStep 3: Conclude that the young man consistently shows no visible emotional expression throughout the video.\n\nStep 4: The predicted emotion is neutral.\n\nQwen2.5-Omni:\n\nStep 1: Analyze the person's eyes for emotional clues, noting that wide open eyes often indicate surprise or shock.\n\nStep 2: Examine the mouth, observing that a slightly open mouth can be a sign of surprise or fear.\n\nStep 3: Based on the visual clues, determine that the predicted emotion is surprise. Obs. 3: Limited visual perception capabilities constrain the emotional intelligence in MLLMs.\n\nAs illustrated in Figure  6 , the failure cases of Video-LLaMA2 and Qwen2.5-Omni reveal that both models struggle primarily due to insufficient visual perception capabilities. In this example, Video-LLaMA2 fails to accurately capture subtle facial expression changes required and thus incorrectly classifies the emotion as neutral. Qwen2.5-Omni demonstrates notable progress by narrowing down the candidate emotions to surprise and fear, eliminating many irrelevant options. However, when it comes to making a fine-grained distinction between these two emotions, it still fails to recognize the subject's fearful expression, leading to an incorrect prediction. These findings highlight the importance of improving the fine-grained visual perception capabilities of MLLMs, with particular emphasis on the accurate interpretation of facial expressions and body movements. Such advancements are essential for the continued development of emotional intelligence in MLLMs.\n\nObs. 4: Incomplete multimodal information limits the emotional intelligence in MLLMs. The failure case of Gemini-2.5-Pro in Figure  6  reveals that the error primarily stems from the absence of audio information. In this example, the character's fearful emotion is clearly conveyed through audio clues. However, the model fails to recognize it due to its inability to jointly process both audio and visual modalities from the video clip. Most current generalist MLLMs are still limited in their capacity to integrate audio and visual information simultaneously, leading to misinterpretations when critical emotional signals are missing. These findings suggest that although current leading models are primarily vision-language models, the future of emotional intelligence lies in advancing more powerful omnimodal models capable of jointly processing audio, visual, and textual information.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced MME-Emotion, a comprehensive multi-task benchmark for evaluating emotional intelligence in MLLMs, accompanied by a holistic evaluation suite. The assessment process was fully automated within a multi-agent system framework and thoroughly validated by human experts. Through rigorous evaluation using three unified metrics and in-depth empirical analysis, we uncovered both the strengths and limitations of cutting-edge MLLMs, laying a solid foundation for future research aimed at advancing emotional intelligence in MLLMs. In future work, we intend to build upon the observations and insights derived from this paper and systematically address the existing shortcomings of MLLMs, developing a powerful emotion-specialist model.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Contents",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A Limitations And Social Impact",
      "text": "While MME-Emotion represents a significant step forward in the field of affective computing and multimodal large language models (MLLMs), several limitations remain worth acknowledging:\n\n• First, although we provide a multi-task and multi-scenario benchmark for evaluating emotional intelligence in MLLMs, it would be beneficial to incorporate task difficulty classification. Similar to the categorization of mathematical problems  (Hendrycks et al., 2021)  into primary school, high school, college, and Olympiad levels, assigning a difficulty rating or level to each sample in our benchmark could offer a more nuanced understanding of MLLMs' capabilities in handling affective computing tasks across varying levels of complexity.\n\n• Second, since our benchmark is constructed from publicly available video datasets, the included data covers a wide range of multilingual scenarios. However, we do not currently distinguish between different languages in the benchmark, nor do we analyze model performance across different linguistic contexts. In future work, we aim to extend our evaluation from a multilingual perspective, enabling a more comprehensive assessment of emotional intelligence in MLLMs.\n\nOverall, we present a systematic benchmark for evaluating both the recognition and reasoning capabilities of MLLMs in emotion-related tasks, which we believe will serve as a valuable resource for both the MLLM and affective computing communities. We hope this work will inspire growing research efforts aimed at bootstrapping emotional intelligence in MLLMs, ultimately advancing downstream applications in education, healthcare, and human-robot interaction.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "B More Related Works B.1 Datasets And Benchmarks For Multimodal Large Language Models",
      "text": "With the rise of large language models (LLMs) and the growing popularity of multimodal large language models (MLLMs), an increasing number of studies have focused on evaluating MLLM performance across a wide range of domains. By collecting data and employing human annotations, MME  (Fu et al., 2023)  evaluates the general understanding capabilities of MLLMs across 14 subtasks, ranging from perception tasks to cognition tasks. Video-MME  (Fu et al., 2025)  assesses the video analysis capabilities of MLLMs using a diverse and high-quality collection of video data. MME-RealWorld  (Zhang et al., 2024c)  aggregates a large set of high-resolution images, combined with manual annotations, to construct a suite of practical and challenging tasks, including optical character recognition (OCR) in the wild, remote sensing, and autonomous driving. FinMME  (Luo et al., 2025)  leverages annotations from 20 domain experts to comprehensively evaluate MLLMs' understanding capabilities in the financial domain. These benchmarks have significantly accelerated progress in MLLM development, enabling improved performance in both general-purpose and domain-specific scenarios, ultimately contributing to a wide array of real-world applications.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C Implementation Details C.1 Evaluated Mllms",
      "text": "We evaluated a total of 20 MLLMs, comprising 15 closed-source and 5 open-source models. A brief introduction to each model is provided below:\n\n• Qwen2-Audio  (Chu et al., 2024)  is the latest open-source large audio-language model developed by Alibaba Group. It is designed to handle a wide range of audio inputs and can perform detailed audio analysis or generate direct textual responses based on spoken instructions. Qwen2-Audio achieves state-of-the-art performance across multiple audiorelated tasks, including automatic speech recognition (ASR), speech-to-text translation (S2TT), speech emotion recognition (SER), and vocal sound classification (VSC).\n\n• Audio-Reasoner  (Xie et al., 2025 ) is a large audio-language model built upon Qwen2-Audio, enhanced with advanced reasoning capabilities. By curating structured chain-of-thought (CoT) data and applying supervised fine-tuning, it achieves superior performance and sets a new state-of-the-art in audio reasoning tasks.\n\n• Qwen2-VL  (Wang et al., 2024 ) is a newly developed series of large vision-language models from Alibaba Group. A key innovation in Qwen2-VL is the incorporation of a naive dynamic resolution mechanism, which allows the model to flexibly adapt to images of different resolutions by converting them into variable numbers of visual tokens. This design enhances both the efficiency and precision of visual representations, in a manner that closely mirrors human visual perception. Consequently, Qwen2-VL series achieve cutting-edge results in visual understanding tasks and exhibits strong capabilities in complex reasoning and decision-making.\n\n• Qwen2.5-VL  (Bai et al., 2025)  series, developed by Alibaba Group, represents one of the most advanced large vision-language model families to date. Leveraging a streamlined and efficient vision encoder alongside dynamic resolution and frame rate training strategies, the Qwen2.5-VL models set new state-of-the-art performance on a wide range of visual tasks.\n\nIn particular, they demonstrate exceptional capabilities in fine-grained video understanding.\n\n• QVQ  (Team, 2024 ) is a large vision-language model developed by Alibaba Group based on the Qwen2-VL architecture, with a particular focus on strengthening visual reasoning capabilities. It represents a significant advancement in AI's ability to perform visual understanding and solve complex problems. QVQ achieves remarkable performance, especially in reasoning-intensive tasks such as mathematical problem solving, marking a notable breakthrough in the field.\n\n• Video-LLaVA  (Lin et al., 2023 ) is a large vision-language model capable of processing both images and videos. Its core innovation lies in projecting image and video features into a unified representation space, enabling the language model to learn cross-modal interactions from a consistent visual embedding. Video-LLaVA demonstrates strong performance across both image understanding and video analysis tasks.\n\n• Video-LLaMA  (Zhang et al., 2023a)  is a large omnimodal model designed to process both visual and audio information through dedicated visual and audio branches. Unlike many existing models that are limited to a single modality, Video-LLaMA enables simultaneous understanding of both visual and acoustic content within videos. By integrating multimodal data, it addresses critical limitations in current video understanding approaches.\n\n• Video-LLaMA2  (Cheng et al., 2024b)  builds upon its predecessor by integrating a customized spatio-temporal convolution connector (STC), which effectively captures the intricate spatial and temporal dynamics inherent in video data. Furthermore, the model incorporates an audio branch through joint training, allowing it to seamlessly fuse audio clues and significantly enhance its multimodal understanding capabilities.\n\n• Qwen2.5-Omni  (Xu et al., 2025)  is a flagship large omnimodal model developed by Alibaba Group. It adopts the Thinker-Talker architecture, specifically designed for comprehensive multimodal perception. The model can seamlessly process a wide range of input modalities, including text, images, audio, and video, while also supporting streaming text generation and natural speech synthesis outputs. Qwen2.5-Omni achieves strong performance across both unimodal and multimodal understanding tasks.\n\n• Emotion-LLaMA  (Cheng et al., 2024a ) is an omnimodal emotion-specialist model designed for multimodal emotion recognition. It leverages audio, visual, and textual encoders to capture comprehensive multimodal representations and employs a two-stage training strategy to enhance learning effectiveness. As a result, Emotion-LLaMA achieves strong performance across various emotion recognition tasks.\n\n• HumanOmni  (Zhao et al., 2025b)  is an omnimodal specialist model tailored for humancentric tasks, including emotion recognition. It utilizes three distinct visual projectors-facerelated, body-related, and interaction-related-combined with a learnable gating mechanism to capture fine-grained, task-relevant visual representations. This design enables HumanOmni to achieve strong performance across a wide range of human-centric downstream tasks.\n\n• R1-Omni  (Zhao et al., 2025a ) is an omnimodal emotion-specialist model built upon the foundation of HumanOmni. By combining reinforcement learning with verifiable reward (RLVR) and rule-based rewards, R1-Omni effectively enhances the model's reasoning capabilities on emotional tasks.\n\n• AffectGPT  (Lian et al., 2024c ) is an omnimodal emotion-specialist model. Leveraging a model-based crowdsourcing strategy, the authors curated a large-scale emotion understanding dataset rich in emotional cues. Leveraging this dataset for training, alongside the incorporation of pre-fusion operations to enhance multimodal integration, AffectGPT demonstrates robust and advanced capabilities in emotion understanding.\n\n• GPT-4o (OpenAI, 2024) is one of the latest MLLMs developed by OpenAI, providing convenient APIs for processing textual, visual, and audio modalities. It demonstrates stateof-the-art performance across a wide range of benchmarks, with significant advancements in visual perception, understanding, and reasoning. Equipped with a unified architecture that facilitates seamless cross-modal interaction, GPT-4o is both highly efficient and adaptable, making it well-suited for real-world multimodal applications.\n\n• GPT-4.1 (OpenAI, 2025) is a powerful and cost-effective MLLM released by OpenAI. It demonstrates significant improvements in programming capabilities and instruction following. Additionally, GPT-4.1 features an expanded context window supporting up to one million tokens, enabling enhanced long-context understanding that better leverages information to improve both efficiency and performance.\n\n• Gemini-2.0-Flash (Google, 2025a) supports input and output across multiple modalities, including images, videos, and audio, while maintaining efficient inference speed. By leveraging advanced reasoning capabilities along with extended context understanding, Gemini-2.0-Flash achieves strong performance on a variety of multimodal reasoning tasks.\n\n• Gemini-2.5-Flash  (Google, 2025b ) is a multimodal hybrid reasoning model that strikes a balance between performance and efficiency. By allowing users to selectively enable reasoning, it offers a flexible trade-off among performance, speed, and cost. Through finegrained control over reasoning tokens, Gemini-2.5-Flash achieves outstanding performance across a variety of multimodal understanding tasks.\n\n• Gemini-2.5-Pro  (Google, 2025c) , a recent release from Google DeepMind, advances multimodal large language modeling with enhanced capabilities in visual understanding. It supports longer context windows and improves the efficiency of cross-modal alignment.\n\nEquipped with robust reasoning skills, Gemini-2.5 Pro excels across a wide spectrum of tasks, such as programming, mathematics, and scientific problem solving.",
      "page_start": 16,
      "page_end": 18
    },
    {
      "section_name": "C.2 Prompt For Evaluated Mllms",
      "text": "We adopt unified prompts across all evaluated MLLMs to ensure a fair comparison, with slight modifications made for audio-language models, vision-language models, and omnimodal models to accommodate their specific input modalities. Examples of the prompts are provided below.\n\nPrompt for ER-Lab, ER-Wild, and Noise-ER\n\nAs an emotion recognition expert, identify the most prominent emotion displayed in the video. Candidate labels: ...... Please think step by step. Enclose your final answer within <answer></answer> tags.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Prompt For Fg-Er",
      "text": "As an emotion recognition expert, identify one or several emotions displayed in the video. Candidate labels: ...... Please think step by step. Enclose your final answer within <answer></answer> tags.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Mme-Emotion",
      "text": "",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Prompt For Ml-Er",
      "text": "As an emotion recognition expert, identify multiple emotions displayed in the video. Candidate labels: ...... Please think step by step. Enclose your final answer within <answer></answer> tags.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Prompt For Sa",
      "text": "As an emotion recognition expert, identify the most prominent sentiment displayed in the video. Candidate labels: ...... Please think step by step. Enclose your final answer within <answer></answer> tags.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Prompt For Fg-Sa",
      "text": "As an emotion recognition expert, identify the most prominent fine-grained sentiment displayed in the video. Candidate labels: ...... Please think step by step. Enclose your final answer within <answer></answer> tags.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Prompt For Ir",
      "text": "As an emotion recognition expert, identify the most prominent intent displayed in the video. Candidate labels: ...... Please think step by step. Enclose your final answer within <answer></answer> tags.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C.3 Prompt For Extracting Audio Clues",
      "text": "An example prompt for extracting audio clues is shown below. Please note that minor adjustments are needed depending on the specific task requirements.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Prompt For Extracting Audio Clues",
      "text": "You are a speech emotion analysis expert, specializing in analyzing the tone and intonation of input audio. Please analyze which features in the audio reflect that the speaker is feeling {Emotion labels}. Pay special attention to emotional features such as crying sounds, laughter, changes in tone, speech rate, pauses, emphasis and stress, voice trembling, and other emotional characteristics. Emotion labels: ......",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "C.4 Prompt For Extracting Key Steps",
      "text": "An example prompt for extracting key answer steps is shown below. Please note that minor adjustments are needed depending on the specific task requirements.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Prompt For Extracting Kye Steps",
      "text": "You are an expert in affective computing and very good at handling tasks related to emotion recognition.\n\nGiven the following answer about the emotion conveyed in a video, please help me extract several key steps from the answer. Each step should be as concise as possible and prefixed with Step X. In the last step, showcase the predicted emotion. If there are no reasoning steps, please showcase the predicted emotion at Step 1. Enclose your result within <step></step> tags. Answer: ......",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Example 1:",
      "text": "Answer:\n\nIn the video, an elderly man wearing a green shirt is in an outdoor nighttime setting. He appears focused and serious, with slightly furrowed brows and a serious expression.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "D.1 Additional Experimental Results Across Tasks",
      "text": "As shown in Table  3 , Table  4 , Table  5 , Table  6 , Table  7 , Table  8 , Table  9 , and Table  10 , we present a fine-grained performance comparison of cutting-edge MLLMs across various emotional tasks.",
      "page_start": 22,
      "page_end": 22
    },
    {
      "section_name": "D.2 Additional Experimental Results Across Categories",
      "text": "As shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , and Figure  12 , we present a finegrained performance comparison of cutting-edge MLLMs across different categories of emotions, sentiments, and intents. We include only single-label tasks and exclude the results of multi-label comparisons.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "D.3 Additional Experimental Results Across Metrics",
      "text": "As shown in Figure  13 , we provide additional results to showcase the relationships among model evaluation metrics. The results are consistent with our previous observations.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "D.4 Additional Case Study",
      "text": "As shown in Figure  14",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Identify the primary subject for emotion recognition (the person on the left, who is more expressive and actively speaking).\n\nStep 2: Analyze the subject's facial expressions and gestures across key frames, focusing on changes in animation, hand movements, and facial cues.\n\nStep 3: Compare observed expressions to candidate emotion labels, ruling out anger, sadness, surprise, happiness, excitement, fear, and neutral based on specific cues.\n\nStep 4: Conclude that the combination of raised eyebrows, open-mouthed speech, and open hand gestures most closely aligns with frustration.\n\nStep 5: The predicted emotion is frustration.\n\nRating:\n\nAudio-Reasoner:\n\nStep 1: Assess the speaker's tone and initial statements for emotional cues.\n\nStep 2: Analyze specific phrases for indicators of uncertainty, helplessness, and financial burden.\n\nStep 3: Consider and rule out alternative emotions such as anger, surprise, happiness, excitement, fear, and neutrality.\n\nStep 4: Focus on the presence of sadness and frustration based on the combination of cues.\n\nStep 5: The predicted emotion is frustration.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Analyze the audio clues for indications of emotional states, such as excitement and anxiety.\n\nStep 2: Examine facial expressions and body language for signs of strong emotional responses.\n\nStep 3: Infer possible emotions (anger, frustration, fear) based on combined audio and video analysis.\n\nStep 4: Note the lack of specific emotional cues, making it difficult to determine the exact emotion.\n\nStep 5: The predicted emotion is unclear due to insufficient evidence. Emotion Labels: Surprise.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Analyze the facial features in the provided frames, focusing on eyes, eyebrows, mouth, nose, cheeks, and overall tension.\n\nStep 2: Evaluate the observed features against candidate emotion labels, ruling out those that do not match the indicators.\n\nStep 3: Determine the most prominent emotion by synthesizing the subtle clues and comparing them to typical emotional expressions.\n\nStep 4: The predicted emotion is neutral.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Rating:",
      "text": "1/1 1/1 1/1 0/1 Rating:\n\nAudio-Reasoner:\n\nStep 1: Assess the audio clip, focusing on the spoken word and its intonation.\n\nStep 2: Analyze the rising pitch and emphasis in the speaker's voice to identify emotional clues.\n\nStep 3: Compare the observed vocal features with typical characteristics of various emotions.\n\nStep 4: Weigh the evidence, considering context and vocal clues to narrow down the emotion.\n\nStep 5: The predicted emotion is surprise.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Analyze the subtitle content \\\"Sister, sister\\\" for context.\n\nStep 2: Observe the woman's crying expression and body language in the video for emotional clues.\n\nStep 3: Note the presence of laughter in the audio as an additional clue.\n\nStep 4: Infer that the sentence may be a greeting or expression of concern based on the woman's negative emotional state.\n\nStep 5: The predicted emotion is negative.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Rating:",
      "text": "1",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Analyze the facial features in the provided frames, focusing on eyebrows, eyes, mouth, and forehead tension.\n\nStep 2: Compare observed features against candidate emotion labels, eliminating those that do not match.\n\nStep 3: Differentiate between the most plausible emotions (angry, sad, worried) by considering the intensity and nature of the expression and any contextual clues.\n\nStep 4: Determine the most prominent emotion based on the synthesis of facial clues and context.\n\nStep 5: The predicted emotion is sad.\n\nRating:\n\nAudio-Reasoner:\n\nStep 1: Note the initial impression of the audio, focusing on its abrupt and sharp nature.\n\nStep 2: Analyze specific vocal features such as pitch, duration, intonation, and vocal quality.\n\nStep 3: Compare these features with typical characteristics of candidate emotions (happy, neutral, angry, surprise, sad, worried).\n\nStep 4: Conclude that the predicted emotion is anger.",
      "page_start": 34,
      "page_end": 35
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Examine the subtitle for self-evaluative or reflective language.\n\nStep 2: Analyze audio clues such as trembling voice and rapid breathing for emotional indicators.\n\nStep 3: Observe video clues like closed eyes and contemplative posture for signs of internal conflict.\n\nStep 4: Integrate verbal, audio, and visual information to infer the emotional state.\n\nStep 5: The predicted emotion is regret or guilt.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Rating:",
      "text": "1",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Identify the main characters in the video and focus on their visible expressions and gestures.\n\nStep 2: Analyze the woman on the left by observing her posture, gaze, mouth, and hand position across frames to infer emotions such as sadness and disquietment.\n\nStep 3: Analyze the woman on the right by noting her attentive gaze, comforting gesture, and facial expressions to infer emotions such as sympathy and affection.\n\nStep 4: Synthesize the observations to determine the dominant emotions for each woman.\n\nStep 5: The predicted emotions are sadness and disquietment for the woman on the left, and sympathy and affection for the woman on the right. Step 1: Listen to the audio and note the characteristics of the sounds (short, sharp, percussive, high-pitched, abrupt, lacking sustained vocalization or background noise).\n\nStep 2: Analyze the qualities of the sounds for emotional clues (abruptness, isolation, lack of context).\n\nStep 3: Consider possible emotional associations based on sound features (discomfort, surprise, disconnection, aversion, fear, disappointment, sadness).\n\nStep 4: Synthesize the analysis and select the most fitting emotion(s) given the ambiguity and sound characteristics.\n\nStep 5: The predicted emotions are disconnection or aversion.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Observe the woman's facial expressions and body language in the office environment.\n\nStep 2: Note the presence of crying and other negative visual clues.\n\nStep 3: Analyze audio clues such as a trembling voice, rapid and shallow breathing, and a tense tone.\n\nStep 4: Consider the caption content \\\"I'm so afraid of this\\\" as an expression of her emotional state.\n\nStep 5: Synthesize visual, auditory, and textual information to determine the predicted emotions.\n\nStep 6: The predicted emotions are fear, anxiety, stress, sadness, and disappointment.\n\nRating:\n\nFigure  17 : Case Study on FG-ER. We showcase the answers and ratings of three MLLMs.",
      "page_start": 36,
      "page_end": 40
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Analyze facial expressions in each video frame, focusing on features such as mouth shape, eye crinkling, and eyebrow position.\n\nStep 2: Note verbal clues and context, such as spoken phrases that may indicate recognition or surprise.\n\nStep 3: Match observed facial and verbal clues to candidate emotion labels.\n\nStep 4: Exclude emotion labels that do not fit the observed clues.\n\nStep 5: The predicted emotions are happiness and surprise.\n\nRating:\n\nRating:\n\n1/1 0/1 0/1 0/2 Audio-Reasoner:\n\nStep 1: Assess the audio clip, noting its brevity and the importance of tone for emotion recognition.\n\nStep 2: List possible emotional interpretations based on vocal characteristics of the \\\"Eek!\\\" utterance.\n\nStep 3: Acknowledge the lack of context, which makes definitive emotion prediction difficult.\n\nStep 4: Conclude that the predicted emotions are ambiguous and could include disgust, fear, contempt, disappointment, sadness, anger, anxiety, or helplessness.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Analyze the subtitle \\\"I want to be with you\\\" for emotional intent.\n\nStep 2: Observe the woman's smiling and laughing expressions and body movements in the video.\n\nStep 3: Note the trembling and shaking of the individual's voice in the audio.\n\nStep 4: Infer that the woman is expressing affection or desire, but also feels anxious or nervous.\n\nStep 5: The predicted emotions are affection/desire and anxiety/nervousness.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Rating:",
      "text": "1 Audio Clues: The speaker expresses a negative sentiment by stating they 'don't really go for the animated type films.' This indicates a lack of interest or preference for such movies, which can be perceived as a negative stance. Additionally, there might be a hint of disapproval or dissatisfaction within the statement, although it's not explicitly stated.\n\nEmotion Labels: Negative.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Analyze each frame for facial expressions and emotional clues.\n\nStep 2: Identify recurring patterns or dominant expressions across the frames.\n\nStep 3: Evaluate the intensity and clarity of the emotional clues.\n\nStep 4: Determine the most prominent sentiment based on the analysis.\n\nStep 5: The predicted sentiment is positive.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Rating:",
      "text": "1 Step 1: Assess the speaker's initial statement for sentiment, noting the lack of strong positive or negative emotion.\n\nStep 2: Analyze the language used for indications of bias or defensiveness, interpreting the stance as neutral.\n\nStep 3: Consider alternative interpretations, such as subtle negativity, but weigh them against the overall context.\n\nStep 4: Weigh all evidence to determine the most prominent sentiment.\n\nStep 5: The predicted sentiment is neutral.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Examine the caption in the text for emotional clues, such as expressions of disappointment or anger.\n\nStep 2: Analyze the woman's facial expressions and body language in the video for signs of anger or frustration.\n\nStep 3: Consider audio clues that indicate a sense of frustration and annoyance.\n\nStep 4: Infer the overall emotional state based on the combination of text, visual, and audio clues.\n\nStep 5: The predicted sentiment is negative.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Rating:",
      "text": "1",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Analyze the facial expressions and body language in each frame to track emotional progression.\n\nStep 2: Identify the overall emotional trend and peak emotion across the sequence.\n\nStep 3: Determine the most prominent sentiment based on the most expressive frames.\n\nStep 4: Match the observed sentiment to the candidate labels.\n\nStep 5: The predicted sentiment is very strong positive.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Rating:",
      "text": "1/1 1/1 1/1 1/1 1/1 Rating:\n\n1/1 1/1 1/1 1/1 0/1 Audio-Reasoner:\n\nStep 1: Assess the audio for key emotional indicators, such as speech and laughter.\n\nStep 2: Analyze the nature of the laughter and any descriptive phrases (e.g., \\\"so cute,\\\"\n\n\\\"so funny\\\") to infer sentiment.\n\nStep 3: Evaluate the speech content for emotional clues, noting if it is neutral or emotionally charged.\n\nStep 4: Integrate findings from laughter and speech to determine the overall sentiment.\n\nStep 5: The predicted sentiment is positive.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Analyze the acoustic information, focusing on voice characteristics and tone.\n\nStep 2: Observe visual clues, such as the woman's smile and gaze into the distance.\n\nStep 3: Infer the possible intent, suggesting the woman is giving a compliment or evaluation.\n\nStep 4: Note the lack of specific contextual information, making the exact sentiment unclear.\n\nStep 5: The predicted sentiment is indeterminate due to insufficient context.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Rating:",
      "text": "1",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Gemini-2.5-Pro:",
      "text": "Step 1: Analyze the facial expression for emotional clues, noting features such as relaxed eyebrows, downward gaze, and a slightly open mouth indicating calmness and thoughtfulness.\n\nStep 2: Evaluate possible intent labels by comparing the observed expression with common emotional clues for each label.\n\nStep 3: Narrow down to the most plausible candidates by comparing the nuances of \\\"suggesting,\\\" \\\"acknowledging,\\\" and \\\"neutral\\\" based on the context and expression.\n\nStep 4: Select the most prominent intent by considering the proactive and thoughtful nature of the expression while speaking.\n\nStep 5: The predicted intent is suggesting. Audio-Reasoner:\n\nStep 1: Assess the audio content and note the spoken word and its intonation.\n\nStep 2: List possible interpretations based on the word and context, such as questioning, surprise, disagreement, or acknowledgement.\n\nStep 3: Analyze the intonation to narrow down the likely intent, focusing on the significance of rising intonation.\n\nStep 4: Consider the limited context and weigh the most probable interpretation.\n\nStep 5: The predicted intent is questioning.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Affectgpt:",
      "text": "Step 1: Identify the context of the interaction, noting that the woman is responding to the man's inquiry or providing an explanation.\n\nStep 2: Analyze audio clues such as a trembling voice and quicker speech pace.\n\nStep 3: Observe video clues, including the woman's focused and absorbed state.\n\nStep 4: Infer possible emotions, such as anxiety, tension, or excitement, based on the combined clues.\n\nStep 5: The predicted intent is responding to a question or providing an explanation while feeling anxious and excited.",
      "page_start": 40,
      "page_end": 40
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of MME-Emotion Statistics. Left: Task Types. MME-Emotion encompasses",
      "page": 2
    },
    {
      "caption": "Figure 2: Performance Comparison of Leading MLLMs on MME-Emotion. Our evaluation suite",
      "page": 2
    },
    {
      "caption": "Figure 1: , MME-Emotion consists of 6,500 video clips associated with task-specific question-",
      "page": 2
    },
    {
      "caption": "Figure 2: ). Even the top-performing model (Gemini-2.5-Pro)",
      "page": 3
    },
    {
      "caption": "Figure 3: Illustration of Our Evaluation Strategy. We leverage a multi-agent system framework to",
      "page": 5
    },
    {
      "caption": "Figure 3: ). This approach allows us to assess the reasoning performance of MLLMs without",
      "page": 5
    },
    {
      "caption": "Figure 3: , the Spearman’s rank correlation coefficient (Spearman,",
      "page": 6
    },
    {
      "caption": "Figure 4: Task-level Performance Comparison (%) on MME-Emotion. We showcase fine-grained",
      "page": 7
    },
    {
      "caption": "Figure 5: Relationships among Model Evaluation Metrics. Left Panel: Relationship between",
      "page": 8
    },
    {
      "caption": "Figure 4: presents a fine-grained comparison of 20 MLLMs across eight distinct emotional",
      "page": 8
    },
    {
      "caption": "Figure 5: , there is a general positive correlation between the average number of answer steps and overall",
      "page": 8
    },
    {
      "caption": "Figure 6: Cases of Error Responses. We showcase some typical error types of cutting-edge MLLMs.",
      "page": 9
    },
    {
      "caption": "Figure 6: , the failure cases of Video-LLaMA2 and Qwen2.5-Omni reveal that both",
      "page": 9
    },
    {
      "caption": "Figure 6: reveals that the error primarily stems from the absence",
      "page": 9
    },
    {
      "caption": "Figure 7: , Figure 8, Figure 9, Figure 10, Figure 11, and Figure 12, we present a fine-",
      "page": 21
    },
    {
      "caption": "Figure 13: , we provide additional results to showcase the relationships among model",
      "page": 21
    },
    {
      "caption": "Figure 14: , Figure 15, Figure 16, Figure 17, Figure 18, Figure 19, Figure 20, and Figure",
      "page": 21
    },
    {
      "caption": "Figure 7: Category-level Performance Comparison (%) on ER-Lab. We showcase fine-grained",
      "page": 26
    },
    {
      "caption": "Figure 8: Category-level Performance Comparison (%) on ER-Wild. We showcase fine-grained",
      "page": 27
    },
    {
      "caption": "Figure 9: Category-level Performance Comparison (%) on Noise-ER. We showcase fine-grained",
      "page": 28
    },
    {
      "caption": "Figure 10: Category-level Performance Comparison (%) on SA. We showcase fine-grained",
      "page": 29
    },
    {
      "caption": "Figure 11: Category-level Performance Comparison (%) on FG-SA. We showcase fine-grained",
      "page": 30
    },
    {
      "caption": "Figure 12: Category-level Performance Comparison (%) on IR. We showcase fine-grained com-",
      "page": 31
    },
    {
      "caption": "Figure 13: Multivariate Analysis of Evaluation Metrics. We showcase pairwise scatter plots with",
      "page": 32
    },
    {
      "caption": "Figure 14: Case Study on ER-Lab. We showcase the answers and ratings of three MLLMs.",
      "page": 33
    },
    {
      "caption": "Figure 15: Case Study on ER-Wild. We showcase the answers and ratings of three MLLMs.",
      "page": 34
    },
    {
      "caption": "Figure 16: Case Study on Noise-ER. We showcase the answers and ratings of three MLLMs.",
      "page": 35
    },
    {
      "caption": "Figure 17: Case Study on FG-ER. We showcase the answers and ratings of three MLLMs.",
      "page": 36
    },
    {
      "caption": "Figure 18: Case Study on ML-ER. We showcase the answers and ratings of three MLLMs.",
      "page": 37
    },
    {
      "caption": "Figure 19: Case Study on SA. We showcase the answers and ratings of three MLLMs.",
      "page": 38
    },
    {
      "caption": "Figure 20: Case Study on FG-SA. We showcase the answers and ratings of three MLLMs.",
      "page": 39
    },
    {
      "caption": "Figure 21: Case Study on IR. We showcase the answers and ratings of three MLLMs.",
      "page": 40
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompt for FG-SA": "As an emotion recognition expert, identify the most prominent fine-grained sentiment displayed in the\nvideo.\nCandidate labels:\n......\nPlease think step by step. Enclose your final answer within <answer></answer> tags."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompt for Extracting Audio Clues": "You are a speech emotion analysis expert, specializing in analyzing the tone and intonation of input\naudio. Please analyze which features in the audio reflect that the speaker is feeling {Emotion labels}.\nPay special attention to emotional features such as crying sounds, laughter, changes in tone, speech rate,\npauses, emphasis and stress, voice trembling, and other emotional characteristics.\nEmotion labels:\n......"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompt for Extracting Kye Steps": "You are an expert in affective computing and very good at handling tasks related to emotion recognition.\nGiven the following answer about the emotion conveyed in a video, please help me extract several key\nsteps from the answer. Each step should be as concise as possible and prefixed with Step X. In the last\nstep, showcase the predicted emotion. If there are no reasoning steps, please showcase the predicted\nemotion at Step 1. Enclose your result within <step></step> tags.\nAnswer:\n......\nExample 1:\nAnswer:\nIn the video, an elderly man wearing a green shirt is in an outdoor nighttime setting. He appears focused\nand serious, with slightly furrowed brows and a serious expression. His eyes are scanning the other\nperson, suggesting he wants to convey something important or ask about a certain thing. As time passes,\nhis gaze shifts from scanning to direct engagement, eventually relaxing and showing a hint of happiness,\nindicating fluctuations in his emotions. Overall, the elderly man experiences a moderate intensity of\nneutral emotion, mixed with slight positive fluctuations.\nResult:\n<step>Step 1: Observe the setting and characters in the video, noting any relevant details such as\nlocation and participants’ appearance. Step 2: Analyze the facial expressions and movements, such as\nfurrowed brows and downturned eyes, looking for indicators of specific emotions. Step 3: Consider\nverbal and non-verbal cues, including mouth movements and gaze direction, that signify communication\nand emotional responses. Step 4: Synthesize observations to understand the overall emotional state\nconveyed by the participant. Step 5: Based on the analysis, determine that\nthe predicted emotion is\nneutral.</step>\nExample 2:\nAnswer:\nhappy\nResult:\n<step>Step 1: The predicted emotion is happy.</step>"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prompt for Evaluation the Performance": "You are an expert in affective computing and very good at handling tasks related to emotion recognition.\nI will first give you some ground truth information about the emotion in a video: visual clue, audio clue,\nand emotion label. I will also give you a model prediction. Please help me rate the performance of the\nprediction.\nRating Requirements:\n1. Rate each step using 0 or 1. (0=wrong, 1=correct)\n2. For the last step, your rating should be 1 if the predicted emotion matches the ground truth emotion\nlabel, otherwise 0. Don’t consider visual/audio clues at this step.\n3. For each other step, only consider predictions clearly contradicting visual/audio clues as incorrect.\n4. Ensure the number of steps in your rating is equal to that in the model prediction.\n5. Output format: <score>Step 1: 0/1, Step 2: 1/1,...</score>.\nInput Data:\n- Visual clue: The video frames (images)\n- Audio clue:\n......\n- Emotion label:\n......\n- Model prediction:\n......\nExample Output:\n<score>Step 1: 1/1, Step 2: 0/1, Step 3: 0/1</score>"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Qwen-vl: A versatile vision-language model for understanding, localization",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Text Reading, and Beyond"
    },
    {
      "citation_id": "3",
      "title": "Qwen2. 5-vl technical report",
      "authors": [
        "Shuai Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge",
        "Sibo Song",
        "Kai Dang",
        "Peng Wang",
        "Shijie Wang",
        "Jun Tang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-vl technical report",
      "arxiv": "arXiv:2502.13923"
    },
    {
      "citation_id": "4",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms",
      "authors": [
        "Zesen Cheng",
        "Sicong Leng",
        "Hang Zhang",
        "Yifei Xin",
        "Xin Li",
        "Guanzheng Chen",
        "Yongxin Zhu",
        "Wenqi Zhang",
        "Ziyang Luo",
        "Deli Zhao"
      ],
      "year": "2024",
      "venue": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "7",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "8",
      "title": "Exploring embodied emotion through a large-scale egocentric video dataset",
      "authors": [
        "Yueying Feng",
        "Wenkang Han",
        "Tao Jin",
        "Zhou Zhao",
        "Fei Wu",
        "Chang Yao",
        "Jingyuan Chen"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "Thermal infrared imaging-based affective computing and its application to facilitate human robot interaction: A review",
      "authors": [
        "Chiara Filippini",
        "David Perpetuini",
        "Daniela Cardone",
        "Antonio Chiarelli",
        "Arcangelo Merla"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "10",
      "title": "Mme: a comprehensive evaluation benchmark for multimodal large language models",
      "authors": [
        "Chaoyou Fu",
        "Peixian Chen",
        "Yunhang Shen",
        "Yulei Qin",
        "Mengdan Zhang",
        "Xu Lin",
        "Zhenyu Qiu",
        "Wei Lin",
        "Jinrui Yang",
        "Xiawu Zheng"
      ],
      "year": "2023",
      "venue": "Mme: a comprehensive evaluation benchmark for multimodal large language models"
    },
    {
      "citation_id": "11",
      "title": "Mme-survey: A comprehensive survey on evaluation of multimodal llms",
      "authors": [
        "Chaoyou Fu",
        "Yi-Fan Zhang",
        "Shukang Yin",
        "Bo Li",
        "Xinyu Fang",
        "Sirui Zhao",
        "Haodong Duan",
        "Xing Sun",
        "Ziwei Liu",
        "Liang Wang"
      ],
      "year": "2024",
      "venue": "Mme-survey: A comprehensive survey on evaluation of multimodal llms",
      "arxiv": "arXiv:2411.15296"
    },
    {
      "citation_id": "12",
      "title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis",
      "authors": [
        "Chaoyou Fu",
        "Yuhan Dai",
        "Yongdong Luo",
        "Lei Li",
        "Shuhuai Ren",
        "Renrui Zhang",
        "Zihan Wang",
        "Chenyu Zhou",
        "Yunhang Shen",
        "Mengdan Zhang"
      ],
      "year": "2025",
      "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference"
    },
    {
      "citation_id": "13",
      "title": "Applications of affective computing in human-robot interaction: State-of-art and challenges for manufacturing",
      "authors": [
        "Riccardo Gervasi",
        "Federico Barravecchia",
        "Luca Mastrogiacomo",
        "Fiorenzo Franceschini"
      ],
      "year": "2023",
      "venue": "Proceedings of the Institution of Mechanical Engineers"
    },
    {
      "citation_id": "14",
      "title": "Gemini 2.0 flash model card",
      "authors": [
        "Google"
      ],
      "year": "2025",
      "venue": "Gemini 2.0 flash model card"
    },
    {
      "citation_id": "15",
      "title": "Gemini 2.5 flash preview model card",
      "authors": [
        "Google"
      ],
      "year": "2025",
      "venue": "Gemini 2.5 flash preview model card"
    },
    {
      "citation_id": "16",
      "title": "Gemini 2.5 pro preview model card",
      "authors": [
        "Google"
      ],
      "year": "2025",
      "venue": "Gemini 2.5 pro preview model card"
    },
    {
      "citation_id": "17",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "Dejian Daya Guo",
        "Haowei Yang",
        "Junxiao Zhang",
        "Ruoyu Song",
        "Runxin Zhang",
        "Qihao Xu",
        "Shirong Zhu",
        "Peiyi Ma",
        "Xiao Wang",
        "Bi"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "arxiv": "arXiv:2501.12948"
    },
    {
      "citation_id": "18",
      "title": "Measuring mathematical problem solving with the math dataset",
      "authors": [
        "Dan Hendrycks",
        "Collin Burns",
        "Saurav Kadavath",
        "Akul Arora",
        "Steven Basart",
        "Eric Tang",
        "Dawn Song",
        "Jacob Steinhardt"
      ],
      "year": "2021",
      "venue": "Measuring mathematical problem solving with the math dataset",
      "arxiv": "arXiv:2103.03874"
    },
    {
      "citation_id": "19",
      "title": "Emobench-m: Benchmarking emotional intelligence for multimodal large language models",
      "authors": [
        "He Hu",
        "Yucheng Zhou",
        "Lianzhong You",
        "Hongbo Xu",
        "Qianning Wang",
        "Zheng Lian",
        "Richard Fei",
        "Fei Yu",
        "Laizhong Ma",
        "Cui"
      ],
      "year": "2025",
      "venue": "Emobench-m: Benchmarking emotional intelligence for multimodal large language models",
      "arxiv": "arXiv:2502.04424"
    },
    {
      "citation_id": "20",
      "title": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench",
      "authors": [
        "Jen-Tse Huang",
        "Man Lam",
        "Eric Li",
        "Shujie Ren",
        "Wenxuan Wang",
        "Wenxiang Jiao",
        "Zhaopeng Tu",
        "Michael R Lyu"
      ],
      "year": "2023",
      "venue": "Emotionally numb or empathetic? evaluating how llms feel using emotionbench",
      "arxiv": "arXiv:2308.03656"
    },
    {
      "citation_id": "21",
      "title": "Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality",
      "authors": [
        "Dongzhi Jiang",
        "Renrui Zhang",
        "Ziyu Guo",
        "Yanwei Li",
        "Yu Qi",
        "Xinyan Chen",
        "Liuhui Wang",
        "Jianhan Jin",
        "Claire Guo",
        "Shen Yan"
      ],
      "year": "2025",
      "venue": "robustness, and efficiency",
      "arxiv": "arXiv:2502.09621"
    },
    {
      "citation_id": "22",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "23",
      "title": "Scaling laws for neural language models",
      "authors": [
        "Jared Kaplan",
        "Sam Mccandlish",
        "Tom Henighan",
        "Tom Brown",
        "Benjamin Chess",
        "Rewon Child",
        "Scott Gray",
        "Alec Radford",
        "Jeffrey Wu",
        "Dario Amodei"
      ],
      "year": "2020",
      "venue": "Scaling laws for neural language models",
      "arxiv": "arXiv:2001.08361"
    },
    {
      "citation_id": "24",
      "title": "Intraclass correlation coefficient. Encyclopedia of statistical sciences",
      "authors": [
        "G Gary",
        "Koch"
      ],
      "year": "2004",
      "venue": "Intraclass correlation coefficient. Encyclopedia of statistical sciences"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in context",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Efficient memory management for large language model serving with pagedattention",
      "authors": [
        "Woosuk Kwon",
        "Zhuohan Li",
        "Siyuan Zhuang",
        "Ying Sheng",
        "Lianmin Zheng",
        "Cody Yu",
        "Joseph Gonzalez",
        "Hao Zhang",
        "Ion Stoica"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles"
    },
    {
      "citation_id": "27",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "28",
      "title": "Seed-bench: Benchmarking multimodal large language models",
      "authors": [
        "Bohao Li",
        "Yuying Ge",
        "Yixiao Ge",
        "Guangzhi Wang",
        "Rui Wang",
        "Ruimao Zhang",
        "Ying Shan"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "30",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "Open-vocabulary multimodal emotion recognition: Dataset, metric, and benchmark",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Lan Chen",
        "Haoyu Chen",
        "Hao Gu",
        "Zhuofan Wen",
        "Shun Chen",
        "Siyuan Zhang",
        "Hailiang Yao"
      ],
      "year": "2024",
      "venue": "Open-vocabulary multimodal emotion recognition: Dataset, metric, and benchmark",
      "arxiv": "arXiv:2410.01495"
    },
    {
      "citation_id": "32",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Affectgpt: Dataset and framework for explainable multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Affectgpt: Dataset and framework for explainable multimodal emotion recognition",
      "arxiv": "arXiv:2407.07653"
    },
    {
      "citation_id": "34",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Yang Ye",
        "Bin Zhu",
        "Jiaxi Cui",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "35",
      "title": "Improved baselines with visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Improved baselines with visual instruction tuning"
    },
    {
      "citation_id": "36",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Xiaofen Xing",
        "Björn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "arxiv": "arXiv:2407.02751"
    },
    {
      "citation_id": "38",
      "title": "Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module",
      "authors": [
        "Yihe Liu",
        "Ziqi Yuan",
        "Huisheng Mao",
        "Zhiyun Liang",
        "Wanqiuyue Yang",
        "Yuanzhe Qiu",
        "Tie Cheng",
        "Xiaoteng Li",
        "Hua Xu",
        "Kai Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 international conference on multimodal interaction"
    },
    {
      "citation_id": "39",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM international conference on multimedia"
    },
    {
      "citation_id": "40",
      "title": "Affective computing for healthcare: Recent trends, applications, challenges, and beyond",
      "authors": [
        "Yuanyuan Liu",
        "Ke Wang",
        "Lin Wei",
        "Jingying Chen",
        "Yibing Zhan",
        "Dapeng Tao",
        "Zhe Chen"
      ],
      "year": "2024",
      "venue": "Affective computing for healthcare: Recent trends, applications, challenges, and beyond",
      "arxiv": "arXiv:2402.13589"
    },
    {
      "citation_id": "41",
      "title": "Visual-rft: Visual reinforcement fine-tuning",
      "authors": [
        "Ziyu Liu",
        "Zeyi Sun",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Haodong Duan",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "year": "2025",
      "venue": "Visual-rft: Visual reinforcement fine-tuning",
      "arxiv": "arXiv:2503.01785"
    },
    {
      "citation_id": "42",
      "title": "Benchmark dataset for financial multi-modal reasoning evaluation",
      "authors": [
        "Junyu Luo",
        "Zhizhuo Kou",
        "Liming Yang",
        "Xiao Luo",
        "Jinsheng Huang",
        "Zhiping Xiao",
        "Jingshu Peng",
        "Chengzhong Liu",
        "Jiaming Ji",
        "Xuanzhe Liu"
      ],
      "year": "2025",
      "venue": "Benchmark dataset for financial multi-modal reasoning evaluation",
      "arxiv": "arXiv:2505.24714"
    },
    {
      "citation_id": "43",
      "title": "Arbee: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Yu Luo",
        "Jianbo Ye",
        "Reginald Adams",
        "Jia Li",
        "Michelle Newman",
        "James Wang"
      ],
      "year": "2020",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "44",
      "title": "Interrater reliability: the kappa statistic",
      "authors": [
        "Mary Mchugh"
      ],
      "year": "2012",
      "venue": "Biochemia medica"
    },
    {
      "citation_id": "45",
      "title": "Gpt-4o system card",
      "year": "2024",
      "venue": "Gpt-4o system card"
    },
    {
      "citation_id": "46",
      "title": "Gpt-4.1 system card",
      "year": "2025",
      "venue": "Gpt-4.1 system card"
    },
    {
      "citation_id": "47",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "48",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "49",
      "title": "Evaluating the emotional intelligence of large language models",
      "authors": [
        "Sahand Sabour",
        "Siyang Liu",
        "Zheyuan Zhang",
        "June Liu",
        "Jinfeng Zhou",
        "S Alvionna",
        "Juanzi Sunaryo",
        "Tatia Li",
        "Rada Lee",
        "Minlie Mihalcea",
        "Huang",
        "Emobench"
      ],
      "year": "2024",
      "venue": "Evaluating the emotional intelligence of large language models",
      "arxiv": "arXiv:2402.12071"
    },
    {
      "citation_id": "50",
      "title": "Mosabench: Multi-object sentiment analysis benchmark for evaluating multimodal large language models understanding of complex image",
      "authors": [
        "Shezheng Song",
        "Chengxiang He",
        "Shasha Li",
        "Shan Zhao",
        "Chengyu Wang",
        "Tianwei Yan",
        "Xiaopeng Li",
        "Qian Wan",
        "Jun Ma",
        "Jie Yu"
      ],
      "year": "2024",
      "venue": "Mosabench: Multi-object sentiment analysis benchmark for evaluating multimodal large language models understanding of complex image",
      "arxiv": "arXiv:2412.00060"
    },
    {
      "citation_id": "51",
      "title": "The proof and measurement of association between two things. 1961. Jianhua Tao and Tieniu Tan",
      "authors": [
        "Charles Spearman"
      ],
      "year": "2005",
      "venue": "International Conference on Affective computing and intelligent interaction"
    },
    {
      "citation_id": "52",
      "title": "a family of highly capable multimodal models",
      "authors": [
        "Gemini Team",
        "Rohan Anil",
        "Sebastian Borgeaud",
        "Jean-Baptiste Alayrac",
        "Jiahui Yu",
        "Radu Soricut",
        "Johan Schalkwyk",
        "Andrew Dai",
        "Anja Hauth",
        "Katie Millican"
      ],
      "year": "2023",
      "venue": "a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "53",
      "title": "Qvq: To see the world with wisdom",
      "authors": [
        "Qwen Team"
      ],
      "year": "2024",
      "venue": "Qvq: To see the world with wisdom"
    },
    {
      "citation_id": "54",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "A Mihalis",
        "Björn Nicolaou",
        "Stefanos Schuller",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of selected topics in signal processing"
    },
    {
      "citation_id": "55",
      "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "authors": [
        "Peng Wang",
        "Shuai Bai",
        "Sinan Tan",
        "Shijie Wang",
        "Zhihao Fan",
        "Jinze Bai",
        "Keqin Chen",
        "Xuejing Liu",
        "Jialin Wang",
        "Wenbin Ge"
      ],
      "year": "2024",
      "venue": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution",
      "arxiv": "arXiv:2409.12191"
    },
    {
      "citation_id": "56",
      "title": "Review of affective computing in education/learning: Trends and challenges",
      "authors": [
        "Chih-Hung Wu",
        "Yueh-Min Huang",
        "Jan-Pan Hwang"
      ],
      "year": "2016",
      "venue": "British Journal of Educational Technology"
    },
    {
      "citation_id": "57",
      "title": "Multimodal large language models: A survey",
      "authors": [
        "Jiayang Wu",
        "Wensheng Gan",
        "Zefeng Chen",
        "Shicheng Wan",
        "Philip Yu"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Big Data (BigData)"
    },
    {
      "citation_id": "58",
      "title": "Audioreasoner: Improving reasoning capability in large audio language models",
      "authors": [
        "Zhifei Xie",
        "Mingbao Lin",
        "Zihang Liu",
        "Pengcheng Wu",
        "Shuicheng Yan",
        "Chunyan Miao"
      ],
      "year": "2025",
      "venue": "Audioreasoner: Improving reasoning capability in large audio language models",
      "arxiv": "arXiv:2503.02318"
    },
    {
      "citation_id": "59",
      "title": "Qwen2. 5-omni technical report",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-omni technical report",
      "arxiv": "arXiv:2503.20215"
    },
    {
      "citation_id": "60",
      "title": "Nurul Fazmidar Binti Mohd Noor, Mohamad Nizam Bin Ayub, Hannyzzura Binti Affal, and Nornazlita Binti Hussin. Affective computing in education: A systematic review and future research",
      "authors": [
        "Elaheh Yadegaridehkordi"
      ],
      "year": "2019",
      "venue": "Computers & education"
    },
    {
      "citation_id": "61",
      "title": "Xiaoming Fu, and Soujanya Poria. Mm-instructeval: Zero-shot evaluation of (multimodal) large language models on multimodal reasoning tasks",
      "authors": [
        "Xiaocui Yang",
        "Wenfang Wu",
        "Shi Feng",
        "Ming Wang",
        "Daling Wang",
        "Yang Li",
        "Qi Sun",
        "Yifei Zhang"
      ],
      "year": "2025",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "62",
      "title": "Enhancing health care via affective computing",
      "authors": [
        "N Georgios",
        "Yannakakis"
      ],
      "year": "2018",
      "venue": "Enhancing health care via affective computing"
    },
    {
      "citation_id": "63",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "64",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "65",
      "title": "When scaling meets llm finetuning: The effect of data, model and finetuning method",
      "authors": [
        "Biao Zhang",
        "Zhongtao Liu",
        "Colin Cherry",
        "Orhan Firat"
      ],
      "year": "2024",
      "venue": "When scaling meets llm finetuning: The effect of data, model and finetuning method",
      "arxiv": "arXiv:2402.17193"
    },
    {
      "citation_id": "66",
      "title": "Mm-llms: Recent advances in multimodal large language models",
      "authors": [
        "Duzhen Zhang",
        "Yahan Yu",
        "Jiahua Dong",
        "Chenxing Li",
        "Dan Su",
        "Chenhui Chu",
        "Dong Yu"
      ],
      "year": "2024",
      "venue": "Mm-llms: Recent advances in multimodal large language models",
      "arxiv": "arXiv:2401.13601"
    },
    {
      "citation_id": "67",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "68",
      "title": "Instruction tuning for large language models: a survey",
      "authors": [
        "Shengyu Zhang",
        "Linfeng Dong",
        "Xiaoya Li",
        "Sen Zhang",
        "Xiaofei Sun",
        "Shuhe Wang",
        "Jiwei Li",
        "Runyi Hu",
        "Tianwei Zhang",
        "Fei Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: a survey",
      "arxiv": "arXiv:2308.10792"
    },
    {
      "citation_id": "69",
      "title": "Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint",
      "authors": [
        "Yi-Fan Zhang",
        "Huanyu Zhang",
        "Haochen Tian",
        "Chaoyou Fu",
        "Shuangqing Zhang",
        "Junfei Wu",
        "Feng Li",
        "Kun Wang",
        "Qingsong Wen",
        "Zhang Zhang"
      ],
      "year": "2024",
      "venue": "Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint",
      "arxiv": "arXiv:2408.13257"
    },
    {
      "citation_id": "70",
      "title": "R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning",
      "authors": [
        "Jiaxing Zhao",
        "Xihan Wei",
        "Liefeng Bo"
      ],
      "year": "2025",
      "venue": "R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning",
      "arxiv": "arXiv:2503.05379"
    },
    {
      "citation_id": "71",
      "title": "A large vision-speech language model for human-centric video understanding",
      "authors": [
        "Jiaxing Zhao",
        "Qize Yang",
        "Yixing Peng",
        "Detao Bai",
        "Shimin Yao",
        "Boyuan Sun",
        "Xiang Chen",
        "Shenghao Fu",
        "Xihan Wei",
        "Liefeng Bo"
      ],
      "year": "2025",
      "venue": "A large vision-speech language model for human-centric video understanding",
      "arxiv": "arXiv:2501.15111"
    },
    {
      "citation_id": "72",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}