{
  "paper_id": "2306.09372v1",
  "title": "Safer: Situation Aware Facial Emotion Recognition",
  "published": "2023-06-14T20:42:26Z",
  "authors": [
    "Mijanur Palash",
    "Bharat Bhargava"
  ],
  "keywords": [
    "Facial Expression Recognition",
    "Emotion Recognition",
    "Deep Learning",
    "Covid-19",
    "Contextual information",
    "Open-world AI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we present SAFER, a novel system for emotion recognition from facial expressions. It employs state-of-the-art deep learning techniques to extract various features from facial images and incorporates contextual information, such as background and location type, to enhance its performance. The system has been designed to operate in an open-world setting, meaning it can adapt to unseen and varied facial expressions, making it suitable for real-world applications. An extensive evaluation of SAFER against existing works in the field demonstrates improved performance, achieving an accuracy of 91.4% on the CAER-S dataset. Additionally, the study investigates the effect of novelty such as face masks during the Covid-19 pandemic on facial emotion recognition and critically examines the limitations of mainstream facial expressions datasets. To address these limitations, a novel dataset for facial emotion recognition is proposed. The proposed dataset and the system are expected to be useful for various applications such as human-computer interaction, security, and surveillance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human emotion recognition (ER) has gained significant research interest in recent years, particularly in the field of Artificial Intelligence (AI). This is due in part to the growing demand for online and remote learning systems as a result of the Covid-19 pandemic, where ER can play a crucial role in maintaining a positive and engaging learning environment by tracking the emotional status of students. Additionally, ER has a wide range of applications in domains such as human-computer interactions  [1] , law enforcement and surveillance  [2] , interactive gaming, consumer behavior analysis, customer service  [3] , and health care  [4] , among others.  Facial emotion recognition (FER) is a widely adopted approach for ER, which primarily relies on the analysis of facial expressions to infer emotional states. Researchers have traditionally categorized basic emotions into seven distinct categories, including anger, happiness, sadness, disgust, fear, contempt, and surprise  [6] . Figure  1  illustrate some common facial expressions associated with each of these emotions. The universality of facial expressions across different cultures, as demonstrated in this well-known study at  [7] ,\n\nhas greatly facilitated the development of FER systems. Additionally, the presence of micro-expressions, which are involuntary facial actions indicative of concealed emotions, also play a critical role in FER.\n\nThe Facial Action Coding System (FACS) is a widely accepted method for describing the movements of various facial muscles associated with different emotions. FACS breaks down facial expressions into individual components of muscle movement, referred to as Action Units (AU). Table  1  lists some common AU activities associated with different emotions.\n\nSimilarly, it is important to note that the situational context surrounding an individual also plays a significant role in shaping their emotional state.\n\nFor instance, a person working in a sweaty coal mine is more likely to exhibit unhappiness as compared to someone walking in a park with their dog.\n\nHowever, in some situations, place type alone may not be sufficient. For example, in a stadium, depending on whether the team wins or loses, some people may be happy and some may be unhappy. Figure  2 (a) and 2(b) illustrate this point, where without the background, the facial expression may be misleading. Therefore, the ability to extract and incorporate situational information, such as scene background and location type, can greatly enhance the accuracy of FER systems.\n\nHowever, during the Covid-19 pandemic, the widespread use of face masks has presented a unique challenge for FER, as masks obscure facial expressions and result in a loss of important information. This can lead to a significant decrease in performance for models trained on datasets without masked subjects, with accuracy drops of up to 29% observed in masked test sets. In this context, situational knowledge becomes critical, and special datasets and models that can deal with masked subjects are needed.\n\nMoreover, the field of explainable AI has gained significant momentum in recent years  [9] , as the lack of transparency and interpretability of black-box deep learning systems has become a major concern. However, current ER works do not focus on this issue, and only classify the emotional status of the subject without providing any explanation for their decision. To address this, we propose the use of facial data, scene background, and place type, to create situational knowledge that can explain the results of our emotion recognition system, SAFER.\n\nThe main contributions of this work are:\n\n1. The development of a novel multi-stream emotion recognition system, SAFER, that utilizes deep learning methods to classify emotions from facial expressions, scene background, and location type.\n\n2. Evaluation of the proposed system on several benchmark datasets, and a comparison of our results with other recent works. Additionally, an ablation study is conducted to evaluate the effects of each stream in the learning process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Background And Related Work",
      "text": "Emotions are a natural response to environmental stimuli. Understanding human emotions is crucial for effective communication. Human perception and judgment of situations or individuals are heavily influenced by their emotional state, which can impact activities such as driving a car, learning in a classroom, or interactions with law enforcement, among others.\n\nIn the field of facial expression recognition (FER), it is crucial to accurately detect and segment the facial region from the surrounding image.\n\nVarious techniques have been proposed for this task, such as the Viola-Jones method  [10, 11] , the combination of PCA and Viola-Jones  [12] , and the Haar Cascades method  [13] . These approaches have demonstrated effectiveness in accurately localizing the facial region for further processing in FER systems.\n\nA recent technique proposed by Bazarevsky et al.  [14]  is capable of detecting six landmarks on a face and can handle the detection of multiple faces within an image.\n\nConvolutional neural networks (CNNs) are a widely-utilized deep learning architecture in image analysis. The convolution operation, which applies repeated filters to an input, results in a map of activations known as a feature map. This highlights the locations and strength of features detected in the input, allowing CNNs to identify important parts of an image that differentiate between classes. In a previous study  [15] , the authors employed a CNN to detect emotions from facial expressions in the FER-2013 dataset  [16] .\n\nGan et al.  [17]  achieved improved accuracy on the FER-2013 dataset using ensemble CNN and a novel label level perturbation strategy. In  [18] , authors used the ensemble method and transfer learning with VGG16 and ResNet-50. The SVM is also popular in facial emotion recognition due to its lightweight architecture compared to CNN. Authors at  [24]  used SVM for emotion classification. Their method achieved 91.8% test accuracy on the CK+ dataset.\n\nDeep Belief Network (DBN) is also used in emotion recognition. Authors in  [25]  reported 98% accuracy on the CK+ dataset using the DBN technique.\n\nBias in machine learning is an important challenge  [26, 27] . Buolamwini et. al.  [27]  showed that machine learning algorithms can discriminate based on classes like race and gender. Zeng et al.  [28]  showed the bias in the class distribution of FER training datasets. He proposed a circuit feedback mechanism to tackle the issue.\n\nThe Places Database  [29]  has a massive collection with 10 million labeled scene photographs from around the world. Additionally, it offers various pretrained CNNs (Places-CNNs) for scene classification, which can be used for scene category and attribute identification.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Datasets",
      "text": "In the field of facial emotion recognition, various datasets have been used\n\nto evaluate the performance of different algorithms. We utilized various datasets including FER-2013  [16] , AffectNet  [30]  and RAF-DB  [31]  for our experiments.\n\nThe AffectNet is a comprehensive dataset of facial expressions that were sourced from the Internet via the use of 1,200 keywords related to emotions.\n\nThis database comprises over one million facial images, with 440,000 of them images that have been collected in both posed and wild settings. These images have been classified into seven basic emotion classes. Additionally, the dataset includes data that can be used to train an emotion recognition model to identify emotions even when the user is wearing a face mask. The specifics of this dataset will be discussed in Section 7.\n\nA summary of all these datasets is presented in table 2.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Our Method: Safer",
      "text": "In this section, we present our proposed emotion recognition system SAFER. Figure  3  shows a high-level diagram of the system and its components.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Input",
      "text": "The input of this system is an RGB image that contains the face and the background. The image can be a still photo, a frame from video footage, or a live video feed for continuous monitoring.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Face Detection",
      "text": "The SAFER system requires the separation of the facial area from the rest of the input image. Face detection is accomplished through the use of the Blazeface technique, as described by Bazarevsky et al.  [14] . This method can detect six landmarks on the face and can handle the detection of multiple faces in an image.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Face Feature Extraction",
      "text": "This module consists of three components: the AU feature generator, the visible feature generator, and the deep feature extractor. These three feature  sets are combined to produce the face feature set F f . The different parts of this module are depicted in Figure  6 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Action Unit (Au) Feature Set",
      "text": "From the face mesh (figure  5 ) generated using Blazeface  [14] , we identify\n\n12 key AU centers in the face based on a set of predefined rules outlined in Table  3 . The centers are determined by selecting the closest landmark positions, which simplifies the process without sacrificing accuracy. Finally, we calculate the distances between all AU points to form our AU feature set.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Visible Feature Set",
      "text": "From the face mesh generated using Blazeface  [14] , we create a group of features shown in the Table  4 . They capture various aspects of the face including width, height, distance, and angle of different facial parts. Our approach leverages the observation that changes in facial expression, such as when shouting or laughing, often result in alterations to specific facial features. For example, when smiling or laughing, the mouth tends to open, leading to increased lip width. Conversely, expressions of surprise often result in increased eye width and height. These features are thus valuable for use in facial emotion recognition.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Deep Feature Set",
      "text": "Deep features are the values we obtain from the output of the deep feature extractor as shown in figure  6 . For the feature extractor we experiment with two different CNN model types.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Background Feature Extraction",
      "text": "We first remove the subject body and face from the scene to extract background. The extracted background is then processed through a deep feature extraction network as depicted in Figure  7 . The network consists of three convolutional layers and one fully connected layer, with all convolutional layers utilizing a filter size of 2 × 2 and a stride of 1. The downsampling layers",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Place Feature Set",
      "text": "In this step, we remove the subject body and face from the scene. We use pre-trained AlexNet provided with the Places dataset and pass the scene through it. Deep features are collected from AlexNet after the final maxpooling operation to produce the Location Feature Set (F l ). We also collect final place categories such as 'classroom' and attributes such as 'no horizon' and 'enclosed area' for explanation generation.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Detection Model",
      "text": "The final feature set (F ) is a concatenation of the face feature set (F p ), background feature set (F b ) and place feature set (F l ). We use two FC layers (figure  3 ) for the final classification. Cross-entropy loss is used for the loss function.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we present the experimental evaluation of SAFER on various facial emotion recognition datasets. Our model is trained on the benchmark datasets listed in Table  2 , and its performance is compared to recent approaches in the literature. The experiments were conducted on a server PC that had 20 cores with a 2.6 GHz Intel Xeon CPU and 96 GB of memory, as well as three NVIDIA TESLA GPUs with 24 GB of memory each.\n\nTo facilitate accelerated computing, we used Python multiprocessing and mixed precision libraries. The datasets were split into training, validation, and test sets in an 80:10:10 ratio. All images were resized to 224 × 224 pixels, and dataset augmentation was performed using cropping, rotation, brightness, and contrast adjustments. We employed an adaptive learning rate that started at 1e -5 and a batch size of 32\n\nWe use test accuracy as our performance criteria. The test accuracy is given by the following equation:\n\nWhere #N c indicates the number of items correctly predicted and #N t indicates the total number of items in the test dataset.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Performance Of Safer On Benchmark Datasets",
      "text": "The results of SAFER with some facial emotion recognition benchmark datasets are shown in the table 5. Our results are comparable with the stateof-the-art works in all datasets. For the FABO dataset, we outperform the accuracy reported by various recent works (table  7 ). For the CK+ dataset,",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Method",
      "text": "Year Test Accuracy(%) ECNN  [40]  2017 66.98 Dhankhar  [18]  2019 67.2 Renda  [41]  2019 71 Gan  [17]  2019 73.73 A-C  [19]  2022 72.03 SAFER 2022 75.8\n\nwe find the best-reported result to be 98.57% accuracy as in  [25] , our accuracy of 98.5% is comparable to it.\n\nBesides, we report the results of several recent works on the CAER-S dataset in table  6 . We see SAFER outperforms closest result reported by Li.\n\net al.  [39]  by 7.8%.\n\nIn FER-2013 dataset (table  7 ), our model outperforms results from  [19]  and  [21] . Similarly, table  8  shows results from several recent works on the Af-fectNet dataset. Here, our accuracy is 63.7%. Which is close to the accuracy values reported by  [28]  and  [20] . In  [28] , author provided a effective solution to the class imbalance issue widespread in most of the FER datasets including AffectNet. In AffectNet dataset 'Happiness' class has 146,198 samples while 'Disgust' has only 5,264 samples. For this reason our model gives lower accuracy in 'Disgust' class. However, our work is orthogonal to  [28]  and both can be implemented together. Similarly,  [20]  uses Deep Metric Learning (DML) method with modified loss functions. They argued that using softmax loss can not provide proper discrimination between classes due to inter-class similarity and intra-class variations. Hence they use sparse center loss in adition to the softmax as the final objective function. This work is also orthogonal to our work as this loss function can be used with our method too.\n\nFrom table 6 and 8, it is clear that our model offers greater improvement in the CAER-S dataset than AffectNet. This is due to the presence of less class imbalance and more contextual information in the CAER-S dataset.\n\nFigure  8  shows the confusion matrices of our model for the FER-2013 and CAER-S datasets. In the figure, we list actual emotion labels along the",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Method",
      "text": "Year Test Accuracy(%) DMUE  [23]  2021 63.11 SCN  [42]  2020 60.23 SL  [36]  2016 58.27 F2E  [28]  2022 64.23 A-C  [19]  2022 63.36 DACL  [20]",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Explanation Generation",
      "text": "Besides determining emotion we also provide a guideline to generate an explanation for that result. We can provide human explainable reasoning by creating an idea of the situation around the subject. Individual modules tell us what information is available from the face and background. For instance, the subject in figure  9  red bounding box has a smiling face and colorful vibrant background. By extracting age, gender, location type and location attributes, we can create our situational knowledge which further enhances this reasoning. In the case of figure  9 , place category output is a day care play room. By combining all these a human understandable explanation of happiness class for the subject can be constructed as \"the subject is a child in a playroom and smiling, has a happy facial expression\". This is an early effort for explainable emotion classification using multiple data modalities as per our knowledge. However, our explanation generation still requires further work. Therefore, we are planning to explore explanation generation in a more detailed manner in our future works.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Ablation Study",
      "text": "We show the resulting outputs from various ablation experiments in table 9. We compare the accuracy of our model on AffectNet and CAER-S datasets in several combinations. The combinations are: face feature set F p only, face feature set F p + background feature set F b , face feature set F p + place feature set F l and all three feature sets combined.\n\nFrom the table, we can see that in the AffectNet dataset, operating on face data alone results in an accuracy of 61.9%. Adding background and place feature sets does not improve accuracy significantly. But for the CAER-S dataset, we see the addition of these two extra feature sets result in a noticeable increase in accuracy. Our understanding is that the AffectNet dataset has only face images with a paltry background. They offer very little information for the background and place streams to extract. However, CAER-S is designed for context-based emotion recognition. Hence, we have lots of background information in the samples. That is why the contribution of background and place streams are significant for CAER-S.\n\nWe also analyse the choice of the deep feature extractor. In this case, pre-trained ResNet-50 performs better in our experiments than the regular CNN extractor discussed in section 4.3.3. We also notice that adding an AU feature set and visible feature has a positive effect on the accuracy of the face stream alone. If we remove them and use only the deep features from the face using ResNet-50, the accuracy drops to a lower value.\n\nThus when contextual information is available, adding background or place features improves accuracy and the best accuracy result is achieved when we use all available feature sets. However, if no contextual information is available then these two modules fail to offer meaningful contributions and our model does not provide the best results.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Issues Of Facial Expression Based Emotion Recognition",
      "text": "",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Covid Mask Issue",
      "text": "During the global Covid-19 pandemic, people used to wear a face mask  [43] .\n\nFace mask covers a good part of our face including the nose, lips, mouth and chin. This unexpected situation is a novelty and creates a challenge for facial-based emotion recognition, as the absence of these facial features can result in the loss of important cues for recognition. To examine the impact of masks on facial emotion recognition, a masked section has been included in our proposed DeFi dataset.\n\nTesting on this mask dataset with a regular dataset-trained model gave us only 38.58% accuracy. This highlights the need for a specialized dataset for masked subjects as models trained on regular face images are not effective at classifying emotions from partially visible faces.\n\nWe further trained and evaluated our model using the masked section of our proposed DeFi dataset. The results showed an increased accuracy of approximately 58%, representing a 29% improvement compared to the results from testing the regular dataset-trained model. Despite this improvement, the accuracy still remains lower compared to the results obtained from training and testing on unmasked data.\n\nWe further analyze this result with the feature maps from different convolution layers of the ResNet-50 of our architecture. The selected feature maps from layer 10, 20, 30 and 40 are shown in Figure  10 . A visual inspection of these maps reveals that the lips and nose regions are highlighted in most of them. This highlights the fact that these facial parts are crucial for emotion classification, as feature maps highlight the most important parts of the image. As such, covering these important facial features through the use of masks significantly hinders the ability of facial expression-based models.\n\nOne potential approach to address the issue of face masks affecting facial expression-based emotion recognition is to employ multi-modal emotion detection, which leverages multiple modalities, such as facial expression, posture, and gait, to recognize emotions. However, these multi-modal systems tend to be more complex compared to single-modality facial expression-based systems, and also require diverse types of datasets that are not typically available in most FER datasets, such as posture and gait information. Thus, multi-modal emotion recognition is left as a future work and not addressed in this current study.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Dataset Bias",
      "text": "A good learnt model is dependent on a good dataset. However, without proper care a dataset can lack proper diversity. The model trained on it then performs worse when it encounters minority subjects of the dataset.\n\nFor example, keyword searching in Google with \"angry face\" resulted in 59 acceptable images in the first 100 images. Of these images, only 9 are women while 50 are men. This pattern holds for other generic keywords such as \"sad people\" or \"happy human\". Preparing a dataset by collecting results from generic keywords instead of more specific ones such as \"black woman sad\" has a higher chance of bias against women and people of color.\n\nA similar situation is also applicable to the volunteer choice for creating an acted dataset. Without the careful selection of people from various genders and ethnic backgrounds, dataset bias can be easily incorporated into the model. Table  10  shows the percentage of images with male subjects for different gender-neutral emotion-related keywords for the first 100 results listed by Google. As we can see, the number of male and female images is not equally represented in the results. For anger-related searches, men have a higher percentage of images while in sadness-related searches women tend to appear in larger numbers. We also show the number of males in the first 100 images in the FER-2013 dataset for the \"Angry\", \"Fear\", \"Happy\" and \"Sad\" categories and report similar and unequal male/female representations.\n\nBesides gender and race, many of the existing FER datasets are biased toward majority classes. Some of the classes have a larger number of samples and hence models tend to show favor towards them. For example, in the AffectNet dataset, the 'Happy' class has more than 130k samples and the 'Disgust' and 'Contempt' classes have only a few thousand samples each.",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "Quality Concern Of Current Datasets",
      "text": "In table  2  we listed some widely used datasets for facial emotion recognition. By having a closer look at them, we found certain issues we can work on resolving. Some of the images from CAER-S, Emotic and FER-2013 are shown in figure  11 . The main issues we found in these datasets are:\n\n• Noise issue: Some of the images are not relevant and contain no face.\n\nWe suspect this is due to the Google image scrapping. In our search, we find that Google includes some irrelevant images in the search results.\n\nWe need to clean them manually. Examples of this type of image are shown in the first row of figure 11.\n\n• Quality issue: Some of the images are cropped and lack parts of the face/ full face as shown in the second row of figure  11 . As we can see from figure  10 , we need all those parts of the face for successful recognition.\n\n• Confusing annotation: These are the images where annotations do not match with the image. This is certainly an issue for those images where there is a lack of consensus among human annotators. In some of the datasets where they used frames from video clips of movies and dramas, they annotated all the frames for a clip as a certain class, even though emotion changes from neutral to apex to back to neutral again.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Proposed Defi Dataset",
      "text": "Recent works have proposed several algorithmic methods to deal with the above-mentioned bias in the datasets  [28, 19, 44] . However, these methods do not mitigate the problem completely. Hence in this work, we intend to propose a new dataset where samples are carefully chosen to be more inclusive. This is an orthogonal approach to the algorithmic solutions and they can work together.\n\nWe first start with improving the existing datasets according to the issues outlined above. We have 8 volunteers, 4 males and 4 females, all aged 18-30 years and university students, checking the datasets for any irrelevant images similar to the first row in the figure  11 . They also annotate the images individually. From these annotations, we keep images with at least 4 annotators agreeing on the label. We call it the 80% consensus method.\n\nAfter these steps, approximately 7.5% images are removed from the original FER-2013 dataset. From FABO dataset videos, we extract the frames at a 10 FPS rate. As the author outlined, subjects in this dataset go from neutral to apex emotion state and then back to neutral. Our volunteers collect the images showing apex emotion states from the extracted frames. We again follow 80% consensus among the annotators for selecting an image in this step.\n\nBesides working on existing datasets we record video clips showing 7 emotional states from a group of 10 volunteers. Due to the Covid 19 pandemic, we do not meet the participants in person, rather they are recruited over social media platforms. These volunteers are aged 18-30 and from both genders with diverse racial and ethnic backgrounds. All are university students.\n\nEach participant records video clips of their best impression of these emotions and provides us with the clips. We extract the video frames as 10 FPS and perform the 80% consensus check by the annotators.\n\nTo increase diversity in the dataset and reduce bias, we include people from multiple geographic and ethnic backgrounds by collecting images from Google searches using keywords such as \"black man happy face\", \"Indian woman sad face\", \"Asian man angry face\" etc. Collected images from these Google searches are checked for irrelevant images and tested for 80% consensus in their annotations. By similar keyword searching on YouTube and other streaming platforms we collect relevant video clips from which we generate dataset images and labels.\n\nBy aggregating these images from various sources we create our new dataset DeFi. We believe it is free of irrelevant images, more accurately  For all the images in DeFi, we create another dataset that imitates the face with the mask. This can be useful for other researchers who want to work with a dataset with masked people. To the best of our knowledge, any such dataset does not exist yet. The complete dataset will be publicly available at the provided web location  [45] .\n\nWe test SAFER in this new dataset. For both types of feature extractor of SAFER the accuracy is shown in the table 11.",
      "page_start": 28,
      "page_end": 29
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In conclusion, this paper presents SAFER, a novel system for emotion recognition from facial expressions that leverages action units, facial features, and state-of-the-art deep learning techniques. The system showed improved performance on benchmark datasets and demonstrated the positive effect of background and place features on recognition accuracy. Furthermore, the impact of mask-wearing during the Covid-19 pandemic on facial expressionbased emotion recognition was analyzed and a novel dataset was proposed to address this novelty situation. In future work, we aim to investigate multi-modal techniques for novelty detection and mitigation in facial emotion recognition. These efforts promise to advance the state of the art in this field and have the potential to improve the robustness and effectiveness of facial emotion recognition systems.",
      "page_start": 31,
      "page_end": 31
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Universal facial expressions of basic emotions [8]",
      "page": 3
    },
    {
      "caption": "Figure 1: illustrate some common facial expressions",
      "page": 3
    },
    {
      "caption": "Figure 2: (a) and 2(b) illus-",
      "page": 4
    },
    {
      "caption": "Figure 2: Importance of background in emotion recognition. (a) Without background",
      "page": 5
    },
    {
      "caption": "Figure 3: The SAFER system architecture consists of three parallel streams working on",
      "page": 6
    },
    {
      "caption": "Figure 3: shows a high-level diagram of the system and its com-",
      "page": 10
    },
    {
      "caption": "Figure 4: Facial area detection and separation",
      "page": 12
    },
    {
      "caption": "Figure 5: Face mesh generation.",
      "page": 12
    },
    {
      "caption": "Figure 6: 4.3.1. Action Unit (AU) Feature Set",
      "page": 12
    },
    {
      "caption": "Figure 5: ) generated using Blazeface [14], we identify",
      "page": 12
    },
    {
      "caption": "Figure 6: Face feature extraction. Three parallel modules work on face image to generate",
      "page": 13
    },
    {
      "caption": "Figure 6: For the feature extractor we experiment",
      "page": 13
    },
    {
      "caption": "Figure 7: The network consists of three",
      "page": 15
    },
    {
      "caption": "Figure 7: Background feature extraction module. Subject face and body is removed from",
      "page": 16
    },
    {
      "caption": "Figure 3: ) for the final classification. Cross-entropy loss is used for the loss",
      "page": 16
    },
    {
      "caption": "Figure 8: shows the confusion matrices of our model for the FER-2013",
      "page": 19
    },
    {
      "caption": "Figure 8: Confusion matrices of our model. (a) FER-2013 dataset (b) CAER-S dataset.",
      "page": 21
    },
    {
      "caption": "Figure 9: Explanation generation",
      "page": 22
    },
    {
      "caption": "Figure 9: red bounding box has a smiling face and colorful",
      "page": 22
    },
    {
      "caption": "Figure 9: , place category output is a day care",
      "page": 22
    },
    {
      "caption": "Figure 10: A visual inspec-",
      "page": 25
    },
    {
      "caption": "Figure 10: SAFER feature maps for an input image on different convolution layers high-",
      "page": 26
    },
    {
      "caption": "Figure 11: The main issues we found in these datasets are:",
      "page": 27
    },
    {
      "caption": "Figure 10: , we need all those parts of the face for successful",
      "page": 28
    },
    {
      "caption": "Figure 11: Quality issues in current datasets. From top to bottom non-relevant images,",
      "page": 29
    },
    {
      "caption": "Figure 11: They also annotate the",
      "page": 29
    }
  ],
  "tables": [
    {
      "caption": "Table 1: A list of facial expressions corresponding to different emotion categories [5]",
      "data": [
        {
          "Emotion Type": "Anger",
          "Corresponding Facial Expression": "Eyebrows pulled down, upper eyelids pulled up,\nlower\neyelids pulled up, margins of\nlips rolled in,\nlips may\nbe tightened"
        },
        {
          "Emotion Type": "Fear",
          "Corresponding Facial Expression": "Eyebrows pulled up and together, upper eyelids\npulled up, mouth stretched"
        },
        {
          "Emotion Type": "Disgust",
          "Corresponding Facial Expression": "Eyebrows pulled down, nose wrinkled, upper lip\npulled up,\nlips loose"
        },
        {
          "Emotion Type": "Happiness",
          "Corresponding Facial Expression": "Muscle around the eyes tightened, “crows feet” wrin-\nkles around the eyes, cheeks raised,\nlip corners raised\ndiagonally"
        },
        {
          "Emotion Type": "Sadness",
          "Corresponding Facial Expression": "Inner corners of eyebrows raised, eyelids loose,\nlip cor-\nners pulled down"
        },
        {
          "Emotion Type": "Surprised",
          "Corresponding Facial Expression": "Entire eyebrow pulled up, eyelids pulled up, mouth\nhangs open, pupils dilated"
        },
        {
          "Emotion Type": "Contempt",
          "Corresponding Facial Expression": "Entire eyebrow pulled up, eyelids pulled up, mouth\nhangs open, pupils dilated"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: Summary of the various Facial Expression Recognition (FER) datasets, with",
      "data": [
        {
          "Name": "CK+",
          "No of Items": "593",
          "Type": "Video",
          "Setting": "Posed and\nsponta-\nneous",
          "Classes": "N, S, Sr,\nH, F, A\nand D",
          "Author": "[32]"
        },
        {
          "Name": "FER-2013",
          "No of Items": "35,887",
          "Type": "Image",
          "Setting": "Posed",
          "Classes": "N, S, Sr,\nH, F, A\nand D",
          "Author": "[16]"
        },
        {
          "Name": "Emotic",
          "No of Items": "23,571",
          "Type": "Image",
          "Setting": "Wild",
          "Classes": "N, S, Sr,\nH, F, A,\nD and\n19 other\nclasses",
          "Author": "[33]"
        },
        {
          "Name": "AffectNet",
          "No of Items": "450,000",
          "Type": "Image",
          "Setting": "Wild",
          "Classes": "N, S, Sr,\nH, F, A,\nD and C",
          "Author": "[30]"
        },
        {
          "Name": "RAF-DB",
          "No of Items": "29672",
          "Type": "Image",
          "Setting": "Wild",
          "Classes": "N, S, Sr,\nH, F, A\nand D",
          "Author": "[31]"
        },
        {
          "Name": "CAER-S",
          "No of Items": "70,000",
          "Type": "Image",
          "Setting": "TV shows",
          "Classes": "N, S, Sr,\nH, F, A\nand D",
          "Author": "[34]"
        },
        {
          "Name": "FABO",
          "No of Items": "206",
          "Type": "Video",
          "Setting": "Posed",
          "Classes": "N, S, Sr,\nH, F, A,\nB, P, Ax\nand D",
          "Author": "[35]"
        },
        {
          "Name": "DeFi",
          "No of Items": "21,000",
          "Type": "Image",
          "Setting": "Posed and\nwild",
          "Classes": "N, S, Sr,\nH, F, A\nand D",
          "Author": "This work"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Rules to select 12 action unit (AU) centers of the face.",
      "data": [
        {
          "AU ID": "1\n2\n4\n6\n7\n10\n12\n14\n15\n17\n23\n24",
          "Name": "Inner Brow Raiser\nOuter Brow Raiser\nBrow Lowerer\nCheek Raiser\nLid Tightener\nUpper Lip Raiser\nLip Corner Puller\nDimpler\nLip Corner Depressor\nChin Raiser\nLip Tightener\nLip Pressor",
          "Rule": "Above inner brow\nAbove outer brow\nAt brow center\nAt cheek center\nTop eye lid center\nUpper lip center\nLip corner\nBelow lip corner\nLip corner\nchin center\nBottom lip center\nBottom lip center"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 3: Rules to select 12 action unit (AU) centers of the face.",
      "data": [
        {
          "Feature Type": "Width",
          "Feature Description": "Left Eye\nRight Eye\nMouth"
        },
        {
          "Feature Type": "Height",
          "Feature Description": "Right Eye\nLeft eye\nRight eye\nMouth"
        },
        {
          "Feature Type": "Distance",
          "Feature Description": "Left and right eyes\nEyes to brows\nEyes to mouth\nEyes and nose\nNose and mouth"
        },
        {
          "Feature Type": "Angle",
          "Feature Description": "Left eye with right eye and mouth\nRight Eye with left eye and mouth\nMouth with both eyes\nMouth with both eyes"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 5: SAFER performance on various emotion recognition datasets",
      "data": [
        {
          "Dataset": "CK+\nFER-2013\nAffectNet\nRAF-DB\nCAER-S\nFABO",
          "Test Accuracy(%)": "98.5\n75.8\n62.1\n87.1\n91.4\n96.1"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 5: SAFER performance on various emotion recognition datasets",
      "data": [
        {
          "Method": "Lee et al.\n[38]\nKosti et al.\n[33]\nLi et al.\n[39]\nSAFER",
          "Year": "2019\n2019\n2021\n2022",
          "Test Accuracy(%)": "73.51\n74.48\n84.82\n91.4"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 5: SAFER performance on various emotion recognition datasets",
      "data": [
        {
          "Method": "ECNN[40]\nDhankhar[18]\nRenda[41]\nGan[17]\nA-C[19]\nSAFER",
          "Year": "2017\n2019\n2019\n2019\n2022\n2022",
          "Test Accuracy(%)": "66.98\n67.2\n71\n73.73\n72.03\n75.8"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 8: SAFER performance comparison on AffectNet dataset.",
      "data": [
        {
          "Method": "DMUE [23]\nSCN [42]\nSL [36]\nF2E [28]\nA-C [19]\nDACL [20]\nSAFER",
          "Year": "2021\n2020\n2016\n2022\n2022\n2021\n2022",
          "Test Accuracy(%)": "63.11\n60.23\n58.27\n64.23\n63.36\n65.2\n63.7"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F": "✓",
          "B": "",
          "P": "",
          "AffectNet": "61.9",
          "CAER-S": "86.7"
        },
        {
          "F": "✓",
          "B": "✓",
          "P": "",
          "AffectNet": "62.01",
          "CAER-S": "89.3"
        },
        {
          "F": "✓",
          "B": "",
          "P": "✓",
          "AffectNet": "61.95",
          "CAER-S": "90.1"
        },
        {
          "F": "✓",
          "B": "✓",
          "P": "✓",
          "AffectNet": "62.1",
          "CAER-S": "91.4"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 2: we listed some widely used datasets for facial emotion recogni-",
      "data": [
        {
          "Keyword": "”Angry people”\n”Fear face”\n”Happy human face”\n”Sad human face”",
          "# Male in Google(%)": "84.7\n60.1\n55.8\n40.0",
          "# Male in FER-2013": "70\n52\n58\n45"
        }
      ],
      "page": 27
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "Fear-type emotion recognition for future audio-based surveillance systems",
      "authors": [
        "C Clavel",
        "I Vasilescu",
        "L Devillers",
        "G Richard",
        "T Ehrette"
      ],
      "year": "2008",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Acoustic and lexical sentiment analysis for customer service calls",
      "authors": [
        "B Li",
        "D Dimitriadis",
        "A Stolcke"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "A novel eeg-based emotion recognition approach for e-healthcare applications",
      "authors": [
        "M Ali",
        "F Al Machot",
        "A Mosa",
        "K Kyamakya"
      ],
      "year": "2016",
      "venue": "Proceedings of the 31st Annual ACM Symposium on Applied Computing"
    },
    {
      "citation_id": "5",
      "title": "the-seven-universal-emotions-we-wear-on-our-face",
      "venue": "the-seven-universal-emotions-we-wear-on-our-face"
    },
    {
      "citation_id": "6",
      "title": "Facial sentiment analysis using ai techniques: state-of-theart, taxonomies, and challenges",
      "authors": [
        "K Patel",
        "D Mehta",
        "C Mistry",
        "R Gupta",
        "S Tanwar",
        "N Kumar",
        "M Alazab"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "7",
      "title": "On the universality and cultural specificity of emotion recognition: a meta-analysis",
      "authors": [
        "H Elfenbein",
        "N Ambady"
      ],
      "year": "2002",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "8",
      "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai)",
      "authors": [
        "A Adadi",
        "M Berrada"
      ],
      "year": "2018",
      "venue": "IEEE access"
    },
    {
      "citation_id": "9",
      "title": "Facial expression recognition and emotion classification system for sentiment analysis",
      "authors": [
        "J Jayalekshmi",
        "T Mathew"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Networks & Advances in Computational Technologies (Ne-tACT)"
    },
    {
      "citation_id": "10",
      "title": "A convolutional neural network cascade for face detection",
      "authors": [
        "H Li",
        "Z Lin",
        "X Shen",
        "J Brandt",
        "G Hua"
      ],
      "year": "2015",
      "venue": "A convolutional neural network cascade for face detection"
    },
    {
      "citation_id": "11",
      "title": "Face expression recognition system based on ripplet transform type ii and least square svm",
      "authors": [
        "N Kar",
        "K Babu",
        "A Sangaiah",
        "S Bakshi"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "12",
      "title": "Analysis of facial landmark features to determine the best subset for finding face orientation",
      "authors": [
        "H Shah",
        "A Dinesh",
        "T Sharmila"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Computational Intelligence in Data Science (ICCIDS)"
    },
    {
      "citation_id": "13",
      "title": "Sub-millisecond neural face detection on mobile gpus",
      "authors": [
        "V Bazarevsky",
        "Y Kartynnik",
        "A Vakunov",
        "K Raveendran",
        "M Grundmann",
        "Blazeface"
      ],
      "year": "2019",
      "venue": "Sub-millisecond neural face detection on mobile gpus",
      "arxiv": "arXiv:1907.05047"
    },
    {
      "citation_id": "14",
      "title": "Content based facial emotion recognition model using machine learning algorithm",
      "authors": [
        "R Jadhav",
        "P Ghadekar"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Advanced Computation and Telecommunication (ICACAT)"
    },
    {
      "citation_id": "15",
      "title": "Fer-2013 learn facial expressions from an image",
      "venue": "Fer-2013 learn facial expressions from an image"
    },
    {
      "citation_id": "16",
      "title": "Facial expression recognition boosted by soft label with a diverse ensemble",
      "authors": [
        "Y Gan",
        "J Chen",
        "L Xu"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "17",
      "title": "Resnet-50 and vgg-16 for recognizing facial emotions",
      "authors": [
        "P Dhankhar"
      ],
      "year": "2019",
      "venue": "International Journal of Innovations in Engineering and Technology (IJIET)"
    },
    {
      "citation_id": "18",
      "title": "Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild",
      "authors": [
        "A Fard",
        "M Mahoor"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "19",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "A Farzaneh",
        "X Qi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "20",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "21",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "22",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "J She",
        "Y Hu",
        "H Shi",
        "J Wang",
        "Q Shen",
        "T Mei"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Integrating geometric and textural features for facial emotion classification using svm frameworks",
      "authors": [
        "S Datta",
        "D Sen",
        "R Balasubramanian"
      ],
      "year": "2017",
      "venue": "Proceedings of International Conference on Computer Vision and Image Processing"
    },
    {
      "citation_id": "24",
      "title": "Semi-supervised facial expression recognition using reduced spatial features and deep belief networks",
      "authors": [
        "A Kurup",
        "M Ajith",
        "M Ramón"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "25",
      "title": "Can the biases in facial recognition be fixed; also, should they?",
      "authors": [
        "P Marks"
      ],
      "year": "2021",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "26",
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Conference on fairness, accountability and transparency"
    },
    {
      "citation_id": "27",
      "title": "Face2exp: Combating data biases for facial expression recognition",
      "authors": [
        "D Zeng",
        "Z Lin",
        "X Yan",
        "Y Liu",
        "F Wang",
        "B Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "Places: A 10 million image database for scene recognition",
      "authors": [
        "B Zhou",
        "A Lapedriza",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "31",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "32",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "33",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "34",
      "title": "Bi-modal emotion recognition from expressive face and body gestures",
      "authors": [
        "H Gunes",
        "M Piccardi"
      ],
      "year": "2007",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "35",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep residual learning for image recognition",
      "arxiv": "arXiv:1512.03385"
    },
    {
      "citation_id": "36",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "37",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "38",
      "title": "Human emotion recognition with relational region-level analysis",
      "authors": [
        "W Li",
        "X Dong",
        "Y Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Ensemble of deep neural networks with probability-based fusion for facial expression recognition",
      "authors": [
        "G Wen",
        "Z Hou",
        "H Li",
        "D Li",
        "L Jiang",
        "E Xun"
      ],
      "year": "2017",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "40",
      "title": "Comparing ensemble strategies for deep learning: An application to facial expression recognition",
      "authors": [
        "A Renda",
        "M Barsacchi",
        "A Bechini",
        "F Marcelloni"
      ],
      "year": "2019",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "41",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "42",
      "title": "",
      "authors": [
        "Cdc"
      ],
      "venue": ""
    },
    {
      "citation_id": "43",
      "title": "Bias in machine learning software: why? how? what to do?",
      "authors": [
        "J Chakraborty",
        "S Majumder",
        "T Menzies"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering"
    },
    {
      "citation_id": "44",
      "title": "Defi dataset",
      "venue": "Defi dataset"
    }
  ]
}