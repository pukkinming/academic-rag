{
  "paper_id": "2401.10946v1",
  "title": "Self Context-Aware Emotion Perception On Human-Robot Interaction",
  "published": "2024-01-18T10:58:27Z",
  "authors": [
    "Zihan Lin",
    "Francisco Cruz",
    "Eduardo Benitez Sandoval"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition plays a crucial role in various domains of human-robot interaction. In long-term interactions with humans, robots need to respond continuously and accurately, however, the mainstream emotion recognition methods mostly focus on short-term emotion recognition, disregarding the context in which emotions are perceived. Humans consider that contextual information and different contexts can lead to completely different emotional expressions. In this paper, we introduce self context-aware model (SCAM) that employs a two-dimensional emotion coordinate system for anchoring and re-labeling distinct emotions. Simultaneously, it incorporates its distinctive information retention structure and contextual loss. This approach has yielded significant improvements across audio, video, and multimodal. In the auditory modality, there has been a notable enhancement in accuracy, rising from 63.10% to 72.46%. Similarly, the visual modality has demonstrated improved accuracy, increasing from 77.03% to 80.82%. In the multimodal, accuracy has experienced an elevation from 77.48% to 78.93%. In the future, we will validate the reliability and usability of SCAM on robots through psychology experiments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human-robot interaction has become increasingly important due to the widespread use of robots in various applications such as manufacturing, healthcare, and personal assistance  [Kyrarini et al., 2021] . Human-robot interaction focuses on how humans and robots can safely and effectively collaborate, requiring natural and intuitive communication between them. To achieve better communication, it is crucial for robots to understand human emotions; otherwise, they may respond incorrectly, leading humans to reject interacting with the robots  [Tsiourti et al., 2019] . However, emotion recognition is a complex process, involving various perceptual dimensions and temporal aspects. Current emotion recognition models primarily focus on multimodal perception but often overlook contextual information  [Poria et al., 2019] . In real-life conversational contexts, emotional fluctuations in individuals often display a sense of continuity. This implies that, under typical circumstances, emotions do not undergo abrupt and dramatic shifts within a brief timeframe, such as sudden transitions from intense anger to extreme happiness. Consequently, when emotions cannot be ascertained, humans frequently depend on contextual information to make judgments  [Sacharin et al., 2012] . Practically, there is often a requirement to rely exclusively on preceding contextual information.\n\nTherefore, this paper introduces self context-aware model (SCAM), enabling a robot to perform emotion recognition on the user while simultaneously considering the user's preceding emotional context and integrating it with the robot's recognition results from the preceding context. This approach allows for a more comprehensive and accurate assessment of the user's emotion state during human-robot interactions.\n\nThe contributions of this work are summarized as follows:\n\n• We utilize the relationship between valence, arousal, and emotion to enable the model to learn basic emotions from non-basic ones.\n\n• We introduce a novel contextual loss, incorporating the model's predictions of context emotion, valence, and arousal, allowing the model to more effectively capture emotional change trends.\n\n• We model the information transfer within the context, preserving valuable features from the preceding context for integration and judgment when making predictions for the subsequent context.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "The application of emotion recognition spans a wide range of domains, including its deployment in various human-computer interaction scenarios and chatbot systems designed to generate emotionally rich dialogues  [Zhou et al., 2018] . Nevertheless, this field is fraught with numerous challenges. For instance, individuals experiencing mental distress may be reluctant to unveil their vulnerabilities, often concealing their true emotional states  [Maithri et al., 2022] . Consequently, researchers have delved extensively into the realm of emotion recognition, exploring various modalities such as visual, auditory, physical, and even the incorporation of EEG and skin conductance signals  [Li et al., 2022] . Typically, amalgamating information from diverse modalities yields enhanced accuracy in emotion recognition  [Wu et al., 2022] . Notably, the fusion of speech and text modalities achieved a remarkable accuracy rate of 80.51% (four categories) on the IEMOCAP dataset  [Atmaja et al., 2022] . However, some studies  [Tsiourti et al., 2019]  underscore the intricacy of emotion recognition when information conflicts arise between modalities. Furthermore, some researchers suggested that better results can be achieved by capturing contextual information within conversations.  [Priyasad et al., 2020]  employed graph neural networks to model inter-dialogue relationships, achieving commendable performance. They introduced an iterative emotion interaction network that employs iteratively predicted emotion labels to explicitly model emotion interactions, culminating in an accuracy of 64.37% (seven categories) on the IEMOCAP dataset. Compared to providing a dialogue-based approach, we take into consideration that during human-robot interaction, the robot may struggle to provide sufficient feedback. Therefore, we propose a method to utilize self context.\n\nAnother pivotal dimension of research in emotion recognition is the exploration of dimensional emotion models. In contrast to discrete emotion models, valencearousal model  [Russell, 1980]  offers a better understanding of the intricate relationships among different emotional states. Research demonstrated that combining valence, arousal, dominance, and the polarity of emotions within a multi-view training framework can yield superior results  [Tompkins et al., 2023] . However, the current use of dimensional models has not effectively leveraged the continuity of dimensional models in emotional expression, failing to fully exploit their advan-tages. In comparison to discrete emotion models, dimensional models can better observe the trend of emotional changes. In our approach, we make the first attempt to utilize this aspect and have achieved excellent results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "During the process of human-robot interaction, longlasting sessions can be divided into multiple segments, and they may vary with their own context and the robot's responses. The emotions in each segment are relatively independent, with a correlation observed between adjacent segments due to the continuous nature of human emotions. Therefore, we group some adjacent segments into a composition (Figure  1 ). By using SCAM to capture this correlation, we achieve better emotion perception results.\n\nFigure  1 : Context interaction in HRI SCAM consists of two main components: a multi-task network for each segment and a self context-aware structure for composition. The multi-task network relabels emotions and then utilizes ResNet101 and Bi-LSTM to recognize emotion, valence, and arousal within a short time. The self context-aware structure incorporates contextual information propagation and context loss, combining the context information and predictions from preceding segments with the current input to predict the current emotion, valence, and arousal. In the following sections, we will provide further details on these components.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "Considering there is currently no publicly available and suitable human-robot interaction dataset, we conduct model training and validation on IEMOCAP dataset  [Busso et al., 2008] , which comprises 10,039 instances performed by 10 actors. These actors are paired, and each pair engages in multiple scripted and spontaneous emotional dialogues, which are appropriate for the simulation of human-robot interaction. Throughout these dialogues, they portray 10 predefined emotional states: angry, sad, happy, neutral, disgust, surprise, fear, excited, other, and unmarked. Each emotional state includes multiple sentences and encompasses various modalities, including audio, video, text, and more. • Frame Segmentation: Divide the audio signal into small segments,\n\n• Windowing: Apply a window function to each frame to reduce the impact of spectral leakage.\n\n• Fourier Transform: Apply the Discrete Fourier Transform (DFT) to each window, transforming the time-domain signal into a frequency-domain signal.\n\n• Magnitude and Squaring: Compute the amplitude spectrum for each frequency component, often by taking the magnitude of the complex values.\n\n• Visualization: Display the obtained spectral information as an image, with the horizontal axis representing time, the vertical axis representing frequency, and color or brightness representing amplitude.\n\nFour emotions (angry, happy, neutral, and sad) and two modalities (auditory and visual) in IEMOCAP are used to verify our approach in order to compare with other methods, as shown in Table  1 . Emotions are annotated using two mainstream approaches. One approach considers emotions as fixed labels, where each emotion is treated as a category such as angry, happy, sad, or neutral. Each sample is assigned only one emotion label, representing a specific emotion category. The other approach  [Russell, 1980]  involves emotion dimensions, where each sample has two labels annotated with valence and arousal. Valence represents one dimension of emotion (e.g., emotional intensity), and arousal represents another dimension (e.g., positivity and negativity). This approach better captures complex emotional changes, as emotions can be regarded as points in an emotion space rather than single categories.\n\nWe calculate the mean of valence and arousal as coordinates for each emotion in the emotion space, as shown in Figure  2 . Within the same composition, emotions tend to undergo a transition and be closer to the emotion of the current segment (the last segment in the composition). Capturing such contextual emotion changes can improve the continuity and accuracy of emotion perception.\n\nFrame Segmentation: Divide the audio signal into small segments, Windowing: Apply a window function to each frame to reduce the impact of spectral leakage.  For the visual modality, considering the short duration of each segment and the limited facial expression changes, three frames are recorded. We extract the start, intermediate, and end frames from the corresponding video segments and crop the facial regions (Figure  4 ). The intermediate frame is calculated using the start and end times of each segment.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Segment Relabeling",
      "text": "In the context of our contextual emotion model, each composition comprises multiple segments, and our objective is to predict the emotion of the last segment in each composition (the emotion at the current time). Given that emotions tend to exhibit relatively small variations within a composition, we select the emotion of the last segment as the emotion label for the entire composition, while valence and arousal remain unchanged, as shown in Table  2 . This approach offers two advantages. Firstly, it effectively leverages data from emotion labels other than the four basic emotions, enriching the dataset. Secondly, one segment's emotion may lead to different current emotions in different compositions. In the following section, we will explain how we utilize this feature.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss In Segment",
      "text": "Due to the relabeling of emotions for segments within each composition, in most cases, relabeled emotions closely match or are similar to the original emotions, owing to the continuity of emotions. However, there are instances where inconsistencies in emotions arise. As mentioned before, we compute the average valence and arousal for different emotions, serving as reference points on the two-dimensional emotion coordinate system. Simultaneously, we retain the original valence and arousal labels for each segment. Therefore, we utilize the Euclidean distance to measure the distance between the relabeled emotions and the original emotions and scale the emotion loss accordingly, as indicated by the following formula:\n\nwhich N represents the number of samples, y ij denotes the actual emotion labels, and p(y ij ) represents the predicted probabilities of emotions by the model. R is defined as: R = 1 (x emo -x seg ) 2 + (y emo -y seg ) 2 .\n\nx emo and y emo represent the valence and arousal of the relabel emotion (e.g., x emo and y emo of anger), and x seg and y seg represent the label of valence and arousal of the segment.\n\nFor valence and arousal regression use mean squared error, the formulas are as follows,\n\nwhich xseg and ŷseg represent the prediction of valence and arousal seperately.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Self Context-Aware Structure",
      "text": "Self context-aware structure is the core optimization component of our model, primarily consisting of two parts: contextual information propagation and context loss. Through these components, we achieve the perception of current emotion with the aid of contextual information. The procedure is shown in Figure  6 .  The output of the Bi-LSTM can be represented as {h (2) 2 , . . .} represents the output of the Bi-LSTM in the second segment, and so forth. Since the Bi-LSTM is bidirectional, each h (j) i contains both forward and backward propagation information and is typically represented as h\n\nwhere --→ h Next, for each segment, we extend the output of the last time step to have the same dimensions as the input features for the LSTM time steps. This extension is represented as:\n\next represents the extended output, and U denotes an upsampling layer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "--→ H (J)",
      "text": "-1 represents the output of the last time step in the forward propagation for the j-th segment, and\n\n-1 represents the output of the last time step in the backward propagation for the j-th segment.\n\nFinally, we concatenate the extended output with all other features in the next segment, which is represented as:\n\nwhich X (j+1) represents the input for the Bi-LSTM in the next segment, h In this way, the model can utilize high-dimensional emotional information from the previous segment to better understand the emotions in the current segment.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Context Loss",
      "text": "The context loss is employed to capture emotional variations between adjacent segments. We represent the valence and arousal of each segment within a composition on a two-dimensional emotion coordinate system, as illustrated in Figure  2 . The vectors formed between consecutive segments depict the trends in emotional changes. We measure the distance between the predicted vectors and actual vectors using cosine similarity, thus forming the context loss, as shown below:\n\n, where j -i = 1.\n\ni represents the ith segment, j represents the jth segment, and j -i = 1 denotes adjacent segments.\n\nThe following notations are used: (x i pre , y i pre ) denotes the predicted coordinates of the valence and arousal for the ith segment. (x j pre , y j pre ) denotes the predicted coordinates of the valence and arousal for the jth segment.\n\n(x i label , y i label ) denotes the true coordinates of the valence and arousal for the ith segment. (x j label , y j label ) denotes the true coordinates of the valence and arousal for the jth segment.\n\nSince j -i = 1, we can form two vectors : v ij pre = (x j pre -x i pre , y j pre -y i pre ), representing the predicted emotion change from i to j. v ij label = (x j label -x i label , y j labely i label ), representing the labeled emotion change from i to j. As shown in Figure  7 , in which orange points rep-",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "Due to the random selection of the test dataset (10% of IEMOCAP of four emotions), some segments have discontinuous context. In such cases, we replicate the current segment as a substitute for the missing context. We compare the results of the auditory modality (A-SCAM), visual modality (V-SCAM), and multimodal (M-SCAM) with the following baselines, as shown in Table  3 .\n\nIn the auditory modality, SCAM achieves an accuracy of 72.46%, in the visual modality, it reaches the highest accuracy of 80.82%, and in the multimodal, it achieves an accuracy of 78.93%. Though the multimodal per-",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Analysis Efficiency Of Context Loss",
      "text": "We also evaluate the results when the loss is minimized on the test set, as shown in Table  4 .\n\nIn our experiments, a difference in the trends of accuracy and loss is observed. The lowest loss occurrs significantly earlier than the highest accuracy. In the case of a single segment, this difference is primarily influenced by the multitask loss. Although valence and arousal are simultaneously predicted, the primary task remains emotion classification. Therefore, loss and accuracy may not be entirely correlated. For SCAM, the loss composition becomes more complex. It includes context loss due to the relabeling process and the necessity to predict the emotion of preceding segments in order to predict the current emotion. This context loss may not necessarily reflect the current emotion and may even conflict with the current emotion, valence, and arousal. Taking the auditory modality as an example, even though the total loss (Figure  11 ) on the test set fluctuates and even increases, the context loss (Figure  12 ) consistently decreases. This indicates that the model is effectively learning the contextual relationships of emotions, resulting in improved emotion classification results, as shown in Figure  13  Ablation Study In the case of using only a single segment, where emotional loss is not scaled and there is no self-context-aware   It can be observed that, with the application of SCAM, the performance of the auditory modality improves by 9.36%, the visual modality improves by 3.79%, and the multimodal approach improves by 1.45%. This further demonstrates the effectiveness of SCAM across different modalities. Furthermore, in Figure  14 , visualization of a composition highlights SCAM's remarkable contextual awareness, independent of the consistency in contextual labels. Even when the context undergoes continuous changes, SCAM correctly identifies emotions.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Multimodal",
      "text": "In general, multimodal performance is expected to surpass unimodal performance. However, in our experiments, the multimodal performance is not superior to the unimodal performance. Therefore, we conduct further analysis of the classification results for the auditory and visual modalities. In the confusion matrix in It can be observed that the visual modality is better at recognizing happy, while neutral and angry have a considerable number of correctly classified samples independently by both modalities. This suggests that the two modalities obtain different features for neutral and angry emotions. In some cases, the auditory modality incorrectly identifies angry as neutral and neutral as angry, but the visual modality correctly identifies them, and vice versa. Furthermore, both modalities' primary errors are concentrated in misclassifying some other emotions as neutral, and samples misclassified as neutral differ significantly. Based on the error distribution, the errors made by both modalities are quite similar.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce SCAM, a method that leverages the user's emotion context and features during longterm human-robot interactions for emotion perception. Additionally, we innovatively combine continuous emotion models with discrete emotion models, anchoring the relationships between different emotions using valence and arousal, achieving outstanding performance. Through ablation experiments, we further demonstrate that SCAM significantly improves accuracy in emotion recognition, valence regression, and arousal regression in auditory, visual, and multimodal modalities. Moreover, through data visualization, SCAM performs effectively even in scenarios with continuous changes in context.\n\nIn future work, we will further collect data on robots to validate the reliability of the methods and conduct psychological experiments to analyze the usability of robot emotion perception. Regarding the multimodal conflicts arising from the similarity in probability distributions between the auditory and visual modalities, we will consider introducing additional modal information for emotion perception.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). By using SCAM",
      "page": 2
    },
    {
      "caption": "Figure 1: Context interaction in HRI",
      "page": 2
    },
    {
      "caption": "Figure 2: Within the same composition, emotions",
      "page": 3
    },
    {
      "caption": "Figure 2: IEMOCAP emotions on Valence-Arousal axis",
      "page": 3
    },
    {
      "caption": "Figure 3: For the visual modality, considering the short dura-",
      "page": 3
    },
    {
      "caption": "Figure 3: Log-Mel spectrogram of one segment",
      "page": 4
    },
    {
      "caption": "Figure 4: Cropped frames of one segment",
      "page": 4
    },
    {
      "caption": "Figure 5: Figure 5: Segment structure (multimodal)",
      "page": 4
    },
    {
      "caption": "Figure 6: Figure 6: Context-aware structure",
      "page": 5
    },
    {
      "caption": "Figure 2: The vectors formed be-",
      "page": 5
    },
    {
      "caption": "Figure 7: , in which orange points rep-",
      "page": 6
    },
    {
      "caption": "Figure 7: Context loss in valence-arousal axis",
      "page": 6
    },
    {
      "caption": "Figure 8: Confusion matrix of A-SCAM",
      "page": 6
    },
    {
      "caption": "Figure 9: Confusion matrix of V-SCAM",
      "page": 6
    },
    {
      "caption": "Figure 8: , Figure 9, and",
      "page": 7
    },
    {
      "caption": "Figure 10: In general, the visual modality performs bet-",
      "page": 7
    },
    {
      "caption": "Figure 10: Confusion matrix of M-SCAM",
      "page": 7
    },
    {
      "caption": "Figure 11: ) on the test set fluctuates and",
      "page": 7
    },
    {
      "caption": "Figure 12: ) consistently",
      "page": 7
    },
    {
      "caption": "Figure 13: Ablation Study",
      "page": 7
    },
    {
      "caption": "Figure 11: Test loss of auditory modality",
      "page": 7
    },
    {
      "caption": "Figure 12: Test loss context of auditory modality",
      "page": 7
    },
    {
      "caption": "Figure 13: Test accuracy of emotion of auditory modality",
      "page": 7
    },
    {
      "caption": "Figure 1: 4, visual-",
      "page": 8
    },
    {
      "caption": "Figure 14: Sample of test composition",
      "page": 8
    },
    {
      "caption": "Figure 16: , we represent cases where V-",
      "page": 8
    },
    {
      "caption": "Figure 15: A-SCAM correct, but V-SCAM wrong",
      "page": 9
    },
    {
      "caption": "Figure 16: V-SCAM wrong, but A-SCAM correct",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table 1: IEMOCAP: Four Emotions",
      "page": 3
    },
    {
      "caption": "Table 1: Emotions are anno-",
      "page": 3
    },
    {
      "caption": "Table 2: This approach offers two advantages. Firstly, it",
      "page": 4
    },
    {
      "caption": "Table 2: Relabel of composition",
      "page": 4
    },
    {
      "caption": "Table 3: In the auditory modality, SCAM achieves an accuracy",
      "page": 6
    },
    {
      "caption": "Table 4: In our experiments, a difference in the trends of accu-",
      "page": 7
    },
    {
      "caption": "Table 3: Performance comparison of prior work and SCAM",
      "page": 8
    },
    {
      "caption": "Table 4: Performance at lowest test loss",
      "page": 8
    },
    {
      "caption": "Table 5: Ablation experiments of SCAM",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A Survey of Robots in Healthcare. Technologies",
      "authors": [
        "Kyrarini"
      ],
      "year": "2021",
      "venue": "A Survey of Robots in Healthcare. Technologies"
    },
    {
      "citation_id": "2",
      "title": "Multimodal integration of emotional signals from voice, body, and context: Effects of (in) congruence on emotion recognition and attitudes towards robots",
      "authors": [
        "Tsiourti"
      ],
      "year": "2019",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Poria"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "The perception of changing emotion expressions",
      "authors": [
        "Sacharin"
      ],
      "year": "2012",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "5",
      "title": "Emotional chatting machine: Emotional conversation generation with internal and external memory",
      "authors": [
        "Zhou"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Automated emotion recognition: Current trends and future perspectives",
      "authors": [
        "Maithri"
      ],
      "year": "2022",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "7",
      "title": "Investigating EEG-based functional connectivity patterns for multimodal emotion recognition",
      "authors": [
        "Wu"
      ],
      "year": "2022",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "8",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell ; Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "9",
      "title": "Multi-View Learning for Speech Emotion Recognition with Categorical Emotion, Categorical Sentiment, and Dimensional Scores",
      "authors": [
        "Atmaja"
      ],
      "year": "2008",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Efficient speech emotion recognition using multi-scale CNN and attention",
      "authors": [
        "Peng"
      ],
      "year": "2020",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "arxiv": "arXiv:2009.04107"
    },
    {
      "citation_id": "11",
      "title": "AV-ITN: A Method of Multimodal Video Emotional Content Analysis",
      "authors": [
        "Fu"
      ],
      "year": "2022",
      "venue": "2022 IEEE Conference on Telecommunications"
    }
  ]
}