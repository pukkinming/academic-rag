{
  "paper_id": "2011.10652v1",
  "title": "Self-Supervised Learning With Cross-Modal Transformers For Emotion Recognition",
  "published": "2020-11-20T21:38:34Z",
  "authors": [
    "Aparna Khare",
    "Srinivas Parthasarathy",
    "Shiva Sundaram"
  ],
  "keywords": [
    "self-supervised",
    "multi-modal",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a challenging task due to limited availability of in-the-wild labeled datasets. Self-supervised learning has shown improvements on tasks with limited labeled datasets in domains like speech and natural language. Models such as BERT learn to incorporate context in word embeddings, which translates to improved performance in downstream tasks like question answering. In this work, we extend self-supervised training to multi-modal applications. We learn multi-modal representations using a transformer trained on the masked language modeling task with audio, visual and text features. This model is fine-tuned on the downstream task of emotion recognition. Our results on the CMU-MOSEI dataset show that this pre-training technique can improve the emotion recognition performance by up to 3% compared to the baseline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human communication is inherently multi-modal in nature. Our expressions and tone of voice augment verbal communication. This can include vocal features like speaking rate, intonation and visual features like facial expressions  [1] . Non-verbal communication is important for tasks that involve higher level cognitive expressions like emotions  [2] , persuasiveness  [3]  and mental health analysis  [4] . We focus on a multi-modal approach to emotion recognition because humans fundamentally express emotions verbally using spoken words  [5] , as well as with acoustic signals  [6]  and visual expressions  [7] .\n\nGetting large-scale labeled datasets for emotion recognition can be challenging. Our primary motivation for this paper is to study effective utilization of large unlabeled datasets to improve performance of multi-modal emotion recognition systems. The signals we consider are speech, visual information and spoken text. Our motivation stems from the popular use of pre-trained models in natural language, speech and visual understanding tasks to circumvent data limitations.\n\nBERT is a popular model for natural language understanding  [9]  that was trained using self-supervision. Devlin et al. use the masked language modeling (LM) task on the Wikipedia corpus for pre-training. The model was successfully finetuned to improve performance on several tasks like question answering and the general language understanding evaluation benchmarks  [9] . Self-supervised learning has also been successfully applied to speech based applications. Schneider et al. in  [10]  use unsupervised pre-training on speech data by distinguishing an audio sample in the future from noise samples. Fine-tuning this model shows state of the art results on automatic speech recognition (ASR).  Liu et al.  show in  [11]  that a BERT-like pre-training approach can be applied to speech. By predicting masked frames instead of masked words, the performance on tasks like speaker recognition, sentiment recognition and phoneme classification can be improved. For emotion recognition,  Tseng et al.  show in  [12]  that text-based self-supervised training can outperform state of the art models. The authors use a language modeling task, that involves predicting a word given its context, to pre-train the model. Another area of work that has leveraged unlabeled data is detection and localization of visual objects and spoken words in multi-modal input. Harwath et al. in  [13, 14]  train an audio-visual model on an image-audio retrieval task. The models are trained to learn a joint audio-visual representation in a shared embedding space. This model can learn to recognize word categories by sounds without explicit labels. Motivated by the success of these approaches, we study if similar methods can be applied to multi-modal emotion recognition. To the best of our knowledge, a joint self-supervised training approach using text, audio and visual inputs has not been well explored for emotion recognition.\n\nMulti-modal emotion recognition models have been well studied in literature and typically outperform uni-modal systems  [8] . These models need to combine inputs with varying sequence lengths. In video, the sequence lengths for audio and visual frames differ from the length of text tokens by orders of magnitude. There has been considerable prior work in fusing multi-modal features. Liang et al. in  [8]  studied multiple fusion techniques for multi-modal emotion recognition and sentiment analysis. Their methods included early and late fusion of modalities, and a dynamic fusion graph based network. They showed that the graph fusion model outperforms other methods. Early fusion and graph fusion techniques both require alignment between various modalities. Late fusion can be performed without alignment, but does not allow interaction of features from different modalities at the frame level. To overcome this limitation, Tsai et al. introduce the cross-modal transformer in  [15] . It scales the features using cross-modal attention. In the process, the modalities are projected into sequences of equal lengths, eliminating the need for any alignment. This architecture has been successfully applied to problems like emotion recognition, sentiment analysis  [15, 16]  and speech recognition  [17] . Recently, another transformerbased method to combine multi-modal inputs was introduced by Rahman et al. in  [18] , which uses a multi-modal adaptation gate.\n\nIn this paper, we propose using the same pre-training scheme as BERT, but extend it to a model that uses audio, visual and text inputs. We discuss the relevance of this approach in Section 2.2. The multi-modal representations learned in pre-training are fine-tuned for emotion recognition. We evaluate the efficacy of the pre-training approach. We also perform experiments to understand the importance of each modality on the CMU-MOSEI dataset and provide case-studies to interpret the results.\n\nThis paper is organized as follows. In Section 2 we describe our model architecture and the self-supervised approach for pre-training, along with further motivation for the self-supervised learning we choose. In Section 3, we discuss the training setup and data. We present our results and analysis in Section 4 and conclude in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Self-Supervised Training With",
      "text": "CROSS-MODAL TRANSFORMERS",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Architecture",
      "text": "Not all information in a given sequence is equally important for emotion recognition. If we consider visual inputs, emotionally relevant cues may appear only in certain frames. Similarly, each spoken word in the sentence does not contribute equally to the expressed emotion. Given this nature of the sequence recognition problem, transformer-based models are a good choice for extracting a fixed length representation for emotion recognition. We use the cross-modal transformer for emotion recognition since it showed state of the art results on sentiment analysis  [15] . We chose a modified version of the proposed model and will describe it in this section. The architecture allows each sample from each modality to interact with each sample from each other modality, providing the benefits of low-level fusion. It also projects all the sequences into equal lengths which allows for frame level late fusion after the transformation.\n\nOur overall architecture is shown in Figure  1 . The transformer model trained for emotion recognition allows for attending to specific input features (visual frames, words, speech segments) that are relevant to the task  [15] . The first part of our model architecture achieves this by using selfattention based transformer encoder for individual modalities. We add positional embeddings to the input features as discussed in  [19] . Intuitively, positional embeddings would be useful in the task because for extracting the context from the input, the order of the words matter. We did not study the importance of positional embeddings since our work is focused on self-supervised learning. These features are processed by the transformer encoder. The architecture of the encoder layers is identical to  [19]  and is shown in Figure  1 . The transformer encoder consists of N layers. The first operation in each layer transforms the input into keys, queries and values. If the input to a given layer for modality M is represented by F M , then the query Q M , the key K M and the value V M for the corresponding modality is computed as\n\nwhere W αβ represents a linear projection. After obtaining the keys, queries and values, the self-attention layer scales the value V M . The output of the attention layer, represented by A M , is computed as\n\nwhere d denotes the dimensionality of the keys. In practice, we use the multi-head version of the scaled dot-product attention that uses k scaled dot-product attention heads. The final output of the transformer encoder layer, S M , is computed as following\n\nwhere O M is the normalized output after adding a residual connection from V M to the output of the scaled dot-product attention layer. We use N encoder layers to obtain the selfattended outputs S A , S V and S T from the audio, visual and text modalities respectively. Next, we combine the uni-modal transformer encoder outputs, S A , S V and S T , to learn the final multi-modal representation for emotion recognition. This is done by the cross-modal transformer, which computes the attention map between features from two different modalities M 1 and M 2 . The crossmodal attention allows for increasing attention weights on features that are deemed important for emotion recognition by more than one modality. This property was shown in  [15] .\n\nThe output E M1->M2 from the cross-modal transformer is computed as:\n\nwhere CM denotes the transformations applied by the crossmodal transformer, shown in Figure  1 . The cross-modal transformer is identical to the transformer encoder, with one exception. The key and value matrices (K E M 1 and V E M 1 ) are obtained from the encoded output S M1 . The query Q E M 2 , which provides contextualization, is obtained from the encoded output of modality S M2 . The keys, queries and values are obtained from the outputs of the corresponding uni-modal transformers by using fully connected layers, similar to the unimodal transformer encoders described above. The final encoder output E AVT is the weighted sum of the encoded output of the text modality and the cross-modal transformer output attending to the audio and visual inputs using the context query from the text domain.\n\nFor our experiments, we used a fixed weight of 0.33 for w 1 , w 2 and w 3 . The classification consists of average pooling followed by a linear layer that maps the representation into emotion classes.\n\nOur model differs from the architecture in  [15]  in two ways. Instead of using convolutional layers to increase temporal awareness, we use uni-modal encoders to encode the various modalities. We used a second modification by limiting the cross-modal transformer layers to only compute attention between audio and text, and visual input and text as described above. We call text the anchor modality in this architecture. For this study, we chose text as the anchor modality for ease of the self-supervised task. This allows us to train the self-supervised model without a decoder. For comparison, we tried training our baseline models with both audio and video as anchors and the results were similar. Note that our focus in this work is to study the impact of self-supervised training, and not to tune the model architecture for emotion recognition. Hence, we chose the simplified model architecture for all the experiments. Our work does not compare our model to the one proposed in  [15]  and we will do so in our future work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self-Supervised Training",
      "text": "Our primary motivation for this study is to understand how to leverage large unlabeled multi-modal datasets to learn representations which can be fine-tuned for emotion recognition.\n\nWe consider what self-supervised task would be relevant given our downstream task of interest. We note that spoken words are one of the strongest expressions of human emotion. This has been studied in psychological literature  [5] , and also in the emotion recognition literature. Embeddings like ELMo, that encode language context, can be applied successfully to the emotion recognition task  [12] . Such representations from text have been learned successfully with self-supervised tasks like skip-gram, continuous bag-of-words model  [20]  and more recently using the masked LM task  [9] . We extend this work by learning representations that encode context using the text input, as well as the audio and visual inputs.\n\nWe choose the masked LM task to train the model, similar to the BERT model  [9] . We propose to predict words by looking at audio and visual context in addition to the context words around the masked input. Intuitively, the auxiliary information present in visual expressions and audio features like intonation would provide relevant input for predicting the masked words. For example, consider the phrase \"This movie is [MASK]\" as an input to the model. The [MASK] word could be predicted as \"amazing\" if the audio and visual features show that speaker is happy. Alternatively, the prediction could be \"terrible\" if the speaker seems discontent while talking. This information cannot be derived from text only input. We posit that the latent representations learned using the masked LM task with multi-modal input will not just encode context, but also the relevant emotional information which could be used to predict those words.\n\nFor training, we mask 15% of the input words and the audio and visual features corresponding to those words. The word boundaries are obtained using an existing ASR system. More details on the ASR system used will be discussed in Section 3.1. The model predicts the words at each sequence, and the loss is computed only for the words that were masked in the input sequence. Instead of providing a mask token as input for masked words, we choose to set the masked input for all modalities to zero. We are able to do so as we use GLoVe embeddings to represent the text input instead of learning an embedding layer. Similarly, to mask the audio and visual inputs for the corresponding masked words, we replace the input features with zeros. For this task, we replace the average pooling and linear classifier described in Section 2.1 with a linear layer of output size equal to the model vocabulary. Since the encoder layer uses bi-directional attention, we do not need a decoder to attend to past predictions from the model. In addition, since the encoder output length is equal to the sequence length of the input text, we do not require a transformer decoder layer. This allows training to be simplified and was one of the reasons we chose the model architecture.\n\nFor the loss function, we use a full softmax loss as well as noise contrastive estimation (NCE)  [21]  to train our models. NCE has been used successfully to learn the inverse language modeling task, that involves predicting the context words given a word. Minh et al. show in  [22]  that NCE can reduce computation by estimating the normalization factor for computing softmax using noise samples. For a task which has a similar vocabulary size as our dataset, they demonstrated a reduction of up to 50% in training time. We compare the models trained with NCE loss with the full softmax loss. This would inform if the multi-modal transformer can be trained with similar accuracy but more efficiently. Our implementation is exactly the same as  [22] , except we use a normalization factor of vocabulary size which we found to be critical for training our model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset And Training Details",
      "text": "Our setup involves first training the cross-modal transformer on the masked LM task on a large dataset, followed by finetuning for emotion recognition. For pre-training, we utilize the publicly available VoxCeleb2 dataset  [23] . We chose this dataset since it provides all the modalities we are interested in and is sufficiently large (1.1 million videos in the train partition). More importantly, this data is emotion rich, as shown in  [24] . This dataset does not provide text transcriptions. We used a TDNN ASR model trained with the standard Kaldi recipe on the Librispeech dataset to get transcriptions  [25] . We use 40-dimensional Log-Filter bank energy (LFBE) features using a 10ms frame rate to represent the audio input. Visual frames are represented by 4096-dimensional features extracted from the VGG-16 model. 300-dimensional GloVe embeddings represent the text input. We chose to use GLoVe embeddings for this task instead of an embedding layer because our dataset has a limited number of sentences and vocabulary. The vocabulary size for this dataset as obtained from ASR transcriptions is 88000. Using GloVe embeddings allows the model to take advantage of pre-trained embeddings trained on billions of words  [26] . The disadvantage is the inability to handle out of vocabulary words, which we ignore for all our experiments.\n\nThe model is pre-trained using pytorch with the learning schedule described in  [27] . We stack 5-frames of the LFBE features for a final audio feature dimensionality of 200. This was done to reduce the memory requirements for training the model. We select only the English language videos from VoxCeleb2 for training. The filtering is done by selecting a heuristic threshold on the likelihood scores from the ASR decoder. For all our experiments, we use only the dev portion of the VoxCeleb2 dataset. Our final training dataset consists of 978k utterances from 4820 speakers. We use the architecture described in Section 2.1 to train the model. The model has keys, values and queries of dimension 512, 4 encoder layers, feed forward layer of dimension 200 and 4 attention heads for both the uni-modal and cross-modal encoders. We trained the model for 20 epochs and chose the final model with the lowest loss on a held-out set.\n\nFor evaluating performance on the emotion recognition task, we fine-tune the model on the CMU-MOSEI dataset  [8] . It is the largest publicly available multi-modal dataset for emotion recognition with natural conversations. It contains  23  We convert the labels into binary targets. A clip is assigned a 0 label if the score on the Likert scale is 0, and 1 otherwise. A greater than 0 score on the Likert scale represents the presence of the specific emotion, and 0 the absence of the emotion. This was reflected in the binary interpretation that we chose. For fine-tuning the model, we remove the decoder layer for the masked LM task and add the average pooling and decoder layer for emotion recognition. Each example in this dataset can be labeled with the presence of multiple emotions. Therefore, we use a sigmoid output for each of the 6 nodes in the output layer to get the probability for each emotion. The positive and negative examples for various emotions in the dataset are imbalanced. During training, we weigh the loss for the each training sample appropriately to ensure that the positive and negative examples across all emotions contribute equally to the loss.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "We use the weighted accuracy (WA) and F1-score for each emotion as the metrics for the task. We also report average of these two metrics over the 6 emotions, keeping in line with prior work  [8] . For evaluating the baseline model, we follow the procedure in  [12] . The model is randomly initialized and trained 10 different times. The best model is chosen based on the average of the weighted accuracy and F1-scores over all the emotions on the dev set over the 10 runs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results On Emotion Recognition",
      "text": "Table  1  shows the results of our experiments and state of the art results on the same dataset from other publications. The transformer baseline outperforms or is comparable to published results for most of the metrics. Our model shows a 2.4% absolute improvement in the weighted accuracy of the anger emotion and a 2.6% absolute improvement in the F1score of the surprise emotion. We observe a degradation in the weighted accuracy of the fear emotion. This comparison with other state of the art models is pertinent for the rest of our work as we want to build upon a strong baseline model.\n\nThe next set of results in Table  1  are with the pre-trained model on the VoxCeleb2 dataset, fine-tuned for emotion recognition. Our results show up to 3% absolute improvement in the weighted accuracy of 4 out of the 6 emotions, with a slight degradation in the weighted accuracy of the anger emotion. The average weighted accuracy over all emotions improves by 0.8%. The weighted accuracy of the surprise and fear emotions improve by 2.2% and 3% absolute respectively. The 95% confidence intervals of these emotions don't overlap with the baseline, demonstrating the statistical significance of the results. The F1-score is comparable to the baseline for all emotions other than happy, where we see a 1% absolute improvement. The model trained using the NCE optimization has similar improvements. It shows that we can achieve the same improvements with a lower computational cost of training. These results validate our hypothesis that we can effectively leverage a large unlabeled multi-modal dataset to improve results on emotion recognition using self-supervised pre-training.\n\nIn order to understand the impact of ASR errors on the model, we generated transcriptions on the CMU-MOSEI dataset using a commercial ASR system. The word error rate of the machine-generated transcriptions was 29%. We then re-evaluated the performance of our baseline model with ASR based transcriptions instead of the transcriptions provided as part of the dataset. We did not observe a degradation in emotion recognition performance. Note that for this experiment, the baseline model was trained with the original transcriptions. ASR errors have been studied well in literature and it has been shown that the top contributors to errors are shorter words like 'on', 'was', 'in' etc.  [28] . These words do not contribute to emotion expression, which would explain the observations we made.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Analysis And Case Studies",
      "text": "We analyze the results to understand the contribution of each modality towards accuracy. We look at predictions from the baseline model with missing inputs from select modalities. Note that we cannot ablate the text input. The output of the cross-modal transformers will be 0 if the text input is 0 since the attention maps will be all zeros. For subjective analysis with missing text input, we trained a baseline model with audio as the anchor modality. As noted in Section 2.1, the choice of anchor modality does not change the performance of the baseline model. We describe our subjective analysis below.\n\nThe first example we observe is ID \"HeZS2-Prhc  [8] \" in the dataset. From visual inspection, the video shows that the speaker is laughing, which conveys a happy emotion. However, the speaker is talking about the cost for drugs and its impact on communities. This is why the visual modality is the key to accurately predicting the emotion, and the model is not able to classify the emotion as happy with text input alone. On the contrary, the second example, ID \"10219  [11]  shows the speaker with a neutral face and a neutral tone of voice. The speaker is talking about a positive movie review, which leads to the text classifying the emotion as happy. The model was not able to classify the emotion in this example as happy without the text input. In the third example, ID \"-9y-fZ3swSY  [1] \", the speaker is talking about a neutral topic with a slightly positive face, but in a very positive tone of voice. The model predicts that the speaker in this video is happy only when the audio features are present. This subjective analysis shows the importance of multi-modal features in human communication, and how each of them contribute to emotion recognition.\n\nNext, we show the overall results with each missing modality in Table  2 . Adding audio and visual input along with text improves both metrics by 2% absolute. The results show that with text alone, we can recover most of the baseline performance. The subjective examples, however, suggest that for several cases, other modalities are required for accurate prediction. Therefore, the importance of text should not be generalized for the problem of emotion recognition in-thewild. However, for the CMU-MOSEI dataset, text is the most important modality for emotion recognition. To analyze this, we look at the distribution of topics in the dataset. The 5 most frequent topics are: reviews (16.2%), debate (2%), consulting (1.8%), financial (1.8%) and speech (1.6%). For these topics, the perceived emotion by a human annotator is strongly based on what is being said. This would explain why text is the most important input. We posit that for more diverse topics, specifically involving human to human communication, the other modalities would start to gain importance for recognizing the emotions accurately.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we present state of the art results on the emotion recognition task using the cross-modal transformer on the CMU-MOSEI dataset. We utilize a BERT-like pre-training scheme using audio, visual and text inputs. We use the Vox-Celeb2 dataset to pre-train the model and fine-tune it for the emotion recognition task. We demonstrate up to a 3% improvement over the baseline with the fine-tuned model. We presented our subjective analysis on the contribution of various modalities to emotion recognition. We also show results with missing input modalities to understand the importance of each modality for the emotion recognition task.\n\nFor our future work, we propose to initialize the text encoder with a text-only model like BERT, before multi-modal self-supervised training. VoxCeleb2 dataset, although large in terms of number of hours of video, is smaller when compared to the Wikipedia corpus which has billions of words. Taking advantage of a larger text-only corpus could provide improvements. We would also like to experiment with adapting the model on the CMU-MOSEI dataset. Both the VoxCeleb2 and CMU-MOSEI datasets are obtained from YouTube, but there could be domain mismatch between the two datasets. Adapting could help bridge the mismatch. We would also like to explore weak labels to adapt the pre-trained representations for the downstream task. Tseng et al. showed in  [29]  that weakly supervised labels can be used to effectively bias the embeddings learned by a pre-trained model. Even though we study the impact of ASR errors on emotion recognition, we do not know how these errors impact the self-supervised training. We would like to study that in the future. As noted before, our model architecture doesn't allow ablation of text. For our future work, we will focus on overcoming that limitation.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The trans-",
      "page": 2
    },
    {
      "caption": "Figure 1: The transformer encoder consists of N layers. The ﬁrst op-",
      "page": 2
    },
    {
      "caption": "Figure 1: Cross-modal transformer based self-supervised learning architecture. The ﬁgure shows the self-attention based trans-",
      "page": 3
    },
    {
      "caption": "Figure 1: The cross-modal trans-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amazon.com, Sunnyvale, CA": "BERT is a popular model for natural language understanding"
        },
        {
          "Amazon.com, Sunnyvale, CA": "[9] that was trained using self-supervision. Devlin et al. use"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "the masked language modeling (LM)\ntask on the Wikipedia"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "corpus\nfor pre-training. The model was\nsuccessfully ﬁne-"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "tuned to improve performance on several\ntasks like question"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "answering and the general\nlanguage understanding evalua-"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "tion benchmarks [9]. Self-supervised learning has also been"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "successfully applied to speech based applications. Schneider"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "et al.\nin [10] use unsupervised pre-training on speech data"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "by distinguishing an audio sample in the future from noise"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "samples. Fine-tuning this model shows state of the art results"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "on automatic speech recognition (ASR). Liu et al. show in"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "[11]\nthat a BERT-like pre-training approach can be applied"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "to speech. By predicting masked frames instead of masked"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "words,\nthe performance on tasks\nlike\nspeaker\nrecognition,"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "sentiment recognition and phoneme classiﬁcation can be im-"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "proved.\nFor emotion recognition, Tseng et al. show in [12]"
        },
        {
          "Amazon.com, Sunnyvale, CA": "that\ntext-based self-supervised training can outperform state"
        },
        {
          "Amazon.com, Sunnyvale, CA": "of the art models. The authors use a language modeling task,"
        },
        {
          "Amazon.com, Sunnyvale, CA": "that\ninvolves predicting a word given its context,\nto pre-train"
        },
        {
          "Amazon.com, Sunnyvale, CA": "the model. Another area of work that has leveraged unlabeled"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "data is detection and localization of visual objects and spoken"
        },
        {
          "Amazon.com, Sunnyvale, CA": "words in multi-modal\ninput. Harwath et al.\nin [13, 14]\ntrain"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "an audio-visual model on an image-audio retrieval\ntask. The"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "models are trained to learn a joint audio-visual representation"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "in a shared embedding space. This model can learn to recog-"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "nize word categories by sounds without explicit labels. Moti-"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "vated by the success of these approaches, we study if similar"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "methods can be applied to multi-modal emotion recognition."
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "To the best of our knowledge, a joint self-supervised training"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "approach using text, audio and visual inputs has not been well"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "explored for emotion recognition."
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "Multi-modal emotion recognition models have been well"
        },
        {
          "Amazon.com, Sunnyvale, CA": "studied in literature and typically outperform uni-modal sys-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "tems [8]. These models need to combine inputs with varying"
        },
        {
          "Amazon.com, Sunnyvale, CA": "sequence lengths. In video, the sequence lengths for audio and"
        },
        {
          "Amazon.com, Sunnyvale, CA": "visual frames differ from the length of text tokens by orders of"
        },
        {
          "Amazon.com, Sunnyvale, CA": "magnitude. There has been considerable prior work in fusing"
        },
        {
          "Amazon.com, Sunnyvale, CA": "multi-modal features. Liang et al.\nin [8] studied multiple fu-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "sion techniques for multi-modal emotion recognition and sen-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "timent analysis. Their methods included early and late fusion"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of modalities,\nand a dynamic fusion graph based network.": "They showed that\nthe graph fusion model outperforms other",
          "which allows for frame level late fusion after the transforma-": "tion."
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "methods. Early fusion and graph fusion techniques both re-",
          "which allows for frame level late fusion after the transforma-": "Our overall architecture is shown in Figure 1. The trans-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "quire alignment between various modalities. Late fusion can",
          "which allows for frame level late fusion after the transforma-": "former model\ntrained\nfor\nemotion\nrecognition\nallows\nfor"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "be performed without alignment, but does not allow interac-",
          "which allows for frame level late fusion after the transforma-": "attending to speciﬁc\ninput\nfeatures\n(visual\nframes, words,"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "tion of features from different modalities at the frame level. To",
          "which allows for frame level late fusion after the transforma-": "speech segments) that are relevant\nto the task [15]. The ﬁrst"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "overcome this limitation, Tsai et al. introduce the cross-modal",
          "which allows for frame level late fusion after the transforma-": "part of our model architecture achieves\nthis by using self-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "transformer\nin [15].\nIt scales the features using cross-modal",
          "which allows for frame level late fusion after the transforma-": "attention based transformer encoder\nfor\nindividual modali-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "attention. In the process, the modalities are projected into se-",
          "which allows for frame level late fusion after the transforma-": "ties. We add positional embeddings to the input\nfeatures as"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "quences of equal\nlengths, eliminating the need for any align-",
          "which allows for frame level late fusion after the transforma-": "discussed in [19].\nIntuitively, positional embeddings would"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "ment. This architecture has been successfully applied to prob-",
          "which allows for frame level late fusion after the transforma-": "be useful\nin the task because for extracting the context from"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "lems\nlike emotion recognition,\nsentiment analysis\n[15, 16]",
          "which allows for frame level late fusion after the transforma-": "the input,\nthe order of\nthe words matter. We did not\nstudy"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "and speech recognition [17]. Recently, another\ntransformer-",
          "which allows for frame level late fusion after the transforma-": "the importance of positional embeddings since our work is"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "based method to combine multi-modal inputs was introduced",
          "which allows for frame level late fusion after the transforma-": "focused on self-supervised learning. These features are pro-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "by Rahman et al.\nin [18], which uses a multi-modal adapta-",
          "which allows for frame level late fusion after the transforma-": "cessed by the transformer encoder. The architecture of\nthe"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "tion gate.",
          "which allows for frame level late fusion after the transforma-": "encoder layers is identical\nto [19] and is shown in Figure 1."
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "In this paper, we propose using the\nsame pre-training",
          "which allows for frame level late fusion after the transforma-": "The transformer encoder consists of N layers. The ﬁrst op-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "scheme\nas BERT, but\nextend it\nto a model\nthat uses\nau-",
          "which allows for frame level late fusion after the transforma-": "eration in each layer\ntransforms the input\ninto keys, queries"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "dio, visual and text\ninputs. We discuss the relevance of this",
          "which allows for frame level late fusion after the transforma-": "and values.\nIf\nthe input\nto a given layer\nfor modality M is"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "approach in Section 2.2. The multi-modal\nrepresentations",
          "which allows for frame level late fusion after the transforma-": "represented by FM,\nthen the query QM,\nthe key KM and the"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "learned in pre-training are ﬁne-tuned for emotion recogni-",
          "which allows for frame level late fusion after the transforma-": "value VM for the corresponding modality is computed as"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "tion. We evaluate the efﬁcacy of\nthe pre-training approach.",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "QM = WqM(FM)"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "We also perform experiments to understand the importance",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "(1)\nKM = WkM(FM)"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "of each modality on the CMU-MOSEI dataset and provide",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "VM = WvM(FM)"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "case-studies to interpret the results.",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "This paper\nis organized as follows.\nIn Section 2 we de-",
          "which allows for frame level late fusion after the transforma-": "where Wαβ represents a linear projection. After obtaining the"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "scribe\nour model\narchitecture\nand\nthe\nself-supervised\nap-",
          "which allows for frame level late fusion after the transforma-": "keys, queries and values,\nthe self-attention layer\nscales\nthe"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "proach for pre-training, along with further motivation for the",
          "which allows for frame level late fusion after the transforma-": "value VM. The output of\nthe attention layer,\nrepresented by"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "self-supervised learning we\nchoose.\nIn Section 3, we dis-",
          "which allows for frame level late fusion after the transforma-": "AM, is computed as"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "cuss the training setup and data. We present our\nresults and",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "QMK T"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "analysis in Section 4 and conclude in Section 5.",
          "which allows for frame level late fusion after the transforma-": "M√\n)VM\nAM = softmax(\n(2)"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "d"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "where d denotes the dimensionality of the keys. In practice,"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "2.\nSELF-SUPERVISED TRAINING WITH",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "we use the multi-head version of the scaled dot-product atten-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "CROSS-MODAL TRANSFORMERS",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "tion that uses k scaled dot-product attention heads. The ﬁnal"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "output of the transformer encoder layer, SM,\nis computed as"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "2.1. Model architecture",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "following"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "Not all\ninformation in a given sequence is equally important",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "for emotion recognition.\nIf we consider visual\ninputs, emo-",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "OM = LayerN orm(VM + AM)"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "tionally relevant cues may appear only in certain frames. Sim-",
          "which allows for frame level late fusion after the transforma-": "(3)"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "",
          "which allows for frame level late fusion after the transforma-": "SM = LayerN orm(OM + F eedF orward(OM))"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "ilarly, each spoken word in the sentence does not contribute",
          "which allows for frame level late fusion after the transforma-": ""
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "equally to the expressed emotion. Given this nature of\nthe",
          "which allows for frame level late fusion after the transforma-": "where OM is the normalized output after adding a residual"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "sequence recognition problem, transformer-based models are",
          "which allows for frame level late fusion after the transforma-": "connection from VM to the output of\nthe scaled dot-product"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "a good choice for extracting a ﬁxed length representation for",
          "which allows for frame level late fusion after the transforma-": "attention layer. We use N encoder layers to obtain the self-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "emotion recognition.",
          "which allows for frame level late fusion after the transforma-": "attended outputs SA, SV and ST from the audio, visual and"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "We use the cross-modal transformer for emotion recogni-",
          "which allows for frame level late fusion after the transforma-": "text modalities respectively."
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "tion since it showed state of the art results on sentiment anal-",
          "which allows for frame level late fusion after the transforma-": "Next, we combine the uni-modal transformer encoder out-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "ysis [15]. We chose a modiﬁed version of the proposed model",
          "which allows for frame level late fusion after the transforma-": "puts, SA, SV and ST, to learn the ﬁnal multi-modal representa-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "and will describe it\nin this section. The architecture allows",
          "which allows for frame level late fusion after the transforma-": "tion for emotion recognition. This is done by the cross-modal"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "each sample from each modality to interact with each sample",
          "which allows for frame level late fusion after the transforma-": "transformer, which computes the attention map between fea-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "from each other modality, providing the beneﬁts of low-level",
          "which allows for frame level late fusion after the transforma-": "tures from two different modalities M1 and M2. The cross-"
        },
        {
          "of modalities,\nand a dynamic fusion graph based network.": "fusion.\nIt also projects all\nthe sequences into equal\nlengths",
          "which allows for frame level late fusion after the transforma-": "modal attention allows\nfor\nincreasing attention weights on"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "(Anchor"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "modality)"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "Fig. 1. Cross-modal\ntransformer based self-supervised learning architecture. The ﬁgure shows the self-attention based trans-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "former encoder layers, the cross-modal attention encoder module and the overall architecture of our self-supervised model."
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "features that are deemed important\nfor emotion recognition\nOur model differs\nfrom the architecture in [15]\nin two"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "by more than one modality. This property was shown in [15].\nways.\nInstead of using convolutional\nlayers to increase tem-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "poral awareness, we use uni-modal encoders to encode the"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "from the cross-modal\ntransformer\nThe output EM1−>M2"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "various modalities. We used a second modiﬁcation by limit-\nis computed as:"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "ing the cross-modal transformer layers to only compute atten-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "tion between audio and text, and visual\ninput and text as de-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": ")"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "(4)\nEM1−>M2 = CM (QEM2\n, KEM1\n, VEM1"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "scribed above. We call text the anchor modality in this archi-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "tecture. For this study, we chose text as the anchor modality"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "where CM denotes the transformations applied by the cross-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "for ease of the self-supervised task. This allows us to train the"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "modal transformer, shown in Figure 1. The cross-modal trans-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "self-supervised model without a decoder. For comparison, we"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "former\nis identical\nto the transformer encoder, with one ex-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "tried training our baseline models with both audio and video"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": ") are ob-\nception. The key and value matrices (KEM1\nand VEM1"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "as anchors and the results were similar. Note that our focus"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": ", which\ntained from the encoded output SM1. The query QEM2"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "in this work is to study the impact of self-supervised training,"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "provides contextualization, is obtained from the encoded out-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "and not\nto tune the model architecture for emotion recogni-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "put of modality SM2. The keys, queries and values are ob-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "tion. Hence, we chose the simpliﬁed model architecture for"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "tained from the outputs of the corresponding uni-modal trans-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "all the experiments. Our work does not compare our model to"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "formers by using fully connected layers, similar\nto the uni-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "the one proposed in [15] and we will do so in our future work."
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "modal\ntransformer encoders described above. The ﬁnal en-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "coder output EAVT is the weighted sum of the encoded out-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "put of the text modality and the cross-modal transformer out-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "2.2.\nSelf-supervised training"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "put attending to the audio and visual inputs using the context"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "Our primary motivation for this study is to understand how to\nquery from the text domain."
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "leverage large unlabeled multi-modal datasets to learn repre-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "sentations which can be ﬁne-tuned for emotion recognition.\n(5)\nEAVT = w1 · ET + w·EA−>T + w3 · EV−>T"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "We\nconsider what\nself-supervised\ntask would\nbe\nrelevant"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "given our downstream task of\ninterest. We note that spoken\nFor our experiments, we used a ﬁxed weight of 0.33 for w1,"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "words are one of the strongest expressions of human emotion.\nw2 and w3. The classiﬁcation consists of average pooling fol-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "This has been studied in psychological literature [5], and also\nlowed by a linear layer that maps the representation into emo-"
        },
        {
          "Video input\nText input\nAudio input\nInput  SM2\nInput Features FM\nInput  SM1": "in the emotion recognition literature. Embeddings like ELMo,\ntion classes."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "that encode language context, can be applied successfully to": "the emotion recognition task [12]. Such representations from",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "a reduction of up to 50% in training time. We compare the"
        },
        {
          "that encode language context, can be applied successfully to": "text have been learned successfully with self-supervised tasks",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "models\ntrained with NCE loss with the\nfull\nsoftmax loss."
        },
        {
          "that encode language context, can be applied successfully to": "like\nskip-gram,\ncontinuous\nbag-of-words model\n[20]\nand",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "This would inform if\nthe multi-modal\ntransformer\ncan be"
        },
        {
          "that encode language context, can be applied successfully to": "more recently using the masked LM task [9]. We extend this",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "trained with similar accuracy but more efﬁciently. Our\nim-"
        },
        {
          "that encode language context, can be applied successfully to": "work by learning representations that encode context using",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "plementation is exactly the same as\n[22],\nexcept we use a"
        },
        {
          "that encode language context, can be applied successfully to": "the text input, as well as the audio and visual inputs.",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "normalization factor of vocabulary size which we found to be"
        },
        {
          "that encode language context, can be applied successfully to": "We choose the masked LM task to train the model, simi-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "critical for training our model."
        },
        {
          "that encode language context, can be applied successfully to": "lar to the BERT model [9]. We propose to predict words by",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "looking at audio and visual context\nin addition to the con-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "3. EXPERIMENTAL SETUP"
        },
        {
          "that encode language context, can be applied successfully to": "text words around the masked input. Intuitively, the auxiliary",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "information present\nin visual expressions and audio features",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "3.1. Dataset and training details"
        },
        {
          "that encode language context, can be applied successfully to": "like intonation would provide relevant input for predicting the",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "masked words. For example, consider the phrase “This movie",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "Our setup involves ﬁrst\ntraining the cross-modal\ntransformer"
        },
        {
          "that encode language context, can be applied successfully to": "is [MASK]” as an input\nto the model.\nThe [MASK] word",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "on the masked LM task on a large dataset, followed by ﬁne-"
        },
        {
          "that encode language context, can be applied successfully to": "could be predicted as “amazing” if the audio and visual fea-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "tuning for emotion recognition.\nFor pre-training, we utilize"
        },
        {
          "that encode language context, can be applied successfully to": "tures show that speaker is happy. Alternatively, the prediction",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "the publicly available VoxCeleb2 dataset [23]. We chose this"
        },
        {
          "that encode language context, can be applied successfully to": "could be “terrible” if the speaker seems discontent while talk-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "dataset since it provides all\nthe modalities we are interested"
        },
        {
          "that encode language context, can be applied successfully to": "ing.\nThis information cannot be derived from text only in-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "in and is sufﬁciently large (1.1 million videos in the train par-"
        },
        {
          "that encode language context, can be applied successfully to": "put. We posit that the latent representations learned using the",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "tition). More importantly,\nthis data is emotion rich, as shown"
        },
        {
          "that encode language context, can be applied successfully to": "masked LM task with multi-modal input will not just encode",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "in [24]. This dataset does not provide text\ntranscriptions. We"
        },
        {
          "that encode language context, can be applied successfully to": "context, but also the relevant emotional\ninformation which",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "used a TDNN ASR model\ntrained with the standard Kaldi"
        },
        {
          "that encode language context, can be applied successfully to": "could be used to predict those words.",
          "a similar vocabulary size as our dataset,\nthey demonstrated": ""
        },
        {
          "that encode language context, can be applied successfully to": "",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "recipe on the Librispeech dataset\nto get\ntranscriptions [25]."
        },
        {
          "that encode language context, can be applied successfully to": "For\ntraining, we mask 15% of\nthe input words and the",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "We use 40-dimensional Log-Filter bank energy (LFBE) fea-"
        },
        {
          "that encode language context, can be applied successfully to": "audio and visual features corresponding to those words. The",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "tures using a 10ms frame rate to represent the audio input. Vi-"
        },
        {
          "that encode language context, can be applied successfully to": "word boundaries are obtained using an existing ASR system.",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "sual frames are represented by 4096-dimensional features ex-"
        },
        {
          "that encode language context, can be applied successfully to": "More details on the ASR system used will be discussed in",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "tracted from the VGG-16 model. 300-dimensional GloVe em-"
        },
        {
          "that encode language context, can be applied successfully to": "Section 3.1. The model predicts the words at each sequence,",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "beddings represent the text input. We chose to use GLoVe em-"
        },
        {
          "that encode language context, can be applied successfully to": "and the loss is computed only for the words that were masked",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "beddings for this task instead of an embedding layer because"
        },
        {
          "that encode language context, can be applied successfully to": "in the input sequence. Instead of providing a mask token as",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "our dataset has a limited number of sentences and vocabulary."
        },
        {
          "that encode language context, can be applied successfully to": "input\nfor masked words, we choose to set\nthe masked in-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "The vocabulary size for\nthis dataset as obtained from ASR"
        },
        {
          "that encode language context, can be applied successfully to": "put\nfor all modalities\nto zero. We are able to do so as we",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "transcriptions is 88000. Using GloVe embeddings allows the"
        },
        {
          "that encode language context, can be applied successfully to": "use GLoVe embeddings to represent\nthe text\ninput\ninstead of",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "model\nto take advantage of pre-trained embeddings trained"
        },
        {
          "that encode language context, can be applied successfully to": "learning an embedding layer. Similarly, to mask the audio and",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "on billions of words [26]. The disadvantage is the inability to"
        },
        {
          "that encode language context, can be applied successfully to": "visual inputs for the corresponding masked words, we replace",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "handle out of vocabulary words, which we ignore for all our"
        },
        {
          "that encode language context, can be applied successfully to": "the input features with zeros. For this task, we replace the av-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "experiments."
        },
        {
          "that encode language context, can be applied successfully to": "erage pooling and linear classiﬁer described in Section 2.1",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "The model\nis pre-trained using pytorch with the learning"
        },
        {
          "that encode language context, can be applied successfully to": "with a linear layer of output size equal\nto the model vocabu-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "schedule described in [27]. We stack 5-frames of\nthe LFBE"
        },
        {
          "that encode language context, can be applied successfully to": "lary. Since the encoder layer uses bi-directional attention, we",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "features for a ﬁnal audio feature dimensionality of 200. This"
        },
        {
          "that encode language context, can be applied successfully to": "do not need a decoder to attend to past predictions from the",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "was done to reduce the memory requirements for training the"
        },
        {
          "that encode language context, can be applied successfully to": "model.\nIn addition, since the encoder output\nlength is equal",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "model. We select only the English language videos\nfrom"
        },
        {
          "that encode language context, can be applied successfully to": "to the sequence length of the input\ntext, we do not require a",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "VoxCeleb2 for training. The ﬁltering is done by selecting a"
        },
        {
          "that encode language context, can be applied successfully to": "transformer decoder layer. This allows training to be simpli-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "heuristic threshold on the likelihood scores from the ASR de-"
        },
        {
          "that encode language context, can be applied successfully to": "ﬁed and was one of the reasons we chose the model architec-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "coder. For all our experiments, we use only the dev portion of"
        },
        {
          "that encode language context, can be applied successfully to": "ture.",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "the VoxCeleb2 dataset. Our ﬁnal\ntraining dataset consists of"
        },
        {
          "that encode language context, can be applied successfully to": "For the loss function, we use a full softmax loss as well as",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "978k utterances from 4820 speakers. We use the architecture"
        },
        {
          "that encode language context, can be applied successfully to": "noise contrastive estimation (NCE) [21] to train our models.",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "described in Section 2.1 to train the model. The model has"
        },
        {
          "that encode language context, can be applied successfully to": "NCE has been used successfully to learn the inverse language",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "keys, values and queries of dimension 512, 4 encoder layers,"
        },
        {
          "that encode language context, can be applied successfully to": "modeling task,\nthat\ninvolves predicting the\ncontext words",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "feed forward layer of dimension 200 and 4 attention heads for"
        },
        {
          "that encode language context, can be applied successfully to": "given a word. Minh et al. show in [22] that NCE can reduce",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "both the uni-modal and cross-modal encoders. We trained the"
        },
        {
          "that encode language context, can be applied successfully to": "computation by estimating the normalization factor for com-",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "model for 20 epochs and chose the ﬁnal model with the lowest"
        },
        {
          "that encode language context, can be applied successfully to": "puting softmax using noise samples.\nFor a task which has",
          "a similar vocabulary size as our dataset,\nthey demonstrated": "loss on a held-out set."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "For evaluating performance on the emotion recognition": "task, we ﬁne-tune the model on the CMU-MOSEI dataset",
          "recognition. Our results show up to 3% absolute improvement": "in the weighted accuracy of 4 out of the 6 emotions, with a"
        },
        {
          "For evaluating performance on the emotion recognition": "[8].\nIt\nis\nthe largest publicly available multi-modal dataset",
          "recognition. Our results show up to 3% absolute improvement": "slight\ndegradation\nin\nthe weighted\naccuracy\nof\nthe\nanger"
        },
        {
          "For evaluating performance on the emotion recognition": "for emotion recognition with natural conversations.\nIt con-",
          "recognition. Our results show up to 3% absolute improvement": "emotion. The average weighted accuracy over all emotions"
        },
        {
          "For evaluating performance on the emotion recognition": "tains 23,453 single-speaker video segments from YouTube.",
          "recognition. Our results show up to 3% absolute improvement": "improves by 0.8%. The weighted accuracy of the surprise and"
        },
        {
          "For evaluating performance on the emotion recognition": "The clips have been manually transcribed and annotated for",
          "recognition. Our results show up to 3% absolute improvement": "fear emotions improve by 2.2% and 3% absolute respectively."
        },
        {
          "For evaluating performance on the emotion recognition": "sentiment and emotion. The dataset consists of 6 emotions;",
          "recognition. Our results show up to 3% absolute improvement": "The 95% conﬁdence intervals of these emotions don’t overlap"
        },
        {
          "For evaluating performance on the emotion recognition": "happy (12135 examples), sad (5856 examples), angry (4903",
          "recognition. Our results show up to 3% absolute improvement": "with the baseline, demonstrating the statistical\nsigniﬁcance"
        },
        {
          "For evaluating performance on the emotion recognition": "examples), disgust\n(4208 examples),\nsurprise (2262 exam-",
          "recognition. Our results show up to 3% absolute improvement": "of the results. The F1-score is comparable to the baseline for"
        },
        {
          "For evaluating performance on the emotion recognition": "ples) and fear (1850 examples). The labels for each class are",
          "recognition. Our results show up to 3% absolute improvement": "all emotions other\nthan happy, where we see a 1% absolute"
        },
        {
          "For evaluating performance on the emotion recognition": "on a Likert scale of [0, 3]. We convert\nthe labels into binary",
          "recognition. Our results show up to 3% absolute improvement": "improvement. The model\ntrained using the NCE optimiza-"
        },
        {
          "For evaluating performance on the emotion recognition": "targets. A clip is assigned a 0 label\nif the score on the Likert",
          "recognition. Our results show up to 3% absolute improvement": "tion has similar improvements.\nIt shows that we can achieve"
        },
        {
          "For evaluating performance on the emotion recognition": "scale is 0, and 1 otherwise. A greater than 0 score on the Lik-",
          "recognition. Our results show up to 3% absolute improvement": "the same improvements with a lower computational cost of"
        },
        {
          "For evaluating performance on the emotion recognition": "ert scale represents the presence of the speciﬁc emotion, and",
          "recognition. Our results show up to 3% absolute improvement": "training. These results validate our hypothesis\nthat we can"
        },
        {
          "For evaluating performance on the emotion recognition": "0 the absence of the emotion. This was reﬂected in the binary",
          "recognition. Our results show up to 3% absolute improvement": "effectively leverage a large unlabeled multi-modal dataset\nto"
        },
        {
          "For evaluating performance on the emotion recognition": "interpretation that we chose.\nFor ﬁne-tuning the model, we",
          "recognition. Our results show up to 3% absolute improvement": "improve results on emotion recognition using self-supervised"
        },
        {
          "For evaluating performance on the emotion recognition": "remove the decoder layer for the masked LM task and add the",
          "recognition. Our results show up to 3% absolute improvement": "pre-training."
        },
        {
          "For evaluating performance on the emotion recognition": "average pooling and decoder\nlayer\nfor emotion recognition.",
          "recognition. Our results show up to 3% absolute improvement": "In order\nto understand the impact of ASR errors on the"
        },
        {
          "For evaluating performance on the emotion recognition": "Each example in this dataset can be labeled with the presence",
          "recognition. Our results show up to 3% absolute improvement": "model, we\ngenerated\ntranscriptions\non\nthe CMU-MOSEI"
        },
        {
          "For evaluating performance on the emotion recognition": "of multiple emotions. Therefore, we use a sigmoid output for",
          "recognition. Our results show up to 3% absolute improvement": "dataset using a commercial ASR system. The word error rate"
        },
        {
          "For evaluating performance on the emotion recognition": "each of the 6 nodes in the output\nlayer to get\nthe probability",
          "recognition. Our results show up to 3% absolute improvement": "of\nthe machine-generated transcriptions was 29%. We then"
        },
        {
          "For evaluating performance on the emotion recognition": "for each emotion. The positive and negative examples for var-",
          "recognition. Our results show up to 3% absolute improvement": "re-evaluated the performance of our baseline model with ASR"
        },
        {
          "For evaluating performance on the emotion recognition": "ious emotions in the dataset are imbalanced. During training,",
          "recognition. Our results show up to 3% absolute improvement": "based transcriptions instead of the transcriptions provided as"
        },
        {
          "For evaluating performance on the emotion recognition": "we weigh the loss for the each training sample appropriately",
          "recognition. Our results show up to 3% absolute improvement": "part of the dataset. We did not observe a degradation in emo-"
        },
        {
          "For evaluating performance on the emotion recognition": "to ensure that\nthe positive and negative examples across all",
          "recognition. Our results show up to 3% absolute improvement": "tion recognition performance. Note that for this experiment,"
        },
        {
          "For evaluating performance on the emotion recognition": "emotions contribute equally to the loss.",
          "recognition. Our results show up to 3% absolute improvement": "the baseline model was\ntrained with the original\ntranscrip-"
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "tions. ASR errors have been studied well\nin literature and it"
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "has been shown that the top contributors to errors are shorter"
        },
        {
          "For evaluating performance on the emotion recognition": "4. RESULTS",
          "recognition. Our results show up to 3% absolute improvement": ""
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "words like ’on’,\n’was’,\n’in’ etc.\n[28].\nThese words do not"
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "contribute to emotion expression, which would explain the"
        },
        {
          "For evaluating performance on the emotion recognition": "We use the weighted accuracy (WA) and F1-score for each",
          "recognition. Our results show up to 3% absolute improvement": ""
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "observations we made."
        },
        {
          "For evaluating performance on the emotion recognition": "emotion as the metrics for the task. We also report average of",
          "recognition. Our results show up to 3% absolute improvement": ""
        },
        {
          "For evaluating performance on the emotion recognition": "these two metrics over\nthe 6 emotions, keeping in line with",
          "recognition. Our results show up to 3% absolute improvement": ""
        },
        {
          "For evaluating performance on the emotion recognition": "prior work [8]. For evaluating the baseline model, we follow",
          "recognition. Our results show up to 3% absolute improvement": ""
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "4.2. Analysis and case studies"
        },
        {
          "For evaluating performance on the emotion recognition": "the procedure in [12]. The model is randomly initialized and",
          "recognition. Our results show up to 3% absolute improvement": ""
        },
        {
          "For evaluating performance on the emotion recognition": "trained 10 different times. The best model is chosen based on",
          "recognition. Our results show up to 3% absolute improvement": "We analyze the results to understand the contribution of each"
        },
        {
          "For evaluating performance on the emotion recognition": "the average of the weighted accuracy and F1-scores over all",
          "recognition. Our results show up to 3% absolute improvement": "modality towards accuracy. We look at predictions from the"
        },
        {
          "For evaluating performance on the emotion recognition": "the emotions on the dev set over the 10 runs.",
          "recognition. Our results show up to 3% absolute improvement": "baseline model with missing inputs from select modalities."
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "Note that we cannot ablate the text\ninput. The output of the"
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "cross-modal transformers will be 0 if the text input is 0 since"
        },
        {
          "For evaluating performance on the emotion recognition": "4.1. Results on emotion recognition",
          "recognition. Our results show up to 3% absolute improvement": ""
        },
        {
          "For evaluating performance on the emotion recognition": "",
          "recognition. Our results show up to 3% absolute improvement": "the attention maps will be all zeros. For subjective analysis"
        },
        {
          "For evaluating performance on the emotion recognition": "Table 1 shows the results of our experiments and state of the",
          "recognition. Our results show up to 3% absolute improvement": "with missing text input, we trained a baseline model with au-"
        },
        {
          "For evaluating performance on the emotion recognition": "art\nresults on the same dataset\nfrom other publications. The",
          "recognition. Our results show up to 3% absolute improvement": "dio as the anchor modality. As noted in Section 2.1, the choice"
        },
        {
          "For evaluating performance on the emotion recognition": "transformer baseline outperforms or\nis comparable to pub-",
          "recognition. Our results show up to 3% absolute improvement": "of anchor modality does not change the performance of\nthe"
        },
        {
          "For evaluating performance on the emotion recognition": "lished results\nfor most of\nthe metrics. Our model\nshows a",
          "recognition. Our results show up to 3% absolute improvement": "baseline model. We describe our subjective analysis below."
        },
        {
          "For evaluating performance on the emotion recognition": "2.4% absolute improvement\nin the weighted accuracy of the",
          "recognition. Our results show up to 3% absolute improvement": "The ﬁrst example we observe is ID “HeZS2-Prhc[8]” in"
        },
        {
          "For evaluating performance on the emotion recognition": "anger emotion and a 2.6% absolute improvement\nin the F1-",
          "recognition. Our results show up to 3% absolute improvement": "the dataset. From visual\ninspection,\nthe video shows that\nthe"
        },
        {
          "For evaluating performance on the emotion recognition": "score of\nthe surprise emotion. We observe a degradation in",
          "recognition. Our results show up to 3% absolute improvement": "speaker is laughing, which conveys a happy emotion. How-"
        },
        {
          "For evaluating performance on the emotion recognition": "the weighted accuracy of the fear emotion. This comparison",
          "recognition. Our results show up to 3% absolute improvement": "ever,\nthe speaker\nis talking about\nthe cost\nfor drugs and its"
        },
        {
          "For evaluating performance on the emotion recognition": "with other state of the art models is pertinent for the rest of",
          "recognition. Our results show up to 3% absolute improvement": "impact on communities. This is why the visual modality is"
        },
        {
          "For evaluating performance on the emotion recognition": "our work as we want to build upon a strong baseline model.",
          "recognition. Our results show up to 3% absolute improvement": "the key to accurately predicting the emotion, and the model"
        },
        {
          "For evaluating performance on the emotion recognition": "The next set of results in Table 1 are with the pre-trained",
          "recognition. Our results show up to 3% absolute improvement": "is not able to classify the emotion as happy with text\ninput"
        },
        {
          "For evaluating performance on the emotion recognition": "model\non\nthe VoxCeleb2\ndataset,\nﬁne-tuned\nfor\nemotion",
          "recognition. Our results show up to 3% absolute improvement": "alone. On the contrary, the second example, ID “10219[11]”,"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: Adding audio and visual input along ments. We would also like to experiment with adapting the",
      "data": [
        {
          "Table 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4": "Model"
        },
        {
          "Table 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4": ""
        },
        {
          "Table 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4": "M-ELMo + NN [12](A+T)"
        },
        {
          "Table 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4": "Graph-MFN [8]"
        },
        {
          "Table 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4": "Transformer (baseline)"
        },
        {
          "Table 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4": "Transformer with pre-training and full softmax loss"
        },
        {
          "Table 1. Emotion recognition results on the CMU-MOSEI task. The 95% conﬁdence interval for all metrics is less than ±1.4": "Transformer with pre-training and NCE loss"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Adding audio and visual input along ments. We would also like to experiment with adapting the",
      "data": [
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": ""
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "topics,\nspeciﬁcally involving human to human communica-"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": ""
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "tion,\nthe other modalities would start\nto gain importance for"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "recognizing the emotions accurately."
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": ""
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": ""
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "5. CONCLUSION"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": ""
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": ""
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "In this paper, we present state of the art results on the emo-"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "tion recognition task using the cross-modal transformer on the"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "CMU-MOSEI dataset. We utilize a BERT-like pre-training"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "scheme using audio, visual and text\ninputs. We use the Vox-"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "Celeb2 dataset\nto pre-train the model and ﬁne-tune it for the"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "emotion recognition task. We demonstrate up to a 3% im-"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "provement over the baseline with the ﬁne-tuned model. We"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "presented our subjective analysis on the contribution of vari-"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "ous modalities to emotion recognition. We also show results"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "with missing input modalities to understand the importance"
        },
        {
          "is the most\nimportant\ninput. We posit\nthat\nfor more diverse": "of each modality for the emotion recognition task."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "IEEE, 2020, pp. 6419–6423."
        },
        {
          "6. REFERENCES": "[1]\nJ. K. Burgoon, L. K. Guerrero, and K. Floyd, Nonverbal",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[12] S.-Y. Tseng, P. Georgiou,\nand S. Narayanan,\n“Mul-"
        },
        {
          "6. REFERENCES": "communication.\nRoutledge, 2016.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "arXiv\ntimodal\nembeddings\nfrom language models,”"
        },
        {
          "6. REFERENCES": "[2] B.\nSchuller, M. Valstar,\nF.\nEyben,\nG. McKeown,",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "preprint arXiv:1909.04302, 2019."
        },
        {
          "6. REFERENCES": "R. Cowie, and M. Pantic, “Avec 2011–the ﬁrst\ninterna-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[13] D. Harwath and J. Glass, “Learning word-like units from"
        },
        {
          "6. REFERENCES": "tional audio/visual emotion challenge,” in International",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "the 55th\njoint audio-visual analysis,” in Proceedings of"
        },
        {
          "6. REFERENCES": "Conference on Affective Computing and Intelligent\nIn-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "Annual Meeting of\nthe Association for Computational"
        },
        {
          "6. REFERENCES": "teraction.\nSpringer, 2011, pp. 415–424.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "Linguistics (Volume 1:\nLong Papers), 2017, pp. 506–"
        },
        {
          "6. REFERENCES": "[3] B. Nojavanasghari, D. Gopinath,\nJ. Koushik, T. Bal-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "517."
        },
        {
          "6. REFERENCES": "truˇsaitis, and L.-P. Morency, “Deep multimodal\nfusion",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[14] D. Harwath, A. Recasens, D. Sur´ıs, G. Chuang, A. Tor-"
        },
        {
          "6. REFERENCES": "the\nfor persuasiveness prediction,”\nin Proceedings of",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "ralba, and J. Glass, “Jointly discovering visual objects"
        },
        {
          "6. REFERENCES": "18th ACM International Conference on Multimodal In-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "Interna-\nand spoken words\nfrom raw sensory input,”"
        },
        {
          "6. REFERENCES": "teraction, 2016, pp. 284–288.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "tional Journal of Computer Vision, vol. 128, no. 3, pp."
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "620–641, 2020."
        },
        {
          "6. REFERENCES": "[4] A. Haque, M. Guo, A.\nS. Miner,\nand L.\nFei-Fei,",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "“Measuring depression symptom severity from spoken",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[15] Y.-H. H. Tsai, S. Bai, P. P. Liang,\nJ. Z. Kolter, L.-"
        },
        {
          "6. REFERENCES": "arXiv\npreprint\nlanguage\nand\n3d\nfacial\nexpressions,”",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "P. Morency, and R. Salakhutdinov, “Multimodal\ntrans-"
        },
        {
          "6. REFERENCES": "arXiv:1811.08592, 2018.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "former for unaligned multimodal\nlanguage sequences,”"
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "in Proceedings of the conference. Association for Com-"
        },
        {
          "6. REFERENCES": "[5] C. Chung and J. W. Pennebaker,\n“The psychological",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "putational Linguistics. Meeting, vol. 2019.\nNIH Public"
        },
        {
          "6. REFERENCES": "Social\nfunctions of\nfunction words,”\ncommunication,",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "Access, 2019, p. 6558."
        },
        {
          "6. REFERENCES": "vol. 1, pp. 343–359, 2007.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[16] A. Khare, S. Parthasarathy, and S. Sundaram, “Multi-"
        },
        {
          "6. REFERENCES": "[6] R. Banse and K. R. Scherer, “Acoustic proﬁles in vocal",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "modal embeddings using multi-task learning for emo-"
        },
        {
          "6. REFERENCES": "emotion expression.” Journal of personality and social",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "tion recognition,” Proc. Interspeech 2020, pp. 384–388,"
        },
        {
          "6. REFERENCES": "psychology, vol. 70, no. 3, p. 614, 1996.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "2020."
        },
        {
          "6. REFERENCES": "[7]\nJ. F. Cohn, “Foundations of human computing:\nfacial",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[17] G. Paraskevopoulos, S. Parthasarathy, A. Khare,\nand"
        },
        {
          "6. REFERENCES": "the 8th in-\nexpression and emotion,” in Proceedings of",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "S. Sundaram, “Multimodal and multiresolution speech"
        },
        {
          "6. REFERENCES": "ternational conference on Multimodal\ninterfaces, 2006,",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "the\nrecognition with transformers,”\nin Proceedings of"
        },
        {
          "6. REFERENCES": "pp. 233–238.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "58th Annual Meeting of\nthe Association for Computa-"
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "tional Linguistics, 2020, pp. 2381–2387."
        },
        {
          "6. REFERENCES": "[8] P. Liang, R. Salakhutdinov, and L.-P. Morency, “Com-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "putational modeling of human multimodal\nlanguage:",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[18] W. Rahman, M. K. Hasan, S. Lee, A. B. Zadeh, C. Mao,"
        },
        {
          "6. REFERENCES": "The mosei dataset and interpretable dynamic fusion,”",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "L.-P. Morency, and E. Hoque, “Integrating multimodal"
        },
        {
          "6. REFERENCES": "2018.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "information in large pretrained transformers,”\nin Pro-"
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "ceedings of the 58th Annual Meeting of the Association"
        },
        {
          "6. REFERENCES": "[9]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "for Computational Linguistics, 2020, pp. 2359–2369."
        },
        {
          "6. REFERENCES": "“Bert:\nPre-training of deep bidirectional\ntransformers",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "for language understanding,” in Proceedings of the 2019",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[19] A. Vaswani, N.\nShazeer, N.\nParmar,\nJ. Uszkoreit,"
        },
        {
          "6. REFERENCES": "Conference of\nthe North American Chapter of\nthe As-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin,"
        },
        {
          "6. REFERENCES": "sociation for Computational Linguistics: Human Lan-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "“Attention is all you need,” in Advances in neural infor-"
        },
        {
          "6. REFERENCES": "guage Technologies, Volume 1 (Long and Short Papers),",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "mation processing systems, 2017, pp. 5998–6008."
        },
        {
          "6. REFERENCES": "2019, pp. 4171–4186.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient"
        },
        {
          "6. REFERENCES": "[10] S. Schneider, A. Baevski, R. Collobert, and M. Auli,",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "estimation\nof word\nrepresentations\nin\nvector\nspace,”"
        },
        {
          "6. REFERENCES": "“wav2vec: Unsupervised pre-training for speech recog-",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "arXiv preprint arXiv:1301.3781, 2013."
        },
        {
          "6. REFERENCES": "nition,” Proc. Interspeech 2019, pp. 3465–3469, 2019.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": ""
        },
        {
          "6. REFERENCES": "",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "[21] M. U. Gutmann and A. Hyv¨arinen, “Noise-contrastive"
        },
        {
          "6. REFERENCES": "[11] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y.",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "estimation of unnormalized statistical models, with ap-"
        },
        {
          "6. REFERENCES": "Lee, “Mockingjay: Unsupervised speech representation",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "plications\nto natural\nimage statistics,” Journal of Ma-"
        },
        {
          "6. REFERENCES": "learning with deep bidirectional\ntransformer encoders,”",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "chine Learning Research, vol. 13, no. Feb, pp. 307–361,"
        },
        {
          "6. REFERENCES": "in ICASSP 2020-2020 IEEE International Conference",
          "on Acoustics, Speech and Signal Processing (ICASSP).": "2012."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "beddings efﬁciently with noise-contrastive estimation,”"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "in Advances in neural\ninformation processing systems,"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "2013, pp. 2265–2273."
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "[23]\nJ. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2:"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "Deep speaker recognition,” in INTERSPEECH, 2018."
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "[24] S. Albanie, A. Nagrani, A. Vedaldi,\nand A. Zisser-"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "man, “Emotion recognition in speech using cross-modal"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "transfer in the wild,” in ACM Multimedia, 2018."
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "[25] V. Peddinti, D. Povey, and S. Khudanpur, “A time de-"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "lay neural network architecture for efﬁcient modeling"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "of long temporal contexts,” in Sixteenth Annual Confer-"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "ence of\nthe International Speech Communication Asso-"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "ciation, 2015."
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "[26]\nJ.\nPennington,\nR.\nSocher,\nand\nC.\nD. Manning,"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "“Glove:\nGlobal vectors\nfor word representation,”\nin"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "Empirical Methods\nin Natural Language Processing"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "(EMNLP), 2014, pp. 1532–1543.\n[Online]. Available:"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "http://www.aclweb.org/anthology/D14-1162"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "[27] L. Dong, S. Xu,\nand B. Xu,\n“Speech-transformer:\na"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "no-recurrence sequence-to-sequence model\nfor\nspeech"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "recognition,”\nin 2018 IEEE International Conference"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "IEEE, 2018, pp. 5884–5888."
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "[28] A. Stolcke\nand\nJ. Droppo,\n“Comparing\nhuman\nand"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "machine errors in conversational speech transcription,”"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "Proc. Interspeech 2017, pp. 137–141, 2017."
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "[29] S.-Y. Tseng, B. Baucom, and P. Georgiou, “Unsuper-"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "vised online multitask learning of behavioral sentence"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "embeddings,” PeerJ Computer Science, vol. 5, p. e200,"
        },
        {
          "[22] A. Mnih and K. Kavukcuoglu,\n“Learning word em-": "2019."
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "J Burgoon",
        "L Guerrero",
        "K Floyd"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Avec 2011-the first international audio/visual emotion challenge",
      "authors": [
        "B Schuller",
        "M Valstar",
        "F Eyben",
        "G Mckeown",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "4",
      "title": "Deep multimodal fusion for persuasiveness prediction",
      "authors": [
        "B Nojavanasghari",
        "D Gopinath",
        "J Koushik",
        "T Baltrušaitis",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "5",
      "title": "Measuring depression symptom severity from spoken language and 3d facial expressions",
      "authors": [
        "A Haque",
        "M Guo",
        "A Miner",
        "L Fei-Fei"
      ],
      "year": "2018",
      "venue": "Measuring depression symptom severity from spoken language and 3d facial expressions",
      "arxiv": "arXiv:1811.08592"
    },
    {
      "citation_id": "6",
      "title": "The psychological functions of function words",
      "authors": [
        "C Chung",
        "J Pennebaker"
      ],
      "year": "2007",
      "venue": "Social communication"
    },
    {
      "citation_id": "7",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "8",
      "title": "Foundations of human computing: facial expression and emotion",
      "authors": [
        "J Cohn"
      ],
      "year": "2006",
      "venue": "Proceedings of the 8th international conference on Multimodal interfaces"
    },
    {
      "citation_id": "9",
      "title": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion",
      "authors": [
        "P Liang",
        "R Salakhutdinov",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion"
    },
    {
      "citation_id": "10",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "11",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "A Liu",
        "S -W. Yang",
        "P.-H Chi",
        "P.-C Hsu",
        "H.-Y Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Multimodal embeddings from language models",
      "authors": [
        "S.-Y Tseng",
        "P Georgiou",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Multimodal embeddings from language models",
      "arxiv": "arXiv:1909.04302"
    },
    {
      "citation_id": "14",
      "title": "Learning word-like units from joint audio-visual analysis",
      "authors": [
        "D Harwath",
        "J Glass"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Jointly discovering visual objects and spoken words from raw sensory input",
      "authors": [
        "D Harwath",
        "A Recasens",
        "D Surís",
        "G Chuang",
        "A Torralba",
        "J Glass"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "17",
      "title": "Multimodal embeddings using multi-task learning for emotion recognition",
      "authors": [
        "A Khare",
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "18",
      "title": "Multimodal and multiresolution speech recognition with transformers",
      "authors": [
        "G Paraskevopoulos",
        "S Parthasarathy",
        "A Khare",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "20",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "22",
      "title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics",
      "authors": [
        "M Gutmann",
        "A Hyvärinen"
      ],
      "year": "2012",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "23",
      "title": "Learning word embeddings efficiently with noise-contrastive estimation",
      "authors": [
        "A Mnih",
        "K Kavukcuoglu"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "26",
      "title": "A time delay neural network architecture for efficient modeling of long temporal contexts",
      "authors": [
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "27",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition",
      "authors": [
        "L Dong",
        "S Xu",
        "B Xu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Comparing human and machine errors in conversational speech transcription",
      "authors": [
        "A Stolcke",
        "J Droppo"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "30",
      "title": "Unsupervised online multitask learning of behavioral sentence embeddings",
      "authors": [
        "S.-Y Tseng",
        "B Baucom",
        "P Georgiou"
      ],
      "year": "2019",
      "venue": "PeerJ Computer Science"
    }
  ]
}