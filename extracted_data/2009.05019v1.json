{
  "paper_id": "2009.05019v1",
  "title": "Multi-Modal Embeddings Using Multi-Task Learning For Emotion Recognition",
  "published": "2020-09-10T17:33:16Z",
  "authors": [
    "Aparna Khare",
    "Srinivas Parthasarathy",
    "Shiva Sundaram"
  ],
  "keywords": [
    "general embeddings",
    "multi-modal",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "General embeddings like word2vec, GloVe and ELMo have shown a lot of success in natural language tasks. The embeddings are typically extracted from models that are built on general tasks such as skip-gram models and natural language generation. In this paper, we extend the work from natural language understanding to multi-modal architectures that use audio, visual and textual information for machine learning tasks. The embeddings in our network are extracted using the encoder of a transformer model trained using multi-task training. We use person identification and automatic speech recognition as the tasks in our embedding generation framework. We tune and evaluate the embeddings on the downstream task of emotion recognition and demonstrate that on the CMU-MOSEI dataset, the embeddings can be used to improve over previous state of the art results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Humans possess the ability to encode and express a wide range of intricate verbal and non-verbal cues based on goal and context. This has evolved into a complementary ability to detect nuanced cues in everyday communication. This ability is a result of top-down processing  [1]  where based on context and learning humans are able to encode and decode person to person information flow efficiently. Context is typically set by what is being communicated and how through multi-modal cues. Inspired by this, several studies have shown that multi-modal input to the systems can improve accuracy on tasks involving human communication, such as speech recognition  [2, 3] , emotion recognition  [4]  and speaker recognition  [5] .\n\nRecently, use of generalized feature representations have become prevalent in the computer vision and natural language research. Computer vision tasks like object detection and semantic segmentation show improved accuracy when the features from the images are extracted using models trained on large amounts of data like ImageNet  [6] . In the natural learning literature, generalized embeddings like GloVe and word2vec have demonstrated state of the art performance in several tasks like word similarity, word analogy and named entity recognition  [7] . For speech applications like automatic speech recognition (ASR), speaker recognition and paralinguistics it is still traditional to use hand-crafted features like MFCCs, LFBEs or features from toolkits like openSMILE  [4] . However, it has also been demonstrated that features learned directly from audio can improve performance when the amount of training data is large enough  [8] .\n\nThe research in the various domains has demonstrated that transfer learning with models trained on large datasets can improve accuracy on subsequent tasks. This is especially important when the size of the labeled datasets is not large. There are a variety of multi-modal tasks like emotion recognition which still do not have large amounts of publicly available datasets. Motivated by this, we propose a model to learn embeddings that combine the features from audio, video, and text modalities to improve the performance on downstream tasks. The main contribution of this paper is to understand if we can leverage large datasets to build these representations that can outperform the models built for specific tasks where the datasets are limited. For our work, we use emotion recognition as the downstream task to evaluate the embeddings. In practical applications, it is possible that all modalities are not available to the machine learning system for inference. For example, for any applications that use video from web-based applications, any disturbance in the communication network can lead to missing audio or visual input. This leads to the second objective of our study; to perform ablation studies to understand the impact of the missing modality, and understand how to compensate for it.\n\nThis paper is organized as follows; in Section 2, we discuss prior work in multi-modal tasks and embedding generation techniques. Our proposed technique for embedding extraction is presented in Section 3. In 4 we discuss the training setup and data. Finally, we present our results in Section 5 and conclude in Section 6.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Prior Work",
      "text": "Leveraging large datasets for representation learning, like billions of words from the Wikipedia dataset to train BERT or millions of images from ImageNet to train ResNET  [9] , has become quite popular in machine learning to improve performance on tasks with smaller labeled datasets. Tseng et al. demonstrate in  [10]  that a model trained on the language modeling (LM) task, which was pre-trained on the 1 Billion Word Language Model Benchmark, can be used to extract ELMo-like sentence embeddings using audio and word tokens to produce state of the art results on the emotion recognition task. Similarly, Tseng et al. also used an encoder-decoder architecture trained on a skip-though vector task on 30 million sentence pairs from the OpenSubtitles Corpus, to extract text-only embeddings in  [11]  . They further show that in conjunction with weakly supervised learning, the features extracted from such a network can improve performance for sentiment analysis. In the natural language domain, the BERT model trained on a 3 billion words from the Wikipedia and BookCorpus corpora has demonstrated state of the art results on various natural language tasks like natural language inference, sentence classification, sentiment analysis and question answering  [12]  . Although not widely adopted, there has been similar work in the speech domain; Chung et al. introduce speech2vec in  [13]  , which is trained similarly as word2vec but with speech as its input. This paper focuses on generating embeddings that have semantic information and semantically similar words that are close in the latent space. There has also been work done to generate embeddings from speech which are similar for similar sounding words  [14] .\n\nWhile models like BERT use unsupervised techniques using a single task to learn generalized representations, there has been a parallel area of work that shows how multi-task training can also help improve performance. As described above,  [11]  employs multi-task learning to improve performance on the emotion recognition task.  [15]  also shows that multi-task training on sentiment and emotion recognition improves performance on both tasks. Luong et al. demonstrate in  [16]  that multi-task training with shared encoder and individual decoders for the machine translation and other supervised and unsupervised tasks like skip-though vector generation and image caption generation can improve performance on the translation task, thus helping the model learn more generalized representations.\n\nAn additional challenge that multi-modal systems present is combining multiple modalities in the system. Several papers have focused on late fusion; combining features from the different modalities after a few neural network layers  [17] .  [10]  employs a feature fusion at the frame level using a gated convolutional architecture. More recently, cross-modal transformers have been employed in order to project one modality into another  [18] , which have been successfully applied to sentiment analysis  [18]  and speech recognition  [19] .  [20]  employs a neural network to extract multi-modal embeddings from visual features by applying a loss function that forces the embedding to be similar to the GloVe embedding of the entity in the image.\n\nIn this paper, we extend the prior work in three main ways. Firstly, we combine the audio, visual and text modalities together in order to learn tri-modal representations from large datasets. Additionally, instead of using a single task for training the model, we use a multi-task training architecture to make the embeddings more general in nature. Finally, we employ the cross-modal transformer to train our model. Transformers lend themselves naturally to multi-modal sequences of different lengths as they allow for an easy combination of modalities without the need to specifically align them as shown in  [18] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multitask Training For Learning Embeddings",
      "text": "We use two tasks for training our model; character level speech recognition and person identification. ASR is a natural equivalent of the language modeling task for the audio domain, and the language modeling task has been applied with success for learning generalized embeddings. The choice of the person identification task is mainly due to the availability of open source datasets with labels for the task. We employ the architecture proposed in  [19]  in order to project the features in the text and visual domain to the audio domain shown in Figure  1 . This is an alternative to the traditional early and late fusion techniques for combining features from different modalities; the cross-modal attention module computes the attention weights between features from two modalities, and weighs the features that are correlated higher, thus paying more attention to the features that are more important for the task. There is a joint encoder that comprises of an audio only encoder, and 2 cross-modal transformer components which project the video and text features to the audio space. The final sequential output from the encoder is a simple weighted sum of the various modalities. For the ASR decoder, we use the Transformer decoder, whereas for the person identification task we average the embeddings over a given sequence and use an affine transformation layer for classifying the speaker. The final loss function is a weighted sum of the In our setup, we use a weight of 0.2 for the person identification task and 0.8 for the ASR task. This quantity was not tuned and we will work on tuning and observing the effects of the weights in our future work. The ASR task is more general in nature; since it involves learning the language which inherently has semantic meaning, we intuitively choose a higher weight on this task.\n\nFor our model, we choose a weight of 0.4 for the audio and visual modalities and 0.2 for the text modality. The text input to the system in the form of GloVe embeddings can be reverse mapped to the characters, therefore we heuristically chose a lower weight on the text modality in order for the model to focus more learning on the audio and video modalities. The text input can be considered the equivalent of a thought vector in this case and is still relevant to the ASR task. Similarly, for the person identification task, text is an auxiliary input which can be used to make the system more text dependent. For the crossmodal transformer component, we chose to transform the text and visual domains into the audio domain as explained in  [18] , but we could have chosen to map the audio and textual domains to visual modality as well. In our future work, we will evaluate the impact of this choice.\n\nOther tasks like skip-thought vector and masked language modeling task, that have been used in the natural language domain can also be applied to this kind of multi-task architecture. These tasks, however, present certain challenges when the audio and visual modalities are involved. For the masked LM task, the model tries to predict a word that has been masked in the input. For audio and visual modalities, this would involve aligning the text with the audio to obtain the time alignment for each word, and computing out the corresponding masks. The skipthought vector task would require the dataset to have continuity. This continuity is easily provided for datasets from the text domain, where the data is typically obtained from documents or Wikipedia. When using videos, preparing such a dataset would require finding sentence boundaries, and segmenting the videos in order to find subsequent sentences to set up the task. These alignments and segmentation tasks can be accomplished by using an ASR system, but introduce complexity in the process. We will include these tasks in our future work.   [23] . We use the standard recipe available in Kaldi in order to train the models. We extract 40-dimensional Log-Filter bank energy (LFBE) features using a 10ms frame rate to represent the audio, 4096-dimensional features extracted from the VGG-16 model to represent the visual modality and 300dimensional GloVe vector to represent the text modality. The model is trained using pytorch with the learning schedule described in  [24]  We stack 5-frames of the LFBE features for a final feature audio dimensionality of 200. The first operation in the transformer encoder is a dot-product attention layer, which computes the attention weights between the query Q and the key K as softmax(QK T / (d)), where d is the dimensionality of K (or Q), and they are matrices of dimension n×d where n is the sequence length. By stacking the features to reduce the sequence length, we reduce the memory required to compute the attention maps.   1 , where the output of the encoder layer goes to a linear classification layer. To train the transformer model, we use equal weights on all modalities. We experimented with learned weights but the final weights learned by the model were not very different from equal weights so we choose to use this as a fixed parameter. In order to leverage the learned embeddings from the Vox-Celeb dataset, we use the model trained with multi-task training as a pre-trained model and tune it with the CMU-MOSEI dataset on the emotion recognition task. We chose to tune the embeddings and not fix them based on prior research from the natural language domain where researchers have demonstrated that tuning the word2vec embeddings for the language modeling task can improve performance.  [25] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Results",
      "text": "For all our evaluations, we report the weighted accuracy (WA) and F1-score for each emotion and the average metrics over the 6 emotions. The threshold for the both metrics was optimized separately on the development set. For all the reported results on the emotion recognition task, the model is randomly initialized and trained 10 different times. The best model is chosen based on the average of the weighted accuracy and F1-scores over the dev set over the 10 runs, as suggested in  [10] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Baseline Results",
      "text": "Table  1  shows the results of the two baseline models. We have also included results from other publications in order to com-   [18]  that this architecture outperforms the late fusion architecture for the MOSEI sentiment task. They however did not report results on the emotion task for the dataset. For the remainder of the paper, our baseline will refer to the transformer-based baseline in this table. Note that the external publications used a different set of features for the audio and visual modalities training their models, which is why we present the late fusion results with our feature set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results With Embeddings",
      "text": "Table  2  shows the results of the experiment where we use the model trained with multi-task training on the VoxCeleb2 dataset to initialize the transformer model for the emotion recognition task. The model outperforms the baseline by more than 1% absolute for the weighted accuracy for the sad and surprise emotions, and F1-score for the anger emotion. We observe a degradation in the weighted accuracy of the fear emotion. The rest of the metrics are comparable to the baseline model. The improvements on the weighted accuracy of the surprise and sad emotions are significant as the 95% confidence intervals are non-overlapping.\n\nTo analyze the results obtained from the embeddings, we look at the distribution of emotions in the VoxCeleb2 dataset. The distribution on the development set is not available, however the authors of the dataset have created a subset of the dataset called EmoVoxceleb  [26] . This dataset is a subset containing 100k videos from the Voxceleb2 dataset. Since the authors haven't mentioned a sampling strategy, we assume that this data was randomly sampled. In  [26] , Albanie et al. present a frame level distribution of the emotions in the dataset. Neutral, happy, surprise and sadness are the dominant emotions in the dataset, whereas fear and disgust are the least frequent. Under the assumption that the Voxceleb2 dataset has a similar distribution of emotions, we posit that the improvements on the sad and surprise emotions and degradation in fear emotion can be attributed to this distribution.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Studies",
      "text": "As mentioned in the introduction, for practical applications we would like the understand the impact of missing input modalities. To understand this, the first experiment we perform is to remove the video and/or text inputs to both the baseline as well as the model pre-trained model during inference only. Since in our architecture we use the cross-modal attention to transform the video and text modalities to the audio domain, the architecture doesn't allow for a missing audio modality. In order to run the ablation study, we just use a 0 weight on the missing modality during the weighted combination of the outputs of the audio encoder and the cross-modal transformers in the model to compute the final embedding.\n\nTable  3  show the results with different combination of the modalities. The results show that in the case of missing modalities, the embeddings improve over the baseline even more than when all the modalities are present. When both the video and text modalities are absent, the model performs the worst. With text and audio input, the performance of the baseline and embeddings-based model is close to the performance with no ablation. Since the GloVe embeddings, which are already pretrained to represent semantic meaning with over 6 billion tokens  [7] , they already provide meaningful features to the system in order to get good performance. Prior work  [18, 27]  has shown that for uni-modal emotion recognition, text based models outperforms audio and visual based models, and our ablation results demonstrate the same; adding text to the input audio boosts the weighted average by 8.6% absolute as compared to adding visual information that improves it by 3.3%.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this paper, we present state of the art results on the CMU-MOSEI emotion recognition task using a cross-modal attention based transformer architecture. We demonstrate that by using a multi-task architecture, we can leverage a large multi-modal dataset like VoxCeleb in order to learn general embeddings that can improve the performance on a downstream task like emotion recognition.\n\nOur future work will address one of the major drawbacks of our architecture in its dependency on audio which doesn't allow for missing audio during inference. In addition, for text input to the system, we require either a human or a machine based transcription. Prior work, however, has demonstrated that we can learn embeddings from audio that capture the semantic meaning  [13] . By incorporating this in the model, we should be able to eliminate the need for the text input altogether. Both the CMU-MOSEI and Voxceleb2 datasets are obtained from YouTube but there could be some mismatch due to the way the videos were selected. This could be bridged by adapting the embeddings on the CMU-MOSEI dataset for improved performance. We would like to increase the number of tasks in our multi-task architecture in order to make the representations even more generalized. Specifically, experimenting with visual tasks like landmark detection which would provide useful information for tasks involving affect recognition. An additional future direction is to try unsupervised methods like skip-thought vector or other selfsupervised training methods. Another interesting approach to consider is to combine the multi-task learning with weak supervision as suggested in  [11]  for each of the tasks in question in order to make the embeddings from discriminative. Finally, we will evaluate our technique on more downstream tasks to study what tasks the representations generalize well to.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This is an",
      "page": 2
    },
    {
      "caption": "Figure 1: Transformer based multi-task architecture",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Amazon.com, Sunnyvale, CA": "apkhare,parsrini,sssundar@amazon.com"
        },
        {
          "Amazon.com, Sunnyvale, CA": "Abstract"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "General embeddings like word2vec, GloVe and ELMo have"
        },
        {
          "Amazon.com, Sunnyvale, CA": "shown a lot of success in natural\nlanguage tasks. The embed-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "dings are typically extracted from models that are built on gen-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "eral tasks such as skip-gram models and natural language gener-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "ation.\nIn this paper, we extend the work from natural\nlanguage"
        },
        {
          "Amazon.com, Sunnyvale, CA": "understanding to multi-modal architectures that use audio, vi-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "sual and textual\ninformation for machine learning tasks.\nThe"
        },
        {
          "Amazon.com, Sunnyvale, CA": "embeddings in our network are extracted using the encoder of"
        },
        {
          "Amazon.com, Sunnyvale, CA": "a transformer model\ntrained using multi-task training. We use"
        },
        {
          "Amazon.com, Sunnyvale, CA": "person identiﬁcation and automatic speech recognition as the"
        },
        {
          "Amazon.com, Sunnyvale, CA": "tasks in our embedding generation framework. We tune and"
        },
        {
          "Amazon.com, Sunnyvale, CA": "evaluate the embeddings on the downstream task of emotion"
        },
        {
          "Amazon.com, Sunnyvale, CA": "recognition and demonstrate that on the CMU-MOSEI dataset,"
        },
        {
          "Amazon.com, Sunnyvale, CA": "the embeddings can be used to improve over previous state of"
        },
        {
          "Amazon.com, Sunnyvale, CA": "the art results."
        },
        {
          "Amazon.com, Sunnyvale, CA": "Index Terms:\ngeneral\nembeddings, multi-modal,\nemotion"
        },
        {
          "Amazon.com, Sunnyvale, CA": "recognition"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "1.\nIntroduction"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "Humans possess the ability to encode and express a wide range"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "of intricate verbal and non-verbal cues based on goal and con-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "text. This has evolved into a complementary ability to detect nu-"
        },
        {
          "Amazon.com, Sunnyvale, CA": ""
        },
        {
          "Amazon.com, Sunnyvale, CA": "anced cues in everyday communication. This ability is a result"
        },
        {
          "Amazon.com, Sunnyvale, CA": "of top-down processing [1] where based on context and learning"
        },
        {
          "Amazon.com, Sunnyvale, CA": "humans are able to encode and decode person to person infor-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "mation ﬂow efﬁciently. Context is typically set by what is being"
        },
        {
          "Amazon.com, Sunnyvale, CA": "communicated and how through multi-modal cues.\nInspired by"
        },
        {
          "Amazon.com, Sunnyvale, CA": "this, several studies have shown that multi-modal\ninput\nto the"
        },
        {
          "Amazon.com, Sunnyvale, CA": "systems can improve accuracy on tasks involving human com-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "munication, such as speech recognition [2, 3], emotion recogni-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "tion [4] and speaker recognition [5]."
        },
        {
          "Amazon.com, Sunnyvale, CA": "Recently, use of generalized feature representations have"
        },
        {
          "Amazon.com, Sunnyvale, CA": "become\nprevalent\nin\nthe\ncomputer\nvision\nand\nnatural\nlan-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "guage research.\nComputer vision tasks\nlike object detection"
        },
        {
          "Amazon.com, Sunnyvale, CA": "and semantic segmentation show improved accuracy when the"
        },
        {
          "Amazon.com, Sunnyvale, CA": "features\nfrom the images are extracted using models\ntrained"
        },
        {
          "Amazon.com, Sunnyvale, CA": "on large\namounts of data\nlike\nImageNet\n[6].\nIn the natu-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "ral\nlearning literature, generalized embeddings like GloVe and"
        },
        {
          "Amazon.com, Sunnyvale, CA": "word2vec have demonstrated state of the art performance in sev-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "eral\ntasks like word similarity, word analogy and named entity"
        },
        {
          "Amazon.com, Sunnyvale, CA": "recognition[7].\nFor speech applications like automatic speech"
        },
        {
          "Amazon.com, Sunnyvale, CA": "recognition (ASR), speaker recognition and paralinguistics it is"
        },
        {
          "Amazon.com, Sunnyvale, CA": "still traditional to use hand-crafted features like MFCCs, LFBEs"
        },
        {
          "Amazon.com, Sunnyvale, CA": "or features from toolkits like openSMILE [4]. However,\nit has"
        },
        {
          "Amazon.com, Sunnyvale, CA": "also been demonstrated that features learned directly from au-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "dio can improve performance when the amount of training data"
        },
        {
          "Amazon.com, Sunnyvale, CA": "is large enough [8]."
        },
        {
          "Amazon.com, Sunnyvale, CA": "The research in the various domains has demonstrated that"
        },
        {
          "Amazon.com, Sunnyvale, CA": "transfer learning with models trained on large datasets can im-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "prove accuracy on subsequent\ntasks. This is especially impor-"
        },
        {
          "Amazon.com, Sunnyvale, CA": "tant when the size of the labeled datasets is not large. There are"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "ing a single task to learn generalized representations,\nthere has"
        },
        {
          "While models like BERT use unsupervised techniques us-": "been a parallel area of work that shows how multi-task train-"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "ing can also help improve performance. As described above,"
        },
        {
          "While models like BERT use unsupervised techniques us-": "[11] employs multi-task learning to improve performance on"
        },
        {
          "While models like BERT use unsupervised techniques us-": "the emotion recognition task.\n[15] also shows that multi-task"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "training on sentiment and emotion recognition improves per-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "formance on both tasks.\nLuong et al.\ndemonstrate in [16]"
        },
        {
          "While models like BERT use unsupervised techniques us-": "that multi-task training with shared encoder and individual de-"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "coders for the machine translation and other supervised and un-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "supervised tasks like skip-though vector generation and image"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "caption generation can improve performance on the translation"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "task, thus helping the model learn more generalized representa-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "tions."
        },
        {
          "While models like BERT use unsupervised techniques us-": "An additional challenge that multi-modal systems present"
        },
        {
          "While models like BERT use unsupervised techniques us-": "is combining multiple modalities in the system. Several papers"
        },
        {
          "While models like BERT use unsupervised techniques us-": "have focused on late fusion; combining features from the dif-"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "ferent modalities after a few neural network layers [17].\n[10]"
        },
        {
          "While models like BERT use unsupervised techniques us-": "employs a feature fusion at the frame level using a gated convo-"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "lutional architecture. More recently, cross-modal\ntransformers"
        },
        {
          "While models like BERT use unsupervised techniques us-": "have been employed in order\nto project one modality into an-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "other [18], which have been successfully applied to sentiment"
        },
        {
          "While models like BERT use unsupervised techniques us-": "analysis [18] and speech recognition [19].\n[20] employs a neu-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "ral network to extract multi-modal embeddings from visual fea-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "tures by applying a loss function that forces the embedding to"
        },
        {
          "While models like BERT use unsupervised techniques us-": "be similar to the GloVe embedding of the entity in the image."
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "In this paper, we extend the prior work in three main ways."
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "Firstly, we combine the audio, visual and text modalities\nto-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "gether\nin order\nto learn tri-modal\nrepresentations\nfrom large"
        },
        {
          "While models like BERT use unsupervised techniques us-": "datasets. Additionally,\ninstead of using a single task for train-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "ing the model, we use a multi-task training architecture to make"
        },
        {
          "While models like BERT use unsupervised techniques us-": "the embeddings more general\nin nature.\nFinally, we employ"
        },
        {
          "While models like BERT use unsupervised techniques us-": "the cross-modal\ntransformer to train our model. Transformers"
        },
        {
          "While models like BERT use unsupervised techniques us-": "lend themselves naturally to multi-modal sequences of differ-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "ent lengths as they allow for an easy combination of modalities"
        },
        {
          "While models like BERT use unsupervised techniques us-": "without the need to speciﬁcally align them as shown in [18]."
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "3. Multitask training for learning"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "embeddings"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "We use two tasks for training our model; character level speech"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "recognition and person identiﬁcation. ASR is a natural equiva-"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "lent of the language modeling task for the audio domain, and the"
        },
        {
          "While models like BERT use unsupervised techniques us-": ""
        },
        {
          "While models like BERT use unsupervised techniques us-": "language modeling task has been applied with success for learn-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "ing generalized embeddings.\nThe choice of\nthe person iden-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "tiﬁcation task is mainly due to the availability of open source"
        },
        {
          "While models like BERT use unsupervised techniques us-": "datasets with labels for\nthe task. We employ the architecture"
        },
        {
          "While models like BERT use unsupervised techniques us-": "proposed in [19] in order to project\nthe features in the text and"
        },
        {
          "While models like BERT use unsupervised techniques us-": "visual domain to the audio domain shown in Figure 1. This is an"
        },
        {
          "While models like BERT use unsupervised techniques us-": "alternative to the traditional early and late fusion techniques for"
        },
        {
          "While models like BERT use unsupervised techniques us-": "combining features from different modalities;\nthe cross-modal"
        },
        {
          "While models like BERT use unsupervised techniques us-": "attention module computes the attention weights between fea-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "tures from two modalities, and weighs the features that are cor-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "related higher,\nthus paying more attention to the features that"
        },
        {
          "While models like BERT use unsupervised techniques us-": "are more important\nfor\nthe task. There is a joint encoder\nthat"
        },
        {
          "While models like BERT use unsupervised techniques us-": "comprises of an audio only encoder, and 2 cross-modal\ntrans-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "former components which project the video and text features to"
        },
        {
          "While models like BERT use unsupervised techniques us-": "the audio space. The ﬁnal sequential output from the encoder is"
        },
        {
          "While models like BERT use unsupervised techniques us-": "a simple weighted sum of the various modalities. For the ASR"
        },
        {
          "While models like BERT use unsupervised techniques us-": "decoder, we use the Transformer decoder, whereas for the per-"
        },
        {
          "While models like BERT use unsupervised techniques us-": "son identiﬁcation task we average the embeddings over a given"
        },
        {
          "While models like BERT use unsupervised techniques us-": "sequence and use an afﬁne transformation layer for classifying"
        },
        {
          "While models like BERT use unsupervised techniques us-": "the speaker. The ﬁnal\nloss function is a weighted sum of\nthe"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Baseline results on the CMU-MOSEI dataset. TF is the transformer-based model": ""
        },
        {
          "Table 1: Baseline results on the CMU-MOSEI dataset. TF is the transformer-based model": "WA"
        },
        {
          "Table 1: Baseline results on the CMU-MOSEI dataset. TF is the transformer-based model": "67.0"
        },
        {
          "Table 1: Baseline results on the CMU-MOSEI dataset. TF is the transformer-based model": "66.3"
        },
        {
          "Table 1: Baseline results on the CMU-MOSEI dataset. TF is the transformer-based model": "53.6"
        },
        {
          "Table 1: Baseline results on the CMU-MOSEI dataset. TF is the transformer-based model": "66.2"
        },
        {
          "Table 1: Baseline results on the CMU-MOSEI dataset. TF is the transformer-based model": "67.8"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "67.7\n65.4\nEmbeddings from the Multi-task model\n67.5\n71.3",
          "66.8": "67.0",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "75.8\n63.9\n74.9\n83.4\n86.9\n67.1\n78.8\n87.8\n64.2"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "4. Experimental setup",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "multiple emotions.\nThe labels for each class are on a Likert"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "scale of [0, 3]; we binarize the problem by giving a segment a"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "4.1. Datasets and training setup",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "label of 1 if\nit has a Likert score greater\nthan 0.\nThe classes"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "thus obtained represent presence versus no presence of a spe-"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "For training the multi-task transformer models, we use the Vox-",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "ciﬁc emotion.\nFor\nthe baseline model, we tried two different"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "Celeb2 dataset [22]. This train partition of the dataset has 1.1",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "approaches to get the best possible baseline. The ﬁrst model is"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "million examples.\nSince there are no transcriptions available",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "a simple late fusion model trained with the same feature inputs"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "with this dataset, we use a TDNN ASR model\ntrained with",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "as the transformer model. We use a 2-layer bidirectional-GRU"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "Kaldi on Librispeech dataset\nin order to extract\ntext\nlabels for",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "model with 200 nodes for each modality. The output from the"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "this dataset [23]. We use the standard recipe available in Kaldi",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "last\ntime step from each modality is then concatenated and a"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "in order\nto train the models. We extract 40-dimensional Log-",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "linear layer of size 100 is used for classiﬁcation. We train a 6-"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "Filter bank energy (LFBE) features using a 10ms frame rate to",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "class classiﬁer using binary cross entropy loss over each class"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "represent\nthe audio, 4096-dimensional features extracted from",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "and use weighted training in order to compensate for the imbal-"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "the VGG-16 model\nto represent\nthe visual modality and 300-",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "anced nature of the dataset. The second architecture we tried is"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "dimensional GloVe vector\nto represent\nthe text modality. The",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "the same cross-modal architecture as show in Figure 1, where"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "model\nis trained using pytorch with the learning schedule de-",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "the output of\nthe encoder\nlayer goes to a linear classiﬁcation"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "scribed in [24] We stack 5-frames of\nthe LFBE features for a",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "layer. To train the transformer model, we use equal weights on"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "ﬁnal feature audio dimensionality of 200. The ﬁrst operation in",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "all modalities. We experimented with learned weights but\nthe"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "the transformer encoder is a dot-product attention layer, which",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "ﬁnal weights learned by the model were not very different from"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "computes the attention weights between the query Q and the",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "equal weights so we choose to use this as a ﬁxed parameter."
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "key K as softmax(QK T /(cid:112)(d)), where d is the dimensional-",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "In order to leverage the learned embeddings from the Vox-"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "ity of K (or Q), and they are matrices of dimension n×d where",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "Celeb dataset, we use the model\ntrained with multi-task train-"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "n is the sequence length. By stacking the features to reduce the",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "ing as a pre-trained model and tune it with the CMU-MOSEI"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "sequence length, we reduce the memory required to compute",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "dataset on the emotion recognition task. We chose to tune the"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "the attention maps. Since the Voxceleb2 dataset consists of data",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "embeddings and not ﬁx them based on prior research from the"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "from multiple languages, we ﬁrst ﬁlter out\nthe data based on",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "natural\nlanguage domain where researchers have demonstrated"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "the likelihood from the ASR decoding to pick only the training",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "that\ntuning the word2vec embeddings for the language model-"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "examples in English. For all our experiments, we use only the",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "ing task can improve performance. [25]."
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "dev portion of the Voxceleb2 dataset. After ﬁltering, we have 1",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "million utterances for training with 5994 unique speakers. We",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "5. Results"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "use the architecture described in Section 3 to train the model.",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "For all our evaluations, we report\nthe weighted accuracy (WA)"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "The model has a dimensionality of 512 with 4 encoder\nlayers",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "and F1-score for each emotion and the average metrics over the"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "for audio and cross-modal encoders, with 2 decoder layers and",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "6 emotions. The threshold for the both metrics was optimized"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "4 attention heads, and a feed forward layer of dimension 200 for",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "separately on the development set. For all\nthe reported results"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "the transformer. The cross-modal encoder has 4 encoder layers",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "on the emotion recognition task,\nthe model is randomly initial-"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "with 4 attention heads. This model architecture was not opti-",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "ized and trained 10 different\ntimes. The best model\nis chosen"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "mized. We evaluate our embeddings on the downstream task",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "based on the average of\nthe weighted accuracy and F1-scores"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "of emotion recognition. We choose the CMU-MOSEI dataset",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "over the dev set over the 10 runs, as suggested in [10]."
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "for\nthis purpose since it has all\nthe modalities available [21].",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "The dataset consists of 6 emotions; happy, sad, angry, disgust,",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "5.1. Baseline results"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "surprise and fear.\nIt contains 23,453 single-speaker video seg-",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": ""
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "ments from YouTube which have been manually transcribed and",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "Table 1 shows the results of the two baseline models. We have"
        },
        {
          "67.8\n72.9\nBaseline\n67.5\n63.9": "annotated for sentiment and emotion. Each segment can have",
          "66.8": "",
          "88.0\n64.4\n74.5\n60.6\n74.8\n82.3\n86.1\n66.4\n78.5": "also included results from other publications in order\nto com-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Baseline\n57.2": "Embeddings from the Multi-task model\n57.6",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "75.1\n60.9\n76.4\n66.2\n78.4\n67.1\n78.8"
        },
        {
          "Baseline\n57.2": "pare our metrics. From [15], we use the multi-task model since",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "run the ablation study, we just use a 0 weight on the missing"
        },
        {
          "Baseline\n57.2": "it demonstrated the best\nresults.\nAs\nthe results demonstrate,",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "modality during the weighted combination of the outputs of the"
        },
        {
          "Baseline\n57.2": "the transformer-based baseline outperforms both the reported",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "audio encoder and the cross-modal transformers in the model to"
        },
        {
          "Baseline\n57.2": "results in literature as well as the late fusion baseline model that",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "compute the ﬁnal embedding."
        },
        {
          "Baseline\n57.2": "we trained for most of the emotion categories in the dataset on",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "Table 3 show the results with different combination of the"
        },
        {
          "Baseline\n57.2": "majority of\nthe emotions, as well as on the average weighted",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "modalities. The results show that in the case of missing modal-"
        },
        {
          "Baseline\n57.2": "accuracy over the emotions. This result is not unexpected, since",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "ities, the embeddings improve over the baseline even more than"
        },
        {
          "Baseline\n57.2": "Tseng et al. already demonstrated in [18] that\nthis architecture",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "when all\nthe modalities\nare present.\nWhen both the video"
        },
        {
          "Baseline\n57.2": "outperforms the late fusion architecture for\nthe MOSEI senti-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "and text modalities are absent,\nthe model performs the worst."
        },
        {
          "Baseline\n57.2": "ment\ntask. They however did not report results on the emotion",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "With text and audio input,\nthe performance of the baseline and"
        },
        {
          "Baseline\n57.2": "task for the dataset. For the remainder of the paper, our baseline",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "embeddings-based model\nis close to the performance with no"
        },
        {
          "Baseline\n57.2": "will refer to the transformer-based baseline in this table. Note",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "ablation. Since the GloVe embeddings, which are already pre-"
        },
        {
          "Baseline\n57.2": "that the external publications used a different set of features for",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "trained to represent semantic meaning with over 6 billion tokens"
        },
        {
          "Baseline\n57.2": "the audio and visual modalities training their models, which is",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "[7],\nthey already provide meaningful features to the system in"
        },
        {
          "Baseline\n57.2": "why we present the late fusion results with our feature set.",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "order to get good performance. Prior work [18, 27] has shown"
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "that for uni-modal emotion recognition, text based models out-"
        },
        {
          "Baseline\n57.2": "5.2. Results with embeddings",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "performs audio and visual based models, and our ablation re-"
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "sults demonstrate the same; adding text to the input audio boosts"
        },
        {
          "Baseline\n57.2": "Table 2 shows the results of the experiment where we use the",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "the weighted average by 8.6% absolute as compared to adding"
        },
        {
          "Baseline\n57.2": "model trained with multi-task training on the VoxCeleb2 dataset",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "visual information that improves it by 3.3%."
        },
        {
          "Baseline\n57.2": "to initialize the transformer model for the emotion recognition",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "task. The model outperforms the baseline by more than 1% ab-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "solute for the weighted accuracy for the sad and surprise emo-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "6. Conclusions and Future work"
        },
        {
          "Baseline\n57.2": "tions, and F1-score for the anger emotion. We observe a degra-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "In this paper, we present state of\nthe art\nresults on the CMU-"
        },
        {
          "Baseline\n57.2": "dation in the weighted accuracy of the fear emotion. The rest",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "MOSEI emotion recognition task using a cross-modal attention"
        },
        {
          "Baseline\n57.2": "of the metrics are comparable to the baseline model. The im-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "based transformer architecture. We demonstrate that by using"
        },
        {
          "Baseline\n57.2": "provements on the weighted accuracy of\nthe surprise and sad",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "a multi-task architecture, we can leverage a large multi-modal"
        },
        {
          "Baseline\n57.2": "emotions are signiﬁcant as\nthe 95% conﬁdence intervals are",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "dataset like VoxCeleb in order to learn general embeddings that"
        },
        {
          "Baseline\n57.2": "non-overlapping.",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "can improve the performance on a downstream task like emo-"
        },
        {
          "Baseline\n57.2": "To analyze the results obtained from the embeddings, we",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "tion recognition."
        },
        {
          "Baseline\n57.2": "look at\nthe distribution of emotions in the VoxCeleb2 dataset.",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "Our future work will address one of the major drawbacks of"
        },
        {
          "Baseline\n57.2": "The distribution on the development set\nis not available, how-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "our architecture in its dependency on audio which doesn’t allow"
        },
        {
          "Baseline\n57.2": "ever\nthe authors of\nthe dataset have created a subset of\nthe",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "for missing audio during inference. In addition, for text input to"
        },
        {
          "Baseline\n57.2": "dataset called EmoVoxceleb [26]. This dataset\nis a subset con-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "the system, we require either a human or a machine based tran-"
        },
        {
          "Baseline\n57.2": "taining 100k videos from the Voxceleb2 dataset. Since the au-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "scription. Prior work, however, has demonstrated that we can"
        },
        {
          "Baseline\n57.2": "thors haven’t mentioned a sampling strategy, we assume that",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "learn embeddings from audio that capture the semantic meaning"
        },
        {
          "Baseline\n57.2": "this data was randomly sampled. In [26], Albanie et al. present",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "[13]. By incorporating this in the model, we should be able to"
        },
        {
          "Baseline\n57.2": "a frame level distribution of the emotions in the dataset. Neu-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "eliminate the need for the text input altogether. Both the CMU-"
        },
        {
          "Baseline\n57.2": "tral, happy, surprise and sadness are the dominant emotions in",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "MOSEI and Voxceleb2 datasets are obtained from YouTube but"
        },
        {
          "Baseline\n57.2": "the dataset, whereas fear and disgust are the least frequent. Un-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "there could be some mismatch due to the way the videos were"
        },
        {
          "Baseline\n57.2": "der the assumption that the Voxceleb2 dataset has a similar dis-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "selected. This could be bridged by adapting the embeddings on"
        },
        {
          "Baseline\n57.2": "tribution of emotions, we posit that the improvements on the sad",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "the CMU-MOSEI dataset for improved performance. We would"
        },
        {
          "Baseline\n57.2": "and surprise emotions and degradation in fear emotion can be",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "like to increase the number of tasks in our multi-task architec-"
        },
        {
          "Baseline\n57.2": "attributed to this distribution.",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "ture in order to make the representations even more generalized."
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "Speciﬁcally, experimenting with visual tasks like landmark de-"
        },
        {
          "Baseline\n57.2": "5.3. Ablation studies",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": ""
        },
        {
          "Baseline\n57.2": "",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "tection which would provide useful\ninformation for\ntasks\nin-"
        },
        {
          "Baseline\n57.2": "As mentioned in the introduction, for practical applications we",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "volving affect recognition. An additional future direction is to"
        },
        {
          "Baseline\n57.2": "would like the understand the impact of missing input modali-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "try unsupervised methods like skip-thought vector or other self-"
        },
        {
          "Baseline\n57.2": "ties. To understand this,\nthe ﬁrst experiment we perform is to",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "supervised training methods. Another\ninteresting approach to"
        },
        {
          "Baseline\n57.2": "remove the video and/or text inputs to both the baseline as well",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "consider is to combine the multi-task learning with weak super-"
        },
        {
          "Baseline\n57.2": "as the model pre-trained model during inference only. Since in",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "vision as suggested in [11] for each of the tasks in question in"
        },
        {
          "Baseline\n57.2": "our architecture we use the cross-modal attention to transform",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "order to make the embeddings from discriminative. Finally, we"
        },
        {
          "Baseline\n57.2": "the video and text modalities to the audio domain,\nthe archi-",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "will evaluate our technique on more downstream tasks to study"
        },
        {
          "Baseline\n57.2": "tecture doesn’t allow for a missing audio modality.\nIn order to",
          "74.4\n60.3\n76.3\n65.8\n77.8\n66.5\n78.5": "what tasks the representations generalize well to."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4299.": "[9] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning"
        },
        {
          "4299.": "for image recognition,” in Proceedings of the IEEE conference on"
        },
        {
          "4299.": "computer vision and pattern recognition, 2016, pp. 770–778."
        },
        {
          "4299.": "[10]\nS.-Y. Tseng, P. Georgiou, and S. Narayanan, “Multimodal embed-"
        },
        {
          "4299.": "dings from language models,” arXiv preprint arXiv:1909.04302,"
        },
        {
          "4299.": "2019."
        },
        {
          "4299.": "[11]\nS.-Y. Tseng, B. Baucom,\nand P. Georgiou,\n“Unsupervised on-"
        },
        {
          "4299.": "line multitask learning of behavioral sentence embeddings,” PeerJ"
        },
        {
          "4299.": "Computer Science, vol. 5, p. e200, 2019."
        },
        {
          "4299.": "[12]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-"
        },
        {
          "4299.": "training of deep bidirectional\ntransformers\nfor\nlanguage under-"
        },
        {
          "4299.": "standing,” arXiv preprint arXiv:1810.04805, 2018."
        },
        {
          "4299.": "[13] Y.-A. Chung and J. Glass, “Speech2vec: A sequence-to-sequence"
        },
        {
          "4299.": "framework for learning word embeddings from speech,” Proc. In-"
        },
        {
          "4299.": "terspeech 2018, pp. 811–815, 2018."
        },
        {
          "4299.": "[14] M. El-Geish,\n“Learning\njoint\nacoustic-phonetic word\nembed-"
        },
        {
          "4299.": "dings,” arXiv preprint arXiv:1908.00493, 2019."
        },
        {
          "4299.": "[15] M. S. Akhtar, D. S. Chauhan, D. Ghosal,\nS. Poria, A. Ek-"
        },
        {
          "4299.": "bal, and P. Bhattacharyya, “Multi-task learning for multi-modal"
        },
        {
          "4299.": "arXiv\npreprint\nemotion\nrecognition\nand\nsentiment\nanalysis,”"
        },
        {
          "4299.": "arXiv:1905.05812, 2019."
        },
        {
          "4299.": "[16] M.-T. Luong, Q. V. Le,\nI. Sutskever, O. Vinyals, and L. Kaiser,"
        },
        {
          "4299.": "arXiv\npreprint\n“Multi-task\nsequence\nto\nsequence\nlearning,”"
        },
        {
          "4299.": "arXiv:1511.06114, 2015."
        },
        {
          "4299.": "[17]\nP. Tzirakis, G. Trigeorgis, M. A. Nicolaou, B. W. Schuller, and"
        },
        {
          "4299.": "S. Zafeiriou, “End-to-end multimodal emotion recognition using"
        },
        {
          "4299.": "deep neural networks,” IEEE Journal of Selected Topics in Signal"
        },
        {
          "4299.": "Processing, vol. 11, no. 8, pp. 1301–1309, 2017."
        },
        {
          "4299.": "[18] Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and"
        },
        {
          "4299.": "R. Salakhutdinov, “Multimodal\ntransformer for unaligned multi-"
        },
        {
          "4299.": "modal language sequences,” pp. 6558–6569, 2019."
        },
        {
          "4299.": "[19] G. Paraskevopoulos, S. Parthasarathy, A. Khare, and S. Sundaram,"
        },
        {
          "4299.": "“Multimodal and multiresolution speech recognition with trans-"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "sentations as multimodal embeddings,” in Thirty-First AAAI Con-"
        },
        {
          "7. References": "[1]\nJ. A. DeVito, S. O’Rourke, and L. O’Neill, Human communica-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "ference on Artiﬁcial Intelligence, 2017."
        },
        {
          "7. References": "tion.\nLongman, 2000.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "[21]\nP. Liang, R. Salakhutdinov, and L.-P. Morency, “Computational"
        },
        {
          "7. References": "[2]\nT. Afouras,\nJ. S. Chung, A. Senior, O. Vinyals, and A. Zisser-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "modeling of human multimodal language: The mosei dataset and"
        },
        {
          "7. References": "man, “Deep audio-visual speech recognition,” IEEE transactions",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "interpretable dynamic fusion,” 2018."
        },
        {
          "7. References": "on pattern analysis and machine intelligence, 2018.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "[22]\nJ. S. Chung, A. Nagrani, and A. Zisserman, “Voxceleb2: Deep"
        },
        {
          "7. References": "[3]\nS. Dupont and J. Luettin, “Audio-visual speech modeling for con-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "speaker recognition,” in INTERSPEECH, 2018."
        },
        {
          "7. References": "tinuous\nspeech recognition,” IEEE transactions on multimedia,",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "vol. 2, no. 3, pp. 141–151, 2000.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "[23] V. Peddinti, D. Povey, and S. Khudanpur, “A time delay neural"
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "network architecture for efﬁcient modeling of long temporal con-"
        },
        {
          "7. References": "[4] B. Schuller, M. Valstar, F. Eyben, G. McKeown, R. Cowie, and",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "texts,” in Sixteenth Annual Conference of the International Speech"
        },
        {
          "7. References": "M. Pantic, “Avec 2011–the ﬁrst international audio/visual emotion",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "Communication Association, 2015."
        },
        {
          "7. References": "challenge,” in International Conference on Affective Computing",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "and Intelligent Interaction.\nSpringer, 2011, pp. 415–424.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "[24]\nL. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence"
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "sequence-to-sequence model\nfor\nspeech recognition,”\nin 2018"
        },
        {
          "7. References": "[5]\nS. O. Sadjadi, C. S. Greenberg, E. Singer, D. A. Reynolds,",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "7. References": "L. Mason, and J. Hernandez-Cordero, “The 2019 nist audio-visual",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "Processing (ICASSP).\nIEEE, 2018, pp. 5884–5888."
        },
        {
          "7. References": "speaker recognition evaluation,” Proc. Speaker Odyssey (submit-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "[25] K. Komiya and H. Shinnou, “Investigating effective parameters"
        },
        {
          "7. References": "ted), Tokyo, Japan, 2020.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "for ﬁne-tuning of word embeddings using only a small corpus,” in"
        },
        {
          "7. References": "[6] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "Proceedings of\nthe Workshop on Deep Learning Approaches for"
        },
        {
          "7. References": "hierarchies for accurate object detection and semantic segmenta-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "Low-Resource NLP, 2018, pp. 60–67."
        },
        {
          "7. References": "the IEEE conference on computer vision\ntion,” in Proceedings of",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "[26]\nS. Albanie, A. Nagrani, A. Vedaldi, and A. Zisserman, “Emotion"
        },
        {
          "7. References": "and pattern recognition, 2014, pp. 580–587.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "recognition in speech using cross-modal\ntransfer in the wild,” in"
        },
        {
          "7. References": "[7]\nJ. Pennington, R. Socher, and C. D. Manning, “Glove: Global",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "ACM Multimedia, 2018."
        },
        {
          "7. References": "vectors for word representation,” in Empirical Methods in Natural",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "[27] N. Majumder, D. Hazarika, A. Gelbukh, E. Cambria, and S. Po-"
        },
        {
          "7. References": "Language Processing (EMNLP), 2014, pp. 1532–1543. [Online].",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "ria, “Multimodal sentiment analysis using hierarchical fusion with"
        },
        {
          "7. References": "Available: http://www.aclweb.org/anthology/D14-1162",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "context modeling,” Knowledge-based systems, vol. 161, pp. 124–"
        },
        {
          "7. References": "[8] D. Palaz, M. M. Doss, and R. Collobert, “Convolutional neural",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": "133, 2018."
        },
        {
          "7. References": "networks-based continuous speech recognition using raw speech",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "signal,”\nin 2015 IEEE International Conference on Acoustics,",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "Speech and Signal Processing (ICASSP).\nIEEE, 2015, pp. 4295–",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "4299.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[9] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual\nlearning",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "for image recognition,” in Proceedings of the IEEE conference on",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "computer vision and pattern recognition, 2016, pp. 770–778.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[10]\nS.-Y. Tseng, P. Georgiou, and S. Narayanan, “Multimodal embed-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "dings from language models,” arXiv preprint arXiv:1909.04302,",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "2019.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[11]\nS.-Y. Tseng, B. Baucom,\nand P. Georgiou,\n“Unsupervised on-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "line multitask learning of behavioral sentence embeddings,” PeerJ",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "Computer Science, vol. 5, p. e200, 2019.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[12]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "training of deep bidirectional\ntransformers\nfor\nlanguage under-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "standing,” arXiv preprint arXiv:1810.04805, 2018.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[13] Y.-A. Chung and J. Glass, “Speech2vec: A sequence-to-sequence",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "framework for learning word embeddings from speech,” Proc. In-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "terspeech 2018, pp. 811–815, 2018.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[14] M. El-Geish,\n“Learning\njoint\nacoustic-phonetic word\nembed-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "dings,” arXiv preprint arXiv:1908.00493, 2019.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[15] M. S. Akhtar, D. S. Chauhan, D. Ghosal,\nS. Poria, A. Ek-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "bal, and P. Bhattacharyya, “Multi-task learning for multi-modal",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "arXiv\npreprint\nemotion\nrecognition\nand\nsentiment\nanalysis,”",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "arXiv:1905.05812, 2019.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[16] M.-T. Luong, Q. V. Le,\nI. Sutskever, O. Vinyals, and L. Kaiser,",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "arXiv\npreprint\n“Multi-task\nsequence\nto\nsequence\nlearning,”",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "arXiv:1511.06114, 2015.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[17]\nP. Tzirakis, G. Trigeorgis, M. A. Nicolaou, B. W. Schuller, and",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "S. Zafeiriou, “End-to-end multimodal emotion recognition using",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "deep neural networks,” IEEE Journal of Selected Topics in Signal",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "Processing, vol. 11, no. 8, pp. 1301–1309, 2017.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[18] Y.-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency, and",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "R. Salakhutdinov, “Multimodal\ntransformer for unaligned multi-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "modal language sequences,” pp. 6558–6569, 2019.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "[19] G. Paraskevopoulos, S. Parthasarathy, A. Khare, and S. Sundaram,",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "“Multimodal and multiresolution speech recognition with trans-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "formers,” in Proceedings of the 58th Annual Meeting of the Asso-",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        },
        {
          "7. References": "ciation for Computational Linguistics, 2020, pp. 2381–2387.",
          "[20] G. Collell, T. Zhang, and M.-F. Moens, “Imagined visual repre-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Human communication",
      "authors": [
        "J Devito",
        "S O'rourke",
        "L O'neill"
      ],
      "year": "2000",
      "venue": "Human communication"
    },
    {
      "citation_id": "3",
      "title": "Deep audio-visual speech recognition",
      "authors": [
        "T Afouras",
        "J Chung",
        "A Senior",
        "O Vinyals",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "4",
      "title": "Audio-visual speech modeling for continuous speech recognition",
      "authors": [
        "S Dupont",
        "J Luettin"
      ],
      "year": "2000",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "5",
      "title": "Avec 2011-the first international audio/visual emotion challenge",
      "authors": [
        "B Schuller",
        "M Valstar",
        "F Eyben",
        "G Mckeown",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "6",
      "title": "The 2019 nist audio-visual speaker recognition evaluation",
      "authors": [
        "S Sadjadi",
        "C Greenberg",
        "E Singer",
        "D Reynolds",
        "L Mason",
        "J Hernandez-Cordero"
      ],
      "year": "2020",
      "venue": "Proc. Speaker Odyssey (submitted)"
    },
    {
      "citation_id": "7",
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "authors": [
        "R Girshick",
        "J Donahue",
        "T Darrell",
        "J Malik"
      ],
      "year": "2014",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "8",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Convolutional neural networks-based continuous speech recognition using raw speech signal",
      "authors": [
        "D Palaz",
        "M Doss",
        "R Collobert"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Multimodal embeddings from language models",
      "authors": [
        "S.-Y Tseng",
        "P Georgiou",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Multimodal embeddings from language models",
      "arxiv": "arXiv:1909.04302"
    },
    {
      "citation_id": "12",
      "title": "Unsupervised online multitask learning of behavioral sentence embeddings",
      "authors": [
        "S.-Y Tseng",
        "B Baucom",
        "P Georgiou"
      ],
      "year": "2019",
      "venue": "PeerJ Computer Science"
    },
    {
      "citation_id": "13",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "14",
      "title": "Speech2vec: A sequence-to-sequence framework for learning word embeddings from speech",
      "authors": [
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Learning joint acoustic-phonetic word embeddings",
      "authors": [
        "M El-Geish"
      ],
      "year": "2019",
      "venue": "Learning joint acoustic-phonetic word embeddings",
      "arxiv": "arXiv:1908.00493"
    },
    {
      "citation_id": "16",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "M Akhtar",
        "D Chauhan",
        "D Ghosal",
        "S Poria",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "arxiv": "arXiv:1905.05812"
    },
    {
      "citation_id": "17",
      "title": "Multi-task sequence to sequence learning",
      "authors": [
        "M.-T Luong",
        "Q Le",
        "I Sutskever",
        "O Vinyals",
        "L Kaiser"
      ],
      "year": "2015",
      "venue": "Multi-task sequence to sequence learning",
      "arxiv": "arXiv:1511.06114"
    },
    {
      "citation_id": "18",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "G Trigeorgis",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Multimodal transformer for unaligned multimodal language sequences"
    },
    {
      "citation_id": "20",
      "title": "Multimodal and multiresolution speech recognition with transformers",
      "authors": [
        "G Paraskevopoulos",
        "S Parthasarathy",
        "A Khare",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Imagined visual representations as multimodal embeddings",
      "authors": [
        "G Collell",
        "T Zhang",
        "M.-F Moens"
      ],
      "year": "2017",
      "venue": "Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion",
      "authors": [
        "P Liang",
        "R Salakhutdinov",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Computational modeling of human multimodal language: The mosei dataset and interpretable dynamic fusion"
    },
    {
      "citation_id": "23",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition"
    },
    {
      "citation_id": "24",
      "title": "A time delay neural network architecture for efficient modeling of long temporal contexts",
      "authors": [
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "25",
      "title": "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition",
      "authors": [
        "L Dong",
        "S Xu",
        "B Xu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Investigating effective parameters for fine-tuning of word embeddings using only a small corpus",
      "authors": [
        "K Komiya",
        "H Shinnou"
      ],
      "year": "2018",
      "venue": "Proceedings of the Workshop on Deep Learning Approaches for Low-Resource NLP"
    },
    {
      "citation_id": "27",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "28",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-based systems"
    }
  ]
}