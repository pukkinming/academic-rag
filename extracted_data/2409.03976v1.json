{
  "paper_id": "2409.03976v1",
  "title": "Decan: A Denoising Encoder Via Contrastive Alignment Network For Dry Electrode Eeg Emotion Recognition",
  "published": "2024-09-06T01:54:29Z",
  "authors": [
    "Meihong Zhang",
    "Shaokai Zhao",
    "Shuai Wang",
    "Zhiguo Luo",
    "Liang Xie",
    "Tiejun Liu",
    "Dezhong Yao",
    "Ye Yan",
    "Erwei Yin"
  ],
  "keywords": [
    "Emotion recognition",
    "EEG",
    "dry electrode",
    "contrastive learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG signal is important for brain-computer interfaces (BCI). Nevertheless, existing dry and wet electrodes are difficult to balance between high signal-to-noise ratio and portability in EEG recording, which limits the practical use of BCI. In this study, we propose a Denoising Encoder via Contrastive Alignment Network (DECAN) for dry electrode EEG, under the assumption of the EEG representation consistency between wet and dry electrodes during the same task. Specifically, DECAN employs two parameter-sharing deep neural networks to extract task-relevant representations of dry and wet electrode signals, and then integrates a representation-consistent contrastive loss to minimize the distance between representations from the same timestamp and category but different devices. To assess the feasibility of our approach, we construct an emotion dataset consisting of paired dry and wet electrode EEG signals from 16 subjects with 5 emotions, named PaDWEED. Results on PaDWEED show that DECAN achieves an average accuracy increase of 6.94% comparing to state-of-the art performance in emotion recognition of dry electrodes. Ablation studies demonstrate a decrease in inter-class aliasing along with noteworthy accuracy enhancements in the delta and beta frequency bands. Moreover, an inter-subject feature alignment can obtain an accuracy improvement of 5.99% and 5.14% in intra-and inter-dataset scenarios, respectively. Our proposed method may open up new avenues for BCI with dry electrodes. PaDWEED dataset used in this study is freely available at https://huggingface.co/datasets/peiyu999/PaDWEED.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Eeg Emotion Recognition Based On Dry Electrode",
      "text": "The portability of dry electrode EEG system enables the potential for brain-computer emotion recognition applications in daily life. However, its low signal-to-noise ratio results in poor system performance. Currently, two mainstream solutions have been proposed to mitigate this issue. From a hardware perspective, enhancements are made to the dry electrode EEG acquisition system. This includes optimizing the electrode structure and replacing the electrode material to reduce contact impedance, thereby enhancing the signal-to-noise ratio of the acquired signal. So far, there are several mature commercial dry electrode devices available for EEG data acquisition in emotion recognition, such as the DSI-24, OpenBCI, EMOTIV EPOC and so on. In another study, a four-channel textile cap was designed with dry electrodes secured by an ultra-soft gel holder while introducing stylish and ergonomic design features to enhance wearability and comfort. The average binary emotion classification accuracy was found to be 81.32% among five healthy elderly participants  [17] .\n\nFrom a software perspective, the performance of the dry electrode EEG system has been enhanced by optimizing the recognition algorithm, including feature extraction, classifier design. Lakhan et al. recruited 43 subjects to watch prelabeled emotional visual stimuli, while using OpenBCI and Empatica4 to collect EEG and peripheral physiological signals. Through a classification algorithm based on K-means clustering, they achieved accuracy rates of 70% for arousal and 67% for valence  [18] . Katsigiannis et al. simultaneously collected EEG and Electrocardiogram (ECG) data from subjects while they were viewing movie clips. When using unimodal features, the SVM-RBF classification method achieved higher accuracy rates for valence, arousal, and dominance using EEG features compared to ECG features. The accuracy rates for valence, arousal, and dominance were 62.49%, 62.17%, and 61.84% respectively  [19] . Javaid et al. employed the SVM-RBF algorithm to classify EEG data collected using OpenBCI, obtained an accuracy of 87.62% for arousal and 83.28% for valence  [20] . Lan et al. collected EEG signals from individuals with major depressive disorder and healthy control subjects. They utilized the topological information among EEG channels for emotion recognition and depression detection. The study evaluated the promising ability of the emotional EEG patterns in distinguishing individuals with depression from the healthy control group  [16] . Xu et al. put forward a novel framework for emotion recognition in VR emotional scenes using EEG signals. They employed feature extraction techniques in the time domain, frequency domain, and spatial domain from the EEG data. The extracted features were then utilized to train an ensemble model using the Model Stacking approach, combining gradient boosting decision tree, random forest, and SVM models. The resulting average accuracy achieved for classifying positive and negative emotions was approximately 81.30%  [21] . However, given that research on dry electrode emotion recognition is still in its nascent stages, the methods adopted in the aforementioned studies primarily rely on traditional signal processing and machine learning algorithms. The extracted EEG features are relatively shallow, and primarily focusing on the recognition of emotion dimension models. In contrast, this study focuses on dry electrode EEG discrete emotion recognition, employing deep learning methods to leverage wet electrode signals with relatively higher signal-to-noise ratio to enhance the performance of dry electrode EEG emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Available Databases For Eeg-Based Emotion Recognition",
      "text": "There has been a consecutive release of emotion databases that containing EEG, within its range this decade, which each of them serves to distinctive experimental motivation  [22] . Specific information of these databases has been reported in Table  I . As an early released multimodal public dataset containing both physiological responses and facial expressions, MAHNOB-HCI has garnered significant attention  [23] ，and numerous studies have been conducted to validate algorithm performance using this dataset  [24] -  [26] . The DEAP dataset, released during the same period, explores the possibility of classifying emotions induced by music videos, where this type of stimulus has never been explored before  [27] . Also from the perspective of selecting stimulus materials, Song et al. eliminated the impact of cultural dependence on the induction of discrete emotions through large-scale and rigorous screening of materials. Finally, they elaborately selected 28 videos as standardized elicitation samples and recorded the physiological responses of subjects while watching the above videos. signal, thereby constructing the MPED database  [28] .\n\nChanges in participants' physiological states over time inevitably lead to a decline in emotion classification performance, posing challenges for practical applications. To address this issue,  Zheng et    physiological sensors  [30] . They posited that emotions are dynamic phenomena that evolve over time in response to stimuli  [31] , thus emphasizing the examination of the temporal nature of emotional changes.\n\nTaking into consideration that emotions are highly subjective phenomena and are influenced by various factors including personality, background, and psychological factors, Subramanian et al. proposed the ASCERTAIN database, which is the first database to link personality traits and emotional states through physiological responses. The database includes bigfive personality scales, self-ratings of emotions from 58 users, as well as their physiological and facial activity data  [32] . In addition, considering the integration of affective computing with various daily applications, the DREAMER dataset has been proposed, which is a database consisting of EEG and ECG signal records collected by portable devices, aiming at identifying the affective state after each stimuli, in terms of valence, arousal, and dominance  [19] .\n\nIn this paper, to better support our research, we develop a new EEG dataset which includes Paired Dry and Wet electrode EEG Data (PaDWEED). This dataset is obtained from 16 participants who watch the same film clip stimuli in two separate experiments. In addition to EEG, physiological measurements such as ECG, electrooculogram (EOG), blood volume pulse (BVP), galvanic skin response (GSR), respiration (RSP), and skin temperature (SKT) are also recorded. To the best of our knowledge, this dataset represents a pioneering effort by offering both wet and dry electrode EEG signals collected from the same set of participants, which is valuable for understanding the underlying mechanisms and developing classification algorithms for emotion recognition using dry electrodes.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Dataset Construction",
      "text": "We build a new EEG emotion dataset with Paired Dry and Wet Electrode EEG Data (PaDWEED) which differs from existing publicly available datasets, to support our research. In our experiment, we recruit the same subjects to participant two separate sessions of both dry and wet electrode system experiments. For each subject, the order of the experiments is not predetermined and there is a minimum interval of two weeks between the two sessions, or a longer duration. A summary of the compiled data is provided in Table  II .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Ethics Statement",
      "text": "All subjects participated in the experiment on the basis of understanding the experimental procedures and equipment safety, and signed an informed consent form before the experiment. The experiment is conducted in accordance with the guidelines of the Declaration of Helsinki and is approved by the ethics committee of the University of Electronic Science and Technology of China.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Experiment Setup",
      "text": "Stimulus selection. In our emotion experiments, we select predefined Chinese movie clips as stimuli for emotion elicitation  [33] . This choice is made considering the potential influence of native cultural factors on emotion elicitation in experimental settings  [34] ,  [35] . Specifically, the 25 videos correspond to the emotions of anger, fear, sadness, happiness, neutrality, with an equal number of stimuli in each emotion category. The length of each edited clip is approximately 2.5 minutes. The selection of these video materials is based on three criteria: (a) stability of video content, (b) retention of characters in the scenes (excluding neutral materials), and (c) the absence of simultaneous presence of positive and negative emotions. Each video clip is edited to ensure consistency between the scenes and emotional content throughout the entire performance.\n\nMaterials. Two PCs are used, one for stimulus presentation, positioned approximately one meter in front of the user, and another for recording data, allowing the experimenter to verify the recorded sensor data. In the wet electrode system experiment, an ESI NeuroScan System2 with a 62-channel active AgCl electrode cap is used to collect EEG data with a sampling frequency of 1000 Hz. In the dry electrode experiment, we utilize the Dry Sensor Interface (DSI-24) and DSI-Streamer to record EEG signals at a sampling rate of 300 Hz. The sensors aree positioned according to the international 10-20 system, with the default setting of the Pz electrode as the reference. The DSI-24 EEG cap and sensor layout for 18 channels are illustrated as Fig.  1 .\n\nAlthough peripheral physiological signals are not the focus of this study, we still utilize the MP160 data acquisition system to collect ECG, GSR, PPG, RSP, and ST signals at a sampling rate of 1000 Hz, which have demonstrated promising performance in emotion estimation research, therefore may be useful for other studies. The stimulus presentation protocol is developed using MATLAB's Psychtoolbox. Synchronization In dry electrode EEG emotion recognition systems, poor recognition performance can be attributed to lower signal-to-noise ratio, as weaker and noisier signals may hinder the detection of subtle emotion patterns and affect the overall recognition performance. Suppose X w =\n\nx j wi , y j wi , ∀i ∈ [1, r] , ∀j ∈ [1, s] and\n\ndenote the set of input data and labels of wet and dry electrode signals, where i and j denote the sample and participant indices respectively. r is the number of samples belonging to each participant and s is the total number of participants. Based on previous observations indicating the presence of similar valuable information in both wet electrode and dry electrode systems, our objective is to enhance the accuracy of dry electrode EEG emotion recognition by leveraging the valuable information obtained from wet electrode EEG signals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Solution Overview",
      "text": "As illustrated in Fig.  3 , our proposed method is a contrastive learning-based architecture. Specifically, we utilize DNN models that have shown superior performance in previous analysis to learn high-level features from both wet and dry electrode EEG signals. Subsequently, we employ contrastive learning to perform pairwise representation alignment (wet versus dry). In the following subsections, we provide a detailed description of each step in our proposed method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Our Approach",
      "text": "Feature extraction. Considering the impact of the length of samples on recognition performance  [12] ,  [36] , we divide a trial into five-second non-overlapping segments to ensure a balanced number and sample length of training samples, where each segment is regarded as a model training sample. The recorded EEG signals are susceptible to contamination from noise and artifacts but not of cerebral origin. Therefore, a series of preprocessing steps are adopted in our research to improve the EEG quality. First, a bandpass filter from 1 Hz to 50 Hz is applied to the raw EEG signals acquired from both dry and wet electrodes. This filtering approach aims to preserve the frequency range of the EEG signals containing emotional information while simultaneously removing any DC offsets and high-frequency interference such as muscle activity or other artifacts. Next, a notch filter at 50 Hz is employed to eliminate the power line interference caused by AC electrical sources or poor grounding connections. Finally, the filtered signals are downsampled to 200 Hz to reduce the complexity of subsequent signal processing and accelerate the overall signal processing speed.\n\nIt is well documented that the differential entropy (DE) is of efficient EEG feature for human emotional states  [37] ,  [38] . Hence, it is employed in this research to facilitate the subsequent classification model. Specifically, the preprocessed EEG signals are separated into five frequency band: delta (1-4 Hz), theta (4-8 Hz), alpha (8-14 Hz), beta (14-31 Hz) and gamma (31-50 Hz), then the DE for Gaussian distribution can be extracted as follows:\n\nwhere X denotes the Gaussian distribution N µ, σ 2 , π and e are constants.\n\nAssuming emotional states are situated in a continuous space and gradual transition, the linear dynamical system is a popular feature smoothing technique widely employed to filter out components irrelevant to emotional states  [38] -  [40] . Given the impressive performance demonstrated by this method in various studies, we also employ it to smooth the extracted DE features.\n\nEncoders. Given a series of paired wet and dry electrode EEG signals X w and X d collected from same participant during the same stimulus segment, we use the extracted DE features from multiple frequency bands\n\nas inputs to the encoder and output high-level features\n\n. Specifically, F w and F d are extracted from the raw signal after filtering and downsampling, as described previously. Then we choose to utilize two partial weight-sharing DNN as the encoders in our approach. This particular architecture has consistently shown superior performance compared to other models in extracting paired features effectively. To promote efficient feature extraction, we have implemented weight sharing by sharing the last two linear sublayers and the projector between the two types of signals.\n\nFeature alignment. Contrastive learning is a highly effective method for self-supervised representation learning, demonstrating significant achievements in pairwise feature learning across diverse domains  [41] -  [43] . Recent advancements in neuroscience have provided inspiration, suggesting that contrastive learning can be applied to extract subtle information from signals within the central nervous system  [12] . This growing body of evidence highlights the potential of utilizing contrastive learning in neural research. Based on the aforementioned rationale, we have chosen to employ contrastive learning to accomplish the task of aligning features between wet and dry electrode EEG signals, details are introduced as follows.\n\nProjectors. Motivated by the SimCLR framework, which suggests that nonlinear projection of encoded features can yield better performance in downstream tasks compared to linear projection or no operation  [43] , we have opted to incorporate a structure comprised of fully connected layers and a ReLU layer. This configuration allows us to obtain the desired projection features, shown in Fig.  3 .\n\nThe contrastive loss. Given a paired of sequences of projection features P w = p w i , p w i+1 , ..., p w T and P d = p d i , p d i+1 , ..., p d T , where T is the number of samples, w and d denote wet and dry electrode respectively. We define samples as positive when they originate from the same subject and different EEG collection systems, while also corresponding to the same time segment of the same emotion type. Conversely, any samples that do not meet these criteria are classified as negatives.\n\nContrastive learning employs a loss function that utilizes similarity as a measure to bring positive samples closer to each other while separating negative samples. Formally, the loss function for a pair of samples p w i , p d i can be formulated as follows:\n\nwhere 1 [i̸ =j] ∈ {0, 1}, it set to 1 iff i ̸ = j. i and j are the sample indices in current batch. τ is the temperature scalar. And sim p w i , p d j represents cosine similarity computed by\n\nBy iterating over all sample time points within the batch, we can calculate the final contrastive loss,\n\nTotal loss. Due to the low signal-to-noise ratio of EEG as a physiological signal, we begin by conducting wet and dry electrode EEG emotion classification tasks. This crucial step ensures that the features extracted by the encoders from both wet and dry electrodes effectively contribute to emotion recognition. By doing so, we establish a foundation for feature alignment steps within the contrastive learning framework.\n\nTo consolidate the aforementioned information, we employ a joint training mechanism and update our final loss function as follows:\n\nwhere L W and L D are the classification losses of wet electrode and dry electrode EEG signals respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Experiment Results",
      "text": "In this section, we first present the baseline emotion recognition results derived from PaDWEED. Following this, we list the performance evaluation results of DECAN in comparison to other established solutions. Moreover, we conduct ablation experiments to analyze the influence of the contrastive learning framework on classification performance within a specific configuration and visualize the embeddings produced by our model. Lastly, we study the generalization performance of DECAN through alignment experiments across subjects and datasets (SEED V), where dry and wet electrodes come from different subjects and datasets respectively.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "PaDWEED. The PaDWEED dataset comprises two subdatasets: (1) The dry electrode sub-dataset includes 24-channel EEG data gathered from 16 subjects. (2) The wet electrode sub-dataset encompasses 64-channel EEG data obtained from the identical group of 16 subjects exposed to the same video stimuli. This paired cross-device dataset facilitates the research of feature alignment between dry and wet electrode recordings. SEED V. The SEED V dataset  [44]  comprises 62-channel wet electrode EEG recordings from 20 subjects, with each subject participating in three sessions. During each session, there are 15 trials, corresponding to 15 movie clips evenly distributed across 5 emotional states (happy, sad, disgust, fear, and neutral).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Implementation Details",
      "text": "To evaluate the performance of our emotion recognition results and establish baseline classification results for the proposed database, we perform individual participant classification using well-established approaches commonly employed in prior research on affective computing. Specifically, we employee a support vector machine (SVM) classifier with a linear kernel, logistic regression (LR), and DNN. These methods have demonstrated their effectiveness in previous studies and serve as reference models in our analysis  [38] ,  [45] -  [47] . Specifically, regarding the LR model, we utilize the default function provided by the scikit-learn module. As for the SVM classifier, we employee the function available in the scikit-learn module with a linear kernel. To determine the optimal misclassification cost parameter C for the linear SVM, we conduct a grid search over the range of [2 -10 , 2 -9 , ..., 2 10 ] and [0.1, 20], using a step size of 0.5 for the large-step and small-step scenarios, respectively. In the DNN model utilized in this paper, we integrate three hidden layers with 128, 64, and 32 hidden units for the wet electrode system, whereas the dry electrode system utilized only the last two hidden layers. The output layer comprised five units, each corresponding to one of the five emotions mentioned above.\n\nIn our quest to optimize the model, we empirically adjust to specific hyperparameters based on preliminary results. To enhance the training process, we employ the RMSprop optimizer with the learning rate selected from [1 -4 , 9 -4 ], [1 -3 , 9 -3 ] and [1 -2 , 9 -2 ] using step size of 2 -4 , 2 -3 , 2 -2 respectively for optimization. For all the experiments conducted, our model is trained for a maximum of 15,000 epochs. In addition, the sample length of 5 seconds is used to split the entire trial into segments, which facilitates consistent evaluation and benchmarking against the baseline performance of the dataset.\n\nTo strike a balance between the effectiveness of the contrastive learning component and the overall stability of the training process for the DECAN model, we make a strategic decision regarding the contrastive learning temperature scalar. Specifically, we selected a value of 0.5 as a compromise. In our efforts to enhance the quality of the projection of representations into the latent space, we conduct tuning of the number of hidden units within the projection head. Specifically, we explore a range of options, including 64, 128, 256, and 512 hidden units.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Baseline Results Of Padweed",
      "text": "Conditioned on intra-subject. To validate the classification performance in the condition of intra-subject, we employee a leave-one-block-out (LOBO) cross-validation technique. In each step of the cross-validation process, one block of samples is held out as the test set, while the classifier is trained on the samples from the remaining blocks. This process is repeated for all blocks' data, ensuring that each sample is used for testing exactly once. Fig.  4  presents the results of three different classifiers, LR, SVM and DNN in wet and dry electrode systems. Specific quantitative results are shown in Table  II  where the best performance in each case is highlighted in bold.\n\nIn an effort to offer valuable insights into optimizing the recognition system for enhanced performance, a paired t-test is conducted on the recognition results of the three models, focusing on performance disparities in both the wet and dry electrode systems. The statistical test results presented in Fig  4 (a ) and (b) demonstrate significant differences among the models. In the wet electrode system, the DNN model exhibits superior performance compared to both the SVM and LR models, with p ≤ 0.001. Similarly, in the dry electrode system, the DNN model achieved the best performance among the three models. However, one notable distinction from the wet electrode system is that the DNN model exhibited a more pronounced significance difference compared to the SVM model, with p ≤ 0.001. These findings highlight the effectiveness of the DNN model in both electrode systems and emphasize its superiority over alternative models.\n\nConditioned on inter-subject. We employ the Leave-One-Subject-Out (LOSO) cross-validation approach. This validation scheme involved utilizing data from 15 participants for training purposes, while the data from one participant is set aside for testing. The accuracy results for all subjects are depicted in Fig.  4 (c)  and (d) , showcasing a comprehensive overview of the findings. A notable observation is the substantial decrease in recognition accuracy for all three models when compared to the intra-subject scenario, regardless of whether the EEG system utilizes dry or wet electrodes. Within the wet electrode EEG system, both the DNN and SVM models achieve significantly superior results compared to the LR model. However, no significant difference is observed between the DNN and SVM models. Conversely, in the dry electrode EEG system, the DNN model consistently outperforms the SVM and LR models. This difference in performance is statistically significant, with p ≤ 0.001 for both the SVM and LR models.\n\nBaseline results comparison. Based on our analysis, we found a statistically significant difference in recognition accuracy between the wet electrode system and the dry electrode system in both intra-subject and inter-subject scenarios (p ≤ 0.001). Average accuracy that is 14.53% higher in intrasubject and 9.20% higher in inter-subject compared to the dry electrode system. The superior performance of the wet electrode system can be ascribed to its ability to mitigate signal noise more effectively, thereby providing more valuable information compared to the dry electrode system. Additionally, in inter-subject experiment, the wet electrode system demonstrates a decrease in emotion recognition accuracy by 16.33%, while the dry electrode system exhibits an 24.43% decrease compared to intra-subject which may be attributed to the increased variability when combining data from all subjects and differences in distribution between source and target domains.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Performance Evaluation Of Decan",
      "text": "Main results. We consider 5 baseline methods, all originally designed for wet-electrode EEG-based emotion recognition rather than dry-electrode systems, so we adjust the settings for these methods and ensured comparability of the results. According to Table  IV , DECAN consistently demonstrates superior performance across all metrics. Specifically, CDRC achieves the highest performance with an accuracy of 55.01% and an F1 score of 48.59%, showing a margin of 3.57% and 4.52% over the second-best baseline method DNN. These enhancements suggest that DECAN can effectively incorporate wet electrode information into dry electrode EEG-based emotion recognition systems and hold great promise for enhancing the accuracy and robustness of such systems. Fig.  5  compares the classification confusion matrices of the DNN (suboptimal method) with the proposed DECAN model in the intra-subject experiment. The experiment results indicate that our model achieves higher classification accuracy for almost all emotion categories, particularly excelling in the recognition of anger among negative emotions.  Ablation studies. We set the framework without the primary component contrastive learning as the baseline method in this experiment. The experimental results of the intra-subject scheme are depicted in Fig.  6 . Upon analysis, it is evident that the emotion recognition accuracy of the dry electrode EEG system exhibits improvement for 13 out of the 16 subjects, with subject 5 and 16 showing enhancements of up to 13.75% and 17.07% respectively. We have further applied the paired t-test to find whether there are significant differences between the results of the baseline approach and our model at the significant level as 0.05. In our analysis, the calculated pvalue was determined to be 0.01. This result suggests that there is indeed a significant difference between the results of the baseline approach and our model, demonstrating the effectiveness of the contrastive learning module.\n\nWe further investigate the effectiveness of introducing the contrastive learning module in our proposed DECAN by visualizing the latent EEG patterns. The feature distributions of three subjects from the PaDWEED dataset are presented in a two-dimensional space using t-SNE. For each subject, two latent features with a length of 5 seconds are randomly visualized, corresponding to the two methods from a trial in the testing set. The results, as depicted in Fig.  7 , clearly demonstrate that our approach, which combines the base emotion model with the contrastive learning strategy, yields more effective dry electrode EEG features.\n\nGeneralization test on inter-subject intra-dataset sce- Generalization test on inter-subject inter-dataset scenario. In this experiment, we additionally take into account the variations between datasets in addition to the individual variances among participants. To achieve inter-dataset pairing, the wet electrode EEG signals in the training data are sourced from the SEED V dataset (subject 1), while the dry electrode signals are obtained from the PaDWEED dataset. We specifically focused on cases with identical emotion categories (such as happy, neutral, sad, and fearful emotions) in both datasets,  VI. DISCUSSION Historically, there have been two mainstreams regarding improving the emotion recognition performance of dry electrode systems, refining prototyping workflows  [16] ,  [18] ,  [20] ,  [21]  and optimizing hardware  [17] . Here, to address the limitation of challenging hardware advancements in the short term, we focused on the former one and proposed a contrastive learning-based method to enhance the dry electrode EEG data emotion recognition performance. Unlike conventional methods that primarily focused on designing encoders to extract efficient EEG features from dry electrode systems  [16] ,  [18] ,  [20] , which often heavily depended on the quality of dry electrode EEG signals, our approach takes into consideration the challenging aspect of extracting efficient features from dry electrode EEG signals with relatively low signal-to-noise ratio. To overcome this challenge, we proposed to leverage the advantages offered by wet electrode EEG systems to enhance the emotion recognition capabilities of dry electrode systems.\n\nThe intra-subject analysis on the PaDWEED dataset demonstrates that our proposed DECAN effectively improves the emotion recognition performance of dry electrode EEG systems by leveraging knowledge obtained from wet electrode EEG systems. This finding supports the existence of shared features between dry and wet electrodes when performing the same task, which is consistent with previous research  [14] -  [16] . It also highlights the potential of contrastive learning methods in uncovering this specific information. Moreover, the inter-subject feature alignment analysis reveal promising results in improving the emotion recognition performance of an individual's dry electrode EEG by utilizing wet electrode  EEG data from other subjects and even from non-homologous dataset. This suggests that DECAN proposed in this study can effectively align EEG signals across devices and subjects. Consequently, it eliminates the need for each subject to undergo the corresponding wet electrode experiment, significantly reducing the complexity of the overall experimental setup. The contrastive learning architecture in DECAN has been shown to be effective in improving dry electrode EEG emotion recognition performance. Here, we further investigate its impact on the accuracy obtained with different frequency subband signals for each of the 16 participants and visualize the average results in Fig.  9 . In the baseline results, the recognition accuracy is notably higher in the high-frequency bands, especially the gamma band, which is consistency with previous studies  [50] -  [52] . Upon employing the DECAN model with the contrastive learning module, enhancements are observed in the recognition accuracy of the full-band signal. Moreover, there has been a significant improvement in the recognition accuracy of the delta and beta bands, while this outcome of other sub-bands has remained stable. This suggests that delta and beta bands also play a substantial role in dry electrode EEG emotion recognition, aligning with findings from prior research  [53] . Lastly, to our knowledge, PaDWEED is the first database which contains paired dry and wet electrode EEG data from collected with the same subjects using the same set of video stimuli and experimental protocol. This database can serve as a valuable research resource for scholars in the field of affective computing. It offers opportunities to investigate emotion recognition and the underlying mechanisms of both wet electrode and dry electrode systems. Additionally, it enables the exploration of the relationship between physiological signal patterns captured by these two types of electrodes. Moreover, the database goes beyond single modality research, as it includes simultaneous collection of peripheral physiological signals such as ECG, EOG, GSR, BVP, RSP and SKT signals. This comprehensive dataset can provide a foundation for conducting research on multimodal emotion recognition, whether using dry electrodes or wet electrodes. By incorporating multiple physiological signals, researchers can further enhance the accuracy and robustness of emotion recognition systems.\n\nIndeed, it must be acknowledged that the achieved performance in dry electrode emotion recognition within this study still leaves ample room for enhancement, which may be attributed to the fact that the current encoder architectures may not be able to fully explore the entire range of EEG features, thereby constraining their capacity to extract the valuable information embedded in wet EEG data effectively. To address this issue, future investigations could concentrate on refining the architecture and configuration of encoders optimized for extracting wet electrode EEG features, which may involve exploring more advanced and complex neural network architectures.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this study, we introduce a denoising encoder via contrastive learning alignment network (DECAN) to enhance the performance of dry electrode EEG emotion recognition with the help of wet electrode EEG signals:  (1)  We propose the DECAN model, a model consisting of two partially shared DNN models and a feature alignment contrastive learning strategy to extract efficient dry electrode EEG emotion features. (  2",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The DSI-24 EEG cap and the sensor layout with 18 channels.",
      "page": 4
    },
    {
      "caption": "Figure 1: Although peripheral physiological signals are not the focus",
      "page": 4
    },
    {
      "caption": "Figure 2: The protocol used in both wet and dry electrode emotion experiments.",
      "page": 4
    },
    {
      "caption": "Figure 3: , our proposed method is a contrastive",
      "page": 4
    },
    {
      "caption": "Figure 3: The architecture of our proposed framework DECAN with pairwise representation alignment with contrastive learning for emotion recognition.",
      "page": 5
    },
    {
      "caption": "Figure 4: (a) and (b) demonstrate significant differences among the",
      "page": 7
    },
    {
      "caption": "Figure 4: The baseline results of the wet-electrode and dry-electrode based emotion classification in the condition of both intra-subject and inter-subject scenarios",
      "page": 8
    },
    {
      "caption": "Figure 5: Confusion matrices of DNN (the second-best method) and the DECAN",
      "page": 9
    },
    {
      "caption": "Figure 8: (b), it demonstrates",
      "page": 9
    },
    {
      "caption": "Figure 6: Emotion recognition performance of our DECAN via ablation study",
      "page": 9
    },
    {
      "caption": "Figure 7: Visualization of latent features using t-SNE on the PaDWEED dataset.",
      "page": 9
    },
    {
      "caption": "Figure 8: Generation tests of DECAN. (a) The results of dry electrode EEG",
      "page": 10
    },
    {
      "caption": "Figure 9: In the baseline results, the recognition",
      "page": 10
    },
    {
      "caption": "Figure 9: Emotion recognition performance of our DECAN via ablation study",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "indicatestheresultsareobtainedbyourownimplementation.Bestresultsarein\n(a) DNN (b) DECAN (Ours)\n)%\n(\n.ccA\nFig.5. ConfusionmatricesofDNN(thesecond-bestmethod)andtheDECAN": "",
          "Column_2": ")%\n(\n.ccA\nCAN"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition from unimodal multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "2",
      "title": "Joint EEG feature transfer and semisupervised cross-subject emotion recognition",
      "authors": [
        "Y Peng",
        "H Liu",
        "W Kong",
        "F Nie",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Ind. Informatics"
    },
    {
      "citation_id": "3",
      "title": "A spatial-temporal ECG emotion recognition model based on dynamic feature fusion",
      "authors": [
        "S Xiao",
        "X Qiu",
        "C Tang",
        "Z Huang"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023"
    },
    {
      "citation_id": "4",
      "title": "Gsr signals features extraction for emotion recognition",
      "authors": [
        "K Kipli",
        "A Latip",
        "K Lias",
        "N Bateni",
        "S Yusoff",
        "N Tajudin",
        "M Jalil",
        "K Ray",
        "M Kaiser",
        "M Mahmud"
      ],
      "year": "2022",
      "venue": "Proceedings of Trends in Electronics and Health Informatics: TEHI 2021"
    },
    {
      "citation_id": "5",
      "title": "Using cardiorespiratory signals to recognize emotions elicited by watching music video clips",
      "authors": [
        "L Mirmohamadsadeghi",
        "A Yazdani",
        "J Vesin"
      ],
      "year": "2016",
      "venue": "18th IEEE International Workshop on Multimedia Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Inter-brain EEG feature extraction and analysis for continuous implicit emotion tagging during video watching",
      "authors": [
        "Y Ding",
        "X Hu",
        "Z Xia",
        "Y Liu",
        "D Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "7",
      "title": "A review on nonlinear methods using electroencephalographic recordings for emotion recognition",
      "authors": [
        "B García-Martínez",
        "A Martínez-Rodrigo",
        "R Alcaraz",
        "A Fernández-Caballero"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "8",
      "title": "Motion artefact management for soft bioelectronics",
      "authors": [
        "J Yin",
        "S Wang",
        "T Tat",
        "J Chen"
      ],
      "year": "2024",
      "venue": "Nature Reviews Bioengineering"
    },
    {
      "citation_id": "9",
      "title": "Diversity and suitability of the state-of-the-art wearable and wireless EEG systems review",
      "authors": [
        "C He",
        "Y Chen",
        "C Phang",
        "C Stevenson",
        "I Chen",
        "T Jung",
        "L Ko"
      ],
      "year": "2023",
      "venue": "IEEE J. Biomed. Health Informatics"
    },
    {
      "citation_id": "10",
      "title": "Impedance spectroscopy of conductive commercial hydrogels for electromyography and electroencephalography",
      "authors": [
        "F Freire",
        "M Becchi",
        "S Ponti",
        "E Miraldi",
        "A Strigazzi"
      ],
      "year": "2010",
      "venue": "Physiological Measurement"
    },
    {
      "citation_id": "11",
      "title": "Joint feature adaptation and graph adaptive label propagation for cross-subject emotion recognition from EEG signals",
      "authors": [
        "Y Peng",
        "W Wang",
        "W Kong",
        "F Nie",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "12",
      "title": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "13",
      "title": "Crosssubject eeg-based emotion recognition with deep domain confusion",
      "authors": [
        "W Zhang",
        "F Wang",
        "Y Jiang",
        "Z Xu",
        "S Wu",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "Intelligent Robotics and Applications -12th International Conference"
    },
    {
      "citation_id": "14",
      "title": "Systematic comparison between a wireless EEG system with dry electrodes and a wired EEG system with wet electrodes",
      "authors": [
        "J Kam",
        "S Griffin",
        "A Shen",
        "S Patel",
        "H Hinrichs",
        "H Heinze",
        "L Deouell",
        "R Knight"
      ],
      "year": "2019",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "15",
      "title": "Comparison between a wireless dry electrode eeg system with a conventional wired wet electrode eeg system for clinical applications",
      "authors": [
        "H Hinrichs",
        "M Scholz",
        "A Baum",
        "J Kam",
        "R Knight",
        "H.-J Heinze"
      ],
      "year": "2020",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "16",
      "title": "Investigating emotion eeg patterns for depression detection with attentive simple graph convolutional network",
      "authors": [
        "Y.-T Lan",
        "D Peng",
        "W Liu",
        "Y Luo",
        "Z Mao",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2023",
      "venue": "2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "17",
      "title": "Textile eeg cap using dry-comb electrodes for emotion detection of elderly people",
      "authors": [
        "Z Fangmeng",
        "P Siriaraya",
        "D Choi",
        "N Kuwahara"
      ],
      "year": "2020",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "18",
      "title": "Consumer grade brain sensing for emotion recognition",
      "authors": [
        "P Lakhan",
        "N Banluesombatkul",
        "V Changniam",
        "R Dhithijaiyratn",
        "P Leelaarporn",
        "E Boonchieng",
        "S Hompoonsup",
        "T Wilaiprasitporn"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "19",
      "title": "DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE J. Biomed. Health Informatics"
    },
    {
      "citation_id": "20",
      "title": "Real-time eeg-based human emotion recognition",
      "authors": [
        "M Javaid",
        "M Yousaf",
        "Q Sheikh",
        "M Awais",
        "S Saleem",
        "M Khalid"
      ],
      "year": "2015",
      "venue": "Neural Information Processing -22nd International Conference"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition using frontal eeg in vr affective scenes",
      "authors": [
        "T Xu",
        "R Yin",
        "L Shu",
        "X Xu"
      ],
      "year": "2019",
      "venue": "2019 IEEE MTT-S International Microwave Biomedical Conference (IMBioC)"
    },
    {
      "citation_id": "22",
      "title": "Recognition of human emotions using EEG signals: A review",
      "authors": [
        "M Rahman",
        "A Sarkar",
        "M Hossain",
        "M Hossain",
        "M Islam",
        "M Hossain",
        "J Quinn",
        "M Moni"
      ],
      "year": "2021",
      "venue": "Comput. Biol. Medicine"
    },
    {
      "citation_id": "23",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "24",
      "title": "Expression-guided EEG representation learning for emotion recognition",
      "authors": [
        "S Rayatdoost",
        "D Rudrauf",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition using effective connectivity and pre-trained convolutional neural networks in eeg signals",
      "authors": [
        "S Bagherzadeh",
        "K Maghooli",
        "A Shalbaf",
        "A Maghsoudi"
      ],
      "year": "2022",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition using heterogeneous convolutional neural networks combined with multimodal factorized bilinear pooling",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "S Wang",
        "T Xia"
      ],
      "year": "2022",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "27",
      "title": "DEAP: A database for emotion analysis ;using physiological signals",
      "authors": [
        "S Koelstra",
        "C Mühl",
        "M Soleymani",
        "J Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "28",
      "title": "MPED: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "29",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W Zheng",
        "B Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. Auton. Ment. Dev"
    },
    {
      "citation_id": "30",
      "title": "A dataset of continuous affect annotations and physiological signals for emotion analysis",
      "authors": [
        "K Sharma",
        "C Castellini",
        "E Van Den Broek",
        "A Albu-Schäffer",
        "F Schwenker"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "31",
      "title": "Analysis of EEG signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "32",
      "title": "ASCERTAIN: emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "33",
      "title": "The increasing instance of negative emotion reduce the performance of emotion recognition",
      "authors": [
        "X Wang",
        "S Zhao",
        "Y Pei",
        "Z Luo",
        "L Xie",
        "Y Yan",
        "E Yin"
      ],
      "year": "2023",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "34",
      "title": "Multimodal emotion recognition using EEG and eye tracking data",
      "authors": [
        "W Zheng",
        "B Dong",
        "B Lu"
      ],
      "year": "2014",
      "venue": "36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "35",
      "title": "Neural patterns between chinese and germans for eeg-based emotion recognition",
      "authors": [
        "S Wu",
        "M Schaefer",
        "W Zheng",
        "B Lu",
        "H Yokoi"
      ],
      "year": "2017",
      "venue": "8th International IEEE/EMBS Conference on Neural Engineering"
    },
    {
      "citation_id": "36",
      "title": "Two-dimensional cnn-based distinction of human emotions from eeg channels selected by multiobjective evolutionary algorithm",
      "authors": [
        "L Moctezuma",
        "T Abe",
        "M Molinas"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "37",
      "title": "Differential entropy feature for eeg-based vigilance estimation",
      "authors": [
        "L Shi",
        "Y Jiao",
        "B Lu"
      ],
      "year": "2013",
      "venue": "35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "38",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W Zheng",
        "J Zhu",
        "B Lu"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "39",
      "title": "Combining eye movements and eeg to enhance emotion recognition",
      "authors": [
        "Y Lu",
        "W.-L Zheng",
        "B Li",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IJCAI"
    },
    {
      "citation_id": "40",
      "title": "Eeg-based emotion recognition using discriminative graph regularized extreme learning machine",
      "authors": [
        "J.-Y Zhu",
        "W.-L Zheng",
        "Y Peng",
        "R.-N Duan",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "2014 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "41",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021"
    },
    {
      "citation_id": "42",
      "title": "Decoding speech perception from non-invasive brain recordings",
      "authors": [
        "A Défossez",
        "C Caucheteux",
        "J Rapin",
        "O Kabeli",
        "J King"
      ],
      "year": "2023",
      "venue": "Nat. Mac. Intell"
    },
    {
      "citation_id": "43",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning, ICML 2020"
    },
    {
      "citation_id": "44",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "45",
      "title": "AMIGOS: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "46",
      "title": "Identifying similarities and differences in emotion recognition with eeg and eye movements among chinese, german, and french people",
      "authors": [
        "W Liu",
        "W.-L Zheng",
        "Z Li",
        "S.-Y Wu",
        "L Gan",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "47",
      "title": "An investigation of olfactory-enhanced video on eeg-based emotion recognition",
      "authors": [
        "M Wu",
        "W Teng",
        "C Fan",
        "S Pei",
        "P Li",
        "Z Lv"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "48",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Eeg conformer: Convolutional transformer for eeg decoding and visualization",
      "authors": [
        "Y Song",
        "Q Zheng",
        "B Liu",
        "X Gao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "50",
      "title": "Fusing frequency-domain features and brain connectivity features for crosssubject emotion recognition",
      "authors": [
        "C Chen",
        "Z Li",
        "F Wan",
        "L Xu",
        "A Bezerianos",
        "H Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "51",
      "title": "Self-weighted semi-supervised classification for joint eeg-based emotion recognition and affective activation patterns mining",
      "authors": [
        "Y Peng",
        "W Kong",
        "F Qin",
        "F Nie",
        "J Fang",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "52",
      "title": "Sparsedgcnn: Recognizing emotion from multichannel eeg signals",
      "authors": [
        "G Zhang",
        "M Yu",
        "Y.-J Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Eeg-based multifrequency band functional connectivity analysis and the application of spatio-temporal features in emotion recognition",
      "authors": [
        "Y Zhang",
        "G Yan",
        "W Chang",
        "W Huang",
        "Y Yuan"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}