{
  "paper_id": "2501.16230v1",
  "title": "Mind-Eeg: Multi-Granularity Integration Network With Discrete Codebook For Eeg-Based Emotion Recognition",
  "published": "2025-01-27T17:29:17Z",
  "authors": [
    "Yuzhe Zhang",
    "Chengxi Xie",
    "Huan Liu",
    "Yuhan Shi",
    "Dalin Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition using electroencephalogram (EEG) signals has broad potential across various domains. EEG signals have ability to capture rich spatial information related to brain activity, yet effectively modeling and utilizing these spatial relationships remains a challenge. Existing methods struggle with simplistic spatial structure modeling, failing to capture complex node interactions, and lack generalizable spatial connection representations, failing to balance the dynamic nature of brain networks with the need for discriminative and generalizable features. To address these challenges, we propose the Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG). The framework employs a multi-granularity approach, integrating global and regional spatial information through a Global State Encoder, an Intra-Regional Functionality Encoder, and an Inter-Regional Interaction Encoder to comprehensively model brain activity. Additionally, we introduce a discrete codebook mechanism for constructing network structures via vector quantization, ensuring compact and meaningful brain network representations while mitigating over-smoothing and enhancing model generalization. The proposed framework effectively captures the dynamic and diverse nature of EEG signals, enabling robust emotion recognition. Extensive comparisons and analyses demonstrate the effectiveness of MIND-EEG, and the source code is publicly available at https://anonymous.4open. science/r/MIND EEG.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is a critical research area with diverse applications such as human-computer interaction, healthcare, and personalized learning  [Li et al., 2018a; Zhao et al., 2018 ; * # Equal contribution.\n\n†* Corresponding author.   et al., 2023]  are easy to recognize for emotions, they are susceptible to intentional manipulation. In contrast, electroencephalography (EEG) signals directly measure brain activities and cannot be consciously controlled, offering high reliability. Moreover, EEG signals have high temporal resolution and the ability to capture both conscious and subconscious emotions  [Jiao et al., 2019] , making them particularly effective for emotion recognition  [Zhang et al., 2022] . EEG signals are inherently multivariate time series data, encompassing both spatial and temporal information. A critical research focus lies in effectively extracting spatial information, which can be approached through the following two key questions:\n\nQ1: How to model the spatial structure of EEG nodes? As illustrated in the upper part of Figure  1 , EEG nodes are distributed across the scalp with specific spatial arrangements but lack explicit structural relationships. Modeling these structural relationships is tantamount to modeling brain functionalities, enabling the capture of interactions and dependencies among different nodes to better recognize emotional states. Some studies approach this by treating the brain as a unified whole  [Zhong et al., 2020]  (Figure1(a1)), while others focus on the anatomical and functional differences between the left and right hemispheres, modeling these two parts independently  [Li et al., 2020]  (Figure  1 (a2)). Additionally, some methods divide the brain into distinct regions based on functionalities, creating a multi-region structural model  [Li et al., 2019; Ding et al., 2023]  (Figure  1 (a3)). However, given the complexity of neural processes, relying on single structural modeling is insufficient for fully capturing the spatial features of EEG data. This underscores the necessity of a more comprehensive and integrative approach to modeling the spatial structure of EEG nodes.\n\nQ2: How to represent the spatial connections between EEG nodes? As depicted in the lower part of Figure1, existing studies can be broadly classified into three categories. Some methods construct a fixed, unified representation for all samples based on established neuroscience knowledge  [Liu et al., 2019]  (Figure  1 (b1)). Others adopt a data-driven approach, creating a learnable unified representation for all samples  [Song et al., 2018]  (Figure  1 (b2)). However, both approaches rely on a single unified network structure for all samples, which overlooks the dynamic and variable nature of brain network structures. To address this limitation, certain methods adaptively learn distinct representations for each sample using neural networks  [Song et al., 2021]  (Figure  1 (b3)). While promising, these approaches often encounter challenges such as overfitting and oversmoothing, particularly when working with small, augmented EEG datasets. In these scenarios, the neural network may compress distinct representations into a narrow region of the latent space, reducing its capacity to capture meaningful and discriminative features.\n\nTo address the above two questions, we propose the Multigranularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG). The proposed framework models the spatial structure from multiple granularities. Specifically, it has a Global State Encoder for modeling the global granularity, an Intra-Regional Functionality Encoder for modeling the regional granularity, and an Inter-Regional Interaction Encoder for modeling the granularity of inter-region interactions. Moreover, for each encoder, we propose an innovative approach using a codebook that stores discrete vector quantized representations of the spatial connections. This method assigns each sample a more compact, versatile, and meaningful spatial connection representation from the codebook, preventing embeddings of similar samples from converging to overly similar values, thus effectively mitigating the over-smoothing issue. Moreover, discrete embeddings are better suited for capturing inter-class differences, and maintaining clear distinctions between different categories in the latent space, thereby enhancing the model's recognition ability and generalization performance.\n\nWe conducted experiments under both subject-dependent and subject-independent scenarios on three benchmarking datasets, namely SEED-IV  [Zheng et al., 2018] , MPED  [Song et al., 2019] , and SEED-V  [Liu et al., 2021] . Our model achieved superior performance under both scenarios, significantly improving the emotion recognition accuracy compared to state-of-the-art models. Ablation studies demon-strate the effectiveness and robustness of each proposed component. Statistical studies on the codebook further validate the effectiveness of this design. The main contributions of this work are summarized as follows:\n\n1) We propose the Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG), which performs multi-granularity modeling and integration of spatial information in EEG signals for emotion recognition.\n\n2) We innovatively introduce a codebook that stores discrete representations in the construction of the spatial connections, to perform vector quantization on the network structures at each granularity for each sample. This ensures the diversity of the network structures and enhances the model's ability to extract class-related information.\n\n3) We conducted extensive comparative experiments on three public datasets and under two experimental scenarios. Our model achieved significant improvements in the subjectdependent scenario and delivered competitive results in the subject-independent scenario. The source code is publicly available at https://anonymous.4open.science/r/MIND EEG.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Spatial Structure Modeling",
      "text": "The spatial information in EEG signals has been modeled in various ways. Early research in EEG signals often modeled the brain as a complete network. For instance, Song et al.  [Song et al., 2018]  modeled the brain as a graph network for emotion recognition. Some studies have taken into account the asymmetry between the two hemispheres of the brain. Ding et al.  [Ding et al., 2022]  proposed a hemispheric convolutional kernel to learn the relationships between the hemispheres. Recent studies have considered the relationships between different functional regions of the brain  [Lindquist et al., 2012] . For example, Li et al.  [Li et al., 2019]  used Bidirectional Long Short-Term Memory (BiLSTM) networks to separately learn spatiotemporal EEG features within and between regions. However, most existing models are limited to single-scale or simplistic partitioning, failing to capture the brain's multi-layered and multi-granular complexity.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Spatial Connection Representation",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Magcn Codebook",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Vg",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Adjacent Matrix",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Look Up Replace",
      "text": "Global State Encoder ... The Intra-Regional Functionality Encoder and Inter-Regional Interaction Encoder divide the EEG signal into regions, extracting functional features within regions and modeling inter-region interactions. Multi-granularity spatial information is integrated for emotion recognition, with all graph networks in the model constructed using a discrete codebook approach.\n\nNevertheless, these dynamic methods construct only a single learned adjacency matrix, without addressing individual differences in EEG node correlations. Song et al.  [Song et al., 2020]  proposed an instance-adaptive graph model to capture individual-specific relationships between EEG nodes. However, these methods often suffer from overfitting or oversmoothing, particularly with small or augmented datasets, where distinct representations may be compressed into narrow latent spaces. Balancing dynamic network modeling with robustness to overfitting remains a key challenge.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we introduce the proposed MIND-EEG. We first present the overall framework of MIND-EEG for modeling the spatial structure, followed by a description of the discrete codebook used in representing specific spatial connections. Finally, we detail the integrative loss for emotion recognition and the training procedures.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Granularity Integration Network Framework",
      "text": "In this section, we provide a detailed explanation of how the proposed Multi-granularity Integration Network Framework models spatial information in EEG signals at multiple granularities. As shown in Figure  2 , our framework consists of two main parts. The first part, the Global State Encoder, extracts the global representation from the entire EEG signal.\n\nThe second part, consisting of the Intra-Regional Functionality Encoder and Inter-Regional Interaction Encoder, divides the EEG signal into multiple regions and captures both the functional representations within each region and the interregion interactions. Below, we describe the processes and model details for each part in turn.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Global State Encoder",
      "text": "Following the extraction of energy features from five distinct frequency bands, specifically the δ band (1-4 Hz), θ band (4-8 Hz), α band (8-14 Hz), β band (14-30 Hz), and γ band (30-50 Hz), an EEG sample is expressed as X ∈ R n×d .\n\nHere, n corresponds to the number of EEG nodes, and d indicates the number of frequency bands. The Global State Encoder evaluates each sample as an integrated whole to extract global state representations. Drawing inspiration from prior works  [Song et al., 2020; Song et al., 2021] , we dynamically construct a graph network for each input sample by merging spatial and frequency-domain information through our Adapative Graph Encoder (AGE) module. This is achieved by projecting X through a combination of left and right multiplication, formulated as:\n\nwhere M ∈ R n×n is the left multiplication matrix used to encode spatial relationships among EEG nodes, B represents the bias matrix, N ∈ R d×d fuses information across frequency bands, and P ∈ R d×nd is the projection matrix. The resulting output A ∈ R n×nd undergoes activation with the ELU function to ensure non-negativity. Finally, A is reshaped into d adjacency matrices, [A * 1 , ..., A * d ], corresponding to graphs derived from the d frequency bands. To normalize the adjacency matrix, each element A ij is scaled by\n\nThis process is formally expressed as\n\n, where D is a diagonal matrix with entries calculated as\n\nOnce the graph network is constructed for each sample, we apply vector quantization using the codebook to efficiently learn more compact and meaningful latent represen- The GCN layer computes the Laplacian matrix L associated with the global brain network A G and facilitates information exchange between adjacent nodes using the following equation:\n\nwhere H (l) and H (l+1) denote the node feature matrices at the input and output of the l-th layer, respectively. The initial input features H (0) correspond to the original input X.\n\nThe matrix W (l) is a learnable weight matrix specific to layer l, and σ is the activation function applied to introduce nonlinearity.\n\n2) The CBAM layer, as a general and efficient network, enhances feature representation by adaptively refining the input feature maps through node and spatial attention mechanisms. It is particularly suitable for EEG data feature extraction, allowing the model to focus on the most informative parts of the features. 3) Residual layers help mitigate the vanishing gradient problem by enabling the gradient to flow directly through the network layers. They also improve model convergence and stability by preserving information from earlier layers. The MAGCN module ultimately outputs the extracted global state representations X G . The SE network and CBAM network, being commonly used plugin modules, are not described in detail here regarding their specific architectures.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Regional Functionality And Interaction Encoders",
      "text": "As shown in the lower part of the Figure  2 , there are two encoders for regional feature extraction: the Intra-regional Functionality Encoder and the Inter-regional Interaction Encoder, which model the intra-regional and inter-regional spatial information, respectively, and extract the corresponding features. Some components are identical to those in the Global State Encoder and the following focuses on the different parts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Adjacent Matrix",
      "text": "... The human cerebral cortex is highly interconnected and centralized across different regions, with some studies dividing it into multiple emotion-related brain regions  [Hagmann et al., 2008; Bruder et al., 2017] . For different tasks and scenarios, we can partition the EEG data into varying numbers of regions and input them into our model. Therefore, the input EEG sample X ∈ R n×d is divided into {X 1 , ..., X Q }, where X i ∈ R ni×d , Q is the number of the regions and i n i = n (i = 1, ..., Q). Similar to the Global State Encoder, each regional sample is used to obtain the corresponding initial intra-regional brain network through Eq. (  1 ). After being quantized by their respective codebooks, the networks are input into the SE module and MAGCN module to extract the intra-regional EEG features X R = {X R1 , ..., X R Q }. Before extracting inter-regional features, we need to fuse the intra-regional features from all regions. Specifically, for each X Ri ∈ R ni×d , the attention-based connectivity matrix a is calculated as follows:",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Codebook",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Look Up Replace",
      "text": "where W is a trainable weight matrix, and • T represents the transpose operation. Subsequently, the weight coefficient\n\nfor each region is obtained by performing a rowwise summation on a. Here, C i reflects the functional connection strength of each node relative to all other nodes within the region. Finally, the fused regional features are computed as:\n\nIn the Inter-regional Interaction Encoder, the fused intraregional features XR = { XRi , i = 1, .., Q} are used to obtain the corresponding initial inter-regional brain network through Eq. (  1 ). The inter-regional brain network is then quantized using the inter-regional graphical codebook and input into the SE module and MAGCN module to extract the final regional EEG features X R .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Discerte Codebook Construction",
      "text": "This section provides a detailed explanation of the codebook mechanism within the model, as shown in the Figure  4 . The discerte codebook stores discrete representations for vector quantization of the dynamically generated network representations for each sample. It assigns each sample a compact, versatile brain network representation from the codebook, preventing similar samples from converging to overly similar embeddings and mitigating over-smoothing. Additionally, discrete embeddings capture inter-class differences, maintaining clear distinctions between categories and improving classification and generalization. We construct codebook for the global brain network denoted as V G , Q codebooks for intra-regional brain networks denoted as V r1 to V r Q , and codebook for the inter-regional brain network denoted as V R . The construction and training methods are similar. The global brain network codebook is used as an example below.\n\nSpecifically, We define a global graphical codebook V G = {v i |i = 1, ..., K} ∈ R K×D , where K is the number of the embeddings in codebook and D is the dimensionality of each embedding. The embeddings contained in the codebook represent different brain network states. To reduce computational cost and enable the codebook to learn more compact representations, the brain network A obtained from the input sample X is flattened and then passed through a linear layer for dimensionality reduction, resulting in Â. Then, the global graphical codebook identifies the nearest neighbor of Â. This operation is expressed as:\n\nwhere z i represents the quantized vector obtained from the codebook. This process is equivalent to finding the embedding with the highest similarity to the input embedding, typically measured by cosine similarity. The output z i is transformed back to its original dimensions and converted into the adjacency matrix used in subsequent processes. To update both the network and the embeddings in the codebook, we use the following loss function:\n\nwhere sg denotes the stop-gradient operation. This function acts as an identity during the forward pass but blocks gradient flow during backpropagation. The first term minimizes the distance between the graph Â and its corresponding embedding v zi , while the second term encourages the codebook to adapt to the network's representations. β balances the weights of the two parts of the loss and is empirically set to 0.25  [Van Den Oord et al., 2017] . Similarly, the intra-regional graphical codebook and inter-regional graphical codebook are updated using the same loss function L BR1 and L BR2 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Integrative Loss And Training",
      "text": "After obtaining the global and local EEG features, we concatenate them and input the result into an attention layer, followed by the final fully connected classification layer, to obtain the final emotion recognition results. We employ the cross-entropy loss function to measure the dissimilarity between the predicted labels and the real labels, with the classification loss denoted as L C . The overall loss function of our framework is summarize as:\n\nwhere α, β, and γ are hyper-parameters to balance the classification loss and the losses from the three codebooks. This paper uses PyTorch to build MIND-EEG and deploy it on a 3090 GPU. The global module contains map layer and MAGCN layer with a size of 62 × 5 for the input and 62 × 50 for the output. For the model's functional region division, we follow the excellent previous work  [Jin et al., 2024]  and set the number of regions Q = 7. Thus, the Intra-regional module contains 7 parallel MAGCN layers with an output size of 78 × 50. The Inter-regional module contains fusion layer and MAGCN layer with an output size of 7 × 60; the emotion recognition network contains a 3-layer fully connected network with an input of 69 × 145. For the optimization of the model, we used the SGD optimizer , setting the learning rate to 1e-2 and the batch size to 32. We evaluated the model using the average accuracy (ACC) and standard deviation (STD).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments 4.1 Experimental Settings Datasets",
      "text": "The SEED-IV  [Zheng et al., 2018] , SEED-V  [Liu et al., 2021] , and MPED  [Song et al., 2019]  datasets all include EEG recordings collected while participants watched movie clips designed to evoke various emotional states. The SEED-IV dataset consists of EEG data from 15 participants (7 males and 8 females) across three sessions, with each session containing 24 trials, corresponding to 2-minute movie clips inducing four emotional states: neutral, sad, fear, and happy. Similarly, the SEED-V dataset includes EEG recordings from 16 participants (10 females and 6 males) over three sessions. In total, 45 movie clips were shown to evoke five emotional states: happy, disgust, neutral, fear, and sad. The MPED dataset, which includes data from 23 participants (10 males and 13 females), consists of EEG recordings for 28 movie clips designed to trigger seven emotions: joy, funny, anger, fear, disgust, sadness, and neutrality. For all datasets, EEG signals were segmented into 1-second samples.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison Methods",
      "text": "We select a wide range of representative and well-established models in this field as comparison methods. Some of these models are non-graph-based approaches, including three representative algorithms based on machine learning and five representative non-graph-based deep learning models. In addition, we compare our method with a variety of graph-based deep learning models, including traditional classic models and the latest state-of-the-art (SOTA) approaches. The list of comparison methods is in Table  1 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Expreimental Protocol",
      "text": "We evaluated the proposed PGCN using both subjectdependent and subject-independent protocols. In subjectdependent experiments, training and test data came from the same subject, with average accuracy and standard deviation computed across all subjects. For SEED-IV, we followed  [Zhong et al., 2020] , using the last two trials of each emotion for testing and the remaining 16 for training. For SEED-V, we adopted the three-fold cross-validation setup in  [Li et al., 2021] , splitting each session into first, middle, and last five trials. For MPED, as in  [Song et al., 2019] , the first 21 trials were used for training and the last 7 for testing. In subject-independent experiments, training and test data were from different subjects. For SEED-IV, we applied leave-one-subject-out cross-validation per  [Li et al., 2020] , averaging results and standard deviations across subjects. For MPED, we used the same leave-one-subject-out approach as  [Song et al., 2019] . As no comparison methods conducted subject-independent experiments on SEED-V, we excluded it from such experiments. All datasets underwent identical preprocessing to ensure fairness. SEED-IV and SEED-V used differential entropy (DE) features  [Duan et al., 2013] , while MPED employed short-time Fourier transform (STFT) features  [Zheng and Lu, 2015b] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Results And Analyses",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Overall Comparison",
      "text": "In this section, we conduct experiments in the subjectdependent and subject-independent scenarios. Table  1  reports the average accuracies and standard deviations of the proposed MIND-EEG and all comparison methods on the SEED-IV, MPED, and SEED-V datasets. The results of all comparison methods in the table are consistent with those reported in the respective published papers. Those \"--\" in the table indicates that the corresponding paper did not conduct exper- As shown in the Table  1 , the proposed MIND-EEG achieves the outstanding average accuracy on all three datasets compared to all the comparison methods under both scenarios, while maintaining comparable standard deviations. Specifically, in the subject-dependent task, our model improves the accuracy by 3.19%, 1.54%, and 2.50% over the best-performing comparison methods on the SEED-IV, MPED, and SEED-V datasets, respectively. In the subjectindependent task, our model also achieved competitive results, demonstrating strong generalization capabilities. This result fully demonstrates our model's superiority.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "To thoroughly evaluate the impact of each module in our model, we conducted ablation studies on the three datasets with ACC and F1 score in the subject-dependent scene. In these studies, we systematically removed individual modules (the whole regional part, the global part, the intra-regional part, and the inter-regional part) from our model to demonstrate the effectiveness of each module. The results of the ablation experiments on the SEED-IV dataset are shown in Table 2, while the results on the other two datasets are provided in the appendix due to space limitations. From these results, we can draw the following findings: 1) Removing any module leads to a decline in model performance, demonstrating that each part plays a crucial role and that they complement each other to achieve the best performance. 2) Compared to removing the global part, removing the regional part results in a more significant performance drop. This suggests that the regional module (both intra-regional and inter-regional) are more important. Although the global part is also crucial, multi-layer and finer-grained graph modeling contributes more to feature extraction. 3) The results of removing either the intra-regional or inter-regional components show that both modules are meaningful, with the intra-regional module being somewhat more important.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Hyperparameter Analysis",
      "text": "In this section, we explore the selection of two critical groups of hyperparameters in the model. The first group involves the sizes of the codebooks in the three modules: the global codebook, intra-regional codebook, and inter-regional codebook, denoted as K 1 , K 2 , and K 3 , respectively. For this analysis, we vary one parameter from {8, 16, 32, 64, 128} while keeping the other two fixed. The second group consists of the weights for the three codebook loss terms: α, β, and γ. Similarly, we vary one weight within the range {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} while keeping the other two constants. The results of these experiments, conducted on the SEED-IV dataset under the subject-dependent scenario, are presented in Figure  5 . Based on the results in Fig.  5  (a) and (b), we set K 1 to 32, K 2 to 64, K 3 to 128, α to 0.2, β to 0.5, and γ to 1. The results indicate that the model's performance is highly sensitive to the two groups of parameters related to the crucial codebook mechanism.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Effectiveness Of Codebook",
      "text": "In this section, we discuss the effectiveness of the codebook. We conducted a statistical analysis on the usage proportion of embeddings in the codebook for all samples from the first subject across the three datasets. The x-axis represents embedding indices in the codebook, sorted by usage frequency. The y-axis shows the percentage of samples using each embedding. Due to space constraints, we only present the results of the global codebook in Fig 6, while the complete results for all codebooks are included in the appendix. From the results, we observe that for each dataset, a large number of embeddings are utilized. This indicates that the codebook has generated multiple distinct network structure representations for different samples. Additionally, in each case, some embeddings are used extensively, which we believe represent common structural features in the EEG signals, while others are used sparingly, representing more individualized characteristics of the network structure. In future work, we plan to further explore the interpretability of these embeddings in the codebook.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This work presents the Multi-Granularity Integration Network with Discrete Codebook (MIND-EEG), a novel framework for EEG-based emotion recognition. By integrating spatial information across multiple levels and utilizing a discrete codebook mechanism, the proposed model effectively models the dynamic and diverse characteristics of brain networks while addressing challenges such as overfitting and over-smoothing. Extensive experiments on three benchmark datasets demonstrate the superiority of MIND-EEG over existing methods in both subject-dependent and subjectindependent scenarios, with significant accuracy improvements. Analysis of the differences and utilization of network structure representations within the codebook also validates the effectiveness of this design. Future work will focus on further exploring the relationship between brain network connectivity and emotional states using the codebook to enhance model interpretability.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Two key questions for utilizing spatial information in EEG",
      "page": 1
    },
    {
      "caption": "Figure 1: , EEG nodes are dis-",
      "page": 1
    },
    {
      "caption": "Figure 1: (a2)). Addition-",
      "page": 2
    },
    {
      "caption": "Figure 1: (a3)). However,",
      "page": 2
    },
    {
      "caption": "Figure 1: (b1)). Others adopt a data-driven ap-",
      "page": 2
    },
    {
      "caption": "Figure 1: (b2)). However, both",
      "page": 2
    },
    {
      "caption": "Figure 2: The framework of the proposed MIND-EEG. The Global State Encoder captures the global representation of the entire EEG signal.",
      "page": 3
    },
    {
      "caption": "Figure 2: , our framework consists of",
      "page": 3
    },
    {
      "caption": "Figure 3: Structure of the MAGCN.",
      "page": 4
    },
    {
      "caption": "Figure 2: , there are two",
      "page": 4
    },
    {
      "caption": "Figure 4: Discerte Codebook Construction.",
      "page": 4
    },
    {
      "caption": "Figure 5: The parameter analysis of our proposed method on the",
      "page": 7
    },
    {
      "caption": "Figure 5: Based on the results in",
      "page": 7
    },
    {
      "caption": "Figure 6: The usage of embeddings in the global codebook across",
      "page": 7
    },
    {
      "caption": "Figure 5: (a) and (b), we set K1 to 32, K2 to 64, K3 to 128,",
      "page": 7
    },
    {
      "caption": "Figure 6: , while the complete re-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dalinzhang@ieee.org ∗†": "Abstract"
        },
        {
          "dalinzhang@ieee.org ∗†": "Emotion recognition using electroencephalogram"
        },
        {
          "dalinzhang@ieee.org ∗†": "(EEG)\nsignals has broad potential across various"
        },
        {
          "dalinzhang@ieee.org ∗†": "domains. EEG signals have ability to capture rich"
        },
        {
          "dalinzhang@ieee.org ∗†": "spatial\ninformation related to brain activity, yet ef-"
        },
        {
          "dalinzhang@ieee.org ∗†": "fectively modeling and utilizing these spatial\nre-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "lationships\nremains\na\nchallenge.\nExisting meth-"
        },
        {
          "dalinzhang@ieee.org ∗†": "ods struggle with simplistic spatial structure mod-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "eling, failing to capture complex node interactions,"
        },
        {
          "dalinzhang@ieee.org ∗†": "and lack generalizable spatial connection represen-"
        },
        {
          "dalinzhang@ieee.org ∗†": "tations,\nfailing to balance the dynamic nature of"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "brain networks with the need for discriminative and"
        },
        {
          "dalinzhang@ieee.org ∗†": "generalizable features. To address these challenges,"
        },
        {
          "dalinzhang@ieee.org ∗†": "we propose the Multi-granularity Integration Net-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "work with Discrete Codebook for EEG-based Emo-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "tion Recognition\n(MIND-EEG). The\nframework"
        },
        {
          "dalinzhang@ieee.org ∗†": "employs a multi-granularity approach,\nintegrating"
        },
        {
          "dalinzhang@ieee.org ∗†": "global and regional spatial\ninformation through a"
        },
        {
          "dalinzhang@ieee.org ∗†": "Global State Encoder, an Intra-Regional Function-"
        },
        {
          "dalinzhang@ieee.org ∗†": "ality Encoder,\nand an Inter-Regional\nInteraction"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "Encoder\nto\ncomprehensively model\nbrain\nactiv-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "ity. Additionally, we introduce a discrete codebook"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "mechanism for constructing network structures via"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "vector quantization, ensuring compact and mean-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "ingful brain network representations while mitigat-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "ing over-smoothing and enhancing model general-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "ization. The proposed framework effectively cap-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "tures the dynamic and diverse nature of EEG sig-"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "nals, enabling robust emotion recognition. Exten-"
        },
        {
          "dalinzhang@ieee.org ∗†": "sive comparisons and analyses demonstrate the ef-"
        },
        {
          "dalinzhang@ieee.org ∗†": "fectiveness of MIND-EEG,\nand the\nsource\ncode"
        },
        {
          "dalinzhang@ieee.org ∗†": "is publicly available\nat https://anonymous.4open."
        },
        {
          "dalinzhang@ieee.org ∗†": "science/r/MIND EEG."
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "1\nIntroduction"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "Emotion recognition is a critical\nresearch area with diverse"
        },
        {
          "dalinzhang@ieee.org ∗†": "applications such as human-computer interaction, healthcare,"
        },
        {
          "dalinzhang@ieee.org ∗†": "and personalized learning [Li et al., 2018a; Zhao et al., 2018;"
        },
        {
          "dalinzhang@ieee.org ∗†": ""
        },
        {
          "dalinzhang@ieee.org ∗†": "∗#Equal contribution."
        },
        {
          "dalinzhang@ieee.org ∗†": "†*Corresponding author."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "ers\nfocus on the anatomical and functional differences be-",
          "strate the effectiveness and robustness of each proposed com-": "ponent.\nStatistical studies on the codebook further validate"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "tween the left and right hemispheres, modeling these two",
          "strate the effectiveness and robustness of each proposed com-": "the effectiveness of\nthis design.\nThe main contributions of"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "parts independently[Li et al., 2020] (Figure 1(a2)). Addition-",
          "strate the effectiveness and robustness of each proposed com-": "this work are summarized as follows:"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "ally, some methods divide the brain into distinct regions based",
          "strate the effectiveness and robustness of each proposed com-": "1) We propose the Multi-granularity Integration Network"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "on functionalities,\ncreating a multi-region structural model",
          "strate the effectiveness and robustness of each proposed com-": "with Discrete Codebook for EEG-based Emotion Recogni-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "[Li et al., 2019; Ding et al., 2023] (Figure 1(a3)). However,",
          "strate the effectiveness and robustness of each proposed com-": "tion (MIND-EEG), which performs multi-granularity model-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "given the complexity of neural processes,\nrelying on single",
          "strate the effectiveness and robustness of each proposed com-": "ing and integration of spatial\ninformation in EEG signals for"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "structural modeling is insufficient for fully capturing the spa-",
          "strate the effectiveness and robustness of each proposed com-": "emotion recognition."
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "tial features of EEG data. This underscores the necessity of",
          "strate the effectiveness and robustness of each proposed com-": "2) We innovatively introduce a codebook that stores dis-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "a more comprehensive and integrative approach to modeling",
          "strate the effectiveness and robustness of each proposed com-": "crete representations in the construction of the spatial connec-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "the spatial structure of EEG nodes.",
          "strate the effectiveness and robustness of each proposed com-": "tions,\nto perform vector quantization on the network struc-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "the\nspatial\nconnections between\nQ2: How to represent",
          "strate the effectiveness and robustness of each proposed com-": "tures at each granularity for each sample.\nThis ensures the"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "EEG nodes? As depicted in the lower part of Figure1, ex-",
          "strate the effectiveness and robustness of each proposed com-": "diversity of the network structures and enhances the model’s"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "isting studies can be broadly classified into three categories.",
          "strate the effectiveness and robustness of each proposed com-": "ability to extract class-related information."
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "Some methods construct a fixed, unified representation for",
          "strate the effectiveness and robustness of each proposed com-": "3) We conducted extensive comparative experiments on"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "all samples based on established neuroscience knowledge[Liu",
          "strate the effectiveness and robustness of each proposed com-": "three public datasets and under\ntwo experimental scenarios."
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "et al., 2019] (Figure 1(b1)). Others adopt a data-driven ap-",
          "strate the effectiveness and robustness of each proposed com-": "Our model achieved significant improvements in the subject-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "proach,\ncreating a\nlearnable unified representation for\nall",
          "strate the effectiveness and robustness of each proposed com-": "dependent scenario and delivered competitive results in the"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "samples[Song et al., 2018]\n(Figure 1(b2)).\nHowever, both",
          "strate the effectiveness and robustness of each proposed com-": "subject-independent\nscenario.\nThe source code is publicly"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "approaches\nrely on a\nsingle unified network structure\nfor",
          "strate the effectiveness and robustness of each proposed com-": "available at https://anonymous.4open.science/r/MIND EEG."
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "all samples, which overlooks the dynamic and variable na-",
          "strate the effectiveness and robustness of each proposed com-": ""
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "ture of brain network structures.\nTo address\nthis\nlimita-",
          "strate the effectiveness and robustness of each proposed com-": ""
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "",
          "strate the effectiveness and robustness of each proposed com-": "2\nRelated Work"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "tion,\ncertain methods\nadaptively learn distinct\nrepresenta-",
          "strate the effectiveness and robustness of each proposed com-": ""
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "",
          "strate the effectiveness and robustness of each proposed com-": "2.1\nSpatial Structure Modeling"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "tions\nfor each sample using neural networks\n[Song et al.,",
          "strate the effectiveness and robustness of each proposed com-": ""
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "2021]\n(Figure 1(b3)). While promising,\nthese approaches",
          "strate the effectiveness and robustness of each proposed com-": "The spatial\ninformation in EEG signals has been modeled in"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "often\nencounter\nchallenges\nsuch\nas\noverfitting\nand\nover-",
          "strate the effectiveness and robustness of each proposed com-": "various ways. Early research in EEG signals often modeled"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "smoothing, particularly when working with small, augmented",
          "strate the effectiveness and robustness of each proposed com-": "the brain as a complete network.\nFor\ninstance, Song et al."
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "EEG datasets.\nIn these scenarios,\nthe neural network may",
          "strate the effectiveness and robustness of each proposed com-": "[Song et al., 2018] modeled the brain as a graph network for"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "compress distinct representations into a narrow region of the",
          "strate the effectiveness and robustness of each proposed com-": "emotion recognition.\nSome studies have taken into account"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "latent space, reducing its capacity to capture meaningful and",
          "strate the effectiveness and robustness of each proposed com-": "the asymmetry between the two hemispheres of\nthe brain."
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "discriminative features.",
          "strate the effectiveness and robustness of each proposed com-": "Ding et al.\n[Ding et al., 2022] proposed a hemispheric con-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "To address the above two questions, we propose the Multi-",
          "strate the effectiveness and robustness of each proposed com-": "volutional kernel to learn the relationships between the hemi-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "granularity Integration Network with Discrete Codebook for",
          "strate the effectiveness and robustness of each proposed com-": "spheres. Recent studies have considered the relationships be-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "EEG-based Emotion Recognition (MIND-EEG). The pro-",
          "strate the effectiveness and robustness of each proposed com-": "tween different functional regions of the brain [Lindquist et"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "posed framework models the spatial structure from multiple",
          "strate the effectiveness and robustness of each proposed com-": "al., 2012]. For example, Li et al.\n[Li et al., 2019] used Bidi-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "granularities. Specifically,\nit has a Global State Encoder for",
          "strate the effectiveness and robustness of each proposed com-": "rectional Long Short-Term Memory (BiLSTM) networks to"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "modeling the global granularity, an Intra-Regional Function-",
          "strate the effectiveness and robustness of each proposed com-": "separately learn spatiotemporal EEG features within and be-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "ality Encoder\nfor modeling the regional granularity, and an",
          "strate the effectiveness and robustness of each proposed com-": "tween regions. However, most existing models are limited to"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "Inter-Regional\nInteraction Encoder\nfor modeling the granu-",
          "strate the effectiveness and robustness of each proposed com-": "single-scale or simplistic partitioning,\nfailing to capture the"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "larity of\ninter-region interactions. Moreover,\nfor each en-",
          "strate the effectiveness and robustness of each proposed com-": "brain’s multi-layered and multi-granular complexity."
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "coder, we propose an innovative approach using a codebook",
          "strate the effectiveness and robustness of each proposed com-": ""
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "",
          "strate the effectiveness and robustness of each proposed com-": "2.2\nSpatial Connection Representation"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "that\nstores discrete vector quantized representations of\nthe",
          "strate the effectiveness and robustness of each proposed com-": ""
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "spatial connections. This method assigns each sample a more",
          "strate the effectiveness and robustness of each proposed com-": "The representation of spatial connections has evolved to ad-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "compact, versatile, and meaningful spatial connection repre-",
          "strate the effectiveness and robustness of each proposed com-": "dress the non-Euclidean nature of EEG nodes. Some methods"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "sentation from the codebook, preventing embeddings of sim-",
          "strate the effectiveness and robustness of each proposed com-": "used a fixed adjacency matrix based on knowledge of neuro-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "ilar samples from converging to overly similar values,\nthus",
          "strate the effectiveness and robustness of each proposed com-": "science. For example, Liu et al.\n[Liu et al., 2019] introduced"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "effectively mitigating the over-smoothing issue. Moreover,",
          "strate the effectiveness and robustness of each proposed com-": "an attention vector\nto select\nthe weights of\nthe EEG nodes,"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "discrete embeddings are better suited for capturing inter-class",
          "strate the effectiveness and robustness of each proposed com-": "thus extracting more discriminative features. Du et al.\n[Du"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "differences, and maintaining clear distinctions between dif-",
          "strate the effectiveness and robustness of each proposed com-": "et al., 2022] weighted the spatial distance matrix and the re-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "ferent categories in the latent space,\nthereby enhancing the",
          "strate the effectiveness and robustness of each proposed com-": "lational communication matrix between the EEG nodes to es-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "model’s recognition ability and generalization performance.",
          "strate the effectiveness and robustness of each proposed com-": "tablish the connections. Considering the dynamic nature of"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "We conducted experiments under both subject-dependent",
          "strate the effectiveness and robustness of each proposed com-": "brain activity, some research proposed a dynamic adjacency"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "and\nsubject-independent\nscenarios\non\nthree\nbenchmark-",
          "strate the effectiveness and robustness of each proposed com-": "matrix to learn the connections.\nSong et al.\n[Song et al.,"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "et\ning\ndatasets,\nnamely\nSEED-IV\n[Zheng\nal.,\n2018],",
          "strate the effectiveness and robustness of each proposed com-": "2018] introduced a learnable adjacency matrix to dynamically"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "MPED [Song et al., 2019], and SEED-V [Liu et al., 2021].",
          "strate the effectiveness and robustness of each proposed com-": "capture the node relationships. Zhong et al.\n[Zhong et al.,"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "Our model achieved superior performance under both scenar-",
          "strate the effectiveness and robustness of each proposed com-": "2020] introduced node-wise domain adversarial\ntraining and"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "ios, significantly improving the emotion recognition accuracy",
          "strate the effectiveness and robustness of each proposed com-": "an emotion-aware distribution learning regularized graph net-"
        },
        {
          "unified whole[Zhong et al., 2020]\n(Figure1(a1)), while oth-": "compared to state-of-the-art models. Ablation studies demon-",
          "strate the effectiveness and robustness of each proposed com-": "work based on learnable adjacency matrices."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "Figure 2: The framework of the proposed MIND-EEG. The Global State Encoder captures the global representation of the entire EEG signal."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "The Intra-Regional Functionality Encoder and Inter-Regional Interaction Encoder divide the EEG signal\ninto regions, extracting functional"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "features within regions and modeling inter-region interactions. Multi-granularity spatial\ninformation is integrated for emotion recognition,"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "with all graph networks in the model constructed using a discrete codebook approach."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "Global State Encoder\nNevertheless, these dynamic methods construct only a sin-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "gle learned adjacency matrix, without addressing individual"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "Following the extraction of energy features from five distinct"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "differences\nin EEG node correlations.\nSong et al.\n[Song"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "frequency bands,\nspecifically the δ band (1–4 Hz), θ band"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "et al., 2020] proposed an instance-adaptive graph model\nto"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "(4–8 Hz), α band (8–14 Hz), β band (14–30 Hz), and γ band"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "capture individual-specific relationships between EEG nodes.\n(30–50 Hz),\nan EEG sample is expressed as X ∈ Rn×d."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "However, these methods often suffer from overfitting or over-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "Here, n corresponds to the number of EEG nodes, and d in-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "smoothing, particularly with small or\naugmented datasets,"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "dicates the number of frequency bands. The Global State En-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "where distinct\nrepresentations may be compressed into nar-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "coder evaluates each sample as an integrated whole to extract"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "row latent spaces. Balancing dynamic network modeling with"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "global state representations. Drawing inspiration from prior"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "robustness to overfitting remains a key challenge."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "works [Song et al., 2020; Song et al., 2021], we dynamically"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "construct a graph network for each input sample by merging"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "3\nMethodology\nspatial and frequency-domain information through our Ada-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "pative Graph Encoder\n(AGE) module.\nThis is achieved by"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "In this section, we introduce the proposed MIND-EEG. We"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "projecting X through a combination of left and right multi-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "first present\nthe overall framework of MIND-EEG for mod-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "plication, formulated as:"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "eling the spatial structure,\nfollowed by a description of\nthe"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "discrete codebook used in representing specific spatial con-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "A = ELU((M X + B)N P ),\n(1)"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "nections.\nFinally, we detail\nthe integrative loss for emotion"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "recognition and the training procedures."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "where M ∈ Rn×n is the left multiplication matrix used to"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "encode spatial relationships among EEG nodes, B represents"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "3.1\nMulti-granularity Integration Network"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "the bias matrix, N ∈ Rd×d\nfuses\ninformation across\nfre-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "Framework"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "∈ Rd×nd\nquency bands,\nand P\nis\nthe projection matrix."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "In this section, we provide a detailed explanation of how the\nThe resulting output A ∈ Rn×nd undergoes activation with"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "proposed Multi-granularity Integration Network Framework\nthe ELU function to ensure non-negativity. Finally, A is re-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "models spatial\ninformation in EEG signals at multiple gran-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "1, ..., A∗\nd], corresponding"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "ularities. As shown in Figure 2, our\nframework consists of\nto graphs derived from the d frequency bands. To normalize"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "1√\ntwo main parts. The first part,\nthe Global State Encoder, ex-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": ".\nthe adjacency matrix, each element Aij is scaled by"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "DiiDjj"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "tracts the global\nrepresentation from the entire EEG signal."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "2 AD− 1\nThis process is formally expressed as Anorm = D− 1\n2 ,\nThe second part, consisting of the Intra-Regional Functional-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "where D is a diagonal matrix with entries calculated as Dii =\nity Encoder and Inter-Regional\nInteraction Encoder, divides"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "(cid:80)"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "the EEG signal\ninto multiple regions and captures both the\nj Aij."
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "Once the graph network is constructed for each sample,\nfunctional\nrepresentations within each region and the inter-"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "we\napply vector quantization using the\ncodebook to effi-\nregion interactions.\nBelow, we describe the processes and"
        },
        {
          "Regional Interaction Encoder\nRegional Functionality Encoder": "ciently learn more compact and meaningful\nlatent represen-\nmodel details for each part in turn."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Adjacent matrix\nReplaced": "adjacent matrix"
        },
        {
          "Adjacent matrix\nReplaced": "Flatten A"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "Figure 4: Discerte Codebook Construction."
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "The human cerebral cortex is highly interconnected and"
        },
        {
          "Adjacent matrix\nReplaced": "centralized across different regions, with some studies divid-"
        },
        {
          "Adjacent matrix\nReplaced": "ing it\ninto multiple emotion-related brain regions [Hagmann"
        },
        {
          "Adjacent matrix\nReplaced": "et al., 2008; Bruder et al., 2017]. For different tasks and sce-"
        },
        {
          "Adjacent matrix\nReplaced": "narios, we can partition the EEG data into varying numbers"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "of regions and input\nthem into our model. Therefore,\nthe in-"
        },
        {
          "Adjacent matrix\nReplaced": "put EEG sample X ∈ Rn×d is divided into {X1, ..., XQ},"
        },
        {
          "Adjacent matrix\nReplaced": "the number of\nthe regions and\nwhere Xi ∈ Rni×d, Q is"
        },
        {
          "Adjacent matrix\nReplaced": "(cid:80)"
        },
        {
          "Adjacent matrix\nReplaced": "Similar\nto the Global State En-"
        },
        {
          "Adjacent matrix\nReplaced": "i ni = n (i = 1, ..., Q)."
        },
        {
          "Adjacent matrix\nReplaced": "coder, each regional sample is used to obtain the correspond-"
        },
        {
          "Adjacent matrix\nReplaced": "ing initial intra-regional brain network through Eq. (1). After"
        },
        {
          "Adjacent matrix\nReplaced": "being quantized by their respective codebooks,\nthe networks"
        },
        {
          "Adjacent matrix\nReplaced": "are input\ninto the SE module and MAGCN module to ex-"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "tract the intra-regional EEG features XR = {XR1, ..., XRQ}."
        },
        {
          "Adjacent matrix\nReplaced": "Before extracting inter-regional features, we need to fuse the"
        },
        {
          "Adjacent matrix\nReplaced": "intra-regional features from all regions. Specifically, for each"
        },
        {
          "Adjacent matrix\nReplaced": "the attention-based connectivity matrix a is\nXRi ∈ Rni×d,"
        },
        {
          "Adjacent matrix\nReplaced": "calculated as follows:"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "(3)\na = XRiW (XRiW )T ,"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "where W is a trainable weight matrix, and ·T represents the"
        },
        {
          "Adjacent matrix\nReplaced": "transpose operation.\nSubsequently,\nthe weight\ncoefficient"
        },
        {
          "Adjacent matrix\nReplaced": "Ci ∈ R1×ni for each region is obtained by performing a row-"
        },
        {
          "Adjacent matrix\nReplaced": "reflects the functional con-\nwise summation on a. Here, Ci"
        },
        {
          "Adjacent matrix\nReplaced": "nection strength of each node relative to all other nodes within"
        },
        {
          "Adjacent matrix\nReplaced": "the region. Finally,\nthe fused regional features are computed"
        },
        {
          "Adjacent matrix\nReplaced": "as:"
        },
        {
          "Adjacent matrix\nReplaced": "(4)\nˆXRi = sof tmax(Ci)XRi."
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "In the Inter-regional\nInteraction Encoder,\nthe fused intra-"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "regional features\nXR = {\nˆXRi , i = 1, .., Q} are used to obtain"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "the corresponding initial inter-regional brain network through"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "Eq.\n(1). The inter-regional brain network is then quantized"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "using the inter-regional graphical codebook and input into the"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "SE module and MAGCN module to extract the final regional"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "EEG features XR."
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "3.2\nDiscerte Codebook Construction"
        },
        {
          "Adjacent matrix\nReplaced": ""
        },
        {
          "Adjacent matrix\nReplaced": "This section provides a detailed explanation of the codebook"
        },
        {
          "Adjacent matrix\nReplaced": "mechanism within the model, as shown in the Figure 4. The"
        },
        {
          "Adjacent matrix\nReplaced": "discerte codebook stores discrete representations for vector"
        },
        {
          "Adjacent matrix\nReplaced": "quantization of the dynamically generated network represen-"
        },
        {
          "Adjacent matrix\nReplaced": "tations for each sample.\nIt assigns each sample a compact,"
        },
        {
          "Adjacent matrix\nReplaced": "versatile brain network representation from the\ncodebook,"
        },
        {
          "Adjacent matrix\nReplaced": "preventing similar samples from converging to overly simi-"
        },
        {
          "Adjacent matrix\nReplaced": "lar embeddings and mitigating over-smoothing. Additionally,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "taining clear distinctions between categories and improving",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "for the output. For the model’s functional region division, we"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "classification and generalization. We construct codebook for",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "follow the excellent previous work [Jin et al., 2024] and set"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "for\nthe global brain network denoted as VG, Q codebooks",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "the number of regions Q = 7. Thus,\nthe Intra-regional mod-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "and\nintra-regional brain networks denoted as Vr1\nto VrQ,",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "ule contains 7 parallel MAGCN layers with an output size of"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "codebook for the inter-regional brain network denoted as VR.",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "78 × 50. The Inter-regional module contains fusion layer and"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "The construction and training methods are similar. The global",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "MAGCN layer with an output size of 7 × 60;\nthe emotion"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "brain network codebook is used as an example below.",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "recognition network contains a 3-layer\nfully connected net-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "Specifically, We define a global graphical codebook VG =",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "work with an input of 69 × 145. For the optimization of the"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "{vi|i = 1, ..., K} ∈ RK×D, where K is the number of the",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "model, we used the SGD optimizer , setting the learning rate"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "embeddings in codebook and D is the dimensionality of each",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "to 1e-2 and the batch size to 32. We evaluated the model using"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "embedding. The embeddings contained in the codebook rep-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "the average accuracy (ACC) and standard deviation (STD)."
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "resent different brain network states.\nTo reduce computa-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "tional cost and enable the codebook to learn more compact",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "4\nExperiments"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "representations, the brain network A obtained from the input",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "4.1\nExperimental Settings"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "sample X is flattened and then passed through a linear layer",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "Datasets"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "for dimensionality reduction, resulting in ˆA. Then, the global",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "The SEED-IV [Zheng et al.,\n2018], SEED-V [Liu et al.,"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "graphical codebook identifies the nearest neighbor of ˆA. This",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "2021],\nand MPED [Song et al., 2019] datasets all\ninclude"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "operation is expressed as:",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "EEG recordings collected while participants watched movie"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "(5)\n|| ˆA − vi||2\nzi = arg min",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "clips designed to evoke various emotional states. The SEED-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "j",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "IV dataset consists of EEG data from 15 participants (7 males"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "represents the quantized vector obtained from the\nwhere zi",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "and 8 females) across three sessions, with each session con-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "codebook. This process is equivalent\nto finding the embed-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "taining 24 trials, corresponding to 2-minute movie clips in-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "ding with the highest similarity to the input embedding,\ntyp-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "ducing four emotional states:\nneutral, sad,\nfear, and happy."
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "is trans-\nically measured by cosine similarity. The output zi",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "Similarly, the SEED-V dataset includes EEG recordings from"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "formed back to its original dimensions and converted into the",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "16 participants (10 females and 6 males) over three sessions."
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "adjacency matrix used in subsequent processes.\nTo update",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "In total, 45 movie clips were shown to evoke five emotional"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "both the network and the embeddings in the codebook, we",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "states:\nhappy, disgust, neutral,\nfear,\nand sad.\nThe MPED"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "use the following loss function:",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "dataset, which includes data from 23 participants (10 males"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "and 13 females), consists of EEG recordings for 28 movie"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "(6)\nLBG = ||sg[ ˆA] − vzi ||2\n2 + β|| ˆA − sg[vzi]||2",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "clips designed to trigger seven emotions:\njoy,\nfunny, anger,"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "where\nsg denotes\nthe\nstop-gradient operation.\nThis\nfunc-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "fear, disgust, sadness, and neutrality.\nFor all datasets, EEG"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "tion acts as an identity during the forward pass but blocks",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "signals were segmented into 1-second samples."
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "gradient flow during backpropagation.\nThe first\nterm mini-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "Comparison Methods"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "ˆ",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "mizes the distance between the graph\nA and its correspond-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "We select a wide range of representative and well-established"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": ", while the second term encourages the\ning embedding vzi",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "models in this field as comparison methods. Some of these"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "codebook to adapt\nto the network’s representations.\nβ bal-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "models are non-graph-based approaches, including three rep-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "ances the weights of the two parts of the loss and is empiri-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "resentative algorithms based on machine learning and five"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "cally set\nto 0.25 [Van Den Oord et al., 2017]. Similarly,\nthe",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "representative non-graph-based deep learning models.\nIn ad-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "intra-regional graphical codebook and inter-regional graphi-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "dition, we compare our method with a variety of graph-based"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "cal codebook are updated using the same loss function LBR1",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "deep learning models,\nincluding traditional classic models"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "and LBR2.",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "and the latest state-of-the-art\n(SOTA) approaches.\nThe list"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "of comparison methods is in Table 1."
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "3.3\nIntegrative Loss and Training",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "After obtaining the global and local EEG features, we con-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "Expreimental Protocol"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "catenate them and input\nthe result\ninto an attention layer,",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "We\nevaluated\nthe\nproposed\nPGCN using\nboth\nsubject-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "followed by the final\nfully connected classification layer,\nto",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "dependent\nand subject-independent protocols.\nIn subject-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "obtain the final emotion recognition results. We employ the",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "dependent\nexperiments,\ntraining and test data\ncame\nfrom"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "cross-entropy loss function to measure the dissimilarity be-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "the same subject, with average accuracy and standard devi-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "tween the predicted labels and the real labels, with the classi-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "ation computed across all\nsubjects.\nFor SEED-IV, we fol-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "fication loss denoted as LC. The overall loss function of our",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "lowed [Zhong et al., 2020], using the last\ntwo trials of each"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "framework is summarize as:",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "emotion for\ntesting and the remaining 16 for\ntraining.\nFor"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "SEED-V, we\nadopted the\nthree-fold cross-validation setup"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "(7)\nL = LC + αLBG + βLBR1 + γLBR2,",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": ""
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "in [Li et al., 2021], splitting each session into first, middle,"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "where α, β, and γ are hyper-parameters to balance the classi-",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "and last five trials. For MPED, as in [Song et al., 2019],\nthe"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "fication loss and the losses from the three codebooks.",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "first 21 trials were used for\ntraining and the last 7 for\ntest-"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "This paper uses PyTorch to build MIND-EEG and deploy",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "ing.\nIn subject-independent experiments,\ntraining and test"
        },
        {
          "discrete\nembeddings\ncapture\ninter-class differences, main-": "it on a 3090 GPU. The global module contains map layer and",
          "MAGCN layer with a size of 62 × 5 for the input and 62 × 50": "data were from different subjects. For SEED-IV, we applied"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and",
      "data": [
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "Subject-Independent EER Experiments on the SEED-IV, MPED, and SEED-V Datasets."
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": ""
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "Method"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": ""
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "DBN [Zheng et al., 2014]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "SVM [Zheng and Lu, 2015a]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "GSCCA [Zheng, 2016]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "BiDANN-S [Li et al., 2018b]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "A-LSTM [Song et al., 2019]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "BiHDM [Li et al., 2020]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "EeT [Liu et al., 2022]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "DMMR [Wang et al., 2024]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "DGCNN [Song et al., 2018]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "RGNN [Zhong et al., 2020]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "IAG [Song et al., 2020]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "MD-AGCN [Li et al., 2021]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "V-IAG [Song et al., 2021]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "Siam-GCAN [Zeng et al., 2022]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "GMSS [Li et al., 2023]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "MSFR-GCN [Pan et al., 2023]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "PGCN [Jin et al., 2024]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "BGAGCN-MT [Yan et al., 2024]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "SGGT [Chang et al., 2024]"
        },
        {
          "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and": "MIND-EEG"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: The Mean Accuracies and Standard Deviations of Proposed MIND-EEG and Comparison Methods for Subject-Dependent and",
      "data": [
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": "SEED-IV"
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": "ACC(%)"
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": "80.82±12.39"
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": "85.39±12.22"
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": "84.80±11.68"
        },
        {
          "Table 2: The results of ablation experiments.": ""
        },
        {
          "Table 2: The results of ablation experiments.": "86.16±09.71"
        },
        {
          "Table 2: The results of ablation experiments.": "92.21±09.05"
        },
        {
          "Table 2: The results of ablation experiments.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u00007\u0000U\u0000H\u0000Q\u0000G\u0000\u0003\u0000&\u0000X\u0000U\u0000Y\u0000H"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0015",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u00003\u0000H\u0000U\u0000F\u0000H\u0000Q\u0000W\u0000D\u0000J\u0000H\u0000\u0003\u0000\u000b\u0000\b\u0000\f\n\u00003\u0000H\u0000U\u0000F\u0000H\u0000Q\u0000W\u0000D\u0000J\u0000H"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0013",
          "\u0000\u0015\u0000\u0013": "\u0000\u0014\u0000\u0013"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u001b\n\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u001b",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000$\u0000F\u0000F\u0000X\u0000U\u0000D\u0000F\u0000\\\n\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0019\n\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0019",
          "\u0000\u0015\u0000\u0013": "\u0000\u0013\n\u0000\u0014\u0000\u0017 \u0000\u0015\u0000\u001a \u0000\u0014 \u0000\u0015\u0000\u001b \u0000\u0015\u0000\u0017 \u0000\u0015\u0000\u0013 \u0000\u0015\u0000\u0015 \u0000\u0014\u0000\u001b \u0000\u0014\u0000\u001c \u0000\u001a \u0000\u0016\u0000\u0015 \u0000\u0016\u0000\u0014 \u0000\u0017\n\u0000\u0016 \u0000\u0014\u0000\u0015 \u0000\u001b \u0000\u0014\u0000\u0016 \u0000\u0015\u0000\u0014 \u0000\u0016\u0000\u0013 \u0000\u0015\u0000\u0016 \u0000\u0015\n\u0000\u0019 \u0000\u0015\u0000\u0018 \u0000\u0014\u0000\u0014 \u0000\u0018 \u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u0019 \u0000\u0015\u0000\u001c \u0000\u0014\u0000\u0018 \u0000\u001c \u0000\u0014\u0000\u0013 \u0000\u0014\u0000\u001a"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u0000,\u0000Q\u0000G\u0000H\u0000[\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000(\u0000P\u0000E\u0000H\u0000G\u0000G\u0000L\u0000Q\u0000J\u0000V\u0000\u0003\u0000L\u0000Q\u0000\u0003\u0000&\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0017",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000/\u0000R\u0000V\u0000V\u0000\u0003\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000\u0003\n\u0000*\u0000O\u0000R\u0000E\u0000D\u0000O\u0000\u0003\u0000&\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0015",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000/\u0000R\u0000V\u0000V\u0000\u0003\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000\u0003\n\u0000,\u0000Q\u0000W\u0000U\u0000D\u0000\u0010\u0000U\u0000H\u0000J\u0000L\u0000R\u0000Q\u0000D\u0000O\u0000\u0003\u0000&\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N",
          "\u0000\u0015\u0000\u0013": "(a) MPED dataset"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u001b\u0000\u0013\n\u0000/\u0000R\u0000V\u0000V\u0000\u0003\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\u0000\u0003\n\u0000,\u0000Q\u0000W\u0000H\u0000U\u0000\u0010\u0000U\u0000H\u0000J\u0000L\u0000R\u0000Q\u0000D\u0000O\u0000\u0003\u0000&\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u0000\u0015\u0000\u0013"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u001a\u0000\u001b\n\u0000\u0013\u0000\u0011\u0000\u001a\u0000\u001b",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000\u0013\u0000\u0011\u0000\u0014 \u0000\u0013\u0000\u0011\u0000\u0015 \u0000\u0013\u0000\u0011\u0000\u0016 \u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0013\u0000\u0011\u0000\u0018 \u0000\u0013\u0000\u0011\u0000\u0019 \u0000\u0013\u0000\u0011\u0000\u001a \u0000\u0013\u0000\u0011\u0000\u001b \u0000\u0013\u0000\u0011\u0000\u001c\n\u0000\u0014\n\u0000\u001b\n\u0000\u0014\u0000\u0019\n\u0000\u0016\u0000\u0015\n\u0000\u0019\u0000\u0017\n\u0000\u0014\u0000\u0015\u0000\u001b",
          "\u0000\u0015\u0000\u0013": "\u00007\u0000U\u0000H\u0000Q\u0000G\u0000\u0003\u0000&\u0000X\u0000U\u0000Y\u0000H"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "\u0000/\u0000R\u0000V\u0000V\u0000\u0003\u0000:\u0000H\u0000L\u0000J\u0000K\u0000W\n\u00006\u0000L\u0000]\u0000H\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000&\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N",
          "\u0000\u0015\u0000\u0013": "\u00003\u0000H\u0000U\u0000F\u0000H\u0000Q\u0000W\u0000D\u0000J\u0000H\u0000\u0003\u0000\u000b\u0000\b\u0000\f\n\u00003\u0000H\u0000U\u0000F\u0000H\u0000Q\u0000W\u0000D\u0000J\u0000H"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u0000\u0014\u0000\u0013"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "(a)\n(b)",
          "\u0000\u0015\u0000\u0013": "\u0000\u0013"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u0000\u0014\u0000\u0019 \u0000\u0015\u0000\u001b \u0000\u0015\u0000\u0014 \u0000\u0016 \u0000\u0014\u0000\u0018 \u0000\u0014\u0000\u0016 \u0000\u0015\u0000\u001a \u0000\u0019\n\u0000\u001c \u0000\u0014\u0000\u0014 \u0000\u0015\u0000\u0017 \u0000\u001b \u0000\u0016\u0000\u0015 \u0000\u0015\u0000\u0013 \u0000\u0014\u0000\u0017 \u0000\u0015\u0000\u001c \u0000\u0015\u0000\u0019 \u0000\u0014\u0000\u001b \u0000\u0018 \u0000\u0015\u0000\u0018 \u0000\u0014\u0000\u001a \u0000\u0016\u0000\u0014 \u0000\u0014\n\u0000\u0015 \u0000\u0016\u0000\u0013 \u0000\u0014\u0000\u0015 \u0000\u0017 \u0000\u0014\u0000\u001c \u0000\u0015\u0000\u0016 \u0000\u001a \u0000\u0014\u0000\u0013 \u0000\u0015\u0000\u0015"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u0000,\u0000Q\u0000G\u0000H\u0000[\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000(\u0000P\u0000E\u0000H\u0000G\u0000G\u0000L\u0000Q\u0000J\u0000V\u0000\u0003\u0000L\u0000Q\u0000\u0003\u0000&\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "Figure 5: The parameter analysis of our proposed method on the",
          "\u0000\u0015\u0000\u0013": "(b) SEED-IV dataset"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "SEED-IV dataset with subject-dependent scene.\n(a) Analysis of the",
          "\u0000\u0015\u0000\u0013": "\u0000\u0015\u0000\u0013\n\u00007\u0000U\u0000H\u0000Q\u0000G\u0000\u0003\u0000&\u0000X\u0000U\u0000Y\u0000H"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "sizes of three types of codebooks.\n(b) Analysis of the weight of the",
          "\u0000\u0015\u0000\u0013": "\u00003\u0000H\u0000U\u0000F\u0000H\u0000Q\u0000W\u0000D\u0000J\u0000H\u0000\u0003\u0000\u000b\u0000\b\u0000\f\n\u00003\u0000H\u0000U\u0000F\u0000H\u0000Q\u0000W\u0000D\u0000J\u0000H\n\u0000\u0014\u0000\u0013"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "three codebook loss terms.",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u0000\u0013\n\u0000\u0015\u0000\u001c \u0000\u001b \u0000\u0015\u0000\u0015 \u0000\u0015\u0000\u001b \u0000\u0014\u0000\u0017 \u0000\u0015\u0000\u0016 \u0000\u0015 \u0000\u0015\u0000\u0014 \u0000\u0016 \u0000\u0016\u0000\u0015 \u0000\u0014\u0000\u001a \u0000\u001c \u0000\u0016\u0000\u0013 \u0000\u0014\u0000\u0016 \u0000\u0015\u0000\u0018 \u0000\u0014\u0000\u0013 \u0000\u001a \u0000\u0014\u0000\u0018 \u0000\u0014\u0000\u001c \u0000\u0014\u0000\u001b \u0000\u0015\u0000\u0017 \u0000\u0015\u0000\u0013 \u0000\u0015\u0000\u001a \u0000\u0014\u0000\u0015 \u0000\u0017\n\u0000\u0018\n\u0000\u0019 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0019 \u0000\u0014 \u0000\u0015\u0000\u0019 \u0000\u0016\u0000\u0014"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "\u0000,\u0000Q\u0000G\u0000H\u0000[\u0000\u0003\u0000R\u0000I\u0000\u0003\u0000(\u0000P\u0000E\u0000H\u0000G\u0000G\u0000L\u0000Q\u0000J\u0000V\u0000\u0003\u0000L\u0000Q\u0000\u0003\u0000&\u0000R\u0000G\u0000H\u0000E\u0000R\u0000R\u0000N"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "(c) SEED-V dataset"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "the best-performing comparison methods on the SEED-IV,",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "MPED, and SEED-V datasets,\nrespectively.\nIn the subject-",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "Figure 6: The usage of embeddings in the global codebook across"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "independent\ntask, our model also achieved competitive re-",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "the three datasets."
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "sults, demonstrating strong generalization capabilities. This",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "result fully demonstrates our model’s superiority.",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "to 128,\nFig. 5 (a) and (b), we set K1\nto 32, K2\nto 64, K3"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "Ablation Studies",
          "\u0000\u0015\u0000\u0013": "α to 0.2, β to 0.5, and γ to 1. The results indicate that\nthe"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "To thoroughly evaluate the impact of each module in our",
          "\u0000\u0015\u0000\u0013": "model’s performance is highly sensitive to the two groups of"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "model, we conducted ablation studies on the three datasets",
          "\u0000\u0015\u0000\u0013": "parameters related to the crucial codebook mechanism."
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "with ACC and F1 score in the subject-dependent scene.\nIn",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "Effectiveness of Codebook"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "these studies, we systematically removed individual modules",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "(the whole regional part,\nthe global part,\nthe intra-regional",
          "\u0000\u0015\u0000\u0013": "In this section, we discuss the effectiveness of the codebook."
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "part, and the inter-regional part)\nfrom our model\nto demon-",
          "\u0000\u0015\u0000\u0013": "We conducted a statistical analysis on the usage proportion"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "strate the effectiveness of each module. The results of the ab-",
          "\u0000\u0015\u0000\u0013": "of embeddings in the codebook for all samples from the first"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "lation experiments on the SEED-IV dataset are shown in Ta-",
          "\u0000\u0015\u0000\u0013": "subject across the three datasets. The x-axis represents em-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "ble 2, while the results on the other two datasets are provided",
          "\u0000\u0015\u0000\u0013": "bedding indices in the codebook, sorted by usage frequency."
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "in the appendix due to space limitations. From these results,",
          "\u0000\u0015\u0000\u0013": "The y-axis shows the percentage of samples using each em-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "we can draw the following findings: 1) Removing any mod-",
          "\u0000\u0015\u0000\u0013": "bedding. Due to space constraints, we only present\nthe re-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "ule leads to a decline in model performance, demonstrating",
          "\u0000\u0015\u0000\u0013": "sults of the global codebook in Fig 6, while the complete re-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "that each part plays a crucial role and that\nthey complement",
          "\u0000\u0015\u0000\u0013": "sults for all codebooks are included in the appendix.\nFrom"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "each other to achieve the best performance. 2) Compared to",
          "\u0000\u0015\u0000\u0013": "the results, we observe that for each dataset, a large number"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "removing the global part,\nremoving the regional part results",
          "\u0000\u0015\u0000\u0013": "of embeddings are utilized. This indicates that the codebook"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "in a more significant performance drop.\nThis suggests that",
          "\u0000\u0015\u0000\u0013": "has generated multiple distinct network structure representa-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "the regional module (both intra-regional and inter-regional)",
          "\u0000\u0015\u0000\u0013": "tions for different samples. Additionally,\nin each case, some"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "are more important.\nAlthough the global part\nis also cru-",
          "\u0000\u0015\u0000\u0013": "embeddings are used extensively, which we believe represent"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "cial, multi-layer and finer-grained graph modeling contributes",
          "\u0000\u0015\u0000\u0013": "common structural features in the EEG signals, while others"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "more to feature extraction.\n3) The results of\nremoving ei-",
          "\u0000\u0015\u0000\u0013": "are used sparingly, representing more individualized charac-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "ther the intra-regional or inter-regional components show that",
          "\u0000\u0015\u0000\u0013": "teristics of the network structure.\nIn future work, we plan to"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "both modules are meaningful, with the intra-regional module",
          "\u0000\u0015\u0000\u0013": "further explore the interpretability of these embeddings in the"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "being somewhat more important.",
          "\u0000\u0015\u0000\u0013": "codebook."
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "Hyperparameter Analysis",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "",
          "\u0000\u0015\u0000\u0013": "5\nConclusion and Future Work"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "In this section, we explore the selection of two critical groups",
          "\u0000\u0015\u0000\u0013": ""
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "of hyperparameters in the model.\nThe first group involves",
          "\u0000\u0015\u0000\u0013": "This work presents\nthe Multi-Granularity Integration Net-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "the sizes of\nthe codebooks in the three modules:\nthe global",
          "\u0000\u0015\u0000\u0013": "work with Discrete Codebook (MIND-EEG), a novel frame-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "codebook,\nintra-regional codebook, and inter-regional code-",
          "\u0000\u0015\u0000\u0013": "work for EEG-based emotion recognition.\nBy integrating"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "respectively.\nFor\nthis\nbook, denoted as K1, K2,\nand K3,",
          "\u0000\u0015\u0000\u0013": "spatial information across multiple levels and utilizing a dis-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "analysis, we vary one parameter\nfrom {8, 16, 32, 64, 128}",
          "\u0000\u0015\u0000\u0013": "crete codebook mechanism,\nthe proposed model effectively"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "while keeping the other\ntwo fixed.\nThe second group con-",
          "\u0000\u0015\u0000\u0013": "models the dynamic and diverse characteristics of brain net-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "sists of\nthe weights\nfor\nthe three codebook loss\nterms: α,",
          "\u0000\u0015\u0000\u0013": "works while addressing challenges\nsuch as overfitting and"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "β, and γ.\nSimilarly, we vary one weight within the range",
          "\u0000\u0015\u0000\u0013": "over-smoothing.\nExtensive\nexperiments\non\nthree\nbench-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1} while keeping the",
          "\u0000\u0015\u0000\u0013": "mark datasets demonstrate\nthe\nsuperiority of MIND-EEG"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "other\ntwo constants. The results of\nthese experiments, con-",
          "\u0000\u0015\u0000\u0013": "over existing methods in both subject-dependent and subject-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "ducted on the SEED-IV dataset under the subject-dependent",
          "\u0000\u0015\u0000\u0013": "independent\nscenarios, with significant\naccuracy improve-"
        },
        {
          "\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u001c\u0000\u0017": "scenario, are presented in Figure 5. Based on the results in",
          "\u0000\u0015\u0000\u0013": "ments. Analysis of the differences and utilization of network"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "structure representations within the codebook also validates": "the effectiveness of\nthis design.\nFuture work will\nfocus on",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Zhang, and Yuan Zong.\nA novel neural network model"
        },
        {
          "structure representations within the codebook also validates": "further exploring the relationship between brain network con-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "based on cerebral hemispheric asymmetry for eeg emotion"
        },
        {
          "structure representations within the codebook also validates": "nectivity and emotional states using the codebook to enhance",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "recognition.\nIn IJCAI, pages 1561–1567, 2018."
        },
        {
          "structure representations within the codebook also validates": "model interpretability.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Li et al., 2018b] Yang Li, Wenming Zheng, Yuan Zong,"
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Zhen Cui,\nTong Zhang,\nand Xiaoyan Zhou.\nA bi-"
        },
        {
          "structure representations within the codebook also validates": "References",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "hemisphere domain adversarial neural network model for"
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "IEEE Transactions on Affective\neeg emotion recognition."
        },
        {
          "structure representations within the codebook also validates": "[Bruder et al., 2017] Gerard E Bruder, Jonathan W Stewart,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Computing, 12(2):494–504, 2018."
        },
        {
          "structure representations within the codebook also validates": "and Patrick J McGrath.\nRight brain,\nleft brain in de-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "pressive disorders: clinical and theoretical implications of",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Li et al., 2019] Yang Li, Wenming Zheng, Lei Wang, Yuan"
        },
        {
          "structure representations within the codebook also validates": "behavioral,\nelectrophysiological and neuroimaging find-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Zong, and Zhen Cui.\nFrom regional\nto global brain: A"
        },
        {
          "structure representations within the codebook also validates": "ings.\nNeuroscience & Biobehavioral Reviews, 78:178–",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "novel hierarchical spatial-temporal neural network model"
        },
        {
          "structure representations within the codebook also validates": "191, 2017.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "IEEE Transactions on Affec-\nfor eeg emotion recognition."
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "tive Computing, 13(2):568–578, 2019."
        },
        {
          "structure representations within the codebook also validates": "[Chang et al., 2024] Yadong Chang, Xianwei Zheng, Yi-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "jun Chen, Xutao Li,\nand Qing Miao.\nSpatiotemporal",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Li et al., 2020] Yang Li, Lei Wang, Wenming Zheng, Yuan"
        },
        {
          "structure representations within the codebook also validates": "gated graph transformer\nfor eeg-based emotion recogni-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Zong, Lei Qi, Zhen Cui, Tong Zhang, and Tengfei Song. A"
        },
        {
          "structure representations within the codebook also validates": "tion.\nIEEE Signal Processing Letters,\n31:1630–1634,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "novel bi-hemispheric discrepancy model for eeg emotion"
        },
        {
          "structure representations within the codebook also validates": "2024.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "IEEE Transactions on Cognitive and Devel-\nrecognition."
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "opmental Systems, 13(2):354–367, 2020."
        },
        {
          "structure representations within the codebook also validates": "[Ding et al., 2022] Yi Ding, Neethu Robinson, Su Zhang,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "Qiuhao Zeng,\nand Cuntai Guan.\nTsception:\nCaptur-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Li et al., 2021] Rui Li, Yiting Wang, and Bao-Liang Lu. A"
        },
        {
          "structure representations within the codebook also validates": "ing temporal dynamics and spatial asymmetry from eeg",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "multi-domain adaptive graph convolutional network for"
        },
        {
          "structure representations within the codebook also validates": "IEEE Transactions on Affective\nfor emotion recognition.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "eeg-based emotion recognition. In ACM MM, pages 5565–"
        },
        {
          "structure representations within the codebook also validates": "Computing, 14(3):2238–2250, 2022.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "5573, 2021."
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Li et al., 2023] Yang Li,\nJi Chen, Fu Li, Boxun Fu, Hao"
        },
        {
          "structure representations within the codebook also validates": "[Ding et al., 2023] Yi Ding, Neethu Robinson, Chengxuan",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Wu, Youshuo Ji, Yijin Zhou, Yi Niu, Guangming Shi, and"
        },
        {
          "structure representations within the codebook also validates": "Tong, Qiuhao Zeng, and Cuntai Guan.\nLggnet: Learn-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Wenming Zheng.\nGmss: Graph-based multi-task self-"
        },
        {
          "structure representations within the codebook also validates": "ing\nfrom local-global-graph\nrepresentations\nfor\nbrain–",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "IEEE\nsupervised learning for eeg emotion recognition."
        },
        {
          "structure representations within the codebook also validates": "IEEE Transactions on Neural Net-\ncomputer\ninterface.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Transactions on Affective Computing, 14(03):2512–2525,"
        },
        {
          "structure representations within the codebook also validates": "works and Learning Systems, 35(7):9773–9786, 2023.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "2023."
        },
        {
          "structure representations within the codebook also validates": "[Du et al., 2022] Guanglong Du, Jinshao Su, Linlin Zhang,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Lindquist et al., 2012] Kristen A Lindquist, Tor D Wager,"
        },
        {
          "structure representations within the codebook also validates": "Kang Su, Xueqian Wang, Shaohua Teng, and Peter Xiaop-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Hedy Kober, Eliza Bliss-Moreau, and Lisa Feldman Bar-"
        },
        {
          "structure representations within the codebook also validates": "ing Liu. A multi-dimensional graph convolution network",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "rett. The brain basis of emotion: a meta-analytic review."
        },
        {
          "structure representations within the codebook also validates": "for eeg emotion recognition. IEEE Transactions on Instru-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Behavioral and brain sciences, 35(3):121–143, 2012."
        },
        {
          "structure representations within the codebook also validates": "mentation and Measurement, 71:1–11, 2022.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Liu et al., 2019] Suyuan\nLiu, Wenming\nZheng,\nTengfei"
        },
        {
          "structure representations within the codebook also validates": "[Duan et al., 2013] Ruo-Nan Duan,\nJia-Yi Zhu,\nand Bao-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Song, and Yuan Zong.\nSparse graphic attention lstm for"
        },
        {
          "structure representations within the codebook also validates": "Liang Lu. Differential entropy feature for eeg-based emo-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "eeg emotion recognition.\nIn ICONIP, pages 690–697,"
        },
        {
          "structure representations within the codebook also validates": "tion classification.\nIn NER, pages 81–84, 2013.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "2019."
        },
        {
          "structure representations within the codebook also validates": "[Hagmann et al., 2008] Patric Hagmann, Leila Cammoun,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Liu et al., 2021] Wei Liu,\nJie-Lin Qiu, Wei-Long Zheng,"
        },
        {
          "structure representations within the codebook also validates": "Xavier Gigandet, Reto Meuli, Christopher J Honey, Van J",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "and Bao-Liang Lu. Comparing recognition performance"
        },
        {
          "structure representations within the codebook also validates": "Wedeen, and Olaf Sporns. Mapping the structural core of",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "and robustness of multimodal deep learning models\nfor"
        },
        {
          "structure representations within the codebook also validates": "human cerebral cortex. PLoS biology, 6(7):e159, 2008.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "IEEE Transactions on\nmultimodal emotion recognition."
        },
        {
          "structure representations within the codebook also validates": "[He et al., 2016] Kaiming He, Xiangyu Zhang,\nShaoqing",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Cognitive\nand Developmental\nSystems,\n14(2):715–729,"
        },
        {
          "structure representations within the codebook also validates": "Ren, and Jian Sun. Deep residual learning for image recog-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "2021."
        },
        {
          "structure representations within the codebook also validates": "nition.\nIn CVPR, pages 770–778, 2016.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Liu et al., 2022]\nJiyao Liu, Hao Wu, Li Zhang, and Yanxi"
        },
        {
          "structure representations within the codebook also validates": "[Hu et al., 2018]\nJie Hu, Li Shen, and Gang Sun. Squeeze-",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Zhao.\nSpatial-temporal\ntransformers\nfor\neeg\nemotion"
        },
        {
          "structure representations within the codebook also validates": "and-excitation networks.\nIn CVPR, pages 7132–7141,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "recognition.\nIn ICAAI, pages 116–120, 2022."
        },
        {
          "structure representations within the codebook also validates": "June 2018.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Liu et al., 2023] Tong Liu,\nJing Li,\nJia Wu, Lefei Zhang,"
        },
        {
          "structure representations within the codebook also validates": "[Jiao et al., 2019] Zhicheng Jiao, Haoxuan You, Fan Yang,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Shanshan Zhao, Jun Chang, and Jun Wan. Cross-domain"
        },
        {
          "structure representations within the codebook also validates": "Xin Li, Han Zhang, and Dinggang Shen. Decoding eeg by",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "facial\nexpression\nrecognition\nvia\ndisentangling\nidentity"
        },
        {
          "structure representations within the codebook also validates": "visual-guided deep neural networks.\nIn IJCAI, volume 28,",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "representation.\nIn IJCAI, pages 1213–1221, 2023."
        },
        {
          "structure representations within the codebook also validates": "pages 1387–1393, 2019.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": ""
        },
        {
          "structure representations within the codebook also validates": "",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "[Liu et al., 2024] Huan Liu, Tianyu Lou, Yuzhe Zhang, Yix-"
        },
        {
          "structure representations within the codebook also validates": "[Jin et al., 2024] Ming Jin, Changde Du, Huiguang He, Ting",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "iao Wu, Yang Xiao, Christian S Jensen, and Dalin Zhang."
        },
        {
          "structure representations within the codebook also validates": "Cai, and Jinpeng Li. Pgcn: Pyramidal graph convolutional",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "Eeg-based multimodal emotion recognition: A machine"
        },
        {
          "structure representations within the codebook also validates": "IEEE Transactions\nnetwork for eeg emotion recognition.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "IEEE Transactions on Instrumenta-\nlearning perspective."
        },
        {
          "structure representations within the codebook also validates": "on Multimedia, 26:9070–9082, 2024.",
          "[Li et al., 2018a] Yang Li, Wenming Zheng, Zhen Cui, Tong": "tion and Measurement, 73, 2024."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Yu Ouyang, Zhe Jia, Chu Wang, and Hong Zeng. Msfr-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "Investigating critical\nfrequency bands\nand channels\nfor"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "gcn: A multi-scale feature reconstruction graph convolu-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "eeg-based emotion recognition with deep neural networks."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "tional network for eeg emotion and cognition recognition.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "IEEE Transactions on autonomous mental development,"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "IEEE Transactions on Neural Systems and Rehabilitation",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "7(3):162–175, 2015."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Engineering, 31:3245–3254, 2023.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "[Zheng and Lu, 2015b] Wei-Long Zheng and Bao-Liang Lu."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Song et al., 2018] Tengfei\nSong, Wenming Zheng,\nPeng",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "Investigating critical\nfrequency bands\nand channels\nfor"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Song,\nand Zhen Cui.\nEeg emotion recognition using",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "eeg-based emotion recognition with deep neural networks."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "IEEE\ndynamical graph convolutional neural networks.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "IEEE Transactions on autonomous mental development,"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Transactions\non\nAffective Computing,\n11(3):532–541,",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "7(3):162–175, 2015."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "2018.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "[Zheng et al., 2014] Wei-Long\nZheng,\nJia-Yi\nZhu,\nYong"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Song et al., 2019] Tengfei Song, Wenming Zheng, Cheng",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "Peng, and Bao-Liang Lu. Eeg-based emotion classification"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Lu, Yuan Zong, Xilei Zhang, and Zhen Cui. Mped: A",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "using deep belief networks.\nIn ICME, pages 1–6, 2014."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "multi-modal physiological emotion database for discrete",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "[Zheng et al., 2018] Wei-Long Zheng, Wei Liu, Yifei Lu,"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "emotion recognition. IEEE Access, 7:12177–12191, 2019.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "Bao-Liang Lu,\nand Andrzej Cichocki.\nEmotionmeter:"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Song et al., 2020] Tengfei\nSong,\nSuyuan Liu, Wenming",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "A multimodal\nframework for\nrecognizing human emo-"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Zheng, Yuan Zong, and Zhen Cui. Instance-adaptive graph",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "tions.\nIEEE transactions on cybernetics,\n49(3):1110–"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "for eeg emotion recognition.\nIn AAAI, volume 34, pages",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "1122, 2018."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "2701–2708, 2020.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "[Zheng, 2016] Wenming Zheng.\nMultichannel\neeg-based"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Song et al., 2021] Tengfei\nSong,\nSuyuan Liu, Wenming",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "emotion recognition via group sparse canonical correla-"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Zheng, Yuan Zong, Zhen Cui, Yang Li,\nand Xiaoyan",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "IEEE Transactions on Cognitive and Devel-\ntion analysis."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Zhou. Variational instance-adaptive graph for eeg emotion",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "opmental Systems, 9(3):281–290, 2016."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "recognition.\nIEEE Transactions on Affective Computing,",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "[Zhong et al., 2020] Peixiang Zhong, Di Wang, and Chun-"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "14(1):343–356, 2021.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "yan Miao. Eeg-based emotion recognition using regular-"
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Van Den Oord et al., 2017] Aaron Van Den Oord, Oriol",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "IEEE Transactions on Affec-\nized graph neural networks."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Vinyals,\net al.\nNeural discrete representation learning.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": "tive Computing, 13(3):1290–1301, 2020."
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Advances\nin neural\ninformation processing systems, 30,",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "2017.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Wang et al., 2024] Yiming Wang, Bin Zhang,\nand Yujiao",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Tang.\nDmmr: Cross-subject domain generalization for",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "eeg-based emotion recognition via denoising mixed mu-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "tual reconstruction.\nIn AAAI, volume 38, pages 628–636,",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "2024.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Woo et al., 2018] Sanghyun Woo,\nJongchan\nPark,\nJoon-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Young Lee, and In So Kweon. Cbam: Convolutional block",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "attention module.\nIn ECCV, pages 3–19, 2018.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Yan et al., 2024] Huachao Yan, Kailing Guo, Xiaofen Xing,",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "and Xiangmin Xu.\nBridge graph attention based graph",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "convolution network with multi-scale transformer for eeg",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "IEEE Transactions\non Affective\nemotion\nrecognition.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Computing, 15(4):2042–2054, 2024.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Zeng et al., 2022] Hong Zeng, Qi Wu, Yanping Jin, Haohao",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Zheng, Mingming Li, Yue Zhao, Hua Hu, and Wanzeng",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Kong.\nSiam-gcan: A siamese graph convolutional atten-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "IEEE Transac-\ntion network for eeg emotion recognition.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "tions on Instrumentation and Measurement, 71:1–9, 2022.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Zhang et al., 2022] Yuzhe Zhang, Huan Liu, Dalin Zhang,",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Xuxu Chen, Tao Qin,\nand Qinghua Zheng.\nEeg-based",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "emotion recognition with emotion localization via hierar-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "chical self-attention. IEEE Transactions on Affective Com-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "puting, 14(3):2458–2469, 2022.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "[Zhao et al., 2018] Sicheng Zhao, Guiguang Ding, Jungong",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "Han, and Yue Gao. Personality-aware personalized emo-",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "tion recognition from physiological\nsignals.\nIn IJCAI,",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        },
        {
          "[Pan et al., 2023] Deng\nPan, Haohao Zheng,\nFeifan Xu,": "pages 1660–1667, 2018.",
          "[Zheng and Lu, 2015a] Wei-Long Zheng and Bao-Liang Lu.": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Right brain, left brain in depressive disorders: clinical and theoretical implications of behavioral, electrophysiological and neuroimaging findings",
      "authors": [
        "Bruder"
      ],
      "year": "2017",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "2",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Lggnet: Learning from local-global-graph representations for braincomputer interface",
      "authors": [
        "Ding"
      ],
      "year": "2008",
      "venue": "Xiangyu Zhang, Shaoqing Ren, and Jian Sun"
    },
    {
      "citation_id": "4",
      "title": "Decoding eeg by visual-guided deep neural networks",
      "authors": [
        "Jiao"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "5",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "6",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "7",
      "title": "A multi-domain adaptive graph convolutional network for eeg-based emotion recognition",
      "authors": [
        "Li"
      ],
      "year": "2021",
      "venue": "ACM MM"
    },
    {
      "citation_id": "8",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "Li"
      ],
      "year": "2012",
      "venue": "ICONIP"
    },
    {
      "citation_id": "9",
      "title": "Msfrgcn: A multi-scale feature reconstruction graph convolutional network for eeg emotion and cognition recognition",
      "authors": [
        "Pan"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "10",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning",
      "authors": [
        "Oord Van Den"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "Dmmr: Cross-subject domain generalization for eeg-based emotion recognition via denoising mixed mutual reconstruction",
      "authors": [
        "Wang"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based emotion recognition with emotion localization via hierarchical self-attention",
      "authors": [
        "Zeng"
      ],
      "year": "2022",
      "venue": "Tao Qin, and Qinghua Zheng"
    },
    {
      "citation_id": "14",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Zhao"
      ],
      "year": "2015",
      "venue": "IJCAI"
    },
    {
      "citation_id": "15",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Lu ; Wei-Long Zheng",
        "Bao-Liang Zheng",
        "; Lu",
        "Zheng"
      ],
      "year": "2014",
      "venue": "ICME"
    },
    {
      "citation_id": "16",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "Wenming Zheng",
        "; Zheng",
        "Zhong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    }
  ]
}