{
  "paper_id": "2501.07213v1",
  "title": "Multi-Face Emotion Detection For Effective Human-Robot Interaction",
  "published": "2025-01-13T11:12:47Z",
  "authors": [
    "Mohamed Ala Yahyaoui",
    "Mouaad Oujabour",
    "Leila Ben Letaifa",
    "Amine Bohi"
  ],
  "keywords": [
    "Emotion detection",
    "Facial expression recognition",
    "Human-Robot Interaction",
    "Deep learning",
    "Graphical user interface"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The integration of dialogue interfaces in mobile devices has become ubiquitous, providing a wide array of services. As technology progresses, humanoid robots designed with human-like features to interact effectively with people are gaining prominence, and the use of advanced human-robot dialogue interfaces is continually expanding. In this context, emotion recognition plays a crucial role in enhancing human-robot interaction by enabling robots to understand human intentions. This research proposes a facial emotion detection interface integrated into a mobile humanoid robot, capable of displaying real-time emotions from multiple individuals on a user interface. To this end, various deep neural network models for facial expression recognition were developed and evaluated under consistent computer-based conditions, yielding promising results. Afterwards, a trade-off between accuracy and memory footprint was carefully considered to effectively implement this application on a mobile humanoid robot. a",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The rapid advancement of technology in recent years has accelerated research in robotics, with a particular emphasis on humanoid robots. Designed to resemble humans in body, hands, and head, humanoid robots are increasingly capable of sophisticated interactions with people, including recognizing individuals and responding to commands. This human-like form and behavior make them particularly well-suited for applications in human-computer interaction, serving as effective platforms for studying and improving user engagement and interaction dynamics. Current examples of humanoid robots include Honda's ASIMO  (Hirose and Ogawa, 2007) , known for its advanced mobility and dexterity; Blue Frog Robotics' Buddy  (Peltier and Fiorini, 2017) , designed for social interaction and domestic assistance; and Aldebaran Robotics' NAO  (Gouaillier et al., 2009) , recognized for its versatility in research and educational settings. These robots showcase the diversity of roles humanoid robots can play, from companionship and entertainment to education and beyond.\n\nDeveloping emotional intelligence in robots is relevant as they increasingly participate in social set-tings. Indeed, beyond performing physical tasks, enhancing robots' ability to perceive, interpret and respond to human needs is essential for effective social Human-Robot Interaction (HRI) and Human-Robot Collaboration (HRC).\n\nIn the realm of social robotics, integrating sensors such as microphone for \"mouth\" or camera for 'eyes' into the humanoid robot, enables the robot to capture human emotions in real-time, and to adapt its response and behavior accordingly  (Justo et al., 2020; Olaso et al., 2021; Palmero et al., 2023) . This capability enhances their utility in various applications and facilitates engagement and intuitive interaction experiences between robots and humans. Detecting emotions from camera starts with face detection, which involves identifying and locating human faces within images or video frames. This process includes preprocessing images, extracting distinct facial features, classifying regions as faces or non-faces, refining detection accuracy, and handling variations in lighting, occlusions, poses and scales. Face emotion recognition (FER) employs computer vision and machine learning techniques to analyze human emotions from face.\n\nOften, emotion recognition systems deals with only one user while he is communicating with a machine. However, multiple users can communicate si-multaneously with it. Multi-face emotion recognition is particularly valuable across various scenarios. For instance, at a comedy club, it provides real-time feedback to comedians, manages lighting and sound, interacts with the audience, and detects disruption.\n\nIn this work, we present a complete facial emotion recognition interface and its deployment in a mobile humanoid robot. The proposed interface can display emotions from multiple individuals in real-time within an advanced user interface. To achieve this, several deep neural network models have been developed and evaluated under the same conditions. Then a tradeoff between system accuracy and model size have been considered in order to implement the optimal solution into a humanoid robot. The model's performance and its confidence interval also guided this choice of solution.\n\nThe remainder of this paper is structured as follows. Section 2 reviews the state of the art related to emotion detection for Human-Robot Interaction (HRI) and Facial Emotion Recognition (FER) systems. Section 3 presents the design and implementation of the proposed emotional interface, detailing the multi-face detection, emotion recognition system, and the graphical user interface. Section 4 describes the integration of the facial emotion recognition system into the Tiago++ humanoid robot, highlighting the processes of face tracking and real-time emotion detection. Section 5 outlines the experimental setup and presents the results, including performance metrics, model comparisons, and user interaction analysis. Finally, Section 6 concludes the paper with a discussion of the findings, limitations of the current approach, and potential directions for future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Although emotions have been investigated in the context of HRI, it remains a significant challenge. In this section, we report recent research in HRI as well as FER systems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Detection For Hri",
      "text": "In social robotics, emotion detection is mimicked by robots to interact naturally and harmoniously with humans. Several studies have focused on implementing facial emotion recognition in robots. For instance, the study  (Zhao et al., 2020)  applied facial emotion recognition on three datasets: FER2013, FERPLUS and FERFIN. The system was implemented on a NAO robot, which responds with actions based on the detected emotions. However, this study has some limita-tions, as it does not provide details on the robot's implementation. Additionally, the research  (Dwijayanti et al., 2022)  integrated a facial detection system with a facial emotion recognition system and implemented it in a robot. They also explored automatic detection of the distance between the camera of the robot and the person. One drawback is that the robot is stationary, so mobility is not considered. The study  (Spezialetti et al., 2020)  serves as a survey of emotion recognition research for human-robot interaction. It reviews emotion recognition models, datasets, and modalities, with a particular emphasis on facial emotion recognition. However, it does not include any research utilizing deep learning models for facial emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Facial Emotion Recognition",
      "text": "Deep learning has revolutionized computer vision tasks, including Facial Emotion Recognition (FER), with numerous studies proposing various methodologies to achieve high classification accuracy using well known benchmark datasets  (Farhat et al., 2024; Goodfellow et al., 2013; Letaifa et al., 2019; Mollahosseini et al., 2017; Justo et al., 2021; Lucey et al., 2010) .\n\nSeveral recent studies have proposed innovative approaches for FER. Farzaneh et al.  (Farzaneh and Qi, 2021)  introduced the Deep Attentive Center Loss (DACL) method, which integrates an attention mechanism to enhance feature discrimination, showing superior performance on RAF-DB and Af-fectNet datasets. Similarly,  Pecoraro et al. (Pecoraro et al., 2022)  proposed the LHC-Net architecture, which employs a multi-head self-attention module tailored for FER tasks, achieving state-of-the-art results on FER2013 with lower computational complexity. In another work, Han et al.  (Han et al., 2022)  presented a triple-structure network model based on MobileNet V1, which captures inter-class and intraclass diversity features, demonstrating strong results on KDEF, MMI, and CK+ datasets. Fard et al.  (Fard and Mahoor, 2022)  introduced the Adaptive Correlation (Ad-Corre) Loss, which improved performance on AffectNet, RAF-DB, and FER2013 datasets when applied to Xception and ResNet50 models. Other notable contributions include the Segmentation VGG-19 model  (Vignesh et al., 2023) , which enhanced FER on FER2013 using segmentation-inspired blocks, and the DDAMFN network by  Zhang et al. (Zhang et al., 2023) , which incorporated dual-direction attention to achieve excellent results on AffectNet and FERPlus. Lastly, in our recent work, we introduced EmoNeXt (El Boudouri and Bohi, 2023), a deep learning framework that has set new state-of-the-art benchmarks on the FER2013 dataset. EmoNeXt integrates a Spatial Transformer Network (STN) for handling facial alignment variations, along with Squeeze-and-Excitation (SE) blocks for channel-wise feature recalibration. Additionally, a self-attention regularization term was introduced to enhance compact feature generation, further improving accuracy.\n\nThis brief review shows that many FER models have focused exclusively on improving accuracy. As a result, today's leading models can reach memory sizes in the order of gigabytes, which poses challenges for deployment in memory-constrained environments, such as the robots.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "The Emotional Interface",
      "text": "One of the challenges in the domain of emotion detection for HRI, is the simultaneous detection of emotions from multiple faces, which is useful where robots interact with groups of people.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Face Detection",
      "text": "We choose the Haarcascade classifier, proposed by Paul Viola and Michael Jones in their seminal paper  (Viola and Jones, 2001) , as a highly effective method for face detection. Other notable methods include the Histogram of Oriented Gradients (HOG) combined with Support Vector Machines (SVM) and deep learning approaches such as the Multi-task Cascaded Convolutional Networks (MTCNN). While these methods have shown promising results in various applications, the Haarcascade classifier is particularly advantageous for real-time scenarios.\n\nThe general principle of the Haarcascade approach is illustrated in Figure  1 . This machine learning-based method involves training a cascade function using a large dataset of positive (face) and negative (non-face) images. The classifier relies on Haar features, which are similar to convolutional kernels, to extract distinguishing characteristics from images. Each Haar feature is a single value calculated by subtracting the sum of pixels under a white rectangle from the sum of pixels under a black rectangle. To efficiently compute these features, the concept of integral images is utilized, reducing the calculation to an operation involving just four pixels, regardless of the feature's size.\n\nDuring training, all possible sizes and positions of these features are applied to the training images, resulting in over 160,000 potential features. To select the most relevant features, the AdaBoost algorithm is utilized, which iteratively adjusts the weights of mis-   (Kim et al., 2015) .\n\nclassified images and selects features with the lowest error rates, thereby creating a strong classifier from a combination of weak classifiers. Despite the high initial number of features, this process narrows it down significantly (e.g., from 160,000 to around 6,000).\n\nFor detection, the image is scanned with a 24x24 pixel window, applying these selected features. To enhance efficiency, the authors introduced a cascade of classifiers. This means that features are grouped into stages, and if a window fails at any stage, it is immediately discarded as a non-face region. This hierarchical approach ensures that only potential face regions undergo the full, more complex evaluation process, allowing for real-time face detection with high accuracy.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Recognition System",
      "text": "Pretrained deep learning models have demonstrated exceptional effectiveness for feature extraction across various domains  (Palmero et al., 2023) . In our emotion recognition system (Figure . 2), we leverage a pretrained convolutional neural network (CNN) model to apply transfer learning using the FER2013 dataset  (Goodfellow et al., 2013) . Specifically, we utilize pretrained CNN models, initially trained on the ImageNet dataset which encompass millions of images from various categories  (Deng et al., 2009) . This extensive training enables these models to extract highly relevant and general visual features through their convolutional layers. These layers detect fundamental elements such as edges, textures, and shapes, which are essential for understanding facial structures. We utilize these convolutional layers to process our input images, leaving out the top portion of the model, specifically the fully connected layers initially designed for the ImageNet classification tasks. Instead, by passing our facial images through the pretrained model's convolutional layers, we generate a feature stack that encapsulates essential visual information. This feature stack, representing a rich set of features extracted from the images, is then flattened into a format suitable for further processing. Subsequently, we introduce additional fully connected lay-ers tailored to the FER2013 dataset to recognize and classify seven distinct emotions: anger, disgust, fear, happiness, sadness, surprise, and neutrality.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Graphical Interface",
      "text": "The graphical interface of our emotion recognition system integrates multiple advanced technologies to provide a seamless and responsive user experience. Upon launching the application, the interface is built using the Tkinter library 1 , creating a user-friendly graphical environment. The system activates the webcam through the OpenCV library 2 , capturing a live video feed for real-time analysis. Captured video frames undergo face detection using the HaarCascade classifier, a robust method for identifying faces under various lighting conditions and angles (see descrip-1 https://docs.python.org/3/library/tkinter.html 2 https://docs.opencv.org/4.x/ tion in subsection 3.1).\n\nOnce a face is detected, the region of interest is extracted and subjected to preprocessing to ensure compatibility with the model's input size. The processed image is then fed into a pretrained CNN model that have been fine-tuned on the FER2013 dataset. This model analyzes the facial image to predict the user's emotional state, categorizing it into distinct emotions such as anger, fear, disgust, happiness, sadness, surprise, and neutrality. The predicted emotion is then displayed on the graphical interface, providing immediate feedback to the user. All these steps are illustrated by Fig.  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Humanoid Robot",
      "text": "The Tiago robot, developed by PAL Robotics  (Pages et al., 2016) , is a humanoid mobile robot 3 . Its modular design allows for customization to meet specific needs. In this section, we outline our approach to equipping the Tiago++ model of the robot with face emotion recognition capabilities. By using the Robot Operating System (ROS) 4  for communication and processing, and integrating a Tkinter-based GUI for real-time visualization, we enhance the ability of the robot to interact with humans. This implementation is divided into two primary tasks: face tracking and emotion detection, each described in the following subsections.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Face Tracking Integration On Tiago++ Robot",
      "text": "We implemented a face tracking module on the Tiago robot by integrating ROS with a Tkinterbased GUI application. The process begins with initializing a ROS node named Tiago FER and setting up essential publishers and subscribers to facilitate communication between the robot and the software. We use the CvBridge 5  library to convert images from ROS format to OpenCV format. Meanwhile, the MediaPipeRos instance processes these images to detect regions of interest (ROI) for face tracking. The application's main loop receives images from the robot's camera through the /xtion/rgb/image ROS topic, processes these images to detect faces, and generates commands to adjust the robot's yaw and pitch. These commands, which control head movements, are published to the head controller/increment/goal topic using the IncrementActionGoal message type, enabling the robot to track the detected faces. These steps are outlined in the diagram generated by ROS, as shown in Figure  4 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Emotion Detection And Gui Display On Tiago++ Screen",
      "text": "Following face tracking, the processed images are analyzed to predict emotions. The detected emotions are displayed on a Tkinter GUI, which features a canvas for image display and progress bars to visualize emotion scores. The processed images and emotion data are published back to the /imagesBack ROS topic. Additionally, incremental commands for torso movements are sent to the /torso controller/safe command topic using the JointTrajectory message type, allowing the robot to dynamically respond to detected emotions (see Fig-",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments And Results",
      "text": "Developing a human-robot interface for FER involves detecting faces and emotions, implementing the user interface, and integrating it into the robot platform. The robot's camera captures images of individuals interacting with it, processes these images to detect emotions, and then displays the detected emotions on the user interface. This interface is visible on the tablet mounted on the robot's chest. Several challenges are to be addressed, particularly focusing on the accuracy of the models and the feasibility of implementing them on the robot.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Face Emotion Detection",
      "text": "In this work, we fine-tuned several pretrained models from the Keras library 6  , initially trained on the ImageNet 1000K dataset. These models were selected based on their strong performance in the Im-ageNet classification task and their ability to generalize well for FER tasks. We applied transfer learning, as explained in subsection 3.2, to the following models: MobileNet  (Howard et al., 2017) , DenseNet201  (Huang et al., 2017) , ResNet152V2  (He et al., 2016b) , ResNet101  (He et al., 2016a) , Xception  (Chollet, 2017) , EfficientNetV2-B0 (Tan and Le, 2021), In-ceptionResNetV2 and InceptionV3  (Szegedy et al., 2017) , VGG16 and VGG19  (Karen, 2014) , and Con-vNeXt (from Tiny to XLarge version)  (Liu et al., 2022) .\n\nFor training, we consistently used data augmentation techniques such as rotation, shift, zoom, horizontal flip and adjustments in brightness and contrast to improve the model's robustness. Additionally, Random Erasing was used to simulate occlusions, while resizing and recropping variations improved robustness to differences in face positioning. The models were optimized using Adam with a learning rate of 0.0001, combined with strategies like EarlyStopping and ReduceLROnPlateau to prevent overfitting and dynamically adjust the learning rate.\n\nThe accuracy and memory footprint of each finetuned model on the FER2013 dataset are reported in Table  1 . While ConvNeXt XLarge achieved the highest accuracy at 72.27%, it comes with a significantly larger memory footprint than the other models.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Confidence Interval",
      "text": "Accuracy is an estimate of the performance of a system, and its reliability depends on the number of tests   conducted that is in our case the number of emotions to be recognized. The measurement of the confidence interval is introduced to assess the trustability of our recognition rate. In  (Zouari, 2007) , the successes are modeled by a binomial distribution. If N is the number of tests and P is the recognition rate, then the confidence interval [P-, P+] at x% is: with z95% = 1.96 and z98% = 2.33. This means that there is a x% chance that the rate falls within the interval [P-, P+].\n\nThe FER2013 dataset consists in 35,887 grayscale images, divided into training (80%), test (10%) and validation (10%). Hence, using each model 3589 samples have been evaluated on the test set. We compute the confidence interval with z98 for all models and report the results in Figure  5 . We notice that several models, including VGG16, Inception-ResNetV2, ConvNeXt Base, EfficientNetV2-B0, and VGG19, show overlapping results. In terms of precision, these models demonstrate similar efficiency. However, there is a notable difference in their sizes, with EfficientNetV2-B0 being the most compact. Due to its smaller size, EfficientNetV2-B0 has been chosen for implementation on the robot.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "User Interface Development",
      "text": "Our emotion recognition application features an intuitive and user-friendly graphical interface designed for both single-face and multi-face emotion detection. The interface allows users to utilize their device's camera to capture live video streams, which are then processed in real-time to detect and classify facial expressions. For single-face emotion recognition, the application highlights the detected face and displays the identified emotion with corresponding confidence levels. In multi-face scenarios, the interface efficiently detects multiple faces within the same frame, assigning emotions to each detected face individually. The results are visually presented using bounding boxes and emotion labels directly on the video feed, providing clear and immediate feedback. Additionally, the interface includes progress bars for the detected emotion, visually representing the confidence level of each prediction. An avatar further enhances user interaction by imitating the predicted emotion in real-time, offering an engaging and dynamic way to understand the results. This comprehensive and interactive interface ensures that users can easily interpret the emotion detection outcomes, mak-ing the application practical for various real-world settings, including human-robot interaction and affective computing.  Figure. 6 and Figure. 7  show some examples of the user interface applied to single and multi-face emotion detection.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Fer Deployment On Tiago++ Robot",
      "text": "The Tiago++ is a humanoid mobile robot with constrained resources (CPU, memory, and storage). Besides interacting with humans, the robot must concurrently perform critical tasks such as navigation and detection, which are also resource-intensive. Consequently, for deploying our application on the Tiago++ robot, it is essential to select a model not only based on its test accuracy but also on the memory footprint of the model. The Tiago++ robot has a maximum capacity of about 150 MB for model files to ensure realtime inference without disrupting other processes running on the robot. According to Table  1  and the previous subsection, EfficientNetV2-B0 stands out with a good balance between accuracy (70.00%) and model size (139 MB), meeting the robot's constraints.\n\nTo illustrate the system's effectiveness, we conducted two sets of experiments. In the first set, a single participant interacted with the robot, displaying a range of emotions. The system's ability to accurately detect the face and classify the emotional state of the participant in real-time was meticulously observed and documented. In the second set, two participants were present simultaneously, engaging in various interactions with the robot. This scenario tested the system's robustness in detecting multiple faces",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we presented a facial emotion detection interface implemented on a mobile humanoid robot. This interface is capable of displaying emotions from multiple individuals in real-time video. To achieve this, we developed and evaluated several deep neural network models under consistent conditions, carefully considering factors such as model size and accuracy to ensure compatibility with both personal computers and mobile robots like the Tiago++.\n\nWhile our system demonstrates strong performance, it is important to note the limitations of relying solely on facial expressions for emotion detection, particularly in contexts where communication may be impaired. Emotions are complex and multifaceted, often requiring the integration of multiple modalities for more accurate recognition. Therefore, future work will focus on incorporating additional modalities, such as voice, text, gestures, and biosignals, to enhance the performance and reliability of emotion recognition systems. Additionally, we will focus on optimizing large models used in FER tasks to ensure their efficiency for deployment on the Tiago++ robot, considering the balance between model size and accuracy.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This machine",
      "page": 3
    },
    {
      "caption": "Figure 1: Cascade structure for Haar classifiers (Kim et al.,",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture of the emotion recognition sys-",
      "page": 4
    },
    {
      "caption": "Figure 3: Figure 3: Global architecture of our real-time multi-face",
      "page": 4
    },
    {
      "caption": "Figure 4: ROS-based Tiago++ face emotion recognition integration process: the diagram in the left (a) depicts the steps",
      "page": 6
    },
    {
      "caption": "Figure 5: Accuracy and confidence intervals of the models",
      "page": 6
    },
    {
      "caption": "Figure 6: The user interface displays face and emotion de-",
      "page": 7
    },
    {
      "caption": "Figure 7: Face detection is followed by emotion detection",
      "page": 7
    },
    {
      "caption": "Figure 8: Multi face emotion detection deployed on robot",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Pretrained models fine-tuned on the FER2013",
      "data": [
        {
          "MobileNet": "ResNet152V2",
          "66.11": "67.28",
          "14.5": "611.3"
        },
        {
          "MobileNet": "DenseNet201",
          "66.11": "67.84",
          "14.5": "221.0"
        },
        {
          "MobileNet": "InceptionV3",
          "66.11": "68.43",
          "14.5": "268.6"
        },
        {
          "MobileNet": "Xception",
          "66.11": "68.93",
          "14.5": "346.9"
        },
        {
          "MobileNet": "ConvNeXt Tiny",
          "66.11": "69.43",
          "14.5": "362"
        },
        {
          "MobileNet": "EfficientNetV2-B0",
          "66.11": "70.00",
          "14.5": "139.0"
        },
        {
          "MobileNet": "ConvNeXt Small",
          "66.11": "70.15",
          "14.5": "566"
        },
        {
          "MobileNet": "InceptionResNetV2",
          "66.11": "70.29",
          "14.5": "648.2"
        },
        {
          "MobileNet": "ConvNeXt Base",
          "66.11": "70.32",
          "14.5": "1120"
        },
        {
          "MobileNet": "VGG16",
          "66.11": "71.18",
          "14.5": "171.0"
        },
        {
          "MobileNet": "ResNet101",
          "66.11": "71.30",
          "14.5": "549.8"
        },
        {
          "MobileNet": "VGG19",
          "66.11": "71.46",
          "14.5": "262.5"
        },
        {
          "MobileNet": "ConvNeXt Large",
          "66.11": "71.57",
          "14.5": "2733"
        },
        {
          "MobileNet": "ConvNeXt XLarge",
          "66.11": "72.27",
          "14.5": "3900"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Xception: Deep learning with depthwise separable convolutions",
      "authors": [
        "F Chollet"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "Realtime implementation of face recognition and emotion recognition in a humanoid robot using a convolutional neural network",
      "authors": [
        "S Dwijayanti",
        "M Iqbal",
        "B Suprapto"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "Emonext: an adapted convnext for facial emotion recognition",
      "authors": [
        "Y El Boudouri",
        "A Bohi"
      ],
      "year": "2023",
      "venue": "2023 IEEE 25th International Workshop on Multimedia Signal Processing (MMSP)"
    },
    {
      "citation_id": "5",
      "title": "Ad-corre: Adaptive correlation-based loss for facial expression recognition in the wild",
      "authors": [
        "A Fard",
        "M Mahoor"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Cg-mer: a card game-based multimodal dataset for emotion recognition",
      "authors": [
        "N Farhat",
        "A Bohi",
        "L Letaifa",
        "R Slama"
      ],
      "year": "2024",
      "venue": "Sixteenth International Conference on Machine Vision (ICMV 2023)"
    },
    {
      "citation_id": "7",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "A Farzaneh",
        "X Qi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "8",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "Neural information processing. 20th international conference"
    },
    {
      "citation_id": "9",
      "title": "Mechatronic design of nao humanoid",
      "authors": [
        "D Gouaillier",
        "V Hugel",
        "P Blazevic",
        "C Kilner"
      ],
      "year": "2009",
      "venue": "IEEE International Conference on Robotics and Automation ICRA"
    },
    {
      "citation_id": "10",
      "title": "A triplestructure network model based upon mobilenet v1 and multi-loss function for facial expression recognition",
      "authors": [
        "B Han",
        "M Hu",
        "X Wang",
        "F Ren"
      ],
      "year": "2022",
      "venue": "Symmetry"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Identity mappings in deep residual networks",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference, Amsterdam"
    },
    {
      "citation_id": "13",
      "title": "Honda humanoid robots development",
      "authors": [
        "M Hirose",
        "K Ogawa"
      ],
      "year": "1850",
      "venue": "Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences"
    },
    {
      "citation_id": "14",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "arxiv": "arXiv:1704.04861"
    },
    {
      "citation_id": "15",
      "title": "Densely connected convolutional net-works",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "A spanish corpus for talking to the elderly. Conversational Dialogue Systems for the Next Decade",
      "authors": [
        "R Justo",
        "L Letaifa",
        "J Olaso",
        "A López-Zorrilla",
        "M Develasco",
        "A Vázquez",
        "M Torres"
      ],
      "year": "2021",
      "venue": "A spanish corpus for talking to the elderly. Conversational Dialogue Systems for the Next Decade"
    },
    {
      "citation_id": "17",
      "title": "Analysis of the interaction between elderly people and a simulated virtual coach",
      "authors": [
        "R Justo",
        "L Letaifa",
        "C Palmero",
        "E Fraile",
        "A Johansen",
        "A Vazquez",
        "G Cordasco",
        "S Schlogl",
        "B Ruanova",
        "M Silva",
        "S Escalera",
        "M Velasco",
        "J Laranga",
        "A Esposito",
        "M Kornes",
        "M Torres"
      ],
      "year": "2020",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "18",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "S Karen"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "19",
      "title": "System architecture for real-time face detection on analog video camera",
      "authors": [
        "M Kim",
        "D Lee",
        "K.-Y Kim"
      ],
      "year": "2015",
      "venue": "International Journal of Distributed Sensor Networks"
    },
    {
      "citation_id": "20",
      "title": "First steps to develop a corpus of interactions between elderly and virtual agents in spanish with emotion",
      "authors": [
        "L Letaifa",
        "M Develasco",
        "R Justo",
        "M Torres"
      ],
      "year": "2019",
      "venue": "International Conference on Statistical Language and Speech Processing"
    },
    {
      "citation_id": "21",
      "title": "A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
      "authors": [
        "Z Liu",
        "H Mao",
        "C.-Y Wu",
        "C Feichtenhofer",
        "T Darrell",
        "S Xie"
      ],
      "year": "2022",
      "venue": "A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "22",
      "title": "The extended cohnkanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 ieee computer society conference on computer vision and pattern recognition-workshops"
    },
    {
      "citation_id": "23",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "The empathic virtual coach: a demo",
      "authors": [
        "J Olaso",
        "A Vázquez",
        "L Letaifa",
        "M De Velasco",
        "A Mtibaa",
        "M Hmani",
        "D Petrovska-Delacrétaz",
        "G Chollet",
        "C Montenegro",
        "A López-Zorrilla"
      ],
      "year": "2021",
      "venue": "The 2021 International Conference on Multimodal Interaction (ICMI'21)"
    },
    {
      "citation_id": "25",
      "title": "Tiago: the modular robot that adapts to different research needs",
      "authors": [
        "J Pages",
        "L Marchionni",
        "F Ferro"
      ],
      "year": "2016",
      "venue": "International workshop on robot modularity"
    },
    {
      "citation_id": "26",
      "title": "Exploring emotion expression recognition in older adults interacting with a virtual coach",
      "authors": [
        "C Palmero",
        "M Develasco",
        "M Amine Hmani",
        "A Mtibaa",
        "L Ben Letaifa",
        "P Buch-Cardona",
        "R Justo",
        "T Amorese",
        "E González-Fraile",
        "B Fernández-Ruanova"
      ],
      "year": "2023",
      "venue": "Exploring emotion expression recognition in older adults interacting with a virtual coach"
    },
    {
      "citation_id": "27",
      "title": "Local multi-head channel self-attention for facial expression recognition",
      "authors": [
        "R Pecoraro",
        "V Basile",
        "V Bono"
      ],
      "year": "2022",
      "venue": "Information"
    },
    {
      "citation_id": "28",
      "title": "Buddy: A companion robot for living assistance",
      "authors": [
        "A Peltier",
        "L Fiorini"
      ],
      "year": "2017",
      "venue": "Journal of Robotics and Automation"
    },
    {
      "citation_id": "29",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "30",
      "title": "Inception-v4, inception-resnet and the impact of residual connections on learning",
      "authors": [
        "C Szegedy",
        "S Ioffe",
        "V Vanhoucke",
        "A Alemi"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "31",
      "title": "Efficientnetv2: Smaller models and faster training",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "32",
      "title": "A novel facial emotion recognition model using segmentation vgg-19 architecture",
      "authors": [
        "S Vignesh",
        "M Savithadevi",
        "M Sridevi",
        "R Sridhar"
      ],
      "year": "2023",
      "venue": "International Journal of Information Technology"
    },
    {
      "citation_id": "33",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "Proceedings of the IEEE computer society conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "A dual-direction attention mixed feature network for facial expression recognition",
      "authors": [
        "S Zhang",
        "Y Zhang",
        "Y Zhang",
        "Y Wang",
        "Z Song"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "35",
      "title": "Lightweight cnn-based expression recognition on humanoid robot",
      "authors": [
        "G Zhao",
        "H Yang",
        "Y Tao",
        "L Zhang"
      ],
      "year": "2020",
      "venue": "KSII Transactions on Internet and Information Systems"
    },
    {
      "citation_id": "36",
      "title": "Vers le temps réel en transcription automatique de la parole grand vocabulaire",
      "authors": [
        "L Zouari"
      ],
      "year": "2007",
      "venue": "Vers le temps réel en transcription automatique de la parole grand vocabulaire"
    }
  ]
}