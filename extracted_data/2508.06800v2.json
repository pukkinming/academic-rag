{
  "paper_id": "2508.06800v2",
  "title": "Hardness-Aware Dynamic Curriculum Learning For Robust Multimodal Emotion Recognition With Missing Modalities",
  "published": "2025-08-09T03:10:56Z",
  "authors": [
    "Rui Liu",
    "Haolin Zuo",
    "Zheng Lian",
    "Hongyu Yuan",
    "Qi Fan"
  ],
  "keywords": [
    "Multimodal Emotion Recognition",
    "Missing Modalities Learning",
    "Dynamic Curriculum Learning",
    "Hardness-Aware Retrieval Augmented"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Missing modalities have recently emerged as a critical research direction in multimodal emotion recognition (MER). Conventional approaches typically address this issue through missing modality reconstruction. However, these methods fail to account for variations in reconstruction difficulty across different samples, consequently limiting the model's ability to handle hard samples effectively. To overcome this limitation, we propose a novel Hardness-Aware Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates in two key stages: first, it estimates the hardness level of each sample, and second, it strategically emphasizes hard samples during training to enhance model performance on these challenging instances. Specifically, we first introduce a Multi-view Hardness Evaluation mechanism that quantifies reconstruction difficulty by considering both Direct Hardness (modality reconstruction errors) and Indirect Hardness (cross-modal mutual information). Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy that dynamically adjusts the training curriculum by retrieving samples with similar semantic information and balancing the learning focus between easy and hard instances. Extensive experiments on benchmark datasets demonstrate that HARDY-MER consistently outperforms existing methods in missing-modality scenarios. Our code will be made publicly available at https://github.com/AI-S2-Lab/HARDY-MER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal Emotion Recognition (MER) with missing modalities has emerged as a critical research direction in affective computing  [18, 24, 34, 37, 38, 44, 52] . In real-world scenarios, missing modalities frequently occur due to device failures  [32, 33, 38, 55] , asynchronous signals  [19, 30] , or low-quality inputs (e.g., degraded videos)  [42, 51] . However, most existing models are trained on complete-modality data, leading to poor performance under missing conditions and limiting their robustness in practical applications.\n\nTo mitigate these challenges, researchers have explored various methods and achieved significant progress  [5, 6, 18, 35, 51, 55] . Among these efforts, mainstream methods focus on reconstructing missing modalities using available modalities  [23, 52, 55, 58] . For instance, Zhao et al.  [55]  proposed an imagination network to recover missing modalities and to learn the joint representation. Yuan et al.  [52]  employed a diffusion model framework, leveraging available modalities to guide the generation of missing modalities and integrating the generated results with available information as a joint representation. Liu et al.  [23]  further improved the reconstruction process using modality-invariant features to strengthen model robustness under incomplete inputs.\n\nDespite recent advances, a critical limitation remains: conventional methods treat all training samples equally, overlooking the varying difficulty of reconstructing missing modalities across different instances, as illustrated in Fig.  1(a) . This homogeneous training strategy fails to acknowledge that certain samples are inherently harder to reconstruct due to factors such as semantic ambiguity, low signal quality, or strong inter-modal dependencies. Consequently, models tend to overfit on easy samples while underexploiting harder  ones, ultimately limiting their ability to generalize and adapt to complex real-world scenarios  [57] .\n\nTo address this limitation, we draw inspiration from educational psychology  [3] , where students often perform more exercises on harder concepts to enhance their understanding. Motivated by this strategy, we retrieve semantically similar examples for hard samples and integrate them into training, thereby encouraging the model to focus more on these challenging instances. We call this novel framework Hardness-Aware Dynamic Curriculum Learning, termed HARDY-MER. To achieve this, we mainly need two key functions: hardness measurement and hardness-aware training. First, we develop a Multi-view Hardness Evaluation mechanism that quantifies hardness based on two criteria: direct hardness, measured by reconstruction errors across modalities, and indirect hardness, assessed through mutual information between modalities. This dual-perspective evaluation enables a more comprehensive and accurate hardness assessment. Second, to prioritize harder examples during training, we propose a Retrieval-based Dynamic Curriculum Learning strategy. Specifically, we design a retrieval mechanism that fuses local similarities across available modalities into a unified global similarity score of the sample, which is then used to identify the most relevant candidate samples. The number of retrieved samples is then dynamically adjusted based on estimated hardness, allocating more training resources to harder samples while reducing emphasis on easier ones. The main contributions of this paper are as follows:\n\nâ€¢ We propose a novel Multi-view Hardness Evaluation mechanism that jointly models direct and indirect hardness to facilitate comprehensive, modality-sensitive training hardness estimation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "â€¢ We Introduce A Retrieval-Based Dynamic Curriculum",
      "text": "Learning strategy that dynamically retrieves semantically relevant samples based on estimated hardness and adaptively adjusts their number to balance learning between easy and hard instances, therefore enhancing model robustness under missing modality conditions. â€¢ Extensive experiments on IEMOCAP and CMU-MOSEI across six missing modality settings demonstrate the superiority of our method over existing baselines, achieving new state-ofthe-art results in per-condition metrics.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work 2.1 Hard Sample Mining",
      "text": "Hard sample mining is a popular technique for enhancing a model's discriminative ability, widely applied in tasks such as face recognition  [28] , object detection  [31, 41] , speech separation  [40] , and masked image/audio reconstruction  [29, 39] , etc. Related studies have shown that hard samples frequently serve as model performance bottlenecks  [17, 36, 47] , and targeting these challenging instances can produce significant performance improvements  [21, 29, 39] . For example, Li et al.  [16]  utilized attention scores to pinpoint important instances from false negative bags, which were then used as hard negative instances to create hard bags, ultimately enhancing classification performance. Wang et al.  [39]  measured the reconstruction hardness of samples based on reconstruction error and performed masked reconstruction on image patches with higher reconstruction hardness to improve the model's ability to reconstruct masked images, thereby enhancing the robustness of visual representation learning. Tang et al.  [36]  proposed a teacherstudent framework with consistency constraints for multi-instance classification tasks. In this approach, the teacher model implicitly mines hard instances based on attention scores, which are then used to train the student model, enabling the student to learn better discriminative boundaries. However, when applied to multimodal tasks, traditional hard example mining methods face the following limitations: 1) Even when a modality is present in the input, its reconstruction hardness can still indicate whether its semantic information is redundant or complementary to other modalities  [18, 51] . A high reconstruction error for an observed modality suggests that the information it carries cannot be easily inferred from the others, thus making the sample intrinsically difficult for the model to learn. 2) Although single metrics such as reconstruction loss  [29, 39]  or attention scores  [36]  can be used to estimate sample hardness, they may not sufficiently capture the complexity of multimodal learning. In particular, these approaches often overlook the importance of crossmodal consistency  [9, 23] . Samples that are easy to reconstruct in individual modalities may still pose learning challenges when cross-modal consistency is weak  [20] .\n\nTo overcome the limitations described above, we propose a composite metric to comprehensively evaluate the learning hardness of multimodal samples. Specifically, our metric consists of two components: direct hardness, which intuitively reflects the sample's difficulty by assessing the reconstruction error of each modality; and indirect hardness, which measures the level of mutual information between different modalities, capturing the sample's challenge from the perspective of cross-modal consistency. By combining these two perspectives, the proposed metric provides a more comprehensive and reliable estimation of the hardness of the sample. This serves as a foundation for the subsequent retrieval of samples and the construction of the curriculum.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Retrieve Augmented Generation",
      "text": "Retrieval-augmented generation (RAG) is a hybrid approach that integrates information retrieval with generative models, aiming to enhance the quality and accuracy of generation tasks. This method equips pre-trained generative models with the ability to incorporate non-parametric memory, enabling them to effectively leverage external knowledge  [15] . In NLP tasks, RAG improves the quality of text generation by retrieving relevant documents  [2, 8, 11, 15, 48] . For example, Borgeaud et al.  [2]  proposed the Retrieval-Enhanced Transformer that enhances auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. Moreover, RAG has also been applied to dialogue generation tasks  [22] , where it is used to generate expressive speech that aligns with conversational styles. The traditional RAG method mainly focuses on directly incorporating the retrieved information into the generation process to improve output quality. In contrast, our approach enhances the training process by using retrieval techniques to find similar samples for challenging instances. Additionally, this is the first work to apply RAG technology to multimodal emotion recognition with missing modality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Curriculum Learning",
      "text": "Curriculum learning (CL) is a training strategy inspired by the structurally sequential learning approach in human education  [7, 49, 54] . Its core idea is to \"start small, \" using an easier subset of data to train the model, and then gradually incorporating more challenging data until the entire training dataset is covered  [1, 43, 46, 57] . Typically, curriculum learning utilizes a predefined  [54, 56, 57]  or automatically learned  [12-14, 29, 39]  difficulty predictor to distinguish between easier and harder samples, followed by a training scheduler that determines how to introduce the more challenging samples into the training process. CL not only accelerates the training process  [13, 27]  but also enhances the model's generalization capability  [45] . Recent extensive research has demonstrated the remarkable effectiveness of curriculum learning in fields such as computer vision  [45, 49] , human-object interaction detection  [57] , acoustic representation learning  [29] , etc. However, influenced by the \"easyto-hard\" training paradigm, traditional curriculum learning often prioritizes easy samples while inadequately addressing hard samples. Our method differs from these approaches in several notable aspects: 1) We innovatively integrate retrieval augmentation into curriculum learning, enabling semantic-aware instance expansion to enhance training sample diversity; 2) During retrieval, we incorporate sample difficulty signals to provide more semantically similar instances for challenging samples, therefore strengthening the model's capability to learn from hard cases. To the best of our knowledge, this represents the first approach that systematically unifies retrieval techniques with curriculum learning.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology 3.1 Overview",
      "text": "As shown in Fig.  2 , the proposed HARDY-MER includes two main components: 1) Multi-view Hardness Evaluation simulates the role of a teacher by assessing the hardness of input samples based on the reconstruction errors of missing modalities and the mutual information across available modalities; 2) Retrieval-based Dynamic Curriculum Learning is designed to retrieve semantically similar samples, construct dynamic curricula, and train the model accordingly. This process consists of three key steps: a) Feature Database Preparation, which builds a multimodal feature index for semantic retrieval; b) Hardness-based Dynamic Multimodal Feature Retrieval, which selects the most relevant samples based on the input's available modalities and adaptively adjusts the retrieval size according to the input's estimated hardness, assigning more training resources to more challenging samples while allocating fewer to easier ones; and c) Retrieval-based Curriculum Training, where the emotion recognition model is trained using the resulting curriculum.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-View Hardness Evaluation",
      "text": "To quantify the learning hardness of each training sample under missing-modality conditions, we propose a unified metric termed multi-view hardness, which consists of two complementary components: (1) direct hardness, reflecting the reconstruction error of the modalities, and (2) indirect hardness, measuring the level of mutual information between different modalities. Stage 1 of Fig.  2  illustrates the overall computation process of this multi-view hardness evaluation. In what follows, we detail the formulation of both metrics and describe the training strategy for the hardness evaluation module.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Semantic Representation Extraction. Given A Multimodal Input Sample (ğ‘¥ ğ‘",
      "text": "miss , ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘£ ), we first extract modality semantic features using a Semantic Feature Encoding module. This module employs three Transformer-based encoders  [50]  to produce representations (ğ‘“ ğ‘ miss , ğ‘“ ğ‘¡ , ğ‘“ ğ‘£ ), where the subscript \"miss\" indicates that the corresponding modality is absent. Following prior work  [18, 23, 55] , we represent the missing modality using a zero vector. These semantic representations are used to compute both direct and indirect hardness scores.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hardness Metric Computation.",
      "text": "Direct Hardness. To estimate direct hardness, we concatenate the semantic features from the three modalities and pass them through a linear reconstruction network to recover each modality:\n\nwhere xğ‘š denotes the reconstructed feature of modality ğ‘š, and ğ‘, ğ‘¡, ğ‘£ denote acoustic, textual, and visual modalities, respectively. ğ‘Š ğ‘š , ğ‘ ğ‘š are trainable parameters. [â€¢; â€¢] denotes feature concatenation across modalities. We adopt the Mean Squared Error (MSE) loss  [21, 39]  to measure the reconstruction quality of each modality: and define the overall direct hardness as:\n\nNote that the reconstruction loss L ğ‘š rec is used only for hardness estimation and does not participate in gradient backpropagation.\n\nIndirect Hardness. In the Indirect Hardness Calculation module, we compute the mutual information (MI) between each pair of modalities using their semantic features (ğ‘“ ğ‘ miss , ğ‘“ ğ‘¡ , ğ‘“ ğ‘£ ). Following the standard definition of mutual information:\n\nwhere ğ» (â€¢) denotes entropy. However, estimating the joint entropy ğ» (ğ‘‹, ğ‘Œ ) directly in high-dimensional feature spaces is notoriously challenging. To address this, we adopt the strategy proposed by Huang et al.  [10] , which approximates the joint distribution via fusion features. Specifically, for a given modality pair ğ‘“ ğ‘ and ğ‘“ ğ‘ , we first apply a cross-attention mechanism to fuse them, treating one modality as the query and the other as the key-value input:\n\nTo ensure symmetric information capture, we swap the query and key-value roles and repeat the operation:\n\nThe final joint representation is then obtained by summing the two fused outputs:\n\nWe then estimate the entropy of each individual modality, ğ» (ğ‘“ ğ‘ ) and ğ» (ğ‘“ ğ‘ ), as well as the entropy of the fused representation ğ» (ğ‘“ ğ‘,ğ‘ ). The mutual information between ğ‘ and ğ‘ is calculated as follows:\n\nFinally, we define the indirect hardness â„ ind as the sum of the mutual information between the modalities:\n\nUnified Hardness Score. We combine direct and indirect hardness into a final unified score using a scaled logistic function:\n\nwhere ğ›¼ 1 and ğ›¼ 2 are weighting factors that balance the contributions of direct and indirect hardness, and ğ›½ is a scaling coefficient that controls the sharpness of the transition. This formulation normalizes the hardness score to the range (0, 1), enabling a smooth and differentiable measure that reflects the overall learning hardness of a sample.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Hardness Module Training.",
      "text": "To ensure the reliability of the estimated hardness scores, we adopt a two-stage training strategy for the Multi-view Hardness Evaluation module. Indirect and direct hardness are trained separately on full and missing modality data, respectively.\n\nStage 1: Indirect Hardness Training. We first train the semantic encoders and Indirect Hardness Calculation components on complete multimodal samples. The training objective in this stage includes two parts: 1) the supervised classification loss based on modality features ğ‘“ ğ‘š , encouraging the encoders to capture sentiment-discriminative information:\n\nwhere CLS ğ‘š denotes a classification head for corresponding modality based on the fully-connected layer, Å·ğ‘š is the predicted emotion class for modality ğ‘š, and ğ‘¦ is the ground truth. CE(â€¢, â€¢) represents the standard cross-entropy loss function. 2) the mutual information regularization loss:\n\nwhich ensures the reliability of mutual information estimation by encouraging the module to capture consistent cross-modal information. The total loss in the stage is:\n\nStage 2: Direct Hardness Training. We further fine-tune the Semantic Feature Encoder components and jointly train the Direct Hardness Calculation module using samples with missing modalities. Given the semantic features extracted from the available modalities, we first perform emotion classification using the concatenated features:\n\nIn parallel, we compute the direct hardness based on modality reconstruction error:\n\nThe total loss for this stage is defined as: L 2  total = L 2 cls + L rec . After the two-stage training, the parameters of the Multi-view Hardness Evaluation module are frozen and used throughout the rest of the framework. Features Preparation: We fine-tune pre-trained models via the emotion classification task to extract emotion features. Specifically, we use DeBERTa-large 1  , Wav2Vec-large 2 , and MANet 3  as frozen backbones for textual, acoustic, and visual modalities, respectively. Two trainable linear layers are appended to each backbone, and the output of the final layer is used as the semantic feature for retrieval. A classification head is trained with cross-entropy loss to guide the feature extraction toward sentiment-relevant representations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Retrieval-Based Dynamic Curriculum Learning",
      "text": "Database Construction: We utilize the FAISS (Facebook AI Similarity Search) library to construct modality semantic feature databases, applying tailored similarity metrics based on the characteristics of each modality. For textual features, we normalize all vectors and use IndexFlatIP 4  to implement cosine similarity. For acoustic and visual features, we adopt IndexFlatL2 to perform Euclidean distance-based retrieval. This process results in three separate databases for text, audio, and visual modalities. The effectiveness of this configuration is validated in Sec. 4.5, where we compare alternative index strategies and demonstrate the superiority of our method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hardness-Based Dynamic Multimodal Features",
      "text": "Retrieval. This module is illustrated in Step 2.2 of Fig.  2 . Given an input sample (ğ‘¥ ğ‘ miss , ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘£ ), we first use modality semantic encoders Enc ğ‘Ÿ ğ‘š to extract high-level embeddings for each modality:\n\nFor each available modality, we query its corresponding FAISS index using the embedding z ğ‘š to retrieve the top-ğ‘˜ most semantically similar samples, and record their indices. We then aggregate the indices retrieved from all available modalities and remove duplicates to construct a unified candidate set. Based on these we retrieve the corresponding multimodal features (acoustic, textual, and visual) from the three modality feature databases. The features retrieved under the same index collectively form a candidate sample.\n\nTo evaluate the overall similarity between a candidate and the input sample, we compute the L2 distance between their corresponding features in each available modality of the candidate sample. We then take the average of these distances as the integrated similarity score. Finally, we rank all candidate samples in ascending order of similarity and select the top-ğ‘˜ most similar ones as the final retrieval results.\n\nBased on the retrieval results, we further construct a hardnessaware curriculum to guide model training. To ensure that harder samples receive more support while easier ones receive less, we use the sample hardness score â„ âˆˆ (0, 1) from Stage-1 to adaptively determine the number of support samples:\n\nWe then select the top-ğ‘˜ â€² entries from the retrieval results as the hardness-aware curriculum for training.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Retrieval-Based Curriculum Training. As Illustrated In",
      "text": "Step 2.3 of Fig.  2 , we integrate the input sample (ğ‘¥ ğ‘ miss , ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘£ ) with its corresponding hardness-aware curriculum retrieved in Step 2.2 to train our emotion recognition model. The model consists of three Transformer-based modality encoders, a reconstruction network based on autoencoders, and a classification head. To ensure that each encoder extracts robust semantic representations, we follow the previous works  [23, 50]  and adopt a two-stage training strategy.\n\nStage 1: Pretraining with complete modality input. We feed the full input (ğ‘¥ ğ‘ , ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘£ ) into the corresponding encoders Enc ğ‘š to obtain complete modality semantic features (ğ‘“ ğ‘ , ğ‘“ ğ‘¡ , ğ‘“ ğ‘£ ). To supervise the representation learning of each modality, we attach three independent classification heads and perform sentiment prediction using the features from each modality separately. The classification heads are trained with cross-entropy loss to guide each encoder in capturing discriminative sentiment-related information.\n\nStage 2: Curriculum-based model training. We then train the full model using the hardness-aware curriculum generated for each input. Each training instance consists of the original input (ğ‘¥ ğ‘ miss , ğ‘¥ ğ‘¡ , ğ‘¥ ğ‘£ ) followed by its retrieved support samples, ordered from high to low similarity. The concatenated semantic features from the three encoders are jointly used for both emotion classification and missing modality reconstruction. During this stage, the entire model is optimized using a combination of classification loss and reconstruction loss, which jointly encourage accurate emotion prediction and robust recovery of the missing modality.\n\nDuring inference, we use the trained model to perform emotion prediction on inputs with missing modalities, without requiring dynamic curriculum retrieval.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment 4.1 Datasets And Evaluation Metrics",
      "text": "To validate the effectiveness of our approach, we conducted extensive experiments on two public benchmark datasets:\n\nIEMOCAP  [4]  is a widely adopted benchmark dataset for multimodal emotion recognition. It is commonly used in prior studies for both four-class classification (i.e., Happy, Sad, Neutral, Angry)  [23, 55, 58]  and six-class classification (i.e., Happy, Angry, Sad, Neutral, Surprised, Fearful)  [18, 25, 26] . In this work, we evaluate our method under both settings to ensure a comprehensive comparison with existing approaches.\n\nCMU-MOSEI is a benchmark dataset for multimodal sentiment analysis, comprising 22856 annotated video clips collected from YouTube. Each utterance is labeled with a continuous sentiment score ranging from -3 to +3, indicating its polarity and intensity. Following prior work  [50] , we formulate this task as binary sentiment classification by labeling utterances with scores greater than zero as positive, and those with scores less than zero as negative.\n\nFor the IEMOCAP dataset, we follow previous work  [23, 50, 55, 58]  and use weighted accuracy (WA) and unweighted accuracy (UA) as evaluation metrics. For the CMU-MOSEI dataset, we use accuracy (Acc) and F1 score as evaluation metrics  [50] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Following prior studies  [18, 23, 50, 55, 58] , we evaluate our model under six missing-modality settings: {a}, {t}, {v}, {a, t}, {a, v}, and {t, v}, where 'a', 't', and 'v' denote the acoustic, textual, and visual modalities, respectively. Each set indicates the modalities that remain available during inference. To ensure fair comparison, we adopt publicly available features from  [18, 50] . All models were trained for 25 epochs using the Adam optimizer with a learning rate of 0.0001 and a dropout rate of 0.5. Hyperparameters were set as ğ‘˜=5, ğ›¼ 1 =0.6, ğ›¼ 2 =0.4, and ğ›½=4. Experiments were conducted on NVIDIA A800 GPUs with PyTorch 1.13.1 and CUDA toolkit 11.1.1.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With Sota Methods",
      "text": "To evaluate the performance of our method under various missing modality conditions, we conduct comparisons with several state-of-the-art (SOTA) methods, including CPMNet  [53] , GCNet  [18] , MMIN  [55] , CIF-MMIN  [23] , and MoMKE  [50] , on two benchmark datasets. All methods are tested under the same fixed missing modality settings. As shown in Tab. 1, our method consistently outperforms prior approaches in both per-condition and average performance across all testing conditions. Specifically, HARDY-MER achieves improvements of 0.0443, 0.0297, and 0.0143 in average WA on the IEMOCAP (4-class), IEMOCAP (6-class), and CMU-MOSEI tasks, respectively, demonstrating strong generalization and robustness under incomplete modality inputs. In particular, we observe the most significant performance gain under the {v} condition. This may be attributed to the inherently higher uncertainty of visual features, which are more difficult to interpret in isolation. In such cases, our hardness-aware retrieval mechanism provides semantically relevant support samples, enhancing both representation quality and prediction reliability. Although slight performance drops (approximately 0.9% -1.5%) occur under the {a,t} and {t,v} conditions in CMU-MOSEI, our method still delivers the best overall performance, achieving improvements of 0.0132 and 0.0163 in ACC and F1, respectively. These results further validate the effectiveness and practical applicability of HARDY-MER for robust multimodal learning with missing inputs.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To thoroughly investigate the effectiveness of different modules in our model, we designed a series of ablation experiments and validated them on the IEMOCAP four-class task:\n\n1) w/o â„ dir & w/o â„ ind : To evaluate the individual impact of each hardness component, we perform ablation studies by removing either the direct hardness (w/o â„ dir ) or indirect hardness (w/o â„ ind ) from the overall sample hardness computation. In the w/o â„ dir setting, we exclude the direct hardness term and calculate sample hardness solely based on the indirect hardness. Conversely, in the w/o â„ ind setting, we rely only on the direct hardness for the sample difficulty estimation. As shown in Tab. 2, both of them lead to performance drops, confirming that each type of difficulty provides complementary value. Notably, excluding â„ dir results in larger degradation, highlighting its stronger correlation with reconstruction difficulty.\n\n2) w/o â„: To evaluate the overall effectiveness of the proposed sample hardness mechanism, we conduct an ablation in which the hardness score is entirely removed from the retrieval process. Instead of adaptively determining the number of retrieved samples based on each sample's difficulty, we assign a fixed Top-ğ‘˜ number of support samples to all training instances, regardless of their reconstruction or semantic complexity. The performance degradation reported in Tab. 2 indicates that adaptive retrieval based on sample difficulty yields more effective results than uniform sampling.\n\nTable  1 : Performance comparison with state-of-the-art methods (SOTA) under six possible missing modality conditions on two benchmark datasets. \"Average\" refers to the average performance of the models across all six missing modality conditions. The best results in each dataset are highlighted in bold, and the second-best results are underlined. The row marked with Î” ğ‘†ğ‘œğ‘¡ğ‘ indicates the improvement or reduction of our method compared to the best-competing method. We perform a T-test on the Average column and * indicates that the p-value < 0.05.  3) w/o retrieval features: To examine the effectiveness of retrievalbased curriculum learning, we remove the retrieval mechanism entirely and train the model using only the original training samples. No additional support samples are retrieved during training. The results in the row of w/o retrieval features in Tab. 2 indicate that solely using the original samples, without allocating additional samples for challenging cases during training, diminishes the model's training efficacy. This observation also validates the effectiveness of our retrieval curriculum.\n\n4) w/o fine-tuning features: To assess the importance of feature quality in the retrieval process, we replace the fine-tuned features used for building the retrieval index with publicly pretrained features from prior work. The results in Tab. 2 show that the model achieves significant improvements after using fine-tuned features, especially in the t condition, indicating that high-quality features are crucial for maintaining retrieval accuracy and model robustness.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "5)",
      "text": "Hyperparameter ablation: To assess the impact of the hyperparameters in Eq. 10 on model performance, we conduct ablation studies on ğ›¼ 1 , ğ›¼ 2 , and ğ›½. We report the average WA and UA scores across six missing modality scenarios, as shown in Tab. 3. The results indicate that increasing ğ›¼ 1 generally enhances performance, suggesting that direct hardness plays a more critical role in assessing overall sample hardness. However, when ğ›¼ 1 exceeds 0.6, the contribution of indirect hardness is overly suppressed, leading to a decline in performance. The parameter ğ›½ serves to normalize the hardness metric within the [0, 1] range; if set too high or too low, it disrupts sensitivity and undermines the model's ability to dynamically adjust the K-value, ultimately affecting overall performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization Analysis",
      "text": "To analyze the impact of fine-tuning on similarity measurement, we visualized the retrieved samples using t-SNE. We randomly selected a sample from the IEMOCAP dataset (four-class) and retrieved the top 1502 most similar samples to this sample from both the original feature index and the fine-tuned feature index. Red and green points in Fig.  3  denote the original and fine-tuned features, respectively, while the black \"X\" marks the queried sample. The results show that fine-tuned features are more concentrated around the queried point across all modalities, indicating improved retrieval accuracy after fine-tuning. We further investigate the impact of different index construction strategies by comparing our default setting with three alternatives, each modifying the distance metric of a single modality: 1) A-IP: replaces IndexFlatL2 with IndexFlatIP for the acoustic index; 2) V-IP: applies IndexFlatIP to the visual index; 3) T-L2: uses IndexFlatL2 for the text index instead of IndexFlatIP. Fig.  4  reports the Weighted Accuracy (WA) and Unweighted Accuracy (UA) under various modality conditions. Results show that using L2 distance for the text index (T-L2) consistently degrades performance, especially in text-only or text-involved settings (e.g., t, at, tv), highlighting the suitability of inner product for normalized textual embeddings. In contrast, switching to cosine similarity for acoustic (A-IP) or visual (V-IP) indexing reduces accuracy, with V-IP showing the most notable drop, particularly under visual-only input. These findings suggest that L2 distance is more effective for acoustic and visual features, which typically retain important magnitude information.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "To improve sensitivity to hard samples and enhance robustness in missing-modality multimodal emotion recognition, we propose HARDY-MER, a novel framework that combines retrieval-augmented learning with curriculum learning. We introduce a multi-view hardness evaluation mechanism based on reconstruction errors and cross-modal mutual information, and design a Retrieval-based Dynamic Curriculum Learning strategy. This involves retrieving semantically relevant support samples from modality-specific feature banks, with retrieval quantity adaptively determined by sample hardness. The resulting hardness-aware curriculum guides model training. Experiments show HARDY-MER outperforms state-ofthe-art methods, and to our knowledge, it is the first to integrate retrieval and curriculum learning in this setting. Future work will explore extending HARDY-MER to large-scale pre-trained multimodal models for greater robustness under challenging conditions.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a). This homogeneous training",
      "page": 1
    },
    {
      "caption": "Figure 1: Comparison between conventional paradigms for",
      "page": 2
    },
    {
      "caption": "Figure 2: , the proposed HARDY-MER includes two main",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the overall computation process of this multi-view hard-",
      "page": 3
    },
    {
      "caption": "Figure 2: The overview of HARDY-MER consists of Multi-view Hardness Evaluation, Feature Database Preparation, Hardness-",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates the structure of the Retrieval-Based",
      "page": 5
    },
    {
      "caption": "Figure 2: Given an input sample",
      "page": 5
    },
    {
      "caption": "Figure 2: , we integrate the input sample (ğ‘¥ğ‘",
      "page": 5
    },
    {
      "caption": "Figure 3: t-SNE visualizations for randomly selected samples in the IEMOCAP four-class across acoustic, textual, and visual",
      "page": 8
    },
    {
      "caption": "Figure 3: denote the original and fine-tuned features, respectively,",
      "page": 8
    },
    {
      "caption": "Figure 4: reports the Weighted Ac-",
      "page": 8
    },
    {
      "caption": "Figure 4: Impact of different index construction methods on",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP\nfour-class",
          "model": "CPMNet [53]\nMMIN [55]\nGCNet [18]\nCIF-MMIN [23]\nMoMKE [50]\nHARDY-MER (our)\nÎ”ğ‘†ğ‘œğ‘¡ğ‘",
          "Testing Condition\na\nv\nt\nat\nav\ntv\nAverage\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA": "0.4685\n0.5172\n0.4495\n0.4449\n0.4563\n0.4532\n0.3481\n0.3623\n0.4867\n0.4933\n0.4562\n0.4657\n0.4442\n0.4561\n0.5658\n0.5900\n0.5252\n0.5060\n0.6657\n0.6802\n0.7294\n0.7114\n0.6399\n0.6343\n0.7167\n0.6861\n0.6405\n0.6347\n0.6558\n0.6876\n0.5796\n0.5254\n0.7233\n0.7042\n0.7702\n0.7687\n0.6740\n0.6564\n0.7563\n0.7362\n0.6932\n0.6798\n0.5753\n0.6006\n0.5346\n0.5156\n0.6722\n0.6899\n0.7419\n0.7259\n0.6499\n0.6353\n0.7240\n0.6991\n0.6497\n0.6444\n0.6953\n0.7021\n0.5680\n0.5203\n0.7730\n0.7766\n0.7903\n0.7988\n0.6857\n0.6622\n0.7555\n0.7418\n0.7113\n0.7003\n0.7265\n0.7387\n0.6319\n0.6054\n0.8249\n0.8269\n0.8167\n0.8243\n0.7419\n0.7450\n0.7918\n0.7851\n0.7556\n0.7542\nâ†‘0.0312\nâ†‘0.0366\nâ†‘0.0523\nâ†‘0.0800\nâ†‘0.0519\nâ†‘0.0503\nâ†‘0.0264\nâ†‘0.0255\nâ†‘0.0562\nâ†‘0.0828\nâ†‘0.0355\nâ†‘0.0433\nâ†‘0.0443âˆ—\nâ†‘0.0539âˆ—"
        },
        {
          "Dataset": "IEMOCAP\nsix-class",
          "model": "CPMNet [53]\nMMIN [55]\nGCNet [18]\nCIF-MMIN [23]\nMoMKE [50]\nHARDY-MER (our)\nÎ”ğ‘†ğ‘œğ‘¡ğ‘",
          "Testing Condition\na\nv\nt\nat\nav\ntv\nAverage\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA": "0.2947\n0.2980\n0.2620\n0.2495\n0.3244\n0.3495\n0.3349\n0.3394\n0.2692\n0.2546\n0.3134\n0.3043\n0.2998\n0.2992\n0.4408\n0.4296\n0.3574\n0.3065\n0.4217\n0.3855\n0.5195\n0.4831\n0.4192\n0.3815\n0.4749\n0.4063\n0.4389\n0.3988\n0.4995\n0.4645\n0.3978\n0.3497\n0.5648\n0.5562\n0.5824\n0.5725\n0.4757\n0.4331\n0.5743\n0.5466\n0.5158\n0.4871\n0.4496\n0.4356\n0.3611\n0.3135\n0.4340\n0.3971\n0.5243\n0.4920\n0.4254\n0.3922\n0.4888\n0.4491\n0.4472\n0.4133\n0.5051\n0.4738\n0.3907\n0.3451\n0.6109\n0.6019\n0.6318\n0.6194\n0.4865\n0.4408\n0.5992\n0.5755\n0.5374\n0.5094\n0.5158\n0.4914\n0.4302\n0.3649\n0.6589\n0.6195\n0.6518\n0.6298\n0.5291\n0.4745\n0.6166\n0.5786\n0.5671\n0.5265\nâ†‘0.0107\nâ†‘0.0176\nâ†‘0.0324\nâ†‘0.0152\nâ†‘0.0480\nâ†‘0.0176\nâ†‘0.0200\nâ†‘0.0104\nâ†‘0.0426\nâ†‘0.0337\nâ†‘0.0174\nâ†‘0.0031\nâ†‘0.0297âˆ—\nâ†‘0.0170âˆ—"
        },
        {
          "Dataset": "Dataset",
          "model": "model",
          "Testing Condition\na\nv\nt\nat\nav\ntv\nAverage\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA": "a\nv\nt\nat\nav\ntv\nAverage\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1\nACC\nF1"
        },
        {
          "Dataset": "CMUMOSEI",
          "model": "CPMNet [53]\nMMIN [55]\nGCNet [18]\nCIF-MMIN [23]\nMoMKE [50]\nHARDY-MER (our)\nÎ”ğ‘†ğ‘œğ‘¡ğ‘",
          "Testing Condition\na\nv\nt\nat\nav\ntv\nAverage\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA": "0.6571\n0.6518\n0.6123\n0.6173\n0.7287\n0.7244\n0.7265\n0.7224\n0.6156\n0.6199\n0.6629\n0.6684\n0.6672\n0.6674\n0.5890\n0.5950\n0.5930\n0.6001\n0.8220\n0.8240\n0.8370\n0.8330\n0.6355\n0.6191\n0.8175\n0.8142\n0.7157\n0.7142\n0.7204\n0.7034\n0.6808\n0.6725\n0.8426\n0.8417\n0.8510\n0.8510\n0.7149\n0.6996\n0.8474\n0.8454\n0.7762\n0.7689\n0.6387\n0.6460\n0.6196\n0.6266\n0.8353\n0.8304\n0.8401\n0.8347\n0.6468\n0.6208\n0.8250\n0.8194\n0.7343\n0.7297\n0.8632\n0.8629\n0.8690\n0.8691\n0.7256\n0.7103\n0.6450\n0.6346\n0.8610\n0.8603\n0.7237\n0.7207\n0.7813\n0.7763\n0.7482\n0.7411\n0.6935\n0.6750\n0.8720\n0.8713\n0.7482\n0.7411\n0.7956\n0.7888\n0.8542\n0.8501\n0.8572\n0.8539\nâ†‘0.0226\nâ†‘0.0308\nâ†‘0.0127\nâ†‘0.0025\nâ†‘0.0110\nâ†‘0.0110\nâ†“-0.0090\nâ†“-0.0128\nâ†‘0.0245\nâ†‘0.0204\nâ†“-0.0118\nâ†“-0.0152\nâ†‘0.0143âˆ—\nâ†‘0.0124âˆ—"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model": "",
          "Testing Condition": "a\nv\nt\nat\nav\ntv\nAverage"
        },
        {
          "model": "",
          "Testing Condition": "WA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA\nWA\nUA"
        },
        {
          "model": "HARDY-MER (our)\nw/o â„dir\nw/o â„ind\nw/o â„\nw/o retrieval features\nw/o fine-tuning features",
          "Testing Condition": "0.7265\n0.7387\n0.6319\n0.6054\n0.8249\n0.8269\n0.8167\n0.8243\n0.7419\n0.745\n0.7918\n0.7851\n0.7556\n0.7542\n0.7202\n0.7246\n0.6196\n0.5933\n0.8149\n0.8184\n0.8087\n0.8164\n0.7345\n0.7307\n0.7778\n0.7754\n0.7460\n0.7431\n0.7231\n0.7281\n0.6186\n0.5945\n0.8161\n0.8161\n0.8090\n0.8129\n0.7374\n0.7374\n0.7867\n0.7775\n0.7485\n0.7444\n0.7215\n0.7301\n0.6136\n0.5852\n0.8142\n0.8175\n0.8124\n0.8184\n0.6889\n0.6881\n0.7719\n0.7693\n0.7371\n0.7348\n0.7201\n0.7217\n0.6200\n0.5806\n0.8128\n0.8137\n0.8093\n0.8132\n0.7331\n0.7282\n0.7883\n0.7719\n0.7473\n0.7382\n0.7126\n0.7218\n0.5916\n0.5345\n0.7299\n0.7421\n0.7496\n0.7647\n0.7364\n0.7390\n0.7387\n0.7404\n0.7098\n0.7071"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Greedy layer-wise training of deep networks",
      "authors": [
        "Yoshua Bengio",
        "Pascal Lamblin",
        "Dan Popovici",
        "Hugo Larochelle"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Improving language models by retrieving from trillions of tokens",
      "authors": [
        "Sebastian Borgeaud",
        "Arthur Mensch",
        "Jordan Hoffmann",
        "Trevor Cai",
        "Eliza Rutherford",
        "Katie Millican",
        "George Van Den Driessche",
        "Jean-Baptiste Lespiau",
        "Bogdan Damoc",
        "Aidan Clark"
      ],
      "year": "2022",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "3",
      "title": "Make It Stick: The Science of Successful Learning",
      "authors": [
        "C Peter",
        "Henry Brown",
        "Mark Roediger",
        "Mcdaniel"
      ],
      "year": "2014",
      "venue": "Make It Stick: The Science of Successful Learning"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "5",
      "title": "Deep adversarial learning for multi-modality missing data completion",
      "authors": [
        "Lei Cai",
        "Zhengyang Wang",
        "Hongyang Gao",
        "Dinggang Shen",
        "Shuiwang Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "6",
      "title": "Semi-supervised deep generative modelling of incomplete multi-modality emotional data",
      "authors": [
        "Changde Du",
        "Changying Du",
        "Hao Wang",
        "Jinpeng Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "7",
      "title": "Hybrid computing using a neural network with dynamic external memory",
      "authors": [
        "Alex Graves",
        "Greg Wayne",
        "Malcolm Reynolds",
        "Tim Harley",
        "Ivo Danihelka",
        "Agnieszka Grabska-BarwiÅ„ska",
        "Sergio Colmenarejo",
        "Edward Grefenstette",
        "Tiago Ramalho",
        "John Agapiou"
      ],
      "year": "2016",
      "venue": "Nature"
    },
    {
      "citation_id": "8",
      "title": "Retrieval augmented language model pre-training",
      "authors": [
        "Kelvin Guu",
        "Kenton Lee",
        "Zora Tung",
        "Panupong Pasupat",
        "Mingwei Chang"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "9",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "10",
      "title": "Dominant SIngle-Modal SUpplementary Fusion (SIMSUF) For Multimodal Sentiment Analysis",
      "authors": [
        "Jian Huang",
        "Yanli Ji",
        "Zhen Qin",
        "Yang Yang",
        "Heng Tao Shen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Leveraging passage retrieval with generative models for open domain question answering",
      "authors": [
        "Gautier Izacard",
        "Edouard Grave"
      ],
      "year": "2020",
      "venue": "Leveraging passage retrieval with generative models for open domain question answering",
      "arxiv": "arXiv:2007.01282"
    },
    {
      "citation_id": "12",
      "title": "Easy samples first: Self-paced reranking for zero-example multimedia search",
      "authors": [
        "Lu Jiang",
        "Deyu Meng",
        "Teruko Mitamura",
        "Alexander Hauptmann"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Self-paced learning with diversity",
      "authors": [
        "Lu Jiang",
        "Deyu Meng",
        "Shoou-I Yu",
        "Zhenzhong Lan",
        "Shiguang Shan",
        "Alexander Hauptmann"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Self-paced curriculum learning",
      "authors": [
        "Lul Jiang",
        "Deyu Meng",
        "Qian Zhao",
        "Shiguang Shan",
        "Alexander Hauptmann"
      ],
      "year": "2015",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
      "authors": [
        "Patrick Lewis",
        "Ethan Perez",
        "Aleksandra Piktus",
        "Fabio Petroni",
        "Vladimir Karpukhin",
        "Naman Goyal",
        "Heinrich KÃ¼ttler",
        "Mike Lewis",
        "Wen-Tau Yih",
        "Tim RocktÃ¤schel"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Deep instance-level hard negative mining model for histopathology images",
      "authors": [
        "Meng Li",
        "Lin Wu",
        "Arnold Wiliem",
        "Kun Zhao",
        "Teng Zhang",
        "Brian Lovell"
      ],
      "year": "2019",
      "venue": "Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference"
    },
    {
      "citation_id": "17",
      "title": "Focus on hard samples: Hierarchical unbiased constraints for cross-domain 3D model retrieval",
      "authors": [
        "Tian-Bao Li",
        "An-An Liu",
        "Dan Song",
        "Wen-Hui Li",
        "Xuan-Ya Li",
        "Yu-Ting Su"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "18",
      "title": "GCNet: Graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "19",
      "title": "Enhancing Resilience to Missing Data in Audio-Text Emotion Recognition with Multi-Scale Chunk Regularization",
      "authors": [
        "Wei-Cheng Lin",
        "Lucas Goncalves",
        "Carlos Busso"
      ],
      "year": "2023",
      "venue": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "20",
      "title": "Completer: Incomplete multi-view clustering via contrastive prediction",
      "authors": [
        "Yijie Lin",
        "Yuanbiao Gou",
        "Zitao Liu",
        "Boyun Li",
        "Jiancheng Lv",
        "Xi Peng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Rho-1: Not all tokens are what you need",
      "authors": [
        "Zhenghao Lin",
        "Zhibin Gou",
        "Yeyun Gong",
        "Xiao Liu",
        "Yelong Shen",
        "Ruochen Xu",
        "Chen Lin",
        "Yujiu Yang",
        "Jian Jiao",
        "Nan Duan"
      ],
      "year": "2024",
      "venue": "Rho-1: Not all tokens are what you need",
      "arxiv": "arXiv:2404.07965"
    },
    {
      "citation_id": "22",
      "title": "Retrieval-Augmented Dialogue Knowledge Aggregation for expressive conversational speech synthesis",
      "authors": [
        "Rui Liu",
        "Zhenqi Jia",
        "Feilong Bao",
        "Haizhou Li"
      ],
      "year": "2025",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "23",
      "title": "Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Bjorn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Multimodal reconstruct and align net for missing modality problem in sentiment analysis",
      "authors": [
        "Wei Luo",
        "Mengying Xu",
        "Hanjiang Lai"
      ],
      "year": "2023",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "25",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "26",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "27",
      "title": "Competence-based Curriculum Learning for Neural Machine Translation",
      "authors": [
        "Otilia Emmanouil Antonios Platanios",
        "Graham Stretcu",
        "BarnabÃ¡s Neubig",
        "Tom PoczÃ³s",
        "Mitchell"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "28",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "29",
      "title": "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning",
      "authors": [
        "Ashish Seth",
        "Ramaneswaran Selvakumar",
        "S Sakshi",
        "Sonal Kumar",
        "Sreyan Ghosh",
        "Dinesh Manocha"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "30",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "Guangyao Shen",
        "Xin Wang",
        "Xuguang Duan",
        "Hongzhi Li",
        "Wenwu Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Training regionbased object detectors with online hard example mining",
      "authors": [
        "Abhinav Shrivastava",
        "Abhinav Gupta",
        "Ross Girshick"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "32",
      "title": "Diffcl: A diffusion-based contrastive learning framework with semantic alignment for multimodal recommendations",
      "authors": [
        "Qiya Song",
        "Jiajun Hu",
        "Lin Xiao",
        "Bin Sun",
        "Xieping Gao",
        "Shutao Li"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "33",
      "title": "Multimodal sparse transformer network for audio-visual speech recognition",
      "authors": [
        "Qiya Song",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "34",
      "title": "Enhancing Emotion Recognition in Incomplete Data: A Novel Cross-Modal Alignment, Reconstruction, and Refinement Framework",
      "authors": [
        "Haoqin Sun",
        "Shiwan Zhao",
        "Shaokai Li",
        "Xiangyu Kong",
        "Xuechen Wang",
        "Jiaming Zhou",
        "Aobo Kong",
        "Yong Chen",
        "Wenjia Zeng",
        "Yong Qin"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "CTFN: Hierarchical learning for multimodal sentiment analysis using coupled-translation fusion network",
      "authors": [
        "Jiajia Tang",
        "Kang Li",
        "Xuanyu Jin",
        "Andrzej Cichocki",
        "Qibin Zhao",
        "Wanzeng Kong"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "36",
      "title": "Multiple instance learning framework with masked hard instance mining for whole slide image classification",
      "authors": [
        "Wenhao Tang",
        "Sheng Huang",
        "Xiaoxian Zhang",
        "Fengtao Zhou",
        "Yi Zhang",
        "Bo Liu"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "37",
      "title": "COLD fusion: Calibrated and ordinal latent distribution fusion for uncertainty-aware multimodal emotion recognition",
      "authors": [
        "Mani Kumar Tellamekala",
        "Shahin Amiriparian",
        "W BjÃ¶rn",
        "Elisabeth Schuller",
        "Timo AndrÃ©",
        "Michel Giesbrecht",
        "Valstar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Accommodating Missing Modalities in Time-Continuous Multimodal Emotion Recognition",
      "authors": [
        "Juan Vazquez-Rodriguez",
        "GrÃ©goire Lefebvre",
        "Julien Cumin",
        "James Crowley"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "39",
      "title": "Hard patches mining for masked image modeling",
      "authors": [
        "Haochen Wang",
        "Kaiyou Song",
        "Junsong Fan",
        "Yuxi Wang",
        "Jin Xie",
        "Zhaoxiang Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Mining hard samples locally and globally for improved speech separation",
      "authors": [
        "Kai Wang",
        "Yizhou Peng",
        "Hao Huang",
        "Ying Hu",
        "Sheng Li"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Towards human-machine cooperation: Self-supervised sample mining for object detection",
      "authors": [
        "Keze Wang",
        "Xiaopeng Yan",
        "Dongyu Zhang",
        "Lei Zhang",
        "Liang Lin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "42",
      "title": "M2R2: Missing-Modality Robust emotion Recognition framework with iterative data augmentation",
      "authors": [
        "Ning Wang",
        "Hui Cao",
        "Jun Zhao",
        "Ruilin Chen",
        "Dapeng Yan",
        "Jie Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "A survey on curriculum learning",
      "authors": [
        "Xin Wang",
        "Yudong Chen",
        "Wenwu Zhu"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "44",
      "title": "Incomplete multimodality-diffused emotion recognition",
      "authors": [
        "Yuanzhi Wang",
        "Yong Li",
        "Zhen Cui"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Efficienttrain: Exploring generalized curriculum learning for training visual backbones",
      "authors": [
        "Yulin Wang",
        "Yang Yue",
        "Rui Lu",
        "Tianjiao Liu",
        "Zhao Zhong",
        "Shiji Song",
        "Gao Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "Stc: A simple to complex framework for weakly-supervised semantic segmentation",
      "authors": [
        "Yunchao Wei",
        "Xiaodan Liang",
        "Yunpeng Chen",
        "Xiaohui Shen",
        "Ming-Ming Cheng",
        "Jiashi Feng",
        "Yao Zhao",
        "Shuicheng Yan"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "47",
      "title": "HGMD: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation",
      "authors": [
        "Lirong Wu",
        "Yunfan Liu",
        "Yufei Huang",
        "Haitao Lin",
        "Cheng Tan",
        "Stan Li"
      ],
      "year": "2024",
      "venue": "HGMD: Rethinking Hard Sample Distillation for GNN-to-MLP Knowledge Distillation"
    },
    {
      "citation_id": "48",
      "title": "Retrieval-augmented generation for natural language processing: A survey",
      "authors": [
        "Shangyu Wu",
        "Ying Xiong",
        "Yufei Cui",
        "Haolun Wu",
        "Can Chen",
        "Ye Yuan",
        "Lianming Huang",
        "Xue Liu",
        "Tei-Wei Kuo",
        "Nan Guan"
      ],
      "year": "2024",
      "venue": "Retrieval-augmented generation for natural language processing: A survey",
      "arxiv": "arXiv:2407.13193"
    },
    {
      "citation_id": "49",
      "title": "Training agent for first-person shooter game with actor-critic curriculum learning",
      "authors": [
        "Yuxin Wu",
        "Yuandong Tian"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "50",
      "title": "Leveraging Knowledge of Modality Experts for Incomplete Multimodal Learning",
      "authors": [
        "Wenxin Xu",
        "Hexin Jiang",
        "Xuefeng Liang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "51",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Ziqi Yuan",
        "Wei Li",
        "Hua Xu",
        "Wenmeng Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "52",
      "title": "Noise imitation based adversarial training for robust multimodal sentiment analysis",
      "authors": [
        "Ziqi Yuan",
        "Yihe Liu",
        "Hua Xu",
        "Kai Gao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Deep partial multi-view learning",
      "authors": [
        "Changqing Zhang",
        "Yajie Cui",
        "Zongbo Han",
        "Joey Zhou",
        "Huazhu Fu",
        "Qinghua Hu"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "54",
      "title": "M2DF: Multi-grained Multi-curriculum Denoising Framework for Multimodal Aspect-based Sentiment Analysis",
      "authors": [
        "Fei Zhao",
        "Chunhui Li",
        "Zhen Wu",
        "Yawen Ouyang",
        "Jianbing Zhang",
        "Xinyu Dai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "55",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "56",
      "title": "CLCL: Non-compositional expression detection with contrastive learning and curriculum learning",
      "authors": [
        "Jianing Zhou",
        "Ziheng Zeng",
        "Suma Bhat"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "57",
      "title": "Learning from easy to hard pairs: Multi-step reasoning network for human-object interaction detection",
      "authors": [
        "Yuchen Zhou",
        "Guang Tan",
        "Mengtang Li",
        "Chao Gou"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "58",
      "title": "Exploiting modality-invariant feature for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "Haolin Zuo",
        "Rui Liu",
        "Jinming Zhao",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}