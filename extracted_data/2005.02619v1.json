{
  "paper_id": "2005.02619v1",
  "title": "Affective Brain-Computer Interfaces: A Tutorial To Choose Performance Measuring Metric",
  "published": "2020-05-06T07:07:06Z",
  "authors": [
    "Md Rakibul Mowla",
    "Rachael I. Cano",
    "Katie J. Dhuyvetter",
    "David E. Thompson"
  ],
  "keywords": [
    "Affective brain-computer interfaces (aBCIs)",
    "balanced accuracy",
    "DEAP database",
    "electroencephalogram (EEG)",
    "support vector machines (SVMs)",
    "emotion classification",
    "performance measurement"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective brain-computer interfaces are a relatively new area of research in affective computing. Estimation of affective states can improve human-computer interaction as well as improve the care of people with severe disabilities. To assess the effectiveness of EEG recordings in recognizing affective state, here we used data collected in our lab as well as the publicly available DEAP database. We also reviewed the articles that used the DEAP database and found that a significant number of articles did not consider the presence of the class imbalance in the DEAP. Failing to consider class imbalance creates misleading results. Further, ignoring class imbalance makes the comparing results between studies impossible, since different datasets will have different class imbalances. Class imbalance also shifts the chance level, hence it is vital to consider class bias while determining if the results are above chance. To properly account the effect of class imbalance, we suggest the use of balanced accuracy as a performance metric and its posterior distribution for computing credible intervals. For classification, we used features mentioned in the literature and additionally theta beta-1 ratio. Results from DEAP and our data suggest that the beta band power, theta band power, and theta beta-1 ratio are better feature sets for classifying valence, arousal, and dominance, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "T HE term \"affective\" is a psychological concept referring to the experience of human emotion or feeling. Braincomputer interfaces (BCIs) are usually defined as a direct means of communication between the brain and external devices or systems which enable the brain signal to control some external activity  [1] . Yet BCIs also allow investigation of brain activity and analysis of brain state. Affective Brain-Computer Interfaces (aBCIs) can be defined as a human affect estimation system from brain signals using BCIs. The interest in automatic detection of people's affective states has increased over the last few decades. Studies have shown that affective states play an important role in human decision making  [2] . The ability to manage one's affective states is also related to the ability of logical reasoning, learning and extracting important information  [3] . According to Goleman's model of emotional intelligence, having knowledge of your own affective states is a key factor behind personal and professional success  [4] .\n\nHowever, estimation of the affective state is a difficult task for several reasons. Human subjects do not always reveal their true emotions, and often inflate their degree of happiness or satisfaction in self-reports  [5] . Additionally, there is some ambiguity in understanding and defining affective states  [6] .\n\nFacial expression analysis is one of the most popular methods  [7]  for estimating affective states, but it is possible to deliberately fake facial expressions unrelated to one's true inner affective state. Therefore, as Picard argued, the estimation may have a high error rate if someone has the ability to disguise his or her emotion  [6] .\n\nWith the improvements in brain imaging techniques, there is a growing interest in relationships between affective states and brain activities. Investigating affective states using electroencephalogram (EEG) is becoming popular among researchers because EEG is one of the most convenient, noninvasive forms of recording brain activity. EEG also has high temporal resolution, which makes it a preferable candidate for fast affective state estimation  [8] . Before using EEG-based BCIs to estimate affective states, one major challenge is to model affective states in a measurable and understandable scale. A current, widely accepted affective state model is the circumplex model of affect (Figure  1 ), which was initially proposed by J. A. Russel  [9] . Finding distinct physiological patterns for each affective state has also always been a major topic of interest for affective computing researchers  [10] . Picard argued that emotion consists of more complex, underlying processes rather than outward physiological expression  [6] .\n\nInterest in EEG-based emotion recognition has increased over time and is still growing. Searching \"EEG emotion recognition\" in Google scholar gives 115000 results in March 2020. Among them, there are 2100 just in the first quarter of 2020. Because these projects rely on individuals' emotional responses, the distribution of affective states (classes) is often uneven. However, most of these articles do not mention the class imbalance percentage but instead only report classification accuracy as a performance measuring metric. This creates a serious ambiguity and makes the results incomparable between works. For example, a publicly available database for emotion recognition known as the DEAP database  [11]  has been cited over 1600 times on March 2020, and using \"EEG emotion recognition\" search keywords within the DEAP-citing articles gives more than 1330 results. Out of those 1330 articles, at least 170 articles have included the DEAP dataset in their analysis. Out of those 170 articles, only approximately thirty-three articles mentioned or considered class imbalance. Classification accuracy, without considering class imbalance, is misleading for reasons we will present in this paper. Additionally, out of those 170 articles, only approximately 30 articles discussed statistical significance. This raised a few serious research questions:\n\n1) Are those classification accuracies better than unskilled classifiers? 2) If so, are those accuracies significantly better than chance? 3) In the presence of class imbalance, what is the correct chance level? 4) What performance evaluation metric should be used in affect classification? The main goal of this work is to investigate these questions. As a case study, we will use our investigations into EEGbased detection of binary (high/low) valence, arousal, and dominance in response to different sets of stimuli. For this investigation, we use both our own data as well as the previously mentioned, publicly available DEAP database  [11] .\n\nAffective states can be elicited through visual  [12] , auditory  [13] , and audio-visual stimuli  [14] , among other methods. The emotional experience is more profound when visual presentations are combined with auditory stimuli, intermediate under visual stimuli and minimal during auditory stimuli  [15] . In our experiment, we used visual stimuli, the International Affective Picture System (IAPS)  [12] , to evoke emotions. The DEAP database used audio-visual stimuli.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In the field of affect recognition, a huge number of studies have been conducted on emotion recognition using EEG signals. With the improvement of dry electrodes, EEG is nearing or at the point of being a practical, out of the lab solution for affect recognition. More detailed EEG-based emotion recognition reviews can be found in  [16] ,  [17] . One major problem in EEG-based emotion recognition research is the lack of publicly available datasets. Consequently, researchers use their own data and as a result studies become more difficult to compare. To solve this problem, a few researchers developed publicly available datasets including the DEAP  [11] , USTC-ERVS  [18]  and MAHNOB-HCI datasets  [19] . Among these datasets, the DEAP is the most cited and used for emotion recognition. Thus, we were motivated to use the DEAP dataset in this work.\n\nStudies where DEAP was used as the benchmark dataset mostly used support vector machine (SVM)  [20] -  [26]  for classification. The second most-used classification technique was the k-nearest neighbor (kNN) classifier  [20] ,  [23] ,  [25] . Other classification techniques, such as deep convolutional neural network  [27] , decision tree  [28] , linear discriminate Valence refers to how pleasant or unpleasant an emotion is, and arousal refers to how exciting or boring it is. Words are placed according to direct circular scaling coordinates for 28 affect words from Russel's article  [9] . analysis (LDA)  [29] , logistic regression  [23] , discriminative graph regularized extreme learning machine (GELM)  [23] , back-propagation neural networks (BPNN)  [30] , probabilistic neural networks (PNN)  [30] , and multilayer perceptron (MLP)  [26]  have also been used to classify emotion on the DEAP dataset. Features used in these studies are statistical features: mean, standard deviation, variance, zero crossing rate  [31] ,  [26] ,  [31] -  [33] , Hjorth parameters  [21] ,  [34] , fractal dimension  [31] ,  [35] , Shannon entropy  [31] , spectral entropy  [26] ,  [31] , kurtosis  [36] , skewness  [37] , different EEG band powers  [32] ,  [38] , relative power spectral density (PSD) for delta, theta, alpha, beta and gamma frequency bands  [39] , differential entropy (DE), differential asymmetry (DASM), rational asymmetry (RASM), asymmetry (ASM)  [23] , wavelet coefficients  [25] , and higher order crossings (HOC)  [20] .\n\nIn the DEAP dataset, emotions are expressed in valence, arousal, and dominance dimensions on discrete 9point scales. To design the classification model those scales need to be labeled. Here also, inconsistencies exist between different studies. Not only are different numbers of classes chosen by different groups, but even within the same number of classes the thresholds are different. In these previously mentioned studies on the DEAP, classification labels were created by splitting the ratings into 3-class (1-3:negative, 4-6:neutral, and 7-9:positive)  [40] , 3-class (1-4.5:negative, 4.5-5.5:neutral, 5.5-9:positive)  [26] , 2-class (High/low, 4.5-9: high)  [41] , 2-class (negative ≤ 5 < positive)  [39] , 2-class (negative < 5 ≥ positive)  [24] ,  [42] ,  [43] , and 2-class (1-3: low and 7-9: high)  [33] . Hence, the class imbalance in all these studies are different based on their individual approach when generating class labels.\n\nEven though all these above-mentioned studies used the DEAP dataset, where significant class imbalance exists, very few studies have considered it while reporting results. Studies where class imbalance was considered mainly reported the F1 score  [11] ,  [22] ,  [28] ,  [42] ,  [44]  and a few other studies used receiver operating characteristic (ROC)  [20] ,  [33] , area under ROC (AUC)  [21]  and balanced accuracy  [45]  along with accuracy metric. But AUC can be a misleading metric for a comparative study especially in the presence of variable class imbalance  [46]  and computing the F1 score for multiclass classification is also not straightforward. For multiclass problems, F1 can be computed using macro-averaging or microaveraging  [47] . The difference between macro-and microaveraged F1 can be large; if studies do not report which was used then comparing results is impossible. For example,  [43]  reported classification accuracies of 67% and 69% and F1 scores of 0.67 and 0.69 for valence and arousal, respectively. It is not clear how these F1 scores were calculated. F1 scores for both classes were not considered in that study which makes the study incomparable and provides misleading results.\n\nTo eliminate those above-mentioned problems we are suggesting to use balanced accuracy as the classification performance evaluation metric in high/low valence, arousal and dominance classification. To our knowledge, this has only been used in  [45] . However, that study did not include the computation of credible intervals for balanced accuracy; here in this study we will further discuss using the posterior distribution of balanced accuracy to compute credible intervals and perform statistical significance testing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Data Description",
      "text": "In this work, we have used data from the publicly available DEAP dataset and EEG recordings from our lab.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Database For Emotion Analysis Using Physiological Signals (Deap)",
      "text": "The DEAP is a publicly available, multimodal dataset consisting of 32-channel EEG, electrooculography (EOG), electromyography (EMG), galvanic skin response, respiration, plethysmograph, and temperature data  [11] . These signals were collected from thirty-two healthy participants, with an equal male-female ratio and an average age of 24.9 years. Data were recorded at a sampling rate of 512Hz and then pre-processed.\n\nMinute-long music videos were used as emotional stimuli. After each video, participants were provided enough time to rate those videos for valence, arousal, and dominance on a discrete 9-point scale using self-assessment manikins (SAM)  [48] . Each participant viewed forty videos.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Data Collected At Brain And Body Sensing (Bbs) Lab",
      "text": "The BCI2000  [49]  system was used to present picture stimuli to the participants. Each picture was displayed for 6.7 seconds, followed by a 20.8s pause for participants' selfreport. A total of 244 pictures were selected from IAPS  [12]  images; the average valence and arousal ratings reported in the IAPS manual of the selected pictures are shown in figure  2 . Pictures were presented in six blocks, with breaks for participant comfort. EEG data were recorded using a Cognionics Mobile-72 EEG system with a sampling frequency of 600Hz. The Mobile-72 EEG system is a highdensity mobile EEG system with active Ag/AgCl electrodes placed according to the modified 10-20 system. Reference and ground were on the right and left mastoids, respectively. In total, we had nine participants. Data from two participants\n\nAverage Valence Ratings Each participant rated each stimulus for valence, arousal, and dominance on a discrete 5-point scale using self-assessment manikins (SAM)  [48] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Pre-Processing",
      "text": "For the DEAP, both raw and pre-processed data are available for use. In this work, we will use this MAT-LAB-ready preprocessed version of the data. Pre-processing includes common-average referencing, down-sampling to 128Hz, band-pass filtering with the cut-off frequency at (4.0 -45.0) Hz, and eye blink artifact removal via independent component analysis. The data contain 32 channels of EEG plus an additional eight channels of other physiological signals and the length of the time segment for each trial is 60 seconds. We have only used EEG recordings for classification. Data were then transformed into scalp surface Laplacian or current-source density (CSD) because it has been argued that CSD transformation gives a more sensitive index of individual variations in frontal asymmetry than other EEG recording montages and also helps to reduce non-frontal contributions to the frontal asymmetry  [50] ,  [51] .\n\nThe data collected at the BBS lab was filtered using a finite impulse response (FIR) bandpass filter at (4.0 -45.0) Hz. Data were then transformed into scalp surface Laplacian or current-source density (CSD). To transform the EEG recordings into surface Laplacian, we used the CSD toolbox  [52]  which provides a MATLAB implementation and uses the spherical spline algorithm  [53]  to estimate the surface Laplacian.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Methods",
      "text": "In our study, we will use x(t) ∈ R T as the time series of a recording from a single electrode with N samples. The first and second derivatives of x(t) with respect to time are x ′ (t) and x ′′ (t), respectively. Standard deviation of x(t), x ′ (t) and x ′′ (t) are denoted as σ x , σ d , and σ dd , respectively. Class labels are denoted by c ∈ {1, 2, . . . ,C} and predicted class labels are denoted by y when classifying. H denotes entropy.\n\nA. Feature sets 1) Frequency domain features: Power spectral density and signal power at different frequency bands are popular for EEG-based affective state classification and have been used as features in several studies (e.g.  [54] ,  [55] ). Spectral density and band powers can be computed using various algorithms, including Fast Fourier Transform, short-Time Fourier Transform, or Welch's power spectral density estimations algorithm. Here, we have used Welch's power spectral density (PSD) estimation method  [56]  and then computed power in each band powers from the resulting PSD. The frequency ranges used for EEG bands varies slightly between different studies. In our analysis, the frequency ranges we have used are theta: (4-8) Hz, alpha: (8-12) Hz, Beta-1: (12-18) Hz, Beta-2: (18-30) Hz, Gamma: (31-63) Hz.\n\nIn a few studies, it has been argued that frontal EEG asymmetry can be a moderator and mediator of affective states  [57] ,  [58] . Frontal alpha asymmetry is mostly used as a discriminator between depressed and healthy individuals  [59] . However, it also can be used for affective state classification. Here, we will use both frontal EEG asymmetry (1-50 Hz) and frontal alpha asymmetry (8-12 Hz) as features for classifying affective states. If R p represents the signal power of electrodes located at the right frontal lobe and L p represents the signal power of electrodes located at the left frontal lobe then frontal EEG asymmetry can be calculated from Frontal asymmetry = ln\n\nAnother form of the frontal asymmetry is the normalized version of equation (  1 ) and is written as\n\nHere, we have used equation (1) to find the frontal asymmetry. We have computed separately the frontal asymmetry index (FAI) and frontal alpha asymmetry index (FAAI). The frequency range of 0 -64Hz is used to compute FAI and the alpha band is used for FAAI. We also used frontal theta beta ratios (TBR) as frequency domain features even though TBR has not been used previously for affective classification. But it has been reported to be related with affective traits  [60] . To compute the frontal TBR we used equation (  3 )\n\nhere θ p represents the theta band power and β p represents the beta band power of electrodes located at the frontal lobe.\n\nFrequency ranges for beta-1 and beta-2 are used in β p to compute TBR1 and TBR2, respectively.\n\n2) Hjorth parameters: Hjorth parameters are time-domain features of EEG recording, proposed by Bo Hjorth  [61] . Hjorth parameters have been recently used in several studies  [34] ,  [55]  as features for affective state estimation. The parameters are Activity, Mobility, and Complexity. Activity is simply the variance of the time signal. If the signal is denoted as x(t), then Activity = σ 2\n\nx and is the measure of the squared standard deviation of amplitudes. Mobility measures the standard deviation of the slope with respect to the standard deviation of the amplitude. Mobility is defined as the square root of the ratio between the variances of the first derivative and the time signal. Complexity is a measure of how much the time signal deviates from a pure sine shape and is defined as the ratio between the mobility of the first derivative of the time signal and the mobility of the time signal.\n\nHere, we have used mobility and complexity as features. For each trial, there will be an equal number of mobility and complexity values and the number equals the EEG electrode number.\n\n3) Entropy: Entropy is a measure of disorder in a system. In the case of EEG, entropy measures the irregularity in the signal. Spectral entropy of EEG recordings has been used to discriminate different affective states in other studies  [62]  and it recently has been used in recognition of emotional states  [23] . In this work, we will use spectral entropy (SE), which is the normalized Shannon entropy of the power spectrum.\n\nwhere X is denoting the power spectrum of the time series x(t), p(X) is the spectral distribution such that ∑ N i=1 p(X = i) = 1, and N is the number of frequency bins.\n\n4) Feature sets: For the valence, arousal and dominance classification we used seventeenth different feature sets which are frontal asymmetry index (FAI), frontal alpha asymmetry index (FAAI), theta beta-1 ratio (TBR1), theta beta-2 ratio (TBR2), theta band power (ThetaP), alpha band power (AlphaP), beta band power (BetaP), gamma band power (GammaP), TBR1 and TBR2 together (TBR-C), theta, alpha, beta and gamma band power all together (TABG), Hjorth parameters (Hjorth), entropy (Entropy), power spectral density (PSD), beta alpha ratio (BARatio), all feature sets mentioned previously together (All), and principal components of all feature sets (All-PCA). In case of All-PCA, we used the principal components which consists the 98% of total variability. Using these different feature sets for all 32 participants have resulted a 17 × 32 classification results in each affective dimension for each classifier.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Classification",
      "text": "The ultimate goal for emotion estimation is a many-class classification or continuous-output regression. However, for this initial investigation, we focused on the easier binary classification problem, following multiple literature examples  [24] ,  [33] ,  [39] ,  [41] -  [43] . Thus, we use a two-class classification system for valence, arousal, and dominance. Participants in our experiments rated each axis from 1 to 5, we have labeled ratings < 3 as low valence, arousal, and dominance and ratings ≥ 3 as high valence, arousal, and dominance. One participant never rated arousal less than 3, so for this participant (number 6) we shifted the split point from 3 to 4. In the DEAP database, participants rated each axis from 1 to 9; we have labeled ratings < 5 as low and ratings ≥ 5 as high following the original work  [11]  and some other related studies  [31] ,  [45] ,  [63] .\n\nIn this study, support vector machine (SVM) and Knearest neighbor (kNN) classifiers were used to test the affect recognition from EEG data. For our data, we will use 10-fold cross-validation. In case of DEAP data, we will use \"Leave-One-Out\" cross-validation technique. Which means at each step of the cross-validation, one sample was used as the test set and the rest were used as training set. The reason of using \"Leave-One-Out\" cross-validation in lieu of \"K-fold\" crossvalidation is to maintain the congruity with other studies  [11] ,  [22] ,  [41] ,  [45] . These classifiers are the most commonly used techniques among published reports using the DEAP dataset (e.g.  [20] ,  [25] ,  [26] ,  [31] ,  [33] ,  [39] ,  [45] ,  [63] .\n\n1) Support vector machines (SVMs): SVM uses a kernel trick and a separating hyperplane to create the support vectors. SVMs can be used for both regression and classification. In SVMs, with the observation vector x the predicted class label can be found using  [64]  f (x) = sgn ŵ0\n\nWhere, α i = λ i y i , λ is the ℓ 1 regularization term and k(x i , x) is the kernel function. For Gaussian kernel SVM, the kernel function is defined by\n\nHere we have used the MATLAB built-in function fitcsvm for SVM classifier with a Gaussian kernel.\n\n2) K-Nearest Neighbours (KNN): kNN is a simple classification algorithm where an example is classified based on the plurality vote of its k number of nearest neighbors. The nearest neighbours are chosen by a distance metric. Distance metrics can be City block distance, Chebychev distance, Minkowski distance, Euclidean distance or Mahalanobis distance. Here we have used the built-in MATLAB function knnsearch using Euclidean distance with k = 9 using Euclidean distance. The kernel and hyperparameters for both classifiers are chosen empirically using a 15% test set partition strategy.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Performance Metrics",
      "text": "The most commonly used classification performance measurement metric is accuracy. Nevertheless, accuracy can be misleading, especially with the presence of class imbalance. In these situations, classifiers can learn from class label proportion rather than the features, a property sometimes known as \"unskilled classification.\" In biased datasets, the unskilled performance is equal to the class imbalance. Thus, the same reported accuracy should be interpreted differently based on class bias. For example, consider a study reporting 80% accuracy in a two-class classification. This may be good performance on a balanced dataset but is at or below unskilled classification levels for biases ≥ 80%.\n\nComparing the performance of a similar classification task with different proportions of class labels is difficult. To make this kind of comparison meaningful, researchers suggest using other performance measuring metrics such as the Kappa statistic or area under ROC curve (AUC) for imbalanced data. But since the multiclass ROC curve analysis is not well developed  [65] , AUC is not recommended for multiclass problems  [66] . Moreover, the accuracy metric is the most widely used, and the most intuitive solution would be to make the accuracy metric meaningful by scaling down the baseline to be the performance of an unskilled classifier. One way to scale the baseline is to compute the balanced accuracy  [67]  where the accuracy in each class is considered separately.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Balanced Accuracy",
      "text": "If there are m number of classes, the balanced accuracy  [67]  is defined as\n\nHere, n k is the total number of observations in class k and C kk is the number of correctly classified observations in that same class label. Since our focus is on two-class classification, here, k=2. If the classifier performs equally well on both classes then the balanced accuracy will be exactly equal to the conventional accuracy  [67] ,  [68] . Since balanced accuracy is the average accuracy of each class, it is unaffected by the class imbalance and is more meaningful than the traditional accuracy metric. Further, it has the convenient property that an unskilled classifier always achieves 1/k accuracy regardless of class imbalance.\n\nAlthough the traditional accuracy metric is a scaled binomial random variable, researchers often use a normal posterior distribution to compute credible intervals. The assumption behind the posterior normal distribution comes from the central limit theorem, where for a sufficiently large number of observations (n ≥ 30), a binomial distribution can be approximated using the normal distribution. Nonetheless, this approximation becomes unreliable for small n. Particularly in the case of imbalanced data, the number of observations for the minority class can be smaller than the required number for the normal approximation. Therefore, finding chance performance and the credible interval of the misclassification rate for balanced accuracy is not as straightforward as it is in the case of traditional accuracy. For the two-class classification case, it is a combination of two separate distributions. In a multi-class scenario, accuracy in each class will have a separate distribution.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "1) Credible Intervals Of Balanced Accuracy: If The Probability Of Predicting Correct Classes Of A Classifier Denoted By A",
      "text": "with a prior distribution p(A), then the posterior is expressed as p(A|D) on observed data D. Lets assume y = 1 and y = 0 for correct and incorrect predictions, respectively. Now the classification predictions can be written as y 1 , y 2 , . . . , y n which resembles the results of a Bernoulli experiment. So we can write\n\nIf the total number of success (correct predictions) of a Bernoulli trial y 1 , y 2 , . . . , y n is c, then it follows a Binomial distribution.\n\nThis suggests choosing Beta density as the prior of since it is the conjugate prior of the Binomial distribution. This implies\n\nNow the posterior can be written using Bayes theorem as\n\nFrom equation 11, we obtain the posterior p(A|c, n) = Beta(A|c + 1, nc + 1) and the posterior\n\nwhere F -1 Beta(•) (•) is the inverse density function of the Beta distribution and for 95% credible interval, α = 0.05. In a multiclass scenario, each class has the distribution shown in equation  (11) . To find the posterior of the balanced accuracy m-fold convolution is used for m classes. Numerical approximations are used to compute the posterior since analytical forms are not available for the m-fold convolution. In this work we have used a MATLAB routine to compute the credible intervals of balanced accuracy provided in  [68] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. F1 Measure",
      "text": "Another alternative performance evaluation metric is the F1-measure which has been used in some papers using the DEAP dataset  [11] ,  [22] ,  [41] . The F-measure was originally proposed by Van Rijsbergen  [70]  and is defined as  [71]  F β = (β 2 + 1)PR\n\nwhere P and R denotes precision and recall and are defined as\n\nβ is a parameter to control balance between P and R. When β = 1, F 1 becomes the harmonic mean of precision and recall. Hence the F 1 measure is\n\nSince P and R are calculated considering one class as a positive class, P and R have to be calculated per class and hence the F1 measure as well. P and R per class can be calculated in two ways: microaveraging and macroaveraging.\n\nMicroaveraging aggregates the individual true positives, false positives, and false negatives of each classes to calculate the P and R.\n\nAn alternative technique is known as macroaveraging. In macroaveraging, P and R are calculated for each classes and then F 1 for each class is computed using P and R of individual classes, and finally the macroaverage is the simple average of individual class F 1 scores.\n\nThe difference between miF 1 and maF 1 can be significant. Macro-averaging gives equal weight to each class, whereas micro-averaging gives equal weight to each per-class classification decision. Since F 1 measure ignores true negatives, the influence of large classes is higher than small classes in micro-averaging  [72] . However, the F 1 measure's harmonic means suggest that the averaging should be over the perclass classification decision of each instances. And in that case macro-averaging is not consistent with the original definition of the F 1 measure  [73] . Hence we yet do not have a convincing argument for choosing between miF 1 and maF 1 for multiclass classification.\n\nVI. RESULTS Since we have used seventeen different feature sets, it is not feasible to show all results here. To summarize the results, the classification results are averaged over all participants for each feature set. Those average classification accuracies, and other performance metrics for different feature sets, are presented in figure  3 , figure  4  and table I . All the results presented here are for the SVM classifier since it performed better than the kNN approach.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "A. Deap Dataset",
      "text": "Figure  3 (a) shows the average classification accuracies and balanced accuracies on DEAP for different feature sets using SVM. The mean classification rates for all features are 0.604, 0.637, and 0.648 for valence, arousal, and dominance, respectively. These results are very comparable with the results reported in DEAP original work  [11]  and other related studies  [22] ,  [41] . But then if we check the balanced accuracies on the right side of the figure  3 (a), we will observe very different results. The mean classification rate in balanced accuracies for all feature are 0.544, 0.521, and 0.531 for valence, arousal and dominance respectively. These results are very different than the results with the simple accuracy metric except for valence recognition. The average class bias rate in these three dimensions are 0.59, 0.64 and 0.66 for valence, arousal, and dominance.\n\nFigure  3 (b) shows the average macro-and micro-averaged f1 measure for different feature sets using SVM. The mean macro-f1 for all feature are 0.49, 0.45 and 0.46 for valence, arousal, and dominance, respectively. On the contrary, the mean micro-f1 for all feature are 0.59, 0.62 and 0.63 for valence, arousal, and dominance, respectively. The best classification rate in the valence dimension is achieved using beta band power as a feature, as we found using balanced accuracy. For valence, the average across all participants macro-f1 for BetaP feature is 0.53 and the micro-f1 is 0.61. For arousal, the average across all participants macro-f1 for ThetaP feature is 0.48 and the micro-f1 is 0.63. For dominance, the average across all participants' macro-f1 for the TBR1 feature is 0.50 and the micro-f1 is 0.65.\n\nTable  I  shows the average balanced accuracies and lower bound of the 95% credible intervals of balanced accuracies for different feature sets using equation  (12) . All results are for the SVM classifier. The highest obtained balanced accuracy across all dimensions is 0.5732, achieved for valence recognition using beta band power. Unfortunately, the average lower limit of the credible intervals, in this case, is not above 0.5 (random chance). Though the average provides an overall recognition rate, it does not reflect the performance of individual participants. Explaining results for all features would be cumbersome; here we will explain classification results for each participant for only the best feature in each dimension. For valence, beta band power worked best. Using this feature, the balanced accuracy obtained for a participant (s10) with 0.75 and the lower bound of the credible interval is 0.622, which means that the valence classification rate is significantly above chance for this participant. Out of 32 participants, balanced accuracy is greater than 0.5 for 23 participants. For 8 of these participants, the lower bound of the credible interval is greater than 0.5. For arousal, theta band power worked best. Using the thetaP feature, the highest balanced accuracy obtained for a participant (s17) is 0.73 and the lower bound of the credible interval is 0.60, which means the arousal classification rate is significantly above chance for this participant. For 21 participants, observed balanced accuracy is greater than 0.5. However, only 4 participants were the lower bound of the credible interval greater than 0.5. For dominance, theta beta-1 ratio worked best. Using TBR1, the highest balanced accuracy obtained for a participant (s17) was 0.74 with a lower bound of 0.61, which means the dominance classification rate is significantly above chance for this participant. For 24 participants, balanced accuracy is greater than 0.5. Yet again, only for 4 participants was the lower bound of the credible interval greater than 0.5.\n\nTable  II  shows the affect recognition rate in terms of balanced accuracy, micro and macro averaged F1 score and also compared with the original work  [11]  and some other related studies. Rather than presenting the best results in each dimension, we chose to present results for one specific feature set for consistency. The results presented under the current study are for beta band power (BetaP) feature using an SVM classifier. Note that our comparison studies seem to have picked the best result in each dimension for their reported results (only Clerico et al.  [45]  unambiguously stated this).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Data At Bbs Lab",
      "text": "The data collected at the BBS lab using IAPS came from seven participants. For 2-class classification, the average class-bias were 0.60, 0.72, and 0.82 for valence, arousal, and dominance, respectively. For valence with SVM, the best 2-class classification results were obtained using gammaband power considering the average of all participants. The obtained accuracy was 0.62 and the balanced accuracy was 0.54. The macro and micro averaged f1 scores were 0.49 and 0.60, respectively.\n\nFor arousal with SVM, the best 2-class classification results were obtained using the power asymmetry index (PASI) considering the average of all participants. The obtained accuracy was 0.73 and the balanced accuracy was 0.54. The macro and micro averaged f1 scores were 0.50 and 0.71, respectively.\n\nFor dominance with SVM, the best 2-class classification results were obtained using beta band power considering the   average of all participants. The obtained accuracy was 0.82 and the balanced accuracy was 0.52. The macro and micro averaged f1 scores were 0.46 and 0.82, respectively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vii. Discussion",
      "text": "For the DEAP, the average class bias or majority class percentage in a 2-class classification scenario for valence, arousal and dominance are 0.59, 0.64 and 0.66 respectively. We have argued that class imbalance is important to understand the results of the classifier and should be reported.\n\nPerformance metrics that include or account the class-biases are thus preferred to use. Any metric that ignores class imbalance will mislead readers. To illustrate this, consider the results from table I where balanced accuracies and its lower bound of the 95% credible interval were presented for different feature sets for DEAP data using SVM. The best average classification accuracy for all participants in the valence dimension was 0.602 using beta band power as a feature, whereas the balanced accuracy, in this case, was 0.573. Without knowing the class bias and considering the  However, class imbalance for each participant for all threedimension (valence, arousal, dominance) would be cumbersome and impractical to report. The biases mentioned earlier were averaged across all participants. Since affective state estimation is a participant-specific task, averaged results do not reflect individual performances. So comparisons using average results are not meaningful. Hence, we need something else which can address both the class imbalance problem and make the average performance meaningful. Considering those above-mentioned problems, balanced accuracy is a promising candidate since the baseline performance for balance accuracy is the same (50%) across all dimensions(valence, arousal, dominance) for all participants. Thus, balanced accuracy will make results easier to understand and compare. For example, just looking at the results in table I, we can easily conclude that the valence recognition rate is better than arousal and dominance recognition. Statistical comparison between the balanced accuracies for valence, arousal and dominance presented in table I is done by using MATLAB inbuilt function ttest2. Two-sample t-test resulted in the rejection of the null hypothesis (two groups are equal) when comparing valence and arousal. The valence recognition rate is significantly better than the arousal and dominance recognition rate with p-values 0.035 and 7.44e -06 . The dominance recognition rate is also significantly better than arousal with p-value of 0.031. These three two-sample t-tests suggested that valence has the highest recognition rate and arousal has the lowest for the DEAP dataset.\n\nAverages for all participants of the balanced accuracies, macro, and micro f1 measure were compared with other related studies in Table  II . Since they have not discussed the methods of statistical analysis, here we will use our obtained results shown in table I for discussion. Our average balanced accuracies are very similar to the highest balanced accuracy reported in  [45] . In  [45] , it has been claimed that all the reported balanced accuracies were better than random voting classifiers with p < 0.05. This statement is true if we perform statistical analysis considering results from all participants as a group rather than individual participants. The number of participants with balanced accuracy above 0.5 is 25 for valence using all frequency band powers, 21 for arousal and 20 for dominance. In this case the probability that overall balanced accuracy is above chance are 0.66, 0.66 and 0.63 with intervals (0.47 -0.82), (0.47 -0.82), and (0.44 -0.79) for valence, arousal and dominance, respectively. But the significance of the experiment as a whole does not capture the significance of each participant's performance. Hence, just based on these statistics we are not comfortable to claim the accuracies are above chance. Rather we suggest using the probability of individual participants' performances being above chance to claim the results are significant. Using the number of participants that are significantly above chance, we have 6 for valence, 3 for arousal and 4 for dominance out of 32 participants. That tells us that the probabilities of a participant's classification accuracy being significantly above chance for valence, arousal and dominance are 0.19, 0.09 and 0.13 bounded by (0.07 -0.36), (0.02 -0.25) and (0.04 -.29), respectively. These are not very encouraging, as valence is only above the typical 0.05 threshold. This low rate of significant performance may be of concern for the EEG based affective computing community, and as a community, we need to be more careful while reporting results.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Viii. Conclusion",
      "text": "In this work, we presented the experimental results for affective state estimation using the publicly available DEAP database and our lab data. We compared our results for DEAP data with the results reported in other few related studies. We used various features mentioned in the literature and also investigated theta-beta1 ratio as a novel features for affect classification. Our findings showed that the beta band power is most suitable for valence classification, theta band power for arousal classification, and theta beta-1 ratio for dominance classification.\n\nIn conclusion, we suggest using balanced accuracy and its posterior distribution as the performance evaluation metric for emotion estimation. Though F1 measure is a popular choice, it is not yet well established which F1 measure (macro/micro) we should use for multiclass classification. As our results demonstrate, that choice is important. Further, if macro-averaging is chosen, the statistical significance of the metric is not well understood.\n\nIn contrast to the F1 measure, balanced accuracy has several advantages. First, balanced accuracy does not have a \"preferred class\" and is thus comparable between groups. Second, the credible bounds can be calculated using known formulas. Third, the extension to large numbers of classes is straightforward. Fourth and finally, balanced accuracy is insensitive to class bias and always has the intuitive 1/k chance performance for unskilled classifiers.\n\nWe note that traditional accuracy metrics would have classified the performance of many more of our participants as statistically significant, relative to the number classified this way by balanced accuracy. Nevertheless, we maintain that balanced accuracy is far less misleading, and that the traditional accuracy metric substantially over-estimates performance is these unbalanced datasets.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ), which was initially",
      "page": 1
    },
    {
      "caption": "Figure 1: An example of the circumplex model where emotions are",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualization of average valence and arousal ratings (from",
      "page": 3
    },
    {
      "caption": "Figure 3: (a) shows the average classiﬁcation accuracies and",
      "page": 7
    },
    {
      "caption": "Figure 3: (b) shows the average macro- and micro-averaged",
      "page": 7
    },
    {
      "caption": "Figure 3: Average classiﬁcation rate of all participants in valence, arousal and dominance recognition for different features using DEAP",
      "page": 8
    },
    {
      "caption": "Figure 4: Average classiﬁcation rate of all participants in valence, arousal and dominance recognition for different features using BBS data.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arousing": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arousing": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Features": "PASI",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5448\n0.4297",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5279\n0.4227",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5317\n0.4262"
        },
        {
          "Features": "FAI",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5222\n0.4089",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5118\n0.4130",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5219\n0.4178"
        },
        {
          "Features": "TBR1",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5479\n0.4267",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5247\n0.4235",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5568\n0.4435"
        },
        {
          "Features": "TBR2",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5381\n0.4198",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5109\n0.4090",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5220\n0.4142"
        },
        {
          "Features": "ThetaP",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5388\n0.4211",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5371\n0.4336",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5302\n0.4206"
        },
        {
          "Features": "AlphaP",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5432\n0.4286",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5238\n0.4281",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5492\n0.4432"
        },
        {
          "Features": "BetaP",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5732\n0.4531",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5303\n0.4263",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5370\n0.4247"
        },
        {
          "Features": "GammaP",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5585\n0.4381",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5282\n0.4265",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5409\n0.4323"
        },
        {
          "Features": "TBR-C",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5663\n0.4482",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5318\n0.4263",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5550\n0.4439"
        },
        {
          "Features": "TABG",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5578\n0.4401",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5090\n0.4122",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5349\n0.4301"
        },
        {
          "Features": "Hjorth",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5323\n0.4159",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5268\n0.4268",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5204\n0.4104"
        },
        {
          "Features": "PASI+FASI",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5473\n0.4355",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5214\n0.4207",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5338\n0.4307"
        },
        {
          "Features": "Avg-Entropy",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5158\n0.4177",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5200\n0.4312",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5176\n0.4269"
        },
        {
          "Features": "PSD",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5525\n0.4451",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5148\n0.4259",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5292\n0.4361"
        },
        {
          "Features": "BARatio",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5230\n0.4077",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5016\n0.4054",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5115\n0.4059"
        },
        {
          "Features": "All",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5517\n0.4447",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5069\n0.4178",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5229\n0.4290"
        },
        {
          "Features": "All-PCA",
          "Valence\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5365\n0.4160",
          "Arousal\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5166\n0.4086",
          "Dominance\nBalanced\nLower bound\nAccuracy (BAcc)\nof BAcc": "0.5484\n0.4329"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence\nbAcc miF1 maF1": "Koelstra et al.\n[11]\n–\n–\n0.563\nDaimi et al.\n[41]\n–\n–\n0.550\nSoleymani et al.\n[22]\n–\n–\n0.645\nClerico et al.\n[45]\n0.604\n–\n–\nCurrent study\n0.573\n0.610\n0.530",
          "Arousal\nbAcc miF1 maF1": "–\n–\n0.583\n–\n–\n0.570\n–\n–\n0.570\n0.583\n–\n–\n0.530\n0.620\n0.460",
          "Dominance\nbAcc miF1 maF1": "–\n–\n–\n–\n–\n0.552\n–\n–\n0.533\n0.564\n–\n–\n0.537\n0.630\n0.460"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Brain-computer interface technology: a review of the first international meeting",
      "authors": [
        "J Wolpaw",
        "N Birbaumer",
        "W Heetderks",
        "D Mcfarland",
        "P Peckham",
        "G Schalk",
        "E Donchin",
        "L Quatrano",
        "C Robinson",
        "T Vaughan"
      ],
      "year": "2000",
      "venue": "IEEE transactions on rehabilitation engineering"
    },
    {
      "citation_id": "2",
      "title": "Mood and judgment: the affect infusion model (aim)",
      "authors": [
        "J Forgas"
      ],
      "year": "1995",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "3",
      "title": "Emotional intelligence",
      "authors": [
        "P Salovey",
        "J Mayer"
      ],
      "year": "1990",
      "venue": "Imagination, cognition and personality"
    },
    {
      "citation_id": "4",
      "title": "Why it can matter more than IQ",
      "authors": [
        "D Goleman"
      ],
      "year": "1996",
      "venue": "Learning"
    },
    {
      "citation_id": "5",
      "title": "Salience of comparison standards and the activation of social norms: Consequences for judgements of happiness and their communication",
      "authors": [
        "F Strack",
        "N Schwarz",
        "B Chassein",
        "D Kern",
        "D Wagner"
      ],
      "year": "1990",
      "venue": "British Journal of Social Psychology"
    },
    {
      "citation_id": "6",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "7",
      "title": "Automatic analysis of facial expressions: The state of the art",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Studies of emotion: A theoretical and empirical review of psychophysiological studies of emotion",
      "authors": [
        "C Niemic",
        "K Warren"
      ],
      "year": "2002",
      "venue": "Journal of Undergraduate Research Rochester"
    },
    {
      "citation_id": "9",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "10",
      "title": "Inferring psychological significance from physiological signals",
      "authors": [
        "J Cacioppo",
        "L Tassinary"
      ],
      "year": "1990",
      "venue": "American psychologist"
    },
    {
      "citation_id": "11",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual",
      "authors": [
        "P Lang",
        "M Bradley",
        "B Cuthbert"
      ],
      "year": "2008",
      "venue": "International affective picture system (IAPS): Affective ratings of pictures and instruction manual"
    },
    {
      "citation_id": "13",
      "title": "International affective digitized sounds (IADS): Stimuli, instruction manual and affective ratings",
      "authors": [
        "P Lang",
        "M Bradley"
      ],
      "year": "1999",
      "venue": "The Center for Research in Psychophysiology, University of Florida"
    },
    {
      "citation_id": "14",
      "title": "Liris-accede: A video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "A review of brain oscillations in perception of faces and emotional pictures",
      "authors": [
        "B Güntekin",
        "E Bas ¸ar"
      ],
      "year": "2014",
      "venue": "Neuropsychologia"
    },
    {
      "citation_id": "16",
      "title": "Electroencephalograph (eeg) based emotion recognition system: A review",
      "authors": [
        "K Wagh",
        "K Vasanth"
      ],
      "year": "2019",
      "venue": "Innovations in Electronics and Communication Engineering"
    },
    {
      "citation_id": "17",
      "title": "A review on nonlinear methods using electroencephalographic recordings for emotion recognition",
      "authors": [
        "B García-Martínez",
        "A Martinez-Rodrigo",
        "R Alcaraz",
        "A Fernández-Caballero"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Hybrid video emotional tagging using users' eeg and video content",
      "authors": [
        "S Wang",
        "Y Zhu",
        "G Wu",
        "Q Ji"
      ],
      "year": "2014",
      "venue": "Multimedia tools and applications"
    },
    {
      "citation_id": "19",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "A mutual information based adaptive windowing of informative eeg for emotion recognition",
      "authors": [
        "L Piho",
        "T Tjahjadi"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Exploring eeg features in cross-subject emotion recognition",
      "authors": [
        "X Li",
        "D Song",
        "P Zhang",
        "Y Zhang",
        "Y Hou",
        "B Hu"
      ],
      "year": "2018",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "22",
      "title": "Toolbox for emotional feature extraction from physiological signals (teap)",
      "authors": [
        "M Soleymani",
        "F Villaro-Dixon",
        "T Pun",
        "G Chanel"
      ],
      "year": "2017",
      "venue": "Frontiers in ICT"
    },
    {
      "citation_id": "23",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Content-based video emotion tagging augmented by users' multiple physiological responses",
      "authors": [
        "S Wang",
        "S Chen",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition based on eeg features in movie clips with channel selection",
      "authors": [
        "M Özerdem",
        "H Polat"
      ],
      "year": "2017",
      "venue": "Brain informatics"
    },
    {
      "citation_id": "26",
      "title": "Affect representation and recognition in 3d continuous valence-arousal-dominance space",
      "authors": [
        "G Verma",
        "U Tiwary"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "27",
      "title": "Human emotion recognition with electroencephalographic multidimensional features by hybrid deep neural networks",
      "authors": [
        "Y Li",
        "J Huang",
        "H Zhou",
        "N Zhong"
      ],
      "year": "2017",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "28",
      "title": "Application of entropy-based metrics to identify emotional distress from electroencephalographic recordings",
      "authors": [
        "B García-Martínez",
        "A Martínez-Rodrigo",
        "R Zangróniz Cantabrana",
        "J García",
        "R Alcaraz"
      ],
      "year": "2016",
      "venue": "Entropy"
    },
    {
      "citation_id": "29",
      "title": "Anytime multipurpose emotion recognition from eeg data using a liquid state machine based framework",
      "authors": [
        "O Al Zoubi",
        "M Awad",
        "N Kasabov"
      ],
      "year": "2018",
      "venue": "Artificial intelligence in medicine"
    },
    {
      "citation_id": "30",
      "title": "Development of filtered bispectrum for eeg signal feature extraction in automatic emotion recognition using artificial neural networks",
      "authors": [
        "P Purnamasari",
        "A Ratna",
        "B Kusumoputro"
      ],
      "year": "2017",
      "venue": "Algorithms"
    },
    {
      "citation_id": "31",
      "title": "Emotion detection from eeg recordings based on supervised and unsupervised dimension reduction",
      "authors": [
        "J Liu",
        "H Meng",
        "M Li",
        "F Zhang",
        "R Qin",
        "A Nandi"
      ],
      "year": "2018",
      "venue": "Concurrency and Computation: Practice and Experience"
    },
    {
      "citation_id": "32",
      "title": "Svm-based feature selection methods for emotion recognition from multimodal data",
      "authors": [
        "C Torres-Valencia",
        "M Álvarez-López",
        "Á Orozco-Gutiérrez"
      ],
      "year": "2017",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "33",
      "title": "Towards emotion recognition for virtual environments: an evaluation of eeg features on benchmark dataset",
      "authors": [
        "M Menezes",
        "A Samara",
        "L Galway",
        "A Sant'anna",
        "A Verikas",
        "F Alonso-Fernandez",
        "H Wang",
        "R Bond"
      ],
      "year": "2017",
      "venue": "Personal and Ubiquitous Computing"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition from eeg signals by using multivariate empirical mode decomposition",
      "authors": [
        "A Mert",
        "A Akan"
      ],
      "year": "2018",
      "venue": "Pattern Analysis and Applications"
    },
    {
      "citation_id": "35",
      "title": "Evolutionary computation algorithms for feature selection of eegbased emotion recognition using mobile sensors",
      "authors": [
        "B Nakisa",
        "M Rastgoo",
        "D Tjondronegoro",
        "V Chandran"
      ],
      "year": "2018",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "36",
      "title": "Brain signal based human emotion analysis by circular back propagation and deep kohonen neural networks",
      "authors": [
        "D Hemanth",
        "J Anitha"
      ],
      "year": "2018",
      "venue": "Computers & Electrical Engineering"
    },
    {
      "citation_id": "37",
      "title": "Cross-subject eeg feature selection for emotion recognition using transfer recursive feature elimination",
      "authors": [
        "Z Yin",
        "Y Wang",
        "L Liu",
        "W Zhang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "38",
      "title": "Eeg-based emotion estimation using bayesian weighted-log-posterior function and perceptron convergence algorithm",
      "authors": [
        "H Yoon",
        "S Chung"
      ],
      "year": "2013",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition with the help of privileged information",
      "authors": [
        "S Wang",
        "Y Zhu",
        "L Yue",
        "Q Ji"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "40",
      "title": "Eeg-based emotion recognition using deep learning network with principal component based covariate shift adaptation",
      "authors": [
        "S Jirayucharoensak",
        "S Pan-Ngum",
        "P Israsena"
      ],
      "year": "2014",
      "venue": "The Scientific World Journal"
    },
    {
      "citation_id": "41",
      "title": "Classification of emotions induced by music videos and correlation with participants' rating",
      "authors": [
        "S Daimi",
        "G Saha"
      ],
      "year": "2014",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "42",
      "title": "Emotion discrimination using spatially compact regions of interest extracted from imaging eeg activity",
      "authors": [
        "J Padilla-Buritica",
        "J Martinez-Vargas",
        "G Castellanos-Dominguez"
      ],
      "year": "2016",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "43",
      "title": "Relevance vector classifier decision fusion and eeg graph-theoretic features for automatic affective state characterization",
      "authors": [
        "R Gupta",
        "T Falk"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "44",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Z Yin",
        "M Zhao",
        "Y Wang",
        "J Yang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "45",
      "title": "Electroencephalography amplitude modulation analysis for automated affective tagging of music video clips",
      "authors": [
        "A Clerico",
        "A Tiwari",
        "R Gupta",
        "S Jayaraman",
        "T Falk"
      ],
      "year": "2018",
      "venue": "Frontiers in computational neuroscience"
    },
    {
      "citation_id": "46",
      "title": "Auc: a misleading measure of the performance of predictive distribution models",
      "authors": [
        "J Lobo",
        "A Jiménez-Valverde"
      ],
      "year": "2008",
      "venue": "Global ecology and Biogeography"
    },
    {
      "citation_id": "47",
      "title": "Macro-and micro-averaged evaluation measures",
      "authors": [
        "V Van Asch"
      ],
      "year": "2013",
      "venue": "Belgium: CLiPS"
    },
    {
      "citation_id": "48",
      "title": "Measuring emotion: the self-assessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "49",
      "title": "BCI2000: a general-purpose brain-computer interface (BCI) system",
      "authors": [
        "G Schalk",
        "D Mcfarland",
        "T Hinterberger",
        "N Birbaumer",
        "J Wolpaw"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on biomedical engineering"
    },
    {
      "citation_id": "50",
      "title": "Should it matter when we record? time of year and time of day as factors influencing frontal eeg asymmetry",
      "authors": [
        "J Velo",
        "J Stewart",
        "B Hasler",
        "D Towers",
        "J Allen"
      ],
      "year": "2012",
      "venue": "Should it matter when we record? time of year and time of day as factors influencing frontal eeg asymmetry"
    },
    {
      "citation_id": "51",
      "title": "Frontal eeg asymmetry as a promising marker of depression vulnerability: Summary and methodological considerations",
      "authors": [
        "J Allen",
        "S Reznik"
      ],
      "year": "2015",
      "venue": "Current opinion in psychology"
    },
    {
      "citation_id": "52",
      "title": "Principal components analysis of laplacian waveforms as a generic method for identifying erp generator patterns: I. evaluation with auditory oddball tasks",
      "authors": [
        "J Kayser",
        "C Tenke"
      ],
      "year": "2006",
      "venue": "Clinical neurophysiology"
    },
    {
      "citation_id": "53",
      "title": "Spherical splines for scalp potential and current density mapping",
      "authors": [
        "F Perrin",
        "J Pernier",
        "O Bertrand",
        "J Echallier"
      ],
      "year": "1989",
      "venue": "Electroencephalography and clinical neurophysiology"
    },
    {
      "citation_id": "54",
      "title": "Eeg-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin",
        "C.-H Wang",
        "T.-P Jung",
        "T.-L Wu",
        "S.-K Jeng",
        "J.-R Duann",
        "J.-H Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "55",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "56",
      "title": "The use of fast fourier transform for the estimation of power spectra: a method based on time averaging over short, modified periodograms",
      "authors": [
        "P Welch"
      ],
      "year": "1967",
      "venue": "IEEE Transactions on audio and electroacoustics"
    },
    {
      "citation_id": "57",
      "title": "Frontal eeg asymmetry as a moderator and mediator of emotion",
      "authors": [
        "J Coan",
        "J Allen"
      ],
      "year": "2004",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "58",
      "title": "Issues and assumptions on the road from raw signals to metrics of frontal eeg asymmetry in emotion",
      "authors": [
        "J Allen",
        "J Coan",
        "M Nazarian"
      ],
      "year": "2004",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "59",
      "title": "Frontal alpha asymmetry as a diagnostic marker in depression: Fact or fiction? a meta-analysis",
      "authors": [
        "N Van Der Vinne",
        "M Vollebregt",
        "M Van Putten",
        "M Arns"
      ],
      "year": "2017",
      "venue": "Frontal alpha asymmetry as a diagnostic marker in depression: Fact or fiction? a meta-analysis"
    },
    {
      "citation_id": "60",
      "title": "Eeg theta/beta ratio in relation to fear-modulated response-inhibition, attentional control, and affective traits",
      "authors": [
        "P Putman",
        "J Van Peer",
        "I Maimari",
        "S Van Der Werff"
      ],
      "year": "2010",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "61",
      "title": "Eeg analysis based on time domain properties",
      "authors": [
        "B Hjorth"
      ],
      "year": "1970",
      "venue": "Electroencephalography and clinical neurophysiology"
    },
    {
      "citation_id": "62",
      "title": "Time-frequency balanced spectral entropy as a measure of anesthetic drug effect in central nervous system during sevoflurane, propofol, and thiopental anesthesia",
      "authors": [
        "A Vakkuri",
        "A Yli-Hankala",
        "P Talja",
        "S Mustola",
        "H Tolvanen-Laakso",
        "T Sampson",
        "H Viertiö-Oja"
      ],
      "year": "2004",
      "venue": "Acta Anaesthesiologica Scandinavica"
    },
    {
      "citation_id": "63",
      "title": "Wavelet-based emotion recognition system using eeg signal",
      "authors": [
        "Z Mohammadi",
        "J Frounchi",
        "M Amiri"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "64",
      "title": "Machine Learning: A Probabilistic Perspective, ser. Adaptive Computation and Machine Learning",
      "authors": [
        "K Murphy"
      ],
      "year": "2012",
      "venue": "Machine Learning: A Probabilistic Perspective, ser. Adaptive Computation and Machine Learning"
    },
    {
      "citation_id": "65",
      "title": "Improving accuracy and cost of twoclass and multi-class probabilistic classifiers using roc curves",
      "authors": [
        "N Lachiche",
        "P Flach"
      ],
      "year": "2003",
      "venue": "Proceedings of the 20th International Conference on Machine Learning (ICML-03)"
    },
    {
      "citation_id": "66",
      "title": "A systematic analysis of performance measures for classification tasks",
      "authors": [
        "M Sokolova",
        "G Lapalme"
      ],
      "year": "2009",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "67",
      "title": "A balanced accuracy function for epistasis modeling in imbalanced datasets using multifactor dimensionality reduction",
      "authors": [
        "D Velez",
        "B White",
        "A Motsinger",
        "W Bush",
        "M Ritchie",
        "S Williams",
        "J Moore"
      ],
      "year": "2007",
      "venue": "Genetic epidemiology"
    },
    {
      "citation_id": "68",
      "title": "The balanced accuracy and its posterior distribution",
      "authors": [
        "K Brodersen",
        "C Ong",
        "K Stephan",
        "J Buhmann"
      ],
      "year": "2010",
      "venue": "Pattern recognition (ICPR), 2010 20th international conference on"
    },
    {
      "citation_id": "69",
      "title": "Probabilistic performance evaluation for multiclass classification using the posterior balanced accuracy",
      "authors": [
        "H Carrillo",
        "K Brodersen",
        "J Castellanos"
      ],
      "year": "2014",
      "venue": "ROBOT2013: First Iberian Robotics Conference"
    },
    {
      "citation_id": "70",
      "title": "Information retrieval. 2nd. newton, ma",
      "authors": [
        "C Van Rijsbergen"
      ],
      "year": "1979",
      "venue": "Information retrieval. 2nd. newton, ma"
    },
    {
      "citation_id": "71",
      "title": "Muc-4 evaluation metrics",
      "authors": [
        "N Chinchor"
      ],
      "year": "1992",
      "venue": "Proceedings of the 4th conference on Message understanding"
    },
    {
      "citation_id": "72",
      "title": "Introduction to information retrieval",
      "authors": [
        "C Manning",
        "P Raghavan",
        "H Schütze"
      ],
      "year": "2010",
      "venue": "Natural Language Engineering"
    },
    {
      "citation_id": "73",
      "title": "What the f-measure doesn't measure: Features, flaws, fallacies and fixes",
      "authors": [
        "D Powers"
      ],
      "year": "2015",
      "venue": "What the f-measure doesn't measure: Features, flaws, fallacies and fixes",
      "arxiv": "arXiv:1503.06410"
    }
  ]
}