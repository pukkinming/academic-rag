{
  "paper_id": "2508.02018v1",
  "title": "Speechr: A Benchmark For Speech Reasoning In Large Audio-Language Models",
  "published": "2025-08-04T03:28:04Z",
  "authors": [
    "Wanqi Yang",
    "Yanda Li",
    "Yunchao Wei",
    "Meng Fang",
    "Ling Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large audio-language models (LALMs) have achieved nearhuman performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speechbased scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiplechoice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-theart LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks. We release the benchmark and evaluation code to facilitate future research. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Driven by the rapid progress in speech processing and largescale multimodal pretraining, large audio-language models (LALMs) have demonstrated strong capabilities in understanding and generating natural language from speech input  (Chu et al. 2024; Ghosh et al. 2024; Touvron et al. 2023 ). These models bridge acoustic perception and linguistic inference, enabling applications that require not only transcription but also contextual interpretation. Such capabilities have expanded the potential of LALMs in real-world scenarios, including voice-based virtual assistants  (Zhang et al. 2023; Huang et al. 2024) , AI-powered educational tools  (Yang and Taele 2025) , and human-computer dialogue systems  (Xue et al. 2024; Rubenstein et al. 2023) . However, while current LALMs excel at transcription and basic speech understanding, their ability to perform contextual and inference-driven reasoning over speech remains underexplored, highlighting the need for systematic benchmarks that go beyond surface-level perception and evaluate diverse reasoning capabilities in speech-based scenarios.\n\nDespite these advancements, existing evaluation efforts remain centered on low-level perceptual tasks. Benchmarks such as automatic speech recognition (ASR)  (Radford et al. 2023; Yao et al. 2023; Bai et al. 2024 ) and emotion classification  (Yoon, Byun, and Jung 2018; Ma et al. 2023; Wang et al. 2024b ) assess fundamental abilities like phoneme decoding or affective state detection, but overlook the models' capacity for nuanced interpretation or complex inference. Moreover, many existing audio datasets, including those for sound event detection  (Ye et al. 2021; Vesperini et al. 2019)  or music tagging  (Liu et al. 2024; Melechovsky et al. 2023; Gardner et al. 2023) , lack the linguistic and contextual richness required for evaluating spoken reasoning. While recent benchmarks such as MMAU  (Sakshi et al. 2024)  and MMAR  (Ma et al. 2025b ) have begun exploring audio-based reasoning, they often define reasoning narrowly as single-step inference over isolated clips or focus on openended generation without clear task-type granularity or reproducible evaluation formats. In contrast, SpeechR offers a structured benchmark tailored to speech reasoning, featuring fine-grained categorization across factual, procedural, and normative tasks, and supporting both multiple-choice and generative evaluations under controlled prosodic and emotional variations.\n\nTo address this gap, we introduce SpeechR, an audiolanguage reasoning benchmark designed to evaluate large audio-language models on reasoning tasks grounded in speech scenarios, encompassing a diverse range of reasoning types. Speech data in SpeechR are generated from curated textual reasoning tasks, allowing precise control over verbal content, prosody, and structure while preserving the cognitive intent of the original tasks. This design enables consistent evaluation across diverse reasoning formats and acoustic conditions.\n\nAs shown in Figure  1 , SpeechR focuses on three major reasoning types central to spoken interaction and decisionmaking: (1) Factual Reasoning, which involves retrieving or confirming concrete information; (2) Procedural Reasoning, which requires understanding step-by-step processes or causal dependencies; and (3) Normative Reasoning, which evaluates judgments based on social, ethical, or behavioral norms. In addition, SpeechR is released in three versions to support a wide range of evaluation needs. The multiplechoice version provides a standardized and transparent format, enabling consistent accuracy measurement across models. The generative version is designed to assess a model's ability to construct coherent and logically grounded reasoning chains, particularly in procedurally or normatively reasoning tasks. The acoustic-feature version emphasizes acoustic diversity, incorporating variations such as emotional tone and prosodic stress, thereby facilitating the study of how acoustic features influence reasoning performance.\n\nFinally, we evaluate SpeechR using eleven state-of-the-art LALMs, including Qwen-Audio  (Chu et al. 2024) , LLama-Omni  (Fang et al. 2024) , GPT-4o-audio  (Achiam et al. 2023) , and others. Our experiments cover all three benchmark versions and focus on three key evaluation perspectives: reasoning accuracy, coherence and logical quality of generated outputs, and model robustness under prosodic variation. This analysis provides a comprehensive view of current LALM capabilities in speech reasoning scenarios.\n\nThe key contributions of this work are: 1. We introduce SpeechR, the first benchmark for systematically evaluating speech reasoning in factual, procedural, and normative tasks.\n\n2. We release three benchmark versions, each aligned with a specific evaluation protocol: discrete choice evaluation for multiple-choice and acoustic-feature versions, and LLM-asa-judge evaluation for generative version assessment.\n\n3. We conduct a comprehensive evaluation of state-of-theart LALMs and offer detailed analyses on reasoning accuracy, logical coherence of response generated, and the impact of acoustic variations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works Large Audio Language Models",
      "text": "Building on LLMs' (OpenAI 2023;  Team et al. 2023; Bai et al. 2023; Touvron et al. 2023 ) demonstrated reasoning capabilities, recent Large Audio Language Models (LALMs) unify audio and text into shared representations for robust cross-modal inference. CLAP  (Elizalde et al. 2023 ) learns joint embeddings via large-scale contrastive learning, enabling zero-shot retrieval and downstream tasks. CompA  (Ghosh et al. 2023 ) extends this foundation with benchmarks for compositional reasoning. Subsequent LALMs  (Chu et al. 2023; Deshmukh et al. 2023; Gong et al. 2023b; Fang et al. 2024 ) integrate audio and text processing within a single pipeline, enabling interactive dialogue, audio summarization, and multistep inference: SpeechGPT  (Zhang et al. 2023 ) and Au-dioGPT  (Huang et al. 2024)  integrate ASR/TTS for interactive dialogue; Qwen2-Audio  (Chu et al. 2024 ) and SALMONN  (Tang et al. 2023 ) enable robust instruction following across speech, music, and environmental sounds; GAMA  (Ghosh et al. 2024 ) employs synthetic instruction tuning for advanced audio understanding; Audio-CoT  (Ma et al. 2025a ) and Audio-Reasoner  (Xie et al. 2025)  introduce chain-of-thought frameworks; and compact models like Mellow  (Deshmukh et al. 2025 ) achieve competitive reasoning under resource constraints. Moreover, multimodal LLMs such as GPT-4o  (Achiam et al. 2023 ) and Gemini 1.5-Pro  (Reid et al. 2024 ) have demonstrated exceptional interactive capabilities across vision, text, and audio. This evolution underscores the necessity for dedicated, context-aware audio reasoning benchmarks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Reasoning Benchmark",
      "text": "Large-scale corpora such as LibriSpeech  (Panayotov et al. 2015) , Common Voice  (Ardila et al. 2019)    (speech, text, label) , where the speech is a synthesized utterance containing either a spoken question or a dialogue. The text component always includes the transcription, while additional elements such as multi-step reasoning chains, answer candidates, and acoustic features are included depending on the specific dataset version. The label encodes the correct answer for evaluation. Note that the transcription is provided solely for evaluation purposes, such as verifying model outputs and facilitating human inspection, but it is never used as input to the model during inference.\n\nThe construction pipeline of the SpeechR benchmark is illustrated in Figure  2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Text Data Construction",
      "text": "While several audio datasets exist  (Sakshi et al. 2024; Ma et al. 2025b,a) , they typically lack the reasoning diversity, structural clarity, and compositional complexity required for evaluating speech reasoning. To address these limitations, SpeechR builds upon high-quality text-based reasoning datasets  (Clark et al. 2019; Cobbe et al. 2021; Talmor et al. 2018; Yu et al. 2020) , which offer diverse tasks, welldefined reasoning structures, and extended inference chains. These properties make them a valuable foundation for constructing a challenging benchmark for spoken reasoning.\n\nTo ensure high-quality and cognitively diverse content, our text data construction follows six steps: (1) reasoning type design, (2) data source selection, (3) readability enhancement, (4) interaction enrichment, (5) acoustic feature annotation, and (6) versions.\n\nReasoning Type Design We categorize reasoning tasks in SpeechR based on three core dimensions that reflect the cognitive characteristics of reasoning: (S1) knowledge dependence, (S2) reasoning transparency, and (S3) evaluation determination.\n\n• S1 identifies the primary knowledge source required to solve the reasoning task: factual knowledge (e.g., commonsense or background knowledge), procedural rules (e.g., mathematical or scientific laws), or normative principles (e.g., moral, social, or behavioral norms).\n\n• S2 evaluates the level of reasoning transparency, i.e., whether solving a task requires intermediate steps such as logical deduction or chain-of-thought reasoning.\n\n• S3 evaluates whether a task has a clearly objective ground-truth answer, as opposed to relying on subjective judgment.\n\nBased on these dimensions, we define three reasoning types in SpeechR shown in Table  1:  • Factual Reasoning involves understanding and retrieving factual or commonsense information. These tasks do not require multi-step reasoning and are evaluated using objective criteria. Examples include \"Why are cats afraid of water?\" or \"Where did you go yesterday?\", which rely on language comprehension and real-world knowledge.\n\n• Procedural Reasoning requires explicit, multi-step logical or numerical inference. Tasks in this category often involve deterministic reasoning chains and verifiable outputs, such as \"What is 12 divided by 3 plus 5?\" or \"If A is taller than B, and B is taller than C, who is the tallest?\". • Normative Reasoning involves assessing actions, intentions, or social behaviors based on implicit moral or social norms. These tasks may involve open-ended answers and often require subjective evaluation, reflecting the inherent ambiguity in social and ethical reasoning. Examples include \"Is this message likely to be a scam?\" or \"Was second speaker's behavior ethically acceptable?\".\n\nIn summary, SpeechR adopts a three-type taxonomy consisting of factual, procedural, and normative reasoning, based on cognitive criteria (S1 to S3), enabling comprehensive evaluation of LALMs' reasoning breadth and depth in speech-based scenarios. Table  1  outlines the criteria used in our categorization, along with the corresponding reasoning types included in SpeechR.\n\nData Source Selection We carefully select data from a broad range of existing text-based reasoning benchmarks. The selected datasets are required to satisfy the following criteria: (1) they must contain at least one of the three reasoning types-Factual, Procedural, or Normative; (2) they must exhibit well-structured formats; and (3) they must provide clearly defined evaluation standards. The specific datasets selected are detailed in Appendix. To mitigate potential data leakage from pretraining or fine-tuning, we exclusively use the official test splits of each dataset. While full overlap checks are infeasible for proprietary LLMs, this design choice ensures maximal separation between benchmark content and training data.\n\nReadability Enhancement To ensure high-quality speech synthesis and maintain semantic clarity, we normalize the text to remove noisy artifacts (e.g., emojis, inconsistent punctuation, abbreviations). This process improves both the fluency of synthesized speech and the interpretability of reasoning content. Detailed rules and examples are included in the Appendix.\n\nInteraction Enrichment To better evaluate dialoguebased reasoning, we transform selected instances into twospeaker conversational formats using rule-based restructuring. This includes adding contextual prompts and adapting pronouns to reflect interpersonal exchanges. Such conversions simulate realistic turn-taking in spoken interaction and assess a model's ability to reason across dialogue boundaries. Implementation details are provided in the Appendix.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Annotation Of Acoustic Features",
      "text": "The key difference between speech and text lies in the presence of acoustic cues, such as prosody and emotion, which convey speaker intent, emphasis, and emotional tone. These cues may influence how information is processed and interpreted during speech reasoning.\n\nTo examine whether large audio-language models can utilize such cues, SpeechR includes annotations for two types of acoustic features: stress, which indicates prosodic emphasis, and emotion, which reflects affective tone. The detailed annotation procedure, including prompt design and synthesis control, is described in Appendix.\n\nVersions SpeechR is released in three versions: the multiple-choice version, the generative version, and the acoustic-feature version.\n\nThe multiple-choice version adopts the format:\n\nThis classification-based setup provides a more standardized and reliable evaluation framework, allowing for direct accuracy-based comparisons across models and reasoning types.\n\nThe generative version is structured as:\n\nThis version removes pre-defined answer options, allowing models to generate free-form responses. It is particularly suited for analyzing whether LALMs can produce coherent and logically valid reasoning chains, especially in procedural and normative reasoning.\n\nThe acoustic-feature version is a 10% random subset of the multiple-choice version, enriched with acoustic annotations. Its format is:\n\nSample acoustic-f eature = (speech, text = {transcription, answer candidates, stress, emotion}, label) This version includes acoustic features such as stress and emotional tone, and aims to explore whether speech-specific attributes influence the reasoning abilities of LALMs.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Generation",
      "text": "We employ a speech tool, Azure Speech SDK, to synthesize speech from the constructed transcriptions. We select this tool due to its fine-grained controllability and high-quality output, which are essential for systematically varying acoustic features in SpeechR. Developed by Microsoft, the Azure Speech SDK is a cloud-based framework that supports highquality text-to-speech (TTS), automatic speech recognition (ASR), and real-time speech translation. It supports six English accents with over 150 neural voice options, including more than 30 emotion-rich styles. Moreover, the SDK provides precise control over acoustic features such as pitch, speaking rate, and stress, making it particularly well-suited for generating natural and expressive speech for speechbased reasoning tasks in our SpeechR benchmark.\n\nFor both the multiple-choice and generative versions, we randomly sample one or two distinct American English voices (representing single-person or two-person dialogues) from the SDK's voice pool. Synthesis uses the default pitch and speaking rate of Azure Speech SDK to ensure consistency. In contrast, the acoustic-feature version introduces acoustic adjustments to simulate expressive speech. Specifically, we increase pitch by 30% and reduce speaking rate by 30% to enhance stress. Each transcription in this version is annotated with emotional tags, which guide synthesis and enrich the emotional tone.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Quality Control",
      "text": "To ensure the reliability and consistency of SpeechR, we implement a multi-stage quality control pipeline designed to verify the 1) textual accuracy, 2) text-audio alignment, and 3) audio human-likeness.\n\n(1) To reduce annotation errors and improve the reliability of acoustic labels, we conduct secondary validation using GPT-4o. For each instance, GPT-4o receives the transcription, candidate answers, and ground-truth label, and independently verifies its correctness. For acoustic annotations (stress and emotion), we generate three predictions and select the most frequent one. Conflicting cases are flagged for manual review to ensure robustness.\n\n(2) All synthesized speech samples are verified for alignment with their corresponding transcriptions. First, we ensure the existence of audio file to avoid broken links or missing data. Then, we re-transcribe the generated audio using an ASR model and apply forced alignment techniques to detect mismatches between ASR output and the original text. This guarantees strict consistency between the generated speech and the reference transcription, a critical requirement for evaluating speech-based reasoning.\n\n(3) Although the audio in SpeechR is synthesized, the use of the Azure Speech SDK ensures naturalness and highquality in the generated speech. we assessed the humanlikeness of the synthesized audio by using perceptual evaluation with native listeners. we conducted a listening test with 10 native English speakers. Each participant was asked to rate 30 randomly sampled audio on a 5-point humanlikeness scale (1 = robotic and unnatural, 5 = indistinguishable from natural human speech). The resulting average score was 4.8, indicating that the generated audio in SpeechR is highly natural and suitable for evaluating spoken language understanding in LALMs.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Benchmark Statistics",
      "text": "As shown in Figure  3 , SpeechR includes 3,366 multimodal reasoning instances covering three major categories: factual, procedural, and normative reasoning. The dataset is organized into three formats: a multiple choice version, a generative version, and an acoustic feature version. Each format is designed to reflect different cognitive demands, includ- On average, each transcription contains 35 words and corresponds to an audio duration of 14 seconds, with sample lengths ranging from 2.06 to 62.10 seconds. SpeechR features 37 American English voices and 15 emotional tones, synthesized using the Azure Speech SDK to ensure expressive and diverse speech data. A detailed comparison with existing benchmarks is provided in the Appendix.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiments Experimental Setup",
      "text": "Models We evaluate representative LALMs on the SpeechR benchmark, which includes a diverse range of reasoning tasks spanning factual, procedural, and normative categories. For the open-source models (LTU (Gong et al. 2023b), GAMA  (Ghosh et al. 2024) , SALMONN  (Tang et al. 2023) , Qwen-Audio series  (Chu et al. 2023 (Chu et al. , 2024)) , LLaMA-Omni  (Fang et al. 2024)  and Mellow  (Deshmukh et al. 2025 )) we perform local inference using published checkpoints; for proprietary services (GPT-4o-audio (Ope-nAI 2023) and Gemini 1.5-Pro  (Reid et al. 2024 )), we query via their APIs under default settings. All models are evaluated under a unified input format, using the same prompts, and are executed with default hyperparameters. Inference was performed on a NVIDIA A800 GPU.\n\nEvaluation Protocol Each SpeechR version is evaluated using a format-specific protocol.\n\n(1) Multiple-choice version. We use a discrete-choice evaluation, where model outputs are scanned for valid option labels (e.g., \"A\", \"B\") and matched to the ground-truth answer. Accuracy is the proportion of correct predictions.\n\n(2) Generative version. We adopt an LLM-as-a-judge framework with GPT-4o. To reduce evaluation bias, model outputs are passed without rephrasing or post-processing. The judge receives only the question, model prediction, and reference answer, and scores responses blindly without access to model identity, using the following rubric: (3) Acoustic-feature version. This version follows the same discrete-choice protocol as the multiple-choice format to assess how prosodic and emotional variations affect classification. Accuracy remains the primary metric.\n\nTogether, these protocols support both accuracy-based classification and fine-grained evaluation of open-ended reasoning, offering a comprehensive assessment of LALMs across spoken formats.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Main Results",
      "text": "Results on Multiple-Choice Version First, we evaluated eleven state-of-the-art LALMs on SpeechR using the discrete-choice protocol, with tasks ranging from binary to five-way classification. Each model was prompted to produce either a binary decision or select the correct option, with accuracy as the primary evaluation metric.\n\nDiscussion As shown in Table  2 , LALM performance on the SpeechR multiple-choice benchmark exhibits substantial variation across reasoning categories, offering several insights into their current strengths and limitations.\n\nFirst, advanced proprietary models such as GPT-4oaudio-preview and Gemini-1.5-Pro consistently outperform others, especially on factual and procedural tasks. This suggests that large-scale pretraining, combined with strong audio-language integration, remains effective for improving reasoning under speech input.\n\nHowever, notable challenges remain. Even the bestperforming models exhibit degraded accuracy on tasks requiring nuanced social inference, such as Moral Judgment. This suggests that LALMs still struggle to model the pragmatic subtleties and context dependencies inherent in conversational normative reasoning. Importantly, we observe a marked performance drop when comparing model results on SpeechR to their text-only counterparts. For instance, tasks like GSM8K and BoolQ routinely exceed 85% accuracy in models such as GPT-4o and Qwen-2.5 under standard text-based evaluation. Yet, when converted into spoken form with cleaned reasoning prompts, accuracy drops significantly-even for the most capable LALMs. This highlights that speech reasoning involves more than transcribing input: it requires robust multimodal alignment and context integration across acoustic and linguistic channels.\n\nTogether, these findings suggest that despite strong language backbones, LALMs still struggle with reasoning under speech input. Future research should focus on enhancing spoken context understanding, diversifying training data, and improving fusion of acoustic and semantic cues.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results On Generative Version",
      "text": "We evaluate eight LALMs on the SpeechR generative version to assess openended reasoning. Three models from the discrete-choice evaluation were excluded due to a lack of observable chainof-thought generation during preliminary testing. The llmas-a-judge protocol is tailored to each task category. For moral questions, we record only final correctness and logical relevance. For procedural CoT prompts, which require structured multi-step inference, we apply all three metrics: final correctness, logical relevance, and CoT coherence. This setup captures both the validity of the answer and the internal logical structure.\n\nDiscussion As shown in Table  3 , most LALMs struggle in the generative setting, especially on normative tasks such as moral judgment. These tasks require open-ended reasoning, sensitivity to social norms, and coherence across two-speaker interactions, all of which remain challenging for current models. In contrast, GPT-4o-audio and Gemini-1.5-Pro perform strongly on procedural items, demonstrating clear multi-step reasoning and logical structure. Their success can be attributed to large-scale pretraining and instruction tuning with chain-of-thought data. Qwen2-Audio-Instruct also shows competitive results, benefiting from extensive instruction tuning. However, other models lack con- Results on Acoustic-Feature Version We evaluate the SpeechR acoustic-feature version under four conditions: original, stress-modified, emotion-modified, and combined audio. All evaluations follow the discrete-choice protocol with accuracy as the primary metric. Results in Table  4  show how prosodic stress and emotional tone individually and jointly affect the reasoning performance of LALMs.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "The acoustic-feature version is designed to examine how prosodic stress and emotional tone influence spoken reasoning. Evaluation results reveal that LALMs respond differently to these acoustic variations, offering insight into how speech expressiveness affects model behavior. Instruction-tuned models such as Qwen2-Audio-Instruct and GAMA show minor performance shifts when acoustic features are introduced, suggesting that while these models can process surface-level cues, their reasoning behavior remains largely invariant to expressive signals.\n\nInterestingly, Mellow demonstrates a moderate performance increase under the Emotion and Both conditions. This may be attributed to its dual-encoder architecture, which encourages sensitivity to fine-grained acoustic cues. Such a result indicates that certain model architectures may be more responsive to expressive speech and able to integrate prosodic signals into the reasoning process.\n\nLarge-scale models like GPT-4o-audio and Gemini-1.5-Pro maintain strong overall performance, but their accuracy drops slightly under stress-enhanced speech. This suggests that while these models are highly capable in general, their reasoning may still be affected by subtle changes in speech delivery, particularly when emphasis patterns are altered.\n\nThese findings underscore the importance of modeling not only the content but also the expressive form of speech. By isolating specific prosodic factors, the acoustic-feature version enables deeper investigation into how acoustic expressiveness shapes reasoning outcomes in LALMs, offering a new perspective on multimodal language understanding.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "We present SpeechR, a benchmark designed to evaluate spoken-language reasoning in large audio-language models. It covers three reasoning types: factual, procedural, and normative, and includes three evaluation formats: multiplechoice, generative, and acoustic-feature versions.\n\nResults show that while some models perform well on factual tasks, most still struggle with procedural and normative reasoning. This highlights the need for better instruction tuning and the integration of acoustic cues into reasoning processes.\n\nSpeechR provides a foundation for future research aimed at building more capable and context-aware audio-language models. We hope SpeechR can guide the development of safer, more socially-aware audio-language systems for applications in education, accessibility, and digital assistants.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Details Of Speechr Dataset Readability Enhancement",
      "text": "We apply a set of controlled rewriting rules to ensure that the speech is syntactically well-formed, semantically faithful, and suitable for high-quality audio synthesis. Specifically, we adopt the following criteria:\n\n• The text length is constrained to fall within 7 to 150 words. • All emojis and special characters (e.g., \":)\" or \"#\") are removed.\n\n• Abbreviations and incomplete words are expanded into their full forms (e.g., \"tkts\" becomes \"tickets\", \"comp\" becomes \"competition\").",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Interaction Enrichment",
      "text": "To simulate natural conversational dynamics, we enhance the interactivity of the original text-based datasets using two strategies: restructuring the data format and modifying pronoun perspectives.\n\nRestructuring the Data Format Some source datasets do not follow a dialogue or question-answer format. For these, we convert each instance into a unified conversational structure.\n\nFor example, in the DailyDilemmas dataset, each instance includes a moral scenario and the outcomes of different actions (e.g., choosing to act or not). We construct the Person A utterance by combining the scenario and a corresponding action-related question, and generate the Person B response by combining the selected action and its consequence. This transformation results in a coherent two-turn dialogue.\n\nModifying Pronoun Perspectives To make the interaction feel more natural, we insert personal pronouns into the conversation. Specifically:\n\n• The Person A prompt is rephrased to include \"you\" (e.g., \"What should one do in this situation?\" → \"What should you do in this situation?\"). • The Person B response is rephrased to include \"I\" (e.g., \"One should avoid this.\" → \"I would avoid this.\").\n\nThese adjustments ensure that the resulting dialogue better mirrors speaker-centric, conversational speech patterns.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Annotation Of Acoustic Features",
      "text": "To enrich SpeechR with acoustic cues relevant to reasoning, we annotate each instance with two types of features: stress and emotion. Annotations are generated using GPT-4o under structured prompt templates, as described below.\n\nStress Annotation. We provide GPT-4o with the transcribed text and the ground-truth answer, asking it to identify the keyword or phrase within the transcription that most strongly determines the correct answer. The model is explicitly instructed to select the word or phrase that should be emphasized if the sentence were spoken aloud.\n\nEmotion Annotation. For emotion annotation, GPT-4o is given the transcribed text, the ground-truth answer, and a predefined list of emotions. It is instructed to select the emotion most appropriate for reading the text aloud, considering both the tone of the content and the underlying reasoning intent.\n\nThese annotations are used to assess whether LALMs can benefit from or are sensitive to prosodic and emotional features during reasoning tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Comparison With Existing Benchmarks",
      "text": "We identify four major limitations in existing audio datasets:\n\n(1) Most are designed for low-level audio understanding tasks such as event classification or speech recognition, rather than high-level reasoning. For example, MELD and IEMOCAP primarily target emotion recognition, while Vox-Celeb focus on speaker identification.\n\n(2) Many datasets fail to simulate realistic dialogue reasoning scenarios, limiting their applicability to natural conversational inference. For instance, MMAU provides an audio dialogue and asks the LALM to identify the roles of the speakers. However, such reasoning is relatively straightforward and lacks inferential depth. In most human or humanmachine conversations, speaker roles are either explicitly stated or easily inferred from surface cues. Therefore, it fails to capture the complexity required for evaluating real-world reasoning.\n\n(3) Existing datasets often lack diversity in reasoning types. For example, temporal reasoning and content-based reasoning benchmarks focus narrowly on specific inference categories, limiting their ability to provide a comprehensive assessment of the reasoning capabilities of LALMs.\n\n(4) Even among reasoning-oriented datasets, the complexity of reasoning remains constrained. For instance, although MMAU introduces dialogue-like audio data, its short average audio length (approximately 10 seconds) significantly limits the depth of reasoning it can support.\n\nIn contrast, our SpeechR benchmark addresses these limitations by offering a broader range of reasoning tasks, spanning factual, procedural, and normative dimensions. It incorporates more realistic and context-rich dialogue scenarios, simulating natural conversational settings. Furthermore, SpeechR supports diverse output formats, including both multiple-choice and generative responses, and emphasizes clearer, more complex reasoning chains that reflect the multi-step inference required in real-world audio interactions. These features enable a more comprehensive and challenging evaluation of LALMs' reasoning capabilities.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Model Descriptions",
      "text": "LTU LTU is a multimodal large language model designed for general audio understanding. Trained on the OpenAQA-5M dataset, it demonstrates strong performance in audio classification and captioning tasks.\n\nGAMA GAMA is a general-purpose large audio-language model that integrates multiple types of audio representations. Fine-tuned with the CompA-R dataset, it enhances complex audio reasoning abilities, outperforming other LALMs in diverse audio understanding tasks.\n\nGAMA-IT An instruction-tuned variant of GAMA, GAMA-IT is designed to improve performance in openended audio question-answering tasks requiring complex reasoning. It leverages instruction tuning to enhance its reasoning capabilities.\n\nMellow Mellow is a compact 167M parameter audiolanguage model optimized for reasoning tasks. It takes in two audio inputs and a text prompt, producing free-form text outputs. Despite its small size, Mellow achieves competitive performance with significantly fewer resources.\n\nSALMONN SALMONN is a large language model enabling speech, audio events, and music inputs. It supports various tasks, including automatic speech recognition, emotion recognition, and audio question-answering.\n\nQwen-Audio-Chat Qwen-Audio-Chat is a multimodal model that accepts diverse audio inputs and text. It is designed for tasks such as speech recognition and audio-text understanding, emphasizing instruction-following capabilities.\n\nQwen2-audio-7B An updated large-scale audio-language model, Qwen2-Audio-7B is capable of handling various audio signals and performing audio analysis or direct textual responses.\n\nQwen2-audio-Instruct This is an instruction-tuned version of Qwen2-Audio-7B, enhancing the model's ability to follow prompts and perform complex reasoning tasks.\n\nLLama-Omni LLaMA-Omni is a speech-language model supporting low-latency, high-quality speech interactions. It can generate both text and speech responses directly from speech instructions with extremely low latency.\n\nGPT-4o-audio-preview OpenAI's GPT-4o-Audio-Preview is a multimodal model integrating real-time audio, vision, and text processing capabilities. It enables natural speech interactions and multilingual translation, allowing users to talk to ChatGPT with real-time responses and interruptions.\n\nGemini-1.5-pro Gemini-1.5-pro is an advanced multimodal model supporting text, code, image, audio, and video inputs. It is designed for high-efficiency reasoning and generation tasks, with capabilities in understanding and interacting with various data modalities.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Source Datasets",
      "text": "In this section, we introduce the set of datasets utilized in the construction of the SpeechR benchmark.\n\nReClor ReClor is a reading comprehension dataset consisting of logical reasoning questions derived from standardized graduate-level entrance exams. Each instance includes a passage, a question, and multiple-choice answers, with a strong focus on deductive reasoning.\n\nBoolQ BoolQ is a yes/no question-answering dataset where each question is paired with a short supporting passage. The questions are naturally occurring and require understanding of factual content from the given context.\n\nCommonsenseQA CommonsenseQA is a multiplechoice question answering dataset that targets commonsense reasoning. It is built on ConceptNet relations and presents challenging examples that often require reasoning beyond surface-level word matching.\n\nRiddleSense RiddleSense is a dataset designed to evaluate lateral thinking and creative commonsense reasoning. It contains multiple-choice riddles, where the correct answer requires both semantic understanding and reasoning through implicit associations.\n\nGSM8K GSM8K is a math word problem dataset designed to assess grade-school level arithmetic reasoning. It includes detailed step-by-step chain-of-thought annotations, making it a standard benchmark for evaluating procedural reasoning in CoT settings.\n\nReveAL-CoT ReveAL-CoT is a scientific reasoning dataset featuring multi-step inference questions across physics, biology, and other science domains. Each question is annotated with chain-of-thought explanations to support structured procedural reasoning.\n\nETHICS ETHICS is a benchmark for moral reasoning that includes scenarios requiring judgments about the ethical permissibility of actions. The dataset covers diverse moral contexts such as fairness, harm, and loyalty.\n\nDailyDilemmas DailyDilemmas contains short narratives that describe everyday situations involving social or ethical decision-making. Each instance asks the model to evaluate the appropriateness or morality of an individual's behavior.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Sms Spam Collection",
      "text": "This dataset contains a collection of labeled SMS messages, with each message categorized as spam or ham (not spam). It is widely used for binary classification tasks involving deception or malicious intent detection.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Enron Email",
      "text": "The Enron Email dataset comprises realworld corporate email communications, with a subset labeled for spam detection. It supports studies in behavioral and normative analysis, especially in identifying unethical or misleading content.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Prompt Templates",
      "text": "In this section, we present the prompt templates used throughout various stages of our dataset construction and evaluation pipeline. Figures  4, 5 , and 6 illustrate three prompt designs employed during the generation of the SpeechR dataset. Specifically, Figure  4  shows the prompt used to enhance readability and linguistic fluency of raw samples, Figure  5  demonstrates the interaction-oriented prompt that encourages more engaging and context-aware formulations, and Figure  6  presents the filtering prompt used for quality control, enabling the exclusion of incoherent or irrelevant data.\n\nIn addition, Figures  7  and 8  display prompt templates used in the evaluation phase. Figure  7  is designed for emotion annotation and highlight word extraction from audio transcripts, and is applied specifically to the mini version of SpeechR. Figure  8  illustrates the prompt format adopted for LLM-as-a-judge evaluation of the generative version, guiding the model to assess response correctness, logical relevance, and reasoning coherence.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "As shown in Table  5 , we present qualitative results from different LALMs across the three reasoning categories in SpeechR: factual reasoning, illustrated with creative puzzles; procedural reasoning, represented by mathematical problems; and normative reasoning, which highlights the models' ability to generate inferences in dialogue-based scenarios.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Reflections And Future Directions",
      "text": "SpeechR is designed as an initial step toward evaluating reasoning in speech-based interactions. While it covers a diverse range of reasoning tasks and introduces controlled acoustic variations, further extensions may broaden its scope in the following directions:\n\nSpeech Variability All speech in SpeechR is consistently synthesized using standardized settings. Expanding to include more varied prosody, speaking styles, and spontaneous speech could offer richer insights into real-world model performance.\n\nLinguistic and Cultural Coverage Current data construction focuses on English with general social contexts. Exploring additional languages and sociocultural scenarios could enable broader applicability across multilingual and multicultural settings.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Interaction Dynamics",
      "text": "The current benchmark emphasizes static single-turn prompts. Incorporating multi-turn dialogue and speaker dynamics could allow future benchmarks to capture more interactive aspects of speech-based reasoning.\n\n#Take SMS as an example.\n\nYou are a speech-content analyzer. You are a professional editor specialized in cleaning and improving the readability of messy SMS messages.\n\nGiven a disorganized, advertisement-like SMS text, your task is to: Correct grammar and expand incomplete words (e.g., \"tkts\" → \"tickets\", \"comp\" → \"competition\"). Add appropriate punctuation and sentence breaks to make the text easy and natural to read aloud.\n\nPreserve the original meaning and promotional intent of the message. Do not invent or add any new information.\n\nFormat the output into short sentences or clear bullet points if it improves flow and readability.\n\nIf any emoticons (e.g., \":)\") are present, remove them directly. Keep the overall length and structure of the original message as much as possible. **Goal**:\n\nMake the message sound natural, clear, and smooth when read aloud.\n\nHere is the SMS to improve: Please output only the cleaned and improved version of the message.. #Take Email as an example.\n\nYou are a professional email content extractor and security analyst.\n\nYou are given an email. Please extract only the email content -exclude metadata such as Subject, Sender, Receiver, Date, etc.\n\nThen, analyze the extracted content and determine whether the email is classified as \"normal\", \"spam or potential threat\" based on its content and any security risks.\n\nThe final output format should be: 1. Exclusion of Special Symbols: -The dialogue must not contain mathematical formulas (e.g., $\\frac{{1}}{{2}}$), special symbols (e.g., ≠, ≈, ∑, →, :), or structured formats like tables or bullet points (e.g., \"1. ... 2. ...\").\n\n-If the dialogue includes visual references (e.g., \"Based on the image below\"), it should be rejected.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Text Length Control:",
      "text": "-The dialogue text should be within **70 words**.\n\n-Estimated audio duration should be ≤30 seconds (calculated at 150 words per minute).\n\n3. Language Simplicity:\n\n-Prefer dialogue set in everyday scenarios (e.g., shopping, time calculations), avoiding complex terms (e.g., \"quadratic equation\").\n\n-Sentences should be clear, without nested clauses. 4. Logical Consistency:\n\n-The dialogue content should clearly adhere to or clearly violate moral principles. The moral principles include honesty, respect, justice, and responsibility.\n\n-Dialogues that do not clearly adhere to or violate moral principles should be rejected.\n\n5. **Audio Feasibility**:\n\n-The dialogue content should be suitable for audio presentation without the need for visual elements or diagrams.\n\n-It should be easy to understand when listened to as audio, without reliance on punctuation or special formatting for clarity.  2. Assign exactly one emotion label from:\n\n['angry  ', 'cheerful', 'excited', 'friendly', 'hopeful', 'sad', 'shouting', 'terrified', 'unfriendly', 'whispering']",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , SpeechR focuses on three major",
      "page": 1
    },
    {
      "caption": "Figure 1: Data examples from the three versions of SpeechR dataset.",
      "page": 2
    },
    {
      "caption": "Figure 2: Text Data Construction",
      "page": 3
    },
    {
      "caption": "Figure 2: SpeechR Benchmark Construction Pipeline.",
      "page": 4
    },
    {
      "caption": "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning",
      "page": 5
    },
    {
      "caption": "Figure 3: , SpeechR includes 3,366 multimodal",
      "page": 5
    },
    {
      "caption": "Figure 4: shows the prompt",
      "page": 11
    },
    {
      "caption": "Figure 5: demonstrates the interaction-oriented",
      "page": 11
    },
    {
      "caption": "Figure 6: presents the filtering prompt used",
      "page": 11
    },
    {
      "caption": "Figure 7: is designed for emo-",
      "page": 11
    },
    {
      "caption": "Figure 8: illustrates the prompt format adopted for",
      "page": 11
    },
    {
      "caption": "Figure 4: Prompt template for dataset readability enhancement.",
      "page": 13
    },
    {
      "caption": "Figure 5: Prompt template for dataset interaction enrichment.",
      "page": 14
    },
    {
      "caption": "Figure 6: Prompt template for dataset filtering.",
      "page": 15
    },
    {
      "caption": "Figure 7: Prompt template for emotion annotation and highlight extraction from audio transcripts for SpeechR mini version.",
      "page": 16
    },
    {
      "caption": "Figure 8: Prompt template for LLM-Based evaluation of the SpeechR generative version.",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "producible evaluation formats. In contrast, SpeechR offers a"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "structured benchmark tailored to speech reasoning, featuring"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "fine-grained categorization across\nfactual, procedural, and"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "normative tasks, and supporting both multiple-choice and"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "generative evaluations under controlled prosodic and emo-"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "tional variations."
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "To address\nthis gap, we introduce SpeechR, an audio-"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "language reasoning benchmark designed to evaluate large"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "audio-language models\non\nreasoning\ntasks\ngrounded\nin"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "speech scenarios, encompassing a diverse range of reason-"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "ing types. Speech data in SpeechR are generated from cu-"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "rated textual reasoning tasks, allowing precise control over"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "verbal content, prosody, and structure while preserving the"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "cognitive intent of\nthe original\ntasks. This design enables"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "consistent evaluation across diverse reasoning formats and"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "acoustic conditions."
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "As shown in Figure 1, SpeechR focuses on three major"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "reasoning types central\nto spoken interaction and decision-"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "making:\n(1) Factual Reasoning, which involves\nretrieving"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "or confirming concrete information; (2) Procedural Reason-"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": ""
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "ing, which requires understanding step-by-step processes or"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "causal dependencies; and (3) Normative Reasoning, which"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "evaluates judgments based on social, ethical, or behavioral"
        },
        {
          "ended generation without clear\ntask-type granularity or\nre-": "norms.\nIn addition, SpeechR is\nreleased in three versions"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "Abstract"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "Large audio–language models (LALMs) have achieved near-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "human performance in sentence-level transcription and emo-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "tion recognition. However, existing evaluations focus mainly"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "on surface-level perception,\nleaving the\ncapacity of mod-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "els for contextual and inference-driven reasoning in speech-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "based scenarios insufficiently examined. To address this gap,"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "we\nintroduce SpeechR,\na\nunified\nbenchmark\nfor\nevaluat-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "ing reasoning over speech in large audio-language models."
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "SpeechR evaluates models along three key dimensions: fac-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "tual retrieval, procedural inference, and normative judgment."
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "It\nincludes\nthree distinct evaluation formats. The multiple-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "choice version measures answer selection accuracy. The gen-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "erative version assesses\nthe\ncoherence\nand logical\nconsis-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "tency of\nreasoning chains. The acoustic-feature version in-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "vestigates whether variations\nin stress\nand emotion affect"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "reasoning performance. Evaluations on eleven state-of-the-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "art LALMs reveal\nthat high transcription accuracy does not"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "translate into strong reasoning capabilities. SpeechR estab-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "lishes a structured benchmark for evaluating reasoning in spo-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "ken language, enabling more targeted analysis of model ca-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "pabilities across diverse dialogue-based tasks. We release the"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "benchmark and evaluation code to facilitate future research.1"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "Introduction"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "Driven by the rapid progress in speech processing and large-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "scale multimodal pretraining,\nlarge audio–language models"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "(LALMs) have demonstrated strong capabilities\nin under-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "standing and generating natural\nlanguage from speech in-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "put (Chu et al. 2024; Ghosh et al. 2024; Touvron et al. 2023)."
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "These models bridge acoustic perception and linguistic in-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "ference,\nenabling applications\nthat\nrequire not only tran-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "scription but also contextual\ninterpretation. Such capabili-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "ties have expanded the potential of LALMs\nin real-world"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "scenarios,\nincluding voice-based virtual assistants\n(Zhang"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "et\nal. 2023; Huang et\nal. 2024), AI-powered educational"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "tools (Yang and Taele 2025), and human–computer dialogue"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "systems\n(Xue et al. 2024; Rubenstein et al. 2023). How-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "ever, while current LALMs excel at\ntranscription and ba-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "sic speech understanding, their ability to perform contextual"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "and inference-driven reasoning over speech remains under-"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "explored, highlighting the need for systematic benchmarks"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": ""
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "Copyright © 2026, Association for the Advancement of Artificial"
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "Intelligence (www.aaai.org). All rights reserved."
        },
        {
          "wychao1987@gmail.com, Meng.Fang@liverpool.ac.uk, ling.chen@uts.edu.au": "1https://github.com/Yanda95/SpeechR"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "to support a wide range of evaluation needs. The multiple-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "choice version provides a standardized and transparent for-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "mat, enabling consistent accuracy measurement across mod-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "els. The generative version is designed to assess a model’s"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "ability to construct\ncoherent\nand logically grounded rea-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "soning chains, particularly in procedurally or normatively"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "reasoning tasks. The\nacoustic-feature version emphasizes"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "acoustic diversity,\nincorporating variations\nsuch as\nemo-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "tional tone and prosodic stress, thereby facilitating the study"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "of how acoustic features influence reasoning performance."
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "Finally, we evaluate SpeechR using eleven state-of-the-art"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "LALMs,\nincluding Qwen-Audio (Chu et al. 2024), LLama-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "Omni\n(Fang\net\nal.\n2024), GPT-4o-audio\n(Achiam et\nal."
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "2023), and others. Our experiments cover all\nthree bench-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "mark versions and focus on three key evaluation perspec-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "tives:\nreasoning accuracy, coherence and logical quality of"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "generated outputs,\nand model\nrobustness under prosodic"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "variation. This analysis provides a comprehensive view of"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "current LALM capabilities in speech reasoning scenarios."
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "The key contributions of this work are:"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "1. We introduce SpeechR, the first benchmark for system-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "atically evaluating speech reasoning in factual, procedural,"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "and normative tasks."
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "2. We release three benchmark versions, each aligned with"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "a specific evaluation protocol: discrete choice evaluation for"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "multiple-choice and acoustic-feature versions, and LLM-as-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "a-judge evaluation for generative version assessment."
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": ""
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "3. We conduct a comprehensive evaluation of state-of-the-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "art LALMs and offer detailed analyses on reasoning accu-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "racy,\nlogical coherence of\nresponse generated, and the im-"
        },
        {
          "Figure 1: Data examples from the three versions of SpeechR dataset.": "pact of acoustic variations."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "reasoning under\nrealistic audio conditions, motivating our"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "SpeechR benchmark."
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "SpeechR Benchmark"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "SpeechR is\na benchmark developed to simulate\nrealistic"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "speech-based reasoning scenarios and evaluate the contex-"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "tual\ninference\ncapabilities of LALMs\nin spoken interac-"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "tions.\nIt covers\nthree major\nreasoning types:\nfactual, pro-"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "cedural, and normative. SpeechR is\nreleased in three ver-"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "sions: multiple-choice, generative, and acoustic-feature. The"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "multiple-choice\nversion\noffers\na\nstandardized\nformat\nfor"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "evaluating answer accuracy. The generative version assesses"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "whether LALMs can produce coherent,\nlogically grounded"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": ""
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "reasoning chains, especially for procedural and normative"
        },
        {
          "open-ended logical deduction, causal\ninference and moral": "tasks. The\nacoustic-feature\nversion\nintroduces\nvariations"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Normative\nSocial norms; ethics; behavioral inference": "Table 1: Three reasoning types and their categorization in SpeechR.",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "Audio Reasoning Benchmark",
          "Partial\nSubjective\nScam detection; moral judgment": "The construction pipeline of the SpeechR benchmark is il-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "lustrated in Figure 2."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "Large-scale corpora such as LibriSpeech (Panayotov et al.",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "2015), Common Voice (Ardila et al. 2019), FSD50K (Fon-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "Text Data Construction"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "seca et al. 2021), and AudioSet (Gemmeke et al. 2017) have",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "driven advances in ASR and audio classification but do not",
          "Partial\nSubjective\nScam detection; moral judgment": "While several audio datasets exist\n(Sakshi et al. 2024; Ma"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "assess higher-order\nreasoning. To explicitly probe reason-",
          "Partial\nSubjective\nScam detection; moral judgment": "et al. 2025b,a),\nthey typically lack the reasoning diversity,"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "ing over audio, OpenASQA (Gong et al. 2023a) unified end-",
          "Partial\nSubjective\nScam detection; moral judgment": "structural\nclarity,\nand compositional\ncomplexity required"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "to-end question answering across multiple speech datasets,",
          "Partial\nSubjective\nScam detection; moral judgment": "for evaluating speech reasoning. To address\nthese limita-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "while CompA (Ghosh et al. 2023) defined compositional",
          "Partial\nSubjective\nScam detection; moral judgment": "tions, SpeechR builds upon high-quality text-based reason-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "probes for event ordering and attribute binding. CAA (Yang",
          "Partial\nSubjective\nScam detection; moral judgment": "ing datasets (Clark et al. 2019; Cobbe et al. 2021; Talmor"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "et al. 2024b) proposed a benchmark of universal adversarial",
          "Partial\nSubjective\nScam detection; moral judgment": "et al. 2018; Yu et al. 2020), which offer diverse tasks, well-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "audio attacks specifically based on conversational scenarios.",
          "Partial\nSubjective\nScam detection; moral judgment": "defined reasoning structures, and extended inference chains."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "Audio-CoT (Ma et al. 2025a) pioneered chain-of-thought",
          "Partial\nSubjective\nScam detection; moral judgment": "These properties make them a valuable foundation for con-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "prompting for structured, multi-step inference on spoken in-",
          "Partial\nSubjective\nScam detection; moral judgment": "structing a challenging benchmark for spoken reasoning."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "puts, and MMAU (Sakshi et al. 2024) introduced a 10 k-clip,",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "To ensure high-quality and cognitively diverse content,"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "27-skill benchmark spanning speech, environmental sounds,",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "our\ntext data construction follows six steps:\n(1)\nreasoning"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "and music\nfor\nexpert-level understanding and reasoning.",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "type design,\n(2) data source selection,\n(3)\nreadability en-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "Generative evaluation frameworks—AIR-Bench (Yang et al.",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "hancement, (4) interaction enrichment, (5) acoustic feature"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "2024a) and AudioBench (Wang et al. 2024a)—benchmark",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "annotation, and (6) versions."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "open-ended instruction following and comprehension. De-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "Reasoning Type Design We categorize reasoning tasks"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "spite\nthis progress, no existing benchmark jointly covers",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "in SpeechR based on three core dimensions that reflect\nthe"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "open-ended logical deduction, causal\ninference and moral",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "cognitive characteristics of\nreasoning:\n(S1) knowledge de-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "reasoning under\nrealistic audio conditions, motivating our",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "pendence, (S2) reasoning transparency, and (S3) evaluation"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "SpeechR benchmark.",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "determination."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "• S1 identifies the primary knowledge source required to"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "SpeechR Benchmark",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "solve the reasoning task:\nfactual knowledge (e.g., com-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "SpeechR is\na benchmark developed to simulate\nrealistic",
          "Partial\nSubjective\nScam detection; moral judgment": "monsense or background knowledge), procedural\nrules"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "speech-based reasoning scenarios and evaluate the contex-",
          "Partial\nSubjective\nScam detection; moral judgment": "(e.g., mathematical or scientific laws), or normative prin-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "tual\ninference\ncapabilities of LALMs\nin spoken interac-",
          "Partial\nSubjective\nScam detection; moral judgment": "ciples (e.g., moral, social, or behavioral norms)."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "tions.\nIt covers\nthree major\nreasoning types:\nfactual, pro-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "• S2 evaluates\nthe\nlevel of\nreasoning transparency,\ni.e.,"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "cedural, and normative. SpeechR is\nreleased in three ver-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "whether solving a task requires intermediate steps such"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "sions: multiple-choice, generative, and acoustic-feature. The",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "as logical deduction or chain-of-thought reasoning."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "multiple-choice\nversion\noffers\na\nstandardized\nformat\nfor",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "• S3\nevaluates whether\na\ntask\nhas\na\nclearly\nobjective"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "evaluating answer accuracy. The generative version assesses",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "ground-truth answer, as opposed to relying on subjective"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "whether LALMs can produce coherent,\nlogically grounded",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "judgment."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "reasoning chains, especially for procedural and normative",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "tasks. The\nacoustic-feature\nversion\nintroduces\nvariations",
          "Partial\nSubjective\nScam detection; moral judgment": "Based on these dimensions, we define\nthree\nreasoning"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "such as stress and emotion to examine how acoustic factors",
          "Partial\nSubjective\nScam detection; moral judgment": "types in SpeechR shown in Table 1:"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "influence reasoning performance.",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "• Factual Reasoning involves understanding and retrieving"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "Each instance in SpeechR is represented as a multimodal",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "factual or commonsense information. These tasks do not"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "triplet (speech, text,\nlabel), where the speech is a synthe-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "require multi-step reasoning and are evaluated using ob-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "sized utterance containing either a spoken question or a di-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "jective criteria. Examples include “Why are cats afraid"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "alogue. The text component always includes the transcrip-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "of water?” or “Where did you go yesterday?”, which rely"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "tion, while additional elements such as multi-step reason-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "",
          "Partial\nSubjective\nScam detection; moral judgment": "on language comprehension and real-world knowledge."
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "ing chains, answer candidates, and acoustic features are in-",
          "Partial\nSubjective\nScam detection; moral judgment": ""
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "cluded depending on the specific dataset version. The label",
          "Partial\nSubjective\nScam detection; moral judgment": "• Procedural Reasoning requires explicit, multi-step log-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "encodes the correct answer for evaluation. Note that the tran-",
          "Partial\nSubjective\nScam detection; moral judgment": "ical or numerical\ninference. Tasks in this category often"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "scription is provided solely for evaluation purposes, such as",
          "Partial\nSubjective\nScam detection; moral judgment": "involve deterministic reasoning chains and verifiable out-"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "verifying model outputs and facilitating human inspection,",
          "Partial\nSubjective\nScam detection; moral judgment": "puts, such as “What is 12 divided by 3 plus 5?” or “If A is"
        },
        {
          "Normative\nSocial norms; ethics; behavioral inference": "but\nit\nis never used as input\nto the model during inference.",
          "Partial\nSubjective\nScam detection; moral judgment": "taller than B, and B is taller than C, who is the tallest?”."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "• Normative Reasoning involves assessing actions,\ninten-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "tions, or social behaviors based on implicit moral or so-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "cial norms. These tasks may involve open-ended answers"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "and often require subjective evaluation, reflecting the in-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "herent ambiguity in social and ethical reasoning. Exam-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "ples include “Is this message likely to be a scam?” or"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "“Was second speaker’s behavior ethically acceptable?”."
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "In summary, SpeechR adopts a three-type taxonomy con-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "sisting\nof\nfactual,\nprocedural,\nand\nnormative\nreasoning,"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "based on cognitive criteria (S1 to S3), enabling comprehen-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "sive evaluation of LALMs’ reasoning breadth and depth in"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "speech-based scenarios. Table 1 outlines the criteria used in"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "our categorization, along with the corresponding reasoning"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "types included in SpeechR."
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "Data Source Selection We carefully select data from a"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "broad range of existing text-based reasoning benchmarks."
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "The selected datasets are required to satisfy the following"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "criteria: (1) they must contain at\nleast one of the three rea-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "soning types—Factual, Procedural, or Normative;\n(2)\nthey"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "must\nexhibit well-structured\nformats;\nand\n(3)\nthey must"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "provide clearly defined evaluation standards. The specific"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "datasets selected are detailed in Appendix. To mitigate po-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "tential data leakage from pretraining or fine-tuning, we ex-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "clusively use the official test splits of each dataset. While full"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "overlap checks are infeasible for proprietary LLMs, this de-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "sign choice ensures maximal separation between benchmark"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "content and training data."
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "Readability\nEnhancement\nTo\nensure\nhigh-quality"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "speech synthesis and maintain semantic clarity, we normal-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "ize the text to remove noisy artifacts (e.g., emojis, inconsis-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "tent punctuation, abbreviations). This process improves both"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "the fluency of synthesized speech and the interpretability of"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "reasoning content. Detailed rules and examples are included"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "in the Appendix."
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "Interaction Enrichment To\nbetter\nevaluate\ndialogue-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "based reasoning, we transform selected instances into two-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "speaker conversational formats using rule-based restructur-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": ""
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "ing. This includes adding contextual prompts and adapting"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "pronouns\nto reflect\ninterpersonal exchanges. Such conver-"
        },
        {
          "Figure 2: SpeechR Benchmark Construction Pipeline.": "sions simulate realistic turn-taking in spoken interaction and"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "types, while the bar chart (right) illustrates the tasks included in SpeechR, along with their corresponding average speech lengths"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "and average prompt lengths."
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "This version includes acoustic features\nsuch as\nstress and"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "emotional tone, and aims to explore whether speech-specific"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "attributes influence the reasoning abilities of LALMs."
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "Speech Generation"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "We employ a speech tool, Azure Speech SDK, to synthesize"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "speech from the constructed transcriptions. We select\nthis"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "tool due to its fine-grained controllability and high-quality"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "output, which are essential for systematically varying acous-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "tic features in SpeechR. Developed by Microsoft, the Azure"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "Speech SDK is a cloud-based framework that supports high-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "quality text-to-speech (TTS), automatic speech recognition"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "(ASR), and real-time speech translation. It supports six En-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "glish accents with over 150 neural voice options,\nincluding"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "more than 30 emotion-rich styles. Moreover,\nthe SDK pro-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "vides precise control over acoustic features such as pitch,"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "speaking rate, and stress, making it particularly well-suited"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "for generating natural\nand expressive\nspeech for\nspeech-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "based reasoning tasks in our SpeechR benchmark."
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "For both the multiple-choice and generative versions, we"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "randomly\nsample\none\nor\ntwo\ndistinct American English"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "voices (representing single-person or two-person dialogues)"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "from the SDK’s voice pool. Synthesis uses the default pitch"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "and speaking rate of Azure Speech SDK to ensure consis-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "tency.\nIn contrast,\nthe acoustic-feature version introduces"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "acoustic adjustments to simulate expressive speech. Specif-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "ically, we increase pitch by 30% and reduce speaking rate"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "by 30% to enhance stress. Each transcription in this version"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "is annotated with emotional tags, which guide synthesis and"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "enrich the emotional tone."
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "Quality Control"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": ""
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "To ensure the reliability and consistency of SpeechR, we im-"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "plement a multi-stage quality control pipeline designed to"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "verify the 1) textual accuracy, 2) text-audio alignment, and"
        },
        {
          "Figure 3: The pie chart (left) presents an overview of the SpeechR composition and its distribution across different reasoning": "3) audio human-likeness."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: , LALM performance on",
      "data": [
        {
          "Model": "",
          "Factual Reasoning": "CS",
          "Procedural Reasoning": "G–CoT",
          "Normative Reasoning": "BA",
          "Avg": ""
        },
        {
          "Model": "LTU",
          "Factual Reasoning": "22.30",
          "Procedural Reasoning": "40.50",
          "Normative Reasoning": "23.08",
          "Avg": "34.94"
        },
        {
          "Model": "GAMA",
          "Factual Reasoning": "21.58",
          "Procedural Reasoning": "37.00",
          "Normative Reasoning": "20.12",
          "Avg": "35.98"
        },
        {
          "Model": "GAMA-IT",
          "Factual Reasoning": "9.35",
          "Procedural Reasoning": "40.50",
          "Normative Reasoning": "13.91",
          "Avg": "23.74"
        },
        {
          "Model": "Mellow",
          "Factual Reasoning": "14.75",
          "Procedural Reasoning": "11.50",
          "Normative Reasoning": "21.89",
          "Avg": "25.34"
        },
        {
          "Model": "SALMONN",
          "Factual Reasoning": "20.86",
          "Procedural Reasoning": "47.00",
          "Normative Reasoning": "23.08",
          "Avg": "34.73"
        },
        {
          "Model": "Qwen-Audio-Chat",
          "Factual Reasoning": "37.77",
          "Procedural Reasoning": "47.50",
          "Normative Reasoning": "26.63",
          "Avg": "33.75"
        },
        {
          "Model": "Qwen2-Audio-7B",
          "Factual Reasoning": "6.83",
          "Procedural Reasoning": "20.50",
          "Normative Reasoning": "6.80",
          "Avg": "12.83"
        },
        {
          "Model": "Qwen2-Audio-Instruct",
          "Factual Reasoning": "36.69",
          "Procedural Reasoning": "39.00",
          "Normative Reasoning": "12.72",
          "Avg": "33.90"
        },
        {
          "Model": "LLaMA-Omni",
          "Factual Reasoning": "31.65",
          "Procedural Reasoning": "64.50",
          "Normative Reasoning": "25.74",
          "Avg": "39.28"
        },
        {
          "Model": "GPT-4o-audio-preview",
          "Factual Reasoning": "66.55",
          "Procedural Reasoning": "36.00",
          "Normative Reasoning": "31.95",
          "Avg": "58.91"
        },
        {
          "Model": "Gemini-1.5-Pro",
          "Factual Reasoning": "74.46",
          "Procedural Reasoning": "66.50",
          "Normative Reasoning": "27.22",
          "Avg": "67.68"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: , LALM performance on",
      "data": [
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "61.02\nGPT-4o-audio-preview\n75.32\n66.55\n73.58",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "31.95\n76.45\n36.00\n29.00\n86.41\n58.91"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "80.26\n74.46\n78.44\nGemini-1.5-Pro\n43.86",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "66.50\n78.67\n89.56\n67.68\n27.22\n63.87"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "Table 2: Performance of LALMs on SpeechR multi-choice version. RC = reading comprehension, CS = commonsense, CreaT",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "= creative thinking(factual); M–CoT, G–CoT = math/general CoT (procedural); MJ = moral judgment, BA = behavior analysis,",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "SMS = scam SMS, Email = spam Email (normative).",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "ing discrete retrieval, open-ended reasoning, and prosody-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "• Final Correctness\n(0\nor\n1): Binary\nscore\nindicating"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "informed interpretation.",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "whether the answer matches the reference."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "On average, each transcription contains 35 words and cor-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "• Logical Relevance (1 to 5, integer): Whether the answer"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "responds to an audio duration of 14 seconds, with sample",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "logically follows from the question."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "lengths ranging from 2.06 to 62.10 seconds. SpeechR fea-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "• CoT Coherence (1 to 5, integer): Whether the reasoning"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "tures 37 American English voices and 15 emotional\ntones,",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "is internally consistent and well-structured."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "synthesized using the Azure Speech SDK to ensure expres-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "All scores are assigned as discrete integers. Higher scores on"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "sive and diverse speech data. A detailed comparison with",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "Logical Relevance and CoT Coherence reflect stronger rea-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "existing benchmarks is provided in the Appendix.",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "soning performance. Full scoring guidelines and examples"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "are provided in Appendix."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "Experiments",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "(3) Acoustic-feature\nversion. This\nversion\nfollows\nthe"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "same discrete-choice protocol as the multiple-choice format"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "Experimental setup",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "to assess how prosodic and emotional variations affect clas-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "Models We evaluate representative LALMs on the SpeechR",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "sification. Accuracy remains the primary metric."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "benchmark, which\nincludes\na\ndiverse\nrange\nof\nreason-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "Together,\nthese\nprotocols\nsupport both\naccuracy-based"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "ing tasks spanning factual, procedural, and normative cat-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "classification and fine-grained evaluation of open-ended rea-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "egories. For\nthe\nopen-source models\n(LTU (Gong\net\nal.",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "soning, offering a\ncomprehensive\nassessment of LALMs"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "2023b), GAMA (Ghosh et\nal. 2024), SALMONN (Tang",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "across spoken formats."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "et al. 2023), Qwen-Audio series\n(Chu et al. 2023, 2024),",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "Main Results"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "LLaMA-Omni\n(Fang et al. 2024) and Mellow (Deshmukh",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "et al. 2025)) we perform local\ninference using published",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "Results\non Multiple-Choice Version\nFirst, we\nevalu-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "checkpoints;\nfor proprietary services (GPT-4o-audio (Ope-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "ated eleven state-of-the-art LALMs on SpeechR using the"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "nAI 2023) and Gemini 1.5-Pro (Reid et al. 2024)), we query",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "discrete-choice protocol, with tasks ranging from binary to"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "via their APIs under default settings. All models are evalu-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "five-way classification. Each model was prompted to pro-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "ated under a unified input format, using the same prompts,",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "duce either a binary decision or\nselect\nthe correct option,"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "and are executed with default hyperparameters.\nInference",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "with accuracy as the primary evaluation metric."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "was performed on a NVIDIA A800 GPU.",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "Discussion\nAs shown in Table 2, LALM performance on"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "Evaluation Protocol Each SpeechR version is evaluated",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "the SpeechR multiple-choice benchmark exhibits\nsubstan-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "using a format-specific protocol.",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": ""
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "tial variation across\nreasoning categories, offering several"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "(1) Multiple-choice\nversion. We\nuse\na\ndiscrete-choice",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "insights into their current strengths and limitations."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "evaluation, where model outputs are scanned for valid op-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "First,\nadvanced\nproprietary models\nsuch\nas GPT-4o-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "tion labels (e.g., “A”, “B”) and matched to the ground-truth",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "audio-preview and Gemini-1.5-Pro consistently outperform"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "answer. Accuracy is the proportion of correct predictions.",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "others,\nespecially\non\nfactual\nand\nprocedural\ntasks. This"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "(2) Generative\nversion. We\nadopt\nan LLM-as-a-judge",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "suggests that\nlarge-scale pretraining, combined with strong"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "framework with GPT-4o. To reduce evaluation bias, model",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "audio-language integration, remains effective for improving"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "outputs are passed without\nrephrasing or post-processing.",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "reasoning under speech input."
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "The judge receives only the question, model prediction, and",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "However,\nnotable\nchallenges\nremain.\nEven\nthe\nbest-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "reference answer, and scores responses blindly without ac-",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "performing models exhibit degraded accuracy on tasks re-"
        },
        {
          "LLaMA-Omni\n58.96\n31.65\n19.41\n12.71": "cess to model identity, using the following rubric:",
          "64.50\n48.50\n25.74\n53.64\n47.42\n39.28": "quiring nuanced social\ninference, such as Moral Judgment."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: modalalignmentandcontextintegrationacrossacousticand show how prosodic stress and emotional tone individually",
      "data": [
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "LTU\n32.93\n32.34\n31.74\n33.23"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "GAMA\n33.83\n35.93\n35.33\n32.63"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "GAMA-IT\n20.36\n18.26\n17.07\n19.46"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Mellow\n23.95\n23.95\n24.25\n26.65"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "SALMONN\n34.43\n33.83\n33.53\n33.83"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Qwen-Audio-Chat\n33.53\n33.23\n33.53\n31.74"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Qwen2-Audio-7B\n9.88\n10.18\n10.18\n9.58"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Qwen2-Audio-Instruct\n34.13\n33.83\n32.33\n32.93"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "LLaMA-Omni\n38.32\n37.72\n36.53\n35.93"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "GPT-4o-preview\n57.78\n55.39\n60.78\n55.09"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "64.67\n64.37\n64.97\n65.87\nGemini-1.5-Pro"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Table 4: Accuracy (%) of LALMs on the SpeechR acoustic-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "feature version."
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "sistent\nreasoning chains and often fail\nto maintain logical"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "coherence. These findings suggest\nthat\nimproving LALMs’"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "generative reasoning, particularly in socially complex do-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "mains, requires further tuning with rich, structured reason-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "ing examples and more diverse discourse formats."
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Results on Acoustic-Feature Version\nWe\nevaluate\nthe"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "SpeechR acoustic-feature\nversion\nunder\nfour\nconditions:"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "original, stress-modified, emotion-modified, and combined"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "audio. All evaluations\nfollow the discrete-choice protocol"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "with accuracy as\nthe primary metric. Results\nin Table 4"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "show how prosodic stress and emotional\ntone individually"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "and jointly affect the reasoning performance of LALMs."
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Discussion\nThe acoustic-feature version is designed to ex-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "amine how prosodic stress and emotional tone influence spo-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "ken reasoning. Evaluation results\nreveal\nthat LALMs\nre-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "spond differently to these acoustic variations, offering in-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "sight\ninto how speech expressiveness affects model behav-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "ior. Instruction-tuned models such as Qwen2-Audio-Instruct"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "and GAMA show minor performance shifts when acoustic"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "features are introduced, suggesting that while these models"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "can process surface-level cues,\ntheir reasoning behavior re-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "mains largely invariant to expressive signals."
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Interestingly, Mellow demonstrates\na moderate perfor-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "mance increase under the Emotion and Both conditions. This"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "may be\nattributed to its dual-encoder architecture, which"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "encourages\nsensitivity to fine-grained acoustic cues. Such"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "a result\nindicates\nthat certain model architectures may be"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "more responsive to expressive speech and able to integrate"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": ""
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "prosodic signals into the reasoning process."
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Large-scale models like GPT-4o-audio and Gemini-1.5-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "Pro maintain strong overall performance, but their accuracy"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "drops slightly under stress-enhanced speech. This suggests"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "that while these models are highly capable in general,\ntheir"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "reasoning may still be affected by subtle changes in speech"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "delivery, particularly when emphasis patterns are altered."
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "These findings underscore the importance of modeling not"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "only the content but also the expressive form of speech. By"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "isolating specific prosodic factors,\nthe acoustic-feature ver-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "sion enables deeper investigation into how acoustic expres-"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "siveness shapes reasoning outcomes in LALMs, offering a"
        },
        {
          "Model\nBase\nStress\nEmotion\nBoth": "new perspective on multimodal language understanding."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2023. Clap learning audio concepts from natural\nlanguage"
        },
        {
          "Conclusion": "We present SpeechR,\na benchmark designed to evaluate",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "supervision.\nIn ICASSP 2023–2023 IEEE International"
        },
        {
          "Conclusion": "spoken-language\nreasoning in large\naudio-language mod-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Conference\non Acoustics,\nSpeech\nand\nSignal Processing"
        },
        {
          "Conclusion": "els. It covers three reasoning types: factual, procedural, and",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "(ICASSP), 1–5. IEEE."
        },
        {
          "Conclusion": "normative, and includes three evaluation formats: multiple-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "choice, generative, and acoustic-feature versions.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Fang, Q.; Guo, S.; Zhou, Y.; Ma, Z.; Zhang, S.; and Feng,"
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Y\n. 2024. LLaMA-Omni: Seamless Speech Interaction with"
        },
        {
          "Conclusion": "Results\nshow that while some models perform well on",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Large Language Models. arXiv preprint arXiv:2409.06666."
        },
        {
          "Conclusion": "factual tasks, most still struggle with procedural and norma-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "tive reasoning. This highlights the need for better instruction",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Fonseca, E.; Favory, X.; Pons,\nJ.; Font, F.; and Serra, X."
        },
        {
          "Conclusion": "tuning and the integration of acoustic cues into reasoning",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2021.\nFsd50k: an open dataset of human-labeled sound"
        },
        {
          "Conclusion": "processes.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "events. IEEE/ACM Transactions on Audio, Speech, and Lan-"
        },
        {
          "Conclusion": "SpeechR provides a foundation for future research aimed",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "guage Processing, 30: 829–852."
        },
        {
          "Conclusion": "at building more capable and context-aware audio-language",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Gardner,\nJ. P.; Durand, S.; Stoller, D.; and Bittner, R. M."
        },
        {
          "Conclusion": "models. We hope SpeechR can guide the development of",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2023. Llark: A multimodal foundation model for music."
        },
        {
          "Conclusion": "safer, more socially-aware audio-language systems for ap-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Gemmeke,\nJ. F.; Ellis, D. P.; Freedman, D.;\nJansen, A.;"
        },
        {
          "Conclusion": "plications in education, accessibility, and digital assistants.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Lawrence, W.; Moore, R. C.; Plakal, M.;\nand Ritter, M."
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2017. Audio set: An ontology and human-labeled dataset"
        },
        {
          "Conclusion": "References",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "for audio events.\nIn 2017 IEEE international conference on"
        },
        {
          "Conclusion": "Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "acoustics, speech and signal processing (ICASSP), 776–780."
        },
        {
          "Conclusion": "Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "IEEE."
        },
        {
          "Conclusion": "arXiv\nAnadkat, S.; et al. 2023.\nGpt-4 technical\nreport.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Ghosh, S.; Kumar, S.; Seth, A.; Evuru, C. K. R.; Tyagi,"
        },
        {
          "Conclusion": "preprint arXiv:2303.08774.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "U.; Sakshi, S.; Nieto, O.; Duraiswami, R.; and Manocha, D."
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2024. Gama: A large audio-language model with advanced"
        },
        {
          "Conclusion": "Ardila, R.; Branson, M.; Davis, K.; Henretty, M.; Kohler,",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "audio understanding and complex reasoning abilities. arXiv"
        },
        {
          "Conclusion": "M.; Meyer, J.; Morais, R.; Saunders, L.; Tyers, F. M.; and",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "preprint arXiv:2406.11768."
        },
        {
          "Conclusion": "Weber, G. 2019. Common voice: A massively-multilingual",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "speech corpus. arXiv preprint arXiv:1912.06670.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Ghosh, S.; Seth, A.; Kumar, S.; Tyagi, U.; Evuru, C. K.; Ra-"
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "maneswaran, S.; Sakshi, S.; Nieto, O.; Duraiswami, R.; and"
        },
        {
          "Conclusion": "Bai, J.; Bai, S.; Chu, Y.; Cui, Z.; Dang, K.; Deng, X.; Fan,",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Manocha, D. 2023. Compa: Addressing the gap in compo-"
        },
        {
          "Conclusion": "Y\n.; Ge, W.; Han, Y.; Huang, F.; et al. 2023. Qwen technical",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "sitional reasoning in audio-language models. arXiv preprint"
        },
        {
          "Conclusion": "report. arXiv preprint arXiv:2309.16609.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "arXiv:2310.08753."
        },
        {
          "Conclusion": "Bai, Y.; Chen, J.; Chen, J.; Chen, W.; Chen, Z.; Ding, C.;",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Gong, Y.; Liu, A. H.; Luo, H.; Karlinsky, L.; and Glass, J."
        },
        {
          "Conclusion": "Dong, L.; Dong, Q.; Du, Y.; Gao, K.; et al. 2024. Seed-asr:",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2023a. Joint audio and speech understanding. In 2023 IEEE"
        },
        {
          "Conclusion": "Understanding diverse speech and contexts with llm-based",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Automatic Speech Recognition and Understanding Work-"
        },
        {
          "Conclusion": "speech recognition. arXiv preprint arXiv:2407.04675.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "shop (ASRU), 1–8. IEEE."
        },
        {
          "Conclusion": "Chu, Y.; Xu, J.; Yang, Q.; Wei, H.; Wei, X.; Guo, Z.; Leng,",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Gong, Y.; Luo, H.; Liu, A. H.; Karlinsky, L.; and Glass,"
        },
        {
          "Conclusion": "Y\n.; Lv, Y.; He, J.; Lin, J.; et al. 2024. Qwen2-audio technical",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "arXiv preprint\nJ. 2023b.\nListen,\nthink, and understand."
        },
        {
          "Conclusion": "report. arXiv preprint arXiv:2407.10759.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "arXiv:2305.10790."
        },
        {
          "Conclusion": "Chu, Y.; Xu,\nJ.; Zhou, X.; Yang, Q.; Zhang, S.; Yan, Z.;",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Huang, R.; Li, M.; Yang, D.; Shi, J.; Chang, X.; Ye, Z.; Wu,"
        },
        {
          "Conclusion": "Zhou, C.; and Zhou, J. 2023. Qwen-audio: Advancing uni-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Y\n.; Hong, Z.; Huang, J.; Liu, J.; et al. 2024. Audiogpt: Un-"
        },
        {
          "Conclusion": "versal\naudio understanding via unified large-scale\naudio-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "derstanding and generating speech, music, sound, and talk-"
        },
        {
          "Conclusion": "language models. arXiv preprint arXiv:2311.07919.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "ing head.\nIn Proceedings of the AAAI Conference on Artifi-"
        },
        {
          "Conclusion": "Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins,",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "cial Intelligence, volume 38, 23802–23804."
        },
        {
          "Conclusion": "M.; and Toutanova, K. 2019. BoolQ: Exploring the Surpris-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Liu, S.; Hussain, A. S.; Sun, C.; and Shan, Y. 2024. Music"
        },
        {
          "Conclusion": "ing Difficulty of Natural Yes/No Questions.\nIn NAACL.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "understanding llama: Advancing text-to-music generation"
        },
        {
          "Conclusion": "Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "with question answering and captioning.\nIn ICASSP 2024-"
        },
        {
          "Conclusion": "Kaiser, L.; Plappert, M.; Tworek,\nJ.; Hilton,\nJ.; Nakano,",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2024 IEEE International Conference on Acoustics, Speech"
        },
        {
          "Conclusion": "R.; Hesse, C.;\nand Schulman,\nJ.\n2021.\nTraining Ver-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "and Signal Processing (ICASSP), 286–290. IEEE."
        },
        {
          "Conclusion": "arXiv\npreprint\nifiers\nto\nSolve Math Word\nProblems.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Ma, Z.; Chen, Z.; Wang, Y.; Chng, E. S.;\nand Chen, X."
        },
        {
          "Conclusion": "arXiv:2110.14168.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "2025a.\nAudio-CoT: Exploring Chain-of-Thought Reason-"
        },
        {
          "Conclusion": "Deshmukh, S.; Dixit, S.; Singh, R.; and Raj, B. 2025. Mel-",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "arXiv\npreprint\ning\nin Large Audio Language Model."
        },
        {
          "Conclusion": "arXiv\nlow: a small audio language model\nfor\nreasoning.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "arXiv:2501.07246."
        },
        {
          "Conclusion": "preprint arXiv:2503.08540.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": ""
        },
        {
          "Conclusion": "",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Ma, Z.; Ma, Y.; Zhu, Y.; Yang, C.; Chao, Y.-W.; Xu, R.;"
        },
        {
          "Conclusion": "Deshmukh, S.; Elizalde, B.; Singh, R.; and Wang, H. 2023.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "Chen, W.; Chen, Y.; Chen, Z.; Cong,\nJ.;\net\nal.\n2025b."
        },
        {
          "Conclusion": "Pengi: An audio language model for audio tasks. Advances",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "MMAR: A Challenging Benchmark for Deep Reasoning"
        },
        {
          "Conclusion": "in Neural\nInformation Processing\nSystems,\n36:\n18090–",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "arXiv preprint\nin Speech, Audio, Music, and Their Mix."
        },
        {
          "Conclusion": "18108.",
          "Elizalde, B.; Deshmukh, S.; Al\nIsmail, M.; and Wang, H.": "arXiv:2505.13032."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "and Chen, X. 2023.\nemotion2vec: Self-supervised pre-",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Y\n.; and Zheng, W. 2024b.\nSpeech swin-transformer: Ex-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv preprint\ntraining for speech emotion representation.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "ploring a hierarchical\ntransformer with shifted windows for"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv:2312.15185.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "speech emotion recognition.\nIn ICASSP 2024-2024 IEEE"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "International Conference on Acoustics, Speech and Signal"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Melechovsky,\nJ.; Guo,\nZ.; Ghosal, D.; Majumder, N.;",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Processing (ICASSP), 11646–11650. IEEE."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Herremans, D.;\nand\nPoria,\nS.\n2023.\nMustango:\nTo-",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv preprint\nward controllable text-to-music generation.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Xie, Z.; Lin, M.; Liu, Z.; Wu, P.; Yan, S.; and Miao, C. 2025."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv:2311.08355.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Audio-reasoner: Improving reasoning capability in large au-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "dio language models. arXiv preprint arXiv:2503.02318."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "OpenAI. 2023. ChatGPT. https://openai.com/blog/chatgpt/.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "1, 2.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Xue, H.; Liang, Y.; Mu, B.; Zhang, S.; Chen, M.; Chen, Q.;"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "and Xie, L. 2024.\nE-chat: Emotion-sensitive Spoken Dia-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Panayotov, V.; Chen, G.; Povey, D.; and Khudanpur, S. 2015.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "logue System with Large Language Models.\nIn 2024 IEEE"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Librispeech: an asr corpus based on public domain audio",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "14th International Symposium on Chinese Spoken Language"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "books.\nIn 2015 IEEE international conference on acoustics,",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Processing (ISCSLP), 586–590. IEEE."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "speech and signal processing (ICASSP), 5206–5210. IEEE.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Yang, C.;\nand Taele, P. 2025.\nAI\nfor Accessible Edu-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Radford, A.; Kim, J. W.; Xu, T.; Brockman, G.; McLeavey,",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "cation: Personalized Audio-Based Learning for Blind Stu-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "C.; and Sutskever,\nI. 2023. Robust speech recognition via",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "dents. arXiv preprint arXiv:2504.17117."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "large-scale weak supervision.\nIn International conference",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "on machine learning, 28492–28518. PMLR.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Yang, Q.; Xu, J.; Liu, W.; Chu, Y.; Jiang, Z.; Zhou, X.; Leng,"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Y\n.; Lv, Y.; Zhao, Z.; Zhou, C.; et al. 2024a.\nAir-bench:"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Reid, M.; Savinov, N.; Teplyashin, D.; Lepikhin, D.; Lilli-",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Benchmarking large audio-language models via generative"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "crap, T.; Alayrac, J.-b.; Soricut, R.; Lazaridou, A.; Firat, O.;",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "comprehension. arXiv preprint arXiv:2402.07729."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Schrittwieser, J.; et al. 2024. Gemini 1.5: Unlocking mul-",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "timodal understanding across millions of tokens of context.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Yang, W.; Li, Y.; Fang, M.; Wei, Y.; Zhou, T.; and Chen,"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv preprint arXiv:2403.05530.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "L. 2024b. Who Can Withstand Chat-Audio Attacks? An"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "arXiv\nEvaluation Benchmark for Large Language Models."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Rubenstein, P. K.; Asawaroengchai, C.; Nguyen, D. D.;",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "preprint arXiv:2411.14842."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Bapna, A.; Borsos, Z.; Quitry, F. d. C.; Chen, P.; Badawy,",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "D. E.; Han, W.; Kharitonov, E.; et al. 2023.\nAudiopalm:",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Yao, Z.; Guo, L.; Yang, X.; Kang, W.; Kuang, F.; Yang, Y.;"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv\nA large language model\nthat can speak and listen.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Jin, Z.; Lin, L.; and Povey, D. 2023.\nZipformer: A faster"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "preprint arXiv:2306.12925.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "and better encoder for automatic speech recognition. arXiv"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "preprint arXiv:2310.11230."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Sakshi, S.; Tyagi, U.; Kumar, S.; Seth, A.; Selvakumar, R.;",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Nieto, O.; Duraiswami, R.; Ghosh, S.;\nand Manocha, D.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Ye, Z.; Wang, X.; Liu, H.; Qian, Y.; Tao, R.; Yan, L.; and"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "2024. Mmau: A massive multi-task audio understanding and",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Ouchi, K. 2021.\nSound event detection transformer: An"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "reasoning benchmark. arXiv preprint arXiv:2410.19168.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "event-based end-to-end model\nfor\nsound event detection."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "arXiv preprint arXiv:2110.02011."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Talmor,\nA.;\nHerzig,\nJ.;\nLourie,\nN.;\nand\nBerant,\nJ.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "2018.\nCommonsenseqa: A question\nanswering\nchal-",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Yoon, S.; Byun, S.; and Jung, K. 2018. Multimodal speech"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv preprint\nlenge targeting commonsense knowledge.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "emotion recognition using audio and text. In 2018 IEEE spo-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv:1811.00937.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "ken language technology workshop (SLT), 112–118. IEEE."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Tang, C.; Yu, W.; Sun, G.; Chen, X.; Tan, T.; Li, W.; Lu,",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Yu, W.;\nJiang, Z.; Dong, Y.; and Feng,\nJ. 2020.\nReClor:"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "L.; Ma, Z.; and Zhang, C. 2023. Salmonn: Towards generic",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "A Reading Comprehension Dataset Requiring Logical Rea-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv preprint\nhearing abilities for large language models.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "soning.\nIn International Conference on Learning Represen-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv:2310.13289.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "tations (ICLR)."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Team, G.; Anil, R.; Borgeaud, S.; Alayrac, J.-B.; Yu, J.; Sori-",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Zhang, D.; Li, S.; Zhang, X.; Zhan, J.; Wang, P.; Zhou, Y.;"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "cut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.; Millican, K.;",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "and Qiu, X. 2023. Speechgpt: Empowering large language"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "et al. 2023. Gemini: a family of highly capable multimodal",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "models with intrinsic cross-modal conversational abilities."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "models. arXiv preprint arXiv:2312.11805.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "arXiv preprint arXiv:2305.11000."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "M.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.;",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Details of SpeechR Dataset"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Azhar, F.; et al. 2023.\nLlama: Open and efficient\nfounda-",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "Readability Enhancement"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "tion language models. arXiv preprint arXiv:2302.13971.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "We apply a set of controlled rewriting rules to ensure that"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Vesperini, F.; Gabrielli, L.; Principi, E.; and Squartini, S.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "the speech is syntactically well-formed, semantically faith-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "2019.\nPolyphonic sound event detection by using capsule",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "ful, and suitable for high-quality audio synthesis. Specifi-"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "IEEE Journal of Selected Topics in Signal\nneural networks.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "cally, we adopt the following criteria:"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Processing, 13(2): 310–322.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "• The text\nlength is constrained to fall within 7 to 150"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Wang, B.; Zou, X.; Lin, G.; Sun, S.; Liu, Z.; Zhang, W.; Liu,",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "words."
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "Z.; Aw, A.; and Chen, N. F. 2024a. Audiobench: A universal",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": ""
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "benchmark for audio large language models. arXiv preprint",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "• All emojis and special characters (e.g., “:)” or “#”) are"
        },
        {
          "Ma, Z.; Zheng, Z.; Ye,\nJ.; Li,\nJ.; Gao, Z.; Zhang, S.;": "arXiv:2406.16020.",
          "Wang, Y.; Lu, C.; Lian, H.; Zhao, Y.; Schuller, B. W.; Zong,": "removed."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Abbreviations and incomplete words are expanded into": "their\nfull\nforms (e.g., “tkts” becomes “tickets”, “comp”",
          "These annotations are used to assess whether LALMs can": "benefit from or are sensitive to prosodic and emotional fea-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "becomes “competition”).",
          "These annotations are used to assess whether LALMs can": "tures during reasoning tasks."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "• URL links\nare\nsimplified to include only the domain",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "Comparison with Existing Benchmarks"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "name.",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "We identify four major limitations in existing audio datasets:"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "• Appropriate punctuation and sentence segmentation are",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "(1) Most\nare designed for\nlow-level\naudio understand-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "added\nto\nimprove\nclarity\nand\nnaturalness when\nread",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "ing tasks such as event classification or speech recognition,"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "aloud.",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "rather\nthan high-level\nreasoning. For example, MELD and"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "• No new content\nis introduced beyond what\nis present\nin",
          "These annotations are used to assess whether LALMs can": "IEMOCAP primarily target emotion recognition, while Vox-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "the original input.",
          "These annotations are used to assess whether LALMs can": "Celeb focus on speaker identification."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "(2) Many datasets fail\nto simulate realistic dialogue rea-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "Interaction Enrichment",
          "These annotations are used to assess whether LALMs can": "soning scenarios,\nlimiting their applicability to natural con-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "versational inference. For instance, MMAU provides an au-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "To simulate natural conversational dynamics, we enhance",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "dio dialogue and asks the LALM to identify the roles of the"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "the interactivity of the original text-based datasets using two",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "speakers. However, such reasoning is relatively straightfor-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "strategies: restructuring the data format and modifying pro-",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "ward and lacks inferential depth. In most human or human-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "noun perspectives.",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "machine conversations,\nspeaker\nroles are either explicitly"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "Restructuring the Data Format\nSome source datasets do",
          "These annotations are used to assess whether LALMs can": "stated or easily inferred from surface cues. Therefore, it fails"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "not follow a dialogue or question-answer format. For these,",
          "These annotations are used to assess whether LALMs can": "to capture the complexity required for evaluating real-world"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "we convert each instance into a unified conversational struc-",
          "These annotations are used to assess whether LALMs can": "reasoning."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "ture.",
          "These annotations are used to assess whether LALMs can": "(3) Existing datasets often lack diversity in reasoning"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "For example, in the DailyDilemmas dataset, each instance",
          "These annotations are used to assess whether LALMs can": "types. For example,\ntemporal\nreasoning and content-based"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "includes a moral scenario and the outcomes of different ac-",
          "These annotations are used to assess whether LALMs can": "reasoning benchmarks focus narrowly on specific inference"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "tions (e.g., choosing to act or not). We construct the Person",
          "These annotations are used to assess whether LALMs can": "categories, limiting their ability to provide a comprehensive"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "A utterance by combining the scenario and a corresponding",
          "These annotations are used to assess whether LALMs can": "assessment of the reasoning capabilities of LALMs."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "action-related question, and generate the Person B response",
          "These annotations are used to assess whether LALMs can": "(4) Even among reasoning-oriented datasets,\nthe\ncom-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "by combining the selected action and its consequence. This",
          "These annotations are used to assess whether LALMs can": "plexity of\nreasoning remains constrained. For\ninstance, al-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "transformation results in a coherent two-turn dialogue.",
          "These annotations are used to assess whether LALMs can": "though MMAU introduces dialogue-like audio data, its short"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "average\naudio length (approximately 10 seconds)\nsignifi-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "Modifying Pronoun Perspectives\nTo make the interac-",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "cantly limits the depth of reasoning it can support."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "tion feel more natural, we insert personal pronouns into the",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "In contrast, our SpeechR benchmark addresses these lim-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "conversation. Specifically:",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "itations\nby\noffering\na\nbroader\nrange\nof\nreasoning\ntasks,"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "• The Person A prompt is rephrased to include “you” (e.g.,",
          "These annotations are used to assess whether LALMs can": "spanning factual, procedural, and normative dimensions. It"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "“What should one do in this situation?” → “What should",
          "These annotations are used to assess whether LALMs can": "incorporates more realistic and context-rich dialogue sce-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "you do in this situation?”).",
          "These annotations are used to assess whether LALMs can": "narios, simulating natural conversational settings. Further-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "more, SpeechR supports diverse output\nformats,\nincluding"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "• The Person B response is rephrased to include “I” (e.g.,",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "both multiple-choice and generative responses, and empha-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "“One should avoid this.” → “I would avoid this.”).",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "sizes\nclearer, more\ncomplex reasoning chains\nthat\nreflect"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "These adjustments ensure that\nthe resulting dialogue bet-",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "the multi-step inference required in real-world audio inter-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "ter mirrors speaker-centric, conversational speech patterns.",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "actions. These features enable a more comprehensive and"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "challenging evaluation of LALMs’ reasoning capabilities."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "Annotation of Acoustic Features",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "To enrich SpeechR with acoustic cues relevant to reasoning,",
          "These annotations are used to assess whether LALMs can": "Model Descriptions"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "we annotate each instance with two types of features: stress",
          "These annotations are used to assess whether LALMs can": "LTU\nLTU is a multimodal large language model designed"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "and emotion. Annotations are generated using GPT-4o under",
          "These annotations are used to assess whether LALMs can": "for general audio understanding. Trained on the OpenAQA-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "structured prompt templates, as described below.",
          "These annotations are used to assess whether LALMs can": "5M dataset,\nit demonstrates\nstrong performance\nin audio"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "Stress Annotation. We provide GPT-4o with the tran-",
          "These annotations are used to assess whether LALMs can": "classification and captioning tasks."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "scribed text and the ground-truth answer, asking it\nto iden-",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "GAMA\nGAMA is a general-purpose large audio-language"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "tify the keyword or phrase within the transcription that most",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "model\nthat\nintegrates multiple\ntypes of\naudio representa-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "strongly determines the correct answer. The model is explic-",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "tions. Fine-tuned with the CompA-R dataset,\nit enhances"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "itly instructed to select\nthe word or phrase that should be",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "complex\naudio\nreasoning\nabilities,\noutperforming\nother"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "emphasized if the sentence were spoken aloud.",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "",
          "These annotations are used to assess whether LALMs can": "LALMs in diverse audio understanding tasks."
        },
        {
          "• Abbreviations and incomplete words are expanded into": "Emotion Annotation. For emotion annotation, GPT-4o",
          "These annotations are used to assess whether LALMs can": ""
        },
        {
          "• Abbreviations and incomplete words are expanded into": "is given the transcribed text, the ground-truth answer, and a",
          "These annotations are used to assess whether LALMs can": "GAMA-IT\nAn\ninstruction-tuned\nvariant\nof\nGAMA,"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "predefined list of emotions. It is instructed to select the emo-",
          "These annotations are used to assess whether LALMs can": "GAMA-IT is designed to improve performance\nin open-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "tion most appropriate for reading the text aloud, considering",
          "These annotations are used to assess whether LALMs can": "ended audio question-answering tasks\nrequiring complex"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "both the tone of\nthe content and the underlying reasoning",
          "These annotations are used to assess whether LALMs can": "reasoning. It leverages instruction tuning to enhance its rea-"
        },
        {
          "• Abbreviations and incomplete words are expanded into": "intent.",
          "These annotations are used to assess whether LALMs can": "soning capabilities."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "language model optimized for\nreasoning tasks.\nIt\ntakes in",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "ate lateral\nthinking and creative commonsense reasoning. It"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "two audio inputs and a text prompt, producing free-form text",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "contains multiple-choice riddles, where the correct answer"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "outputs. Despite its small size, Mellow achieves competitive",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "requires both semantic understanding and reasoning through"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "performance with significantly fewer resources.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "implicit associations."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "SALMONN\nSALMONN is a large language model en-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "GSM8K\nGSM8K is\na math word problem dataset de-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "abling speech, audio events, and music inputs.\nIt supports",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "signed to assess grade-school\nlevel arithmetic reasoning. It"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "various tasks, including automatic speech recognition, emo-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "includes detailed step-by-step chain-of-thought annotations,"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "tion recognition, and audio question-answering.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "making it a standard benchmark for evaluating procedural"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "reasoning in CoT settings."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "Qwen-Audio-Chat\nQwen-Audio-Chat\nis\na multimodal",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "model\nthat accepts diverse audio inputs and text.\nIt\nis de-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "ReveAL-CoT\nReveAL-CoT\nis\na\nscientific\nreasoning"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "signed for\ntasks such as speech recognition and audio-text",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "dataset\nfeaturing multi-step\ninference\nquestions\nacross"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "understanding, emphasizing instruction-following capabili-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "physics, biology, and other science domains. Each question"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "ties.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "is annotated with chain-of-thought explanations to support"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "Qwen2-audio-7B\nAn updated large-scale audio-language",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "structured procedural reasoning."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "model, Qwen2-Audio-7B is capable of handling various au-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "ETHICS\nETHICS is\na benchmark for moral\nreasoning"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "dio signals and performing audio analysis or direct\ntextual",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "that includes scenarios requiring judgments about the ethical"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "responses.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "permissibility of actions. The dataset covers diverse moral"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "Qwen2-audio-Instruct\nThis\nis an instruction-tuned ver-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "contexts such as fairness, harm, and loyalty."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "sion of Qwen2-Audio-7B, enhancing the model’s ability to",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "follow prompts and perform complex reasoning tasks.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "DailyDilemmas\nDailyDilemmas contains short narratives"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "that describe everyday situations involving social or ethical"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "LLama-Omni\nLLaMA-Omni is a speech-language model",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "decision-making. Each instance asks the model\nto evaluate"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "supporting low-latency, high-quality speech interactions.\nIt",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "the appropriateness or morality of an individual’s behavior."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "can generate both text and speech responses directly from",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "speech instructions with extremely low latency.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "SMS Spam Collection\nThis dataset contains a collection"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "of labeled SMS messages, with each message categorized as"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "GPT-4o-audio-preview\nOpenAI’s\nGPT-4o-Audio-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "spam or ham (not spam). It is widely used for binary classi-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "Preview is a multimodal model\nintegrating real-time audio,",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "fication tasks involving deception or malicious intent detec-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "vision, and text processing capabilities.\nIt enables natural",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "tion."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "speech interactions\nand multilingual\ntranslation,\nallowing",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "users\nto\ntalk\nto ChatGPT with\nreal-time\nresponses\nand",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "Enron Email\nThe Enron Email dataset comprises\nreal-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "interruptions.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "world corporate email communications, with a subset\nla-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "Gemini-1.5-pro\nGemini-1.5-pro\nis\nan\nadvanced multi-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "beled for spam detection.\nIt supports studies in behavioral"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "modal model supporting text, code, image, audio, and video",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "and normative analysis, especially in identifying unethical"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "inputs. It is designed for high-efficiency reasoning and gen-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "or misleading content."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "eration tasks, with capabilities in understanding and inter-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "acting with various data modalities.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "Prompt Templates"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "In\nthis\nsection, we\npresent\nthe\nprompt\ntemplates\nused"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "Source Datasets",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "throughout various\nstages of our dataset construction and"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "In this section, we introduce the set of datasets utilized in the",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "evaluation\npipeline. Figures\n4,\n5,\nand\n6\nillustrate\nthree"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "construction of the SpeechR benchmark.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "prompt\ndesigns\nemployed\nduring\nthe\ngeneration\nof\nthe"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "ReClor\nReClor\nis a reading comprehension dataset con-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "SpeechR dataset. Specifically, Figure 4 shows\nthe prompt"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "sisting of logical reasoning questions derived from standard-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "used to enhance readability and linguistic fluency of\nraw"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "ized graduate-level entrance exams. Each instance includes",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "samples,\nFigure\n5\ndemonstrates\nthe\ninteraction-oriented"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "a passage, a question, and multiple-choice answers, with a",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "prompt\nthat encourages more engaging and context-aware"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "strong focus on deductive reasoning.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "formulations, and Figure 6 presents the filtering prompt used"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "for quality control, enabling the exclusion of incoherent or"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "BoolQ\nBoolQ is\na\nyes/no\nquestion-answering\ndataset",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "irrelevant data."
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "where each question is paired with a short supporting pas-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "In addition, Figures 7 and 8 display prompt\ntemplates"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "sage. The questions are naturally occurring and require un-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "used in the evaluation phase. Figure 7 is designed for emo-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "derstanding of factual content from the given context.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": ""
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "tion annotation and highlight word extraction from audio"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "CommonsenseQA\nCommonsenseQA\nis\na\nmultiple-",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "transcripts, and is applied specifically to the mini version of"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "choice question answering dataset that targets commonsense",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "SpeechR. Figure 8 illustrates the prompt format adopted for"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "reasoning.\nIt\nis built on ConceptNet\nrelations and presents",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "LLM-as-a-judge evaluation of the generative version, guid-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "challenging examples that often require reasoning beyond",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "ing the model\nto assess response correctness,\nlogical\nrele-"
        },
        {
          "Mellow\nMellow is\na\ncompact\n167M parameter\naudio-": "surface-level word matching.",
          "RiddleSense\nRiddleSense is a dataset designed to evalu-": "vance, and reasoning coherence."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 5: , we present qualitative results from",
      "data": [
        {
          "Qualitative Analysis": "As\nshown in Table 5, we present qualitative results\nfrom"
        },
        {
          "Qualitative Analysis": "different LALMs across\nthe three reasoning categories\nin"
        },
        {
          "Qualitative Analysis": "SpeechR:\nfactual\nreasoning,\nillustrated with creative puz-"
        },
        {
          "Qualitative Analysis": "zles;\nprocedural\nreasoning,\nrepresented\nby mathematical"
        },
        {
          "Qualitative Analysis": "problems;\nand normative\nreasoning, which highlights\nthe"
        },
        {
          "Qualitative Analysis": "models’ ability to generate inferences in dialogue-based sce-"
        },
        {
          "Qualitative Analysis": "narios."
        },
        {
          "Qualitative Analysis": "Reflections and Future Directions"
        },
        {
          "Qualitative Analysis": "SpeechR is designed as\nan initial\nstep toward evaluating"
        },
        {
          "Qualitative Analysis": "reasoning in speech-based interactions. While it covers a"
        },
        {
          "Qualitative Analysis": "diverse range of\nreasoning tasks and introduces controlled"
        },
        {
          "Qualitative Analysis": "acoustic variations, further extensions may broaden its scope"
        },
        {
          "Qualitative Analysis": "in the following directions:"
        },
        {
          "Qualitative Analysis": "Speech Variability\nAll speech in SpeechR is consistently"
        },
        {
          "Qualitative Analysis": "synthesized using standardized settings. Expanding to in-"
        },
        {
          "Qualitative Analysis": "clude more varied prosody, speaking styles, and spontaneous"
        },
        {
          "Qualitative Analysis": "speech could offer richer insights into real-world model per-"
        },
        {
          "Qualitative Analysis": "formance."
        },
        {
          "Qualitative Analysis": "Linguistic\nand Cultural Coverage\nCurrent\ndata\ncon-"
        },
        {
          "Qualitative Analysis": "struction focuses on English with general\nsocial contexts."
        },
        {
          "Qualitative Analysis": "Exploring additional\nlanguages and sociocultural scenarios"
        },
        {
          "Qualitative Analysis": "could enable broader applicability across multilingual and"
        },
        {
          "Qualitative Analysis": "multicultural settings."
        },
        {
          "Qualitative Analysis": "Interaction Dynamics\nThe\ncurrent\nbenchmark\nempha-"
        },
        {
          "Qualitative Analysis": "sizes static single-turn prompts. Incorporating multi-turn di-"
        },
        {
          "Qualitative Analysis": "alogue\nand speaker dynamics\ncould allow future bench-"
        },
        {
          "Qualitative Analysis": "marks to capture more interactive aspects of speech-based"
        },
        {
          "Qualitative Analysis": "reasoning."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Readability Enhancement": "#Take SMS as an example."
        },
        {
          "Readability Enhancement": "You are a speech-content analyzer."
        },
        {
          "Readability Enhancement": "You are a professional editor specialized in cleaning and improving the readability of"
        },
        {
          "Readability Enhancement": "messy SMS messages."
        },
        {
          "Readability Enhancement": "Given a disorganized, advertisement-like SMS text, your task is to:"
        },
        {
          "Readability Enhancement": "Correct grammar and expand incomplete words (e.g., \"tkts\" → \"tickets\", \"comp\" →"
        },
        {
          "Readability Enhancement": "\"competition\")."
        },
        {
          "Readability Enhancement": "Add appropriate punctuation and sentence breaks to make the text easy and natural to"
        },
        {
          "Readability Enhancement": "read aloud."
        },
        {
          "Readability Enhancement": "Preserve the original meaning and promotional intent of the message."
        },
        {
          "Readability Enhancement": "Do not invent or add any new information."
        },
        {
          "Readability Enhancement": "Format the output into short sentences or clear bullet points if it improves flow and"
        },
        {
          "Readability Enhancement": "readability."
        },
        {
          "Readability Enhancement": "If any emoticons (e.g., \":)\") are present, remove them directly."
        },
        {
          "Readability Enhancement": "Keep the overall length and structure of the original message as much as possible."
        },
        {
          "Readability Enhancement": "**Goal**:"
        },
        {
          "Readability Enhancement": "Make the message sound natural, clear, and smooth when read aloud."
        },
        {
          "Readability Enhancement": "Here is the SMS to improve:"
        },
        {
          "Readability Enhancement": "Please output only the cleaned and improved version of the message.."
        },
        {
          "Readability Enhancement": "#In-context examples"
        },
        {
          "Readability Enhancement": "[Output format]"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Interaction Enrichment": "#Take Email as an example."
        },
        {
          "Interaction Enrichment": "You are a professional email content extractor and security analyst."
        },
        {
          "Interaction Enrichment": "You are given an email. Please extract only the email content — exclude metadata"
        },
        {
          "Interaction Enrichment": "such as Subject, Sender, Receiver, Date, etc."
        },
        {
          "Interaction Enrichment": "Then, analyze the extracted content and determine whether the email is classified as"
        },
        {
          "Interaction Enrichment": "\"normal\", \"spam or potential threat\" based on its content and any security risks."
        },
        {
          "Interaction Enrichment": "The final output format should be:"
        },
        {
          "Interaction Enrichment": "messageid: [The message ID of the email]"
        },
        {
          "Interaction Enrichment": "emailcontent: [The extracted pure content of the email]"
        },
        {
          "Interaction Enrichment": "label: [normal / spam or potential threat]"
        },
        {
          "Interaction Enrichment": "Requirements:"
        },
        {
          "Interaction Enrichment": "Focus only on the main body text of the email."
        },
        {
          "Interaction Enrichment": "Make a judgment based solely on the extracted content."
        },
        {
          "Interaction Enrichment": "Ensure the output strictly follows the required format."
        },
        {
          "Interaction Enrichment": "#In-context examples"
        },
        {
          "Interaction Enrichment": "[Output format]"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "result with keys `valid` and `index`.": "**Dialogue**:"
        },
        {
          "result with keys `valid` and `index`.": "{question}"
        },
        {
          "result with keys `valid` and `index`.": "**Index**:"
        },
        {
          "result with keys `valid` and `index`.": "{index}"
        },
        {
          "result with keys `valid` and `index`.": "**Selection Criteria**:"
        },
        {
          "result with keys `valid` and `index`.": "1. Exclusion of Special Symbols:"
        },
        {
          "result with keys `valid` and `index`.": "- The dialogue must not contain mathematical formulas (e.g.,"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{index}": "**Selection Criteria**:"
        },
        {
          "{index}": "1. Exclusion of Special Symbols:"
        },
        {
          "{index}": "- The dialogue must not contain mathematical formulas (e.g.,"
        },
        {
          "{index}": "$\\frac{{1}}{{2}}$), special symbols (e.g., ≠, ≈, ∑, →, :), or structured formats like"
        },
        {
          "{index}": "tables or bullet points (e.g., \"1. ... 2. ...\")."
        },
        {
          "{index}": "- If the dialogue includes visual references (e.g., \"Based on the image below\"),"
        },
        {
          "{index}": "it should be rejected."
        },
        {
          "{index}": "2. Text Length Control:"
        },
        {
          "{index}": "- The dialogue text should be within **70 words**."
        },
        {
          "{index}": "- Estimated audio duration should be ≤30 seconds (calculated at 150 words per"
        },
        {
          "{index}": "minute)."
        },
        {
          "{index}": "3. Language Simplicity:"
        },
        {
          "{index}": "- Prefer dialogue set in everyday scenarios (e.g., shopping, time calculations),"
        },
        {
          "{index}": "avoiding complex terms (e.g., \"quadratic equation\")."
        },
        {
          "{index}": "- Sentences should be clear, without nested clauses."
        },
        {
          "{index}": "4. Logical Consistency:"
        },
        {
          "{index}": "- The dialogue content should clearly adhere to or clearly violate moral"
        },
        {
          "{index}": "principles. The moral principles include honesty, respect, justice, and responsibility."
        },
        {
          "{index}": "- Dialogues that do not clearly adhere to or violate moral principles should be"
        },
        {
          "{index}": "rejected."
        },
        {
          "{index}": "5. **Audio Feasibility**:"
        },
        {
          "{index}": "- The dialogue content should be suitable for audio presentation without the"
        },
        {
          "{index}": "need for visual elements or diagrams."
        },
        {
          "{index}": "- It should be easy to understand when listened to as audio, without reliance on"
        },
        {
          "{index}": "punctuation or special formatting for clarity."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "You are a speech-content analyzer."
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "Given a snippet of dialogue or context, you must:"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "1. Identify exactly 1–3 **words**, **short phrases**, or **brief clauses** (up to"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "~20 words each) to emphasize when spoken, and return them as an array in the"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "“highlight” field."
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "2. Assign exactly one emotion label from:"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "['angry', 'cheerful', 'excited', 'friendly', 'hopeful', 'sad', 'shouting', 'terrified',"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "'unfriendly', 'whispering’]"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "3. If only one speaker is present, judge that speaker’s emotion; if two speakers, judge"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "**only Person B**’s emotion."
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "**Important constraints:**"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "- **Output strictly one** JSON object with **only** the keys `\"highlight\"` and"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "`\"emotion\"`."
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "- The `\"highlight\"` array may contain **single words**, **phrases**, or **short"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "clauses** (not full paragraphs)."
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "- **Do not** emit any extra text, explanation, or markdown—**only** the JSON."
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "#In-context examples"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "[Output format]"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "```json"
        },
        {
          "Emotion Annotation and Highlight Extraction from Audio Transcripts": "{"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LLM verifier for SpeechR generative version evaluation": "You are an expert evaluator. Given the question, model prediction, and its chain-of-"
        },
        {
          "LLM verifier for SpeechR generative version evaluation": "thought (if any), provide the following metrics in JSON:"
        },
        {
          "LLM verifier for SpeechR generative version evaluation": "1. final_correct: For yes/no questions (reference is \"Yes\" or \"No\"), interpret the"
        },
        {
          "LLM verifier for SpeechR generative version evaluation": "overall stance in the prediction and compare. For classification/list tasks, 1 if the"
        },
        {
          "LLM verifier for SpeechR generative version evaluation": "prediction mentions at least one reference item or is semantically equivalent; else 0."
        },
        {
          "LLM verifier for SpeechR generative version evaluation": "2. logic_relevance: an integer from 1 to 5 indicating how strongly the prediction"
        },
        {
          "LLM verifier for SpeechR generative version evaluation": "logically follows the question:"
        },
        {
          "LLM verifier for SpeechR generative version evaluation": "1 = no relevance"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "logically follows the question:": "1 = no relevance"
        },
        {
          "logically follows the question:": "2 = very weak relevance"
        },
        {
          "logically follows the question:": "3 = moderate relevance"
        },
        {
          "logically follows the question:": "4 = strong relevance"
        },
        {
          "logically follows the question:": "5 = very strong relevance"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4 = strong relevance": "5 = very strong relevance"
        },
        {
          "4 = strong relevance": "3. cot_coherence_score: an integer from 1 to 5 assessing the chain-of-thought"
        },
        {
          "4 = strong relevance": "coherence (only if provided):"
        },
        {
          "4 = strong relevance": "1 = no coherence or no chain-of-thought(disjointed, illogical)"
        },
        {
          "4 = strong relevance": "2 = low coherence (many gaps)"
        },
        {
          "4 = strong relevance": "3 = moderate coherence (some gaps)"
        },
        {
          "4 = strong relevance": "4 = high coherence (clear, minor issues)"
        },
        {
          "4 = strong relevance": "5 = excellent coherence (very clear, logical flow)"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(Factual Reasoning)": "There was a green house in the green house was a white house in that white house was a red house"
        },
        {
          "(Factual Reasoning)": "in that red house where eggs. What is it?"
        },
        {
          "(Factual Reasoning)": "B: jar"
        },
        {
          "(Factual Reasoning)": "Ground Truth: E"
        },
        {
          "(Factual Reasoning)": "LTU: The best option is A. salt."
        },
        {
          "(Factual Reasoning)": "GAMA: The best option is D. supermarket. The audio mentions ’the park’ and ’watermelon’,"
        },
        {
          "(Factual Reasoning)": "but not ’supermarket’."
        },
        {
          "(Factual Reasoning)": "GAMA-IT: The audio does not provide enough information to accurately determine the location."
        },
        {
          "(Factual Reasoning)": "Mellow: a particular way is best said about the best option..."
        },
        {
          "(Factual Reasoning)": "SALMONN: E. watermelon"
        },
        {
          "(Factual Reasoning)": "Qwen-Audio-Chat: The best option is C. the park."
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": "the white house refers to the white inner layer, the red house refers to the red flesh, and the eggs"
        },
        {
          "(Factual Reasoning)": "refer to the seeds inside. Therefore, the best option is E. watermelon."
        },
        {
          "(Factual Reasoning)": "Gemini-1.5-Pro: The riddle describes a watermelon. Therefore the answer is E. watermelon."
        },
        {
          "(Factual Reasoning)": "(Procedural Reasoning)"
        },
        {
          "(Factual Reasoning)": "John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business."
        },
        {
          "(Factual Reasoning)": "How many hours a week does he spend taking care of dogs?"
        },
        {
          "(Factual Reasoning)": "B: 35"
        },
        {
          "(Factual Reasoning)": "Ground Truth: B"
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": "GAMA-IT: The correct option is D.30"
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": "SALMONN: B.35"
        },
        {
          "(Factual Reasoning)": "Qwen-Audio-Chat: The answer is A.65."
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": "Gemini-1.5-Pro: B"
        },
        {
          "(Factual Reasoning)": "(Normative Reasoning)"
        },
        {
          "(Factual Reasoning)": "Person A: You’re a project manager in a marketing firm. You notice a colleague using company"
        },
        {
          "(Factual Reasoning)": "resources for a personal project during office hours, undermining the company’s policy."
        },
        {
          "(Factual Reasoning)": "Should you take initiative and report this to your superior?"
        },
        {
          "(Factual Reasoning)": "Person B: Yes. I report the colleague to my superior. The colleague is reprimanded and harbors"
        },
        {
          "(Factual Reasoning)": "resentment towards me, creating a hostile work environment. This affects team morale"
        },
        {
          "(Factual Reasoning)": "and productivity, leading to a decline in the quality of work."
        },
        {
          "(Factual Reasoning)": "Question: What moral values are reflected in this response?"
        },
        {
          "(Factual Reasoning)": "Ground Truth: “respect for rules”, “accountability”, “responsibility”, “honesty”, “trust”, “integrity”, “fairness”"
        },
        {
          "(Factual Reasoning)": "Mellow: The response from the other woman is a brief, soft utterance, likely a comment..."
        },
        {
          "(Factual Reasoning)": "SALMONN: The speaker values taking initiative and reporting unethical behavior to their superior."
        },
        {
          "(Factual Reasoning)": "Qwen-Audio-Chat: The second speaker mentions that they value honesty and integrity"
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": ""
        },
        {
          "(Factual Reasoning)": "to uphold them, even if it may lead to personal or team conflict"
        },
        {
          "(Factual Reasoning)": "Gemini-1.5-Pro: The speaker prioritizes upholding company policy and addressing misconduct,"
        },
        {
          "(Factual Reasoning)": ""
        }
      ],
      "page": 18
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "2",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "3",
      "title": "Qwen technical report",
      "authors": [
        "J Bai",
        "S Bai",
        "Y Chu",
        "Z Cui",
        "K Dang",
        "X Deng",
        "Y Fan",
        "W Ge",
        "Y Han",
        "F Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "4",
      "title": "Seed-asr: Understanding diverse speech and contexts with llm-based speech recognition",
      "authors": [
        "Y Bai",
        "J Chen",
        "J Chen",
        "W Chen",
        "Z Chen",
        "C Ding",
        "L Dong",
        "Q Dong",
        "Y Du",
        "K Gao"
      ],
      "year": "2024",
      "venue": "Seed-asr: Understanding diverse speech and contexts with llm-based speech recognition",
      "arxiv": "arXiv:2407.04675"
    },
    {
      "citation_id": "5",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Y Chu",
        "J Xu",
        "Q Yang",
        "H Wei",
        "X Wei",
        "Z Guo",
        "Y Leng",
        "Y Lv",
        "J He",
        "J Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "6",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "authors": [
        "Y Chu",
        "J Xu",
        "X Zhou",
        "Q Yang",
        "S Zhang",
        "Z Yan",
        "C Zhou",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "7",
      "title": "BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions",
      "authors": [
        "C Clark",
        "K Lee",
        "M.-W Chang",
        "T Kwiatkowski",
        "M Collins",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL"
    },
    {
      "citation_id": "8",
      "title": "Training Verifiers to Solve Math Word Problems",
      "authors": [
        "K Cobbe",
        "V Kosaraju",
        "M Bavarian",
        "M Chen",
        "H Jun",
        "L Kaiser",
        "M Plappert",
        "J Tworek",
        "J Hilton",
        "R Nakano",
        "C Hesse",
        "J Schulman"
      ],
      "year": "2021",
      "venue": "Training Verifiers to Solve Math Word Problems",
      "arxiv": "arXiv:2110.14168"
    },
    {
      "citation_id": "9",
      "title": "Mellow: a small audio language model for reasoning",
      "authors": [
        "S Deshmukh",
        "S Dixit",
        "R Singh",
        "B Raj"
      ],
      "year": "2025",
      "venue": "Mellow: a small audio language model for reasoning",
      "arxiv": "arXiv:2503.08540"
    },
    {
      "citation_id": "10",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "S Deshmukh",
        "B Elizalde",
        "R Singh",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Al Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
      "authors": [
        "Q Fang",
        "S Guo",
        "Y Zhou",
        "Z Ma",
        "S Zhang",
        "Y Feng"
      ],
      "year": "2024",
      "venue": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
      "arxiv": "arXiv:2409.06666"
    },
    {
      "citation_id": "13",
      "title": "Fsd50k: an open dataset of human-labeled sound events",
      "authors": [
        "E Fonseca",
        "X Favory",
        "J Pons",
        "F Font",
        "X Serra"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gardner",
        "S Durand",
        "D Stoller",
        "R Bittner",
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "Audio set: An ontology and human-labeled dataset for audio events"
    },
    {
      "citation_id": "15",
      "title": "Gama: A large audio-language model with advanced audio understanding and complex reasoning abilities",
      "authors": [
        "S Ghosh",
        "S Kumar",
        "A Seth",
        "C Evuru",
        "U Tyagi",
        "S Sakshi",
        "O Nieto",
        "R Duraiswami",
        "D Manocha"
      ],
      "year": "2024",
      "venue": "Gama: A large audio-language model with advanced audio understanding and complex reasoning abilities",
      "arxiv": "arXiv:2406.11768"
    },
    {
      "citation_id": "16",
      "title": "2023a. Joint audio and speech understanding",
      "authors": [
        "S Ghosh",
        "A Seth",
        "S Kumar",
        "U Tyagi",
        "C Evuru",
        "S Ramaneswaran",
        "S Sakshi",
        "O Nieto",
        "R Duraiswami",
        "D Manocha",
        "Y Liu",
        "A Luo",
        "H Karlinsky",
        "L Glass",
        "Y Luo",
        "H Liu",
        "A Karlinsky",
        "L Glass"
      ],
      "year": "2023",
      "venue": "Compa: Addressing the gap in compositional reasoning in audio-language models",
      "arxiv": "arXiv:2310.08753"
    },
    {
      "citation_id": "17",
      "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
      "authors": [
        "R Huang",
        "M Li",
        "D Yang",
        "J Shi",
        "X Chang",
        "Z Ye",
        "Y Wu",
        "Z Hong",
        "J Huang",
        "J Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Music understanding llama: Advancing text-to-music generation with question answering and captioning",
      "authors": [
        "S Liu",
        "A Hussain",
        "C Sun",
        "Y Shan"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model",
      "authors": [
        "Z Ma",
        "Z Chen",
        "Y Wang",
        "E Chng",
        "X Chen"
      ],
      "year": "2025",
      "venue": "CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model",
      "arxiv": "arXiv:2501.07246"
    },
    {
      "citation_id": "20",
      "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix",
      "authors": [
        "Z Ma",
        "Y Ma",
        "Y Zhu",
        "C Yang",
        "Y.-W Chao",
        "R Xu",
        "W Chen",
        "Y Chen",
        "Z Chen",
        "J Cong"
      ],
      "year": "2025",
      "venue": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix",
      "arxiv": "arXiv:2505.13032"
    },
    {
      "citation_id": "21",
      "title": "emotion2vec: Self-supervised pretraining for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pretraining for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "22",
      "title": "Mustango: Toward controllable text-to-music generation",
      "authors": [
        "J Melechovsky",
        "Z Guo",
        "D Ghosal",
        "N Majumder",
        "D Herremans",
        "S Poria"
      ],
      "year": "2023",
      "venue": "Mustango: Toward controllable text-to-music generation",
      "arxiv": "arXiv:2311.08355"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "26",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "M Reid",
        "N Savinov",
        "D Teplyashin",
        "D Lepikhin",
        "T Lillicrap",
        "J.-B Alayrac",
        "R Soricut",
        "A Lazaridou",
        "O Firat",
        "J Schrittwieser"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "27",
      "title": "Audiopalm: A large language model that can speak and listen",
      "authors": [
        "P Rubenstein",
        "C Asawaroengchai",
        "D Nguyen",
        "A Bapna",
        "Z Borsos",
        "F Quitry",
        "P Chen",
        "D Badawy",
        "W Han",
        "E Kharitonov"
      ],
      "year": "2023",
      "venue": "Audiopalm: A large language model that can speak and listen",
      "arxiv": "arXiv:2306.12925"
    },
    {
      "citation_id": "28",
      "title": "Mmau: A massive multi-task audio understanding and reasoning benchmark",
      "authors": [
        "S Sakshi",
        "U Tyagi",
        "S Kumar",
        "A Seth",
        "R Selvakumar",
        "O Nieto",
        "R Duraiswami",
        "S Ghosh",
        "D Manocha",
        "A Talmor",
        "J Herzig",
        "N Lourie",
        "J Berant"
      ],
      "year": "2018",
      "venue": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
      "arxiv": "arXiv:2410.19168"
    },
    {
      "citation_id": "29",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "C Tang",
        "W Yu",
        "G Sun",
        "X Chen",
        "T Tan",
        "W Li",
        "L Lu",
        "Z Ma",
        "C Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "30",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A Dai",
        "A Hauth",
        "K Millican"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "31",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "32",
      "title": "Polyphonic sound event detection by using capsule neural networks",
      "authors": [
        "F Vesperini",
        "L Gabrielli",
        "E Principi",
        "S Squartini"
      ],
      "year": "2019",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "A universal benchmark for audio large language models",
      "authors": [
        "B Wang",
        "X Zou",
        "G Lin",
        "S Sun",
        "Z Liu",
        "W Zhang",
        "Z Liu",
        "A Aw",
        "N Chen"
      ],
      "year": "2024",
      "venue": "A universal benchmark for audio large language models",
      "arxiv": "arXiv:2406.16020"
    },
    {
      "citation_id": "34",
      "title": "Speech swin-transformer: Exploring a hierarchical transformer with shifted windows for speech emotion recognition",
      "authors": [
        "Y Wang",
        "C Lu",
        "H Lian",
        "Y Zhao",
        "B Schuller",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Audio-reasoner: Improving reasoning capability in large audio language models",
      "authors": [
        "Z Xie",
        "M Lin",
        "Z Liu",
        "P Wu",
        "S Yan",
        "C Miao",
        "H Xue",
        "Y Liang",
        "B Mu",
        "S Zhang",
        "M Chen",
        "Q Chen",
        "L Xie"
      ],
      "year": "2024",
      "venue": "2024 IEEE 14th International Symposium on Chinese Spoken Language Processing",
      "arxiv": "arXiv:2503.02318"
    },
    {
      "citation_id": "36",
      "title": "AI for Accessible Education: Personalized Audio-Based Learning for Blind Students",
      "authors": [
        "C Yang",
        "P Taele"
      ],
      "year": "2025",
      "venue": "AI for Accessible Education: Personalized Audio-Based Learning for Blind Students",
      "arxiv": "arXiv:2504.17117"
    },
    {
      "citation_id": "37",
      "title": "Benchmarking large audio-language models via generative comprehension",
      "authors": [
        "Q Yang",
        "J Xu",
        "W Liu",
        "Y Chu",
        "Z Jiang",
        "X Zhou",
        "Y Leng",
        "Y Lv",
        "Z Zhao",
        "C Zhou"
      ],
      "venue": "Benchmarking large audio-language models via generative comprehension",
      "arxiv": "arXiv:2402.07729"
    },
    {
      "citation_id": "38",
      "title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models",
      "authors": [
        "W Yang",
        "Y Li",
        "M Fang",
        "Y Wei",
        "T Zhou",
        "L Chen"
      ],
      "year": "2024",
      "venue": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Language Models",
      "arxiv": "arXiv:2411.14842"
    },
    {
      "citation_id": "39",
      "title": "Zipformer: A faster and better encoder for automatic speech recognition",
      "authors": [
        "Z Yao",
        "L Guo",
        "X Yang",
        "W Kang",
        "F Kuang",
        "Y Yang",
        "Z Jin",
        "L Lin",
        "D Povey"
      ],
      "year": "2023",
      "venue": "Zipformer: A faster and better encoder for automatic speech recognition",
      "arxiv": "arXiv:2310.11230"
    },
    {
      "citation_id": "40",
      "title": "Sound event detection transformer: An event-based end-to-end model for sound event detection",
      "authors": [
        "Z Ye",
        "X Wang",
        "H Liu",
        "Y Qian",
        "R Tao",
        "L Yan",
        "K Ouchi"
      ],
      "year": "2021",
      "venue": "Sound event detection transformer: An event-based end-to-end model for sound event detection",
      "arxiv": "arXiv:2110.02011"
    },
    {
      "citation_id": "41",
      "title": "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung",
        "Ieee",
        "W Yu",
        "Z Jiang",
        "Y Dong",
        "J Feng"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "42",
      "title": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "authors": [
        "D Zhang",
        "S Li",
        "X Zhang",
        "J Zhan",
        "P Wang",
        "Y Zhou",
        "X Qiu"
      ],
      "year": "2023",
      "venue": "Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities",
      "arxiv": "arXiv:2305.11000"
    }
  ]
}