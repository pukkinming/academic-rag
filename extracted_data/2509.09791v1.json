{
  "paper_id": "2509.09791v1",
  "title": "The Msp-Podcast Corpus",
  "published": "2025-09-11T18:51:58Z",
  "authors": [
    "Carlos Busso",
    "Reza Lotfian",
    "Kusha Sridhar",
    "Ali N. Salman",
    "Wei-Cheng Lin",
    "Lucas Goncalves",
    "Srinivas Parthasarathy",
    "Abinay Reddy Naini",
    "Seong-Gyun Leem",
    "Luz Martinez-Lucas",
    "Huang-Cheng Chou",
    "Pravin Mote"
  ],
  "keywords": [
    "Affective computing",
    "speech emotional database",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The availability of large, high-quality emotional speech databases is essential for advancing speech emotion recognition (SER) in real-world scenarios. However, many existing databases face limitations in size, emotional balance, and speaker diversity. This study describes the MSP-Podcast corpus, summarizing our ten-year effort. The corpus consists of over 400 hours of diverse audio samples from various audiosharing websites, all of which have Common Licenses that permit the distribution of the corpus. We annotate the corpus with rich emotional labels, including primary (single dominant emotion) and secondary (multiple emotions perceived in the audio) emotional categories, as well as emotional attributes for valence, arousal, and dominance. At least five raters annotate these emotional labels. The corpus also has speaker identification for most samples, and human transcriptions of the lexical content of the sentences for the entire corpus. The data collection protocol includes a machine learningdriven pipeline for selecting emotionally diverse recordings, ensuring a balanced and varied representation of emotions across speakers and environments. The resulting database provides a comprehensive, high-quality resource, better suited for advancing SER systems in practical, real-world scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotional Databases",
      "text": "Table  I  presents some emotional databases. Although the research community has access to numerous emotional databases, they come with certain limitations that restrict their effectiveness in tackling ongoing research problems. These limitations include the lack of naturalness in the emotional expressions, unbalanced emotional content, and constraints in size and speaker diversity.\n\nTraditional emotional corpora designed for emotion recognition largely depended on actors who were directed to vocalize sentences with intended emotions. This practice was used to create several well-known emotional databases, such as the Emo-DB  [4] , RAVDESS  [5] , TESS  [37] , CREMA-D  [2] , and the Chen Bimodal  [34]  databases. While these datasets have played an essential role in early research efforts, the use of acted emotions presents challenges in truly mirroring the complex and spontaneous nature of genuine human emotions, as discussed by Devillers et al.  [38]  and Batliner et al.  [39] . Some databases have been designed to address this limitation. The DUSHA corpus  [20]  was constructed using a hybrid data collection methodology, combining elicited speech from non-professional actors with spontaneous speech extracted from podcasts. This approach aims to balance the experimental control inherent in acted performances with the ecological validity of naturalistic recordings.\n\nOther databases, such as the USC-IEMOCAP  [40] , MSP-IMPROV  [8] , and THAI-SER  [25]  corpora, aimed to bridge this gap by incorporating more naturally occurring emotional expressions within dyadic interactions, thereby deviating from the more scripted monologues of previous databases. These endeavors made significant strides in producing dialogue that closely mimics the nuances of real-world emotional exchanges. Yet, the usage of professional actors remained a barrier to capturing naturalistic emotional responses.\n\nIn the pursuit of authenticity, other datasets have relied on spontaneous interactions derived from sources such as colloquial conversations (SEMAINE  [33] , RECOLA  [9] , TUM-AVIC  [27] ), television programs (VAM  [12] , MELD  [13] , CHEAVD  [41] , UrduSER  [35] ), the Internet (BIIC-Podcast  [15] , WHiSER  [32] , CMU-MOSI  [36] , CMU-MOSEI  [24] , CMU-MOSEAS  [23] ), and customer service calls (CEMO  [26] ). This shift towards spontaneity was critical in capturing genuine emotional displays, but these databases faced the obstacle of skewed emotional representations, constrained by the contexts from which they were sourced. For instance, television programs broadcasting relationship issues might lean towards negative emotions  [12] , while casual conversations might predominantly exhibit positive emotions  [9] . The emotional imbalance also poses a challenge for SER models, which require diverse and evenly distributed emotional examples to learn effectively. For example, Naini et al.  [42]  demonstrated SER improvements by just undersampling the training set to match the emotional distribution of the target domain.\n\nA prominent trend in emotion corpus development involves leveraging crowdsourcing to acquire data from a large pool of participants using their personal, consumergrade devices. In this paradigm, exemplified by corpora such as Emozionalmente  [31]  and the dataset by Smith et al.  [21] , annotation is also frequently crowdsourced to enhance scalability and cost-effectiveness. A direct consequence of this methodology is significant acoustic variability due to differences in microphone types and recording environments. More recent approaches automate this process; for instance, MIKU-EmoBench  [22]  is constructed by applying an automated pipeline to extract and label content from large-scale, user-generated video platforms. Although the acquisition is automated, this strategy retains the core benefit of crowdsourcing by capturing a wide spectrum of speech from the varied settings and diverse speaker demographics present in the original online content. While crowdsourcing and automated retrieval have expanded the scale and diversity of emotional databases, these approaches often struggle with annotation consistency, emotional ambiguity, and quality control. As a result, many large-scale corpora exhibit high variability in recording conditions and occasional inaccuracies in emotional labeling. These limitations highlight the need for frameworks that not only scale to large datasets but also maintain annotation reliability and emotion authenticity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Relation To Prior Work",
      "text": "The effort to collect the MSP-Podcast corpus was motivated by retrieval-based strategies explored by Mari-  ooryad et al.  [19] . The core idea was to identify emotional segments with machine learning models. We noticed this approach can scale if we design an emotion perceptual evaluation using crowdsourcing  [43] . Lotfian and Busso  [17]  formally introduced the original protocol, describing early results, showing the effectiveness of our strategy in retrieving emotional speech with the intended emotional content (e.g., finding positive speech with high valence values). Since then, we have released early versions of the corpus over the years, from version 1.0 in November 2017 to version 1.12 in June 2024. With this study, we release version 2.0 of the MSP-Podcast corpus, the final release.\n\nWe have prepared this paper minimizing the overlap with the protocol described in Lotfian and Busso  [17] . Instead, we have focused on describing the final release of the corpus and the modifications that we implemented to improve the quality of the annotations. The resulting corpus consists of 409 hours of speech, offering much broader emotional and speaker diversity than previous databases. The enhancements make the final version of the MSP-Podcast corpus a far more comprehensive and robust resource for SER research, positioning it as a superior dataset for real-world emotion recognition tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Protocol For The Msp-Podcast Corpus",
      "text": "The protocol for data collection in the MSP-Podcast corpus is explained in Lotfian and Busso  [17] . This section summarizes the protocol, with a focus on the changes implemented to enhance the quality of the data. Figure  1  shows a diagram of the data collection protocol.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Selection Of Podcasts",
      "text": "We source our speech data from online sources that host publicly available audio. Our goal is to have an emotionally diverse and gender-balanced corpus. We also want speaker diversity. Therefore, we collect podcasts, talk shows, and lectures about sports, popular media, politics, personal struggles, societal issues, public health, crime, technology, and daily life. We use five criteria when searching for podcasts: (1) clean audio, no background music or speech, and not too much noise, (2) English speech, (3) emotional speech, prioritizing queries likely to convey target emotions, (4) diverse speaker demographic, and (5) appropriate license. The podcasts were identified primarily through manual searches, where researchers selected search terms that could elicit emotional topics and chose podcasts that met the aforementioned criteria. 4,743 (78.9%) podcasts in the corpus were found in this way (manually). Eventually, we wrote a script to automatically find podcasts. A researcher can input a list of search terms, and the script will find podcasts that meet the criteria and download them. The script first downloads the metadata of some of the search results, then filters them by language (if available) and license. The script then downloads the audio of the chosen podcasts. We implement automatic steps to filter podcasts based on a music detector  [44]  and a noise detector  [45] . Finally, a researcher briefly listens to each podcast selected by the script, verifying whether the chosen recordings meet the target criteria. 1,265 (21.1%) podcasts in the corpus were found this way (automatically). In total, the MSP-Podcast corpus includes recordings from 6,007 unique podcasts. We select podcasts that are shared with licenses that allow us to distribute and modify them freely. We mainly focus on podcasts with Public Domain licenses or Creative Commons licenses with minimal restrictions (https: //creativecommons.org/). Table  II  shows the number and percentage of podcasts in the corpus that were selected with specific licenses. Our practice was to save a screenshot of the website to document the license of the podcasts. There are 40 podcasts whose license information was not saved when initially collected, despite being selected with the target Creative Commons license. When we searched for the license information at a later date, the podcasts had been removed from the online website. Therefore, we do not have precise license information for these 40 podcasts in the corpus, which we denote as having an \"Unknown\" license in Table  II .\n\nAfter choosing and downloading the podcasts, we convert all of them to the same audio format as described in Lotfian and Busso  [17] . We convert the podcasts to wave audio format with a mono channel, a sample rate of 16kHz, and 16-bit pulse code modulation (PCM) with the Librosa toolbox  [46] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Data Segmentation",
      "text": "The next step in the pipeline is to split the podcasts into speaking turns. We define a speaking turn as a segment spoken by a speaker, which may comprise one or more sentences or phrases. We started the project by manually conducting this step. Researchers manually split the first 279 (4.64%) podcasts. However, this process was very time-consuming considering the final size of the corpus. We decided to use an automated tool to split the remaining podcasts. Since podcasts can contain music or noisy segments and often feature multiple speakers, we need a tool that can segment the audio into speaking turns while also keeping speakers and noise separate. The diarization of the podcasts into sentences was mostly done using the Microsoft Azure Video Indexer 1 . 3,667 (61.0%) podcasts in the corpus were segmented using this tool. We eventually switched to using the Whisper model  [47] . 797 (13.3%) podcasts were segmented using the pre-trained large Whisper model in the HuggingFace library  [48] . During the last part of the project, we switched to the pretrained large-v2 Whisper model. 1,265 (21.1%) podcasts were segmented using that model. In addition to speaker diarization, these tools provide automatic transcription of the entire podcasts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Automatic Filtering & Selection Of Speaking Turns",
      "text": "After the podcasts are split into speaking turns, the next step involves employing multiple filters designed to aid our system in selecting only the highest-quality recordings to proceed with our annotation process (single speaker, no music, clean recording, with target duration, and target emotion). During this stage, we conduct several key operations: speaking turn duration estimation, resegmentation of long segments using word alignment, music detection, noise estimation, multiple speaker detection, gender prediction, automatic emotion retrieval, and final inspection by a trained human worker. This section explains each of these filters used to select the speaking turns to be included in the corpus.\n\nThe initial step involves verifying the timings and word content of the speech segments. Our goal is to have speaking turns with a duration between 2.75 and 11 seconds. The lower threshold is justified by the need to have enough context for a rater to reliably infer an emotional label during the perceptual evaluation. The higher threshold was imposed because emotions can vary during a speaking turn, so having a single label may not accurately reflect the emotional content of the speaking turn. Audios shorter than 2.75 seconds are automatically excluded, while those exceeding 11 seconds undergo a re-segmentation process. This step involves utilizing the automatic transcriptions from Section III-B and executing an automatic word-level alignment with the audio segments using a Python module  [49] . This module facilitates interaction with Praat's TextGrid  [50]  to align transcripts with audio. We then evaluate the alignments to identify pauses in speech lasting at least 0.3 seconds, at which point we crop the audio to create smaller segments within the target range of 2.75 to 11.0 seconds. The 0.3-second 1 https://azure.microsoft.com/en-us/products/ai-video-indexer threshold is applied to identify pauses indicative of a potential sentence completion by the speaker. Following this resegmentation, we aggregate all audio segments within the 2.75 to 11.0-second duration and automatically review their transcriptions to exclude any speaking turns with fewer than five words, thus eliminating segments lacking substantial spoken content. Figure  2  shows the distribution of the durations of the selected speaking turns included in the corpus.\n\nThe audio segments are then evaluated with music detection and noise estimation algorithms. In particular, we employ a pre-trained audio tagging model  [44]  to identify segments where music is present. Segments where music constitutes more than 50% of the duration are filtered out. Following this step, we estimate the signalto-noise ratio (SNR) using the WADA-SNR algorithm  [51] , based on waveform amplitude distribution analysis (WADA). Audio segments with an SNR below 15dB are subsequently rejected. The remaining audio segments are further processed using the pyannote.audio speaker diarization toolkit  [52] ,  [53]  to ensure that each audio segment contains speech from only a single speaker. The use of this toolkit enables the automatic exclusion of samples containing multiple speakers.\n\nAll audio segments that meet the aforementioned filters are then subjected to a series of predictive models to automatically identify speaker and recording characteristics. One of the traits is gender. Gender prediction is achieved through a pre-trained speech long short-term memory (LSTM)-based model, capable of distinguishing between \"Female\" and \"Male\"  [54] . This process is done to gender balance the selected speaking turns.\n\nWe have millions of valid speaking turns obtained from the 6,007 podcasts that passed our criteria. Most of these segments are expected to be emotionally neutral. As explained in Lotfian and Busso  [17] , we can prioritize the annotation of emotional recordings by selecting speaking turns predicted to have target emotions. Therefore, we implement an automatic emotional retrieval step. We mitigate the potential problem of biasing the selected speaking turns towards specific SER systems by employing multiple models and formulations. The SER models encompass multiple versions of emotion classification  [55] , emotion attribute prediction  [56] ,  [57] , ranking-based preference learning prediction  [58] , and textual sentiment analysis  [59] . We consider open-source implementations  [55] -  [57] ,  [59] -  [62]  and internally trained variants. The final retrieval system relies on over 48 criteria dictated by emotion models. It employs various pre-trained models developed from extensive emotional corpora, including CREMA-D  [2] , MSP-IMPROV  [8] , IEMOCAP  [63] , earlier versions of MSP-Podcast  [17] , and Twitter sentiment data  [64] . These models also utilize a comprehensive range of inputs, including low-level descriptors (LLDs), high-level descriptors (HLDs), raw audio for foundational self-supervised learning (SSL) models, and textual data derived from audio transcriptions. The models were updated and retrained multiple times during the project. This emotion retrieval step is crucial for assembling an emotionally diverse and naturalistic corpus that spans a broad spectrum of emotional states.\n\nAfter running all these models on the audios, we compile a set of master lists with predictions retrieved for each task using each model, and rank these predictions from high to low accordingly for each model. We ensure that the lists are set up to dynamically change as new data is processed and entered into our master lists. Such a ranking system is instrumental in our methodology, helping us select high-emotional content and minority emotional states for annotation. Additionally, we created separate master lists for each gender. We fine-tune our selection using dynamic thresholds to maintain a balanced representation of genders and emotional states, adapting our approach as new data enters the annotation pipeline. This strategy ensures the creation of a more inclusive and precise annotated dataset, effectively minimizing bias. Updates to our master lists ensure that each sample is selected only once, avoiding redundancy in future selections. Moreover, we document the rationale behind each selection (e.g., a sample A is chosen due to its high emotional rating by model B), facilitating an evaluation of our models' effectiveness in identifying emotionally relevant samples for subsequent selection rounds and threshold adjustments or model removals. We weekly monitored the performance of these SER models during the project.\n\nSelected samples are then forwarded to a trained evaluator who conducts a thorough review, listening to each audio to confirm its suitability for annotation. This final check aims to identify any samples that, despite passing through our filters, might still present issues such as background music, low signal-to-noise ratios, unintelligible speech, foreign language usage, extremely brief sentences, profanity, multiple speakers, or excessive background noise. The evaluator's task is to identify and exclude samples based on these criteria, compiling a final list to be used for annotation. Notice that the evaluator listens only to the selected samples, instead of the millions of speaking turns in the entire pool considered for the corpus.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Perceptual Evaluation",
      "text": "The last step in the protocol is to annotate the selected speaking turns. We annotate emotional categories (e.g., anger, happiness, etc.) and emotional attributes (valence, arousal, and dominance). Sections IV-A and IV-B describe the instrument used to annotate the corpus. The original protocol employed a slightly modified crowdsourcing strategy introduced in Burmania et al.  [43] . The approach tracks the quality of annotations provided by a worker in real-time during a session, stopping the session if the quality drops below a given threshold. We can measure quality by including reference sentences that we have already annotated so that we can estimate inter-evaluator agreements. Lotfian and Busso  [17]  introduced specific changes to the original protocol, aiming to increase the frequency of checkpoints and incorporate primary emotional annotations and attribute-based annotations into the quality estimation. We followed this approach for the first part of the project.\n\nAround September 2021, we noticed important issues with our crowdsourcing platform. We noticed that human intelligent tasks (HITs) were immediately taken when we uploaded them, suggesting the presence of bots. Several HITs returned with random annotations (e.g., all the sentences in the batch were labeled as \"happy\"). Our first step was to suspend every worker found to be showing this behavior. Next, we audited and hardened the perceptual evaluation code, adding safeguards to thwart automated bot submissions and improve overall robustness. While refining our code, we developed an alternative approach to prevent delays in the annotations. We decided to hire student workers from the University of Texas at Dallas (UT Dallas) to annotate the corpus. Because emotion-recognition skill varies across individuals, we created and administered a screening test to ensure we could retain only high-performing candidates. The resulting student annotations proved consistently higher in quality than those obtained through traditional crowdsourcing. This new process enabled us to provide regular feedback to our student workers, which was not possible with crowdsourcing workers. As a result, we decided to discontinue our crowdsourcing effort and transition entirely to perceptual evaluations conducted by our student workers. Regularly, we had between 14 and 20 student workers annotating the corpus. We developed a website that connected to the server used for the perceptual evaluation, displaying the number of annotations provided by each student worker in real-time, thereby providing a powerful tool to track our progress. It was easy to identify student workers who were not actively involved in the evaluation.\n\nWe collect five or more annotations from different workers for the crowdsourcing evaluation and the perceptual evaluation conducted by our student workers. Some of the speaking turns have more than five evaluations, since they were used as reference sentences in our crowdsourcing protocol. Figure  3  shows the distribution of the number of annotations per sentence in the corpus. By providing",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Week Placement",
      "text": "Your placement averaged over the 2024 Spring semester compared to other workers.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Semester Placement",
      "text": "Best agreement",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Best Agreement",
      "text": "Best agreement",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Best Agreement",
      "text": "The least number of surveys The most number of surveys multiple annotations per speaking turn, we enable the exploration of multiple research problems related to utilizing the subjectivity of human emotional perception, such as curriculum learning training strategies  [65] , exploring cooccurrence of emotion to improve the cost function  [66] , training with soft labels  [67] -  [71] , implementing oversampling strategies for minority classes  [72] , and finding trends across annotations  [73] ,  [74] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Surveys",
      "text": "With our student workers, we did not implement the crowdsourcing strategy to track the quality in real-time. Instead, we focused on providing weekly feedback. A research assistant trained the student workers before they began annotating data, describing emotional descriptors, particularly the concepts of valence, arousal, and domi- nance. The student worker completed the first session with the research assistant, who answered any questions raised during the perceptual evaluation. In addition, we wanted to provide frequent feedback to the student workers, so they were aware if we were satisfied with their annotations. We implemented a weekly report that provides their relative ranking with respect to other student workers. Figure  4  shows an example of the document shared with our student workers. The report presents weeklybased performance (top part of the report) and semesterbased reports (bottom part of the report). Instead of providing the actual values of the metrics used to estimate inter-evaluator agreements, we provide a relative ranking comparing the worker with the rest of the workers. For each indicator, we denote the performance with an arrow placed between two extremes. The closer to the right extreme, the better (see Figure  4 ). The bars also include a black vertical line that indicates the lower threshold we tolerate. The first indicator includes the number of annotations completed by the student workers. Then, the report includes the agreement for primary emotions and attribute-based annotations (arousal, valence, and dominance). It also includes the overall score, which is the average of all the emotional descriptors. In the example in Figure  4 , the student worker was very good at annotating primary emotions and valence (both for the current week and the entire semester). However, the annotations for arousal and dominance were average. In all cases, the quality of the worker was above our minimum threshold. The reports were automatically generated, so this process did not require much continuous effort from our team. We also implemented a targeted training to re-train our student workers with lower inter-evaluator agreements. We created a training website that focuses on a single emotional descriptor (primary emotions, valence, arousal, or dominance). Therefore, the student workers only work on the emotional descriptor that they are struggling with. For example, if a student worker has low inter-evaluator agreement on dominance, the application only includes samples to improve this emotional attribute. We automatically identify speaking turns where the annotations from the target student workers differ from consistent annotations obtained from other student workers. The application asks the student workers to re-annotate these carefully selected samples. Then, it lists their original annotations and the annotations made by the other student workers. These annotations are only revealed after the student worker re-annotates the speaking turn. Figure  5  shows an example for primary emotions (Figure  5(a) ) and for valence (Figure  5(b) ). Not shown on the figures are the precise instructions given to the student workers to understand the corresponding emotional descriptors. This training was mandatory for student workers with quality below our minimum thresholds, and optional for all others who may want to practice to solidify their understanding of the emotional descriptors used in this corpus.\n\nA later addition to the perceptual evaluation website was an optional field where a student worker could indicate that a speaking turn still had issues, despite our efforts to filter out overlapped speech, silence, noisy recordings, foreign language, or speech with background music (see bottom part of the questionnaire in Figure  6 ). When a file was flagged, it was immediately separated from the perceptual evaluation until we manually checked if the speaking turn should be removed entirely from the database. This step was very important to avoid annotating data that we would later discard.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Annotation Of The Corpus",
      "text": "A key feature of the corpus is the annotations of the speaking turns. This section describes the annotations for emotions, speaker identification, human transcription, and phonetic alignment. For emotions, we utilize both categorical and dimensional attributes to describe emotions adequately.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Annotation Of Categorical Emotions",
      "text": "The MSP-Podcast corpus offers a rich set of emotion content from natural conversational speech. Figure  6  shows the questionnaire used for the perceptual evaluation for the evaluations using crowdsourcing and student workers. The categorical annotation (bottom part in Figure  6 ) was inspired by the work of Devillers et al.  [38] , which includes dominant (Major) and secondary (Minor) labels to capture mixtures of emotions. The primary emotions in the perceptual evaluation include anger, sadness, happiness, surprise, fear, disgust, contempt, and neutral speech. The workers can also select \"other\" and add their label to add flexibility and avoid the forced-choice response bias discussed by Russell  [75] . The workers select only one primary emotion.\n\nEnter the code at the end of the video: Please rate the negative vs. positive aspects of the video. Click on the image that best fits the video Please rate the calm vs. excited aspect of the video. Click on the image that best fits the video Please rate the weak vs strong aspects of the video. Click on the image that best fits the video Is any of these emotions the primary emotion in the audio? If not, select Other and specify the emotion",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Anger Sadness Happiness Surprise Fear Disgust Contempt Neutral Other",
      "text": "Please pick all the emotional classes that you perceived in the audio (Include the primary emotions selected in the previous question) Figure  7 (a) shows the number of speaking turns assigned to each primary emotion category using the plurality rule. We include the class \"no agreement\" for speaking turns that do not reach agreement under the plurality rule. The histogram reflects the frequency at which emotions appear in natural conversation, with many samples for classes such as happiness, anger, sadness, and neutral speech, and few samples for surprise, fear, disgust, and contempt. Neutral speech is the most dominant class in regular conversation. However, we only have 28% of the speaking turns labeled as neutral, demonstrating the effectiveness of our retrieval-based strategy (Section III-C). Figure  8 (a) shows the word cloud of the labels provided when workers selected \"other\" as the primary emotions. The figure identifies the emotions \"confused,\" \"excited,\" and \"concerned\" as the most common terms. These emotions are potential candidates for inclusion in the primary emotions for future evaluations.\n\nThe secondary emotions extend the list of eight primary emotions by adding frustration, annoyance, depression, disappointment, excitement, amusement, concern, and confusion (16 emotions). We also include the \"other\" option, allowing them to add their own labels. The workers are asked to select all the secondary emotions that they perceived in the speaking turn. We explicitly requested that the primary class be included as one of the secondary emotions, but the workers did not always follow this instruction. Secondary emotions can play a crucial role in understanding the complex blend of emotions expressed in the speaking turns. Figure  7 (b) shows the histogram of secondary labels selected in the individual annotations. We did not aim to obtain consensus labels like the case with primary emotions. For consistency, we added the For secondary emotions, we present a histogram of the secondary emotions selected in the individual evaluations.\n\nprimary emotion to the secondary emotion list when the worker did not include it. Neutral, happiness, and anger are the most commonly selected classes. If we do not include the primary emotions, the most popular selections were concern, amusement, and frustration. The classes confusion and depression were the least frequent selections. Figure  8 (b) shows the word cloud with the emotional labels provided by the workers when they selected the option \"other.\" The classes \"curious,\" \"hopeful,\" \"neutral,\" are the most frequent labels, followed by \"passionate,\" \"confident,\" \"interested,\" \"calm and \"content.\" The word cloud figures highlight the nuanced and co-occurring nature of emotions that need richer expressive descriptors to represent affective states.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Annotation Of Emotional Attributes",
      "text": "Emotional attributes are an alternative, powerful strategy to characterize emotions. We include the emotional attributes of valence (negative to positive), arousal (calm to active), and dominance (weak to strong). The top part of Figure  6  shows the questionnaire for these attributes. We rely on self-assessment manikin (SAM)  [76]  to visually capture the essence of each emotional attribute. We use a Likert scale from 1 to 7, with 1 indicating the lower extreme (e.g., very negative, very calm, or very weak) and 7 indicating the higher extreme (e.g., very positive, very active, or very strong). The consensus label for an attribute is the average score assigned across workers for each speaking turn.\n\nFigure  9  illustrates the emotional attribute histograms of the speaking turns. Each distribution resembles a unimodal Gaussian distribution. For valence, the center of the distribution is around 4, which corresponds to the neutral range in this emotional dimension. For arousal and dominance, the distributions are slightly shifted to the right, indicating more active and dominant speech recordings.\n\nFigure  10  displays each speaking turn in the arousalvalence space, with colors indicating the consensus primary emotion assigned to them. The name of each emotional class is positioned at the mean arousal and valence coordinates for that emotion. The figure shows that we have speaking turns with expressive content covering most of the arousal-valence space. The emotional classes are located in the expected quadrant of the arousal-valence space. The figure also reinforces the importance of having categorical and attribute-based annotations. We observe important intra-class variability for primary emotions, indicating that speaking turns assigned to the same class can exhibit a wide range of emotional variability (e.g., cold anger versus hot anger). By having both emotional",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Inter-Evaluator Agreement",
      "text": "Having quality emotional annotations has been a key goal of our effort. Given the struggles we experienced with crowdsourcing evaluations, we decided to estimate the inter-evaluator agreement for each worker, especially those recruited in our crowdsourcing perceptual evaluation. Based on the agreements, we removed 430 crowdsourcing workers and their 44,968 annotations. These speaking turns were reannotated with our student workers. After these corrections, we have 1,446,270 emotional annotations from 13,280 workers. Out of them, we have 13,205 crowdsourcing workers who completed 494,340 annotations (34.18% of the annotations), and 75 student workers who completed 951,930 annotations (65.82% of the annotations). The release of the corpus include the age and gender of the annotators. The inter-evaluator agreement significantly increased after re-annotating labels provided by unreliable crowdsourcing workers. The weekly feedback and the training procedure also helped improve the reliability of the labels.\n\nTable  III  presents the inter-evaluator agreement for the entire database and individual partitions (as described in Section V-A). For primary emotions, the Fleiss kappa statistic is 0.411 for the entire data. This agreement is high, considering the naturalness of the recordings and the inclusion of eight classes. For emotional attributes, the value for Krippendorff's α for valence is better than the value for arousal. Dominance is the dimension with lower agreement, although its score is above α > 0.38.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Speaker Information",
      "text": "It is essential to ensure that data splits for train, validation, and test are speaker-independent for effective SER performance that replicates the expected results on unseen data. This step requires speaker information. Knowing the identity of the speakers is also helpful to explore the role of emotions in other speech tasks such as speaker verification and identification  [77] -  [81]  and speech synthesis  [82] ,  [83] . Therefore, we manually annotate the speaker information of most of the corpus.\n\nAs an initial step in the manual annotation process, we identify all speakers participating in a podcast session Fig.  11 . Annotation process for speaking information using Elan. An audio track contains a previously annotated tier related to speaker 4314, providing contextual information for new annotations. A tier named 'TBD' contains the speaking turns, without speaker information, to be annotated.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Click On The Video To Play.",
      "text": "This is video number 1. Currently working on speaker: 793. View instructions again",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Submit Prev Exit",
      "text": "Fig.  12 . Interface of the verification website to correct the speaker information. The annotator listens to both the reference audio and a clip that is supposed to belong to the same speaker (speaker 793 in the example). Sequentially listening all speaking turns associated with a given speaker facilitates identifying potential errors in speaker annotations.\n\nusing the available information on the source webpage. Then, we listen to each speaking turn selected from that podcast and assign it to its respective speaker. Figure  11  shows the Elan interface used for this annotation. For each audio track, we create tiers for existing speaker annotations and a new tier for speaking turns without speaking information that we aim to annotate. As illustrated in Figure  11 , an audio track contains two annotation tiers named '4314' and 'TBD', which indicate the previously annotated speaking turns associated with speaker 4314 and the one to be annotated. We then listen to the audio around the segments, assigning speaker information to each. To maintain anonymity, each speaker is assigned a unique identification number. Some speaking turns are very hard to assign to a speaker in the conversation, even after listening to the context from nearby segments. The instruction was to mark these speakers as \"unknown,\" prioritizing precision in the annotations. We conducted a manual speaker verification process to correct potential mistakes made in the speaker annotations. During this process, all speaking turns associated with an individual speaker are reviewed sequentially using the user interface shown in Figure  12 . For each individual speaker, a 30-second reference audio is created by con-  catenating manually selected, error-free audio segments. Each speaking turn is then evaluated against this reference audio and marked to indicate whether the current clip belongs to the reference speaker. The annotators can directly compare the voice of the reference speaker with the voice of each speaking turn associated with that speaker. This method facilitates filtering outliers and inconsistencies in speaker annotations. The speaking turns flagged with wrong speaker information by this verification step are manually re-annotated to refine the speaker identities. In total, we have 3,641 unique speakers, where 2,043 are females and 1,598 are males. Table  IV  provides the number of speakers for the entire corpus and for the partitions described in Section V-A.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "E. Transcription",
      "text": "Linguistic content can provide rich information for predicting an emotion. Including text, for example, was key in recent emotion recognition challenges  [60] ,  [84] . Therefore, we provide transcription for the collected speech samples. We ask human annotators to transcribe the speaking turns in the corpus. For this purpose, we provided the collected audio files to REV.com, which generated transcripts. Transcribers provide several indicators to describe nonverbal sounds that do not include spoken words, such as laughter or affirmative sounds. We remove indicators irrelevant to spoken information, such as (music) or\n\nFor consistency, we also cluster indicators that denote similar sounds, leaving eight non-verbal indicators in our transcript shown in Table  V . We evaluate the quality of the annotated transcript by comparing the prediction result of robust automatic speech recognition (ASR) systems with the collected transcript. We use OpenAI WhisperX  [47]  and NVIDIA NeMo Canary  [85]  ASR systems for this process. We downloaded the following pre-trained checkpoints: whisper-medium.en for OpenAI WhisperX and canary-1B for NVIDIA NeMo Canary. These ASR systems were at the top of the rank in the Open ASR Leaderboard  [86]  (observed on Oct/23/2024). With these checkpoints, we get the ASR prediction for each speaking turn. We modified the configuration of the ASR model to make it only predicts alphabet characters without having any digits or special characters. We then compute the word error rate (WER) between the prediction and the annotated transcript, resulting in two WERs for each of the annotated speaking turns. We ignore non-verbal indicators while computing the WER. We re-annotate transcripts for the speaking turns when both WERs are above 70%.\n\nThe corpus contains 4.3 million tokens and 50,677 unique words, reflecting a high degree of lexical diversity. The average length of the speaking turns is 15.89 words, capturing the natural variability and spontaneity of conversational speech. Figure  13  shows a histogram of the number of words per speaking turn, with a peak at 11 words. This distribution is consistent with conversational speech, where speakers tend to produce short but semantically rich segments.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "F. Phonetic Alignment",
      "text": "We provide time-aligned phonetic information for each speech segment in the corpus. This level of granularity enables fine-grained analysis of how phonetic structure interacts with emotions, which can support both acoustic modeling and prosody-aware emotion recognition. Importantly, these alignments facilitate cross-lingual and crosscorpus comparisons for emotion recognition, where phonelevel correspondences often provide a more robust basis for knowledge transfer than lexical content alone  [87] -  [89] . To generate these alignments, we use the Montreal Forced Aligner (MFA)  [90] , a widely-used tool that performs state-of-the-art alignment of phonetic units for speech given its corresponding transcript. MFA utilizes an acoustic mode implemented with Gaussian mixture models (GMM) and hidden Markov models (HMM). The GMM-HMM model utilizes a pronunciation dictionary to align sequences of phonemes with audio, resulting in precise timestamps for each individual phone. We used the English pretrained model and default settings provided by MFA.\n\nThe resulting alignments are released in TextGrid format.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "V. Organization And Sharing Of The Corpus",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "A. Partitions",
      "text": "The entire dataset is divided into multiple splits for training, development, and evaluation purposes. Table  VI  shows the distribution of primary emotions across splits. The class imbalance observed with each split is proportionally consistent with the class distribution across the whole dataset, except for test 2 and test 3, as explained later in this section. A key distinction of our database is the addition of three test sets, which have different characteristics. The test 1 set has approximately 17.2% of the corpus collected from 465 speakers (Table  VI ). Table  III  shows inter-evaluator agreements very similar to the values observed for the entire corpus.\n\nThe test 2 set was collected without the retrieval-based protocol presented in Section III-C. An early feedback we received was that machine learning models may bias the selection of speaking turns. We mitigate this issue by utilizing over 48 criteria based on different SER formulations, trained on different databases, features, and modalities, as explained in Section III-C. In response to this problem, we also created the test 2 set. We selected 117 podcasts for this set, annotating all the speaking turns that satisfy our requirements, except the emotion retrieval step (Figure  1 ). A consequence of this distinction is higher proportion of speaking turns labeled as neutral (around 45.8% -Table  VI ). This test set includes recordings from 112 known speakers. An observation from this set in Table  III  is the lower inter-evaluator agreement compared to other partitions since neutral speech tends to be more uncertain  [91] .\n\nThe test 3 set comprises 3,200 speaking turns, with a balanced representation based on primary categorical emotions (Table  VI ). These speaking turns come from 428 speakers. We are not releasing the emotional labels, transcriptions, or speaker information for this set, as it aims to provide an unbiased test set where different groups can evaluate their models and compare their results. Early versions of this test set were successfully used for SER challenges (Odyssey 2024  [60]  and Interspeech 2025  [84] ). We have developed a website-based interface for research groups to submit their results for classification of primary emotions and prediction of emotional attributes 2 . The website displays a leaderboard for each of these two SER formulations, which are automatically updated with the results of new submissions. Notice that the balance of emotional classes resulted in higher inter-evaluator agreements (Table  III ).\n\nThe development set has 12.9% of the corpus (Table  VI ), and its purpose is to allow research teams to optimize the performance of their SER models on this set during training, including hyperparameters. This practice avoids using the test set(s) during training. The set includes recordings from 704 speakers, which are not included in either the test sets or the training set. The training set includes recordings of the remaining 2,220 speakers and the speaking turns with unknown speakers. The partitions aim to be speaker-independent, although some unknown speakers in the training set may overlap with those in the development or test partitions. The test sets should never be used for training SER models, since there is speaker overlap between test sets (e.g., data from some speakers are included in both test 1 and test 2 sets).",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "B. Sharing Early Versions Of The Corpus",
      "text": "The effort to collect the MSP-Podcast corpus started in 2015. Instead of waiting for the full corpus to be ready, we have provided partial releases so the community can benefit from this resource. Figure  14(a)  shows the number of speaking turns completed over time. The vertical lines indicate the different releases of the corpus. After transitioning to perceptual evaluations with student workers, the size of the corpus began to grow more rapidly (from 2022 to 2024). For example, in 2024 the median number of fully annotated speaking turns per week was 1,588 (up from 403 in 2020, the last year we fully relied on crowdsourcing). Figure  14(b)  shows the number of annotations over time, indicating in blue the crowdsourcing worker annotations and in red the student worker annotations. The plot also shows an increased rate in the number of annotations from the time we fully transitioned to perceptual evaluation conducted by student workers. By the end of the project, 65.82% of the annotations were provided by our student workers.\n\nAt the time of writing this paper, we have signed data transfer agreements with 329 academic research groups worldwide: Africa (4), Asia (166), Australia  (8) , Europe  (93) , North America  (51) , and South America  (7) . The corpus is widely used today, playing a key role in advancing the area of speech emotion recognition.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vi. Baseline",
      "text": "This section presents SER results that can serve as a baseline for other researchers using this corpus. We use pre-trained SSL models built on WavLM  [92] , Wav2vec 2.0  [93] , or HuBERT  [94] . These models contain 24 transformer layers and are comprised of ∼310M parameters. We utilized the pre-trained off-the-shelf models from Hugging Face  [48] . As evidenced in previous studies  [55] ,  [57] ,  [60] ,  [95] ,  [96] , fine-tuning pre-trained SSL models for SER can lead to a significant performance boost. For categorical emotion recognition, we fine-tuned the models on eight emotion classes using focal loss, with a simple twolayer fully connected head. For attribute prediction, we adopted a staged fine-tuning strategy: first, adapting SSL models using concordance correlation coefficient (CCC) loss to predict valence, arousal, and dominance, and then jointly training with categorical classification using focal loss. After the fine-tuning stage, for attribute-based predictions, we employ single-task setup, where we train a separate regression model for each of the three emotion attributes, while keeping the SSL encoder frozen and updating only the head. We fine-tuned both models for 20 epochs, with a learning rate of 1e-5, a batch size of 32, and the Adam optimizer.\n\nTable  VII  summarizes baseline results for categorical emotion recognition and emotional attribute prediction. Overall, we observed consistent improvements across all test partitions compared to the previous MSP-Podcast v1.12 release, highlighting the benefit of expanding the training set and removing low-agreement labels. On the speech emotion recognition benchmark (SERB)  [84] , these",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Vii. Discussion",
      "text": "The MSP-Podcast corpus opens new research possibilities due to its unique features, including its diversity in speakers, emotions, and environments. Wagner et al.  [57]  and Naini et al.  [96]  demonstrated that finetuning SSL models such as WavLM with emotional data is beneficial for SER tasks. This corpus is sufficiently large to support effective finetuning, providing a stronger starting point for models tailored to a specific domain where less annotated data may be available. This database unlocks a range of novel opportunities. We focus here on highlighting a few notable ones.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "A. Perception Of Emotions",
      "text": "With 1,446,224 annotations from 13,278 workers, this corpus is well-suited for studying human emotion perception. We are releasing all individual annotations, along with the timestamps indicating when each annotation was completed. This information enables research that incorporates contextual factors into emotion perception. For instance, it allows investigation of the priming effect -how previously annotated sentences influence the perception of subsequent speaking turns  [97] ,  [98] . The sequential order of the annotations can also support preference learning strategies, where direct comparisons are used to establish relative labels (e.g., one speaking turn is more positive than another)  [99] .\n\nA related resource is the MSP-Conversation corpus  [18] , which includes time-continuous annotations of 10-20 minute segments from the same podcasts used in the MSP-Podcast corpus. These annotations provide continuous traces of perceived changes in valence, arousal, and dominance over time. There are 12,561 segments in the MSP-Podcast that overlap with the recordings in the MSP-Conversation corpus. This overlapping set offers an opportunity to study the relationship between continuoustime annotations (MSP-Conversation) and sentence-level annotations (MSP-Podcast)  [100] .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Robustness To Environments",
      "text": "The variety of podcasts used in this corpus provides a perfect resource for evaluating speech models that are robust to multiple environments. We highlight two prominent efforts in this area. Leem et al.  [61]  recorded an early version of the MSP-Podcast corpus by playing the speaking turns and radio noise in a single-walled sound booth (release 1.8). The microphone and the speaker were strategically placed at different locations to achieve target SNRs. This noisy version of the corpus has been extremely useful to explore robust SER models  [101] . The second effort is the work of Grageda et al.  [102] ,  [103] , which recorded a noisy version of the MSP-Podcast corpus in the context of human robot interaction (HRI) (test1 of release 1.9). The microphone was mounted on a robot, which moved, changing the relative distance between the noise source, the speech source, and the microphone. This effort has led to improvements in distant SER models  [104] .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "C. Emotions And Other Speech Tasks",
      "text": "The size of the corpus and the speaker information make this resource ideal for exploring how emotion affects other speech tasks, such as speaker verification and speaker recognition tasks  [77] -  [81] . To support these tasks, we made a key decision to collect multiple podcast episodes from the same speakers whenever possible. Speaker verification evaluations are often conducted across sessions collected on different days under different conditions. Different episodes are often collected on different days, which approaches this evaluation setting where several speakers appear in multiple podcasts. Likewise, many applications and experimental settings require sufficient recordings from individual speakers, which we ensured by including multiple episodes per speaker. For example, speaker verification tasks require an enrollment set to build the models. Also, text-to-speech (TTS) requires enough data to build a speaker model.  15  shows an accumulative plot with the number of speakers having a given amount of data. For example, there are 1,015 speakers with 300 seconds (5 minutes), and 141 speakers with 1,500 seconds (25 minutes) of data. These features make this corpus ideal for voice conversion (VC) and TTS tasks  [82] ,  [83] .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D. Rich Emotional Descriptors",
      "text": "Most emotional corpora provide either categorical or attribute-based annotations. In contrast, the MSP-Podcast offers both, along with secondary emotion labels, where annotators select all emotions they perceive in a recording. We have shown the value of secondary emotions by using them as auxiliary tasks in classification problems  [105] , and in retrieval tasks aimed at finding recordings with emotions similar to a reference (anchor) sample  [106] ,  [107] . As described in Section IV-A, the annotation protocol allows evaluators to provide their own labels for both primary and secondary emotions when none of the predefined options are appropriate. This information is also valuable, as demonstrated by Chou et al.  [108] , who transformed the free-text labels into polarity vectors (negative, positive, ambiguous) using LIWC  [109] . These examples showcase the potential of the rich emotional descriptors provided in the corpus.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "E. Support For Other Data Collections",
      "text": "The focus of this project is on speech recordings in English. There is a need to collect similar databases in other languages. We created the affective naturalistic database consortium (AndC) 3 . This initiative aims to provide all the tools used to collect the MSP-Podcast corpus to other researchers, enabling them to create new databases and expand the infrastructure for affective computing. We have partnered with collaborators from the National Tsing Hua University in Taiwan to test this initiative. They followed the code and protocol used for our corpus. The result of this effort is the BIIC-Podcast corpus  [15] , with recordings in Taiwanese Mandarin. Another example is the collection of the White House tapes speech emotion recognition (WHiSER) corpus  [32] . Using a variation of the proposed protocol, we annotated the emotions of ambient recordings from the Oval Office during the presidency of Richard Nixon. This set provides a perfect test set for SER models in challenging recording conditions (distant speech, low-quality microphones, noisy environment). We expect that this consortium will encourage the creation of new resources.\n\n3 http://andc.ai/ Another collaboration that started from this effort is the NaturalVoices corpus  [110] ,  [111] . This database uses the 6,007 recordings used in the MSP-Podcast corpus (5,046 hours). While MSP-Podcast was originally developed for SER, NaturalVoices is tailored for speech generation tasks, particularly voice conversion (VC)  [110]  and emotional voice conversion (EVC)  [111] . Its annotations and data processing pipeline are specifically designed to support these tasks, although the corpus is also suitable for other speech synthesis applications such as text-to-speech (TTS). The original podcast recordings are freely available 4 . The MSP-Conversation corpus  [18]  also beneficed from the collection of the MSP-Podcast corpus.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Viii. Conclusions",
      "text": "This paper presented the results of a 10-year effort to develop the MSP-Podcast corpus -a large, naturalistic emotional speech database containing diverse recordings from multiple speakers across various environments. The database reflects the emotions observed in daily human interactions. The corpus includes a rich set of emotional descriptors, enabling new research in emotion analysis, recognition, and synthesis. To ensure high-quality annotations, we implemented several strategies, including a screening test for student workers prior to hiring, weekly feedback, and targeted training to improve consistency in labeling. In addition to releasing the final version of the corpus, we also provide the code used in the protocol (Section VII-E), with the intention of supporting replication efforts that will expand affective computing resources in other languages.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Protocol for the data collection of the MSP-Podcast",
      "page": 4
    },
    {
      "caption": "Figure 1: shows a diagram of the data collection protocol.",
      "page": 4
    },
    {
      "caption": "Figure 2: Histogram showing the distribution of speaking turn du-",
      "page": 5
    },
    {
      "caption": "Figure 2: shows the",
      "page": 5
    },
    {
      "caption": "Figure 3: shows the distribution of the number",
      "page": 6
    },
    {
      "caption": "Figure 3: Histogram showing the number of files in the MSP-Podcast",
      "page": 7
    },
    {
      "caption": "Figure 4: An example of the weekly email report sent to student work-",
      "page": 7
    },
    {
      "caption": "Figure 5: Example of training interface for primary emotions and",
      "page": 7
    },
    {
      "caption": "Figure 4: shows an example of the document shared",
      "page": 7
    },
    {
      "caption": "Figure 4: ). The bars also include",
      "page": 7
    },
    {
      "caption": "Figure 4: , the student worker was very good at annotating",
      "page": 7
    },
    {
      "caption": "Figure 5: shows an example for primary emotions (Figure 5(a))",
      "page": 8
    },
    {
      "caption": "Figure 5: (b)). Not shown on the figures",
      "page": 8
    },
    {
      "caption": "Figure 6: ). When a file",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the questionnaire used for the perceptual evaluation",
      "page": 8
    },
    {
      "caption": "Figure 6: show the survey for annotating the MSP-Podcast audios.",
      "page": 8
    },
    {
      "caption": "Figure 7: (a) shows the number of speaking turns as-",
      "page": 8
    },
    {
      "caption": "Figure 8: (a) shows the word cloud of the labels",
      "page": 8
    },
    {
      "caption": "Figure 7: (b) shows the histogram",
      "page": 8
    },
    {
      "caption": "Figure 7: Histogram of the emotional classes selected by the workers",
      "page": 9
    },
    {
      "caption": "Figure 8: (b) shows the word cloud with the emotional",
      "page": 9
    },
    {
      "caption": "Figure 6: shows the questionnaire for these attributes.",
      "page": 9
    },
    {
      "caption": "Figure 8: Word cloud representing the manually typed emotions",
      "page": 9
    },
    {
      "caption": "Figure 9: illustrates the emotional attribute histograms",
      "page": 9
    },
    {
      "caption": "Figure 10: displays each speaking turn in the arousal-",
      "page": 9
    },
    {
      "caption": "Figure 9: Histogram distributions of valence, arousal, and dominance",
      "page": 10
    },
    {
      "caption": "Figure 10: Illustration of the emotional distribution of the MSP-",
      "page": 10
    },
    {
      "caption": "Figure 11: Annotation process for speaking information using Elan.",
      "page": 11
    },
    {
      "caption": "Figure 12: Interface of the verification website to correct the speaker",
      "page": 11
    },
    {
      "caption": "Figure 11: shows the Elan interface used for this annotation. For each",
      "page": 11
    },
    {
      "caption": "Figure 11: , an audio track contains two annotation tiers",
      "page": 11
    },
    {
      "caption": "Figure 12: For each individual",
      "page": 11
    },
    {
      "caption": "Figure 13: Histogram of number of words in the speaking turns",
      "page": 12
    },
    {
      "caption": "Figure 13: shows a histogram of",
      "page": 12
    },
    {
      "caption": "Figure 1: ). A consequence of this distinction",
      "page": 12
    },
    {
      "caption": "Figure 14: (a) shows the",
      "page": 13
    },
    {
      "caption": "Figure 14: (b) shows the",
      "page": 13
    },
    {
      "caption": "Figure 14: Development of the MSP-Podcast corpus over time. The",
      "page": 13
    },
    {
      "caption": "Figure 14: (b), the blue lines correspond to crowdsourcing evalu-",
      "page": 13
    },
    {
      "caption": "Figure 15: Cumulative distribution of speakers with increasing record-",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2.0\n1.12\n1.11": "1.10\n1.9\n1.7"
        },
        {
          "2.0\n1.12\n1.11": "1.5\n1.3\n1.1\n1.8\n1.6\n1.4\n1.2"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Test1\nWavLM\nWav2vec 2.0\nHuBERT",
          "Valence\nArousal\nDominance": "0.722\n0.724\n0.645\n0.692\n0.718\n0.639\n0.720\n0.708\n0.648"
        },
        {
          "Model": "Test2\nWavLM\nWav2vec 2.0\nHuBERT",
          "Valence\nArousal\nDominance": "0.549\n0.547\n0.467\n0.479\n0.553\n0.467\n0.541\n0.533\n0.465"
        },
        {
          "Model": "Test3\nWavLM\nWav2vec 2.0\nHuBERT",
          "Valence\nArousal\nDominance": "0.632\n0.632\n0.479\n0.625\n0.634\n0.476\n0.641\n0.630\n0.489"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "in Social emotions in nature and artifact: emotions in human and human-computer interaction",
      "authors": [
        "C Busso",
        "M Bulut",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "in Social emotions in nature and artifact: emotions in human and human-computer interaction"
    },
    {
      "citation_id": "2",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Emotional prosody speech and transcripts",
      "authors": [
        "M Liberman",
        "K Davis",
        "M Grossman",
        "N Martey",
        "J Bell"
      ],
      "year": "2002",
      "venue": "Linguistic Data Consortium"
    },
    {
      "citation_id": "4",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "9th European Conference on Speech Communication and Technology (Interspeech'2005 -Eurospeech)"
    },
    {
      "citation_id": "5",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "6",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Journal of Language Resources and Evaluation"
    },
    {
      "citation_id": "7",
      "title": "A new emotion database: considerations, sources and scope",
      "authors": [
        "E Douglas-Cowie",
        "R Cowie",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion"
    },
    {
      "citation_id": "8",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2nd International Workshop on Emotion Representation, Analysis and Synthesis in Continuous Time and Space"
    },
    {
      "citation_id": "10",
      "title": "NNIME: The NTHU-NTUA Chinese interactive multimodal emotion corpus",
      "authors": [
        "H.-C Chou",
        "W.-C Lin",
        "L.-C Chang",
        "C.-C Li",
        "H.-P Ma",
        "C.-C Lee"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "11",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "G Shen",
        "X Wang",
        "X Duan",
        "H Li",
        "W Zhu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, ser. MM '20",
      "doi": "10.1145/3394171.3413909"
    },
    {
      "citation_id": "12",
      "title": "The Vera am Mittag German audio-visual emotional speech database",
      "authors": [
        "M Grimm",
        "K Kroschel",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "IEEE International Conference on Multimedia and Expo (ICME 2008)"
    },
    {
      "citation_id": "13",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "MSP-face corpus: A natural audiovisual emotional database",
      "authors": [
        "A Vidal",
        "A Salman",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2020",
      "venue": "ACM International Conference on Multimodal Interaction (ICMI 2020)"
    },
    {
      "citation_id": "15",
      "title": "An intelligent infrastructure toward large scale naturalistic affective speech corpora collection",
      "authors": [
        "S Upadhyay",
        "W.-S Chien",
        "B.-H Su",
        "L Goncalves",
        "Y.-T Wu",
        "A Salman",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII 2023)"
    },
    {
      "citation_id": "16",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Int. J. Comput. Vision",
      "doi": "10.1007/s11263-019-01158-4"
    },
    {
      "citation_id": "17",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "The MSPconversation corpus",
      "authors": [
        "L Martinez-Lucas",
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2020",
      "venue": "Interspeech 2020"
    },
    {
      "citation_id": "19",
      "title": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora",
      "authors": [
        "S Mariooryad",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2014",
      "venue": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora"
    },
    {
      "citation_id": "20",
      "title": "Hybrid Dataset for Speech Emotion Recognition in Russian Language",
      "authors": [
        "V Kondratenko",
        "N Karpov",
        "A Sokolov",
        "N Savushkin",
        "O Kutuzov",
        "F Minkin"
      ],
      "year": "2023",
      "venue": "Hybrid Dataset for Speech Emotion Recognition in Russian Language"
    },
    {
      "citation_id": "21",
      "title": "Crowdsourcing emotional speech",
      "authors": [
        "J Smith",
        "A Tsiartas",
        "V Wagner",
        "E Shriberg",
        "N Bassiou"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling",
      "authors": [
        "Y Cheng",
        "R Zhang",
        "J Shi"
      ],
      "year": "2025",
      "venue": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling"
    },
    {
      "citation_id": "23",
      "title": "CMU-MOSEAS: A multimodal language dataset for",
      "authors": [
        "A Bagher Zadeh",
        "Y Cao",
        "S Hessner",
        "P Liang",
        "S Poria",
        "L.-P Morency",
        "Spanish",
        "German Portuguese"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "J Vanbriesen",
        "S Poria",
        "E Tong",
        "E Cambria",
        "M Chen",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "ACL 2004)"
    },
    {
      "citation_id": "25",
      "title": "THAI Speech Emotion Recognition (THAI-SER) corpus",
      "authors": [
        "J Chaksangchaichot",
        "S Sangnark",
        "P Prakrankamanant",
        "K Gangwanpongpun",
        "S Boonpunmongkol",
        "P Milindasuta",
        "D Na-Pombejra",
        "S Nutanong",
        "E Chuangsuwanich"
      ],
      "year": "2025",
      "venue": "THAI Speech Emotion Recognition (THAI-SER) corpus"
    },
    {
      "citation_id": "26",
      "title": "Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs",
      "authors": [
        "L Devillers",
        "L Vidrascu"
      ],
      "year": "2006",
      "venue": "Interspeech -International Conference on Spoken Language (ICSLP)"
    },
    {
      "citation_id": "27",
      "title": "Audiovisual recognition of spontaneous interest within conversations",
      "authors": [
        "B Schuller",
        "R Müeller",
        "B Höernler",
        "A Höethker",
        "H Konosu",
        "G Rigoll"
      ],
      "year": "2007",
      "venue": "9th international conference on Multimodal interfaces (ICMI 2007)"
    },
    {
      "citation_id": "28",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the FAU Aibo emotion corpus",
      "authors": [
        "A Batliner",
        "S Steidl",
        "E Nöth"
      ],
      "year": "2008",
      "venue": "Second International Workshop on Emotion: Corpora for Research on Emotion and Affect, International conference on Language Resources and Evaluation (LREC 2008)"
    },
    {
      "citation_id": "29",
      "title": "MEC 2017: Multimodal Emotion Recognition Challenge",
      "authors": [
        "Y Li",
        "J Tao",
        "B Schuller",
        "S Shan",
        "D Jiang",
        "J Jia"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "30",
      "title": "Demos: An italian emotional speech corpus: Elicitation methods, machine learning, and perception",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "31",
      "title": "Emozionalmente: A Crowdsourced Corpus of Simulated Emotional Speech in Italian",
      "authors": [
        "F Catania",
        "J Wilke",
        "F Garzotto"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "32",
      "title": "WHiSER: White House Tapes speech emotion recognition corpus",
      "authors": [
        "A Reddy Naini",
        "L Goncalves",
        "M Kohler",
        "D Robinson",
        "E Richerson",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Interspeech 2024"
    },
    {
      "citation_id": "33",
      "title": "The SEMAINE database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "G Mckeown",
        "M Valstar",
        "R Cowie",
        "M Pantic",
        "M Schröder"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Joint processing of audio-visual information for the recognition of emotional expressions in human-computer interaction",
      "authors": [
        "L Chen"
      ],
      "year": "2000",
      "venue": "Joint processing of audio-visual information for the recognition of emotional expressions in human-computer interaction"
    },
    {
      "citation_id": "35",
      "title": "UrduSER: A comprehensive dataset for speech emotion recognition in Urdu language",
      "authors": [
        "M Akhtar",
        "R Jahangir",
        "Q Ain",
        "M Nauman",
        "M Uddin",
        "S Ullah"
      ],
      "year": "2025",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "36",
      "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos"
    },
    {
      "citation_id": "37",
      "title": "Toronto emotional speech set (tess)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Scholars Portal Dataverse"
    },
    {
      "citation_id": "38",
      "title": "Challenges in reallife emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "39",
      "title": "Desperately seeking emotions or: actors, wizards and human beings",
      "authors": [
        "A Batliner",
        "K Fischer",
        "R Huber",
        "J Spilker",
        "E Nöth"
      ],
      "year": "2000",
      "venue": "ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion"
    },
    {
      "citation_id": "40",
      "title": "Recording audio-visual emotional databases from actors: a closer look",
      "authors": [
        "C Busso",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Second International Workshop on Emotion: Corpora for Research on Emotion and Affect, International conference on Language Resources and Evaluation (LREC 2008)"
    },
    {
      "citation_id": "41",
      "title": "CHEAVD: a Chinese natural emotional audio-visual database",
      "authors": [
        "Y Li",
        "J Tao",
        "L Chao",
        "W Bao",
        "Y Liu"
      ],
      "year": "2017",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "42",
      "title": "Domain-specific adaptation in speech emotion recognition using emotional distribution alignment",
      "authors": [
        "A Reddy Naini",
        "D Robinson",
        "E Richerson",
        "C Busso"
      ],
      "year": "2025",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2025)"
    },
    {
      "citation_id": "43",
      "title": "Increasing the reliability of crowdsourcing evaluations using online quality assessment",
      "authors": [
        "A Burmania",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms",
      "authors": [
        "J Lee",
        "J Park",
        "K Kim",
        "J Nam"
      ],
      "year": "2017",
      "venue": "Proc. 14th Int. Conf. Sound and Music Computing Conference"
    },
    {
      "citation_id": "45",
      "title": "Deep learning for minimum mean-square error approaches to speech enhancement",
      "authors": [
        "A Nicolson",
        "K Paliwal"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "46",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Python in Science Conference"
    },
    {
      "citation_id": "47",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "48",
      "title": "HuggingFace's transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "A Rush"
      ],
      "year": "2019",
      "venue": "HuggingFace's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771v5"
    },
    {
      "citation_id": "49",
      "title": "Python classes for Praat TextGrid and TextTier files (and HTK .mlf files)",
      "authors": [
        "K Gorman"
      ],
      "year": "2017",
      "venue": "Python classes for Praat TextGrid and TextTier files (and HTK .mlf files)"
    },
    {
      "citation_id": "50",
      "title": "Praat, a system for doing phonetics by computer",
      "authors": [
        "P Boersma"
      ],
      "year": "2001",
      "venue": "Glot International"
    },
    {
      "citation_id": "51",
      "title": "Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis",
      "authors": [
        "C Kim",
        "R Stern"
      ],
      "year": "2008",
      "venue": "Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis"
    },
    {
      "citation_id": "52",
      "title": "Powerset multi-class cross entropy loss for neural speaker diarization",
      "authors": [
        "A Plaquet",
        "H Bredin"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "53",
      "title": "pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe",
      "authors": [
        "H Bredin"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "54",
      "title": "An effective gender recognition approach using voice data via deeper lstm networks",
      "authors": [
        "F Ertam"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "55",
      "title": "Improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Improving speech emotion recognition using self-supervised learning with domain-specific audiovisual tasks"
    },
    {
      "citation_id": "56",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "57",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "58",
      "title": "Unsupervised domain adaptation for preference learning based speech emotion recognition",
      "authors": [
        "A Naini",
        "M Kohler",
        "C Busso"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "59",
      "title": "TweetEval: Unified benchmark and comparative evaluation for tweet classification",
      "authors": [
        "F Barbieri",
        "J Camacho-Collados",
        "L Anke"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "60",
      "title": "Odyssey 2024 -speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Naini",
        "L Moro-Velázquez",
        "T Thebaud",
        "P Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "61",
      "title": "Separation of emotional and reconstruction embeddings on ladder network to improve speech emotion recognition robustness in noisy conditions",
      "authors": [
        "S.-G Leem",
        "D Fulford",
        "J.-P Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2021",
      "venue": "Interspeech 2021"
    },
    {
      "citation_id": "62",
      "title": "Ladder networks for emotion recognition: Using unsupervised auxiliary tasks to improve predictions of emotional attributes",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Ladder networks for emotion recognition: Using unsupervised auxiliary tasks to improve predictions of emotional attributes"
    },
    {
      "citation_id": "63",
      "title": "Scripted dialogs versus improvisation: Lessons learned about emotional elicitation techniques from the IEMOCAP database",
      "authors": [
        "C Busso",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Interspeech 2008 -Eurospeech"
    },
    {
      "citation_id": "64",
      "title": "TSATC: Twitter Sentiment Analysis Training Corpus",
      "authors": [
        "I Naji"
      ],
      "year": "2012",
      "venue": "TSATC: Twitter Sentiment Analysis Training Corpus"
    },
    {
      "citation_id": "65",
      "title": "Curriculum learning for speech emotion recognition from crowdsourced labels",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "66",
      "title": "Exploiting cooccurrence frequency of emotions in perceptual evaluations to train a speech emotion classifier",
      "authors": [
        "H.-C Chou",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Interspeech 2022"
    },
    {
      "citation_id": "67",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2016",
      "venue": "International Joint Conference on Neural Networks (IJCNN 2016)"
    },
    {
      "citation_id": "68",
      "title": "Formulating emotion perception as a probabilistic model with application to categorical emotion classification",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "69",
      "title": "Generative approach using soft-labels to learn uncertainty in predicting emotional attributes",
      "authors": [
        "K Sridhar",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2021",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII 2021)"
    },
    {
      "citation_id": "70",
      "title": "Embracing ambiguity and subjectivity using the all-inclusive aggregation rule for evaluating multi-label speech emotion recognition systems",
      "authors": [
        "H.-C Chou",
        "H Wu",
        "L Goncalves",
        "S.-G Leem",
        "A Salman",
        "C Busso",
        "H.-Y Lee",
        "C.-C Lee"
      ],
      "year": "2024",
      "venue": "IEEE Spoken Language Technology Workshop (SLT 2024)"
    },
    {
      "citation_id": "71",
      "title": "Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule",
      "authors": [
        "H.-C Chou",
        "L Goncalves",
        "S.-G Leem",
        "A Salman",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "72",
      "title": "Over-sampling emotional speech data based on subjective evaluations provided by multiple individuals",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "73",
      "title": "Predicting emotionally salient regions using qualitative agreement of deep neural network regressors",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "74",
      "title": "Preference-learning with qualitative agreement for sentence level emotional annotations",
      "year": "2018",
      "venue": "Interspeech 2018, Hyderabad"
    },
    {
      "citation_id": "75",
      "title": "Forced-choice response format in the study of facial expression",
      "authors": [
        "J Russell"
      ],
      "year": "1993",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "76",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Bradley",
        "P Lang"
      ],
      "year": "1994",
      "venue": "Journal of Behavior Therapy and Experimental Psychiatry"
    },
    {
      "citation_id": "77",
      "title": "Predicting speaker recognition reliability by considering emotional content",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2017",
      "venue": "International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "78",
      "title": "X-Vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "79",
      "title": "A study of speaker verification performance with expressive speech",
      "authors": [
        "S Parthasarathy",
        "C Zhang",
        "J Hansen",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "80",
      "title": "Exploring the intersection between speaker verification and emotion recognition",
      "authors": [
        "M Bancroft",
        "R Lotfian",
        "J Hansen",
        "C Busso"
      ],
      "year": "2019",
      "venue": "International Workshop on Social & Emotion AI for Industry (SEAIxI)"
    },
    {
      "citation_id": "81",
      "title": "Revealing emotional clusters in speaker embeddings: A contrastive learning strategy for speech emotion recognition",
      "authors": [
        "I Ülgen",
        "Z Du",
        "C Busso",
        "B Sisman"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)"
    },
    {
      "citation_id": "82",
      "title": "Can emotion fool anti-spoofing?",
      "authors": [
        "A Mahapatra",
        "I Ülgen",
        "A Reddy Naini",
        "C Busso",
        "B Sisman"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "83",
      "title": "We need variations in speech synthesis: Sub-center modelling for speaker embeddings",
      "authors": [
        "I Ülgen",
        "C Busso",
        "J Hansen",
        "B Sisman"
      ],
      "year": "2024",
      "venue": "We need variations in speech synthesis: Sub-center modelling for speaker embeddings",
      "arxiv": "arXiv:2407.04291"
    },
    {
      "citation_id": "84",
      "title": "The Interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "A Reddy Naini",
        "L Goncalves",
        "A Salman",
        "P Mote",
        "I Ülgen",
        "T Thebaud",
        "L Moro-Velazquez",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "85",
      "title": "New standard for speech recognition and translation from the nvidia nemo canary model",
      "authors": [
        "K Puvvada",
        "P Żelasko",
        "H Huang",
        "O Hrinchuk",
        "N Koluguri",
        "S Majumdar",
        "E Rastorgueva",
        "K Dhawan",
        "Z Chen",
        "V Larukhin",
        "J Balam",
        "B Ginsburg"
      ],
      "year": "2024",
      "venue": "HuggingFace repository"
    },
    {
      "citation_id": "86",
      "title": "Open automatic speech recognition leaderboard",
      "authors": [
        "V Srivastav",
        "S Majumdar",
        "N Koluguri",
        "A Moumen",
        "S Gandhi",
        "H Team",
        "N Team",
        "S Team"
      ],
      "year": "2023",
      "venue": "Open automatic speech recognition leaderboard"
    },
    {
      "citation_id": "87",
      "title": "Phonetically-anchored domain adaptation for crosslingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "L Martinez-Lucas",
        "W Katz",
        "C Busso",
        "C.-C Lee"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "88",
      "title": "Phonetic anchor-based transfer learning to facilitate unsupervised cross-lingual speech emotion recognition",
      "authors": [
        "S Upadhyay"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)"
    },
    {
      "citation_id": "89",
      "title": "Analysis of phonetic level similarities across languages in emotional speech",
      "authors": [
        "P Mote",
        "A Reddy Naini",
        "D Robinson",
        "E Richerson",
        "C Busso"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "90",
      "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
      "authors": [
        "M Mcauliffe",
        "M Socolof",
        "S Mihuc",
        "M Wagner",
        "M Sonderegger"
      ],
      "year": "2017",
      "venue": "Montreal forced aligner: Trainable text-speech alignment using kaldi"
    },
    {
      "citation_id": "91",
      "title": "Modeling uncertainty in predicting emotional attributes from spontaneous speech",
      "authors": [
        "K Sridhar",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "92",
      "title": "WavLM: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "93",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in selfsupervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in selfsupervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "94",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "95",
      "title": "EMO-SUPERB: An indepth look at speech emotion recognition",
      "authors": [
        "H Wu",
        "H.-C Chou",
        "K.-W Chang",
        "L Goncalves",
        "J Du",
        "J.-S Jang",
        "C.-C Lee",
        "H.-Y Lee"
      ],
      "year": "2024",
      "venue": "EMO-SUPERB: An indepth look at speech emotion recognition",
      "arxiv": "arXiv:2402.13018"
    },
    {
      "citation_id": "96",
      "title": "Generalization of self-supervised learning-based representations for cross-domain speech emotion recognition",
      "authors": [
        "A Reddy Naini",
        "M Kohler",
        "E Richerson",
        "D Robinson",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)"
    },
    {
      "citation_id": "97",
      "title": "Analyzing the effect of affective priming on emotional annotations",
      "authors": [
        "L Martinez-Lucas",
        "A Salman",
        "S.-G Leem",
        "S Upadhyay",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2023",
      "venue": "International Conference on Affective Computing and Intelligent Interaction (ACII 2023)"
    },
    {
      "citation_id": "98",
      "title": "Affective priming in emotional annotations and its effect on speech emotion recognition",
      "authors": [
        "L Martinez-Lucas",
        "A Salman",
        "S.-G Leem",
        "W.-S Chien",
        "S Upadhyay",
        "C.-C Lee",
        "C Busso"
      ],
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "99",
      "title": "Preference learning labels by anchoring on consecutive annotations",
      "authors": [
        "A Reddy Naini",
        "A Salman",
        "C Busso"
      ],
      "year": "2023",
      "venue": "Interspeech 2023"
    },
    {
      "citation_id": "100",
      "title": "Analyzing continuous-time and sentence-level annotations for speech emotion recognition",
      "authors": [
        "L Martinez-Lucas",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "101",
      "title": "Not all features are equal: Selection of robust features for speech emotion recognition in noisy environments",
      "authors": [
        "S.-G Leem",
        "D Fulford",
        "J.-P Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)"
    },
    {
      "citation_id": "102",
      "title": "Distant speech emotion recognition in an indoor human-robot interaction scenario",
      "authors": [
        "N Grágeda",
        "C Busso",
        "E Alvarado",
        "R Mahu",
        "N Becerra Yoma"
      ],
      "year": "2023",
      "venue": "Interspeech 2023"
    },
    {
      "citation_id": "103",
      "title": "Speech emotion recognition in real static and dynamic human-robot interaction scenarios",
      "authors": [
        "N Grágeda",
        "C Busso",
        "E Alvarado",
        "R García",
        "R Mahu",
        "N Becerra Yoma"
      ],
      "year": "2025",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "104",
      "title": "Speech emotion recognition with deep learning beamforming on a distant human-robot interaction scenario",
      "authors": [
        "R Garcia",
        "R Mahu",
        "N Grágeda",
        "A Luzanto",
        "N Bohmer",
        "C Busso",
        "N Becerra Yoma"
      ],
      "year": "2024",
      "venue": "Interspeech 2024"
    },
    {
      "citation_id": "105",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Interspeech 2018"
    },
    {
      "citation_id": "106",
      "title": "Quantifying emotional similarity in speech",
      "authors": [
        "J Harvill",
        "S.-G Leem",
        "M Abdelwahab",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "107",
      "title": "Retrieving speech samples with similar emotional content using a triplet loss function",
      "authors": [
        "J Harvill",
        "M Abdelwahab",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "108",
      "title": "Exploiting annotators' typed description of emotion perception to maximize utilization of ratings for speech emotion recognition",
      "authors": [
        "H.-C Chou",
        "W.-C Lin",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)"
    },
    {
      "citation_id": "109",
      "title": "Linguistic inquiry and word count: LIWC2015",
      "authors": [
        "J Pennebaker",
        "R Booth",
        "R Boyd",
        "M Francis"
      ],
      "year": "2015",
      "venue": "Pennebaker Conglomerates"
    },
    {
      "citation_id": "110",
      "title": "Towards naturalistic voice conversion: Naturalvoices dataset with an automatic processing pipeline",
      "authors": [
        "A Salman",
        "Z Du",
        "S Chandra",
        "I Ülgen",
        "C Busso",
        "B Sisman"
      ],
      "year": "2024",
      "venue": "Interspeech 2024"
    },
    {
      "citation_id": "111",
      "title": "Naturalvoices: A large-scale podcast dataset for emotional and real-world voice conversion",
      "authors": [
        "Z Du",
        "S Chandra",
        "A Salman",
        "I Ulgen",
        "A Mahapatra",
        "C Busso",
        "B Sisman"
      ],
      "year": "2025",
      "venue": "ArXiv e-prints (arXiv:***)"
    }
  ]
}