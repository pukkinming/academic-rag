{
  "paper_id": "2310.07648v1",
  "title": "Hypercomplex Multimodal Emotion Recognition From Eeg And Peripheral Physiological Signals",
  "published": "2023-10-11T16:45:44Z",
  "authors": [
    "Eleonora Lopez",
    "Eleonora Chiarantano",
    "Eleonora Grassucci",
    "Danilo Comminiello"
  ],
  "keywords": [
    "Hypercomplex Neural Networks",
    "Hypercomplex Algebra",
    "EEG",
    "Multimodal Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition from physiological signals is receiving an increasing amount of attention due to the impossibility to control them at will unlike behavioral reactions, thus providing more reliable information. Existing deep learning-based methods still rely on extracted handcrafted features, not taking full advantage of the learning ability of neural networks, and often adopt a single-modality approach, while human emotions are inherently expressed in a multimodal way. In this paper, we propose a hypercomplex multimodal network equipped with a novel fusion module comprising parameterized hypercomplex multiplications. Indeed, by operating in a hypercomplex domain the operations follow algebraic rules which allow to model latent relations among learned feature dimensions for a more effective fusion step. We perform classification of valence and arousal from electroencephalogram (EEG) and peripheral physiological signals, employing the publicly available database MAHNOB-HCI surpassing a multimodal state-ofthe-art network. The code of our work is freely available at https://github.com/ispamm/MHyEEG.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion is an essential part of human communication that plays a vital role in the overall quality and outcome of interactions. Thus, automatic emotion recognition and affective computing have gained much interest, also considering the wide range of applications in human-computer interaction (HCI)  [1] . Humans manifest emotions in a multimodal way, including facial expressions, speech, body language, and physiological signals. While behavioral reactions can be easily controlled, for example, real emotion can be concealed by adjusting expressions or tone of voice, physiological signals cannot be governed at will, thus being more reliable for recognizing human emotion  [2] . Therefore, on account of the development of non-invasive and inexpensive wearable devices, physiological-based emotion recognition has become a hot topic in affective computing research. Among these, electroencephalography (EEG) is a measure of the electrical activity of the brain that is directly correlated with the cognitive process and can provide key information regarding emotional states being characterized by excellent temporal resolution  [3] . Therefore, EEG-based analysis has received an increasing amount of attention for a variety of applications such as epileptic seizure detection  [4] , general EEG classification  [5]  and emotion recognition  [6] .\n\nNevertheless, most studies do not take full advantage of the learning ability of deep learning models and most of the time focus on a single-modality approach. In fact, the input to the neural model is often extracted features instead of the raw data and corresponding to a single modality, generally EEG, when in reality human emotions are intrinsically multimodal, with different modalities describing different aspects of an emotional reaction and correlations among them providing critical information if exploited correctly  [2] . Recent works have started to take a multimodal approach, but most rely on trivial techniques and few studies explore more emerging paradigms such as multimodal learning  [7] . Therefore, effectively learning from multiple physiological signals to produce more powerful feature representations is still an open problem. Motivated by the described challenges, in this paper we address the more difficult approach of learning directly from raw signals and propose a multimodal architecture with a novel fusion module that exploits the properties of algebras in the hypercomplex domain to truly take advantage of correlations characteristic of EEG and peripheral physiological signals.\n\nParameterized hypercomplex neural networks (PHNNs) are an emerging family of models which operate in a hypercomplex number domain  [8, 9] . They have been introduced in order to generalize the more common quaternion neural networks (QNNs) which are defined in the quaternion domain and are thus limited to 4D input data but possess very powerful capabilities  [10] . In fact, thanks to quaternion algebra operations, such as the Hamilton product, these models are endowed with the ability to capture not only global relations as any neural network but also local relations among input data, unlike real-valued counterparts, as well as being more lightweight  [11] . Thanks to the introduction of parameterized hypercomplex multiplication (PHM) and convolution (PHC), these advantages have been extended to inputs of any dimensionality n, with a reduction of parameters of 1/n.\n\nOwing to these advantages, we design a hypercomplex multimodal network with a novel fusion module defined in the hypercomplex domain, thus comprising PHM layers that thanks to hypercomplex algebra properties endow the architecture with the ability to model correlations among the learned latent features, thus learning a more effective fused representation. Specifically, we perform classification of valence and arousal from EEG, electrocardiogram (ECG), galvanic skin response (GSR), and eye data, and we validate the proposed approach on a publicly available benchmark, MAHNOB-HCI  [12] , showing how our method outperforms a multimodal state-of-the-art network.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "A plethora of machine learning approaches for emotion recognition have been proposed  [13, 14, 15] . However, employing such methods requires extensive domain knowledge to extract relevant features. On the other hand, deep learning models are able to learn features directly from the raw data and thus learn a powerful latent representation. Due to these advantages, many deep learning-based methods have been investigated  [16, 17, 18, 19] . Nonetheless, even though such works employ neural networks, they still rely on extracted features, such as power spectral density (PSD) and differential entropy (DE), instead of taking full advantage of the representational learning ability of neural models. Rather, a study that employs raw EEG signals has proposed a 3D representation of the data to be processed by a 3D convolutional neural network (CNN)  [20] . However, all aforementioned methods focus on a single-modality approach which is suboptimal  [2] . Thus in order to exploit the information contained in different modalities, recent studies adopt a multimodal approach for emotion recognition, some still relying on extracted features  [2, 21, 22]  and very few that directly employ raw data  [23, 24, 25] , where the latter focuses on perceived mental workload classification instead of emotion recognition.\n\nAside from feature extraction, the crucial step of multimodal learning is the fusion strategy. Surely, much of the research in this field has focused on this aspect, as there are a multitude of manners to incorporate information from different modalities. Starting from the most trivial, i.e. early fusion in which data from different modalities is concatenated to form a single input to the model, and late fusion which consists in aggregating decisions of different networks trained separately on each modality to obtain a final output. Both strategies suffer from several problems, where the first does not take into account the different nature of the input modalities, not taking advantage of complementary information and not allowing to identify relations among them, while the second does not exploit cross-modal information during learning at all, also requiring to optimize a different network for each modality. Instead, more complex strategies fall under the name of intermediate fusion, which consists in first learning modality-specific latent representations that are subsequently fused together for further processing  [7] . Thus, in this paper, we investigate and propose a novel technique that allows to effectively grasp correlations between the different modalities during learning thanks to its definition in a hypercomplex algebraic system.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Hypercomplex Neural Models",
      "text": "Hypercomplex neural networks are neural models defined in a hypercomplex number system H which is regulated by the respective algebra rules that define addition and multiplication operations. A generic hypercomplex number is defined as\n\nwhere h 0 , . . . , h n ∈ R and îi , . . . , în ∈ H are the imaginary units.\n\nThe general hypercomplex domain H includes various algebraic systems such as the complex C domain when n = 2 and the quaternion Q domain when n = 4, where quaternion neural networks (QNNs) operate in. In fact, algebra rules are defined only at predefined dimensions of n = 2 m , with m ∈ N, owing to the fact that hypercomplex algebras are included in the family of Cayley-Dickson algebras. Thus, each of these number systems is identified by the number of imaginary units and consequently by the different definitions of the multiplication operation as a result of the disparate interactions among imaginary units. For example, in the quaternion domain, the product is non-commutative, with î1 î2 ̸ = î2 î1 . Therefore, in the latter domain, the Hamilton product was introduced, which also regulates the matrix multiplication in fully connected layers and the convolution operation in convolutional layers, since both the weight matrix and the input are encapsulated into a quaternion in the following way:\n\n, respectively. As a consequence, the matrix multiplication of a general fully connected layer becomes\n\nFrom eq. (  2 ) it can be seen that the filter submatrices are shared among input dimensions, thus not only reducing the number of free parameters by 1/4, resulting in a more lightweight model, but additionally endowing the neural network with the ability to grasp latent relations among channel dimensions. Nonetheless, QNNs are limited to 4D inputs, therefore parameterized hypercomplex multiplication (PHM)  [8]  and convolution (PHC)  [9]  have been introduced to bridge this gap. The core idea of these methods lies in expressing the weight matrix as a sum of n ∈ N Kronecker products, thus we have\n\nwhereby matrices A i encode the algebra rules, directly learned from the data, and F i represent the filters. As a result of eq. (  3 ), a parameterization of W is obtained, meaning that n is a user-defined hyperparameter that decides in which domain the neural model operates (e.g., n = 4 for the quaternion domain), thus extending the aforementioned properties of QNNs to general input domains nD. Specifically, PHM and PHC layers employ 1/n free parameters with respect to real-valued counterparts and still possess the ability to model correlations present in the data, unlike real-valued networks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Hypercomplex Fusion Network",
      "text": "To address the challenges presented in Section 2 we propose HyperFuseNet, a multimodal architecture that exploits hypercomplex algebra properties to effectively fuse the learned latent representations as can be seen in Fig.  1 . The neural model comprises two main components, that is the encoder and a hypercomplex fusion module. Concretely, the encoder is composed of four different branches in the real domain, one for each modality, and has the objective of learning modalityspecific latent representations directly from the raw signals, thus with a modality-level focus. Thereafter, these features in the latent space are merged together and processed by the proposed hypercomplex fusion module. The latter is composed of PHM layers with the hyperparameter n set to 4, as there are four feature vectors in input to the module corresponding to the four modalities, and has the role of learning a fused representation, thus performing a patient-level analysis. More in detail, by defining multiplications in the hypercomplex domain, the proposed fusion module possesses the capability of grasping cross-modal interactions between the learned latent features of the EEG, ECG, GSR, and eye data signals, which are highly correlated. Thus, the hypercomplex fusion module captures both global and local relations between feature dimensions, unlike real-valued networks, accordingly learning a more powerful representation by truly exploiting the correlations present in the different physiological signals.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "To validate the proposed approach we adopt a publicly available dataset, that is MAHNOB-HCI  [12] . It is a multimodal dataset for affect recognition which includes synchronized recordings of face video, audio signal, eye gaze data, and peripheral/central nervous system physiological signals of 27 participants while watching emotional video clips. The eye gaze data comprises pupil dimensions, gaze coordinates, and eye distances, while for the physiological signals, we focus on EEG, ECG, and GSR, as these are highly related to emotional changes  [12] . The database provides labels related to arousal, i.e., calm, medium aroused, and excited, and valence, i.e., unpleasant, neutral valence, and pleasant.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing And Data Augmentation",
      "text": "Firstly, we downsample EEG, ECG, and GSR signals from 256Hz to 128Hz, while we keep eye data at 60Hz. Then, we filter EEG and ECG signals with a band-pass filter at 1-45Hz and 0.5-45Hz, respectively  [14, 23] , while a low-pass filter at 60Hz is applied to GSR signals  [26] , and for all of them an additional notch filter at 50Hz  [16] , with all EEG signals being firstly referenced to average. Additionally, we perform a baseline correction on the GSR signal with respect to the mean value within the 200ms preceding each trial to eliminate the initial offset of the signal. Finally, as for EEG data, we select 10 channels out of the original 32, i.e., F3, F4, F7, F8, FC5, FC6, T7, T8, P7, and P8, as these are the most related to emotion  [27, 28] . Instead, regarding eye data, we take the average between the signals related to the two eyes and we keep -1 values as they correspond to blinks or rapid movements which are relevant to the task at hand. We extract samples by dividing the last 30s of each trial into three segments of 10s, as measurements toward the end of the clips reflect the emotion of the subjects' rating  [23] . Finally, we split the dataset in a stratified fashion by taking 20% of the data for testing. Training samples are then augmented by applying scaling and noise addition. Firstly, two scaling factors are uniformly sampled over two intervals, i.e., [0.7, 0.8] and [1.2, 1.3], and applied to the original sample to generate two augmented versions. Then, a Gaussian noise signal with zero mean is added to each sample, with its standard deviation being computed modality-wise such that the augmented signal has a signal-to-noise ratio (SNR) of 5dB. A total of 30 augmented signals are generated for each original sample.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Architecture And Training Recipe",
      "text": "The proposed architecture comprises four branches that compose the encoder and a hypercomplex fusion module. The branches consist of three fully-connected layers, except for the GSR branch which has two, with 128 units for eye data and GSR, 512 for ECG, and 1024 for EEG, interleaved with batch normalization and ReLU activation function, inspired by  [25] . Then, the learned latent representations are merged together and processed by the proposed fusion module which comprises four PHM layers with n = 4, with the same interleaved layers and the number of units halved at each layer, a dropout layer, and the final output layer. The model is trained using the Adam optimizer, with a categorical cross-entropy loss and a one-cycle policy. The best hyperparameters are found by doing a bayesian search, sampling the learning rate from [0.001, 0.008]. The number of epochs is set to 100 with early stopping with patience at 20.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "We report in Tab. 1 the results of the conducted experimental analysis, showing the mean over 3 runs of the F1-score and accuracy, which indeed is not always representative due to imbalance of classes. In detail, we compare the proposed architecture against a state-of-the-art multimodal network that also operates with raw signals and is originally designed for mental workload classification. We train it using the same approach we employed for our network on the same database. Firstly, we can observe that the employed data augmentation is effective and improves the performance of both networks. Secondly, and most importantly, the proposed hypercomplex architecture outperforms the method employed as comparison in both augmentation scenarios, thus demonstrating the efficacy of the PHM layers in the fusion step which yield better emotion recognition accuracy as a result of the grasped crossmodal correlations thanks to hypercomplex algebra rules.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed a multimodal architecture with a novel hypercomplex fusion module for emotion recognition from EEG and peripheral physiological signals, in which a modality-specific representation is firstly learned in the real domain and consequently processed together by the fusion module in the hypercomplex domain. The latter was found to be effective to perform a more proper fusion step than classical real-valued fully-connected layers, in fact, by employing hypercomplex multiplications the module is capable of capturing relations among the learned latent features and as a result learn a more discriminant representation. In future efforts, we aim at additionally exploiting intra-modality correlations with parameterized hypercomplex convolutions, thus bringing the advantages of the fusion step also at the encoder level.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: HyperFuseNet architecture. The encoder learns modality-specific latent representations in the real domain, which are",
      "page": 3
    },
    {
      "caption": "Figure 1: The neural model",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "activity of the brain that is directly correlated with the cogni-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "tive process and can provide key information regarding emo-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "tional states being characterized by excellent\ntemporal reso-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "lution [3]. Therefore, EEG-based analysis has received an in-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "creasing amount of attention for a variety of applications such"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "as epileptic seizure detection [4], general EEG classification"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "[5] and emotion recognition [6]."
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "Nevertheless, most studies do not\ntake full advantage of"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "the learning ability of deep learning models and most of the"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "time focus on a single-modality approach.\nIn fact,\nthe in-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "put\nto the neural model\nis often extracted features instead of"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "the raw data and corresponding to a single modality, gener-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "ally EEG, when in reality human emotions are intrinsically"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "multimodal, with different modalities describing different as-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "pects of an emotional reaction and correlations among them"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "providing critical\ninformation if exploited correctly [2]. Re-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "cent works have started to take a multimodal approach, but"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "most rely on trivial\ntechniques and few studies explore more"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "emerging paradigms such as multimodal learning [7]. There-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "fore, effectively learning from multiple physiological signals"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "to produce more powerful\nfeature representations is still an"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "open problem. Motivated by the described challenges, in this"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "paper we address the more difficult approach of learning di-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "rectly from raw signals and propose a multimodal architecture"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "with a novel fusion module that exploits the properties of al-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "gebras in the hypercomplex domain to truly take advantage of"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": ""
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "correlations characteristic of EEG and peripheral physiologi-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "cal signals."
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "Parameterized hypercomplex neural networks\n(PHNNs)"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "are an emerging family of models which operate in a hyper-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "complex number domain [8, 9]. They have been introduced in"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "order to generalize the more common quaternion neural net-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "works (QNNs) which are defined in the quaternion domain"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "and are thus limited to 4D input data but possess very pow-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "erful capabilities [10].\nIn fact,\nthanks to quaternion algebra"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "operations, such as the Hamilton product,\nthese models are"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "endowed with the ability to capture not only global relations"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "as any neural network but also local\nrelations among input"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "data, unlike real-valued counterparts, as well as being more"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "lightweight [11]. Thanks to the introduction of parameterized"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "hypercomplex multiplication (PHM) and convolution (PHC),"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "these advantages have been extended to inputs of any dimen-"
        },
        {
          "Dept. of Information Eng., Electronics and Telecom., Sapienza University of Rome, Italy": "sionality n, with a reduction of parameters of 1/n."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Owing to these advantages, we design a hypercomplex": "multimodal network with a novel\nfusion module defined in",
          "name of intermediate fusion, which consists in first\nlearning": "modality-specific latent representations that are subsequently"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "the hypercomplex domain,\nthus comprising PHM layers that",
          "name of intermediate fusion, which consists in first\nlearning": "fused together for further processing [7]. Thus, in this paper,"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "thanks\nto hypercomplex algebra properties\nendow the\nar-",
          "name of intermediate fusion, which consists in first\nlearning": "we investigate and propose a novel\ntechnique that allows to"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "chitecture with the ability to model correlations among the",
          "name of intermediate fusion, which consists in first\nlearning": "effectively grasp correlations between the different modali-"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "learned latent\nfeatures,\nthus learning a more effective fused",
          "name of intermediate fusion, which consists in first\nlearning": "ties during learning thanks to its definition in a hypercomplex"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "representation.\nSpecifically, we\nperform classification\nof",
          "name of intermediate fusion, which consists in first\nlearning": "algebraic system."
        },
        {
          "Owing to these advantages, we design a hypercomplex": "valence\nand arousal\nfrom EEG,\nelectrocardiogram (ECG),",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "galvanic skin response (GSR), and eye data, and we validate",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "3. METHODOLOGY"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "the proposed approach on a publicly available benchmark,",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "MAHNOB-HCI [12], showing how our method outperforms",
          "name of intermediate fusion, which consists in first\nlearning": "3.1. Hypercomplex neural models"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "a multimodal state-of-the-art network.",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "Hypercomplex neural networks are neural models defined in"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "a hypercomplex number system H which is regulated by the"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "2. BACKGROUND",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "respective algebra rules that define addition and multiplica-"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "tion operations. A generic hypercomplex number is defined"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "A plethora\nof machine\nlearning\napproaches\nfor\nemotion",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "as"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "recognition have been proposed [13, 14, 15]. However, em-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "ploying such methods requires extensive domain knowledge",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "i = 1, . . . , n\n(1)\nh = h0 + hiˆıi + . . . + hnˆın,"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "to extract relevant features. On the other hand, deep learning",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "models are able to learn features directly from the raw data",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "where h0, . . . , hn ∈ R and ˆıi, . . . , ˆın ∈ H are the imaginary"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "and thus learn a powerful\nlatent representation. Due to these",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "units."
        },
        {
          "Owing to these advantages, we design a hypercomplex": "advantages, many deep learning-based methods have been",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "The general hypercomplex domain H includes various al-"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "investigated [16, 17, 18, 19]. Nonetheless, even though such",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "gebraic systems such as the complex C domain when n = 2"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "works employ neural networks,\nthey still\nrely on extracted",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "and the quaternion Q domain when n = 4, where quaternion"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "features,\nsuch as power\nspectral density (PSD) and differ-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "neural networks\n(QNNs) operate in.\nIn fact,\nalgebra rules"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "ential entropy (DE),\ninstead of\ntaking full advantage of\nthe",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "are defined only at predefined dimensions of n = 2m, with"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "representational\nlearning ability of neural models.\nRather,",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "m ∈ N, owing to the fact\nthat hypercomplex algebras are"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "a study that employs\nraw EEG signals has proposed a 3D",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "included in the family of Cayley-Dickson algebras.\nThus,"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "representation of the data to be processed by a 3D convolu-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "each of\nthese number\nsystems\nis\nidentified by the number"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "tional neural network (CNN)\n[20]. However, all aforemen-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "of\nimaginary units and consequently by the different defini-"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "tioned methods focus on a single-modality approach which",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "tions of\nthe multiplication operation as a result of\nthe dis-"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "is suboptimal\n[2].\nThus in order\nto exploit\nthe information",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "parate interactions among imaginary units.\nFor example,\nin"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "contained in different modalities, recent studies adopt a mul-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "the quaternion domain, the product is non-commutative, with"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "timodal approach for emotion recognition, some still relying",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "Therefore,\nin the latter domain,\nthe Hamil-\nı1ˆı2\n̸= ˆı2ˆı1."
        },
        {
          "Owing to these advantages, we design a hypercomplex": "on extracted features [2, 21, 22] and very few that directly",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "ton product was introduced, which also regulates the matrix"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "employ raw data [23, 24, 25], where the latter\nfocuses on",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "multiplication in fully connected layers and the convolution"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "perceived mental workload classification instead of emotion",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "operation in convolutional\nlayers, since both the weight ma-"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "recognition.",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "trix and the input are encapsulated into a quaternion in the"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "Aside from feature extraction,\nthe crucial step of multi-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "following way: W = W0 + W1ˆı1 + W2ˆı2 + W3ˆı3 and"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "modal\nlearning is\nthe fusion strategy.\nSurely, much of\nthe",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "respectively. As a conse-\nx = x0 + x1ˆı1 + x2ˆı2 + x3ˆı3,"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "research in this field has focused on this aspect, as there are",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "quence, the matrix multiplication of a general fully connected"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "a multitude of manners to incorporate information from dif-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        },
        {
          "Owing to these advantages, we design a hypercomplex": "",
          "name of intermediate fusion, which consists in first\nlearning": "layer becomes"
        },
        {
          "Owing to these advantages, we design a hypercomplex": "ferent modalities. Starting from the most trivial, i.e. early fu-",
          "name of intermediate fusion, which consists in first\nlearning": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modal": "",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "research in this field has focused on this aspect, as there are",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "a multitude of manners to incorporate information from dif-",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "ferent modalities. Starting from the most trivial, i.e. early fu-",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "sion in which data from different modalities is concatenated",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "to form a single input",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "to the model, and late fusion which",
          "the": ""
        },
        {
          "modal": "",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "consists in aggregating decisions of different networks trained",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "separately on each modality to obtain a final output.",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": "Both"
        },
        {
          "modal": "",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "strategies suffer from several problems, where the first does",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "not take into account the different nature of the input modali-",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        },
        {
          "modal": "ties, not taking advantage of complementary information and",
          "learning is": "",
          "the fusion strategy.": "",
          "Surely, much of": "",
          "the": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PHM\nBatch": "Dense\nDropout"
        },
        {
          "PHM\nBatch": "Norm"
        },
        {
          "PHM\nBatch": "Fig. 1. HyperFuseNet architecture. The encoder learns modality-specific latent representations in the real domain, which are"
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": "of PHM layers with the hyperparameter n set\nto 4, as there"
        },
        {
          "PHM\nBatch": "are four\nfeature vectors in input\nto the module correspond-"
        },
        {
          "PHM\nBatch": "ing to the four modalities, and has the role of learning a fused"
        },
        {
          "PHM\nBatch": "representation, thus performing a patient-level analysis. More"
        },
        {
          "PHM\nBatch": "in detail, by defining multiplications in the hypercomplex do-"
        },
        {
          "PHM\nBatch": "main, the proposed fusion module possesses the capability of"
        },
        {
          "PHM\nBatch": "grasping cross-modal\ninteractions between the learned latent"
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": "features of the EEG, ECG, GSR, and eye data signals, which"
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": "are highly correlated. Thus, the hypercomplex fusion module"
        },
        {
          "PHM\nBatch": "captures both global and local\nrelations between feature di-"
        },
        {
          "PHM\nBatch": "mensions, unlike real-valued networks, accordingly learning"
        },
        {
          "PHM\nBatch": "a more powerful representation by truly exploiting the corre-"
        },
        {
          "PHM\nBatch": "lations present in the different physiological signals."
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": "4. EXPERIMENTAL RESULTS"
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": "4.1. Dataset"
        },
        {
          "PHM\nBatch": ""
        },
        {
          "PHM\nBatch": "To validate the proposed approach we adopt a publicly avail-"
        },
        {
          "PHM\nBatch": "able dataset,\nthat\nis MAHNOB-HCI [12].\nIt\nis a multimodal"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": ""
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": "Model"
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": ""
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": "Dolmans [25]"
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": ""
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": "HyperFuseNet (ours)"
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": "Dolmans [25]"
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": ""
        },
        {
          "Table 1. Results on MAHNOB-HCI of the proposed method compared against a state-of-the-art model with and without data": "HyperFuseNet (ours)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dolmans [25]\n38.86 ± 1.11": "✓",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "HyperFuseNet (ours)\n39.65 ± 1.75",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "41.56 ± 1.33\n43.60 ± 2.22\n44.30 ± 2.01"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "we filter EEG and ECG signals with a band-pass filter at 1-",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "using the Adam optimizer, with a categorical cross-entropy"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "45Hz and 0.5-45Hz,\nrespectively [14, 23], while a low-pass",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "loss and a one-cycle policy.\nThe best hyperparameters are"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "filter at 60Hz is applied to GSR signals [26], and for all of",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "found by doing a bayesian search, sampling the learning rate"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "them an additional notch filter at 50Hz [16], with all EEG",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "from [0.001, 0.008]. The number of epochs is set to 100 with"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "signals being firstly referenced to average. Additionally, we",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "early stopping with patience at 20."
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "perform a baseline correction on the GSR signal with respect",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "to the mean value within the 200ms preceding each trial\nto",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "4.4. Results"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "eliminate the initial offset of the signal. Finally, as for EEG",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "data, we select 10 channels out of the original 32, i.e., F3, F4,",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "We report in Tab. 1 the results of the conducted experimental"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "F7, F8, FC5, FC6, T7, T8, P7, and P8, as these are the most",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "analysis, showing the mean over 3 runs of the F1-score and"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "related to emotion [27, 28].\nInstead,\nregarding eye data, we",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "accuracy, which indeed is not always representative due to"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "take the average between the signals related to the two eyes",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "imbalance of classes.\nIn detail, we compare the proposed ar-"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "and we keep −1 values as they correspond to blinks or rapid",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "chitecture against a state-of-the-art multimodal network that"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "movements which are relevant to the task at hand.",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "also operates with raw signals and is originally designed for"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "We extract samples by dividing the last 30s of each trial",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "mental workload classification. We train it using the same ap-"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "into three segments of 10s, as measurements toward the end",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "proach we employed for our network on the same database."
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "of\nthe clips reflect\nthe emotion of\nthe subjects’\nrating [23].",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "Firstly, we can observe that\nthe employed data augmentation"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "Finally, we split\nthe dataset\nin a stratified fashion by taking",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "is effective and improves the performance of both networks."
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "20% of the data for testing. Training samples are then aug-",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "Secondly, and most\nimportantly,\nthe proposed hypercomplex"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "mented by applying scaling and noise addition. Firstly,\ntwo",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "architecture outperforms the method employed as comparison"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "scaling factors are uniformly sampled over two intervals, i.e.,",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "in both augmentation scenarios,\nthus demonstrating the effi-"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "[0.7, 0.8] and [1.2, 1.3], and applied to the original sample to",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "cacy of the PHM layers in the fusion step which yield better"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "generate two augmented versions.\nThen, a Gaussian noise",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "emotion recognition accuracy as a result of the grasped cross-"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "signal with zero mean is added to each sample, with its stan-",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "modal correlations thanks to hypercomplex algebra rules."
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "dard deviation being computed modality-wise such that\nthe",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "augmented signal has a signal-to-noise ratio (SNR) of 5dB. A",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "5. CONCLUSION"
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "total of 30 augmented signals are generated for each original",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "sample.",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [25]\n38.86 ± 1.11": "",
          "40.90 ± 0.62\n38.33 ± 1.24\n40.24 ± 1.04": "In this paper, we proposed a multimodal architecture with a"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "method for\nselecting effective models and feature groups\nin"
        },
        {
          "6. REFERENCES": "[1] M. Wu, S. Hu, B. Wei, and Z. Lv, “A novel deep learning model",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "emotion\nrecognition\nusing\nan\nasian multimodal\ndatabase,”"
        },
        {
          "6. REFERENCES": "based on the ICA and Riemannian manifold for EEG-based",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "Electronics, vol. 9, no. 12, p. 1988, 2020."
        },
        {
          "6. REFERENCES": "emotion recognition,” Journal of Neuroscience Methods, vol.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[17] X. Du, C. Ma, G. Zhang, J. Li, Y.-K. Lai, G. Zhao, X. Deng, Y.-"
        },
        {
          "6. REFERENCES": "378, p. 109642, 2022.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "J. Liu, and H. Wang, “An efficient LSTM network for emotion"
        },
        {
          "6. REFERENCES": "[2] Y. Zhang, C. Cheng,\nand Y. Zhang,\n“Multimodal\nemotion",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "recognition from multichannel EEG signals,” IEEE Trans. on"
        },
        {
          "6. REFERENCES": "recognition based on manifold learning and convolution neural",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "Affective Computing, vol. 13, no. 3, pp. 1528–1540, 2022."
        },
        {
          "6. REFERENCES": "network,” Multimedia Tools and Applications, vol. 81, no. 23,",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[18] K.-Y. Wang, Y.-L. Ho, Y.-D. Huang, and W.-C. Fang, “Design"
        },
        {
          "6. REFERENCES": "pp. 33 253–33 268, 2022.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "of intelligent EEG system for human emotion recognition with"
        },
        {
          "6. REFERENCES": "[3] Z. Zhang,\nS.-h. Zhong,\nand Y. Liu,\n“GANSER: A self-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "convolutional neural network,” in IEEE Int. Conf. on Artificial"
        },
        {
          "6. REFERENCES": "supervised data augmentation framework for EEG-based emo-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "Intelligence Circuits and Systems (AICAS), 2019, pp. 142–145."
        },
        {
          "6. REFERENCES": "tion recognition,” IEEE Trans. on Affective Computing, pp. 1–",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[19]\nS. Rayatdoost and M. Soleymani, “Cross-corpus EEG-based"
        },
        {
          "6. REFERENCES": "1, 2022.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "emotion recognition,” in Int. Workshop on Machine Learning"
        },
        {
          "6. REFERENCES": "[4]\nP. Boonyakitanont, A. Lek-Uthai, and J. Songsiri, “Automatic",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "for Signal Processing (MLSP), 2018, pp. 1–6."
        },
        {
          "6. REFERENCES": "epileptic seizure onset-offset detection based on CNN in scalp",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "EEG,” in IEEE Int. Conf. on Acoust., Speech and Signal Pro-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[20] E. S. Salama, R. A. El-Khoribi, M. E. Shoman, and M. A. W."
        },
        {
          "6. REFERENCES": "cess. (ICASSP).\nIEEE, 2020, pp. 1225–1229.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "Shalaby,\n“EEG-based emotion recognition using 3D convo-"
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "lutional neural networks,” International Journal of Advanced"
        },
        {
          "6. REFERENCES": "[5] C. Tan, F. Sun,\nand W. Zhang,\n“Deep transfer\nlearning for",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "Computer Science and Applications, vol. 9, no. 8, 2018."
        },
        {
          "6. REFERENCES": "EEG-based brain computer\ninterface,” in IEEE Int. Conf. on",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "Acoust., Speech and Signal Process. (ICASSP), 2018, pp. 916–",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[21] C. Tan, G. Ceballos, N. Kasabov, and N. Puthanmadam Sub-"
        },
        {
          "6. REFERENCES": "920.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "ramaniyam, “FusionSense: Emotion classification using fea-"
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "ture fusion of multimodal data and deep learning in a brain-"
        },
        {
          "6. REFERENCES": "[6] C. Li, B. Chen, Z. Zhao, N. Cummins, and B. W. Schuller,",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "inspired spiking neural network,” Sensors, vol. 20, no. 18, p."
        },
        {
          "6. REFERENCES": "“Hierarchical attention-based temporal convolutional networks",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "5328, 2020."
        },
        {
          "6. REFERENCES": "for EEG-based emotion recognition,”\nin IEEE Int. Conf. on",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "Acoust.,\nSpeech\nand\nSignal Process.\n(ICASSP),\n2021,\npp.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[22]\nS. Rayatdoost, D. Rudrauf, and M. Soleymani, “Expression-"
        },
        {
          "6. REFERENCES": "1240–1244.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "guided EEG representation learning for emotion recognition,”"
        },
        {
          "6. REFERENCES": "[7]\nS. R. Stahlschmidt, B. Ulfenborg, and J. Synnergren, “Mul-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "in IEEE Int. Conf. on Acoust., Speech and Signal Process."
        },
        {
          "6. REFERENCES": "timodal deep learning for biomedical data fusion:\na review,”",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "(ICASSP), 2020, pp. 3222–3226."
        },
        {
          "6. REFERENCES": "Briefings in Bioinformatics, vol. 23, no. 2, p. bbab569, 2022.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[23]\nF. Zeng, Y. Lin, P. Siriaraya, D. Choi, and N. Kuwahara, “Emo-"
        },
        {
          "6. REFERENCES": "[8] A. Zhang, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "tion detection using EEG and ECG signals from wearable tex-"
        },
        {
          "6. REFERENCES": "J. Fu, “Beyond fully-connected layers with quaternions: Pa-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "tile devices for elderly people,” Journal of Textile Engineering,"
        },
        {
          "6. REFERENCES": "rameterization of hypercomplex multiplications with 1/n pa-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "vol. 66, no. 6, pp. 109–117, 2020."
        },
        {
          "6. REFERENCES": "rameters,” Int. Conf. on Machine Learning (ICML), 2021.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[24] B. Nakisa, M. N. Rastgoo, A. Rakotonirainy, F. Maire, and"
        },
        {
          "6. REFERENCES": "[9] E. Grassucci, A. Zhang,\nand D. Comminiello,\n“PHNNs:",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "V\n. Chandran, “Automatic emotion recognition using temporal"
        },
        {
          "6. REFERENCES": "Lightweight neural networks via parameterized hypercomplex",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "multimodal deep learning,” IEEE Access, vol. 8, pp. 225 463–"
        },
        {
          "6. REFERENCES": "convolutions,” IEEE Trans. on Neural Netwowrks and Learn-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "225 474, 2020."
        },
        {
          "6. REFERENCES": "ing Systems, pp. 1–13, dec 2022.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[25] T. C. Dolmans, M. Poel, J.-W. J. van’t Klooster, and B. P. Veld-"
        },
        {
          "6. REFERENCES": "[10] T. Parcollet, M. Morchid, and G. Linar`es, “A survey of quater-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "kamp, “Perceived mental workload classification using inter-"
        },
        {
          "6. REFERENCES": "nion neural networks,” Artif. Intell. Rev., Aug. 2019.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "mediate fusion multimodal deep learning,” Frontiers in human"
        },
        {
          "6. REFERENCES": "[11] ——, “Quaternion convolutional neural networks for heteroge-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "neuroscience, vol. 14, p. 609096, 2021."
        },
        {
          "6. REFERENCES": "neous image processing,” in IEEE Int. Conf. on Acoust., Speech",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[26]\nJ. A. Miranda-Correa, M. K. Abadi, N. Sebe, and I. Patras,"
        },
        {
          "6. REFERENCES": "and Signal Process. (ICASSP), Brighton, UK, May 2019, pp.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "“AMIGOS: A dataset for affect, personality and mood research"
        },
        {
          "6. REFERENCES": "8514–8518.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "on individuals and groups,” IEEE Trans. on Affective Comput-"
        },
        {
          "6. REFERENCES": "[12] M. Soleymani, J. Lichtenauer, T. Pun, and M. Pantic, “A mul-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "ing, vol. 12, no. 2, pp. 479–493, 2018."
        },
        {
          "6. REFERENCES": "timodal database for affect recognition and implicit\ntagging,”",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[27] A. Topic, M. Russo, M. Stella, and M. Saric, “Emotion recog-"
        },
        {
          "6. REFERENCES": "IEEE Trans. on Affective Computing, vol. 3, no. 1, pp. 42–55,",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "nition using a reduced set of EEG channels based on holo-"
        },
        {
          "6. REFERENCES": "2011.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "graphic feature maps,” Sensors, vol. 22, no. 9, p. 3248, 2022."
        },
        {
          "6. REFERENCES": "[13] Z. He, N. Zhuang, G. Bao, Y. Zeng,\nand B. Yan,\n“Cross-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "[28]\nJ. R. Msonda, Z. He,\nand C. Lu,\n“Feature\nreconstruction"
        },
        {
          "6. REFERENCES": "day EEG-based emotion recognition using transfer component",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "based channel selection for emotion recognition using EEG,”"
        },
        {
          "6. REFERENCES": "analysis,” Electronics, vol. 11, no. 4, p. 651, 2022.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "in IEEE Signal Processing in Medicine and Biology Sympo-"
        },
        {
          "6. REFERENCES": "[14] Y.-J. Liu, M. Yu, G. Zhao, J. Song, Y. Ge, and Y. Shi, “Real-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": "sium (SPMB), 2021, pp. 1–7."
        },
        {
          "6. REFERENCES": "time movie-induced discrete emotion recognition from EEG",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "signals,” IEEE Trans. on Affective Computing, vol. 9, no. 4,",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "pp. 550–562, 2017.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "[15] L. A. Mart´ınez-Tejada, A. Puertas-Gonz´alez, N. Yoshimura,",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "and Y. Koike, “Exploring EEG characteristics to identify emo-",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "tional\nreactions under videogame scenarios,” Brain Sciences,",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        },
        {
          "6. REFERENCES": "vol. 11, no. 3, p. 378, 2021.",
          "[16]\nJ.-H. Maeng, D.-H. Kang,\nand D.-H. Kim,\n“Deep learning": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A novel deep learning model based on the ICA and Riemannian manifold for EEG-based emotion recognition",
      "authors": [
        "M Wu",
        "S Hu",
        "B Wei",
        "Z Lv"
      ],
      "year": "2022",
      "venue": "Journal of Neuroscience Methods"
    },
    {
      "citation_id": "3",
      "title": "Multimodal emotion recognition based on manifold learning and convolution neural network",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "4",
      "title": "GANSER: A selfsupervised data augmentation framework for EEG-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S.-H Zhong",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Automatic epileptic seizure onset-offset detection based on CNN in scalp EEG",
      "authors": [
        "P Boonyakitanont",
        "A Lek-Uthai",
        "J Songsiri"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Deep transfer learning for EEG-based brain computer interface",
      "authors": [
        "C Tan",
        "F Sun",
        "W Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "7",
      "title": "Hierarchical attention-based temporal convolutional networks for EEG-based emotion recognition",
      "authors": [
        "C Li",
        "B Chen",
        "Z Zhao",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "8",
      "title": "Multimodal deep learning for biomedical data fusion: a review",
      "authors": [
        "S Stahlschmidt",
        "B Ulfenborg",
        "J Synnergren"
      ],
      "year": "2022",
      "venue": "Briefings in Bioinformatics"
    },
    {
      "citation_id": "9",
      "title": "Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters",
      "authors": [
        "A Zhang",
        "Y Tay",
        "S Zhang",
        "A Chan",
        "A Luu",
        "S Hui",
        "J Fu"
      ],
      "venue": "Int. Conf. on Machine Learning (ICML)"
    },
    {
      "citation_id": "10",
      "title": "PHNNs: Lightweight neural networks via parameterized hypercomplex convolutions",
      "authors": [
        "E Grassucci",
        "A Zhang",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Neural Netwowrks and Learning Systems"
    },
    {
      "citation_id": "11",
      "title": "A survey of quaternion neural networks",
      "authors": [
        "T Parcollet",
        "M Morchid",
        "G Linarès"
      ],
      "year": "2019",
      "venue": "Artif. Intell. Rev"
    },
    {
      "citation_id": "12",
      "title": "Quaternion convolutional neural networks for heterogeneous image processing",
      "year": "2019",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "13",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Crossday EEG-based emotion recognition using transfer component analysis",
      "authors": [
        "Z He",
        "N Zhuang",
        "G Bao",
        "Y Zeng",
        "B Yan"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "15",
      "title": "Realtime movie-induced discrete emotion recognition from EEG signals",
      "authors": [
        "Y.-J Liu",
        "M Yu",
        "G Zhao",
        "J Song",
        "Y Ge",
        "Y Shi"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Exploring EEG characteristics to identify emotional reactions under videogame scenarios",
      "authors": [
        "L Martínez-Tejada",
        "A Puertas-González",
        "N Yoshimura",
        "Y Koike"
      ],
      "year": "2021",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "17",
      "title": "Deep learning method for selecting effective models and feature groups in emotion recognition using an asian multimodal database",
      "authors": [
        "J.-H Maeng",
        "D.-H Kang",
        "D.-H Kim"
      ],
      "year": "1988",
      "venue": "Electronics"
    },
    {
      "citation_id": "18",
      "title": "An efficient LSTM network for emotion recognition from multichannel EEG signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Design of intelligent EEG system for human emotion recognition with convolutional neural network",
      "authors": [
        "K.-Y Wang",
        "Y.-L Ho",
        "Y.-D Huang",
        "W.-C Fang"
      ],
      "year": "2019",
      "venue": "IEEE Int. Conf. on Artificial Intelligence Circuits and Systems (AICAS)"
    },
    {
      "citation_id": "20",
      "title": "Cross-corpus EEG-based emotion recognition",
      "authors": [
        "S Rayatdoost",
        "M Soleymani"
      ],
      "year": "2018",
      "venue": "Int. Workshop on Machine Learning for Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "EEG-based emotion recognition using 3D convolutional neural networks",
      "authors": [
        "E Salama",
        "R El-Khoribi",
        "M Shoman",
        "M Shalaby"
      ],
      "year": "2018",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "22",
      "title": "FusionSense: Emotion classification using feature fusion of multimodal data and deep learning in a braininspired spiking neural network",
      "authors": [
        "C Tan",
        "G Ceballos",
        "N Kasabov",
        "N Subramaniyam"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "23",
      "title": "Expressionguided EEG representation learning for emotion recognition",
      "authors": [
        "S Rayatdoost",
        "D Rudrauf",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process"
    },
    {
      "citation_id": "24",
      "title": "Emotion detection using EEG and ECG signals from wearable textile devices for elderly people",
      "authors": [
        "F Zeng",
        "Y Lin",
        "P Siriaraya",
        "D Choi",
        "N Kuwahara"
      ],
      "year": "2020",
      "venue": "Journal of Textile Engineering"
    },
    {
      "citation_id": "25",
      "title": "Automatic emotion recognition using temporal multimodal deep learning",
      "authors": [
        "B Nakisa",
        "M Rastgoo",
        "A Rakotonirainy",
        "F Maire",
        "V Chandran"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Perceived mental workload classification using intermediate fusion multimodal deep learning",
      "authors": [
        "T Dolmans",
        "M Poel",
        "J.-W Van't Klooster",
        "B Veldkamp"
      ],
      "year": "2021",
      "venue": "Frontiers in human neuroscience"
    },
    {
      "citation_id": "27",
      "title": "AMIGOS: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Emotion recognition using a reduced set of EEG channels based on holographic feature maps",
      "authors": [
        "A Topic",
        "M Russo",
        "M Stella",
        "M Saric"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "29",
      "title": "Feature reconstruction based channel selection for emotion recognition using EEG",
      "authors": [
        "J Msonda",
        "Z He",
        "C Lu"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing in Medicine and Biology Symposium (SPMB)"
    }
  ]
}