{
  "paper_id": "2501.15111v1",
  "title": "Humanomni: A Large Vision-Speech Language Model For Human-Centric Video Understanding",
  "published": "2025-01-25T07:26:37Z",
  "authors": [
    "Jiaxing Zhao",
    "Qize Yang",
    "Yixing Peng",
    "Detao Bai",
    "Shimin Yao",
    "Boyuan Sun",
    "Xiang Chen",
    "Shenghao Fu",
    "Weixuan chen",
    "Xihan Wei",
    "Liefeng Bo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In human-centric scenes, the ability to simultaneously understand visual and auditory information is crucial. While recent omni models can process multiple modalities, they generally lack effectiveness in human-centric scenes due to the absence of large-scale, specialized datasets and non-targeted architectures. In this work, we developed HumanOmni, the industry's first human-centric Omnimultimodal large language model. We constructed a dataset containing over 2.4 million human-centric video clips with detailed captions and more than 14 million instructions, facilitating the understanding of diverse human-centric scenes. Hu-manOmni includes three specialized branches for understanding different types of scenes. It adaptively fuses features from these branches based on user instructions, significantly enhancing visual understanding in scenes centered around individuals. Moreover, HumanOmni integrates audio features to ensure a comprehensive understanding of environments and individuals. Our experiments validate HumanOmni's advanced capabilities in handling human-centric scenes across a variety of tasks, including emotion recognition, facial expression description, and action understanding. Our model will be open-sourced to facilitate further development and collaboration within both academia and industry.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the era of rapid digital and intelligent development, understanding human-centric scenes has become increasingly critical. These scenes extend beyond video chat  [26]  to encompass education, healthcare, social interactions, and entertainment. In these human-centric scenes, vision and speech are typically present simultaneously. For certain tasks, both visual and auditory information provide significant benefits, such as in emotion recognition  [9, 11, 32, 66]  and speaker-specific speech recognition. Speaker-specific speech recognition builds upon automatic speech recognition by incorporating additional description about the speaker. We are currently defining this task and collecting such a dataset, with plans to release it in the next version of our work.\n\nCurrent methods predominantly focus on Vision-Language models  [1, 16, 24, 25, 30, 36, 37, 43, [55] [56] [57] 67] , which effectively handle visual and textual information but generally lack the capability to process audio inputs. This limitation results in an incomplete understanding of scenes. In recent years, some omni models  [17, 18, 28, 51, 54]  have been proposed to address multiple modalities, including visual, auditory, and textual data. However, these models often emphasize generic scenes and lack targeted training for human-centric scenes. Additionally, they do not incorporate specialized model designs, leading to weaker performance in understanding such scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Our Model",
      "text": "In Fig.  1 , we illustrate the HumanOmni pipeline, which is capable of processing a multimodal input encompassing textual, auditory and visual data.\n\nFor visual component, facial expressions, body movements, individual attributes, and interactions with the environment are crucial elements for understanding human-centric video content. Different types of features are critical for different tasks; for instance, emotion analysis heavily relies on facial expressions, action recognition focuses more on body movements, and social interaction analysis depends on interactions between individuals and their environment or objects. To address these diverse requirements, we have designed three specialized branches: the Face-related Branch, Bodyrelated Branch, and Interaction-related Branch. These branches capture distinct features to enhance the model's performance across various human-related tasks. Leveraging advanced visual encoders SigLIP  [58]  and large language models Qwen2.5  [45] , which exhibit strong feature extraction and representation capabilities, our branch architectures remain flexible and do not require task-specific modifications. To guide each branch to focus on specific tasks, we train them using different video clips and instructions, ensuring that each branch specializes in extracting different types of features, as detailed in the training section.\n\nIn particular, while the three branches share a generic architecture, they differ in their visual projector components. The face-related branch employs a detail-sensitive projector MLP2xGeLU  [23]  to better capture subtle facial changes. In contrast, the body-related branch and interaction-related branch utilize a spatial-temporal projector STC  [12] , handling continuous actions and interaction scenes. Importantly, despite using two different types of projectors, the features derived from both approaches remain spatially and temporally aligned, ensuring consistency and effectiveness in feature fusion.\n\nThe features from these three branches are complementary to some extent. However, directly concatenating them would lead to an excessive number of visual tokens, imposing additional computational and analytical burdens on the LLM. While simply summing the features is one approach, we have devised a more sophisticated method for feature fusion. Inspired by LLAVA-Octopus  [65] , we use the rich information contained in the user instructions to dynamically adjust the weighting of features Figure  1 : Pipeline of HumanOmni. HumanOmni is a vision-speech language model that focus on human-centric scenes. For the visual component, we pre-trained three distinct branches using separate data. The features from these branches are fused based on user instructions. HumanOmni also supports audio input, enhancing its ability to fully understand complex human-centric scenes.\n\nfrom each branch. For example, when the instruction pertains to emotion recognition, the model places greater emphasis on features from the face-related branch; for interaction scenes, it prioritizes the interaction-related branch.\n\nSpecifically, to process user instructions, we employ BERT  [15]  for encoding the commands. We focus on the [CLS] token produced by BERT, which encapsulates the semantic essence of the instruction. We chose BERT as our text encoder due to its robust pre-training, enabling it to capture deep semantic information from text. BERT utilizes a bidirectional transformer architecture to encode input text, with the [CLS] token effectively summarizing the semantics of the entire sentence. This provides a strong foundation for subsequent weight generation processes.\n\nNext, we introduce two MLPs for generating feature weights. The first MLP receives the [CLS] token as input and, through multiple layers of neural network processing, produces intermediate feature representations that capture high-level semantic details from the instructions. The second MLP then takes these intermediate representations as input and further refines them to generate final weight values, each corresponding to one of the visual projectors. These generated weights are used to dynamically adjust and combine the visual features extracted by the three projectors, selecting the most suitable features for the task at hand. Suppose the three projectors extract features denoted as F 1 , F 2 and F 3 , and the generated weights are w 1 , w 2 and w 3 , respectively. Then, the final visual representation F is given by:\n\nThis instruction-driven feature fusion approach enhances the model's flexibility and adaptability while ensuring efficient resource utilization. It allows the model to automatically adjust its focus on different types of features based on task requirements.\n\nFor the auditory component, we follow  [54]  utilizing the audio preprocessor and encoder from Whisper-large-v3  [40]  to process audio data. Specifically, the audio input first undergoes preliminary processing through the audio preprocessor, generating a format suitable for encoding. Subsequently, the preprocessed audio data is encoded using Whisper's encoder, extracting robust audio features.\n\nTo ensure that audio features can be effectively integrated with visual and textual features in the same domain, we employ MLP2xGeLU as the projector. This projector maps the audio features into the text domain.\n\nFor the text, we directly used the corresponding text encoder module from the LLM to encode the text. Consequently, the audio tokens, along with visual and text tokens, are concatenated within a unified representation space using specific tokens to distinguish between features from different modalities, and then fed into the LLM decoder for further processing. We employ scene detection and segmentation to divide the video into clips to prevent unnatural temporal changes caused by instantaneous scene transitions. Then, the clips with relatively low resolution are removed, and the key frames detection algorithms are applied, which helps to quantify the temporal changes in clips. To further improve learning efficiency, we generate brief captions based on advanced multimodal model, and eliminate the clips similar in contexts. Finally, in addition to being automatically annotated with human and face bounding boxes, the remaining video clips will be processed by several state-of-the-art multimodal models to generate detailed captions. Subsequently, a large language model will be used to synthesize the common content across these captions, while filtering out unique content that may result from model hallucinations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Human-Centric Data Collection",
      "text": "Although there are currently many multimodal annotated datasets, including OCR and visual navigation, there is a lack of a large-scale human-centric dataset with fine-grained annotations, limiting the development of human-centric video understanding. Based on the existing large-scale video datasets, we have carefully designed a data processing workflow and present the largest human-centric dataset for comprehensive human-centric video understanding.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Video Collection",
      "text": "We form our dataset based on the existing web-scale dataset: Panda-70M  [10]  which covers various scenes and contents. Despite initial processing and caption generation, much useful information, particularly related to human subjects, remains underutilized. Existing research has demonstrated that data quality is crucial for model performance; simply increasing the quantity of low-quality data does not lead to significant performance improvements. Therefore, we have further optimized our dataset to create a human-centric collection with the following characteristics: high video resolution (above 480P), rich temporal dynamics, inclusion of face and body detection bounding boxes, and captions verified through a dual-check process. Our data processing includes the following step and an illustration is in Fig.  2 .\n\n-Stage 1: Temporal processing. Temporal dynamics is a key feature distinguishing video data from static images. Videos lacking temporal changes offer limited learning value, while those with excessive and unrealistic temporal variations can confuse models. To extract naturally smooth temporal changes in the video, we employed scene recognition algorithms to identify multiple scenes within videos and segment them into clips, as shown in the center of Fig.  2 . Scene detection and segmentation avoid drastic changes from multiple different perspectives in the video clip. Besides, it ensures that clips with intense temporal changes are broken down into shorter segments, which can then be filtered out.\n\nFurthermore, we extracted keyframes for each clip to find the temporal variations. The clips with minimal temporal changes would have very few keyframes identified, allowing us to screen out such Caption: In the video, the character exhibits a sense of happiness and relaxation. His facial expressions and body language convey satisfaction with the exercise, indicating a positive attitude towards physical fitness and personal challenge. His face appears relaxed, with a smooth, uncreased forehead. His eyes are bright and focused, and a subtle smile on his lips. The character is dressed in workout gear that includes a black tank top, black shorts, and vibrant red sneakers. He hangs from the pull-up bar with fully extended arms, setting himself ready for the exercise. He skillfully performs hanging leg raises by bringing his knees toward his chest. Regrading interactions, he begins by approaching the pull-up bar, gripping it with both hands to ensure a secure hold. Once ready, he lifts himself into position, hanging with fully extended arms. Throughout the exercise, his movements are fluid and controlled, indicating a practiced routine.\n\n{\"Face-related\": [ { \"emotion\": { \"question\": \"What emotion is the character in the video expressing?\", \"answer\": \" Happiness and relaxation.\" } }, { \"expression\": { \"question\": \"How does the character's expression convey his emotional state?\", \"answer\": \"His facial expressions show satisfaction with the exercise, indicating a positive attitude. \"} }, { \"appearance\": { \"question\": \"Describe the appearance of the character's face.\", \"answer\": \"The character's face is relaxed, with bright, focused eyes and a subtle smile.\" } } ],\n\n\"Body-related\": [ { \"appearance\": { \"question\": \"Describe the character's outfit.\", \"answer\": \"The character is dressed in workout gear that includes a black tank top, black shorts, and vibrant red sneakers.\" } }, { \"pose\": { \"question\": \" Describe the character's pose.\", \"answer\": \"He hangs from the pull-up bar with fully extended arms, setting himself up.\" } }, { \"action\": { \"question\": \"Describe the character's action.\", \"answer\": \"He skillfully performs hanging leg raises by bringing his knees toward his chest}],\n\n\"Interaction-related\": [ { \" Interaction \": { \"question\": \"Describe the interactions between the character and the environment.\", \"answer\": \"The character interacts with the pull-up bar by approaching it and gripping it securely with both hands. He then lifts himself into a hanging position with fully extended arms. While hanging, he performs leg raises, bringing his knees to his chest, using the bar for leverage. His movements are fluid and controlled, indicating familiarity with the equipment. Throughout the exercise, his relaxed expression and subtle smile suggest a comfortable and positive engagement with t he environment.\" } } ] } Figure  3 : Instruction Data Generation Process for face-related, body-related, and interaction-related branches. We generate structured instruction data by leveraging Qwen2.5 with specifically designed prompts to process the detailed captions we have previously obtained. videos based on the number of detected keyframes. We also removed clips with resolutions below 480P to enhance the overall quality of the data.\n\n-Stage 2: Reducing Redundancy. To improve learning efficiency, we eliminated redundant video segments with similar contexts or meanings, reducing data redundancy as shown in the bottom of Fig.  2 . Specifically, we used an advanced multimodal model, QWen2-VL-72B  [44] , to generate brief descriptions for each clip, focusing on the general contextual information. By calculating the semantic similarity between these brief descriptions using language model, we were able to filter out clips with high semantic similarity and repeated patterns.\n\n-Stage 3: Fine-grained Annotations. For the remaining data, we generated detailed descriptions using the advanced multimodal model (QWen2-VL-72B) to maximize the utility of the data as shown on the right side of Fig.  2 . Given the well-known issue of hallucination in large models, we implemented a dual verification method to eliminate hallucinations from detailed captions. Specifically, we used an additional multimodal large model  [49]  to generate multiple detailed descriptions for each clip. Given that the content of hallucinations is not related to the actual content of in the video, we hypothesize that hallucinations generated by different models are distinct. Hence, we heuristically employed a large language model (QWen2.5-72B  [45] ) to summarize the common points across the different detailed descriptions, ensuring the accuracy of the final descriptions and",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Manual Correction Manual Consolidation",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion:",
      "text": "Happy minary Annotations: This video showcases the emotio anges of a young man with fair skin, short light brown ell-defined facial features, and a black top. e beginning of the video, his eyes are slightly squinted, relaxed eyelids, expressing a sense of ease and pleasure. e video progresses, the corners of his eyes gradually lift, is eyes widen, radiating a feeling of joy and excitement. yebrows also rise, particularly the right one, which lifts r, and the wrinkles between his brows deepen slightly, i ting a sense of surprise and happiness. outh transitions from a smile to a hearty laugh, with his lip lifting to reveal white teeth, and his jaw relaxing an ing downward. The corners of his mouth pull up notice and his entire facial muscles work together to convey th ful emotion, appearing very natural and relaxed. middle of the video, his head tilts slightly to the right a n returns to an upright position, seemingly expressing a of relaxation and ease. e video continues, his expression takes on a hint of play ss, with his upper lip lifting slightly, his lower lip gently inward, and his mouth closing slightly, forming a smil he can hardly suppress. The fine lines at the corners of es become more pronounced, further enhancing the joyf osphere. ly, his head tilts forward slightly, his gaze remains bright ull of joy, and his overall expression shifts from relaxed py, ultimately immersing him in a broad, joyful smile.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Facial Detailed Description:",
      "text": "This vi deo shows an elderly man with gray hair that is short and neatly kept, wh ite eyebrows, and wrinkles on his fa ce and forehead. The man looks at th e camera with a smile, his mouth cor ners upturned, teeth showing, and ey ebrows raised.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Video Frames",
      "text": "Speaker Through the aforementioned steps, we collected 2.4M human-centric video clips with captions. We then utilize Qwen2.5-72B to generate structured data from detail captions for the pre-training of different branches, as illustrated in the Fig.  3 . We systematically constructed instruction pairs for the face branch, body branch, and interaction branch by utilizing the structured data.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Instructions Generation",
      "text": "For the face-related branch, we filtered out videos that did not include descriptions of faces, emotions, or expressions. This resulted in a dataset of 1.78M videos. We then created detailed instruction pairs for facial features, emotions, and expressions, totaling 4.12M instruction pairs. These pairs were used to train the face-related branch.\n\nFor the body-related branch, we applied a similar method. We filtered out videos that lacked information on human appearance attributes, actions, or poses, leaving us with 2.21M videos. We then created specific instruction pairs for human appearance attributes, actions, and poses, resulting in a total of 5.75M instruction pairs. These pairs were used to train the body-related branch.\n\nFinally, for the interaction-related module, we created specific instruction pairs for interactions with the external environment. Additionally, to ensure that our caption information was fully utilized, we incorporated detailed captions as instruction data in this module. This process resulted in a total of 4.8M instruction pairs, including 2.4M interaction instruction pairs and 2.4M detailed caption instruction pairs. These pairs were used to train the interaction-related module.\n\nAll of these instructions were used during the pre-training phase. The instructions come from captions that we double-checked, ensuring higher accuracy. By reasonably segmenting and filtering video clips, we also improved video quality, which helps the model better understand human-centric scenes.\n\nAdditionally, we manually annotated a subset of our human-centric video data by randomly sampling 50K video clips containing both vision and speech. These clips were annotated for emotion recognition, detailed facial expression descriptions, and speaker-specific speech annotations. Due to varying annotation difficulties, we completed emotion annotations for all 50K videos, detailed facial expression descriptions for 5K videos, and speaker-specific speech annotations for 20K videos, resulting in a total of 75K instruction pairs. The process is illustrated in Fig.  4 . These annotated data were used in the fine-tuning of the visual component, further enhancing its feature extraction capabilities. Because these data include both visual and speech components and are manually annotated for high quality, they were also utilized in Cross-Modal Interaction Integration.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training",
      "text": "To build a multimodal large model capable of accurately understanding human-centric video information and possessing cross-modal interaction capabilities, our training strategy is divided into three stages. Initially, we focus on pretraining and fine-tuning the model's visual abilities using a substantial amount of human-centric video data, enabling the model to learn rich spatio-temporal feature representations and patterns of human behavior, thereby achieving a deep understanding of human-related details in video content. Next, we conduct standalone audio capability training with audio data, allowing the model to recognize and interpret speech. Finally, we perform cross-modal interaction training by integrating auditory and visual data, enhancing the model's ability to process and associate information across different modalities, ensuring it can provide accurate understanding and responses in complex multimedia environments.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Visual Capability Construction",
      "text": "Our model's visual component includes three specialized branches: face-related branch, body-related branch, and interaction-related branch. For each branch, we generated specific instruction data as described in the above section. This instruction data was used to pre-train each branch, during which only the projector parameters were updated. The aim was to keep all other parameters identical across the three branches to facilitate integration during fine-tuning.\n\nDuring fine-tuning, we used manually annotated data consisting of 50K emotion recognition instructions and 5K facial expression description instructions, along with general Oryx  [33]  fine-tuning data. We integrated the three branches using an instruction-driven fusion module. In this process, we froze the parameters of the visual encoder and BERT, while training the parameters of the three projectors, the large language model, and two MLPs that generate the fusion weights.\n\nDuring this phase, even though some of the videos contain both auditory and visual information, we only utilized the visual part.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Auditory Capability Development",
      "text": "In this stage, we aim to align the modalities between text and audio, enhancing the model's ability to understand and respond to audio in various contexts. We exclusively sample audio data from tasks such as automatic audio captioning, automatic speech recognition, and sound event classification, resulting in a total of approximately 18,000 hours of data used to train the audio projector.\n\nSpecifically, we utilize the WavCaps  [35]  dataset, which provides around 7,500 hours of annotated audio, offering detailed captions that describe the audio events. This dataset plays a crucial role in helping the model understand and generate descriptive audio analyses. We also select multiple comprehensive ASR datasets including WenetSpeech  [59] , GigaSpeech  [6] , CommonVoice15  [4] ,\n\nand LibriSpeech  [39] . These datasets cover extensive and diverse speech data, which are important in training models for speech recognition tasks. For SEC, the VGGSound  [7]  dataset is chosen due to its extensive collection of audio events. For the different tasks, we designed multiple question templates to prompt the model in generating captions, performing speech recognition, and classifying sounds, which in turn enables the model to thoroughly understand and process human-related audio information.\n\nTask Type Datasets Duration (hours) AAC WavCaps  [35]  ~7.5k ASR WenetSpeech  [59] , GigaSpeech  [6] , CommonVoice15  [4] , LibriSpeech  [39]  ~10k SEC VGGSound  [7]  ~0.5k Table  1 : Details of audio datasets for training audio projectors.\n\nWe use the encoder and the audio preprocessor from the Whisper-large-v3  [40]  as the audio encoder and processor. Specifically, We resample each audio to a frequency of 16kHz and convert the waveform into 128-channel mel-spectrogram using a window size of 25ms and a hop size of 10ms. To reduce the token length of the audio, we introduce an average pooling layer with a stride of 3, resulting in each audio frame from the audio encoder corresponding to a 60ms segment of the original audio. We use two linear layers to connect this to the LLM decoder. Additionally, we wrap each audio embedding with a pair of special tokens to indicate the start and end positions of the audio embedding.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Cross-Modal Interaction Integration",
      "text": "To enhance our model's video-audio interaction capabilities, we synthesized a series of visualauditory cross-modal interaction data. For audio data, we collected a diverse dataset covering various audio tasks, including samples from the audio pre-training phase, emotion recognition datasets, and audio question-answering datasets, totaling 7,000 hours of audio. For video data, we used all the aforementioned manually annotated 20K speaker-specific speech recognition data, as well as the instruction data used for visual fine-tuning. Additionally, we incorporate multi-modal emotion recognition datasets, converting classification labels with GPT-4o into a question-answer format, which includes DFEW  [19] , MAFW  [32] , CAER  [21] , and FERV39k  [48] .\n\nTo better distinguish features from different modalities, we encapsulate the embeddings of audio and visual data using distinct special tokens. We initialize the visual projectors and LLM decoder with parameters obtained from the Visual Capability Construction phase, while the audio projector is initialized with parameters from the Auditory Capability Development phase. During this training phase, we jointly fine-tune the LLM decoder, all projectors and two multi-layer perceptrons (MLPs) that generate the fusion weights to optimize their performance in handling multi-modal inputs.\n\nTo ensure that our HumanOmni can understand both scenes that include visual and auditory information and those with only visual input, for each video that contains audio, we also generate a version without audio for training. The model determines which modality to use based on special tokens in the instructions. Additionally, if either the auditory or visual part is missing, we fill in with default tokens to ensure consistent and complete inputs. This design allows the model to maintain stable performance across different modality combinations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "We evaluated HumanOmni's ability to understand audio-visual inputs on several human-related tasks, such as emotion recognition, facial expression description, and action understanding. We also tested HumanOmni's performance on speech recognition using only audio inputs. Finally, we explored how different modalities affect model performance across these human-centric tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Evaluations On Emotion Recognition",
      "text": "Both DFEW and MAFW are video-clip-based datasets designed for Dynamic Facial Emotion Recognition task, with DFEW providing a 7-dimensional expression distribution vector and MAFW providing an 11-dimensional expression distribution vector for each video clip.  As shown in Tab. 2, while VLM methods possess broader capabilities, they still exhibit a performance gap compared to specialized methods in dynamic emotion recognition tasks. In this task, both video and audio information play crucial roles, which is where the HumanOmni model excels. Experimental results demonstrate that HumanOmni significantly outperforms existing video-language multimodal models, audio-language multimodal large models, recently proposed omni model and specialized methods in this field. Moreover, it also shows a clear advantage over recently proposed Omni models for emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Evaluations On Facial Expression",
      "text": "Facial expressions refer to external features displayed through facial muscle movements, such as smiling or frowning, while emotions denote internal emotional states, such as happiness or sadness.\n\nAlthough facial expressions are one way to convey emotions, not all expressions directly correspond to specific emotional states, and the same expression can represent different emotions in various contexts. In this evaluation, we utilized the recently proposed DFEC dataset for facial expression description and adopted the evaluation methods recommended by DFEC.\n\nIn Tab. 3, our experimental results show that the HumanOmni model with combined video and audio input not only outperforms other open-source models but also surpasses the FaceTrack-MM  [64]  method proposed in DFEC, achieving superior performance in facial expression description tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluations On Actions Understanding",
      "text": "MVBench is a comprehensive video understanding benchmark covering 20 tasks organized in the form of multiple-choice questions. From this extensive suite of challenges, we select a specialized benchmark focusing on human-related subtasks, demonstrating in Tab.  (FA). This refined selection aims to provide a focused evaluation framework for the nuanced aspects of human activity recognition within video content.\n\nThe experimental results show that on the MVBench dataset, HumanOmni significantly outperforms nearly all mainstream methods with the same parameter size, with the exception of a few methods that utilized the full MVBench dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluations On Speech Recognition",
      "text": "Speech recognition capability is a crucial component of human-computer interaction. To demonstrate the advantages of our approach within the domain of speech recognition, Tab. 5 presents results from four widely recognized benchmarks in this field: LibriSpeech  [39] , WenetSpeech  [59] , and Fleurs  [14] . These benchmarks are specifically chosen for their distinct characteristics and contributions to evaluating speech recognition systems across different languages and contexts. LibriSpeech focuses on English speech recognition, while Fleurs is dedicated to evaluating cross-lingual speech representations. From the table, it can be seen that our method is leading among the current Omni models. However, compared to proprietary speech recognition approaches, current audio-visual methods can still be improved.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Explorations Of Modality Effects",
      "text": "Here we explored modalities efects on human-centric task performance. In Table  6 , we evaluated HumanOmni's performance on emotion recognition, facial expression description, and action understanding under different input modalities. As expected, in the emotion recognition task, single-modal configurations (using only video or only audio) performed notably lower compared to the multi-modal configuration that used both video and audio inputs. For facial expression description, even when using only video input, the HumanOmni model maintained excellent performance, only slightly lower than with combined inputs. This is because facial expression recognition primarily relies on visual information, with limited added value from audio data. In the action understanding task, where actions are mainly represented by visual content, the contribution of audio was even more limited, as confirmed by our experimental results. These results demonstrate HumanOmni's robust performance across different input modalities. Additionally, they show that for all tasks, the combined visual-auditory input consistently achieved the best results, underscoring the necessity of joint audio and video inputs in human-centric scenes.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we developed HumanOmni, the first human-centric multi-modal large language model. We constructed a dataset containing over 2.4 million human-centric video clips annotated with more than 14 million detailed captions and instructions to facilitate the understanding of diverse humancentered scenes. HumanOmni features a specialized architecture with three branches: a face-related branch, a body-related branch, and an interaction-related branch. Each branch addresses specific categories of human-centric scenes. By using user instructions to guide the adaptive fusion of features from these branches, HumanOmni significantly enhances its robustness across various scenarios. Additionally, HumanOmni supports joint audio and video input, enabling a more comprehensive understanding of scenes. We evaluated HumanOmni's performance through extensive experiments on multiple human-centric tasks, demonstrating its effectiveness in understanding complex humancentered interactions. To promote community-driven development and further research, we will open-source our code and model.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we illustrate the HumanOmni pipeline, which is capable of processing a multimodal input",
      "page": 2
    },
    {
      "caption": "Figure 1: Pipeline of HumanOmni. HumanOmni is a vision-speech language model that focus",
      "page": 3
    },
    {
      "caption": "Figure 2: Data processing flow. We employ scene detection and segmentation to divide the video",
      "page": 4
    },
    {
      "caption": "Figure 2: - Stage 1: Temporal processing. Temporal dynamics is a key feature distinguishing video data",
      "page": 4
    },
    {
      "caption": "Figure 2: Scene detection and",
      "page": 4
    },
    {
      "caption": "Figure 3: Instruction Data Generation Process for face-related, body-related, and interaction-related",
      "page": 5
    },
    {
      "caption": "Figure 2: Specifically, we used an advanced multimodal model, QWen2-VL-72B [44], to generate",
      "page": 5
    },
    {
      "caption": "Figure 2: Given the well-known issue of hallucination in large models,",
      "page": 5
    },
    {
      "caption": "Figure 4: Illustration of the data annotation process. We annotate the data from the perspectives of",
      "page": 6
    },
    {
      "caption": "Figure 3: We systematically constructed instruction pairs for the",
      "page": 6
    },
    {
      "caption": "Figure 4: These annotated data were",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task Type": "AAC",
          "Datasets": "WavCaps [35]",
          "Duration (hours)": "~7.5k"
        },
        {
          "Task Type": "ASR",
          "Datasets": "WenetSpeech [59], GigaSpeech [6],\nCommonVoice15 [4], LibriSpeech [39]",
          "Duration (hours)": "~10k"
        },
        {
          "Task Type": "SEC",
          "Datasets": "VGGSound [7]",
          "Duration (hours)": "~0.5k"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "2",
      "title": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "authors": [
        "Qian Keyu An",
        "Chong Chen",
        "Zhihao Deng",
        "Changfeng Du",
        "Zhifu Gao",
        "Yue Gao",
        "Ting Gu",
        "Hangrui He",
        "Kai Hu",
        "Hu"
      ],
      "year": "2024",
      "venue": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "arxiv": "arXiv:2407.04051"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Anthropic",
        "Claude"
      ],
      "year": "2009",
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "authors": [
        "Guoguo Chen",
        "Shuzhou Chai",
        "Guanbo Wang",
        "Jiayu Du",
        "Wei-Qiang Zhang",
        "Chao Weng",
        "Dan Su",
        "Daniel Povey",
        "Jan Trmal",
        "Junbo Zhang"
      ],
      "year": "2021",
      "venue": "Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio",
      "arxiv": "arXiv:2106.06909"
    },
    {
      "citation_id": "7",
      "title": "Vggsound: A large-scale audio-visual dataset",
      "authors": [
        "Honglie Chen",
        "Weidi Xie",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "8",
      "title": "Sharegpt4video: Improving video understanding and generation with better captions",
      "authors": [
        "Lin Chen",
        "Xilin Wei",
        "Jinsong Li",
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Zehui Chen",
        "Haodong Duan",
        "Bin Lin",
        "Zhenyu Tang"
      ],
      "year": "2024",
      "venue": "Sharegpt4video: Improving video understanding and generation with better captions",
      "arxiv": "arXiv:2406.04325"
    },
    {
      "citation_id": "9",
      "title": "Motionllm: Understanding human behaviors from human motions and videos",
      "authors": [
        "Ling-Hao Chen",
        "Shunlin Lu",
        "Ailing Zeng",
        "Hao Zhang",
        "Benyou Wang",
        "Ruimao Zhang",
        "Lei Zhang"
      ],
      "year": "2024",
      "venue": "Motionllm: Understanding human behaviors from human motions and videos",
      "arxiv": "arXiv:2405.20340"
    },
    {
      "citation_id": "10",
      "title": "Panda-70m: Captioning 70m videos with multiple cross-modality teachers",
      "authors": [
        "Aliaksandr Tsai-Shien Chen",
        "Willi Siarohin",
        "Ekaterina Menapace",
        "Hsiang-Wei Deyneka",
        "Byung Chao",
        "Yuwei Eun Jeon",
        "Hsin-Ying Fang",
        "Jian Lee",
        "Ming-Hsuan Ren",
        "Sergey Yang",
        "Tulyakov"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Jingdong Sun",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "arxiv": "arXiv:2406.11161"
    },
    {
      "citation_id": "12",
      "title": "Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms",
      "authors": [
        "Zesen Cheng",
        "Sicong Leng",
        "Hang Zhang",
        "Yifei Xin",
        "Xin Li",
        "Guanzheng Chen",
        "Yongxin Zhu",
        "Wenqi Zhang",
        "Ziyang Luo",
        "Deli Zhao",
        "Lidong Bing"
      ],
      "year": "2024",
      "venue": "Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms",
      "arxiv": "arXiv:2406.07476"
    },
    {
      "citation_id": "13",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "14",
      "title": "Fleurs: Few-shot learning evaluation of universal representations of speech",
      "authors": [
        "Alexis Conneau",
        "Min Ma",
        "Simran Khanuja",
        "Yu Zhang",
        "Vera Axelrod",
        "Siddharth Dalmia",
        "Jason Riesa",
        "Clara Rivera",
        "Ankur Bapna"
      ],
      "year": "2022",
      "venue": "Fleurs: Few-shot learning evaluation of universal representations of speech",
      "arxiv": "arXiv:2205.12446"
    },
    {
      "citation_id": "15",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "16",
      "title": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
      "authors": [
        "Xiaoyi Dong",
        "Pan Zhang",
        "Yuhang Zang",
        "Yuhang Cao",
        "Bin Wang",
        "Linke Ouyang",
        "Xilin Wei",
        "Songyang Zhang",
        "Haodong Duan",
        "Maosong Cao"
      ],
      "year": "2024",
      "venue": "Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model",
      "arxiv": "arXiv:2401.16420"
    },
    {
      "citation_id": "17",
      "title": "Llama-omni: Seamless speech interaction with large language models",
      "authors": [
        "Qingkai Fang",
        "Shoutao Guo",
        "Yan Zhou",
        "Zhengrui Ma",
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "year": "2024",
      "venue": "Llama-omni: Seamless speech interaction with large language models",
      "arxiv": "arXiv:2409.06666"
    },
    {
      "citation_id": "18",
      "title": "Towards open-source interactive omni multimodal llm",
      "authors": [
        "Chaoyou Fu",
        "Haojia Lin",
        "Zuwei Long",
        "Yunhang Shen",
        "Meng Zhao",
        "Yifan Zhang",
        "Shaoqi Dong",
        "Xiong Wang",
        "Di Yin",
        "Long Ma",
        "Xiawu Zheng",
        "Ran He",
        "Rongrong Ji",
        "Yunsheng Wu",
        "Caifeng Shan",
        "Xing Sun",
        "Vita"
      ],
      "year": "1012",
      "venue": "Towards open-source interactive omni multimodal llm"
    },
    {
      "citation_id": "19",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "authors": [
        "Jin Peng",
        "Ryuichi Takanobu",
        "Caiwan Zhang",
        "Xiaochun Cao",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Chat-univi: Unified visual representation empowers large language models with image and video understanding",
      "arxiv": "arXiv:2311.08046"
    },
    {
      "citation_id": "21",
      "title": "Contextaware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoonn Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "22",
      "title": "Otter: A multi-modal model with in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Jingkang Yang",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Otter: A multi-modal model with in-context instruction tuning",
      "arxiv": "arXiv:2305.03726"
    },
    {
      "citation_id": "23",
      "title": "Llava-onevision: Easy visual task transfer",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Dong Guo",
        "Renrui Zhang",
        "Feng Li",
        "Hao Zhang",
        "Kaichen Zhang",
        "Yanwei Li",
        "Ziwei Liu",
        "Chunyuan Li"
      ],
      "year": "2024",
      "venue": "Llava-onevision: Easy visual task transfer",
      "arxiv": "arXiv:2408.03326"
    },
    {
      "citation_id": "24",
      "title": "LLaVA-med: Training a large language-and-vision assistant for biomedicine in one day",
      "authors": [
        "Chunyuan Li",
        "Cliff Wong",
        "Sheng Zhang",
        "Naoto Usuyama",
        "Haotian Liu",
        "Jianwei Yang",
        "Tristan Naumann",
        "Hoifung Poon",
        "Jianfeng Gao"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "25",
      "title": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models",
      "authors": [
        "Feng Li",
        "Renrui Zhang",
        "Hao Zhang",
        "Yuanhan Zhang",
        "Bo Li",
        "Wei Li",
        "Zejun Ma",
        "Chunyuan Li"
      ],
      "year": "2024",
      "venue": "Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models",
      "arxiv": "arXiv:2407.07895"
    },
    {
      "citation_id": "26",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "27",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2024",
      "venue": "Mvbench: A comprehensive multi-modal video understanding benchmark"
    },
    {
      "citation_id": "28",
      "title": "Baichuan-omni technical report",
      "authors": [
        "Yadong Li",
        "Haoze Sun",
        "Mingan Lin",
        "Tianpeng Li",
        "Guosheng Dong",
        "Tao Zhang",
        "Bowen Ding",
        "Wei Song",
        "Zhenglin Cheng",
        "Yuqi Huo",
        "Song Chen",
        "Xu Li",
        "Da Pan",
        "Shusen Zhang",
        "Xin Wu",
        "Zheng Liang",
        "Jun Liu",
        "Tao Zhang",
        "Keer Lu",
        "Yaqi Zhao",
        "Yanjun Shen",
        "Fan Yang",
        "Kaicheng Yu",
        "Tao Lin",
        "Jianhua Xu",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "year": "2024",
      "venue": "Baichuan-omni technical report",
      "arxiv": "arXiv:2410.08565"
    },
    {
      "citation_id": "29",
      "title": "Llama-vid: An image is worth 2 tokens in large language models",
      "authors": [
        "Yanwei Li",
        "Chengyao Wang",
        "Jiaya Jia"
      ],
      "year": "2024",
      "venue": "Llama-vid: An image is worth 2 tokens in large language models"
    },
    {
      "citation_id": "30",
      "title": "Llava-next: Improved reasoning, ocr, and world knowledge",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Bo Li",
        "Yuanhan Zhang",
        "Sheng Shen",
        "Yong Jae Lee"
      ],
      "year": "2001",
      "venue": "Llava-next: Improved reasoning, ocr, and world knowledge"
    },
    {
      "citation_id": "31",
      "title": "St-llm: Large language models are effective temporal learners",
      "authors": [
        "Ruyang Liu",
        "Chen Li",
        "Haoran Tang",
        "Yixiao Ge",
        "Ying Shan",
        "Ge Li"
      ],
      "year": "2024",
      "venue": "St-llm: Large language models are effective temporal learners",
      "arxiv": "arXiv:2404.00308"
    },
    {
      "citation_id": "32",
      "title": "MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2022",
      "venue": "MAFW: A Large-scale, Multi-modal, Compound Affective Database for Dynamic Facial Expression Recognition in the Wild"
    },
    {
      "citation_id": "33",
      "title": "Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution",
      "authors": [
        "Zuyan Liu",
        "Yuhao Dong",
        "Ziwei Liu",
        "Winston Hu",
        "Jiwen Lu",
        "Yongming Rao"
      ],
      "year": "2024",
      "venue": "Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution",
      "arxiv": "arXiv:2409.12961"
    },
    {
      "citation_id": "34",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2024",
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)"
    },
    {
      "citation_id": "35",
      "title": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "Xinhao Mei",
        "Chutong Meng",
        "Haohe Liu",
        "Qiuqiang Kong",
        "Tom Ko",
        "Chengqi Zhao",
        "Mark Plumbley",
        "Yuexian Zou",
        "Wenwu Wang"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "36",
      "title": "OpenAI. Gpt-4 technical report",
      "year": "2023",
      "venue": "OpenAI. Gpt-4 technical report"
    },
    {
      "citation_id": "37",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "38",
      "title": "OpenAI. Gpt-4o system card",
      "year": "2024",
      "venue": "OpenAI. Gpt-4o system card"
    },
    {
      "citation_id": "39",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "40",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "41",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "42",
      "title": "Hicmae: Hierarchical contrastive masked autoencoder for self-supervised audio-visual emotion recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Hicmae: Hierarchical contrastive masked autoencoder for self-supervised audio-visual emotion recognition",
      "arxiv": "arXiv:2401.05698"
    },
    {
      "citation_id": "43",
      "title": "Gemini: A family of highly capable multimodal models",
      "authors": [
        "Gemini Team"
      ],
      "year": "2024",
      "venue": "Gemini: A family of highly capable multimodal models"
    },
    {
      "citation_id": "44",
      "title": "Qwen team. Qwen2-vl",
      "year": "2009",
      "venue": "Qwen team. Qwen2-vl"
    },
    {
      "citation_id": "45",
      "title": "Qwen2.5: A party of foundation models",
      "authors": [
        "Qwen Team"
      ],
      "year": "2005",
      "venue": "Qwen2.5: A party of foundation models"
    },
    {
      "citation_id": "46",
      "title": "A comparison of discrete and soft speech units for improved voice conversion",
      "authors": [
        "Marc-Andr Benjamin Van Niekerk",
        "Julian Carbonneau",
        "Matthew Zadi",
        "Hugo Baas",
        "Herman Seut",
        "Kamper"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "47",
      "title": "Tarsier: Recipes for training and evaluating large video description models",
      "authors": [
        "Jiawei Wang",
        "Liping Yuan",
        "Yuchen Zhang",
        "Haomiao Sun"
      ],
      "year": "2024",
      "venue": "Tarsier: Recipes for training and evaluating large video description models"
    },
    {
      "citation_id": "48",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Yiwen Huang",
        "Zhongying Liu",
        "Shuyong Gao",
        "Wei Zhang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos"
    },
    {
      "citation_id": "49",
      "title": "Internvideo2: Scaling video foundation models for multimodal video understanding",
      "authors": [
        "Yi Wang",
        "Kunchang Li",
        "Xinhao Li",
        "Jiashuo Yu",
        "Yinan He",
        "Guo Chen",
        "Baoqi Pei",
        "Rongkun Zheng",
        "Jilan Xu",
        "Zun Wang"
      ],
      "year": "2024",
      "venue": "Internvideo2: Scaling video foundation models for multimodal video understanding",
      "arxiv": "arXiv:2403.15377"
    },
    {
      "citation_id": "50",
      "title": "Videollamb: Long video understanding with recurrent memory bridges",
      "authors": [
        "Yuxuan Wang",
        "Cihang Xie",
        "Yang Liu",
        "Zilong Zheng"
      ],
      "year": "2024",
      "venue": "arxiv"
    },
    {
      "citation_id": "51",
      "title": "Mini-omni: Language models can hear, talk while thinking in streaming",
      "authors": [
        "Zhifei Xie",
        "Changqiao Wu"
      ],
      "year": "2024",
      "venue": "Mini-omni: Language models can hear, talk while thinking in streaming"
    },
    {
      "citation_id": "52",
      "title": "Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities",
      "authors": [
        "Zhifei Xie",
        "Changqiao Wu"
      ],
      "year": "2024",
      "venue": "Mini-omni2: Towards open-source gpt-4o with vision, speech and duplex capabilities"
    },
    {
      "citation_id": "53",
      "title": "Pllava : Parameter-free llava extension from images to videos for video dense captioning",
      "authors": [
        "Lin Xu",
        "Yilin Zhao",
        "Daquan Zhou",
        "Zhijie Lin",
        "See Ng",
        "Jiashi Feng"
      ],
      "year": "2024",
      "venue": "Pllava : Parameter-free llava extension from images to videos for video dense captioning"
    },
    {
      "citation_id": "54",
      "title": "Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis",
      "authors": [
        "Qize Yang",
        "Detao Bai",
        "Yi-Xing Peng",
        "Xihan Wei"
      ],
      "year": "2025",
      "venue": "Omni-emotion: Extending video mllm with detailed face and audio modeling for multimodal emotion analysis",
      "arxiv": "arXiv:2501.09502"
    },
    {
      "citation_id": "55",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yi Zhou",
        "Junyan Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi",
        "Chenliang Li",
        "Yuanhong Xu",
        "Hehong Chen",
        "Junfeng Tian",
        "Qiang Qi",
        "Ji Zhang",
        "Feiyan Huang"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "56",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "57",
      "title": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Haowei Liu",
        "Qi Qian",
        "Ji Zhang",
        "Fei Huang",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration"
    },
    {
      "citation_id": "58",
      "title": "Sigmoid loss for language image pre-training",
      "authors": [
        "Xiaohua Zhai",
        "Basil Mustafa",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "59",
      "title": "Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition",
      "authors": [
        "Binbin Zhang",
        "Hang Lv",
        "Pengcheng Guo",
        "Qijie Shao",
        "Chao Yang",
        "Lei Xie",
        "Xin Xu",
        "Hui Bu",
        "Xiaoyu Chen",
        "Chenchen Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "60",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "61",
      "title": "Internlmxcomposer2.5-omnilive: A comprehensive multimodal system for long-term streaming video and audio interactions",
      "authors": [
        "Pan Zhang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Yuhang Zang",
        "Rui Qian",
        "Xilin Wei",
        "Lin Chen",
        "Yifei Li",
        "Junbo Niu",
        "Shuangrui Ding",
        "Qipeng Guo",
        "Haodong Duan",
        "Xin Chen",
        "Han Lv",
        "Zheng Nie",
        "Min Zhang",
        "Bin Wang",
        "Wenwei Zhang",
        "Xinyue Zhang",
        "Jiaye Ge",
        "Wei Li",
        "Jingwen Li",
        "Zhongying Tu",
        "Conghui He",
        "Xingcheng Zhang",
        "Kai Chen",
        "Yu Qiao",
        "Dahua Lin",
        "Jiaqi Wang"
      ],
      "year": "2024",
      "venue": "Internlmxcomposer2.5-omnilive: A comprehensive multimodal system for long-term streaming video and audio interactions",
      "arxiv": "arXiv:2412.09596"
    },
    {
      "citation_id": "62",
      "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "authors": [
        "Renrui Zhang",
        "Jiaming Han",
        "Chris Liu",
        "Peng Gao",
        "Aojun Zhou",
        "Xiangfei Hu",
        "Shilin Yan",
        "Pan Lu",
        "Hongsheng Li",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "arxiv": "arXiv:2303.16199"
    },
    {
      "citation_id": "63",
      "title": "Llava-next: A strong zero-shot video understanding model",
      "authors": [
        "Yuanhan Zhang",
        "Bo Li",
        "Liu",
        "Liangke Yong Jae Lee",
        "Di Gui",
        "Jiashi Fu",
        "Ziwei Feng",
        "Chunyuan Liu",
        "Li"
      ],
      "year": "2009",
      "venue": "Llava-next: A strong zero-shot video understanding model"
    },
    {
      "citation_id": "64",
      "title": "Facial dynamics in video: Instruction tuning for improved facial expression perception and contextual awareness",
      "authors": [
        "Jiaxing Zhao",
        "Boyuan Sun",
        "Xiang Chen",
        "Xihan Wei"
      ],
      "year": "2025",
      "venue": "Facial dynamics in video: Instruction tuning for improved facial expression perception and contextual awareness"
    },
    {
      "citation_id": "65",
      "title": "Llava-octopus: Unlocking instruction-driven adaptive projector fusion for video understanding",
      "authors": [
        "Jiaxing Zhao",
        "Boyuan Sun",
        "Xiang Chen",
        "Xihan Wei",
        "Qibin Hou"
      ],
      "year": "2025",
      "venue": "Llava-octopus: Unlocking instruction-driven adaptive projector fusion for video understanding"
    },
    {
      "citation_id": "66",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Zengqun Zhao",
        "Ioannis Patras"
      ],
      "year": "2023",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "67",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}