{
  "paper_id": "2108.09669v1",
  "title": "Using Large Pre-Trained Models With Cross-Modal Attention For Multi-Modal Emotion Recognition",
  "published": "2021-08-22T09:01:52Z",
  "authors": [
    "Krishna D N"
  ],
  "keywords": [
    "multimodal emotion recognition",
    "cross-modal attention",
    "self-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, self-supervised pre-training has shown significant improvements in many areas of machine learning, including speech and NLP. We propose using large self-supervised pretrained models for both audio and text modality with crossmodality attention for multimodal emotion recognition. We use Wav2Vec2.0 [1] as an audio encoder base for robust speech features extraction and the BERT model  [2]  as a text encoder base for better contextual representation of text. These high capacity models trained on large amounts of unlabeled data contain rich feature representations and improve the downstream task's performance. We use the cross-modal attention [3] mechanism to learn alignment between audio and text representations from self-supervised models. Cross-modal attention also helps in extracting interactive information between audio and text features. We obtain utterance-level feature representation from framelevel features using statistics pooling for both audio and text modality and combine them using the early fusion technique. Our experiments show that the proposed approach obtains a 1.88% absolute improvement in accuracy compared to the previous state-of-the-art method [3] on the IEMOCAP dataset  [35] . We also conduct unimodal experiments for both audio and text modalities and compare them with previous best methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition is one of the main components of human-computer interaction systems. Humans express emotions in many different modalities, including speech, facial expression, and language. Much of previous research has shown that speech emotion is recognition is one of the challenging problems in a speech community. Building emotion recognition models by combining multiple modalities have shown improvements in emotion recognition over the recent years. In this paper, we study multimodal emotion recognition by using both audio and text.\n\nBefore the deep learning era, many researchers proposed classical machine learning algorithms like Support vector machines, hidden Markov models, and Gaussian mixture models. Hidden Markov models and SVM's based methods were used in  [4]  for speech emotion recognition. Work done by  [5]  proposes Gaussian Mixture models for the emotion recognition task. Recent studies show that deep learning-based models outperform classical methods for many areas of speech, including speech recognition  [6] , speaker recognition  [7] , speaker diarization  [8] , and so on. Most of the recent works on speech emotion recognition involve deep learning due to its effectiveness in extracting high-level feature representations from the audio, improving classification accuracy  [10, 11, 12] . Convolution neural network has also been used for speech emotion recognition by  [11] , and they perform well for SER task. Since speech is a temporal sequence, Recurrent neural networks are the best fit for processing speech signals.  [12]  shows that using Recurrent neural networks(Bi-LSTM) is better for extracting high-level features and improving speech emotion recognition accuracy. Work done by  [14]  uses phoneme embeddings extracted from the text as extra information during classification.  [15]  uses speech embeddings along with acoustic features for improving emotion recognition compared to word embedding-based models. Attention-based methods  [3, 34, 16, 37]  have been used for emotion recognition task. Cross-modal attention was introduced in  [3]  to learn alignment between speech and text, and this helps in learning and selecting features from high-level feature sequences for improving emotion recognition accuracy.\n\nBuilding supervised emotion recognition models need a large amount of labeled data, and collecting labeled data is costly. Recent trends in self-supervised models  [30]  have shown us how to leverage unlabeled data to learn a good feature representation. These self-supervised models are then can be finetuned with a small amount of labeled data for better classification. For speech application, the self-supervised models are training with different types of objective functions, including Contrastive predictive coding (CPC)  [31] , masked predictive coding  [32] , auto-regressive predictive coding  [33] . CPC has been widely used in many self-supervised models recently. The wav2vec2.0  [1]  is one such model trained on ∼53K hours of unlabelled audio data in a self-supervised fashion using CPC objective. Work by  [1]  shows that wav2vec2.0 can be fine-tuned with just 10mins of labeled audio data and achieve state-of-theart WER for English. Similarly, BERT  [2]  model trained on 3.3 Billion word tokens using self-supervised learning has shown great promise in downstream NLP applications.\n\nIn this work, we propose to use large self-supervised pretrained models along with cross-modal attention for the multimodal emotion recognition task. Our architecture consists of an audio encoder base, an audio feature encoder, CMA-1 (Cross-Modal Attention) as part of the audio network, a text encoder base, and CMA-2 in the text network as shown in Figure  1 . The audio encoder base uses wav2vec2.0  [1]  model, and the text encoder use BERT  [2]  architecture. We use cross-modal attention layers to learn interactive information between the two modalities by aligning speech features and text features in both directions. These aligned features are pooled using statistics pooling and concatenated to get utterance level feature representation. The classification layer takes the utterance level features vectors and predicts the emotion label. We conduct all the experiments on the IEMOCAP dataset.\n\nThe organization of the paper is as follows. Section 2 explains our proposed approach in detail. In section 3, we give details of the dataset. In section 4, we explain our experimental setup in detail. Finally, section 5 describes our results.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "Self-supervised learning has been showing great promise in machine learning. The idea of self-supervised learning is to leverage unlabeled data to learn a good representation of data for downstream applications. This work shows that we can jointly fine-tune the pre-trained model in a multi-modal setting for better emotion recognition performance. We use the cross-modal attention  [3]  mechanism to learn the alignment between audio and text features, which helps select relevant features for improving overall system performance. Our network architecture is shown in Figure  1 . The model has a two-streams in the network called the audio network and text network. The audio network contains an audio encoder base, an audio feature encoder, and a CMA-1 block. On the other hand, the text network contains a text encoder base and CMA-2 block, as shown in Fig 1 . The audio encoder base is initialized with pre-trained wav2vec2.0 [1] model 1  weights. Similarly, the text encoder base is initialized with BERT [2] model weights  2  . The audio feature encoder consists of 2 layers of 1-D convolution followed by a single BLSTM layer. The audio feature encoder helps in reducing the frame rate of the audio features for cross-modal attention. The feature vectors from the text encoder base are projected into a lower dimension using a single projection layer. These feature sequences from the audio feature encoder and text encoder base are fed into a cross-modal attention module in the audio network and text network, as shown in Figure  1 . The cross-modal attention (CMA-1) module in the audio network takes the audio feature encoder's output from the last BLSTM layer as query vectors and output of the text encoder base as key and value vectors and applies multi-head scaled dot product attention. Similarly, The cross-modal attention (CMA-2) module in the text network (CMA-2) takes the output from the text encoder base after projection as query vectors and output from BLSTM of the audio feature encoder as key and value vectors and applies multi-head scaled dot product attention. This bidirectional cross-modal attention mechanism helps in learning alignment from both modalities in both directions. The CMA-1 and CMA-2 also help in learning and selecting features which more relevant and helpful for emotion recognition. We finally pool the features from both CMA-1 and CMA-2 using the statistics pooling layer. These pooled features are concatenated to form an utterance-level feature vector and fed to the classification layer to predict the emotion label. We explain these blocks in detail in the following section.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Audio Encoder Base",
      "text": "The Audio encoder base consists of a pre-trained Wav2Vec2.0 architecture, as shown in Figure  1  Studies have shown that these kinds of self-supervised models are very useful for many downstream speech applications.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Audio Feature Encoder",
      "text": "The Audio feature encoder consists of two 1D convolution blocks followed by a single BLSTM layer. The Audio feature encoder takes a sequence of contextual features C from the last layer of the wav2vec2.0 model as input and performs a 1D convolution operation on the feature sequences. Each convolutional block consists of a convolution operation followed by Batch normalization, Relu operation. Each convolutional layer has a kernel size of 3 and stride of 2, and both use 256 filters. Since the convolutional layer does not capture temporal information, we use Bi-LSTM of 128 hidden dimensions right after the second convolution block. Let C = [c1, c2..cn, ...cN] be sequence of N feature vectors from the last layer of context encoder block from wav2vec2.0.\n\nWhere, Convolution is a sequence of 2 1D convolutional layers applied to the feature sequence C as shown in Figure  1 . After convolution, we obtain a feature sequence F A = [f1, f2.....fT] of length T (T<N). The input feature dimension for the first convolution layer is 1024, and the output dimension of the last convolution layer is 256. We consider the number of filters as the feature dimension due to the 1D convolution operation.\n\nWhere Bi-LSTM represents a single bidirectional LSTM layer whose hidden dimension is 128. H A = [h1, h2.....hT] represents the output sequence from the Bi-LSTM layer. The output feature dimension after BLSTM is 256. The Audio feature encoder reduces the frame rate by four times and helps in computing cross-modal attentions operation efficiently.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Text Encoder Base",
      "text": "We use BERT (Bidirectional Encoder Representation from Transformer) as the text encoder base in our architecture. In the NLP community, the BERT model is used for many downstream tasks because of its rich representational power. It learns representations from large unlabeled text data by conditioning both left and right contexts in each layer. Usually, when training any language models, only the left context is used to predict the future words, whereas, in BERT, the model takes in both left and right contexts. Unlike language models, BERT used Masked LM (MLM) and next sentence prediction tasks to optimize the neural network. The masked LM objective predicts the masked out words in the input text (like fill in the blanks). The task of sentence prediction is to predict if the two sentences are next to each other in the dataset or not. BERT minimizes the average loss of the two tasks during training. We use the pre-trained model released by Hugginface 3 library to initialize the text encoder base model. This pre-trained model is trained on 3.3 Billion word tokens from Book Corpus and English Wikipedia to learn a good representation of text data. The model takes a sequence of tokenized words as input and produces feature representations that contain rich contextual information. These features are of 768 dimension vectors for every token in the input. We use a projection layer to convert to reduce the dimensionality of the features. Text encoder takes sequence of N feature vectors from BERT W = [w1, w2.....wN] as input to 1D convolutional layer, where w i is a 768 dimensional feature vector for the i th token. The convolutional layer acts as a projection layer where it projects the word embedding dimension from 786 to 256 and does not alter the time dimension.\n\nWhere H T = [h1, h2.....hN] is a feature sequence after convolution operation. It can be noted that W and F T has the same number of frames due to kernel size one during convolution operation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Cross Modal Attention",
      "text": "We use the cross-modal attention mechanism proposed in  [3]  to understand speech and text representations better. The crossmodal attention block uses multi-head scaled dot product attention  [22]  to attend to different speech features and text features. The scaled dot product attention is the core building block of cross-modal attention. A detailed Cross-modal attention block is shown in Figure  2 .\n\nIt consists of M linear layers for query Key and Value matrices, where M is the number of heads in the multi-head attention. The cross-modal attention layer takes audio features and text features as input and applies attention logic to find the interactive information between different modalities. It takes the audio feature or the text features and applies linear transform to create Qi, Ki and Vi using i th linear transform where, i = [1, 2.....M ] and M is the total number of attention heads. The Qi, Ki and Vi are fed into scaled dot product attention  represents cross modal attention from text to audio modality layer followed by matrix multiplication between the value matrix and attention weights. The scaled dot product attention Ai for i th head is defined as follows.\n\nWe combine the attention output from all the heads using simple concatenation and feed them into the feed-forward layer.\n\nThe CMA-1 block is responsible for learning alignment from audio to text, and CMA-2 is responsible for learning alignment from text to audio. In the case of CMA-1, we consider H A as a query matrix and H T as the key and value matrix. Similarly, in the case of CMA-2, H T is considered as query matrix and H A as the key and value matrix. This bidirectional attention from both modalities helps capture interactive information from both directions and thus improves emotion classification accuracy. Both CMA-1 and CMA-2 consist of 8 attention heads. The audio feature matrix H A consists of T vectors, dimension 256, and text feature H T consist of N vectors of dimension 256.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Statistics Pooling",
      "text": "The statistics pooling layer separately computes the utterancelevel feature vector for both audio and text networks. It computes the mean and standard deviation across the time axis and concatenates the vectors to form an utterance-level feature vector. We use statistics pooling to get audio utterance-level features by computing the mean and standard deviation of the output from CMA-1, and similarly, we use statistics pooling to get text utterance-level features by computing the mean and standard deviation of the output from CMA-2. We perform an early fusion of audio and text features to combine both modalities. In this case, we use simple concatenation as the early fusion operator.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We conduct all our experiments on the IEMOCAP dataset  [35] . IEMOCAP is a publicly available dataset for emotion recog-nition research purposes. It contains information about facial expression along with audio and transcripts. The dataset contains the recording of the conversation between 2 people, and it is manually annotated and verified by human annotators. The dataset contains 12hrs recordings of audio and video. It has ten speakers and five sessions, and in each session, two people dialogues are recorded. These recordings are later segmented and annotated by professional annotators and validated by three different people. The dataset consists of a total of 5531 utterances from all five sessions. We use 4 emotions angry(1103), happy(1636), neutral(1708) and sad(1084). The happy data is a combination of happy and excited classes. We use the leaveone-out session validation approach to test our models as proposed in the previous publications. We keep four sessions for training and test on the remaining session, and we repeat this procedure for all five sessions. The final accuracy is the average of all the session's accuracy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "The audio encoder base consisting of a feature encoder and context encoder block. The feature encoder contains seven 1D convolution blocks and each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths  (10, 3, 3, 3, 3, 2, 2) . The context encoder of wav2vec2.0 has 24 transformer blocks with model dimension 1024, inner dimension 4096, and 16 attention heads. We freeze the feature encoder during training and update only the weights of the context network along with the rest of the model. The audio encoder base takes raw waveform signal as input and generates a 1024 dimensional feature representation of the speech signal every 20ms. The audio feature encoder block consists of 2 temporal convolution blocks and a single BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM in the audio feature encoder. Each convolution layer has 256 filters operating with a kernel size of 3 and stride 2. For text encoder base, we use a BERT model variant called BERT-base, which contains 12 layers of transformer layers with 768 hidden dimensions. Each transformer contains 12 attention heads. BERT takes a sequence of word tokens as input and generates 768-dimensional contextual representation for every token. We project these features to a smaller dimension (256) using a projection layer. We use a single layer Multi-head attention for both CMA-1 and CMA-2. Each multi-head attention layer uses eight attention heads and layer-normalization before scaled dot product operation. The forward feed layer inside MHA has a hidden dimension similar to its input, which, in our case, is 256. We use Adam optimizer  [21]  with an initial learning rate of 0.00001, and the learning rate is decayed using a learning rate scheduler based on test loss.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "This section compares our system performance to previous approaches. We use large self-supervised models for both audio and text modalities and jointly fine-tune the network with cross-modal attention for better emotion recognition performance. Comparison of model performance with previous works is shown in Table  1 . We show that using our model architecture, we can obtain 1.88% absolute improvement in unweighted accuracy compared to the previous approach  [3] . This is due to the model's ability to access rich speech and text representation from large pre-trained models. We also experiment by freezing the updates for pre-trained models during training and only using the speech and text features for updating upper layers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "System",
      "text": "Unweighted Accuracy E-Vector  [29]  57.25% MCNN + LSTM  [13]  64.33% MCCN+phoneme embeddings  [14]  68.50% H.Xu et. al  [37]  70.90% Cross Modal Transformer  [3]  72.82% Proposed Approach (no fine-tuning) 71.20% Proposed Approach",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "74.71%",
      "text": "The results of these experiments are in Table  1  (last but second row). We also conduct experiments on different modalities to see the performance variations. We conduct audio-only and text-only emotion recognition tasks by separating the audio network and text network before the early fusion stage. We use the wav2vec2.0 as the audio encoder base and audio feature encoder for audio-only experiments as described in section 4, but we remove the cross-modal attention block for this experiment. We obtain unweighted accuracy of 60.01% for audio-only experiments. We compare our results to the previous approach, as shown in Table  2 . Similarly, for the text-only experiment, we fine-tune the BERT model, and we obtain the best performance compared to previous approaches, as shown in Table  2 (last row). Audio-only TDNN+LSTM  [18]  60.70% LSTM+Attn  [28]  58.80% Self-Attn+LSTM  [3]  55.60% Wav2vec2.0+CNN+BLSTM (Ours) 60.01%\n\nText-only H.Xu et. al  [2]  57.80% Speech-Embedding  [15]  60.40% Self-Attn+LSTM  [3]  65.90% BERT fine-tuning(Ours) 71.01%",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "Speech emotion recognition is one of the most challenging and still unsolved problems in the speech community. In this work, we show how to leverage large pre-trained models to improve speech emotion recognition performance. We use wav2vec2.0 and BERT pre-trained models for speech and text modality, respectively. We show that fine-tuning these pre-trained representations along with cross-modal attention improves overall system accuracy. Our experiments show that the proposed methodology outperforms the previous approach by 1.88% absolute improvement in unweighted accuracy. We also show that our proposed approach obtains competitive accuracy for unimodal models compared to the previous best approaches.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed model architecture.",
      "page": 2
    },
    {
      "caption": "Figure 1: The model has a two-streams in the net-",
      "page": 2
    },
    {
      "caption": "Figure 1: The audio encoder base is initialized with pre-trained",
      "page": 2
    },
    {
      "caption": "Figure 1: The wav2vec2.0 model is",
      "page": 2
    },
    {
      "caption": "Figure 1: After convolution, we obtain a feature sequence FA =",
      "page": 2
    },
    {
      "caption": "Figure 2: It consists of M linear layers for query Key and Value ma-",
      "page": 3
    },
    {
      "caption": "Figure 2: Cross modal attention. CMA-1 (left) represents cross",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Freshworks Inc.": "krishna.nanjappa@freshworks.com"
        },
        {
          "Freshworks Inc.": "Abstract"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "Recently,\nself-supervised\npre-training\nhas\nshown\nsigniﬁcant"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "improvements\nin many areas of machine learning,\nincluding"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "speech and NLP. We propose using large self-supervised pre-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "trained models\nfor both audio and text modality with cross-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "modality attention for multimodal emotion recognition. We use"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "Wav2Vec2.0 [1] as an audio encoder base for robust speech fea-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "tures extraction and the BERT model [2] as a text encoder base"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "for better contextual representation of text. These high capac-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "ity models trained on large amounts of unlabeled data contain"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "rich feature representations and improve the downstream task’s"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "performance. We use the cross-modal attention [3] mechanism"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "to learn alignment between audio and text representations from"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "self-supervised models. Cross-modal attention also helps in ex-"
        },
        {
          "Freshworks Inc.": "tracting interactive information between audio and text features."
        },
        {
          "Freshworks Inc.": "We obtain utterance-level\nfeature representation from frame-"
        },
        {
          "Freshworks Inc.": "level\nfeatures using statistics pooling for both audio and text"
        },
        {
          "Freshworks Inc.": "modality and combine them using the early fusion technique."
        },
        {
          "Freshworks Inc.": "Our experiments\nshow that\nthe proposed approach obtains a"
        },
        {
          "Freshworks Inc.": "1.88% absolute improvement in accuracy compared to the pre-"
        },
        {
          "Freshworks Inc.": "vious state-of-the-art method [3] on the IEMOCAP dataset [35]."
        },
        {
          "Freshworks Inc.": "We also conduct unimodal experiments for both audio and text"
        },
        {
          "Freshworks Inc.": "modalities and compare them with previous best methods."
        },
        {
          "Freshworks Inc.": "Index Terms: multimodal emotion recognition, cross-modal at-"
        },
        {
          "Freshworks Inc.": "tention, self-supervised learning."
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "1.\nIntroduction"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "Speech emotion recognition is one of the main components of"
        },
        {
          "Freshworks Inc.": "human-computer\ninteraction systems.\nHumans express emo-"
        },
        {
          "Freshworks Inc.": "tions in many different modalities,\nincluding speech, facial ex-"
        },
        {
          "Freshworks Inc.": "pression, and language. Much of previous research has shown"
        },
        {
          "Freshworks Inc.": "that\nspeech emotion is\nrecognition is one of\nthe challenging"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "problems in a speech community. Building emotion recogni-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "tion models by combining multiple modalities have shown im-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "provements in emotion recognition over the recent years. In this"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "paper, we study multimodal emotion recognition by using both"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "audio and text."
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "Before the deep learning era, many researchers proposed"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "classical machine learning algorithms like Support vector ma-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "chines, hidden Markov models,\nand Gaussian mixture mod-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "els. Hidden Markov models and SVM’s based methods were"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "used in [4] for speech emotion recognition. Work done by [5]"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "proposes Gaussian Mixture models for the emotion recognition"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "task. Recent studies show that deep learning-based models out-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "perform classical methods for many areas of speech,\nincluding"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "speech recognition [6], speaker recognition [7], speaker diariza-"
        },
        {
          "Freshworks Inc.": ""
        },
        {
          "Freshworks Inc.": "tion [8], and so on. Most of the recent works on speech emo-"
        },
        {
          "Freshworks Inc.": "tion recognition involve deep learning due to its effectiveness"
        },
        {
          "Freshworks Inc.": "in extracting high-level feature representations from the audio,"
        },
        {
          "Freshworks Inc.": "improving classiﬁcation accuracy [10,11,12]. Convolution neu-"
        },
        {
          "Freshworks Inc.": "ral network has also been used for speech emotion recognition"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "statistics pooling layer. These pooled features are concatenated": "to form an utterance-level feature vector and fed to the classi-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "ﬁcation layer\nto predict\nthe emotion label. We explain these"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "blocks in detail in the following section."
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "2.1. Audio Encoder Base"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "The Audio encoder base consists of a pre-trained Wav2Vec2.0"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "architecture, as shown in Figure 1. The wav2vec2.0 model\nis"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "pre-trained on ∼53K hrs of English audiobook recordings. The"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "wav2vec2.0 consists of\nthree main components a Feature en-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "coder, a context encoder, and a quantization module. The fea-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "ture encoder module contains temporal convolution blocks fol-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "lowed by layer normalization and GELU activations.\nIt\ntakes"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "raw waveform X as input and outputs latent speech representa-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "tion Z. The feature encoder helps the model learn latent repre-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "sentation directly from raw audio every 20ms (assuming 16Khz"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "sampling rate). The context encoder block helps capture contex-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "tual information and high-level feature representations from the"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "latent feature sequence Z. It consists of multilayer self-attention"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "blocks similar\nto transformers, but\ninstead of ﬁxed positional"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "encoding, the model uses relative positional encoding. The con-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "text encoder block takes masked latent vectors Z as input and"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "builds contextualized representations C as outputs. The quan-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "tization block quantizes the continuous latent representations Z"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "into quantized representation Q. CPC objective function is used"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "as the main loss during pre-training of the wav2vec2.0 model."
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "It also uses the masking technique during the training, where"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "it\nrandomly masks out parts of\nlatent\nrepresentation Z before"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "passing it\nto the context encoder. During pre-training,\nthe con-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "trastive learning objective forces the model\nto distinguish the"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "quantized representation at masked time steps from a set of dis-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "tractors from other\ntime steps. This training technique seems"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "to be quite powerful for learning better speech representation."
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "Studies have shown that\nthese kinds of self-supervised models"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "are very useful for many downstream speech applications."
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "2.2. Audio Feature Encoder"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "The Audio feature\nencoder\nconsists of\ntwo 1D convolution"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "blocks\nfollowed by a single BLSTM layer.\nThe Audio fea-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "ture encoder takes a sequence of contextual features C from the"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "last layer of the wav2vec2.0 model as input and performs a 1D"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "convolution operation on the feature sequences.\nEach convo-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "lutional block consists of a convolution operation followed by"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "Batch normalization, Relu operation. Each convolutional\nlayer"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "has a kernel size of 3 and stride of 2, and both use 256 ﬁlters."
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "Since the convolutional\nlayer does not capture temporal\ninfor-"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "mation, we use Bi-LSTM of 128 hidden dimensions right after"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "the second convolution block.\nLet C = [c1, c2..cn, ...cN]"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "be sequence of N feature vectors from the last\nlayer of context"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "encoder block from wav2vec2.0."
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "FA = Convolution(C)\n(1)"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "Where, Convolution is a sequence of 2 1D convolutional"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "layers applied to the feature sequence C as shown in Figure"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "1.\nAfter convolution, we obtain a feature sequence FA ="
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "[f1, f2.....fT] of length T (T<N). The input feature dimension"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "for the ﬁrst convolution layer is 1024, and the output dimension"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "of\nthe last convolution layer\nis 256. We consider\nthe number"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "of ﬁlters as the feature dimension due to the 1D convolution"
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "operation."
        },
        {
          "statistics pooling layer. These pooled features are concatenated": ""
        },
        {
          "statistics pooling layer. These pooled features are concatenated": "HA = Bi-LSTM(FA)\n(2)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "layer whose hidden dimension is 128. HA = [h1, h2.....hT]"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "represents the output sequence from the Bi-LSTM layer. The"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "output feature dimension after BLSTM is 256. The Audio fea-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "ture encoder reduces the frame rate by four times and helps in"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "computing cross-modal attentions operation efﬁciently."
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "2.3. Text Encoder Base"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "We\nuse BERT (Bidirectional Encoder Representation\nfrom"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "Transformer) as the text encoder base in our architecture.\nIn"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "the NLP community,\nthe BERT model\nis used for many down-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "stream tasks because of its rich representational power. It learns"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "representations from large unlabeled text data by conditioning"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "both left and right contexts in each layer. Usually, when training"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "any language models, only the left context is used to predict the"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "future words, whereas, in BERT, the model takes in both left and"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "right contexts. Unlike language models, BERT used Masked"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "LM (MLM) and next sentence prediction tasks to optimize the"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "neural network. The masked LM objective predicts the masked"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "out words in the input\ntext (like ﬁll\nin the blanks). The task of"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "sentence prediction is to predict if the two sentences are next to"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "each other in the dataset or not. BERT minimizes the average"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "loss of\nthe two tasks during training. We use the pre-trained"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "model\nreleased by Hugginface 3\nlibrary to initialize the text"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "encoder base model. This pre-trained model\nis trained on 3.3"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "Billion word tokens from Book Corpus and English Wikipedia"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "to learn a good representation of\ntext data.\nThe model\ntakes"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "a sequence of\ntokenized words as input and produces feature"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "representations that contain rich contextual information. These"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "features are of 768 dimension vectors\nfor every token in the"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "input. We use a projection layer\nto convert\nto reduce the di-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "mensionality of the features. Text encoder takes sequence of N"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "feature vectors from BERT W = [w1, w2.....wN] as input to"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "1D convolutional layer, where wi is a 768 dimensional feature"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "vector for the ith\ntoken. The convolutional\nlayer acts as a pro-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "jection layer where it projects the word embedding dimension"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "from 786 to 256 and does not alter the time dimension."
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "HT = Convolution(W)\n(3)"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "Where HT = [h1, h2.....hN] is a feature sequence after"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "convolution operation.\nIt can be noted that W and FT has the"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "same number of frames due to kernel size one during convolu-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "tion operation."
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "2.4. Cross Modal Attention"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "We use the cross-modal attention mechanism proposed in [3] to"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "understand speech and text\nrepresentations better.\nThe cross-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "modal attention block uses multi-head scaled dot product atten-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "tion [22] to attend to different speech features and text features."
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "The scaled dot product attention is the core building block of"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "cross-modal attention. A detailed Cross-modal attention block"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "is shown in Figure 2."
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "It consists of M linear layers for query Key and Value ma-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "trices, where M is the number of heads in the multi-head at-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "tention.\nThe cross-modal attention layer\ntakes audio features"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "and text features as input and applies attention logic to ﬁnd the"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "interactive information between different modalities.\nIt\ntakes"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "the audio feature or\nthe text\nfeatures and applies linear\ntrans-"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "form to create Qi, Ki and Vi using ith linear transform where,"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "i = [1, 2.....M ] and M is the total number of attention heads."
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "The Qi, Ki and Vi are fed into scaled dot product attention"
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": ""
        },
        {
          "Where Bi-LSTM represents a single bidirectional LSTM": "3https://huggingface.co/"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: (last but sec-",
      "data": [
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "in the audio feature encoder. Each convolution layer has 256",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "ﬁlters operating with a kernel size of 3 and stride 2.\nFor\ntext",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "System"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "encoder base, we use a BERT model variant called BERT-base,",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "Audio-only"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "which contains 12 layers of\ntransformer\nlayers with 768 hid-",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "TDNN+LSTM [18]"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "den dimensions. Each transformer contains 12 attention heads.",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "LSTM+Attn [28]"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "BERT takes a sequence of word tokens as input and generates",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "Self-Attn+LSTM [3]"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "768-dimensional contextual representation for every token. We",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "Wav2vec2.0+CNN+BLSTM (Ours)"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "project these features to a smaller dimension (256) using a pro-",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "jection layer. We use a single layer Multi-head attention for both",
          "Bold indicates the best performance": "Text-only"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "CMA-1 and CMA-2. Each multi-head attention layer uses eight",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "H.Xu et. al [2]"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "attention heads and layer-normalization before scaled dot prod-",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "Speech-Embedding [15]"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "uct operation. The forward feed layer inside MHA has a hidden",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "Self-Attn+LSTM [3]"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "dimension similar to its input, which, in our case, is 256. We use",
          "Bold indicates the best performance": ""
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "",
          "Bold indicates the best performance": "BERT ﬁne-tuning(Ours)"
        },
        {
          "BLSTM layer. We also use a dropout of 0.2 for the Bi-LSTM": "Adam optimizer\n[21] with an initial\nlearning rate of 0.00001,",
          "Bold indicates the best performance": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: (last but sec-",
      "data": [
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "tion systems. Bold indicates the best performance"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "System\nUnweighted Accuracy"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "E-Vector [29]\n57.25%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "MCNN + LSTM [13]\n64.33%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "MCCN+phoneme embeddings [14]\n68.50%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "H.Xu et. al [37]\n70.90%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Cross Modal Transformer [3]\n72.82%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Proposed Approach (no ﬁne-tuning)\n71.20%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Proposed Approach\n74.71%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "The results of\nthese experiments are in Table 1 (last but sec-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "ond row). We also conduct experiments on different modalities"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "to see the performance variations. We conduct audio-only and"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "text-only emotion recognition tasks by separating the audio net-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "work and text network before the early fusion stage. We use the"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "wav2vec2.0 as\nthe audio encoder base and audio feature en-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "coder for audio-only experiments as described in section 4, but"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "we remove the cross-modal attention block for this experiment."
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "We obtain unweighted accuracy of 60.01% for audio-only ex-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "periments. We compare our\nresults to the previous approach,"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "as shown in Table 2.\nSimilarly,\nfor\nthe text-only experiment,"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "we ﬁne-tune the BERT model, and we obtain the best perfor-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "mance compared to previous approaches,\nas\nshown in Table"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "2(last row)."
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Table 2: Comparison of uni-modal emotion recognition models."
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Bold indicates the best performance"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "System\nUnweighted Accuracy"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Audio-only"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "TDNN+LSTM [18]\n60.70%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "LSTM+Attn [28]\n58.80%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Self-Attn+LSTM [3]\n55.60%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Wav2vec2.0+CNN+BLSTM (Ours)\n60.01%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Text-only"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "H.Xu et. al [2]\n57.80%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Speech-Embedding [15]\n60.40%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Self-Attn+LSTM [3]\n65.90%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "BERT ﬁne-tuning(Ours)\n71.01%"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "6. Conclusions"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": ""
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "Speech emotion recognition is one of the most challenging and"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "still unsolved problems in the speech community.\nIn this work,"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "we show how to leverage large pre-trained models to improve"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "speech emotion recognition performance. We use wav2vec2.0"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "and BERT pre-trained models for speech and text modality, re-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "spectively. We show that ﬁne-tuning these pre-trained represen-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "tations along with cross-modal attention improves overall sys-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "tem accuracy. Our experiments show that the proposed method-"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "ology outperforms\nthe previous approach by 1.88% absolute"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "improvement\nin unweighted accuracy. We also show that our"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "proposed approach obtains competitive accuracy for unimodal"
        },
        {
          "Table 1: Comparison of previous multimodal emotion recogni-": "models compared to the previous best approaches."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "T. B J, “Language Independent Gender\nIdentiﬁcation from Raw"
        },
        {
          "7. References": "[1]\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “Wav2vec 2.0:",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Waveform Using Multi-Scale Convolutional Neural Networks,”"
        },
        {
          "7. References": "A framework for self-supervised learning of speech representa-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "ICASSP 2020 - 2020 IEEE International Conference on Acous-"
        },
        {
          "7. References": "tions,” Advances in Neural Information Processing Systems, vol.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "tics, Speech and Signal Processing (ICASSP), Barcelona, Spain,"
        },
        {
          "7. References": "33, 2020.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "2020, pp. 6559-6563."
        },
        {
          "7. References": "[2]\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[20]\nJ. Pennington, R. Socher, and C. Manning, “Glove: Global vectors"
        },
        {
          "7. References": "training of deep bidirectional\ntransformers\nfor\nlanguage under-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "the 2014 conference\nfor word representation,” in Proceedings of"
        },
        {
          "7. References": "standing,” in NAACLHLT, 2019.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "on empirical methods in natural\nlanguage processing (EMNLP),"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "2014, pp. 1532–1543."
        },
        {
          "7. References": "[3]\nN., K.D., Patil, A. (2020) “Multimodal Emotion Recognition Us-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "ing Cross-Modal Attention and 1D Convolutional Neural Net-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[21] Diederik P. Kingma\nand\nJimmy Ba,\n“Adam:\nA Method\nfor"
        },
        {
          "7. References": "works.” Proc. Interspeech 2020, 4243-4247.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "the\nInternational\nStochastic Optimization”,\nIn Proceedings of"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Conference on Learning Representations (ICLR), 2014"
        },
        {
          "7. References": "[4]\nY\n.L.LI, G. Wei,\n“Speech emotion recognition based on HMM",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N."
        },
        {
          "7. References": "the Fourth International Conference\nand SVM”, Proceedings of",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”"
        },
        {
          "7. References": "on Machine Learning and Cybernetics, Vol.8, 18-21 Aug. 2005,",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "in Advances in Neural Information Processing Systems, 2017, pp."
        },
        {
          "7. References": "pp.4898 4901",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "5998–6008."
        },
        {
          "7. References": "[5]\nD. Neiberg, K. Elenius, and K. Laskowski, “Emotion recognition",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[23]\nS. Yoon, S. Byun,\nand K.\nJung,\n“Multimodal\nspeech emotion"
        },
        {
          "7. References": "in spontaneous speech using GMMs,” in Ninth International Con-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "recognition using audio and text,” in IEEE SLT, 2018."
        },
        {
          "7. References": "ference on Spoken Language Processing, 2006",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[24] A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P. Morency,"
        },
        {
          "7. References": "[6]\nW. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, “Listen, Attend and",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "“Tensor\nfusion network for multimodal\nsentiment analysis,”\nin"
        },
        {
          "7. References": "Spell: A Neural Network for Large Vocabulary Conversational",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Proceedings of\nthe 2017 Conference on Empirical Methods\nin"
        },
        {
          "7. References": "Speech Recognition”, in ICASSP, 2016",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Natural Language Processing, 2017, pp. 1103–1114."
        },
        {
          "7. References": "[7]\nD. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[25]\nS. Poria, E. Cambria, D. Hazarika, N. Majumder, A. Zadeh, and"
        },
        {
          "7. References": "pur, “X-vectors: Robust DNN embeddings for speaker\nrecogni-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "L.-P. Morency,\n“Context-dependent\nsentiment analysis\nin user-"
        },
        {
          "7. References": "IEEE International Conference on Acoustics, Speech and\ntion”,",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "the 55th Annual Meeting of\ngenerated videos,” in Proceedings of"
        },
        {
          "7. References": "Signal Processing (ICASSP) 2018, April 2018, pp. 53295333.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "the Association for Computational Linguistics (Volume 1: Long"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Papers), vol. 1, 2017, pp. 873–883."
        },
        {
          "7. References": "[8]\nA. Zhang, Q. Wang, Z. Zhu, J. Paisley and C. Wang, “Fully Su-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[26] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nico-"
        },
        {
          "7. References": "pervised Speaker Diarization”, ICASSP 2019 - 2019 IEEE Inter-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "laou, B. Schuller, and S. Zafeiriou, “Adieu features?\nend-to-end"
        },
        {
          "7. References": "national Conference on Acoustics, Speech and Signal Processing",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "speech emotion recognition using a deep convolutional recurrent"
        },
        {
          "7. References": "(ICASSP), Brighton, United Kingdom, 2019, pp. 6301-6305.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "network,” in 2016 ICASSP. IEEE, 2016, pp. 5200– 5204."
        },
        {
          "7. References": "[9]\nD. Bahdanau, K. Cho,\nand Y. Bengio,\n“Neural machine trans-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[27]\nPaszke, Adam and Gross,\nSam and Chintala,\nSoumith\nand"
        },
        {
          "7. References": "lation by jointly learning to align and translate”, arXiv preprint",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Chanan, Gregory and Yang, Edward and DeVito, Zachary and"
        },
        {
          "7. References": "arXiv:1409.0473, 2014",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer,"
        },
        {
          "7. References": "[10] K. Han, D. Yu, and I. Tashev,“Speech emotion recognition using",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "“Adam: Automatic differentiation in PyTorch”, in NIPS, 2017"
        },
        {
          "7. References": "deep neural network and extreme learning machine,” in Fifteenth",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[28]\nS. Mirsamadi, E. Barsoum,\nand C. Zhang,\n“Automatic speech"
        },
        {
          "7. References": "annual conference of the international speech communication as-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "emotion recognition using recurrent neural networks with local"
        },
        {
          "7. References": "sociation, 2014.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "attention,” in IEEE International Conference on Acoustics,Speech"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "and Signal Processing. IEEE, 2017"
        },
        {
          "7. References": "[11] D. Bertero and P. Fung, “A ﬁrst\nlook into a convolutional neu-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "ral network for speech emotion detection,” in IEEE International",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[29] Q. Jin, C. Li, S. Chen, and H. Wu, “Speech emotion recognition"
        },
        {
          "7. References": "Conference on Acoustics, Speech and Signal Processing (ICASSP",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "with acoustic and lexical features,” in Acoustics, Speech and Sig-"
        },
        {
          "7. References": "2017), New Orleans, LA, USA, March 2017, pp. 51155119.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "nal Processing (ICASSP), 2015 IEEE International Conference"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "on. IEEE, 2015, pp. 4749–4753"
        },
        {
          "7. References": "[12]\nJ. Lee and I. Tashev, “High-level feature representation using re-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "current neural network for speech emotion recognition,” in Six-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[30]\nJaiswal, Ashish, A. R. Babu, Mohammad Zaki Zadeh, D. Baner-"
        },
        {
          "7. References": "teenth Annual Conference of the International Speech Communi-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "jee and F. Makedon. “ A Survey on Contrastive Self-supervised"
        },
        {
          "7. References": "cation Association, 2015.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Learning.” ArXiv abs/2011.00362 (2020):"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[31] A. van den Oord, Y. Li, and O. Vinyals. “ Representation learning"
        },
        {
          "7. References": "[13] Cho, J., Pappagari, R., Kulkarni, P., Villalba, J., Carmiel, Y., De-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "with contrastive predictive coding.” arXiv, abs/1807.03748, 2018."
        },
        {
          "7. References": "hak, N, “Deep Neural Networks for Emotion Recognition Com-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "bining Audio and Transcripts”, Proc. Interspeech 2018.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[32] Ruixiong Zhang, Haiwei Wu, Wubo Li, Dongwei Jiang, Wei Zou,"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "and Xiangang Li, “Transformer based unsupervised pre-training"
        },
        {
          "7. References": "[14]\nP. Yenigalla, A. Kumar, S. Tripathi, C. Singh, S. Kar, and J. Vepa,",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "for acoustic representation learning,” CoRR, vol. abs/2007.14602,"
        },
        {
          "7. References": "“Speech emotion recognition using spectrogram and phoneme",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "2020."
        },
        {
          "7. References": "embedding”, Proc. Interspeech 2018, pp. 3688–3692, 2018.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[33] C. Yu-An, H. Wei-Ning, T. Hao, and G. James, “An unsupervised"
        },
        {
          "7. References": "[15] N, Krishna\nand Reddy,\nSai,\n“Multi-Modal\nSpeech Emotion",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "autoregressive model\nfor speech representation learning,” Inter-"
        },
        {
          "7. References": "Recognition Using Speech Embeddings\nand Audio Features”,",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "speech 2019, Sep 2019."
        },
        {
          "7. References": "AVSP 2019 Melbourne ,Autralia",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[34] Xu H, Zhang H, Han K, Wang Y, Peng Y, Li X, “Learning Align-"
        },
        {
          "7. References": "[16] Yao-Hung Hubert\nTsai,\nShaojie Bai,\nPaul\nPu\nLiang,\n,\nJ.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "ment for Multimodal Emotion Recognition from Speech”, Proc."
        },
        {
          "7. References": "Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov,",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "Interspeech 2019, 3569-3573"
        },
        {
          "7. References": "“Multimodal Transformer\nfor Unaligned Multimodal Language",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[35] C. Busso, M. Bulut, C.-C.Lee, A. Kazemzadeh, E. Mower,S. Kim,"
        },
        {
          "7. References": "the Annual Meeting of\nthe Asso-\nSequences”,\nIn Proceedings of",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "J. N. Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive"
        },
        {
          "7. References": "ciation for Computational Linguistics (ACL), 2019.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "emotional dyadic motion capture database,” Language resources"
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "and evaluation, vol. 42, no. 4, p. 335, 2008"
        },
        {
          "7. References": "[17] M. Ravanelli and Y. Bengio,\n“Speaker Recognition from Raw",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "Waveform with SincNet,” 2018 IEEE Spoken Language Technol-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[36]\nSchmidhuber,\nJ¨urgen,\n“Deep learning in neural networks: An"
        },
        {
          "7. References": "ogy Workshop (SLT), Athens, Greece, 2018, pp. 1021-1028.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "overview.” Neural networks”, 61 (2015): 85-117."
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "[37]\nS. Siriwardhana, A. Reis, R. Weerasekera, and S.Nanayakkara,"
        },
        {
          "7. References": "[18] M. Sarma, P. Ghahremani, D. Povey, N. K. Goel, K. K. Sarma,and",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "“Jointly ﬁne-tuning” bert-like” self supervised models to improve"
        },
        {
          "7. References": "N. Dehak, “Emotion identiﬁcation from raw speech signals us-",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "multimodal\nspeech\nemotion\nrecognition,”\narXiv:2008.06682,"
        },
        {
          "7. References": "ing DNNs,” in Proc. INTERSPEECH, Hyderabad, India,2018, pp.",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        },
        {
          "7. References": "",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": "2020."
        },
        {
          "7. References": "3097–3101",
          "[19] K. D N, A. D, S. S. Reddy, A. Acharya, P. A. Garapati\nand": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "3",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "4",
      "title": "Multimodal Emotion Recognition Using Cross-Modal Attention and 1D Convolutional Neural Networks",
      "authors": [
        "K Patil"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition based on HMM and SVM",
      "authors": [
        "Y Li",
        "G Wei"
      ],
      "year": "2005",
      "venue": "Proceedings of the Fourth International Conference on Machine Learning and Cybernetics"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in spontaneous speech using GMMs",
      "authors": [
        "D Neiberg",
        "K Elenius",
        "K Laskowski"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition",
      "authors": [
        "W Chan",
        "N Jaitly",
        "Q Le",
        "O Vinyals"
      ],
      "year": "2016",
      "venue": "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition"
    },
    {
      "citation_id": "8",
      "title": "X-vectors: Robust DNN embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Fully Supervised Speaker Diarization",
      "authors": [
        "A Zhang",
        "Q Wang",
        "Z Zhu",
        "J Paisley",
        "C Wang"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "12",
      "title": "A first look into a convolutional neural network for speech emotion detection",
      "authors": [
        "D Bertero",
        "P Fung"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "Sixteenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "14",
      "title": "Deep Neural Networks for Emotion Recognition Combining Audio and Transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition using spectrogram and phoneme embedding",
      "authors": [
        "P Yenigalla",
        "A Kumar",
        "S Tripathi",
        "C Singh",
        "S Kar",
        "J Vepa"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Multi-Modal Speech Emotion Recognition Using Speech Embeddings and Audio Features",
      "authors": [
        "Krishna Reddy"
      ],
      "year": "2019",
      "venue": "AVSP"
    },
    {
      "citation_id": "17",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "18",
      "title": "Speaker Recognition from Raw Waveform with SincNet",
      "authors": [
        "M Ravanelli",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "19",
      "title": "Emotion identification from raw speech signals using DNNs",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "20",
      "title": "Language Independent Gender Identification from Raw Waveform Using Multi-Scale Convolutional Neural Networks",
      "authors": [
        "S Reddy",
        "A Acharya",
        "P Garapati"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "22",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Proceedings of the International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "23",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "IEEE SLT"
    },
    {
      "citation_id": "25",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Context-dependent sentiment analysis in usergenerated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network"
    },
    {
      "citation_id": "28",
      "title": "Adam: Automatic differentiation in PyTorch",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Soumith Chintala",
        "Gregory Chanan",
        "Edward Devito",
        "Zachary Lin",
        "Zeming Desmaison",
        "Alban Antiga",
        "Luca Lerer"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "29",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics,Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on"
    },
    {
      "citation_id": "31",
      "title": "A Survey on Contrastive Self-supervised Learning",
      "authors": [
        "Ashish Jaiswal",
        "A Babu",
        "Mohammad Zaki Zadeh",
        "D Banerjee",
        "F Makedon"
      ],
      "year": "2020",
      "venue": "A Survey on Contrastive Self-supervised Learning"
    },
    {
      "citation_id": "32",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Van Den Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding"
    },
    {
      "citation_id": "33",
      "title": "Transformer based unsupervised pre-training for acoustic representation learning",
      "authors": [
        "Ruixiong Zhang",
        "Haiwei Wu",
        "Wubo Li",
        "Dongwei Jiang",
        "Wei Zou",
        "Xiangang Li"
      ],
      "year": "2007",
      "venue": "CoRR"
    },
    {
      "citation_id": "34",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "C Yu-An",
        "H Wei-Ning",
        "T Hao",
        "G James"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "35",
      "title": "Learning Alignment for Multimodal Emotion Recognition from Speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "37",
      "title": "Deep learning in neural networks: An overview",
      "authors": [
        "Jürgen Schmidhuber"
      ],
      "year": "2015",
      "venue": "Neural networks"
    },
    {
      "citation_id": "38",
      "title": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning\" bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "arxiv": "arXiv:2008.06682"
    }
  ]
}