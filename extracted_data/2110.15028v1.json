{
  "paper_id": "2110.15028v1",
  "title": "Facial Emotion Recognition: A Multi-Task Approach Using Deep Learning",
  "published": "2021-10-28T11:23:00Z",
  "authors": [
    "Aakash Saroop",
    "Pathik Ghugare",
    "Sashank Mathamsetty",
    "Vaibhav Vasani"
  ],
  "keywords": [
    "Facial Emotion Recognition",
    "Multi-task learning",
    "Multi-output model",
    "Convolutional Neural Networks",
    "Deep Learning",
    "Image Processing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Emotion Recognition is an inherently difficult problem, due to vast differences in facial structures of individuals and ambiguity in the emotion displayed by a person. Recently, a lot of work is being done",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As AI continues to become an increasing integral part of our lives, the need for machines to understand the state of the human emotions is of critical importance. This has the potential to take human computer interaction to the next level, with a direct impact in the field of voice assistants, mental health therapy, recommendation systems etc.\n\nSince there are unique local languages and local moral orders, different cultures can use the same emotion and expression in very different ways  [1] . This makes facial emotion recognition is an ambiguous task as each person picks up visual cues differently. The State of the art algorithms for Facial Emotion Recognition (FER) give much lower accuracy on a reasonably large dataset obtained from real world images taken in a non controlled environment  [2, 17] , as compared to other computer vision tasks like object detection  [18] , image classification  [19]  etc.\n\nIn this paper, we hypothesise that a Convolutional Neural Network, subjected to multi-task learning on the same data, i.e. learning to predict emotion, age, gender and race of a subject from the same facial image would perform better on individual tasks, by transferring knowledge across different domains. The summary of contributions through this paper is:\n\n• Validation that the proposed multi-task learning approach of combining FER with age, gender and race classification outperforms the multi-task approach of combining FER and facial action unit detection and the conventional single-task learning approach.\n\n• Presenting a CNN architecture which gives outputs corresponding to all the labels mentioned above.\n\nThe rest of this paper is organised as follows: Related work has been reviewed in Section II. The proposed approach has been discussed in Section III. Experimental setup has been shared in Section IV. Experimental results are shared in Section V. Further discussions on the observations made are discussed in Section VI. Conclusion and future work are suggested in Section VII.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "The work done in the field of FER can be broadly divided into two categories based on whether the features were hand-crafted or generate through a Neural Network. Before the widespread use of CNNs in the field of computer vision, FER used to be carried out by identifying facial components or landmarks from the facial region  [23, 24, 25, 26] . Recent work in the field of Facial Emotion Recognition deals with using Real World Images instead of taking images in a controlled environment. This leads to several pre-processing methods being used on the images obtained . Some of these approaches are used for preprocessing the images, before feeding them to a CNN  [2, 5] . In  [2] , feature extraction approaches such as HoG, LBP are used to extract features from the image, which requires relatively lower computing power and memory than Deep Learning approaches. In  [5] , Milad Zadeh et. al. discusses the use of pre-processing method of Gabor filters to increase the accuracy of Facial emotion recognition. In this paper, a very small dataset  [12]  of 213 images which lead to overfitting on the Neural Network.  [14]  draws the Bezier curve on the eye and mouth and classifies the emotion of the characteristic with Hausdroff distance.\n\nEver since the in popularity of Deep Learning, CNNs have been used for FER, which provide the output by enabling \"end-to-end\" learning to occur in the pipeline directly from the input images  [27] . In  [4] , Mehdipour Ghazi et. al. discusses an approach to deal with Emotion recognition using DL under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localisation. They have proposed that using preprocessing methods for pose and illumination normalisation along with pre-trained deep learning models or accounting for these variations during training substantially resolve this weakness present in the dataset.  [15]  approaches the problem of emotion classification based on thermal images of the face. The ideas presented in this paper have been heavily built upon the work done in  [3] . Gerard Pons et. al. take the work of multi-task, multi-label and multidomain learning with residual convolutional networks for emotion recognition. They had claimed that using the same CNN, when trained to predict both emotion and facial action units, it performed better at each individual task. In this paper, it was suggested that other features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Aprroach",
      "text": "Here the approach of multi-task learning, i.e. a single model generating labels for emotion, gender, age and race, is to be compared with the approach of singletask learning, i.e. a model predicting only emotion of a subject, as shown in Figure1. The CNN architecture used for this study is shown in Figure2. More details about the CNN can be found in the appendix.\n\nEach of the classification tasks have a separate output layer, which means that each output layer has a softmax function which is independent from the For emotion recognition, the model classifies the input image in one of 7 classes consisting of the basic human emotions: Surprise, fear, disgust, happy, sad, angry and neutral. Gender is classified as male, female or unsure. Race is classified as Caucasian, African-American or Asian. For age estimation, the classes are divided into a range of ages: 0-3, 4-19, 20-39, 40-69 and 70+. The datasets used are FER, containing labels for only emotion and RAF-Db containing labels corresponding to all 4 tasks.\n\nIn the pre-processing stage, the images have been subjected to pose normalisation. This has been achieved by identifying the eye centres of the faces presented in the images, and the images being rotated such that the line joining the eye centres becomes horizontal. The eye detection has been carried out using the Cascade classifier of opencv library  [16] . There were a few cases in which the pre-trained algorithm was incorrectly identifying the eye centres and the images were being rotated by an angle of large magnitude, distorting the images(Figure  3 ). To counter this, the max range to which the image can be rotated has been capped at 10 degrees. In the images presented in both the data sets, the faces only have minor rotation presented. Capping the maximum rotation at 10 degrees ensures that the examples in which eye centers are identified correctly are normalised, but the examples in which eyes are not correctly identified, are not rotated by a large magnitude.\n\nThe CNN architecture has been strongly built upon [6], with the change in dropout rates to improve the accuracy. The dropout rates in [6] were 0.4, 0.4, 0.5, 0.6 in increasing layers of the neural network. In the neural network On the FER dataset, the cross validation accuracy of the previous model was 63%. Upon making the changes, the accuracy has increased to 67%. The reasoning for why this change having performed better than the original model could be: in deeper layers of the Neural Network, each layer learns to detect more complex patterns  [7] , which are of increasing importance in our classification task, and need to have higher probability of being retained in each pass through the neural network. The second last layer of the neural network provides an input to 4 separate output layers, one for each of the classification tasks. So, except the output layer, all the classification tasks have the same weights for the rest of the neural network. For each of the classification tasks, categorical cross entropy is used to calculate the loss of each task. Hence there are 4 loss functions. To calculate the loss function of the entire Neural network, adequate weights needs to be assigned to the loss function of each of the tasks, to get a meaningful result in each of the separate tasks. The weights assigned to emotion, age, race and gender loss were 2, 4, 1.5, 0.1 respectively.\n\nUsing the above weights, the overall loss is calculated. Then the model backpropagates this loss to improve its performance in subsequent iterations. To ensure that the CNN gives optimal performance, two callbacks were used:\n\nEarly stopping: The validation accuracy for emotion was calculated for each epoch, during the training of the neural network. This is used to determine when the model started overfitting on the data. When overfitting has been identified, further training of the model is stopped and the weights from the epoch which provided the best validation accuracy for the data, have been restored to the final model.\n\nReduce on Plateau: If the validation accuracy for emotion output fails to improve for 5 epochs, the learning rate is reduced to 0.2 times the previous model to continue training of the model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "In this paper the following datasets have been used: RAF-DB  [9] : Real-world Affective Faces Database (RAF-DB) is a large-scale facial expression database with around 30K great-diverse facial images downloaded from the Internet. Based on the crowdsourcing annotation, each image has been independently labeled by about 40 annotators. Images in this database are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc. RAF-DB has large diversities, large quantities, and rich annotations. FER  [8] : The FER dataset classifies facial expressions from 35,685 examples of 48x48 pixel grayscale images of faces. Images are categorized based on the emotion shown in the facial expressions (happiness, neutral, sadness, anger, surprise, disgust, fear).\n\nThe model was given the data in the form of {x i , y i }, where x i is a 50x50 grayscale image containing a human face, and y i is the list of numpy arrays containing the labels corresponding to the image. A sample output i: [[1, 0, 0, 0, 0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0, 0, 0]] Signifies that the person present in the image is surprised, male, caucasian and has an age of 0-3.\n\nThe multi label CNN was trained using a batch size of 32 images. The initial learning rate was set to 3e-4 and the number of epochs was equal to 100. All the models were implemented in Tensorflow  [13] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "The validation accuracies recorded can be seen in Table  1 . In the multi la- In addition to this, while trying to perform transfer learning on neural networks which had been pre-trained on facial recognition, the following accuracies were obtained for single task learning of facial emotions on the FER dataset:  VGG16  [20] : 18.03% ResNet50v2  [21] : 48.08% FaceNet  [22] : 30.88% These are all lower than the accuracy obtained on the neural network used for single task learning without any transfer learning, which was 63%.Hence the final model used did not use transfer learning. Sample outputs of the model are shown in Figure  5 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussions",
      "text": "In case of multi-task learning the validation accuracy is 79% when trained for all the labels on RAF-Db, which reduces to 53% when the same model, pre-trained on RAF-Db was further trained single task learning on FER, as this dataset has labels only corresponding to emotions. Hence the final multi-task learning model, which gives the best results, has been trained only on RAF-Db.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this paper we developed an approach to perform multi-task learning, i.e. predicting gender, age and race along with emotion. The results obtained were drastically better than the classical approach of single task learning using the same CNN architecture. This paper was an effort at improving the accuracy of CNNs for Facial Emotion Recognition. Future work will consist of:\n\n• The approach of multi-task learning can be tested on various CNN architectures for facial emotion recognition.\n\n• To effectively train more data having all the labels. Labels for age, gender and race can be generated using pre-trained open source models for the FER dataset and used as a part of the training set.\n\n• Various filters such as Gabor, HOG, LBP, SIFT can be applied during the preprocessing step and their effect on the results can be studied.\n\n• Finding the optimal values of the weights of the loss function of the 4 branches of the CNN, instead of hardcoding the values.\n\n• Finding the optimal values of the weights of the loss function of the 4 branches of the CNN, instead of hardcoding the values.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Architecture of the model",
      "page": 4
    },
    {
      "caption": "Figure 3: ). To counter this, the max range to which the image can be",
      "page": 4
    },
    {
      "caption": "Figure 2: Architecture of the model",
      "page": 5
    },
    {
      "caption": "Figure 3: (a) Showing the original image in the dataset with incorrectly identi-",
      "page": 5
    },
    {
      "caption": "Figure 4: Accuracy and loss evolution of the label emotion.(a) On RAF-Db (b)",
      "page": 8
    },
    {
      "caption": "Figure 5: Sample Outputs of the model",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Emotion Validation Accuracies",
      "data": [
        {
          "Label": "emotion",
          "RAFDB (Basic Emotions)": "0.7926",
          "FER (After training on RAFDB)": "0.5312"
        },
        {
          "Label": "gender",
          "RAFDB (Basic Emotions)": "0.7832",
          "FER (After training on RAFDB)": "N/A*"
        },
        {
          "Label": "race/ ethnicity",
          "RAFDB (Basic Emotions)": "0.8610",
          "FER (After training on RAFDB)": "N/A*"
        },
        {
          "Label": "age",
          "RAFDB (Basic Emotions)": "0.7476",
          "FER (After training on RAFDB)": "N/A*"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Emotion Validation Accuracies",
      "data": [
        {
          "Label": "emotion",
          "RAFDB (Basic Emotions)": "0.6956",
          "FER (After training on RAFDB)": "1.2371"
        },
        {
          "Label": "gender",
          "RAFDB (Basic Emotions)": "0.5110",
          "FER (After training on RAFDB)": "N/A*"
        },
        {
          "Label": "race/ ethnicity",
          "RAFDB (Basic Emotions)": "0.4524",
          "FER (After training on RAFDB)": "N/A*"
        },
        {
          "Label": "age",
          "RAFDB (Basic Emotions)": "0.9003",
          "FER (After training on RAFDB)": "N/A*"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The social constructionist viewpoint",
      "authors": [
        "Rom Harre"
      ],
      "year": "1986",
      "venue": "The social constructionist viewpoint"
    },
    {
      "citation_id": "2",
      "title": "A Brief Review of Facial Emotion Recognition Based on Visual Information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors",
      "doi": "10.3390/s18020401"
    },
    {
      "citation_id": "3",
      "title": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "authors": [
        "Gerard Pons",
        "David Masip"
      ],
      "year": "2018",
      "venue": "Multi-task, multi-label and multi-domain learning with residual convolutional networks for emotion recognition",
      "arxiv": "arXiv:1802.06664"
    },
    {
      "citation_id": "4",
      "title": "A comprehensive analysis of deep learning based representation for face recognition",
      "authors": [
        "Mehdipour Ghazi",
        "Hazim Kemal Mostafa",
        "Ekenel"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "5",
      "title": "Fast facial emotion recognition using convolutional neural networks and Gabor filters",
      "authors": [
        "Milad Zadeh",
        "Maryam Mohammad Taghi",
        "Babak Imani",
        "Majidi"
      ],
      "year": "2019",
      "venue": "Fast facial emotion recognition using convolutional neural networks and Gabor filters"
    },
    {
      "citation_id": "6",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "Matthew Zeiler",
        "Rob Fergus"
      ],
      "year": "2014",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "7",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "Z Chuang",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "8",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "9",
      "title": "The Japanese Female Facial Expression (JAFFE) Dataset [Data set]",
      "authors": [
        "Michael Lyons",
        "Kamachi",
        "Miyuki",
        "Jiro Gyoba"
      ],
      "year": "1998",
      "venue": "Zenodo",
      "doi": "10.5281/zenodo.3451524"
    },
    {
      "citation_id": "10",
      "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "authors": [
        "Martín Abadi",
        "Ashish Agarwal",
        "Paul Barham",
        "Eugene Brevdo",
        "Zhifeng Chen",
        "Craig Citro",
        "Greg Corrado",
        "Andy Davis",
        "Jeffrey Dean",
        "Matthieu Devin",
        "Sanjay Ghemawat",
        "Ian Goodfellow",
        "Andrew Harp",
        "Geoffrey Irving",
        "Michael Isard",
        "Rafal Jozefowicz",
        "Yangqing Jia",
        "Lukasz Kaiser",
        "Manjunath Kudlur",
        "Josh Levenberg",
        "Dan Mané",
        "Mike Schuster",
        "Rajat Monga",
        "Sherry Moore",
        "Derek Murray",
        "Chris Olah",
        "Jonathon Shlens",
        "Benoit Steiner",
        "Ilya Sutskever",
        "Kunal Talwar",
        "Paul Tucker",
        "Vincent Vanhoucke",
        "Vijay Vasudevan",
        "Fernanda Viégas",
        "Oriol Vinyals",
        "Pete Warden",
        "Martin Wattenberg",
        "Martin Wicke",
        "Yuan Yu",
        "Xiaoqiang Zheng"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-scale machine learning on heterogeneous systems"
    },
    {
      "citation_id": "11",
      "title": "Recognition of facial emotion through face analysis based on quadratic bezier curves",
      "authors": [
        "Yong-Hwan Lee"
      ],
      "year": "2015",
      "venue": "Indian Journal of Science and Technology"
    },
    {
      "citation_id": "12",
      "title": "Human emotion recognition from facial thermal image based on fused statistical feature and multi-class SVM",
      "authors": [
        "Anushree Basu"
      ],
      "year": "2015",
      "venue": "2015 Annual IEEE India Conference (INDICON)"
    },
    {
      "citation_id": "13",
      "title": "The OpenCV Library. Dr. Dobb&#x27;s Journal of Software Tools",
      "authors": [
        "G Bradski"
      ],
      "year": "2000",
      "venue": "The OpenCV Library. Dr. Dobb&#x27;s Journal of Software Tools"
    },
    {
      "citation_id": "14",
      "title": "Facial emotion recognition using convolutional neural networks",
      "authors": [
        "K Sarvakar",
        "R Senkamalavalli",
        "S Raghavendra",
        "J Kumar",
        "R Manjunath",
        "S Jaiswal"
      ],
      "year": "2021",
      "venue": "Materials Today: Proceedings"
    },
    {
      "citation_id": "15",
      "title": "Fedvision: An online visual object detection platform powered by federated learning",
      "authors": [
        "Yang Liu",
        "Anbu Huang",
        "Yun Luo",
        "He Huang",
        "Youzhi Liu",
        "Yuanyuan Chen",
        "Lican Feng",
        "Tianjian Chen",
        "Han Yu",
        "Qiang Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Analysis of artificial intelligence based image classification techniques",
      "authors": [
        "S Shakya"
      ],
      "year": "2020",
      "venue": "Journal of Innovative Image Processing"
    },
    {
      "citation_id": "17",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "19",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "InProceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Development of a Facial Emotion Recognition Method based on combining AAM with DBN",
      "authors": [
        "K Ko",
        "K Sim"
      ],
      "year": "2010",
      "venue": "2010 International Conference on Cyberworlds"
    },
    {
      "citation_id": "21",
      "title": "Geometric feature-based facial expression recognition in image sequences using multi-class adaboost and support vector machines",
      "authors": [
        "D Ghimire",
        "J Lee"
      ],
      "year": "2013",
      "venue": "Sensors"
    },
    {
      "citation_id": "22",
      "title": "A real time facial expression classification system using local binary patterns",
      "authors": [
        "S Happy",
        "A George",
        "A Routray"
      ],
      "year": "2012",
      "venue": "2012 4th International conference on intelligent human computer interaction (IHCI)"
    },
    {
      "citation_id": "23",
      "title": "Facial expression recognition based on local region specific features and support vector machines. Multimedia Tools and Applications",
      "authors": [
        "D Ghimire",
        "S Jeong",
        "J Lee",
        "S Park"
      ],
      "year": "2017",
      "venue": "Facial expression recognition based on local region specific features and support vector machines. Multimedia Tools and Applications"
    },
    {
      "citation_id": "24",
      "title": "Deep structured learning for facial expression intensity estimation",
      "authors": [
        "R Walecki",
        "O Rudovic",
        "V Pavlovic",
        "B Schuller",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image Vis. Comput"
    },
    {
      "citation_id": "25",
      "title": "Appendix: The appendix contains the model architecture of the CNN used",
      "venue": "Appendix: The appendix contains the model architecture of the CNN used"
    }
  ]
}