{
  "paper_id": "2509.08470v1",
  "title": "Joint Learning Using Mixture-Of-Expert-Based Representation For Enhanced Speech Generation And Robust Emotion Recognition",
  "published": "2025-09-10T10:18:56Z",
  "authors": [
    "Jing-Tong Tzeng",
    "Carlos Busso",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "Speech emotion recognition",
    "speech enhancement",
    "multi-task learning",
    "mixture of experts",
    "noise robustness"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) plays a critical role in building emotion-aware speech systems, but its performance degrades significantly under noisy conditions. Although speech enhancement (SE) can improve robustness, it often introduces artifacts that obscure emotional cues and adds computational overhead to the pipeline. Multi-task learning (MTL) offers an alternative by jointly optimizing SE and SER tasks. However, conventional shared-backbone models frequently suffer from gradient interference and representational conflicts between tasks. To address these challenges, we propose the Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT), a flexible MTL framework that applies framewise expert routing over self-supervised speech representations. Sparse MERIT incorporates task-specific gating networks that dynamically select from a shared pool of experts for each frame, enabling parameter-efficient and task-adaptive representation learning. Experiments on the MSP-Podcast corpus show that Sparse MERIT consistently outperforms baseline models on both SER and SE tasks. Under the most challenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERIT improves SER F1macro by an average of 12.0% over a baseline relying on a SE pre-processing strategy, and by 3.4% over a naive MTL baseline, with statistical significance on unseen noise conditions. For SE, Sparse MERIT improves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline and by 20.0% over the naive MTL baseline. These results demonstrate that Sparse MERIT provides robust and generalizable performance for both emotion recognition and enhancement tasks in noisy environments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH emotion recognition (SER) plays a vital role in advancing Human-Computer Interaction (HCI) by enabling machines to perceive and respond to human emotions through vocal cues. This capability supports a range of emotion-aware applications, including virtual assistants  [1] -  [4] , mental health monitoring systems  [5] -  [7] , and customer service automation platforms  [8] -  [10] . However, speech is often corrupted by background noise in real-world deployment scenarios. Such non-stationary background noise can obscure emotion-relevant acoustic features and significantly degrade SER performance, thereby limiting its reliability and generalizability.\n\nTo improve the noise robustness of SER, numerous approaches have been explored, including robust feature engi-neering  [11] -  [13] , data augmentation  [14] -  [17] , environmentdependent compensations  [18] ,  [19] , and domain adaptation  [20] -  [22] . While these strategies have demonstrated effectiveness in enhancing SER performance under noisy conditions, their inability to produce cleaned speech limits their usefulness in applications that require human intervention or audio auditing, such as emergency response systems  [23] -  [25] . In such scenarios, access to intelligible speech is as critical as accurate emotion recognition. For example, human operators may need to directly review the spoken content to make informed decisions, assess urgency, or validate automated predictions. Given these limitations, speech enhancement (SE) offers a more interpretable and versatile solution by generating denoised speech that supports both automated processing and human-in-the-loop analysis  [26] -  [28] . However, SE models are typically optimized for perceptual intelligibility and signal fidelity, objectives that do not necessarily align with preserving the emotion-discriminative features needed for SER  [29] . As a result, emotional nuances may be unintentionally suppressed during enhancement. Additionally, incorporating SE as a standalone front-end module increases model complexity and computational overhead, which can hinder its practical deployment in resource-constrained environments.\n\nIn our previous work  [30] , we addressed this mismatch and computational overhead by jointly training SE and SER models using shared self-supervised speech pre-trained model representations. This multi-task learning (MTL) framework improved noise robustness while reducing model redundancy. However, MTL models with a single shared backbone often suffer from unstable training dynamics. As noted in prior studies  [31] -  [33] , shared parameters can receive conflicting gradient signals from different task objectives, leading to suboptimal convergence and biased feature representations. This issue is further compounded when the tasks differ significantly in complexity. For example, speech enhancement requires finegrained, low-level signal reconstruction, whereas speech emotion recognition involves high-level abstraction and semantic understanding. These differences pose a challenge for a single backbone to serve both tasks effectively, often resulting in suboptimal convergence and degraded performance. This paper proposes the Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT), a flexible MTL framework designed to integrate speech self-supervised representations for both SE and SER. Sparse MERIT addresses the limitations of conventional shared-backbone architectures by incorporating a Mixture-of-Experts (MoE) structure that expands model capacity and enables more effective represen-tation integration across tasks. Rather than relying on a single shared pathway, Sparse MERIT introduces multiple expert modules along with task-specific gating networks that dynamically select expert outputs based on the input. This design mitigates negative interference between conflicting objectives and better accommodates the different levels of complexity required by SE and SER tasks.\n\nExperiments on the MSP-Podcast corpus  [34]  demonstrate that Sparse MERIT improves generalization for both tasks, particularly under challenging noisy conditions. Under the most difficult setting of -5 dB signal-to-noise ratio (SNR), Sparse MERIT outperforms baseline relying on a SE preprocessing strategy by an average of 12.0% F1-macro. It also improves upon our previously proposed naive MTL framework by 3.4%, across two unseen noise datasets, with statistical significance. In addition, Sparse MERIT consistently improves SE performance across multiple standard enhancement metrics. These results confirm that jointly learning SE and SER through our Sparse MERIT architecture leads to more robust and effective performance than prior MTL strategies.\n\nThe main contributions of this paper are summarized as follows:\n\n• We show that combining SE and SER in a multitask framework improves both enhancement quality and emotion recognition performance under diverse noise conditions. • We introduce Sparse MERIT, a flexible MoE-based architecture that goes beyond our prior work by enhancing representation capacity and reducing task interference via task-specific expert routing. • We validate Sparse MERIT through extensive experiments, showing consistent gains over a SE pre-processing baseline, a naive MTL baseline, and other mainstream techniques, especially in unseen noise conditions. The rest of this paper is organized as follows. Section II reviews related work on SER in noisy conditions and MTL strategies. Section III introduces the proposed Sparse MERIT framework, including its representation integration, expert routing mechanism, and task-specific components. Section IV describes the experimental setup, including datasets, implementation details, and baseline comparisons. Section V presents the results and analysis for both SER and SE tasks, along with ablation studies. Finally, Section VI concludes the paper and discusses directions for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Speech Emotion Recognition Under Noisy Conditions",
      "text": "Recent studies have demonstrated significant progress in SER  [35] . However, the performance of SER systems remains highly vulnerable to degradation in noisy environments, posing a major barrier to their deployment in real-world applications. One line of research addresses this challenge through noiserobust feature selection. For example, Schuller et al.  [12]  applied information gain ratio-based feature selection and demonstrated improved performance under both clean and noisy conditions. Leem et al.  [13]  identified a subset of noiserobust low-level descriptors (LLDs), which outperformed the full LLD set in noisy settings. Building on this idea, Leem et al.  [36]  proposed a generative adversarial network (GAN)based feature enhancement model that strengthens weak features while preserving robust ones. Similarly, Chakraborty et al.  [11]  employed a denoising autoencoder to enhance Mel-Frequency Cepstral Coefficient (MFCC) features, achieving notable improvements in robustness.\n\nAnother direction improves SER by discarding noisy frames. Pandharipande et al.  [37] ,  [38]  used a front-end voice activity detector (VAD) to identify and discard noisy frames prior to feature extraction. Leem et al.  [39]  extended this approach by replacing dropped frames with enhanced speech, thereby preserving lexical content and improving recognition accuracy.\n\nA third strategy focuses on increasing data diversity by contaminating clean training speech with various noise types. This approach exposes the model to a wider range of acoustic conditions during training. Tiwari et al.  [17]  proposed a generative model capable of synthesizing diverse noise profiles in the Mel-filterbank energy domain. Wu et al.  [14]  introduced a dynamic augmentation strategy that selects distortion levels based on their impact on performance. Ranjan et al.  [16]  developed a reinforcement learning (RL)-based augmentation method that adaptively chooses noise types to optimize performance under unseen conditions.\n\nA fourth line of work incorporates environmental information directly into the model to enhance noise robustness. Leem et al.  [18]  proposed skip-connection adapters composed of environment-agnostic and environment-specific modules to denoise speech representations within a transformer encoder. Additionally, they used text-based environment descriptions to further enrich the contextual representation and improve robustness in their later work  [19] .\n\nAnother research direction frames the noise robustness problem as a domain mismatch issue. Leem et al.  [21]  employed a ladder network  [40] ,  [41] , separating the final-layer embeddings into two branches: one for emotion classification and another for reconstructing clean speech representations. This dual-branch design encourages the learning of discriminative features while mitigating background noise. In a separate study, Leem et al.  [22]  proposed a contrastive teacher-student framework to align noisy embeddings with clean counterparts, improving generalization to unseen noise. Wilf and Provost  [20]  introduced a MoE structure alongside a Domain Separation Network (DSN)  [42] , enabling input-dependent routing to specialized encoders based on noise characteristics and enhancing robustness in both unimodal and multimodal settings.\n\nHowever, the aforementioned approaches do not generate enhanced speech signals that can be inspected by humans, which limits their practical utility in real-world settings. Using a front-end SE module has been explored as a more practical solution, as it not only improves SER performance but also increases transparency and user trust by providing humaninterpretable, denoised speech signals. Triantafyllopoulos et al.  [26]  incorporated SE as a front-end component to improve SER performance, particularly under low SNR conditions. Kshirsagar et al.  [27]  employed front-end SE with a mimic loss  [43]  originally developed for automatic speech recognition (ASR), and demonstrated improved SER performance in a multimodal framework. Chen et al.  [28]  proposed an SNRlevel detection module to reduce the aliasing effects of SE on speech signals with little or no background noise. To further explore the interaction between SE and SER, Avila et al.  [29]  investigated the correlation between perceptual speech quality and emotion classification accuracy. Despite their effectiveness, these two-stage approaches are often resource-intensive, increasing model complexity and limiting their suitability for deployment in resource-constrained settings. Moreover, the perceptual speech quality metrics used in the first stage are not specifically designed to capture emotional cues, which may result in a mismatch between enhancement objectives and the needs of emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Multi-Task Learning",
      "text": "To reduce computational cost and address the mismatch between speech intelligibility and model recognition performance, MTL offers a promising solution by enabling the joint optimization of multiple objectives. Several studies have shown that incorporating reconstruction loss as an auxiliary objective can enhance SER performance  [20] ,  [21] ,  [44] ,  [45] . However, these approaches primarily focus on reconstructing intermediate feature representations rather than the waveform itself. As a result, their practical utility in applications requiring human-audible outputs remains limited.\n\nSpeech self-supervised learning (SSL) models have demonstrated strong performance across a wide range of speech processing tasks  [46] -  [50] , including both SE  [51] -  [53]  and SER  [54] -  [56] . Therefore, there is a strong motivation to adopt a unified SSL backbone for MTL involving both tasks. Our previous work further supports this direction, showing that jointly learning SE and SER from shared SSL representations improves SER robustness under unseen noisy conditions without compromising SE performance  [30] .\n\nAlthough MTL offers potential benefits through shared representation learning, MTL models do not always outperform their single-task counterparts across all tasks in practice  [57] ,  [58] . This inconsistency is often attributed to several inherent challenges, including gradient interference between tasks, training instability, and imbalanced learning dynamics caused by differences in task complexity. To address these issues, various strategies have been proposed. One such approach is uncertainty-based loss weighting  [59] , which introduces taskdependent homoscedastic uncertainty as learnable parameters to dynamically adjust the contribution of each task's loss. This strategy allows the model to adaptively balance the competing objectives, without requiring manual loss reweighting.\n\nAnother line of work focuses on directly manipulating task gradients to address training instability and reduce conflicts in multi-task optimization. GradNorm  [31]  is a representative example that balances learning across tasks by dynamically adjusting gradient magnitudes based on the relative training speed of each task. By equalizing the rate at which taskspecific losses decrease, GradNorm helps prevent any single task from dominating the optimization process. In contrast, projected conflicting gradient (PCGrad)  [32]  addresses gradient interference by projecting out the conflicting components between task gradients, reducing destructive updates and improving training stability.\n\nCompared to approaches that automatically balance task losses or adjust gradient magnitudes and directions to stabilize training, MoE architectures offer an alternative solution to MTL through architectural design  [33] ,  [60] . An MoE framework typically consists of a shared pool of expert networks and a gating mechanism that determines which subset of experts to activate for a given input. This conditional routing mechanism increases model capacity without proportional computational overhead and allows the model to learn more flexible, input-or task-sensitive processing paths. Early applications of MoE in MTL used sample-level routing, where each input is assigned to a subset of experts. For example, Wilf and Provost  [20]  applied an MoE model with a noise-type classifier to dynamically route inputs to different feature encoders, performing both SER and feature reconstruction to improve noise robustness. While effective, sample-level routing often lacks granularity because it assumes a uniform expert assignment across all frames of an utterance. This strategy can lead to suboptimal performance when local acoustic or emotional variations are present. To address this limitation, token-level MoE has emerged as a more flexible alternative, allowing each token to be routed independently based on its local representation. In the domain of language modeling, approaches such as the Switch Transformer  [61]  have shown that sparse tokenwise MoE architectures can significantly scale model capacity without increasing inference cost. This design has been widely adopted in large language models (LLMs)  [62] -  [65] , where it improves both computational efficiency and model expressiveness. These studies highlight the potential benefits of sparse token-level MoE, motivating its adoption for speechbased multi-task learning.\n\nToken-wise MoE has also been adapted to MTL settings, where it helps support task heterogeneity and feature specialization. For instance, Liang et al.  [66]  introduced M 3 ViT for vision tasks and showed that sparse patch-level expert selection improves multi-task performance. In the speech domain, frame-wise MoE has also been effective. You et al.  [67] ,  [68]  applied MoE to speech recognition with favorable results. Further improvements have been demonstrated in multilingual speech recognition  [69] ,  [70] .\n\nBuilding on these advances, we propose Sparse MERIT, an MoE-based framework designed for MTL over speech selfsupervised representations, targeting both SER and SE. Sparse MERIT leverages dynamic expert routing at the frame level to reduce gradient interference, support parameter-efficient specialization across tasks, and improve generalization without increasing inference cost.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "This section outlines the architecture of Sparse MERIT, our proposed MTL framework for SER and SE. Sparse MERIT builds on our preliminary work  [30]  by introducing a framewise MoE layer over multi-layer self-supervised speech rep- resentations. This framework consists of three main components: (1) a layer-wise feature construction from a pre-trained SSL model, (2) an expert-based integration using frame-level sparse routing, and (3) task-specific heads for SE and SER trained under a joint objective. Figure  1  illustrates the overall architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Layer-Wise Representation Construction",
      "text": "Given a noisy input waveform x noisy ∈ X noisy and its corresponding clean reference waveform x clean ∈ X clean , we extract hidden representations from a pre-trained selfsupervised learning (SSL) model parameterized by θ. The SSL model consists of an input feature extractor followed by L transformer layers. Let H 0 ∈ R T ×D denote the input to the first transformer layer, and let S l θ (x noisy ) ∈ R T ×D denote the output of the l-th transformer layer for l = 1, . . . , L, where T is the number of frames and D is the feature dimensionality.\n\nFor notational convenience, we define:\n\nWe construct a comprehensive multi-layer representation by concatenating the input H 0 and all transformer outputs H 1 , . . . , H L along the feature dimension:\n\nThis frame-level sequence captures multi-scale contextual information across multiple abstraction levels, serving as input to the MoE module.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Mixture-Of-Experts Integration",
      "text": "To process the concatenated multi-layer representation F concat θ , we introduce a frame-wise MoE module. Each frame embedding\n\nEach expert E n is implemented as a two-layer feedforward network that projects the high-dimensional input into a lowerdimensional embedding space of size D. This mapping re-duces the concatenated feature dimensionality while preserving temporal resolution:\n\nTo enable task-specific routing, we introduce two independent gating networks, G ϕSER and G ϕSE , parameterized by ϕ SER and ϕ SE , respectively. For each frame t, the gating network for task τ ∈ {SER, SE} produces a softmax-normalized routing score over the N experts:\n\nWe use a Top-K routing strategy to select the most relevant experts for each frame. The gating network outputs a probability distribution over experts, and the TopK operator selects the top-K values, zeroing out the rest. This approach allows each frame to be processed by a sparse subset of experts, which improves computational efficiency and encourages expert specialization. While Sparse MERIT supports arbitrary K ≥ 1, we adopt K = 1 in this study, following the sparse routing design of the Switch Transformer  [61] .\n\nFormally, let TopK(v, k) ∈ R N denote the operator that retains the top k values of a vector v, setting the remaining entries to zero:\n\nUsing Top-K routing, the MoE output for frame t and task τ is computed as a weighted combination of expert outputs, where each expert's output is scaled by its corresponding gating weight:\n\nThe final MoE output sequence for task τ is thus:\n\nThis design enables task-specific expert selection at the frame level, balancing specialization and parameter sharing, while ensuring a consistent output shape across both tasks.\n\nC. Task-Specific Heads 1) Speech Emotion Recognition: The SER task takes the MoE-transformed sequence Z SER ∈ R T ×D as input. We apply attentive statistics pooling  [71]  to convert frame-level features into a fixed-length utterance representation. This strategy is a temporal aggregation method that computes the weighted mean and standard deviation across time using learned attention weights. This pooled vector is passed to a task-specific classification head Π SER , parameterized by θ SER , to predict the emotionl label:\n\n2) Speech Enhancement: For the SE task, we first compute a spectral representation from the noisy waveform using the magnitude of the short-time Fourier transform (STFT). The result is then compressed using the log1p function, defined as log1p(x) = log(1 + x), which has been shown to improve SE performance  [72] :\n\nWe concatenate this spectral feature with the MoEtransformed output Z SE along the feature dimension and feed it to the SE head Π SE , parameterized by θ SE , to reconstruct the enhanced spectrogram:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Multi-Task Objective",
      "text": "We jointly optimize the SE and SER tasks using an MTL objective. The model is trained to minimize the sum of a weighted cross-entropy loss for SER and an L 1 loss for SE:\n\nThe weighted cross-entropy loss L WCE is used to compensate for the imbalanced class distribution in the emotion dataset, ensuring that underrepresented categories are not neglected during training. The L 1 loss encourages accurate reconstruction of clean spectral features for speech enhancement. This combined objective guides the model to learn representations that support both high-level semantic discrimination (emotion classification) and low-level signal reconstruction (enhancement), while allowing shared learning through the unified self-supervised backbone.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experimental Settings A. Data Preparation",
      "text": "We conduct our experiments using the MSP-Podcast corpus  [34] , a large-scale, naturalistic emotional speech dataset derived from a diverse range of podcast recordings. The selected utterances, ranging from 2.75 to 11 seconds in duration, are carefully filtered to exclude background music and overlapping speech. To ensure acoustic quality, only recordings with a predicted SNR above 20 dB are retained. For this study, we focus on four emotion categories: anger, sadness, happiness, and neutral state. We utilize version 1.11 of the corpus, which contains 100,896 labeled segments (Anger: 10,342; Sadness: 8,347; Happiness: 29,454; Neutral: 52,753). The training partition is used to fine-tune a pre-trained speech representation model, and the development set is employed for model selection and early stopping. We evaluate the results on the test 1 set of the corpus.\n\nTo introduce realistic noise conditions during training, we augment the clean data by generating babble noise through speech overlay using samples from the CRSS-4ENGLISH-14 corpus  [73] . The training and development sets are corrupted at an SNR of 5 dB to simulate moderate background interference. For evaluation, we apply the same corruption process to the test 1 set using 4 SNR levels: -5 dB, 0 dB, 5 dB, and 10 dB, covering a range of low to high noise intensities. To further test the robustness of the model against unseen noise types, we introduce ambient noise collected from the Freesound repository  [74] , using the same SNR levels for consistency. Additionally, we incorporate noise samples from the ICASSP 2023 Deep Noise Suppression (DNS) Challenge dataset  [75] , which includes diverse real-world noise recordings. To avoid data redundancy, we remove overlapping segments between DNS and Freesound. For experimental simplicity and to isolate noise effects, we exclude room impulse responses from the DNS samples.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Implementation Details",
      "text": "We implement our proposed Sparse MERIT framework using the WavLM Large model  [48]  as the shared selfsupervised backbone. WavLM Large is a 24-layer transformer model pre-trained on 94K hours of both clean and noisy speech. It has demonstrated strong performance across a wide range of speech processing tasks, including SER and SE, as shown in the speech processing universal performance benchmark (SUPERB)  [49] ,  [50]  and recent works  [51] ,  [56] . This versatility, along with its robustness to noise, makes WavLM well-suited for our MTL framework.\n\nFollowing the WavLM backbone, we apply our proposed MoE integration layer to process the concatenated multi-layer representations. The input to the MoE has a dimensionality of 1, 024 × 25, formed by concatenating the hidden states from the input to the transformer encoder (pre-layer representation) together with the outputs from all 24 transformer layers (each of dimensionality 1,024). The MoE module consists of N = 3 experts. Each expert first reduces the input dimension from 25 × 1, 024 to 4,096 and then projects it into a 1,024-dimensional output. This design compresses the highdimensional concatenated input into a compact task-adapted representation suitable for the downstream processing.\n\nFor the SER task, the MoE output is passed to a task-specific classification head composed of attentive statistics pooling  [71]  followed by fully connected layers, based on the baseline from the Interspeech 2025 Challenge on Speech Emotion Recognition in Naturalistic Conditions  [56] . For the SE task, the MoE output is concatenated with a log-compressed spectrogram of the noisy input. The combined representation is processed by the SE decoder, which adopts the architecture of the BSSE-SE model  [51] , designed to reconstruct clean spectral features in noisy environments.\n\nDuring preprocessing, all input waveforms are normalized using the Z-normalization, with mean and standard deviation estimated from the entire training set. We use a two-phase training procedure. In the first phase, we freeze the SSL backbone and train the SE and SER heads independently. The SE head is trained using the AdamW optimizer for 130 epochs with a batch size of 16 and a learning rate of 5 × 10 -5 . The SER head is trained for 20 epochs with a batch size of 32 and the same optimizer settings. In the second phase, we jointly fine-tune the full model using the pre-trained head weights. The full pipeline is trained for an additional 20 epochs with a batch size of 32. We continue using the AdamW optimizer, setting the learning rate to 5 × 10 -5 for the expert networks, gating networks, and task-specific heads. We set the learning rate to 2.5×10 -5 for the Transformer layers of the SSL model. The CNN-based feature extractor of the WavLM backbone remains frozen during both training phases.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Baseline Methods For Ser",
      "text": "We compare our proposed method, Sparse MERIT, with seven SER baselines:\n\n• Original: Fine-tunes the SER model on clean emotional speech without any adaptation to noisy conditions. • SE Pre-process (SE-P): Applies an SE model as a frontend module to denoise the input audio before SER. The SE model is pre-trained on the VCTK-DEMAND dataset  [76]  and fine-tuned on the MSP-Podcast corpus. The SER model is then trained on the enhanced speech. • Fine-tuning Entire Model (FT-M): Fine-tunes both the SSL backbone and the SER classification head directly on noisy speech data. • Naive Fine-tuning w/ Multi-task Learning (FT-MTL):\n\nJointly trains SE and SER using a shared SSL backbone, where a weighted sum of layer-wise representations is used as the input to both task-specific heads. The model is trained on noisy speech by combining the enhancement and classification losses, following the approach proposed in  [30] . • FT-MTL w/ Uncertainty: Extends FT-MTL by applying task uncertainty-based loss weighting  [59]  to automatically balance the SE and SER objectives during training. • FT-MTL w/ PCGrad: Builds on FT-MTL by applying PCGrad  [32]  to mitigate gradient interference between tasks and improve training stability.\n\nIn addition to our proposed Sparse MERIT approach, we implement a variation to evaluate our decision to only use the Top-1 frame-wise expert routing, where each frame is processed by a single selected expert, enabling efficient taskadaptive specialization with reduced computational overhead.\n\n• Dense MERIT: Implements the MERIT framework with dense expert selection, where each frame-level representation is routed to all experts with continuous soft weights. This allows all experts to contribute to every frame.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Baseline Methods For Se",
      "text": "Although SER is the primary task of interest, the quality of the enhanced speech is also crucial for real-world applications in which humans may interact with or listen to the audio output. Poor enhancement quality can degrade recordings' quality and hinder both human understanding and downstream processing. Moreover, evaluating SE performance provides insight into how well an MTL method resolves conflicts between competing objectives. Since SE and SER often require different feature characteristics, joint training can lead to suboptimal performance if the model fails to disentangle the two tasks. Therefore, we compare SE performance across various MTL strategies, as well as a model fine-tuned solely for speech enhancement, to assess their effectiveness in mitigating task interference and preserving signal quality.\n\n• Fine-tuned: Enhanced speech produced by a model finetuned exclusively for the SE task using the MSP-Podcast corpus contaminated with recordings from the CRSS-4ENGLISH-14 training set. The model is initialized from a pre-trained checkpoint trained on the VCTK-DEMAND dataset, without an emotion recognition objective. • FT-MTL Variants: Speech enhancement outputs generated from jointly trained SE+SER models are considered here. This includes standard FT-MTL and its variants with uncertainty weighting, PCGrad, and both Dense and Sparse MERIT integration strategies.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Emotion Recognition",
      "text": "We evaluate SER performance using both F1-macro and F1-micro scores across four SNR levels, under both seen and unseen noise conditions. Each method is trained using four different random seeds, and the test set is divided into five non-overlapping subsets per condition, yielding 20 evaluation scores per method (4 runs × 5 test sets). These scores are used to compute average performance and conduct statistical comparisons. We apply one-tailed Welch's t-tests to compare each method against all other baselines. Statistical significance is determined at a threshold of p ≤ 0.05. Significance markers in Table  I  indicate whether a method outperforms a given baseline, with symbol definitions provided in the table caption.\n\nOur proposed Sparse MERIT framework achieves statistically significant improvements over baselines under low-SNR and unseen noisy conditions, demonstrating strong robustness and generalization across diverse acoustic scenarios. At -5 dB, the most challenging condition, Sparse MERIT yields an F1macro improvement of 3.6% over fine-tuning directly on noisy speech (FT-M), 12.4% over the SE pre-processing (SE-P) baseline, and 3.8% over the naive MTL setup (FT-MTL) on the Freesound-contaminated test set. Similar gains are observed on the DNS-contaminated test set, with respective improvements of 3.8% (FT-M), 11.6% (SE-P), and 2.9% (FT-MTL). While SE-P performs relatively well under seen noise conditions, its performance degrades notably under unseen noise, highlighting its limited generalization. In contrast, Sparse MERIT maintains robust performance across both unseen noisy conditions. Furthermore, under high-SNR conditions, our method performs comparably or better than SE-P even on the seen CRSS test set, suggesting that it avoids the artifacts and loss of emotional nuance introduced by front-end enhancement methods applied to minimally corrupted signals, as reported in prior work  [28] .\n\nBeyond baseline comparisons, we also evaluate Sparse MERIT against other MTL strategies designed to mitigate task conflicts (FT-MTL w/ Uncertainty and FT-MTL w/ PCGrad). Uncertainty-based loss weighting yields promising results and generally outperforms standard FT-MTL. However, PCGrad fails to show consistent benefits in our setting and underperforms naive MTL, indicating its limited utility in this task combination. When we compare architectural variants of our approach, Sparse MERIT outperforms Dense MERIT, achieving F1-macro score gains of 1.5% using Freesound noises and 1.6% using DNS noises. These results suggest that Top-1 expert routing, by assigning each frame to a single expert, encourages more focused and stable specialization, leading to better generalization and efficiency under noisy conditions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Speech Enhancement",
      "text": "We evaluate the SE performance of each method across three noisy conditions and four SNR levels (-5 dB, 0 dB, 5 dB, and 10 dB). Each model is trained using a fixed random seed to ensure consistency. We report six widely used objective metrics to assess SE quality: PESQ (Perceptual Evaluation of Speech Quality), CSIG (Mean Opinion Score of signal distortion), CBAK (Mean Opinion Score of background noise intrusiveness), COVL (Mean Opinion Score of overall quality), SSNR (Segmental SNR), and STOI (Short-Time Objective Intelligibility). These metrics offer a comprehensive assessment of both the perceptual quality and intelligibility of the enhanced speech across varying noise levels and conditions.\n\nAs shown in Table  II , the model fine-tuned exclusively for the SE task achieves the best performance across all four SNR levels under the seen CRSS noise condition. However, when tested on unseen noise conditions such as the Freesoundcontaminated test set, Sparse MERIT consistently outperforms all baselines across all SNR levels. Under the -5 dB condition, Sparse MERIT shows a 1.8% drop in PESQ, but 14.1% improvement in CSIG, 5.3% in CBAK, 9.3% in COVL, 12.1% in SSNR, and a 20.0% improvement in STOI compared to the SE-only model. Against the naive multi-task learning baseline (FT-MTL), Sparse MERIT shows 0.9% higher PESQ, 6.1% higher CSIG, 2.6% higher CBAK, 4.5% higher COVL, 8.6% higher SSNR, and 3.1% higher STOI.\n\nOn the DNS-contaminated test set, Sparse MERIT again demonstrates superior performance at -5, 0, and 5 dB, and performs comparably to other methods at 10 dB. At -5 dB, compared to the SE-only model, it yields equal PESQ, but achieves 15.1% higher CSIG, 7.7% higher CBAK, 10.3% higher COVL, 44.4% higher SSNR, and 19.6% higher STOI. Relative to FT-MTL, it improves 1.8% on PESQ, 7.6% on CSIG, 4.0% on CBAK, 6.0% on COVL, 31.5% on SSNR, and 4.7% on STOI. These results demonstrate that Sparse MERIT not only generalizes better across unseen noise but also enhances intelligibility and perceptual quality under extremely low-SNR conditions.\n\nWhile uncertainty loss weighting improves SER performance over the FT-MTL baseline, it does not yield noticeable gains for SE. In contrast, PCGrad does not improve performance on either task, yielding results that are comparable to or worse than those of naive multi-task learning. Sparse MERIT, on the other hand, demonstrates consistent benefits across both tasks. For SER, Sparse MERIT achieves superior performance, likely due to its more focused expert routing. For SE, both the dense and sparse MERIT variants deliver strong and comparable results, indicating that the expertbased integration mechanism supports robust enhancement regardless of the routing strategy.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Impact Of Expert Network Size",
      "text": "In this section, we investigate how varying the number of experts affects both SE and SER performance. We evaluate models with 1, 3, 5, 7, and 9 experts using a fixed random seed to ensure a fair comparison. For the SE evaluation, we use SSNR, as it provides a more neutral assessment of enhancement quality. In contrast to perceptual metrics such As shown in Table  III , performance trends differ between the two tasks. SER performance peaks when using 3 experts, whereas SE performance generally improves with more experts, reaching its highest SSNR at 5. These findings suggest that, unlike in large language models where increasing expert count often improves performance  [61] , multi-task SE and SER learning does not exhibit this pattern. Given that our primary goal is to enhance SER robustness, we adopt three experts, as they consistently deliver the best SER results across unseen noise conditions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Analysis Of Gating Behavior",
      "text": "To better understand how the MoE mechanism operates under different acoustic and emotional conditions, we analyzed three aspects of gating behavior: switching dynamics, agreement between SE and SER gates, and expert usage distributions across SNR levels and emotion classes. The reported values are aggregated from all three testing sets.\n\n1) Switching dynamics: The switching rate quantifies how frequently the gating function changes its expert selection across consecutive frames. A higher switching rate indicates less temporal stability and greater responsiveness to acoustic variations. As shown in Table  IV , switching rates slightly 2) Agreement between SE and SER gates: Agreement measures the proportion of frames where the SE and SER tasks select the same expert. Across SNR levels, agreement values were remarkably stable, indicating that noise conditions did not strongly affect the extent of shared expert usage as shown in Table  IV . Clear differences emerged across emotions in Table  V , where Angry utterances showed the lowest agreement and Happy utterances achieved the highest agreement. This suggests that emotional content, rather than noise, primarily drives divergence or convergence between SE and SER gating.\n\n3) Expert usage distributions: Expert usage reflects the long-term allocation of frames to each expert. For SER, Expert 0 dominated across all conditions, but its contribution decreased with increasing SNR, while Experts 1 and 2 became more utilized as illustrated in Fig.  2 . This indicates that under cleaner conditions, the gate distributes its reliance more evenly. For SE, the trend was less pronounced: Expert 0 and 2 usage decreased with SNR, but usage across Experts 0, 1, and 2 remained relatively steady at 0, 5, and 10 dB as shown in Fig.  2 . Considering emotion classes in Fig.  3 , SER remained heavily reliant on Expert 0 overall, with Happy utterances also showing strong reliance on Expert 2. For SE, specialization was clearer: Neutral relied more on Expert 0, Sad on Expert 1, and Angry/Happy on Expert 2. These patterns suggest that while SER favors a dominant expert with some emotion-dependent variation, SE distributes responsibilities more evenly and shows stronger emotiondependent specialization.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "E. Ablation Study",
      "text": "1) Effect of Expert Balancing Loss: Many previous MoE models  [61] ,  [77] , such as the Switch Transformer, introduce an expert balancing loss to encourage uniform expert utilization. This auxiliary loss penalizes uneven expert usage during training, with the goal of preventing the model from over-  relying on a small subset of experts and thus limiting its capacity.\n\nWe evaluate the impact of this loss in our MTL setup by conducting an ablation study. Specifically, we adopt the expert balancing loss formulation from the Switch Transformer and compare models trained with and without it. As shown in Table  VI , for the SER task, the balancing loss improves performance under the seen noise conditions at -5 dB and 0 dB SNR. However, under most other noise conditions, particularly in unseen environments, models trained without the expert balancing loss achieve better performance.\n\nA similar trend is observed for the SE task, as shown in Table  VII . While the balancing loss leads to better results under the seen noise condition, it does not improve generalization to unseen noise. We hypothesize that the loss may force experts to be uniformly shared across tasks or input conditions, even when doing so is suboptimal. This could introduce conflicting gradient signals and hinder task-specific specialization, as discussed in  [78] . Based on these findings, we do not include the expert balancing loss in our final model.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Conclusions",
      "text": "This paper proposed Sparse MERIT, a MoE framework for MTL of SE and SER. Sparse MERIT integrates multilayer self-supervised representations through frame-wise expert routing, enabling task-specific specialization while maintaining a shared backbone. The model uses task-dependent gating networks to select from a shared set of experts at the frame level, improving learning flexibility and reducing interference between tasks.\n\nExperiments on the MSP-Podcast corpus demonstrate that Sparse MERIT significantly improves robustness under noisy conditions. For the SER task, Sparse MERIT achieves a 12.4% F1-macro improvement over a baseline relying on SE pre-processing and a 3.8% improvement over a naive Under another unseen noise condition using DNS noise, Sparse MERIT improves F1-macro by 11.6% compared to the SE pre-processing baseline and by 2.9% over the naive MTL baseline. In addition to its strong performance under low-SNR and unseen noise, Sparse MERIT also performs competitively under high-SNR scenarios, even on seen noise conditions where the SE pre-processing baseline typically performs better at low SNR. This finding suggests that Sparse MERIT can help prevent the distortion effects that front-end enhancement models may introduce under low-interference conditions. Furthermore, Sparse MERIT consistently outperforms adaptive MTL baselines, including uncertainty-based loss weighting and PCGrad. For the SE task, Sparse MERIT also demonstrates superior performance across commonly used speech quality metrics under the same test conditions. These results indicate that Sparse MERIT architecture improves MTL effectiveness, offering better generalizability and robustness across both tasks.\n\nIn future work, we plan to explore more flexible expert mechanisms, such as allowing the model to dynamically determine the number of experts to activate per frame instead of using a fixed Top-K selection. We also aim to investigate the use of shared experts across tasks and evaluate Sparse MERIT in more complex conditions, including reverberant environments and multilingual speech, to further assess its generalizability. Furthermore, we plan to extend our method to more diverse speech-related multi-task learning scenarios, such as combining speech recognition, speaker identification, or affective attribute prediction, to evaluate its scalability and task-transfer potential.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed Sparse MERIT framework for enhanced speech emotion recognition, leveraging unified self-supervised speech representations through",
      "page": 4
    },
    {
      "caption": "Figure 1: illustrates the overall",
      "page": 4
    },
    {
      "caption": "Figure 2: This indicates that",
      "page": 9
    },
    {
      "caption": "Figure 2: Considering emotion classes in Fig.",
      "page": 9
    },
    {
      "caption": "Figure 2: Expert usage distributions across SNR conditions.",
      "page": 9
    },
    {
      "caption": "Figure 3: Frame-level expert usage distributions across emotion classes.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CRSS-4ENGLISH-14 (Seen)": "SNR\nModel\nPESQ\nCSIG\nCBAK\nCOVL\nSSNR\nSTOI",
          "Freesound (Unseen)": "PESQ\nCSIG\nCBAK\nCOVL\nSSNR\nSTOI",
          "DNS (Unseen)": "PESQ\nCSIG\nCBAK\nCOVL\nSSNR\nSTOI"
        },
        {
          "CRSS-4ENGLISH-14 (Seen)": "Noisy\n1.08\n1.77\n1.37\n1.34\n-5.38\n0.49\n1.29\n2.77\n2.19\n2.02\n2.06\n0.68\nFine-tuned\nFT-MTL\n1.16\n2.48\n1.91\n1.78\n-0.01\n0.60\n-5 dB\nFT-MTL w/ Uncertainty\n1.17\n2.52\n1.94\n1.81\n0.20\n0.62\nFT-MTL w/ PCGrad\n1.16\n2.46\n1.94\n1.77\n0.25\n0.60\nDense MERIT\n1.14\n2.38\n1.83\n1.71\n-0.69\n0.58\nSparse MERIT\n1.13\n2.32\n1.81\n1.67\n-0.85\n0.57",
          "Freesound (Unseen)": "1.09\n2.14\n1.48\n1.55\n-5.32\n0.63\n1.14\n1.99\n1.52\n1.50\n-4.72\n0.55\n1.11\n2.14\n1.56\n1.57\n-4.54\n0.64\n1.11\n2.17\n1.55\n1.58\n-4.59\n0.64\n1.11\n2.18\n1.55\n1.59\n-4.63\n0.64\n1.12\n2.23\n1.59\n1.62\n-4.22\n0.65\n2.27\n1.60\n1.64\n-4.15\n0.66\n1.12",
          "DNS (Unseen)": "1.11\n1.99\n1.68\n1.50\n-2.58\n0.65\n1.15\n1.85\n1.69\n1.45\n-2.23\n0.56\n1.13\n1.98\n1.75\n1.51\n-1.81\n0.64\n1.13\n2.00\n1.75\n1.52\n-1.84\n0.64\n1.13\n2.00\n1.74\n1.52\n-1.90\n0.64\n1.14\n2.06\n1.78\n1.56\n-1.51\n0.65\n1.15\n2.13\n1.82\n1.60\n-1.24\n0.67"
        },
        {
          "CRSS-4ENGLISH-14 (Seen)": "Noisy\n1.10\n2.16\n1.68\n1.57\n-2.35\n0.62\n1.76\n3.51\n2.72\n2.66\n5.24\n0.83\nFine-tuned\nFT-MTL\n1.59\n3.34\n2.55\n2.48\n4.35\n0.81\n0dB\nFT-MTL w/ Uncertainty\n1.59\n3.34\n2.56\n2.48\n4.38\n0.81\nFT-MTL w/ PCGrad\n1.55\n3.28\n2.52\n2.43\n4.22\n0.80\nDense MERIT\n1.57\n3.31\n2.52\n2.45\n4.13\n0.80\nSparse MERIT\n1.48\n3.18\n2.44\n2.33\n3.70\n0.78",
          "Freesound (Unseen)": "1.15\n2.50\n1.79\n1.79\n-2.28\n0.73\n1.26\n2.69\n1.99\n1.95\n-0.51\n0.75\n1.23\n2.68\n1.96\n1.94\n-0.80\n0.76\n1.23\n2.70\n1.95\n1.94\n-0.87\n0.77\n1.23\n2.71\n1.94\n1.94\n-0.95\n0.76\n0.78\n1.28\n2.84\n2.04\n2.04\n-0.09\n1.29\n2.86\n2.06\n2.06\n0.12\n0.78",
          "DNS (Unseen)": "1.16\n2.34\n1.98\n1.72\n0.52\n0.74\n1.27\n2.48\n2.15\n1.85\n1.99\n0.72\n1.24\n2.51\n2.16\n1.86\n2.02\n0.76\n1.24\n2.51\n2.15\n1.86\n1.95\n0.76\n1.23\n2.52\n2.14\n1.86\n1.88\n0.76\n0.78\n1.29\n2.68\n2.24\n1.97\n2.75\n1.31\n2.73\n2.27\n2.01\n2.98\n0.78"
        },
        {
          "CRSS-4ENGLISH-14 (Seen)": "Noisy\n1.20\n2.61\n2.06\n1.88\n1.20\n0.74\n2.31\n4.04\n3.19\n3.21\n8.03\n0.89\nFine-tuned\nFT-MTL\n2.16\n3.92\n3.10\n3.07\n7.74\n0.88\n0.89\n5dB\nFT-MTL w/ Uncertainty\n2.16\n3.92\n3.10\n3.07\n7.76\nFT-MTL w/ PCGrad\n2.12\n3.88\n3.07\n3.03\n7.63\n0.88\nDense MERIT\n2.14\n3.91\n3.08\n3.06\n7.66\n0.88\nSparse MERIT\n1.97\n3.74\n2.96\n2.89\n7.19\n0.87",
          "Freesound (Unseen)": "1.29\n2.91\n2.16\n2.09\n1.25\n0.82\n1.66\n3.38\n2.59\n2.53\n4.39\n0.86\n1.59\n3.32\n2.51\n2.46\n3.80\n0.86\n1.59\n3.33\n2.52\n2.47\n3.84\n0.86\n1.57\n3.33\n2.49\n2.46\n3.61\n0.86\n0.87\n1.71\n3.50\n2.64\n2.63\n4.67\n1.74\n3.53\n2.68\n2.65\n5.07\n0.87",
          "DNS (Unseen)": "1.29\n2.75\n2.35\n2.02\n4.06\n0.82\n1.63\n3.23\n2.73\n2.44\n6.74\n0.84\n1.55\n3.16\n2.68\n2.37\n6.44\n0.85\n1.55\n3.15\n2.67\n2.36\n6.40\n0.85\n1.54\n3.16\n2.67\n2.37\n6.37\n0.85\n0.87\n1.70\n3.40\n2.83\n2.57\n7.46\n1.72\n3.41\n2.84\n2.59\n7.49\n0.87"
        },
        {
          "CRSS-4ENGLISH-14 (Seen)": "Noisy\n1.42\n3.09\n2.50\n2.26\n5.15\n0.83\n2.81\n4.46\n3.63\n3.68\n10.82\n0.93\nFine-tuned\nFT-MTL\n2.68\n4.36\n3.55\n3.56\n10.67\n0.92\n10 dB\nFT-MTL w/ Uncertainty\n2.68\n4.35\n3.55\n3.56\n10.68\n0.92\nFT-MTL w/ PCGrad\n2.64\n4.33\n3.53\n3.53\n10.58\n0.92\nDense MERIT\n2.65\n4.34\n3.54\n3.54\n10.61\n0.92\nSparse MERIT\n2.47\n4.18\n3.41\n3.36\n10.13\n0.91",
          "Freesound (Unseen)": "1.57\n3.36\n2.60\n2.48\n5.17\n0.88\n0.92\n2.28\n4.03\n3.23\n3.19\n8.99\n0.92\n2.20\n3.97\n3.16\n3.12\n8.48\n0.92\n2.22\n3.99\n3.19\n3.14\n8.68\n0.92\n2.18\n3.97\n3.14\n3.11\n8.29\n0.92\n2.32\n4.10\n3.26\n3.24\n9.04\n2.33\n4.11\n3.29\n3.26\n9.34\n0.92",
          "DNS (Unseen)": "1.53\n3.20\n2.77\n2.38\n7.90\n0.88\n2.18\n3.88\n3.32\n3.07\n10.98\n0.91\n2.10\n3.81\n3.26\n2.99\n10.67\n0.91\n2.10\n3.80\n3.26\n2.99\n10.67\n0.91\n2.09\n3.81\n3.25\n2.99\n10.64\n0.91\n2.29\n4.01\n3.41\n3.19\n11.46\n0.92\n0.92\n2.28\n3.99\n3.39\n3.18\n11.25"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Adaptive virtual assistant interaction through real-time speech emotion analysis using hybrid deep learning models and contextual awareness",
      "authors": [
        "E Zadeh",
        "M Alaeifard"
      ],
      "year": "2023",
      "venue": "International Journal of Advanced Human Computer Interaction"
    },
    {
      "citation_id": "2",
      "title": "Real-time speech emotion analysis for smart home assistants",
      "authors": [
        "R Chatterjee",
        "S Mazumdar",
        "R Sherratt",
        "R Halder",
        "T Maitra",
        "D Giri"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Consumer Electronics"
    },
    {
      "citation_id": "3",
      "title": "Enabling intelligent environment by the design of emotionally aware virtual assistant: A case of smart campus",
      "authors": [
        "P.-S Chiu",
        "J.-W Chang",
        "M.-C Lee",
        "C.-H Chen",
        "D.-S Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "Multilayer neural network based speech emotion recognition for smart assistance",
      "authors": [
        "S Kumar",
        "M Haq",
        "A Jain",
        "C Jason",
        "N Moparthi",
        "N Mittal",
        "Z Alzamil"
      ],
      "year": "2023",
      "venue": "Computers, Materials & Continua"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using supervised deep recurrent system for mental health monitoring",
      "authors": [
        "N Elsayed",
        "Z Elsayed",
        "N Asadizanjani",
        "M Ozer",
        "A Abdelgawad",
        "M Bayoumi"
      ],
      "year": "2022",
      "venue": "2022 IEEE 8th World Forum on Internet of Things"
    },
    {
      "citation_id": "6",
      "title": "Automatic speech emotion recognition using machine learning: digital transformation of mental health",
      "authors": [
        "S Madanian",
        "D Parry",
        "O Adeleye",
        "C Poellabauer",
        "F Mirza",
        "S Mathew",
        "S Schneider"
      ],
      "venue": "Proceedings of the Annual Pacific Asia Conference on Information Systems (PACIS)"
    },
    {
      "citation_id": "7",
      "title": "Depression severity classification from speech emotion",
      "authors": [
        "S Harati",
        "A Crowell",
        "H Mayberg",
        "S Nemati"
      ],
      "year": "2018",
      "venue": "2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition for power customer service",
      "authors": [
        "X Li",
        "R Lin"
      ],
      "year": "2021",
      "venue": "2021 7th International Conference on Computer and Communications (ICCC)"
    },
    {
      "citation_id": "9",
      "title": "Ordinal learning for emotion recognition in customer service calls",
      "authors": [
        "W Han",
        "T Jiang",
        "Y Li",
        "B Schuller",
        "H Ruan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "End-to-end continuous speech emotion recognition in real-life customer service call center conversations",
      "authors": [
        "Y Feng",
        "L Devillers"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "11",
      "title": "Front-end feature compensation and denoising for noise robust speech emotion recognition",
      "authors": [
        "R Chakraborty",
        "A Panda",
        "M Pandharipande",
        "S Joshi",
        "S Kopparapu"
      ],
      "year": "2019",
      "venue": "Front-end feature compensation and denoising for noise robust speech emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in the noise applying large acoustic feature sets",
      "authors": [
        "B Schuller",
        "D Arsic",
        "F Wallhoff",
        "G Rigoll"
      ],
      "year": "2006",
      "venue": "Speech Prosody"
    },
    {
      "citation_id": "13",
      "title": "Not all features are equal: Selection of robust features for speech emotion recognition in noisy environments",
      "authors": [
        "S.-G Leem",
        "D Fulford",
        "J.-P Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)"
    },
    {
      "citation_id": "14",
      "title": "Metricaug: A distortion metric-lead augmentation strategy for training noise-robust speech emotion recognizer",
      "authors": [
        "Y.-T Wu",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "15",
      "title": "Best practices for noise-based augmentation to improve the performance of deployable speech-based emotion recognition systems",
      "authors": [
        "E Provost"
      ],
      "year": "2021",
      "venue": "Best practices for noise-based augmentation to improve the performance of deployable speech-based emotion recognition systems",
      "arxiv": "arXiv:2104.08806"
    },
    {
      "citation_id": "16",
      "title": "Reinforcement learning based data augmentation for noise robust speech emotion recognition",
      "authors": [
        "S Ranjan",
        "R Chakraborty",
        "S Kopparapu"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Multi-conditioning and data augmentation using generative noise model for speech emotion recognition in noisy conditions",
      "authors": [
        "U Tiwari",
        "M Soni",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Computation and memory efficient noise adaptation of Wav2Vec2.0 for noisy speech emotion recognition with skip connection adapters",
      "authors": [
        "S.-G Leem",
        "D Fulford",
        "J.-P Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2023",
      "venue": "Interspeech 2023"
    },
    {
      "citation_id": "19",
      "title": "Describe where you are: Improving noise-robustness for speech emotion recognition with text description of the environment",
      "year": "2024",
      "venue": "Describe where you are: Improving noise-robustness for speech emotion recognition with text description of the environment",
      "arxiv": "arXiv:2407.17716"
    },
    {
      "citation_id": "20",
      "title": "Towards noise robust speech emotion recognition using dynamic layer customization",
      "authors": [
        "A Wilf",
        "E Provost"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "21",
      "title": "Separation of emotional and reconstruction embeddings on ladder network to improve speech emotion recognition robustness in noisy conditions",
      "authors": [
        "S.-G Leem",
        "D Fulford",
        "J.-P Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2021",
      "venue": "Interspeech 2021"
    },
    {
      "citation_id": "22",
      "title": "Adapting a self-supervised speech representation for noisy speech emotion recognition by using contrastive teacher-student learning",
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023)"
    },
    {
      "citation_id": "23",
      "title": "End-to-end speech emotion recognition: Challenges of real-life emergency call centers data recordings",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "24",
      "title": "Enhancing emergency response through speech emotion recognition: A machine learning approach",
      "authors": [
        "P Deb",
        "H Mahrin",
        "A Bhuiyan"
      ],
      "year": "2023",
      "venue": "2023 26th International Conference on Computer and Information Technology (ICCIT)"
    },
    {
      "citation_id": "25",
      "title": "Investigating transformer encoders and fusion strategies for speech emotion recognition in emergency call center conversations",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2022",
      "venue": "Companion Publication of the 2022 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos",
        "G Keren",
        "J Wagner",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Towards robust speech emotion recognition using deep residual networks for speech enhancement"
    },
    {
      "citation_id": "27",
      "title": "Task-specific speech enhancement and data augmentation for improved multimodal emotion recognition under noisy conditions",
      "authors": [
        "S Kshirsagar",
        "A Pendyala",
        "T Falk"
      ],
      "year": "2023",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "28",
      "title": "Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement",
      "authors": [
        "Y.-W Chen",
        "J Hirschberg",
        "Y Tsao"
      ],
      "year": "2023",
      "venue": "Noise robust speech emotion recognition with signal-to-noise ratio adapting speech enhancement",
      "arxiv": "arXiv:2309.01164"
    },
    {
      "citation_id": "29",
      "title": "Investigating speech enhancement and perceptual quality for speech emotion recognition",
      "authors": [
        "A Avila",
        "M Alam",
        "D O'shaughnessy",
        "T Falk"
      ],
      "year": "2018",
      "venue": "Investigating speech enhancement and perceptual quality for speech emotion recognition"
    },
    {
      "citation_id": "30",
      "title": "Noise-robust speech emotion recognition using shared self-supervised representations with integrated speech enhancement",
      "authors": [
        "J.-T Tzeng",
        "S.-G Leem",
        "A Salman",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2025",
      "venue": "ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
      "authors": [
        "Z Chen",
        "V Badrinarayanan",
        "C.-Y Lee",
        "A Rabinovich"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "32",
      "title": "Gradient surgery for multi-task learning",
      "authors": [
        "T Yu",
        "S Kumar",
        "A Gupta",
        "S Levine",
        "K Hausman",
        "C Finn"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts",
      "authors": [
        "J Ma",
        "Z Zhao",
        "X Yi",
        "J Chen",
        "L Hong",
        "E Chi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "34",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech From Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Odyssey 2024speech emotion recognition challenge: Dataset, baseline framework, and results",
      "authors": [
        "L Goncalves",
        "A Salman",
        "A Reddy Naini",
        "L Moro-Velazquez",
        "T Thebaud",
        "P Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2024",
      "venue": "The Speaker and Language Recognition Workshop"
    },
    {
      "citation_id": "36",
      "title": "Selective acoustic feature enhancement for speech emotion recognition with noisy speech",
      "authors": [
        "S.-G Leem",
        "D Fulford",
        "J.-P Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing"
    },
    {
      "citation_id": "37",
      "title": "Robust front-end processing for emotion recognition in noisy speech",
      "authors": [
        "M Pandharipande",
        "R Chakraborty",
        "A Panda",
        "S Kopparapu"
      ],
      "year": "2018",
      "venue": "2018 11th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "38",
      "title": "An unsupervised frame selection technique for robust emotion recognition in noisy speech",
      "year": "2018",
      "venue": "2018 26th European Signal Processing Conference"
    },
    {
      "citation_id": "39",
      "title": "Keep, delete, or substitute: Frame selection strategy for noise-robust speech emotion recognition",
      "authors": [
        "S.-G Leem",
        "D Fulford",
        "J Onnela",
        "D Gard",
        "C Busso"
      ],
      "year": "2024",
      "venue": "Interspeech 2024"
    },
    {
      "citation_id": "40",
      "title": "From neural pca to deep unsupervised learning",
      "authors": [
        "H Valpola"
      ],
      "year": "2015",
      "venue": "Advances in independent component analysis and learning machines"
    },
    {
      "citation_id": "41",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "42",
      "title": "Domain separation networks",
      "authors": [
        "K Bousmalis",
        "G Trigeorgis",
        "N Silberman",
        "D Krishnan",
        "D Erhan"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Spectral feature mapping with mimic loss for robust speech recognition",
      "authors": [
        "D Bagchi",
        "P Plantinga",
        "A Stiff",
        "E Fosler-Lussier"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition"
    },
    {
      "citation_id": "45",
      "title": "Versatile audio-visual learning for emotion recognition",
      "authors": [
        "L Goncalves",
        "S.-G Leem",
        "W.-C Lin",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "48",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "50",
      "title": "SUPERB-SG: Enhanced speech processing universal PERformance benchmark for semantic and generative capabilities",
      "authors": [
        "H.-S Tsai",
        "H.-J Chang",
        "W.-C Huang",
        "Z Huang",
        "K Lakhotia",
        "S -W. Yang",
        "S Dong",
        "A Liu",
        "C.-I Lai",
        "J Shi",
        "X Chang",
        "P Hall",
        "H.-J Chen",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H.-Y Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "51",
      "title": "Boosting self-supervised embeddings for speech enhancement",
      "authors": [
        "K.-H Hung",
        "S Fu",
        "H.-H Tseng",
        "H.-T Chiang",
        "Y Tsao",
        "C.-W Lin"
      ],
      "venue": "Boosting self-supervised embeddings for speech enhancement"
    },
    {
      "citation_id": "52",
      "title": "Exploring wavlm on speech enhancement",
      "authors": [
        "H Song",
        "S Chen",
        "Z Chen",
        "Y Wu",
        "T Yoshioka",
        "M Tang",
        "J Shin",
        "S Liu"
      ],
      "year": "2023",
      "venue": "2022 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "53",
      "title": "Speech enhancement using selfsupervised pre-trained model and vector quantization",
      "authors": [
        "X.-Y Zhao",
        "Q.-S Zhu",
        "J Zhang"
      ],
      "year": "2022",
      "venue": "2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "54",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "55",
      "title": "Evaluating self-supervised speech representations for speech emotion recognition",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "56",
      "title": "The Interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "A Reddy Naini",
        "L Goncalves",
        "A Salman",
        "P Mote",
        "I Ülgen",
        "T Thebaud",
        "L Moro-Velazquez",
        "L Garcia",
        "N Dehak",
        "B Sisman",
        "C Busso"
      ],
      "year": "2025",
      "venue": "Interspeech 2025"
    },
    {
      "citation_id": "57",
      "title": "Multitask sequence to sequence learning",
      "authors": [
        "M.-T Luong",
        "Q Le",
        "I Sutskever",
        "O Vinyals",
        "L Kaiser"
      ],
      "year": "2015",
      "venue": "Multitask sequence to sequence learning",
      "arxiv": "arXiv:1511.06114"
    },
    {
      "citation_id": "58",
      "title": "One model to learn them all",
      "authors": [
        "L Kaiser",
        "A Gomez",
        "N Shazeer",
        "A Vaswani",
        "N Parmar",
        "L Jones",
        "J Uszkoreit"
      ],
      "year": "2017",
      "venue": "One model to learn them all",
      "arxiv": "arXiv:1706.05137"
    },
    {
      "citation_id": "59",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "A Kendall",
        "Y Gal",
        "R Cipolla"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "60",
      "title": "Sparsely activated mixture-of-experts are robust multi-task learners",
      "authors": [
        "S Gupta",
        "S Mukherjee",
        "K Subudhi",
        "E Gonzalez",
        "D Jose",
        "A Awadallah",
        "J Gao"
      ],
      "year": "2022",
      "venue": "Sparsely activated mixture-of-experts are robust multi-task learners",
      "arxiv": "arXiv:2204.07689"
    },
    {
      "citation_id": "61",
      "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
      "authors": [
        "W Fedus",
        "B Zoph",
        "N Shazeer"
      ],
      "year": "2022",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "62",
      "title": "Mixtral of experts",
      "authors": [
        "A Jiang",
        "A Sablayrolles",
        "A Roux",
        "A Mensch",
        "B Savary",
        "C Bamford",
        "D Chaplot",
        "D Casas",
        "E Hanna",
        "F Bressand"
      ],
      "year": "2024",
      "venue": "Mixtral of experts",
      "arxiv": "arXiv:2401.04088"
    },
    {
      "citation_id": "63",
      "title": "Qwen3 technical report",
      "authors": [
        "A Yang",
        "A Li",
        "B Yang",
        "B Zhang",
        "B Hui",
        "B Zheng",
        "B Yu",
        "C Gao",
        "C Huang",
        "C Lv"
      ],
      "year": "2025",
      "venue": "Qwen3 technical report",
      "arxiv": "arXiv:2505.09388"
    },
    {
      "citation_id": "64",
      "title": "Llama-moe: Building mixture-of-experts from llama with continual pretraining",
      "authors": [
        "T Zhu",
        "X Qu",
        "D Dong",
        "J Ruan",
        "J Tong",
        "C He",
        "Y Cheng"
      ],
      "year": "2024",
      "venue": "Llama-moe: Building mixture-of-experts from llama with continual pretraining",
      "arxiv": "arXiv:2406.16554"
    },
    {
      "citation_id": "65",
      "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models",
      "authors": [
        "D Dai",
        "C Deng",
        "C Zhao",
        "R Xu",
        "H Gao",
        "D Chen",
        "J Li",
        "W Zeng",
        "X Yu",
        "Y Wu"
      ],
      "year": "2024",
      "venue": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models",
      "arxiv": "arXiv:2401.06066"
    },
    {
      "citation_id": "66",
      "title": "M 3 vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design",
      "authors": [
        "H Liang",
        "Z Fan",
        "R Sarkar",
        "Z Jiang",
        "T Chen",
        "K Zou",
        "Y Cheng",
        "C Hao",
        "Z Wang"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "67",
      "title": "Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts",
      "authors": [
        "Z You",
        "S Feng",
        "D Su",
        "D Yu"
      ],
      "venue": "Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts"
    },
    {
      "citation_id": "68",
      "title": "Speechmoe2: Mixture-of-experts model with improved routing",
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "69",
      "title": "Language-routing mixture of experts for multilingual and code-switching speech recognition",
      "authors": [
        "W Wang",
        "G Ma",
        "Y Li",
        "B Du"
      ],
      "venue": "Language-routing mixture of experts for multilingual and code-switching speech recognition"
    },
    {
      "citation_id": "70",
      "title": "Mixture-of-expert conformer for streaming multilingual asr",
      "authors": [
        "K Hu",
        "B Li",
        "T Sainath",
        "Y Zhang",
        "F Beaufays"
      ],
      "year": "2023",
      "venue": "Mixture-of-expert conformer for streaming multilingual asr"
    },
    {
      "citation_id": "71",
      "title": "Attentive statistics pooling for deep speaker embedding",
      "authors": [
        "K Okabe",
        "T Koshinaka",
        "K Shinoda"
      ],
      "year": "2018",
      "venue": "Attentive statistics pooling for deep speaker embedding"
    },
    {
      "citation_id": "72",
      "title": "Boosting objective scores of a speech enhancement model by metricgan post-processing",
      "authors": [
        "S.-W Fu",
        "C.-F Liao",
        "T.-A Hsieh",
        "K.-H Hung",
        "S.-S Wang",
        "C Yu",
        "H.-C Kuo",
        "R Zezario",
        "Y.-J Li",
        "S.-Y Chuang"
      ],
      "year": "2020",
      "venue": "2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "73",
      "title": "Gating Neural Network for Large Vocabulary Audiovisual Speech Recognition",
      "authors": [
        "F Tao",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "74",
      "title": "Freesound datasets: a platform for the creation of open audio datasets",
      "authors": [
        "E Fonseca",
        "J Pons Puig",
        "X Favory",
        "F Font",
        "D Corbera",
        "A Bogdanov",
        "S Ferraro",
        "A Oramas",
        "X Porter",
        "Serra"
      ],
      "year": "2017",
      "venue": "Proceedings of the 18th ISMIR Conference"
    },
    {
      "citation_id": "75",
      "title": "Icassp 2023 deep noise suppression challenge",
      "authors": [
        "H Dubey",
        "A Aazami",
        "V Gopal",
        "B Naderi",
        "S Braun",
        "R Cutler",
        "A Ju",
        "M Zohourian",
        "M Tang",
        "M Golestaneh"
      ],
      "year": "2024",
      "venue": "IEEE Open Journal of Signal Processing"
    },
    {
      "citation_id": "76",
      "title": "Investigating RNN-based speech enhancement methods for noise-robust Textto-Speech",
      "authors": [
        "C Valentini-Botinhao",
        "X Wang",
        "S Takaki",
        "J Yamagishi"
      ],
      "year": "2016",
      "venue": "Proc. 9th ISCA Workshop on Speech Synthesis Workshop"
    },
    {
      "citation_id": "77",
      "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "authors": [
        "N Shazeer",
        "A Mirhoseini",
        "K Maziarz",
        "A Davis",
        "Q Le",
        "G Hinton",
        "J Dean"
      ],
      "year": "2017",
      "venue": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "arxiv": "arXiv:1701.06538"
    },
    {
      "citation_id": "78",
      "title": "Mod-squad: Designing mixtures of experts as modular multi-task learners",
      "authors": [
        "Z Chen",
        "Y Shen",
        "M Ding",
        "Z Chen",
        "H Zhao",
        "E Learned-Miller",
        "C Gan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}