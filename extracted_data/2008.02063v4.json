{
  "paper_id": "2008.02063v4",
  "title": "Compact Graph Architecture For Speech Emotion Recognition",
  "published": "2020-08-05T12:09:09Z",
  "authors": [
    "A. Shirian",
    "T. Guha"
  ],
  "keywords": [
    "Speech emotion recognition",
    "graph convolutional networks",
    "graph signal processing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose a deep graph approach to address the task of speech emotion recognition. A compact, efficient and scalable way to represent data is in the form of graphs. Following the theory of graph signal processing, we propose to model speech signal as a cycle graph or a line graph. Such graph structure enables us to construct a Graph Convolution Network (GCN)-based architecture that can perform an accurate graph convolution in contrast to the approximate convolution used in standard GCNs. We evaluated the performance of our model for speech emotion recognition on the popular IEMOCAP and MSP-IMPROV databases. Our model outperforms standard GCN and other relevant deep graph architectures indicating the effectiveness of our approach. When compared with existing speech emotion recognition methods, our model achieves comparable performance to the state-of-the-art with significantly fewer learnable parameters (∼30K) indicating its applicability in resource-constrained devices. Our code is available at /github.com/AmirSh15/Compact SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Machine recognition of emotional content in speech is crucial in many human-centric systems, such as behavioral health monitoring and empathetic conversational systems. Speech Emotion Recognition (SER) in general is a challenging task due to the huge variability in emotion expression and perception across speakers, languages and cultures.\n\nMany SER approaches follow a two-stage framework, where a set of Low-Level Descriptors (LLDs) are first extracted from raw audio. The LLDs are then input to a deep learning model to generate discrete (or continuous) emotion labels  [1, 2, 3, 4] . While using hand-crafted acoustic features is still common in SER, lexical features  [5, 6]  and log Mel spectrograms are also used as inputs  [7] . Spectrograms are often used with Convolutional Neural Networks (CNNs)  [7]  that does not explicitly model the speech dynamics. Explicit modeling of the temporal dynamics is important in SER as it reflects the changes in emotion dynamics  [8] . To capture the temporal dynamics of emotion, recurrent models, especially the Long-Short Term Memory networks (LSTMs), are popular  [2, 3, 4] . The recurrent models, though predominant in SER, often lead to complex architecture with millions of trainable parameters.\n\nA compact, efficient and scalable way to represent data is in the form of graphs. Graph Convolutional Networks (GCNs)  [9]  have been successfully used to address various problems in computer vision and natural language processing, such as action recognition  [10] , object tracking  [11]  and text classification  [12] . In the context of audio analysis, we are aware of only one recent work that proposed an attention-based graph neural network architecture for few-shot audio classification  [13] .\n\nMotivated by the recent success of GCNs, we propose to adopt a deep graph approach to SER. We base our work on spectral GCNs which have a strong foundation on graph signal processing  [14] . Spectral GCNs perform convolution operation on the spectrum of the graph Laplacian considering the convolution kernel (a diagonal matrix) to be learnable  [15] . This involves eigen decomposition of the graph Laplacian matrix, which is computationally expensive. To reduce the computational cost, ChebNet approximates the convolution operation (including the learnable convolution kernel) in terms of Chebyshev polynomials  [16] . The most popular form of GCN uses a first order approximation of the Chebyshev polynomial to further simplify the convolution operation to a linear projection  [9] . Such GCN models are simple to implement, and have been successfully used for various node classification tasks in social media networks and citation networks  [9] .\n\nIn this paper, we cast SER as a graph classification problem. We model a speech signal as a simple graph, where each node corresponds to a short windowed segment of the signal. Each node is connected to only two adjacent nodes thus transforming the signal to a line graph or a cycle graph. Owing to this particular graph structure, we take advantage of certain results in graph signal processing  [17]  to perform accurate graph convolution (in contrast to the approximations used in popular GCNs). This leads to a light-weight GCN architecture with superior emotion recognition performance on the IEMOCAP  [18]  and the MSP-IMPROV  [19]  databases.\n\nTo summarize, the contributions of our work are as follows: (i) To the best of our knowledge, this is the first work that takes a graph classification approach to SER. (ii) Leveraging theories from graph signal processing, we propose a GCN-based graph classification approach that can efficiently perform accurate graph convolution. (iii) Our model has significantly fewer trainable parameters (∼30K only). Despite its smaller size, our model achieves superior performance on both IEMOCAP and MSP-IMPROV databases outperforming relevant and competitive baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "In this section, we describe our graph classification approach to SER. First, we construct a graph from each speech sample. Next, we develop a new GCN architecture that assigns a discrete emotion label to each (speech sample transformed to) graph. Fig.  1  gives an overview of our approach. Below, we describe each component in detail.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Graph Construction",
      "text": "Given a speech signal (utterance), the first step is to construct a corresponding graph G = (V, E), where V ∈ {vi} M i=1 is the set of M nodes, and E is the set of all edges between the nodes. The adjacency matrix of G is denoted by A ∈ R M ×M where an element (A)ij denotes the edge weight connecting vi and vj. Our graph construction strategy follows a simple frame-to-node transformation, where M frames (short, overlapping segments) of the speech signal form the M nodes in G (see Fig.  2(a) ). Since the graph structure is not naturally defined here, we investigate two simple undirected graph structures: (i) a cycle graph defined by the adjacency matrix Ac, and (ii) a line graph defined by adjacency A l . The two graph structures are shown in Fig.  2(b)-(c) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fully Connected Layer",
      "text": "These two graph structures are important because of the special structures of their graph Laplacians, which significantly simplifies spectral GCN operations. This is discussed in the following section in more detail. Each node vi is also associated with a node feature vector xi ∈ R P . A node feature vector contain LLDs extracted from the corresponding speech segment. A feature matrix X ∈ R M ×P containing all node feature vectors is defined as",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Graph Classification",
      "text": "Given a set of (utterances transformed to) graphs {G1, ..., GN } and their true labels {y1, ..., yN } represented as one-hot vectors, our task is to develop a GCN architecture that is able to recognize the emotional content in the utterances. Our architecture comprises two graph convolution layers, a pooling layer that yields a graph-level embedding vector, followed by a fully connected layer that produces the discrete emotion labels (Fig.  1 ). Graph convolution layer. We base our model on a spectral GCN, which performs graph convolution in the spectral domain. Following the theory of graph signal processing  [14] , graph convolution in time domain is defined as\n\n. . . where w is the graph convolution kernel (learnable) and xi is the input node features. This is equivalent to a product in the graph spectral domain. ĥ = ŵ\n\nwhere ĥ, xi, and ĝ denote the output, node features and the convolution filter in the graph spectral domain i.e., their graph Fourier transforms (GFT). Considering the node feature matrix and sdopting a matrix notation, we get\n\nIn order to have X and Ŵ, we usually compute the normalized graph Laplacian matrix\n\nwhere D is the degree matrix, L = D-A where A is the adjacency matrix of the graph. The eigen decomposition of L can be written as\n\nwhere λi is the i th eigen value of L corresponding to the eigen vector ui, Λ = diag(λi) and\n\nThe exact graph convolution operation is thus defined as\n\nThe graph convolution propagation at k th layer thus becomes\n\nwhere H (0) = X and W is learnable. Note that for A = Ac (cycle graph), L takes the following form\n\nThe L is circulant and GFT is equivalent to the Discrete Fourier Transform (DFT)  [17] . Similarly, for A = A l (line graph), GFT is equivalent to Discrete Cosine Transform (DCT). This makes the convolution operation convenient and computationally efficient as we can avoid eigen decomposition that can be computationally expensive for arbitrary graph structures. Following a recent work on GCN  [20] , we propose to learn the convolution kernel in Eq. (  6 ) in terms of a Multi-Layer Perceptron (MLP). Finally, our convolution operation takes the following form\n\nwhere, only the MLP parameters are learnable. Pooling layer. Our objective is to classify entire graphs (as opposed to the more popular task of graph node classification). Hence, we need a function to attain a graph-level representation hG ∈ R Q from the node-level embeddings. This can be obtained by pooling the node-level embeddings H (k) at the final layer before passing them onto the classification layer. Common choices for pooling functions in graph domain are mean, max and sum pooling  [9, 21] . Max and mean pooling often fail to preserve the underlying information about the graph structure while sum pooling has shown to be a better alternative  [20] . We use sum pooling to obtain the graph-level representation:\n\nThe pooling layer is followed by one fully-connected layer which produces the classification labels. Our GCN model is trained with the cross-entropy loss =n yn log ỹn.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we present experimental results and analysis to evaluate the performance of the proposed GCN architecture.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database",
      "text": "We evaluate our model on two most popular SER databases: IEMO-CAP  [18]  and MSP-IMPROV  [26] . For both databases, a single utterance may have multiple labels owing to different annotators. We consider only the label which has majority agreement.\n\nThe IEMOCAP database contains a total of 12 hours of data collected over 5 dyadic sessions with 10 subjects. To be consistent with previous studies, we used four emotion classes :anger, joy, neutral, and sadness. The final dataset contains a total of 4490 utterances including 1103 anger, 595 joy, 1708 neutral and 1084 sad.\n\nThe MSP-IMPROV database contains utterances from 12 speakers collected across six sessions. The dataset contains a total of 7798 utterances including 792 samples for anger, 3477 for neutral, 885 for sad and 2644 samples for joy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Node Features",
      "text": "We extract a set of low-level descriptors (LLDs) from the speech utterances as proposed for Interspeech2009 emotion challenge  [27]  using the OpenSMILE toolkit  [28] . The feature set includes Melfrequency cepstral coefficients (MFCCs), zero-crossing rate, voice probability, fundamental frequency (F0) and frame energy. For each sample, we use a sliding window of length 25ms (with a stride length of 10ms) to extract the LLDs locally. Each feature is then smoothed using a moving average filter, and the smoothed version is used to compute their respective first order delta coefficients. In addition, we also add spontaneity as a binary feature for IEMOCAP as this information is known to help SER  [29] . The spontaneity information comes with the database. Altogether this yields node feature vectors of dimension P = 35 for IEMOCAP and P = 34 for MSP-IMPROV (no spontaneity feature).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "Each speech sample produces a graph of M = 120 nodes, where each node corresponds to a (overlapping) speech segment of length 25ms. Padding is used to make the samples of equal length. The dimension of the graph embedding is set to Q = 64. We perform a 5-fold cross-validation and report both average weighted accuracy (WA) and unweighted accuracy (UA). All parameters and validation remain the same for both the databases. Our network weights are initialized following the Xavier initialization  [30] . We used Adam optimizer with a learning rate of 0.01 and a decay rate of 0.5 after each 50 epochs for all experiments. We used Pytorch for our experiments on an NVIDIA RTX-2080Ti GPU.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "Comparison with graph-based models. We compare our model against three state-of-the-art deep graph models using the same node features and a cycle graph structure. GCN  [9] . A natural baseline to compare with our model is a spectral GCN in its standard form. The original network  [9]  is designed for node classification and only yields node-level embeddings. To obtain a graph-level embedding, we used the sum pooling function.\n\nPATCHY-SAN  [21] . A recent architecture that learns CNNs for arbitrary graphs. This architecture is originally developed for graph classification.\n\nPATCHY-Diff  [22] . A recent work on hierarchical GCN proposes to use differentiable pooling layer between graph convolution layers. We used this pooling layer with PATCHY-SAN as in the original paper.\n\nTable  1  and Table  2  compare our model against these existing graph models (Graph baselines) in terms of SER accuracy. All these models use the same node features. Clearly, our model outperforms all the graph baselines by a significant margin. Compared to the popular GCN  [9] , our model improves the recognition accuracy by more than 9% and 3% on IEMOCAP and MSP-IMPROV respectively. This result indicates that accurate convolution in graph domain improves the accuracy significantly. Comparison with SER state-of-the-art. In addition to the graph baselines, we compare our model with a number of recent SER models. These include a Bayesian model  [23] , CNN models (  [25] , SegCNN  [7] ), Recurrent Neural Network (RNN) architectures (  [4] , RCNN  [24] ), LSTM models (  [25] , Attn-BLSTM  [3] ) and a CNN-LSTM model  [25] . The majority of the above models (except the models by Latif et al.  [25] ) use LLDs as input. Tables  1  and 2  show that our model outperforms (i) all graph baselines despite a simpler architecture and (ii) all LSTM-based architectures (a class of models most commonly used in SER) on both databases by a significant margin. Our model (with the cycle graph structure) achieves highest WA on IEMOCAP and comparable to the state-of-the-art WA on MSP-IMPROV. In terms of UA, our model's performance is comparable to SegCNN on IEMOCAP. Nevertheless, our model has significantly fewer parameters: 30K learnable parameters vs. 8.8M in SegCNN and 0.8M in LSTM.\n\nNetwork size. Table  3  compares the number of learnable network parameters for various models with ours. All graph networks are smaller (an order of magnitude smaller) than LSTM architectures yet highly accurate. Our model has the highest accuracy with half the parameters of other graph-based networks. This is owing to the lightweight convolution operation and because of the choice of our graph structure. In our approach graph structure remains the same for all samples, which requires us to compute the eigen-decomposition only once. This operation can even be replaced by directly using DFT or DCT kernels as mentioned earlier.\n\nAblation: Between the two graph structures we investigated, higher SER accuracy is achieved using the cyclic structure on both the databases. With the line graph, the model accuracy is slightly lower. We also compare our model's performance for different pooling strategies (used to compute graph-level representation from node embeddings) in Table  4 . Aligned with observations made in past works (e.g.,  [20] ), sumpool shows improvement over meanpool and maxpool by 2.8% and 3.6% on IEMOCAP. When using graph convolution without MLP (see Eq. 6) performance drops by 1% (see Table  1  and 2 ). These results confirm that each proposed component in our network (MLP convolution, sumpool) contributes positively towards its performance.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We proposed a compact and efficient GCN architecture (with only 30K learnable parameters) for recognizing emotion content in speech. To the best of our knowledge, this is the first graph-based approach to SER. We transformed speech utterances to graphs with simple structures that largely simplify the convolution operation on graphs. Also, the graph structure we defined remains the same for all samples as our edges are not weighted. This leads to a light-weight GCN architecture which outperforms LSTMs, standard GCNs and several other recent graph models in SER. Our model produces higher or comparable performance to the state-of-the-art on two benchmark databases with significantly fewer learnable parameters.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: gives an overview",
      "page": 1
    },
    {
      "caption": "Figure 1: Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding",
      "page": 2
    },
    {
      "caption": "Figure 2: (a)). Since the",
      "page": 2
    },
    {
      "caption": "Figure 2: Graph construction from speech input. (a) LLDs are ex-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "University of Warwick, UK": "Motivated by the recent success of GCNs, we propose to adopt"
        },
        {
          "University of Warwick, UK": "a deep graph approach to SER. We base our work on spectral GCNs"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "which have a strong foundation on graph signal processing [14]."
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "Spectral GCNs perform convolution operation on the spectrum of the"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "graph Laplacian considering the convolution kernel (a diagonal ma-"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "trix) to be learnable [15]. This involves eigen decomposition of the"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "graph Laplacian matrix, which is computationally expensive. To re-"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "duce the computational cost, ChebNet approximates the convolution"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "operation (including the learnable convolution kernel)\nin terms of"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "Chebyshev polynomials [16]. The most popular form of GCN uses"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "a ﬁrst order approximation of the Chebyshev polynomial\nto further"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "simplify the convolution operation to a linear projection [9]. Such"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "GCN models are simple to implement, and have been successfully"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "used for various node classiﬁcation tasks in social media networks"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "and citation networks [9]."
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "In this paper, we cast SER as a graph classiﬁcation problem."
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "We model a speech signal as a simple graph, where each node corre-"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "sponds to a short windowed segment of the signal. Each node is con-"
        },
        {
          "University of Warwick, UK": "nected to only two adjacent nodes thus transforming the signal\nto a"
        },
        {
          "University of Warwick, UK": "line graph or a cycle graph. Owing to this particular graph structure,"
        },
        {
          "University of Warwick, UK": "we take advantage of certain results in graph signal processing [17]"
        },
        {
          "University of Warwick, UK": "to perform accurate graph convolution (in contrast\nto the approxi-"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "mations used in popular GCNs). This leads to a light-weight GCN"
        },
        {
          "University of Warwick, UK": "architecture with superior emotion recognition performance on the"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "IEMOCAP [18] and the MSP-IMPROV [19] databases."
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "To summarize,\nthe contributions of our work are as\nfollows:"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "(i) To the best of our knowledge,\nthis is the ﬁrst work that\ntakes a"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "graph classiﬁcation approach to SER. (ii) Leveraging theories from"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "graph signal processing, we propose a GCN-based graph classiﬁ-"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "cation approach that can efﬁciently perform accurate graph convo-"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "lution.\n(iii) Our model has signiﬁcantly fewer trainable parameters"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "(∼30K only). Despite its smaller size, our model achieves superior"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "performance on both IEMOCAP and MSP-IMPROV databases out-"
        },
        {
          "University of Warwick, UK": ""
        },
        {
          "University of Warwick, UK": "performing relevant and competitive baselines."
        },
        {
          "University of Warwick, UK": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": "Our graph construction strategy follows a simple frame-to-node"
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": "(i) a cycle graph deﬁned by the ad-"
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        },
        {
          "Fig. 1. Our proposed graph-based architecture for SER consists of two graph convolution layers and a pooling layer to learn graph embedding": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "ple undirected graph structures:\n(i) a cycle graph deﬁned by the ad-"
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "jacency matrix Ac, and (ii) a line graph deﬁned by adjacency Al."
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "The two graph structures are shown in Fig. 2(b)-(c)."
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "0\n1\n0\n· · ·\n1\n0\n1\n0\n· · ·\n0"
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "1\n0\n1\n· · ·\n0\n1\n0\n1\n· · ·\n0"
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": " \n \n \n \n0\n1\n0\n· · ·\n0\n0\n1\n0\n· · ·\n0"
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "Ac =\nAl ="
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ".\n.\n.\n.\n.\n.\n.\n."
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ".\n.\n.\n.\n.\n.\n.\n.\n. . .\n. . .\n.\n.\n.\n.\n.\n.\n.\n."
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "1\n0\n· · ·\n1\n0\n0\n0\n· · ·\n1\n0"
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "These\ntwo graph structures\nare\nimportant because of\nthe\nspecial"
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": "structures of\ntheir graph Laplacians, which signiﬁcantly simpliﬁes"
        },
        {
          "graph structure is not naturally deﬁned here, we investigate two sim-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "matrix of the graph. The eigen decomposition of L can be written as"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "T"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "M(cid:88) i\nL = UΛUT =\nλiuiui"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "=1"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "is the ith eigen value of L corresponding to the eigen vec-"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "tor ui, Λ = diag(λi) and U = [u1, u2 · · · uN ]. The exact graph"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "H = (UT X)(UT W)"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "H = U ˆH"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "The graph convolution propagation at kth layer thus becomes"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "(cid:16)"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "H(k+1) = U\n(UT H(k))(UT W(k))"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "where H(0) = X and W is learnable. Note that for A = Ac (cycle"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ""
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "2\n−1\n0\n· · ·\n−1"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "−1\n2\n−1\n· · ·\n0"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": " \n \n0\n−1\n2\n· · ·\n0"
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "L ="
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ".\n.\n.\n."
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": ".\n.\n.\n.\n. . .\n.\n.\n.\n."
        },
        {
          "where D is the degree matrix, L = D−A where A is the adjacency": "−1\n0\n· · ·\n−1\n2"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: SER results and comparison on the MSP-IMPROV Table 4. Comparing different pooling strategies for our model on",
      "data": [
        {
          "2.\nSER results\nand": "databases in terms of weighted accuracy (WA) and unweighted ac-",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "Model",
          "comparison\non": "WA (%)",
          "the MSP-IMPROV": "UA (%)",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "Graph baselines",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "GCN [9]",
          "comparison\non": "54.71",
          "the MSP-IMPROV": "51.42",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "PATCHY-SAN [21]",
          "comparison\non": "55.47",
          "the MSP-IMPROV": "52.33",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "In addition to the graph"
        },
        {
          "2.\nSER results\nand": "PATCHY-Diff [22]",
          "comparison\non": "56.18",
          "the MSP-IMPROV": "53.12",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "recent SER"
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "SER models",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "models. These include a Bayesian model [23], CNN models ([25],"
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "SegCNN [7]), Recurrent Neural Network (RNN) architectures ([4],"
        },
        {
          "2.\nSER results\nand": "ProgNet 2017 [19]",
          "comparison\non": "58.40",
          "the MSP-IMPROV": "-",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "RCNN [24]), LSTM models ([25], Attn-BLSTM [3]) and a CNN-"
        },
        {
          "2.\nSER results\nand": "CNN 2019 [25]",
          "comparison\non": "50.84",
          "the MSP-IMPROV": "-",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "the"
        },
        {
          "2.\nSER results\nand": "LSTM 2019 [25]",
          "comparison\non": "51.21",
          "the MSP-IMPROV": "-",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "Tables 1 and 2 show that our model outperforms (i) all graph"
        },
        {
          "2.\nSER results\nand": "CNN-LSTM 2019 [25]",
          "comparison\non": "52.36",
          "the MSP-IMPROV": "-",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "baselines despite a simpler architecture and (ii) all LSTM-based ar-"
        },
        {
          "2.\nSER results\nand": "Ours (cycle)",
          "comparison\non": "57.82",
          "the MSP-IMPROV": "55.42",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "chitectures (a class of models most commonly used in SER) on both"
        },
        {
          "2.\nSER results\nand": "Ours (line)",
          "comparison\non": "57.08",
          "the MSP-IMPROV": "54.75",
          "Table 4. Comparing different pooling strategies for our model on": "databases by a signiﬁcant margin. Our model (with the cycle graph"
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "structure) achieves highest WA on IEMOCAP and comparable to the"
        },
        {
          "2.\nSER results\nand": "Ours (cycle w/o MLP)",
          "comparison\non": "56.82",
          "the MSP-IMPROV": "53.22",
          "Table 4. Comparing different pooling strategies for our model on": ""
        },
        {
          "2.\nSER results\nand": "",
          "comparison\non": "",
          "the MSP-IMPROV": "",
          "Table 4. Comparing different pooling strategies for our model on": "state-of-the-art WA on MSP-IMPROV. In terms of UA, our model’s"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "dergheynst,\n“Graph signal processing: Overview, challenges,"
        },
        {
          "5. REFERENCES": "[1] D Tang,\nJ Zeng,\nand M Li,\n“An end-to-end deep learning",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "Proceedings of\nand applications,”\nthe IEEE, vol. 106, no. 5,"
        },
        {
          "5. REFERENCES": "framework for speech emotion recognition of atypical individ-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "pp. 808–828, 2018."
        },
        {
          "5. REFERENCES": "uals.,” in INTERSPEECH, 2018, pp. 162–166.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[18] C Busso, M Bulut, C-C Lee, A Kazemzadeh, E Mower, S Kim,"
        },
        {
          "5. REFERENCES": "[2] Z\nZhao,\nY Zheng,\nZ\nZhang,\nH Wang,\nY Zhao,\nand",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "J N Chang, S Lee, and S S Narayanan,\n“Iemocap:\nInterac-"
        },
        {
          "5. REFERENCES": "C Li,\n“Exploring spatio-temporal representations by integrat-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "tive emotional dyadic motion capture database,” Language re-"
        },
        {
          "5. REFERENCES": "ing attention-based bidirectional-lstm-rnns and fcns for speech",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "sources and evaluation, vol. 42, no. 4, pp. 335, 2008."
        },
        {
          "5. REFERENCES": "emotion recognition,” in INTERSPEECH, 2018, pp. 272–276.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[19]\nJ Gideon, S Khorram, Z Aldeneh, D Dimitriadis,\nand E M"
        },
        {
          "5. REFERENCES": "[3] C-W Huang and S S Narayanan, “Attention assisted discovery",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "Provost,\n“Progressive neural networks\nfor\ntransfer\nlearning"
        },
        {
          "5. REFERENCES": "of sub-utterance structure in speech emotion recognition,”\nin",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "in emotion recognition,”\nin INTERSPEECH, 2017, pp. 1098–"
        },
        {
          "5. REFERENCES": "INTERSPEECH, 2016, pp. 1387–1391.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "1102."
        },
        {
          "5. REFERENCES": "[4]\nS Mirsamadi, E Barsoum, and C Zhang,\n“Automatic speech",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[20] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka,"
        },
        {
          "5. REFERENCES": "emotion recognition using recurrent neural networks with local",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "“How powerful are graph neural networks?,”\nin International"
        },
        {
          "5. REFERENCES": "attention,” in ICASSP, 2017, pp. 2227–2231.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "Conference on Learning Representations (ICLR), 2019."
        },
        {
          "5. REFERENCES": "[5] Z Aldeneh, S Khorram, D Dimitriadis,\nand E M Provost,",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[21] M Niepert, M Ahmed, and K Kutzkov,\n“Learning convolu-"
        },
        {
          "5. REFERENCES": "“Pooling acoustic and lexical features for the prediction of va-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "tional neural networks for graphs,” in International Conference"
        },
        {
          "5. REFERENCES": "lence,”\nin Proc. International Conference on Multimodal In-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "on Machine Learning (ICML), 2016, pp. 2014–2023."
        },
        {
          "5. REFERENCES": "teraction, 2017, pp. 68–72.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "[6] Q Jin, C Li, S Chen, and H Wu, “Speech emotion recognition",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[22] Z Ying, J You, C Morris, X Ren, W Hamilton, and J Leskovec,"
        },
        {
          "5. REFERENCES": "with acoustic and lexical\nfeatures,”\nin ICASSP.\nIEEE, 2015,",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "“Hierarchical graph representation learning with differentiable"
        },
        {
          "5. REFERENCES": "pp. 4749–4753.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "pooling,”\nin Advances in Neural Information Processing Sys-"
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "tems, 2018, pp. 4800–4810."
        },
        {
          "5. REFERENCES": "[7]\nS Mao, PC Ching, and T Lee,\n“Deep learning of segment-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "level feature representation with multiple instance learning for",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[23] X Ma, Z Wu, J Jia, M Xu, H Meng, and L Cai, “Speech emo-"
        },
        {
          "5. REFERENCES": "utterance-level speech emotion recognition,” INTERSPEECH,",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "tion recognition with emotion-pair based framework consider-"
        },
        {
          "5. REFERENCES": "pp. 1686–1690, 2019.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "ing emotion distribution information in dimensional emotion"
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "space.,” in INTERSPEECH, 2017, pp. 1238–1242."
        },
        {
          "5. REFERENCES": "[8] Wenjing Han, Huabin Ruan, Xiaomin Chen, Zhixiang Wang,",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "Haifeng Li, and Bj¨orn W Schuller,\n“Towards temporal mod-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[24] D Luo, Y Zou, and D Huang, “Investigation on joint represen-"
        },
        {
          "5. REFERENCES": "elling of categorical speech emotion recognition.,”\nin Inter-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "tation learning for robust feature extraction in speech emotion"
        },
        {
          "5. REFERENCES": "speech, 2018, pp. 932–936.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "recognition.,” in INTERSPEECH, 2018, pp. 152–156."
        },
        {
          "5. REFERENCES": "[9] T N. Kipf and M Welling, “Semi-supervised classiﬁcation with",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[25]\nS Latif, R Rana, S Khalifa, R Jurdak, and J Epps,\n“Direct"
        },
        {
          "5. REFERENCES": "graph convolutional networks,” in International Conference on",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "modelling of\nspeech emotion from raw speech,”\npp. 3920–"
        },
        {
          "5. REFERENCES": "Learning Representations (ICLR), 2017.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "3924, 2019."
        },
        {
          "5. REFERENCES": "[10]\nS Yan, Y Xiong, and D Lin,\n“Spatial\ntemporal graph convo-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[26] C Busso,\nS\nParthasarathy, A Burmania, M AbdelWahab,"
        },
        {
          "5. REFERENCES": "lutional networks for skeleton-based action recognition,”\nin",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "N Sadoughi, and EM Provost,\n“Msp-improv: An acted cor-"
        },
        {
          "5. REFERENCES": "AAAI, 2018.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "pus of dyadic interactions to study emotion perception,” IEEE"
        },
        {
          "5. REFERENCES": "[11]\nJ Gao, T Zhang, and C Xu, “Graph convolutional tracking,” in",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "Transactions on Affective Computing, vol. 8, no. 1, pp. 67–80,"
        },
        {
          "5. REFERENCES": "CVPR, 2019, pp. 4649–4659.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "2016."
        },
        {
          "5. REFERENCES": "[12] L Yao, C Mao, and Y Luo, “Graph convolutional networks for",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[27] B Schuller, S Steidl, and A Batliner,\n“The interspeech 2009"
        },
        {
          "5. REFERENCES": "text classiﬁcation,” in AAAI, 2019, vol. 33, pp. 7370–7377.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "emotion challenge,” in INTERSPEECH, 2009."
        },
        {
          "5. REFERENCES": "[13]\nS Zhang, Y Qin, K Sun, and Y Lin, “Few-shot audio classiﬁca-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[28] F Eyben, F Weninger, F Gross, and B Schuller,\n“Recent de-"
        },
        {
          "5. REFERENCES": "tion with attentional graph neural networks,” INTERSPEECH,",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "velopments in opensmile, the munich open-source multimedia"
        },
        {
          "5. REFERENCES": "pp. 3649–3653, 2019.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "feature extractor,”\nin Proc. ACM international conference on"
        },
        {
          "5. REFERENCES": "[14] D I Shuman, S K Narang, P Frossard, A Ortega, and P Van-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "Multimedia, 2013, pp. 835–838."
        },
        {
          "5. REFERENCES": "dergheynst,\n“The\nemerging ﬁeld\nof\nsignal\nprocessing\non",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[29] K Mangalam and T Guha,\n“Learning spontaneity to improve"
        },
        {
          "5. REFERENCES": "graphs: Extending high-dimensional data analysis to networks",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "emotion recognition in speech,” in INTERSPEECH, 2018, pp."
        },
        {
          "5. REFERENCES": "IEEE Signal Processing Maga-\nand other irregular domains,”",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "946–950."
        },
        {
          "5. REFERENCES": "zine, vol. 30, no. 3, pp. 83–98, May 2013.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "[30] X Glorot and Y Bengio, “Understanding the difﬁculty of train-"
        },
        {
          "5. REFERENCES": "[15]\nJ Bruna, W Zaremba, A Szlam, and Y LeCun,\n“Spectral net-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "ing deep feedforward neural networks,”\nin International Con-"
        },
        {
          "5. REFERENCES": "works and locally connected networks on graphs,” in Interna-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "ference on Artiﬁcial Intelligence and Statistics, 2010, pp. 249–"
        },
        {
          "5. REFERENCES": "tional Conference on Learning Representations (ICLR), 2013.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": "256."
        },
        {
          "5. REFERENCES": "[16] M Defferrard, X Bresson, and P Vandergheynst,\n“Convolu-",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "tional neural networks on graphs with fast\nlocalized spectral",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "information processing sys-\nﬁltering,”\nin Advances in neural",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        },
        {
          "5. REFERENCES": "tems, 2016, pp. 3844–3852.",
          "[17] A Ortega, P Frossard, J Kovaˇcevi´c, J MF Moura, and P Van-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "An end-to-end deep learning framework for speech emotion recognition of atypical individuals",
      "authors": [
        "Tang",
        "M Zeng",
        "Li"
      ],
      "year": "2018",
      "venue": "An end-to-end deep learning framework for speech emotion recognition of atypical individuals"
    },
    {
      "citation_id": "3",
      "title": "Exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition",
      "authors": [
        "Y Zhao",
        "Z Zheng",
        "H Zhang",
        "Y Wang",
        "C Zhao",
        "Li"
      ],
      "year": "2018",
      "venue": "Exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition"
    },
    {
      "citation_id": "4",
      "title": "Attention assisted discovery of sub-utterance structure in speech emotion recognition",
      "authors": [
        "C-W Huang",
        "S S Narayanan"
      ],
      "year": "2016",
      "venue": "Attention assisted discovery of sub-utterance structure in speech emotion recognition"
    },
    {
      "citation_id": "5",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Mirsamadi",
        "C Barsoum",
        "Zhang"
      ],
      "year": "2017",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Pooling acoustic and lexical features for the prediction of valence",
      "authors": [
        "Aldeneh",
        "D Khorram",
        "E M Dimitriadis",
        "Provost"
      ],
      "year": "2017",
      "venue": "Proc. International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Jin",
        "S Li",
        "H Chen",
        "Wu"
      ],
      "year": "2015",
      "venue": "ICASSP"
    },
    {
      "citation_id": "8",
      "title": "Deep learning of segmentlevel feature representation with multiple instance learning for utterance-level speech emotion recognition",
      "authors": [
        "Mao",
        "Ching",
        "Lee"
      ],
      "year": "2019",
      "venue": "Deep learning of segmentlevel feature representation with multiple instance learning for utterance-level speech emotion recognition"
    },
    {
      "citation_id": "9",
      "title": "Towards temporal modelling of categorical speech emotion recognition",
      "authors": [
        "Wenjing Han",
        "Huabin Ruan",
        "Xiaomin Chen",
        "Zhixiang Wang",
        "Haifeng Li",
        "Björn Schuller"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "10",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "Welling"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "11",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "Y Yan",
        "Xiong",
        "Lin"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "12",
      "title": "Graph convolutional tracking",
      "authors": [
        "Gao",
        "C Zhang",
        "Xu"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Graph convolutional networks for text classification",
      "authors": [
        "C Yao",
        "Y Mao",
        "Luo"
      ],
      "year": "2019",
      "venue": "AAAI"
    },
    {
      "citation_id": "14",
      "title": "Few-shot audio classification with attentional graph neural networks",
      "authors": [
        "Y Zhang",
        "K Qin",
        "Y Sun",
        "Lin"
      ],
      "year": "2019",
      "venue": "Few-shot audio classification with attentional graph neural networks"
    },
    {
      "citation_id": "15",
      "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
      "authors": [
        "D I Shuman",
        "S K Narang",
        "P Frossard",
        "Ortega",
        "Vandergheynst"
      ],
      "year": "2013",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "16",
      "title": "Spectral networks and locally connected networks on graphs",
      "authors": [
        "Bruna",
        "Zaremba",
        "Y Szlam",
        "Lecun"
      ],
      "year": "2013",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "17",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "Defferrard",
        "Bresson",
        "Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Graph signal processing: Overview, challenges, and applications",
      "authors": [
        "P Ortega",
        "J Frossard",
        "J Kovačević",
        "Mf Moura",
        "Vandergheynst"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "M Busso",
        "C-C Bulut",
        "Lee",
        "E Kazemzadeh",
        "S Mower",
        "J N Kim",
        "S Chang",
        "S S Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "Gideon",
        "Z Khorram",
        "D Aldeneh",
        "E M Dimitriadis",
        "Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "How powerful are graph neural networks?",
      "authors": [
        "Keyulu Xu",
        "Weihua Hu",
        "Jure Leskovec",
        "Stefanie Jegelka"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "22",
      "title": "Learning convolutional neural networks for graphs",
      "authors": [
        "M Niepert",
        "Ahmed",
        "Kutzkov"
      ],
      "year": "2016",
      "venue": "International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "23",
      "title": "Hierarchical graph representation learning with differentiable pooling",
      "authors": [
        "J Ying",
        "C You",
        "X Morris",
        "W Ren",
        "Hamilton",
        "Leskovec"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Speech emotion recognition with emotion-pair based framework considering emotion distribution information in dimensional emotion space",
      "authors": [
        "Z Ma",
        "J Wu",
        "M Jia",
        "H Xu",
        "L Meng",
        "Cai"
      ],
      "year": "2017",
      "venue": "Speech emotion recognition with emotion-pair based framework considering emotion distribution information in dimensional emotion space"
    },
    {
      "citation_id": "25",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "Luo",
        "Zou",
        "Huang"
      ],
      "year": "2018",
      "venue": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition"
    },
    {
      "citation_id": "26",
      "title": "Direct modelling of speech emotion from raw speech",
      "authors": [
        "R Latif",
        "S Rana",
        "R Khalifa",
        "Jurdak",
        "Epps"
      ],
      "year": "2019",
      "venue": "Direct modelling of speech emotion from raw speech"
    },
    {
      "citation_id": "27",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Busso",
        "Parthasarathy",
        "M Burmania",
        "N Abdelwahab",
        "E Sadoughi",
        "Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "The interspeech 2009 emotion challenge",
      "authors": [
        "Schuller",
        "Steidl",
        "Batliner"
      ],
      "year": "2009",
      "venue": "The interspeech 2009 emotion challenge"
    },
    {
      "citation_id": "29",
      "title": "Recent developments in opensmile, the munich open-source multimedia feature extractor",
      "authors": [
        "Eyben",
        "F Weninger",
        "Gross",
        "Schuller"
      ],
      "year": "2013",
      "venue": "Proc. ACM international conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Learning spontaneity to improve emotion recognition in speech",
      "authors": [
        "K Mangalam",
        "T Guha"
      ],
      "year": "2018",
      "venue": "Learning spontaneity to improve emotion recognition in speech"
    },
    {
      "citation_id": "31",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "International Conference on Artificial Intelligence and Statistics"
    }
  ]
}