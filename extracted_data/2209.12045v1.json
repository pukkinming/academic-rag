{
  "paper_id": "2209.12045v1",
  "title": "Song Emotion Recognition: A Performance Comparison Between Audio Features And Artificial Neural Networks",
  "published": "2022-09-24T16:13:25Z",
  "authors": [
    "Karen Rosero",
    "Arthur Nicholas dos Santos",
    "Pedro Benevenuto Valadares",
    "Bruno Sanches Masiero"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "When songs are composed or performed, there is often an intent by the singer/songwriter of expressing feelings or emotions through it. For humans, matching the emotiveness in a musical composition or performance with the subjective perception of an audience can be quite challenging. Fortunately, the machine learning approach for this problem is simpler. Usually, it takes a data-set, from which audio features are extracted to present this information to a data-driven model, that will, in turn, train to predict what is the probability that a given song matches a target emotion. In this paper 1 , we studied the most common features and models used in recent publications to tackle this problem, revealing which ones are best suited for recognizing emotion in a cappella songs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Music is art, which is a form of expression. By the time it reaches an audience, a spectrum of emotional reactions can be provoked, in account that Music Emotion Recognition (MER) is a process that is highly intertwined with people's life experiences and cognitive capacities. In contrast, as a sub-field of Music Information Retrieval (MIR), MER deals with classification of music according to affective computing  [1] .\n\nCurrently, the importance of MER can be justified by the dependency of search and recommendation engines on metadata which, in simple terms, is just data about data. For example, when a person uses a smartphone to take a picture of a cat, the data is the picture itself. However, a series of other information about that data is also recorded, e.g., the time, date and geographical coordinates where it was shot etc. All of which are metadata that can be used to tag the data itself, to either retrieve it in the future or find similar content. Likewise, song metadata, e.g., (genre, composer, artist, album, year of release etc.) are commonly used by streaming services to help users to find what they may be prone to like, or even recommend songs based on their listening history. However, the mood of a song is also an interesting metadata, that could be used to relate a certain song to similar content.\n\nIn this paper, we studied articles with state-of-the-art results published on MER, for both song and instrumental music. Our findings reveal that, usually, timbral features, e.g., Mel spectrogram, Mel-Frequency Cepstral Coefficients (MFCC) etc., are employed as front-ends, whilst regarding Artificial Neural Network (ANN) models, the Multi-Layer Perceptron (MLP) and Convolutional Neural Network (CNN) architectures are most commonly employed as back-end. However, when comparing the performance of different audio features and ANN models, our experiments showed that the chromagram, which is a harmonic feature, combined with either one-dimensional (1-D) or two-dimensional (2-D) CNN architectures yields even better results.\n\nThe remainder of this paper is organized as follows: Section 2 details our findings on what are the most commonly used audio features to represent music samples to the most commonly used machine learning (ML) model architectures. Section 3 describes our experiment, being synthesized different ANN models based on the information retrieved from the previous section. Section 4 shows our results, comparing it to previous state-of-the-art works that used a same data-set as we did. Finally, Section 5 presents some pertinent considerations to conclude this study.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Features And Models",
      "text": "According to  [2] , musical dimensions can be related to emotions by a set of high-level features, namely: melody, harmony, rhythm, dynamics, tone color (timbre), expressivity, texture, form and vocals. On the other hand, computational features are considered low-level, because they only provide primitive descriptions by which individual high-level ones may be identified.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Features",
      "text": "By reviewing the works of  [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] , we found 47 different low-level computational features being used separately or concatenated to better represent training data-sets, depending on different ML architectures used. All these features are available off-the-shelf on Python libraries and MATLAB toolboxes, and 6 of them were found to be used on 76.6% of the publications reviewed, which are:\n\n• Chromagram: relates to harmony, i.e., the sound produced by the combination of various pitches and indicates energy distribution along a 12-dimensional vector (one for each semitone in the super-just harmonic scale, i.e., from A to G#.); • Mel spectrogram: relates to tone color, i.e., timbre, and decomposes an audio signal into a series of frequency channels inspired by the human cochlea, enabling to study the signal's frequency distribution into so-called critical bands; • Mel-Frequency Cepstral Coefficients (MFCC): also relates to tone color and measures spectral shape. Can be derived from a log magnitude Mel spectrogram based on the Discrete Cosine Transform (DCT). Typically, only the first 8 to 13 MFCCs are used for voiced signals; • Spectral centroid: also relates to tone color and represents the mean of the magnitude spectrum of the Short-Time Fourier Transform (STFT); • Spectral roll-off : also relates to tone color and indicates the frequency below which approximately 85% of the magnitude spectrum distribution is concentrated; • Zero-Crossing Rate (ZCR): also relates to tone color and represents the number of times a waveform changes sign in a window, indicating change of frequency and noisiness.\n\nAs for the other 41 audio features (which were used on the other 23.4% of the publications reviewed), 13 of them were related to rhythm, 10 were kindred to tone color, 6 were cognated to harmony, 5 were related to melody, 5 were related to dynamics, only 1 was connected to texture, 1 to musical form and 1 to vocals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ml Models",
      "text": "According to  [3] , what dictates which and how many features can be used as front-end for an ML model is the architecture of the model itself. By reviewing the works of  [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] , we found 12 different architectures being used, separately or combined. 3 of these were found to be used on 17% of the publications reviewed, namely: Support Vector Machine (SVM), Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN); and another 2 on 10% of the publications reviewed, being: Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) blocks, and random forest. Since the initial purpose of our work was to compare the performance of ANN models, we opted to leave out SVM and random forest, which are ML models that are not based on ANNs, thus focusing on the remaining 3 models:\n\n• MLP: is a type of ANN that models the relationship between a set of training data and a group of known targets. Its architecture is based on a simplified understanding of how the human brain responds to stimuli from sensory organs and is best suited to problems where the relationship between input and output data is well understood, yet the process that relates both is extremely complex;\n\n• CNN: is a type of ANN based on convolutional operations that can extract high-level features from 1-D or 2-D low-level features. It can deeply extract underlying features contained in each frame, while retaining time-series features in the same direction. In classification problems, an MLP layer is usually employed at the end of a CNN architecture, to output its predictions;\n\n• RNN-LSTM: is a type of ANN which commonly relies on sequential data, i.e., time-series vectors. In the special case of an LSTM block, it can learn dependencies in the time-dimension of Time-Frequency (T-F) features, incorporating local context in ANN predictions. Hence, when trained on top of CNNs, for instance, the input data is no longer an individual \"image\", e.g., a spectrogram, but rather a sequence, more like in a \"movie\". Moreover, when using a bidirectional LSTM (Bi-LSTM) block, it can also handle the forward and backward flow of information in ANNs.\n\nAs for the other 7 architectures, K-Nearest Neighbors (K-NN) was found to be used on 7% of the publications reviewed, RNN with Gated Recurrent Unit (GRU) blocks, decision tree (CART and C4.5) and State Vector Regressor (SRV) were found to be used on 4% of the publications reviewed (each) and logistic regression was set to be used on 3% of the publications reviewed.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "To experiment with the aforementioned features and models, a portion of the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) data-set was chosen, comprising 1,012 audio-only files of song recordings, performed by 24 actors, singing 2 lexically matched statements in a neutral North American accent. Song emotions include neutral, calm, happy, sad, angry, and fearful expressions  [13] . To extract audio features from this data-set, the Python package for music and audio analysis Librosa (version 0.8.0) was used, and all ANN models were synthesized using TensorFlow (version 2.2.0), compiled using ADAM optimization, categorical cross-entropy as loss function, and trained using a NVIDIA TITAN V Graphics Processing Unit (GPU).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mlp Model",
      "text": "Since an MLP model can have an input layer with as many neurons as necessary, all input features could be concatenated and flattened into a 1-D input vector. After extracting all 6 audio features from the data-set, the Principal Component Analysis (PCA) technique was used to visualize the minimum number of variables that keeps the maximum amount of information about how each feature data is distributed, as illustrated in Figure  1 .\n\nFigure  1 : PCA plots for all 6 audio features analyzed in this paper.\n\nSince Mel spectrogram showed the worst clustering and the highest dimensionality compared to the other features, we chose to train an MLP model using the concatenation of the other 5 features, i.e., chromagram, MFCC, spectral centroid, spectral roll-off and ZCR. Hence, our MLP model consisted of an input layer with 11,394 neurons, followed by 2 hidden layers with 1,024 and 128 neurons each, totaling over 141M free parameters. Rectified Linear Unit (ReLU) was used as an activation function for all layers, except for the output one, where softmax was used. For regularization, we used dropout, which randomly ignores a percentage of neurons during training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "2-D Cnn Model",
      "text": "For our 2-D CNN model, we selected the 2-D feature that showed the clearest clustering in the PCA visualization in Figure  1 , i.e., the chromagram. Thus, the input for this architecture had a shape of (12, 422, 1), followed by three convolutional blocks, with 24, 48, and 48 filters respectively, kernel sizes of (5, 5), (2, 2), (3, 3), convolutional layers' stride sizes of (1, 1), and max pooling layers in the first two blocks, with (2, 4), (1, 3) pooling sizes and stride sizes of the same dimensions respectively. After the last convolutional block, the data was flattened into a 1-D vector with 1,536 elements, to be fed into 2 Fully Connected (FC) layers with 64 and 6 neurons respectively, totaling almost 125k free parameters. For regularization, a dropout of 0.5 was used before each FC layer. ReLU was used as the activation function for all layers, except for the output one, where softmax was used, to obtain the probability associated with each emotion.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "1-D Cnn Model",
      "text": "To assess the performance of 1-D CNN architectures, we reshaped the chromagram feature to match a model with an input length of 5064 samples. Following this, 3 convolutional blocks were added, with 16, 32 and 32 filters respectively and kernel size of 4. The max pooling operation was implemented for all blocks with a pooling size of 3 and a stride of 1. The data outputted by the last convolutional block was flattened before being passed on to 3 FC layers with 1024, 128 and 6 neurons, respectively, totaling over 6M free parameters. Regularization and activation functions were maintained as in the 2-D CNN model.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Convolutional Recurrent Neural Network (Crnn) Model",
      "text": "Finally, we also experimented with the addition of a Bi-LSTM block after the convolutional layers of the model described in Section 3.3, aiming to learn with the long short-term temporal dependencies of audio signals. The convolutional blocks have the same parameters as described before, except for the number of filters, which are 16 in every block. The bidirectional wrapper takes an LSTM layer as argument with 100 memory units, using ReLU as activation function and a dropout of 0.5 as regularization. Ultimately, the Bi-LSTM layer output is flattened and passed on to 3 FC layers with the same characteristics detailed in Section 3.3, totaling over 19M free parameters.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Augmentation",
      "text": "In this work, we also explored suitable Data Augmentation (DA) techniques, i.e., creating slightly modified new data derived from the original one. Since ANNs consider these new data as genuine, they can benefit from it, learning new parameters to achieve even better performances without over-fitting. For this reason, we used the Audiomentations library to add Gaussian noise to the original audio samples, also shift the song's pitches by 1 or 2 octaves. Models trained without DA were trained for 100 epochs, while models trained with DA were trained for 200 epochs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "To train our models, the data-set was split into 612 samples for training, 200 for validation and 200 for test. In a previous work (  [14] ), it was already evinced that 2-D CNN models can surpass the overall test accuracy of MLP models. Therefore, our MLP model was trained only without data augmentation, first using the concatenation of 5 features described in Section 3.1 and then in a feature ablation manner, as detailed in Table  1 . Next, our 2-D CNN model was trained with and without DA to compare its performance under these different conditions. Since the use of DA improved the system's metrics as evinced in Table  1 , we continued using DA for the training of our 1-D CNN and CRNN architectures, always saving the epoch that achieved the best validation accuracy. The results show that the 1-D and 2-D CNN models achieved the best performances compared to all other models, while taking less time to train, compared with our CRNN model.\n\nMoreover, we also resorted to the works of  [15] [16] [17]  to compare our results with other works that used the same data-set, but not necessarily the same features and model architectures.\n\nAccording to  [18] , harmony can be effectively used in songwriting to encode hidden meanings, e.g., to imprint emotiveness regardless of rhythm, lyrics etc., which is corroborated by our findings in terms of accuracy. However, according to  [16, 17] , we were expecting that adding a Bi-LSTM cell would improve the accuracy of our 1-D CNN model, since a same musical scale sang in different orders can imprint different moods. In spite of that, our CRNN model was significantly outperformed by our 1-D and 2-D CNN models, with DA.  Using 5 features concatenated as front-end, the MLP model achieves its best accuracy, which can be justified by its late divergence. Regarding the 2-D CNN models, the ones with chromagram and Mel spectrogram as front-end start to diverge early in their training, yet stabilizing at high values of validation accuracy earlier as well. With DA, the 2-D CNN model stabilizes even earlier than others, which justifies its good performance. Finally, regarding the 1-D CNN models, it's noticeable that, without the Bi-LSTM layer, the model stabilizes at higher values of validation accuracy, without ever fully over-fitting, contrary to its CRNN counterpart.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "Although the most popular computational features for MER used in recent publications relate to tone color, in our experiments the chromagram, which relates to harmony, was found to be best suited for song emotion recognition. Also, our 1-D and 2-D CNN models performed better than both our MLP and CRNN models, despite earlier works showing that RNN layers should improve the accuracy of 1-D CNN models. Moreover, our best result (0.84 ± 0.02 test accuracy) was obtained using data augmentation, evincing that ANNs do benefit from this technique. Ultimately, regarding song emotion recognition, our results are state-of-the-art, compared with recent publications, but it's important to mention that the data-set used in our experiments is far from being a general representation of the human population. Since it's samples lack accent, language, ethnical and gender diversities, disability inclusion etc., when put to test with samples different from its own corpus, it will hardly produce exciting results, such as in Table  1 .",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Figure 1: PCA plots for all 6 audio features analyzed in this paper.",
      "page": 3
    },
    {
      "caption": "Figure 1: , i.e., the chromagram. Thus, the input for this architecture had a shape of (12, 422, 1), followed by three",
      "page": 4
    },
    {
      "caption": "Figure 2: Learning curves for all 12 combinations of features and models tested. Shadows enveloping the curves",
      "page": 5
    },
    {
      "caption": "Figure 2: , it’s observable that,",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "♥\nMLP",
          "Features": "Chromagram",
          "Val. acc.": "0.78±0.07",
          "Val.\nloss": "1.5±0.01",
          "Test acc.": "0.77±0.03",
          "Test loss": "1.54±0.08",
          "Tr. time": "18.24s"
        },
        {
          "Model": "",
          "Features": "MFCC",
          "Val. acc.": "0.70±0.03",
          "Val.\nloss": "3.71±0.11",
          "Test acc.": "0.70±0.01",
          "Test loss": "3.04±0.97",
          "Tr. time": "28.06s"
        },
        {
          "Model": "",
          "Features": "Spec. centroid",
          "Val. acc.": "0.44±0.02",
          "Val.\nloss": "3.15±1.11",
          "Test acc.": "0.44±0.02",
          "Test loss": "3.17±1.27",
          "Tr. time": "3.13s"
        },
        {
          "Model": "",
          "Features": "Spec. roll-off",
          "Val. acc.": "0.45±0.01",
          "Val.\nloss": "2.91±1.01",
          "Test acc.": "0.45±0.02",
          "Test loss": "2.91±0.95",
          "Tr. time": "3.03s"
        },
        {
          "Model": "",
          "Features": "ZCR",
          "Val. acc.": "0.43±0.03",
          "Val.\nloss": "2.99±0.25",
          "Test acc.": "0.38±0.07",
          "Test loss": "3.03±0.48",
          "Tr. time": "3.18s"
        },
        {
          "Model": "",
          "Features": "5 feats. concat.",
          "Val. acc.": "0.65±0.06",
          "Val.\nloss": "2.5±0.76",
          "Test acc.": "0.65±0.09",
          "Test loss": "2.66±1.74",
          "Tr. time": "5.14min"
        },
        {
          "Model": "♥\n2-D CNN",
          "Features": "Mel spec.",
          "Val. acc.": "0.78±0.01",
          "Val.\nloss": "0.87±0.17",
          "Test acc.": "0.78±0.07",
          "Test loss": "0.79±0.11",
          "Tr. time": "16.74s"
        },
        {
          "Model": "",
          "Features": "MFCC",
          "Val. acc.": "0.70±0.01",
          "Val.\nloss": "0.81±0.04",
          "Test acc.": "0.63±0.02",
          "Test loss": "0.86±0.01",
          "Tr. time": "7.74s"
        },
        {
          "Model": "",
          "Features": "Chromagram",
          "Val. acc.": "0.84±0.01",
          "Val.\nloss": "0.49±0.02",
          "Test acc.": "0.80±0.01",
          "Test loss": "0.62±0.02",
          "Tr. time": "8.42s"
        },
        {
          "Model": "",
          "Features": "Chromagram /\nDA",
          "Val. acc.": "0.80±0.01",
          "Val.\nloss": "0.62±0.05",
          "Test acc.": "0.84±0.02",
          "Test loss": "0.57±0.08",
          "Tr. time": "17.66s"
        },
        {
          "Model": "♥\n1-D CNN",
          "Features": "Chromagram /\nDA",
          "Val. acc.": "0.87±0.02",
          "Val.\nloss": "0.42±0.07",
          "Test acc.": "0.83±0.01",
          "Test loss": "0.42±0.05",
          "Tr. time": "44.82s"
        },
        {
          "Model": "1-D CNN\n♥\n+ BiLSTM",
          "Features": "Chromagram /\nDA",
          "Val. acc.": "0.81±0.03",
          "Val.\nloss": "0.81±0.45",
          "Test acc.": "0.75±0.02",
          "Test loss": "0.89±0.44",
          "Tr. time": "11.21min"
        },
        {
          "Model": "Logistic\n♦\nRegression",
          "Features": "MATLAB Audio\nAnalysis Library",
          "Val. acc.": "-",
          "Val.\nloss": "-",
          "Test acc.": "0.48",
          "Test loss": "-",
          "Tr. time": "-"
        },
        {
          "Model": "♠\nRNN (LSTM)",
          "Features": "Librosa HSF",
          "Val. acc.": "-",
          "Val.\nloss": "-",
          "Test acc.": "0.82",
          "Test loss": "-",
          "Tr. time": "-"
        },
        {
          "Model": "1-D CNN\n♣\n+ BiLSTM",
          "Features": "MFCC /\nDA",
          "Val. acc.": "-",
          "Val.\nloss": "-",
          "Test acc.": "0.73",
          "Test loss": "-",
          "Tr. time": "-"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Music emotion recognition: A state of the art review",
      "authors": [
        "Erik Youngmoo E Kim",
        "Raymond Schmidt",
        "Migneco",
        "Patrick Brandon G Morton",
        "Jeffrey Richardson",
        "Jacquelin Scott",
        "Douglas Speck",
        "Turnbull"
      ],
      "year": "2010",
      "venue": "Proc. ismir"
    },
    {
      "citation_id": "2",
      "title": "Audio features for music emotion recognition: a survey",
      "authors": [
        "Renato Panda",
        "Ricardo Manuel Malheiro",
        "Rui Pedro"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.3032373"
    },
    {
      "citation_id": "3",
      "title": "A comparison of classifiers for musical genres classification and music emotion recognition",
      "authors": [
        "Beatriz Flamia",
        "Glaucia Bressan"
      ],
      "year": "2018",
      "venue": "Advances in Mathematical Sciences and Applications"
    },
    {
      "citation_id": "4",
      "title": "A novel music emotion recognition model for scratch-generated music",
      "authors": [
        "Zijing Gao",
        "Lichen Qiu",
        "Peng Qi",
        "Yan Sun"
      ],
      "year": "2020",
      "venue": "2020 International Wireless Communications and Mobile Computing (IWCMC)",
      "doi": "10.1109/IWCMC48107.2020.9148471"
    },
    {
      "citation_id": "5",
      "title": "Creating a speech and music emotion recognition system for mixed source audio",
      "authors": [
        "Laugs Casper"
      ],
      "year": "2020",
      "venue": "Creating a speech and music emotion recognition system for mixed source audio"
    },
    {
      "citation_id": "6",
      "title": "Cochleogram-based approach for detecting perceived emotions in music",
      "authors": [
        "Mladen Russo",
        "Luka Kraljević",
        "Maja Stella",
        "Marjan Sikora"
      ],
      "year": "2020",
      "venue": "Information Processing & Management",
      "doi": "10.1016/j.ipm.2020.102270"
    },
    {
      "citation_id": "7",
      "title": "Express musical emotion based on neural network. Master's thesis",
      "authors": [
        "Wooyeon Kim",
        "Musemo"
      ],
      "year": "2020",
      "venue": "Express musical emotion based on neural network. Master's thesis"
    },
    {
      "citation_id": "8",
      "title": "Cross-Dataset Music Emotion Recognition: an End-to-End Approach",
      "authors": [
        "Ana Gabriela Pandrea",
        "Juan Sebastián Gómez-Cañón",
        "Perfecto Herrera"
      ],
      "year": "2020",
      "venue": "Cross-Dataset Music Emotion Recognition: an End-to-End Approach",
      "doi": "10.5281/zenodo.4076772"
    },
    {
      "citation_id": "9",
      "title": "Dynamic music emotion recognition based on cnn-bilstm",
      "authors": [
        "Pengfei Du",
        "Xiaoyong Li",
        "Yali Gao"
      ],
      "year": "2020",
      "venue": "2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC)",
      "doi": "10.1109/ITOEC49072.2020.9141729"
    },
    {
      "citation_id": "10",
      "title": "Emotional classification of music using neural networks with the mediaeval dataset. Personal and Ubiquitous Computing",
      "authors": [
        "Yesid Ospitia Medina",
        "José Ramón Beltrán",
        "Sandra Blázquez",
        "Baldassarri"
      ],
      "year": "2020",
      "venue": "Emotional classification of music using neural networks with the mediaeval dataset. Personal and Ubiquitous Computing",
      "doi": "10.1007/s00779-020-01393-4"
    },
    {
      "citation_id": "11",
      "title": "Musical instrument emotion recognition using deep recurrent neural network",
      "authors": [
        "Sangeetha Rajesh",
        "N J Nalini"
      ],
      "year": "2020",
      "venue": "Procedia Computer Science",
      "doi": "10.1016/j.procs.2020.03.178"
    },
    {
      "citation_id": "12",
      "title": "Supervised machine learning for audio emotion recognition. Personal and Ubiquitous Computing",
      "authors": [
        "Stuart Cunningham",
        "Harrison Ridley",
        "Jonathan Weinel",
        "Richard Picking"
      ],
      "year": "2020",
      "venue": "Supervised machine learning for audio emotion recognition. Personal and Ubiquitous Computing",
      "doi": "10.1007/s00779-020-01389-0"
    },
    {
      "citation_id": "13",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "14",
      "title": "Song emotion recognition: A study of the state of the art. Anais do XVIII Simpósio Brasileiro de Computação Musical",
      "authors": [
        "Arthur Nicholas Dos Santos",
        "Karen Gissell Rosero",
        "Bruno Jácome",
        "Masiero Sanches"
      ],
      "year": "2021",
      "venue": "Song emotion recognition: A study of the state of the art. Anais do XVIII Simpósio Brasileiro de Computação Musical"
    },
    {
      "citation_id": "15",
      "title": "High-level analysis of audio features for identifying emotional valence in human singing",
      "authors": [
        "Stuart Cunningham",
        "Jonathan Weinel",
        "Richard Picking"
      ],
      "year": "2018",
      "venue": "Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion",
      "doi": "10.1145/3243274.3243313"
    },
    {
      "citation_id": "16",
      "title": "A multilingual framework of cnn and bi-lstm for emotion classification",
      "authors": [
        "Ashima Yadav",
        "Dinesh Kumar"
      ],
      "year": "2020",
      "venue": "2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",
      "doi": "10.1109/ICCCNT49239.2020.9225614"
    },
    {
      "citation_id": "17",
      "title": "On the differences between song and speech emotion recognition: Effect of feature sets, feature types, and classifiers",
      "authors": [
        "Bagus Tris",
        "Masato Akagi"
      ],
      "year": "2020",
      "venue": "2020 IEEE REGION 10 CONFERENCE (TENCON)",
      "doi": "10.1109/TENCON50793.2020.9293852"
    },
    {
      "citation_id": "18",
      "title": "The subversive songs of bossa nova: Tom jobim in the era of censorship",
      "authors": [
        "Irina Priore",
        "Chris Stover"
      ],
      "year": "2014",
      "venue": "Analytical Approaches to World Music"
    }
  ]
}