{
  "paper_id": "2211.04769v1",
  "title": "Interpretable Explainability In Facial Emotion Recognition And Gamification For Data Collection",
  "published": "2022-11-09T09:53:48Z",
  "authors": [
    "Krist Shingjergji",
    "Deniz Iren",
    "Felix Bottger",
    "Corrie Urlings",
    "Roland Klemke"
  ],
  "keywords": [
    "Affective computing",
    "facial emotion recognition",
    "gamification",
    "explainable AI",
    "interpretable machine learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Training facial emotion recognition models requires large sets of data and costly annotation processes. To alleviate this problem, we developed a gamified method of acquiring annotated facial emotion data without an explicit labeling effort by humans. The game, which we named Facegame, challenges the players to imitate a displayed image of a face that portrays a particular basic emotion. Every round played by the player creates new data that consists of a set of facial features and landmarks, already annotated with the emotion label of the target facial expression. Such an approach effectively creates a robust, sustainable, and continuous machine learning training process. We evaluated Facegame with an experiment that revealed several contributions to the field of affective computing. First, the gamified data collection approach allowed us to access a rich variation of facial expressions of each basic emotion due to the natural variations in the players' facial expressions and their expressive abilities. We report improved accuracy when the collected data were used to enrich well-known in-the-wild facial emotion datasets and consecutively used for training facial emotion recognition models. Second, the natural language prescription method used by the Facegame constitutes a novel approach for interpretable explainability that can be applied to any facial emotion recognition model. Finally, we observed significant improvements in the facial emotion perception and expression skills of the players through repeated game play.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expressions are imperative to non-verbal human communication as they provide a means of conveying information regarding the emotional state  [1]  as well as the behavioral intentions  [2]  of the individual. Emotions are fundamental components of social interaction  [3] , and the ability to express and perceive emotions is an invaluable asset for building social connections. The holy grail of affective computing is to empower computer systems with the ability to perceive and express emotions, and be able to form social ties with human users  [4] . Until very recently, this ability has been considered unique to humans. However, especially with the recent advances in Artificial Intelligence (AI), many studies have been conducted that focus on the automated recognition of emotions  [5] .\n\nThe common approach of training machine learning models for facial emotion recognition (FER) is supervised learning, which requires large sets of data  [6] . Specifically, deep FER models are challenged by a lack of sufficient data for training  [7] . Collecting and curating such large datasets is a costly and time-consuming endeavour since labeling by human annotators is necessary  [8] . This poses an obstacle to achieving significant performance improvements in emotion recognition research.\n\nAnother major challenge lies in the explainability and interpretability of emotion recognition models. Studies mostly evaluate emotion recognition models using accuracy and confusion matrices; however, these metrics often fall short in reporting the utility of the models for humans. Interpretable models should provide explanations that are simple enough to be understood by their users, and are given in a language that is meaningful to them  [9] . The explainability of emotion recognition models has been very rarely addressed in literature. The approaches to achieve explainability are limited to modelagnostic methods that explain the output of the model based on the inputs, and model-transparent methods (e.g.,  [10] ,  [11] ) that highlight the activation in different layers of artificial neural networks  [12] . However, neither approach necessarily provides human-friendly explanations that are interpretable by their users.\n\nThe challenges regarding collecting and curating excessive amounts of labeled data for training FER models, and yielding interpretable explanations from such models call for heterodox methods. In this study, we propose a gamification approach towards the collection of annotated facial emotion data. The proposed game, which is named; Facegame, embodies a method for providing natural language prescription as feedback to the players, effectively serving as a means of achieving interpretable explainability. In summary, our contributions to the field of affective computing are as follows:\n\n• We present a gamification approach for rapidly collecting 978- • We propose a novel approach for interpretable explanability by translating the intermediary facial features into natural language prescriptions, and providing them as an explanation for the emotion classifications provided by any FER model. • The presented gamification approach leads to significant improvements in the facial emotion perception and expression skills of the players. The remaining of this paper is structured as follows. Section II provides a literature review on emotion recognition, explainable AI, and gamified data collection. Section III describes the core contributions. Section IV presents the details of our experimental study. Section V discloses the results of our experiments. Finally, Section VI provides a discussion on the theoretical and practical implications of our contributions, and concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "A. Face expressions, action units, and their automated recognition Facial expressions are a means for humans to express their emotions; thus, their automated detection is an important goal of affective computing. Facial expressions are movements and positions of the facial muscles that can be identified by Action Units (AUs); hierarchical components of movements of individual or groups of facial muscles that describe the changes in facial expressions  [13] . There are studies focusing on the correlation between AUs and the basic emotions, namely, happiness, sadness, fear, disgust, anger, and surprise  [14] . For example, Reisenzein et al.  [15]  reported coherence between amusement and smiling, and Wegrzyn et al.  [16]  presented a detailed mapping between the basic emotions and different parts of the face, e.g., lid raiser is essential for fear detection and lid tightener for anger. Apart from the basic emotions, there are studies aiming at detecting more complex emotional states, such as confusion, by utilizing AUs  [17] .\n\nThe strong relationship between emotional facial expressions and AUs has motivated researchers to develop AUdetection algorithms as well as curating AU-labeled face expression datasets such as CK+  [18]  and DISFA  [19] . For instance, Baltrušaitis et al.  [20]  presented an AU occurrence and intensity algorithm based on Histogram of Oriented Gradients (HOG); a method to describe an image by the distribution of intensity gradients or edge direction  [21] ), and geometrical features (e.g., shape and landmarks; detection and localization of certain characteristic points on the face  [22] ). This work also highlights the positive impact of using various datasets to the generalizability of the model performance. Shao et al.  [23]  presented a framework for detecting 10 AUs using the attention mechanism, i.e., finding the region-of-interest for each AU. Jacob and Stenger  [24]  outperformed their previous model by employing a correlation network based on a transformer-encoder architecture, to capture the relationship between different AUs for a wide range of expressions of emotions. Other prominent examples of architectures for AU detection are the JAA-Net  [25] , which uses high-level features of face alignment for AU detection, and DRML  [26]  which uses feed-forward functions to induce regions on the face that are important.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Explainable And Interpretable Ai",
      "text": "As AI finds widespread application across many domains, the need for explainable AI (XAI) is rapidly growing as well. However, most explainability approaches do not target endusers, and their outcomes are not directly interpretable by humans. One way to address this issue is to improve the transparency of AI models. Model transparency focuses on explaining \"how the system made a decision\"  [27] . There are models that are transparent by design, e.g., decision trees, and others that are \"black box\" and require additional tools for explainability  [28] . In recent years, explanation tools have been designed to provide users insights on the decision-making process of a system. The study of Jeyakumar  [12]  showed that the users prefer explanations by example in most of the cases. Rosenfeld  [29]  presents a set of metrics that are suitable for evaluating the effectiveness of explainable AI, namely, i) the difference between the explanations' logic and the agent's actual performance, ii) the number of rules in the agent's explanation, iii) the number of features used to construct the explanation, and iv) the stability of the agent's explanation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Crowdsourcing And Gamification For Data Collection",
      "text": "Crowdsourcing is defined as the act of outsourcing a task that is commonly performed by designated agents to a large number of individuals  [30] . Crowdsourcing has been used both by industry and scientific communities for a variety of purposes, one of which is labeled data collection. In most cases, crowd workers complete a task with the motivation of monetary gain. Even though this approach has been proven cost-effective, it has also been criticized because it potentially leads to questionable data quality unless the necessary quality assurance mechanisms are put in place  [31] . A subtype of crowdsourcing, games-with-a-purpose  [32] , provides a different kind of incentive  [33]  for the workers to complete the tasks to the best of their abilities, and it generally incurs no additional costs to the employer. The design of crowdsourcing tasks in the form of a game is considered a part of a much larger concept; gamification. Gamification can be defined as a technique of using game elements in non-game systems to improve user experience and engagement  [34] , increasing the motivation of the respondents by satisfying psychological and social needs  [35] . Gamification of data collection finds application in different domains  [36] , such as education  [37]  [38] and health  [39] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iii. Gamified Data Collection And Interpretable Fer",
      "text": "In this section, we present our solution that addresses the challenges of labeled data collection for FER model training and devising human-friendly explanations for emotion recognition systems. Specifically, we elaborate on the gamified data collection approach of Facegame and the underlying interpretable FER method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Facegame",
      "text": "Emotional facial expressions emerge rapidly and mostly involuntarily on human faces. Nevertheless, humans are exceptionally good at recognizing even the subtlest cues that appear on the faces of others. Even though humans inherently possess these abilities, it is surprisingly challenging to exercise them deliberately. The motivation of Facegame is to provide the players with a challenging way to exercise the skills of facial expression perception and mimicking.\n\nIn Facegame, the goal of the players is to mimic the facial expression shown on a target image. The interface of Facegame (Fig.  1 ) displays two images together; (a) a target image from the database of the game, which contains a face that exhibits a certain basic emotion and (b) the player's camera feed. Thus, the interface allows the player to compare both faces and try to imitate the target face.\n\nAll target images in the database are labeled based on six basic emotions  [13]  by human experts, as well as the 20 AUs (Table  I  ) automatically using a pre-trained classifier provided by Py-Feat  [40] . The classifier's inputs are the following two vectors: the facial landmarks, a (68×2) vector of the landmark locations that is computed with the dlib package  [41] , and the HOGs; a vector of (5408 × 1) features that describe an image as a distribution of orientations  [21] . The HOGs are calculated for the faces that are aligned using the position of the two eyes, and masked using the positions of the landmarks. The model's output is a list of AUs detected on the face image. The pipeline of the AU detection is shown in Fig.  2 .\n\nThe player is given five seconds to mimic the target expression. Afterward, the player's image is processed and automatically labeled with AUs. The set of AUs on the target image are already known beforehand. Consecutively, the Jaccard Index of the two AU sets yields the score as seen in the Equation  1 ; P and T depicting the AU sets of the player and target respectively. Players can retry imitating the same facial image to increase their scores or move on to another image.  Every time a player plays the game, new data are generated. The score as a game element provides feedback to the player, and the players are motivated to do better and improve their scores, thus, generating better representations of the target facial emotion. This acts as an inherent quality assurance mechanism. Nevertheless, some players may show poor performance due to various reasons. For instance, they might be just exploring and testing the game, or their lighting conditions might be sub-optimal, or they simply do not feel motivated to do good in the game. In any case, such turns would yield low scores, and the data originating from players that consistently score low can easily be eliminated, thus, diminishing noise in data.\n\nThe turns that yield a high score are considered good representations of the emotional face expression on the target image, which is already labeled with one of six basic emotions. Thus, the players' image can automatically be annotated with the same label as the target image. The minor differences between the player and target AU sets provide a desirable variance in the distribution of AUs corresponding to a certain emotional face expression. This way, the variance in the AU distribution is created naturally by human players instead of automatically generated by means of simulation, potentially improving the in-the-wild performance of FER when used in training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Interpretable Fer Explanation",
      "text": "Even though there is no precise formula for how combinations of AUs translate into emotional expressions, some strong correlations exist. For instance, a happy face generally exhibits a smile, which is characterized by the existence of the \"lip corner puller.\" We propose using AUs as a means for explaining the the outcome of FER models. Specifically, we utilize AU detection parallel to FER, and translate the identified AUs into natural language descriptions, which constitute humanfriendly, interpretable explanations of FER (Figure  3 ). The natural language descriptions are generated by a rule-based dictionary approach.\n\nWe created the dictionary of AU descriptions based on the definitions on Facial Action Coding System  [13] . The dictionary contains AU combinations categorized based on the facial muscle types and areas of appearance: cheeks, eyebrows, eyelids, lips, chin and nose, mouth, horizontal, oblique, and orbital. Every combination of the AUs that fall into these categories are represented in the dictionary. One example entry from the dictionary is as follows:\n\nEyebrows, AU4: \"brow lowerer\" description: \"eyebrows are lowered.\" prescription (+): \"lower your eyebrows.\" prescription (-): \"do not lower your eyebrows.\" The prescriptions of what players must do in order to improve their scores are given based on the outcome of a comparison between the target AU set and the player AU set. The intersection of both sets (P for the player AUs and T; target AUs) are the correctly mimicked AUs, while the difference between them show two kinds of mistakes; The set P-T includes the AUs that should be removed from the player's expression, and the set T-P covers the AUs that are missing on the player's expression to mimic the target successfully. The AUs in both sets of mistakes are expressed as natural language prescriptions in different polarities; for example; \"raise your eyebrows\" and \"do not raise your eyebrows\".",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Methodology",
      "text": "In this section, we present the overall methodology of this study, and describe the experimental setup for the data collection and analysis. In order to evaluate the efficacy and utility of the proposed gamified data collection method, i.e., Facegame, we pose the first research question as follows.\n\nRQ1: Do the data generated by the players represent emotional facial expressions accurately? We also hypothesize that through playing the game repeatedly, players exercise deliberate practice of their face expression and perception skills, and thus, be able to improve them. Hence, our second research question is as follows.\n\nRQ2: Do the players improve their facial expression and perception skills through repeated play? Finally, to evaluate how the proposed explanation method is perceived, and whether this yield an improved level of understanding regarding the outcome of the FER model. Hence, we formulate the third research question as follows.\n\nRQ3: Do the natural language explanations help players understand the outcome of the FER model?\n\nA. Experiment setup and procedure\n\nIn the experiment design, we adjusted the Facegame slightly. Participants were asked to play six rounds of the game, each round corresponding to one of the six target images, and each image representing one basic emotion. Participants were shown each target image five times in a row to be able observe the score change in each try. They were given five seconds, indicated by a countdown on the screen, to mimic the face. Following, the score of their attempt was displayed (Eq. 1). To examine the potential effect of the natural language prescriptions, we divided the participants into two groups. One group received only the score (control group), and the other received the natural language prescriptions as well as the score (treatment group)",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Analysis Of The Collected Emotional Facial Image Data",
      "text": "To answer the first research question, we analyzed the collected emotional facial image data, and evaluate to what extend the collected data can be used to model facial expressions of emotions. To this end, we used two commonly used in-thewild data sets; Facial Expression Recognition 2013 (FER2013)  [42] , and The Real-world Affective Face Database (RAF-DB)  [43]  [44], and we trained a simple neural network classifier.\n\nFER2013 is a large-scale dataset that includes images collected automatically by the Google image search API labelled with the six basic emotions and neutral. RAF-DB is a real-world dataset that includes images of faces collected from the Internet and manually labelled with crowdsourcing. The neural network architecture we trained is similar to the baseline model used in the study of Bishay et al.  [45] , a shallow convolutional neural network (CNN) including four convolutional layers with 32, 32, 64, and 64 filters respectively and the ReLU activation function. The first three layers were followed by a max-pooling layer with a 2×2 filter, and the last one by a flatten layer. The final two layers are fully-connected. The first fully-connected layer has 96 neurons, while the second has six sigmoid units representing the predictions of the six emotions.\n\nConsidering that FER2013 and RAF-DB are large-scale, unbalanced datasets, samples were taken in order to respect the size and class distribution of the Facegame data. The samples were created by random sampling without replacement 200 and 50 instances from each of the six emotion from the training and testing set, respectively. With this technique we obtained a balanced sample of each set containing 1,200 instances for training and 300 instances for testing. To achieve generalizabile observations, we formed five different samples from each of both FER2013 and RAF-DB datasets. Finally, we compared the performance of the models trained on each sample set twice; first, without the inclusion of the Facegame Data, second, with the inclusion of Facegame Data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Participant Survey",
      "text": "To receive quantitative and qualitative feedback regarding the design of Facegame, and to partially answer the third research question, we prepared a questionnaire. The participants were asked to fill in the questionnaire after completing the game experiment. The questions covered demographics questions, i.e., age and gender, technical information, i.e., type of device and browser used for the game, and their quantitative and qualitative feedback on the game. The quantitative feedback was given with a 5-Likert scale score (Very Satisfied, Somewhat Satisfied, Neutral, Somewhat Unsatisfied, Very Unsatisfied) on different aspects of the game; the ease of use, time to load and browser compatibility, and design. The survey for the participants in the treatment group included additional information regarding the natural language prescriptions. Specifically, they were asked to give a score on the usefulness and the design of the prescriptions. The qualitative feedback was sought with two open-ended questions about comments and suggestions regarding the functionality and the design of the game. Similar to the quantitative feedback, the participants of the treatment group were asked two additional open-ended questions regarding the clarity and other comments on the prescriptions that were displayed. The analysis of the survey data consisted of descriptive statistics on the age of the participants, quantitative analysis of the satisfaction scores regarding the design aspects of the game, and qualitative analysis on the open-ended questions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Analysis Of Skill Improvement",
      "text": "To answer the second and the third research questions, we analyzed the consecutive scores of the players in each round. Specifically, we compared the first score of each round against the mean of the remaining four scores of the same round. We tested the significance of the difference between the mean of the scores for both groups, and also between the groups using a t-test.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Facegame Scores And Skill Improvement",
      "text": "A total number of 36 individuals participated in the experiment of which 18 received the natural language prescriptions while the remaining 18 received only a score. In total, 216 games were played, each game yielding five consecutive scores: S 1 , S 2 , S 3 , S 4 , and S 5 . We examined the score change by comparing the distributions of S1 and the mean of the rest; M rest = M (S 2 , S 3 , S 4 , S 5 ).\n\nOur results show that, for all games (N = 216) the score increased significantly between S 1 (M = 0.4, SD = 0.23) and M rest (M = 0.45, SD = 0.21) with t (215) = 2.61, p < 0.01 which is a clear indication of the learning effect of Facegame.\n\nFor the control group (N = 108), the same comparison also yielded a significant increase in the distributions S 1 (M = 0.41, SD = 0.23) and M rest (M = 0.46, SD = 0.21), t (107) = 2.26, p = 0.02.. For the treatment group (N = 108) that received natural language prescriptions, the comparison showed an insignificant increase in the score distributions S 1 (M = 0.40, SD = 0.24) and M rest (M = 0.44, SD = 0.22), t (107) = 1.44, p = 0.15.\n\nAdditionally, we investigated the number of times a game ends with an increased score for both groups. Our observations showed that 62.9% of the games resulted with an increased score when the participants received natural language descriptions, while the score increased 57.4% when the participants received only a score.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Survey Results And Player Feedback",
      "text": "In the online survey, 36 participants (22 male; 14 female) provided feedback. The age of the participants ranged between 25 and 55 years (M = 33.77, SD = 7.66). Fig.  4  presents the satisfaction scores provided by all participants regarding various aspects of the design of the game. Fig.  5  shows the feedback of the participants who received natural language prescriptions (N = 18) in the experiment regarding their content, design, and usefulness. The results revealed that most of the participants were satisfied or neutral to the content and the design of the prescriptions while attention should be given to the usefulness prescriptions.\n\nWe manually semantically grouped the answers from the two open-ended questions. The open-ended question that inquires participants' suggestions on the prescriptions shows that participants find the use of visualization, prioritization, and personalization useful. Specifically, four participants mentioned that on-screen visualization of the part of the face that they needed to change would help them follow the natural language prescriptions better. Three of the participants found the text too long to read in a short time span and suggested the display of a few of the most important instead. Two participants indicated that more personalized prescriptions would be helpful. Lastly, the 12 participants that commented on the natural language of the prescriptions stated that they found them understandable.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Facial Emotion Recognition With Facegame Data",
      "text": "The Facegame Data that were collected in the experiment consist of 636 images depicting the six basic emotions (86 angry, 89 disgusted, 127 fearful, 132 happy, 99 sad, and 103 surprised). The accuracy for each of the five samples for FER2013 and RAF-DB is shown in Fig.  6  and Fig.  7 , respectively. The average accuracy of the model trained on instances from FER2013 is 32.80%, while for the model trained on the instances from the combination of FER2013 and the Facegame Data is 33,20%. Similarly, the average accuracy of the model trained on instances from RAF-DB is 51,27%, while for the model trained on the instances from the combination of FER2013 and the Facegame Data is 51,87%. The observed low accuracies of the models were expected considering the nature of the sets and the simplicity of our neural network architecture. The images in FER2013 and RAF-DB represent in-the-wild conditions with subtle facial expressions. Therefore, the emotion recognition task is more challenging, and it requires complex classifiers to achieve better accuracy. Moreover, our goal was to examine the quality of the data collected via Facegame in comparison with the existing in-the-wild sets, and for this purpose, a simple neural network sufficed. The results show the we were able to collect labelled data that can potentially improve FER in-the-wild.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. The Mapping Of Aus To Emotion Classes And The Variability",
      "text": "The results of the correlation between the six basic emotions and the AUs are shown in Fig.  8 . For this analysis, the data from the trials that scored below 1  3 were excluded. The results suggest that we were able to capture some strong correlations between certain emotional facial expressions and The results show that we were able to define the emotion classes as a distribution of multiple AUs. Such naturally occurring variety in facial expression data can potentially be used to improve FER in-the-wild.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Discussion And Conclusion",
      "text": "In this paper, we introduced a novel approach for explainable FER that promises three related contributions. First, by means of gamification, we have developed a method for collecting annotated face expression data continuously, which allows us to describe the facial expressions of six basic emotions as a distribution of AUs. Secondly, we proposed and evaluated an interpretable FER explainability method that uses AUs as features to describe the outcomes of FER models, i.e., facial emotion classes. The experimental observations indicate that the natural language explanations of face expressions are interpretable by humans. Our quantitative and qualitative results highlight improvement opportunities regarding the design of Facegame and how we communicate the face expression explanations. Finally, we observed that the players were able to improve their face expression and perception skills by playing the game.\n\nOur results have potential theoretical and practical implications. Our method of acquiring nuanced face expressions (i.e., distributions of AUs) that correlate with facial emotion classes provides a means to improve the performance of inthe-wild FER models. Such performance improvements may pave the way for novel practical approaches in many domains, such as online synchronous learning  [46] . Moreover, the gamification approach offers a sustainable, continuous selftraining process of FER models. Our explainability method that uses AUs as intermediary features to describe facial emotions provides a novel approach towards achieving interpretable, human-friendly explanations of FER models. In this study, we conducted our experiment and analyses on a limited sample. In the future, based on the observations and feedback collected during this study, we will continue this line of research and improve the ways of providing prescriptions to the players of Facegame by combining the natural language explanations with graphical methods. Finally, the quality of the data was assured by using a threshold of the score as a selection criterion. The score of the trial was calculated by the presence/absence of the AUs regardless of the intensity which has the risk of including data with exaggerating facial expressions and introducing bias to the data set  [47] . In future studies, we aim to include the intensity of the AUs as well as a higher threshold in the selection criteria to increase the accuracy and fairness of the collected data set.\n\nThe goal of this study is limited to the evaluation of gamification as a data collection method. In the future, we aim to collect more data by crowdsourcing a large number of players which will result in a more in-depth analysis of the collected data set. In this setting, the player was asked to mimic the facial expressions of a face displaying one emotion and not triggered to experience that emotion. This approach has limitations considering that there are differences between the posed and spontaneous facial expressions in morphological and dynamic aspects of certain emotions  [48]  as well as demographical mismatches between the target image and the player. In future studies, we aim to increase the variance of the dataset by including in the set of the target images faces with spontaneous facial expressions as well as increasing the demographic diversity of the target images.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "In this study, the personal data collection was limited to facial images of the players. The collected data were securely stored in the server of the research institute which can only be accessed by the researchers of this study, The participants were informed regarding the experiment and data collection, and they provided consent prior to taking part in the experiment.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) displays two images together; (a) a target",
      "page": 3
    },
    {
      "caption": "Figure 2: The player is given ﬁve seconds to mimic the target",
      "page": 3
    },
    {
      "caption": "Figure 1: The interface of the Facegame.",
      "page": 3
    },
    {
      "caption": "Figure 2: The pipeline of the AU detection.",
      "page": 3
    },
    {
      "caption": "Figure 3: A graphical representation of the overall explainability model.",
      "page": 4
    },
    {
      "caption": "Figure 5: shows the",
      "page": 5
    },
    {
      "caption": "Figure 4: Likert scale",
      "page": 6
    },
    {
      "caption": "Figure 5: Likert scale",
      "page": 6
    },
    {
      "caption": "Figure 6: and Fig. 7,",
      "page": 6
    },
    {
      "caption": "Figure 8: For this analysis, the",
      "page": 6
    },
    {
      "caption": "Figure 6: Performance of the models trained on FER2013 dataset and FER2013",
      "page": 6
    },
    {
      "caption": "Figure 7: Performance of the models trained on RAF-DB and RAF-DB",
      "page": 6
    },
    {
      "caption": "Figure 8: Heatmap of the occurrences of AUs detected on the participants and",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AU1\nInnerBrow\nRaiser": "AU6\nCheekRaiser",
          "AU2\nOuterBrow\nRaiser": "AU7\nLidTightener",
          "AU4\nBrowLowerer": "AU9\nNoseWrinkler",
          "AU5\nUpperLid\nRaiser": "AU10\nUpperLip\nRaiser"
        },
        {
          "AU1\nInnerBrow\nRaiser": "AU11\nNasolabial\nDeepener",
          "AU2\nOuterBrow\nRaiser": "AU12\nLipCorner\nPuller",
          "AU4\nBrowLowerer": "AU14\nDimpler",
          "AU5\nUpperLid\nRaiser": "AU15\nLipCorner\nDepressor"
        },
        {
          "AU1\nInnerBrow\nRaiser": "AU17\nChinRaiser",
          "AU2\nOuterBrow\nRaiser": "AU20\nLipStretcher",
          "AU4\nBrowLowerer": "AU23\nLipTightener",
          "AU5\nUpperLid\nRaiser": "AU24\nLipPressor"
        },
        {
          "AU1\nInnerBrow\nRaiser": "AU25\nLipsPart",
          "AU2\nOuterBrow\nRaiser": "AU26\nJawDrop",
          "AU4\nBrowLowerer": "AU28\nLipSuck",
          "AU5\nUpperLid\nRaiser": "AU43\nEyesClosed"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional experiences in everyday life: A survey approach",
      "authors": [
        "K Scherer",
        "P Tannenbaum"
      ],
      "year": "1986",
      "venue": "Motivation and Emotion"
    },
    {
      "citation_id": "2",
      "title": "Human facial expression: An evolutionary view",
      "authors": [
        "A Fridlund"
      ],
      "year": "2014",
      "venue": "Human facial expression: An evolutionary view"
    },
    {
      "citation_id": "3",
      "title": "Facial expressions of emotion",
      "authors": [
        "D Matsumoto",
        "K Dacher",
        "M Shiota",
        "M O'sullivan",
        "M Frank"
      ],
      "year": "2008",
      "venue": "Facial expressions of emotion"
    },
    {
      "citation_id": "4",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "5",
      "title": "Automated Emotion Recognition: Current Trends and Future Perspectives",
      "authors": [
        "M Maithri",
        "U Raghavendra",
        "A Gudigar",
        "J Samanth",
        "D Prabal",
        "M Murugappan"
      ],
      "year": "2022",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "6",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "7",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "D Weihong"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "8",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for unconstrained facial expression recognition",
      "authors": [
        "S Li",
        "D Weihong"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "9",
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "authors": [
        "L Gilpin",
        "D Bau",
        "B Yuan",
        "A Bajwa",
        "M Specter",
        "L Kagal"
      ],
      "year": "2018",
      "venue": "2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)"
    },
    {
      "citation_id": "10",
      "title": "Towards the Explainability of Multimodal Speech Emotion Recognition",
      "authors": [
        "P Kumar",
        "P Kaushik",
        "B Raman"
      ],
      "year": "2021",
      "venue": "Towards the Explainability of Multimodal Speech Emotion Recognition"
    },
    {
      "citation_id": "11",
      "title": "Skeleton-Based Explainable Bodily Expressed Emotion Recognition Through Graph Convolutional Networks",
      "authors": [
        "E Ghaleb",
        "A Mertens",
        "S Asteriadis",
        "G Weiss"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "12",
      "title": "How can i explain this to you? an empirical study of deep neural network explanation methods",
      "authors": [
        "J Jeyakumar",
        "J Noor",
        "Y Cheng",
        "L Garcia",
        "M Srivastava"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "14",
      "title": "Facial expression and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1993",
      "venue": "American psychologist"
    },
    {
      "citation_id": "15",
      "title": "Coherence between emotion and facial expression: Evidence from laboratory experiments",
      "authors": [
        "R Reisenzein",
        "M Studtmann",
        "G Horstmann"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "16",
      "title": "Mapping the emotional face. How individual face parts contribute to successful emotion recognition",
      "authors": [
        "M Wegrzyn",
        "M Vogt",
        "B Kireclioglu",
        "J Schneider",
        "J Kissler"
      ],
      "year": "2017",
      "venue": "PloS one"
    },
    {
      "citation_id": "17",
      "title": "Classifying confusion: autodetection of communicative misunderstandings using facial action units",
      "authors": [
        "N Borges",
        "L Lindblom",
        "B Clarke",
        "A Gander",
        "R Lowe"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "18",
      "title": "The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE computer society conference on computer vision and pattern recognitionworkshops"
    },
    {
      "citation_id": "19",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "T Baltrušaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "21",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05)"
    },
    {
      "citation_id": "22",
      "title": "A comparative study of face landmarking techniques",
      "authors": [
        "S Ulukaya",
        "B Sankur"
      ],
      "year": "2013",
      "venue": "EURASIP Journal on Image and Video Processing"
    },
    {
      "citation_id": "23",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "Y Wu"
      ],
      "year": "2019",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "24",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "JAA-Net: joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "Y Wu"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "26",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W Chu"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Explainability in human-agent systems",
      "authors": [
        "A Rosenfeld",
        "A Richardson"
      ],
      "year": "2019",
      "venue": "Autonomous Agents and Multi-Agent Systems"
    },
    {
      "citation_id": "28",
      "title": "A survey of methods for explaining black box models",
      "authors": [
        "R Guidotti",
        "A Monreale",
        "S Ruggieri",
        "F Turini",
        "F Giannotti",
        "D Pedreschi"
      ],
      "year": "2018",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "29",
      "title": "Better metrics for evaluating explainable artificial intelligence",
      "authors": [
        "A Rosenfeld"
      ],
      "year": "2021",
      "venue": "Proceedings of the 20th international conference on autonomous agents and multiagent systems"
    },
    {
      "citation_id": "30",
      "title": "The rise of crowdsourcing",
      "authors": [
        "J Howe"
      ],
      "year": "2006",
      "venue": "The rise of crowdsourcing"
    },
    {
      "citation_id": "31",
      "title": "Cost of quality in crowdsourcing",
      "authors": [
        "D Iren",
        "S Bilgen"
      ],
      "year": "2014",
      "venue": "Human Computation"
    },
    {
      "citation_id": "32",
      "title": "Human computation: a survey and taxonomy of a growing field",
      "authors": [
        "A Quinn",
        "B Bederson"
      ],
      "year": "2011",
      "venue": "Proceedings of the SIGCHI conference on human factors in computing systems"
    },
    {
      "citation_id": "33",
      "title": "Crowdsourcing: A taxonomy and systematic mapping study",
      "authors": [
        "M Hosseini",
        "A Shahri",
        "K Phalp",
        "J Taylor",
        "R Ali"
      ],
      "year": "2015",
      "venue": "Computer Science Review"
    },
    {
      "citation_id": "34",
      "title": "From game design elements to gamefulness: defining\" gamification",
      "authors": [
        "S Deterding",
        "D Dixon",
        "R Khaled",
        "L Nacke"
      ],
      "year": "2011",
      "venue": "Proceedings of the 15th international academic MindTrek conference: Envisioning future media environments"
    },
    {
      "citation_id": "35",
      "title": "Analysis and application of gamification",
      "authors": [
        "A Aparicio",
        "F Vela",
        "J Sánchez",
        "J Montes"
      ],
      "year": "2012",
      "venue": "Proceedings of the 13th International Conference on Interacción Persona-Ordenador"
    },
    {
      "citation_id": "36",
      "title": "Accessible and Ethical Data Annotation with the Application of Gamification",
      "authors": [
        "V Gurav",
        "M Parkar",
        "P Kharwar"
      ],
      "year": "2019",
      "venue": "International Conference on Recent Developments in Science, Engineering and Technology"
    },
    {
      "citation_id": "37",
      "title": "Taking the pulse of a classroom with a gamified audience response system",
      "authors": [
        "J López-Jiménez",
        "J Fernández-Alemán",
        "L González",
        "O Sequeros",
        "B Valle",
        "J García-Berná"
      ],
      "year": "2022",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "38",
      "title": "The effect of using Kahoot! for learning-A literature review",
      "authors": [
        "A Wang",
        "R Tahir"
      ],
      "year": "2020",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "39",
      "title": "Engaging people to participate in data collection",
      "authors": [
        "P Tobien",
        "L Lischke",
        "M Hirsch",
        "R Krüger",
        "P Lukowicz",
        "A Schmidt"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct"
    },
    {
      "citation_id": "40",
      "title": "Py-feat: Python facial expression analysis toolbox",
      "authors": [
        "J Cheong",
        "T Xie",
        "S Byrne",
        "L Chang"
      ],
      "year": "2021",
      "venue": "Py-feat: Python facial expression analysis toolbox",
      "arxiv": "arXiv:2104.03509"
    },
    {
      "citation_id": "41",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "42",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner"
      ],
      "year": "2013",
      "venue": "International conference on neural information processing"
    },
    {
      "citation_id": "43",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "44",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "45",
      "title": "Which CNNs and Training Settings to Choose for Action Unit Detection? A Study Based on a Large-Scale Dataset",
      "authors": [
        "M Bishay",
        "A Ghoneim",
        "M Ashraf",
        "M Mavadati"
      ],
      "year": "2021",
      "venue": "16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "46",
      "title": "Sense the classroom: AI-supported synchronous online education for a resilient new normal",
      "authors": [
        "K Shingjergji",
        "D Iren",
        "C Urlings",
        "R Klemke"
      ],
      "year": "2021",
      "venue": "EC-TEL (Doctoral Consortium)"
    },
    {
      "citation_id": "47",
      "title": "Exaggerating facial expressions: A way to intensify emotion or a way to the uncanny valley?",
      "authors": [
        "M Mäkäräinen",
        "J Kätsyri",
        "T Takala"
      ],
      "year": "2014",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "48",
      "title": "Spontaneous Facial Expressions Are Different from Posed Facial Expressions: Morphological Properties and Dynamic Sequences",
      "authors": [
        "S Namba",
        "S Makihara",
        "R Kabir",
        "M Miyatani",
        "T Nakao"
      ],
      "year": "2017",
      "venue": "Current Psychology"
    }
  ]
}