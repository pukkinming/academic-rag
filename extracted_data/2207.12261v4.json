{
  "paper_id": "2207.12261v4",
  "title": "Graphcfc: A Directed Graph Based Cross-Modal Feature Complementation Approach For Multimodal Conversational Emotion Recognition",
  "published": "2022-07-06T13:56:48Z",
  "authors": [
    "Jiang Li",
    "Xiaoping Wang",
    "Guoqing Lv",
    "Zhigang Zeng"
  ],
  "keywords": [
    "Emotion Recognition in Conversation",
    "Multimodal Fusion",
    "Graph Neural Networks",
    "Cross-modal Feature Complementation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. We extract various types of edges from the constructed graph for encoding, thus enabling GNNs to extract crucial contextual and interactive information more accurately when performing message passing. Furthermore, we design a GNN structure called GAT-MLP, which can provide a new unified network framework for multimodal learning. The experimental results on two benchmark datasets show that our GraphCFC outperforms the state-of-the-art (SOTA) approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTIONS pervade our personal and professional lives, shape our relationships and social interactions  [1] . Consequently, research on emotion recognition and understanding is crucial. Emotion recognition in conversation (ERC), which aims to automatically determine the emotional state of a speaker during a conversation based on information about human behavior such as text content, facial expressions and audio signals, has received extensive attention and study in recent years  [2] -  [5] . Emotion recognition can be applied to many practical scenarios such as medical diagnosis  [6] , opinion mining  [7] , fake news detection  [8]  and dialogue generation  [9] , to provide high-quality and humanized empathetic services. ERC will play an increasingly vital role as Human-Computer Interaction (HCI) technology advances.\n\nIn a multi-person dialogue scenario, each speaker generates a succession of ups and downs in emotional reactions. The majority of prior techniques have been centered on the study of contextual ERC systems. DialogueGCN  [2]  utilizes a relational Graph ATtention network (GAT) to capture long-distance contextual dependencies in conversations, and leverages self-and inter-dependency of the speakers to improve context understanding for ERC. Shen et al.  [10]  model the conversation as a directed acyclic graph in an attempt to combine the respective strengths of Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs). DialogueCRN  [4]  designs multiturn reasoning modules to extract and integrate the emotional clues in order to fully understand the conversational context from a cognitive perspective. Nonetheless, these approaches only take into account information from a single modality. The ERC system will fail if the uni-modality signals do not carry a significant emotional signature. Furthermore, the notorious emotional-shift issue plagues uni-modal emotion recognition systems  [3] ,  [10] .\n\nIn real scenarios, people can instinctively obtain complex emotional cues from multiple information sources including the facial expressions, personality and tone of speaker, as well as the conversation history to infer the real emotions of others. Multimodal ERC follows this idea and attempts to combine simultaneously information from multiple modalities such as textual, acoustic and visual modalities to comprehensively identify emotions in conversations. Fig.  1  shows an instance of a multimodal conversation system. The ERC system takes each modality as input and then performs emotional prediction. bc-LSTM  [11]  employs textual, visual and acoustic modalities for multimodal emotion recognition, feeding each modality separately into a bidirectional Long Short-Term Memory (LSTM) network to acquire contextual information. CMN  [12]  uses Gated Recurrent Unit (GRU) and multimodal features for contextual modeling, as well as applies an attention mechanism to pick the most valuable historical utterances. ICON  [13]  models the contextual knowledge of self-and inter-speaker impacts via a GRU-based multi-hop memory network, while capturing essential emotional cues applying an attention module. DialogueRNN  [3]  detects current sentiment by tracking the contextual information of the utterance and considering the characteristics of the speaker. These approaches, nevertheless, directly concatenate multimodal information without incorporating the interaction between modalities. In addition, recurrence-based approaches tend to use recent utterances for modeling, which makes these models difficult to gather longdistant information.  Recently, GNNs have attracted wide attention in a variety of fields because they can model relationships. MMGCN  [5]  achieves outstanding performance in multimodal conversational emotion recognition by employing GNNs to capture long-distance contextual information and inter-modal interactive information. However, MMGCN connects the current node directly to all other nodes in the dialogue, perhaps resulting in redundant information. Besides that MMGCN simply divides all edges into two types (i.e., inter-modal edges and intra-modal edges) and leverages the angular similarity to represent edge weight, which can cause the inability of the GNN to accurately select important information when aggregating neighbor information.\n\nThere is a heterogeneity gap  [14] ,  [15]  between distinct modalities in multimodal fusion, which makes it challenging to effectively fuse multimodal features. MMGCN directly puts the utterance of three modalities into the graph as the same type of nodes, and then performs multimodal feature fusion by GNN. This approach not only adds redundant information due to inconsistent data distribution among modalities, but also may risk losing diverse information in the conversational graph. Therefore, we propose a novel graph-based multimodal feature fusion method to alleviate the aforementioned limitations. In the Graph based Cross-modal Feature Complementation (GraphCFC) module, unlike MMGCN treating all utterances as neighbor nodes, we model the conversation as a multimodal directed heterogeneous graph with variable contextual information and extract more than two types of edges from the graph based on the perspective of modality type and speaker identity; then, we utilize multiple subspace extractors to simultaneously preserve the consistency and diversity of multimodal features; finally, we employ the Pairwise Cross-modal Complementation (PairCC) strategy to gradually achieve feature complementation and fusion. In addition, we propose a novel GNN layer, GAT-MLP, to provide a unified network model for multimodal feature fusion, which can also effectively minimize the over-smoothing problem  [16]  of GNNs. Our main contributions in this paper are as follows: ① We propose a directed Graph based Cross-modal Feature Complementation (GraphCFC) module. GraphCFC can not only effectively alleviate the heterogeneity gap issue of multimodal fusion, but also sufficiently extract the diverse information from multimodal dialogue graphs. ② A new GNN layer named GAT-MLP is designed, which not only alleviates the over-smoothing problem of GNNs, but also provides a new network framework for multimodal learning. ③ The conversations are represented as a multimodal directed graph with variable contextual utterances and extract distinct types of edges from this graph for encoding, so that GAT-MLP can accurately select the critical contextual and interactive information. ④ Extensive comparative experiments and ablation studies are conducted on two benchmark datasets. The experimental results reveal that our proposed GraphCFC is capable of productive complementation and fusion of multimodal features, attaining optimal performance in comparison to previous SOTA approaches.\n\nThe remainder of this paper is organized as follows. The related works of this paper is briefly mentioned in Section II. Section III depicts the proposed graph-based multimodal ERC method. Section IV presents the experimental setup of this work, and the experimental results are analyzed in detail in Section V. Section VI summarizes and prospects to this work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Emotion Recognition In Conversation",
      "text": "Emotion Recognition in Conversation (ERC), which aims to predict the emotion label of each utterance, plays a crucial role in affective dialogue due to facilitating the understanding of the user's emotions and responding with empathy. This task has been recently attached much importance by numerous NLP researchers for its potential applications in extensive areas such as opinion mining in social media  [7] , empathy building in dialogue systems  [17]  and detection of fake news  [8] . The emotion of a query utterance is easily influenced by many factors, such as the identity of speaker and the context of conversation. Indeed, the way to model the conversational context is the core of this task  [18] .\n\nMassive methods have been taken to model the conversation context on the textual modality, which can be divided into two categories: graph-based methods and recurrence-based methods. Besides, models based on multimodal inputs have been proposed, which improve the performance of ERC tasks by leveraging multimodal dependencies and complementarities.\n\nGraph-based methods: DialogGCN  [2]  constructs a dialogue graph where each utterance is related with the surrounding utterances. Ishiwatari et al.  [19]  improves DialogGCN by taking positional encoding into account. ConGCN  [20]  constructs a large heterogeneous graph by treating the speakers and utterances as nodes. KET  [21]  leverages a context-aware affective graph attention mechanism to dynamically capture external commonsense knowledge. DAG-ERC  [10]  combines the advantages of both graph neural networks and recurrent neural networks, and performs excellently without the aid of external knowledge.\n\nRecurrence-based methods: ICON  [13]  and CMN  [12]  both utilize Gated Recurrent Unit (GRU) and memory networks. HiGRU  [22]  is made up of two GRUs, one is an utterance encoder and the other is a conversation encoder. DialogRNN  [3]  is a sequence-based method, where several RNNs model the dialogue dynamically. COSMIC  [23]  constructs a network that is closely similar to DialogRNN and performs better by adding external commonsense knowledge. DialogueCRN  [4]  utilizes bidirectional LSTM to build ERC model from a cognitive perspective.\n\nMultimodal-based methods: CMN  [12]  leverages multimodal information by concatenating the features from three modalities but fails to consider the interaction between modalities. bc-LSTM  [11]  adopts an utterance-level LSTM to capture multimodal information. MFN  [24]  conducts multi-views information fusion and aligns the features of different modalities, but it is unable to model speaker information. MMGCN  [5]  utilizes an undirected graph to explore a more effective way of multimodal fusion, which outperforms significantly other approaches under the multimodal dialogue setting. There have been a range of works  [15] ,  [25] -  [27]  associated with multimodal learning in sentiment analysis. These efforts, however, do not highlight the social interaction and contextual cues between the speaker and listener in a conversation, thus they do not fall under the purview of ERC. In addition, most sentiment analysis tasks only need to distinguish positive, negative, and neutral opinions. Thus it is difficult to divide emotion into numerous categories like Happy, Excited, Sad, Angry, Frustrated as in the case of ERC tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Fusion",
      "text": "Multimodal fusion is one of the most important parts in machine learning, which can integrate information from multiple modalities to predict a result  [28] . It can be intuitively assumed that multimodal fusion can provide three benefits: providing more robust prediction results, capturing complementary information, and working without certain modalities  [29] . In recent years, multimodal fusion in sentiment analysis  [25] -  [27]  has been researched widely and lots of methods such as multiple kernel learning and various neural networks have been explored to cope with it. However, multimodal sentiment analysis rarely involves multi-person conversational information and focuses mainly on utterance-level prediction. Multimodal ERC is the study of conversations with two or more participants and is a conversation-level emotion prediction. For instances, an individual's emotion is not only derived from self-expression, but is also influenced by the expressions of others. Furthermore, Guo et al.  [14]  have noted that multimodal fusion faces several challenges, one of which is the heterogeneity gap  [15]  between modalities. For this reason, our proposed GraphCFC concentrates on alleviating the heterogeneity gap dilemma of conversational emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Graph Neural Networks",
      "text": "In recent years, an increasing number of non-Euclidean data have been represented as graphs. The complexity of graph data has posed significant challenges to existing neural network models. Graph Neural Networks (GNNs) have attracted much attention for their ability to effectively deal with non-Euclidean data. GNNs have been applied in a wide range of applications, including recommendation system, computer vision, natural language processing, biomedicine and traffic forecasting. Graph convolutional networks, a type of GNNs, can be divided into two main streams: the spectral-based and the spatialbased approaches. Spectral-based approaches implement graph convolution by defining filters in a manner similar to graph signal processing. Spatial-based approaches define graph convolution by information propagation, and they have recently gained rapid momentum due to their attractive efficiency, flexibility, and generality. Graph-SAGE  [30] , GAT  [31] , and FastGCN  [32]  are widely-used GNN techniques.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Methods",
      "text": "Given the challenges of multimodal emotion recognition mentioned above, we introduce a novel graph-based multimodal feature fusion approach for ERC in this section. The section consists of four parts, including general overview, uni-modal encoder, Graph based Cross-modal Feature Complementation (GraphCFC) module, and multimodal emotion classifier.\n\nA. General Overview 1) Problem Definition: In an ERC scenario, a dialogue is defined as a sequence of n utterances [(u 1 , s u1 ), (u 2 , s u2 ), ..., (u n , s un )]. Where n denotes the number of utterances, u i is the i-th utterance in this dialogue sequence, and s ui indicates the speaker who utters utterance u i . Each utterance u i consists of m i tokens, i.e., u i can be expressed as [t i1 , t i2 , ..., t imi ]. The number of speaker s in a dialogue system should be greater than or equal to 2; if s ui = s uj (i ̸ = j), then utterance u i and u j are uttered by the same participant in the dialogue. Each utterance u involves three modalities, including textual, acoustic and visual modalities, so utterance u i can also be denoted as\n\nwhere t, a, v denote textual, acoustic and visual modalities, respectively. Given the defined emotion labels Y = [y 1 , y 2 , ..., y l ] (y i is generally represented by one-hot encoding), the objective of the multimodal ERC task is to predict the emotion state label y i for each utterance u i based on the available inter-modal interaction and intramodal context. The quantity of emotion labels in various datasets varies, e.g., 6 for IEMOCAP and 7 for MELD. We also experimented with coarsened emotion labels, which consisted of Positive, Negative and Neutral. For instance, in the IEMOCAP dataset, Happy and Excited are categorized as Positive; Sad, Angry and Frustrated are categorized as Negative; and Neutral remained unchanged.\n\n2) Overall Architecture: Fig.  2  shows the overall architecture of graph-based multimodal ERC in this paper, which mainly consists of uni-modal encoding, Graph based Crossmodal Feature Complementation (GraphCFC) and multimodal emotion classification. Firstly, we encode the uni-modal features by means of three uni-modal encoders. Next, a crossmodal feature complementation module based on GNN is employed for collecting long-distance intra-modal contextual information and inter-modal interactive information. Finally, we utilize multiple loss functions to build multitask learning model for multimodal emotion classification.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Uni-Modal Encoder",
      "text": "To capture the context-aware feature information of textual modality, referring to MMGCN  [5] , we leverage a bidirectional Long Short Term Memory (BiLSTM) network. The feature pre-extraction of textual modality can be formulated as:\n\nwhere x t i and x t h,i are the output and hidden vector of preextractor, respectively;\n\n←--→ LSTM and Θ t ls denote the BiLSTM network and trainable parameter, respectively.\n\nFor acoustic and visual modalities, again as with MMGCN, we use a fully connected network for uni-modal feature preextraction as follows:\n\nwhere x τ i is the output vector of pre-extractor; FC and Θ τ f c\n\nare the fully connected network and trainable parameter, respectively; a and v denote acoustic and visual modalities, respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Graph Based Cross-Modal Feature Complementation Module",
      "text": "We propose a Graph based Cross-modal Feature Complementation (GraphCFC) module for efficient multimodal feature fusion. The module includes two types of information complementation, i.e., intra-modal contextual information and inter-modal interactive information. The four primary reasons for which GraphCFC is proposed are as follows. First, to simultaneously preserve the consistency and diversity information of multimodal features. Second, to select crucial intra-modal contextual information and inter-modal interaction information as accurately as possible. Third, to alleviate the heterogeneity gap problem of multimodal ERC. Last, to propose a network model that can be applied to visual, acoustic, and textual modalities simultaneously.\n\nThe GraphCFC Module is divided into five main parts. First, we describe how to construct the graph; second, we introduce multiple subspace mappings which are leveraged to simultaneously ensure the consistency and diversity of multimodal features; third, we present a new graph neural network structure named GAT-MLP; fourth, we introduce GAT-MLP based Pair-wise Cross-modal Complementation (PairCC) for alleviating the heterogeneity gap issue of multimodal ERC; finally, we detail the GAT structure of GAT-MLP used in this work.\n\n1) Graph Construction: In uni-modal ERC, a dialogue with n utterances is represented as a directed graph G = (V, E); where V is the node set, which denotes the set of utterances, i.e., V = {u 1 , u 2 , ..., u n }; E is the set of relational dependencies between nodes; and if an edge exists for two nodes, then e ij ∈ E has two key properties: edge weight and edge type.\n\nAssuming the existence of two modalities P, Q, we construct the dialogue graph as follows.\n\nNodes: In a graph, each utterance u i (i = 1, 2, ..., n) is considered as node u P i and node u Q i , represented as vector x P i and vector x Q i . If there are n utterances, then V can be denoted as\n\nis the number of modalities and n is the number of utterances.\n\nEdges: In a graph, an edge is defined as a connection between nodes. In the dialogue graph of multiple modalities, we define edges in two perspectives: the contextual connection of intra-modal utterance, and the interactive connection of intermodal utterance. Particularly, we term these two types of edges as intra-edge (E intra ) and inter-edge (E inter ), respectively. The intra-edge is utilized for capturing intra-modal contextual information, whereas the inter-edge is utilized for capturing cross-modal interactive information.\n\nThe intra-edge is defined as follows. Assuming the existence of modality P, we connect the current utterance node u P i with the previous/past j utterance nodes u P i-j , u P i-j+1 , ..., u P i-1 . Similarly, we connect u P i with the next/future k utterance nodes u P i+1 , u P i+2 , ..., u P i+k . Therefore, we can formalize E intra as follows:\n\nwhere i, j, k are constants, t is a variable; and i, j, k are less than n; i, j, k, t all belong to N + . The inter-edge is defined as follows. In a dialogue, we connect the utterance node u P i of modality P to the corresponding utterance node u Q i of modality Q. Thus, we can formulate E inter as follows:\n\nwhere i < n and i ∈ N + .\n\nEdge types: Based on the definition of edges above, we may divide all edges into two types: intra-edge type and interedge type, labeled as ET intra and ET inter , respectively. It is commonly known that if two utterance nodes in a dialogue has edge, it may or may not be from the same speaker. Therefore, ET intra can be subdivided according to the perspective of speaker. Specifically, suppose that there are 3 speakers (s 1 , s 2 , s 3 ) in a dialogue, then the set of existing edge types when only a single modality is considered can be written as:\n\nIt can be easily concluded that if there are D speakers in a dialogue, then there are D×(D+1)/2 kinds of edges/relations. If there are M modalities, then there are M × (D 2 + D)/2 elements in ET intra . ET inter denotes the set of inter-modal edge types. Suppose that there are 3 modalities (mod 1 , mod 2 , mod 3 ) of the same utterance, then ET inter can be formalized as:\n\nTherefore, if there are M modalities in the same utterance, then existing M × (M -1)/2 kinds of edges/relations. In this work, we consider three modalities of an utterance, so that there are a total of three elements in ET inter . Edge weights: Edge weights are utilized to identify the relevance of distinct neighboring nodes when GNNs aggregates information. We employ a learnable attention module with edge features, which is detailed in Section III-C5.\n\n2) Multi-Subspace Extractor: Inspired by MMGCN  [5] , we consider the speaker information is of importance. The embedding of multi-speaker S emb can be formalized as:\n\nwhere S denotes the set of speakers, D is the number of speakers. To encode the speaker identity information, we add the speaker embedding to the features of utterance nodes:\n\nwhere X ζ (ζ ∈ {t, a, v}) is the feature matrix from uni-modal encoder, and\n\nspk denotes the feature matrix adding the speaker embedding; µ ∈ [0, 1] is the ratio of the speaker embedding.\n\nCurrently, one of the quandaries we confront in multimodal fusion is the existence of heterogeneity gap  [14] . In other words, the distribution of data is inconsistent across modalities. Therefore, before performing cross-modal feature complementation, we map the features of each modality into a shared subspace to maintain the consistency of feature representation across modalities. But yet the more similar the feature representations of multiple modalities are, the less complementary the feature between modalities are. To put it another way, we want to preserve the diversity of feature representations of multiple modalities so that the features of one modality can complement those of others. In view of this, we map the features of each modality into separate subspaces for capturing the diversity of feature representations across modalities. We argue that capturing the diversity and consistency information of multiple modalities simultaneously facilitates the complementation and fusion between modalities.\n\nIn order to capture the consistency of multimodal information in the shared subspace, we use three mapping functions F shr with the same trainable parameter Θ shr . For the separate subspace, we use three mapping functions F sep with different trainable parameters Θ ζ sep to capture the variety of multimodal information. The two kinds of mapping methods are shown in Fig.  2 , and are formulated as follows:\n\nwhere ∥ denotes concatenation operation; X  neural network layer, etc. In this paper, we define the mapping function F as follows:\n\nthe mapping function F is actually two fully connected layers; where X is the input of F; Lin, σ, Drop and Norm denote the linear, non-linear activation, dropout and normalization functions, respectively; Θ denotes the learnable parameter. Despite the fact that the shared mapping function F shr and the separate mapping function F sep are utilized to extract consistency and diversity features, they should have the equivalent learning goal, i.e., the features of the same utterance mapped by different functions should correspond to the same emotion label. Therefore, we utilize four subspace loss functions to limit the features extracted by muti-subspace extractor such that they do not deviate from the ultimate goal task. The shared subspace loss function is computed as:\n\nwhere x vat shr,i ∈ X vat shr ; N is the number of dialogues, n(i) is the number of utterances in dialogue i; y ij denotes the ground truth label of the j-th utterance in the i-th dialogue, p ′ ij denotes the probability distribution of predicted emotion label of the j-th utterance in the i-th dialogue; λ is the L2regularizer weight, and\n\nre are the trainable parameters. Similarly, the separate subspace loss function is computed as:\n\nwhere\n\nv} is the type of modalities, i.e., textual, acoustic and visual modalities;\n\nre are the learnable parameters.\n\n3) GAT-MLP Layer: It is well known that the dilemma of over-smoothing exists in GNNs. Experimental studies have shown that the performance of the model begins to deteriorate dramatically as the number of layers in the GNN reaches a specific threshold. This is due to the impact of graph convolutions in that it inherently makes representations of adjacent nodes closer to each other  [16] . Thus, after multiple graph convolutions, the node features within the same connected component of the network tend to be similar and the model degenerates.\n\nInspired by the ResNet  [33]  model, ResGCN  [34]  was proposed to address the over-smoothing of GNNs. ResGCN and most of the extant others, however, concatenate the output of each layer, which potentially limit the expressiveness of the model. Transformer has seen success in the fields of computer vision  [35] ,  [36] , natural language processing  [37] , and speech recognition  [38]  in recent years, and its network structure is regarded as excellent. As depicted in Fig.  3 , we design a new GNN layer called GAT-MLP based on the ideas of ResNet and Transformer. The GAT-MLP layer can be formulated as:\n\nwhere X in (X out ) denotes the input (output) matrix of node features; E denotes the set of edge; MultiGAT is the multihead graph attention network; Θ gat and Θ fed are the trainable parameters; FeedForward and Norm are the feedforward and normalization functions, respectively. The layer normalization function is used as Norm in this work. The feedforward function is computed as follows:\n\nwhere Drop and Lin are the dropout and linear functions, respectively; σ is the non-linear activation function (e.g., Relu); Θ 0 and Θ 1 are the trainable parameters. The MultiGAT is designed as follows:\n\nwhere\n\nwhere SingleGAT in this paper will be described in detail in Section III-C5.\n\nIf the Norm operation is placed before MultiGAT and FeedForward, then it can be modified as follows:\n\nWe argue intuitively that unlike the textual and acoustic tasks which rely on sequence-level context modeling, the visual task relies more on the features directly expressed in the current image. A model that is capable of both sequencelevel context modeling and feature-level modeling is desired for the multimodal feature fusion. The MultiGAT sublayer in GAT-MLP can capture sequence-level contextual information, while the FeedForward sublayer compensates for the failure to efficiently capture feature-level information. Therefore, the combination of the MultiGAT and FeedForward in the GAT-MLP layer can mutually compensate for encoding disparities of various modalities in the multimodal task.\n\n4) GAT-MLP Based PairCC: If the features of multiple modalities are concatenated together directly, then it will not only be challenging to fuse due to heterogeneity gap, but it will also neglect cross-modal interactive information. What's worse, the larger the number of modalities is, the more serious the problem of heterogeneity gap between modalities is. Therefore, we propose the strategy of GAT-MLP based Pair-wise Cross-modal Complementation (PairCC) for crossmodal feature interaction and minimizing the heterogeneity gap. The process of GAT-MLP based PairCC is shown in Fig.  2 , which mainly consists of GAT-MLP and concatenation layer. Specifically, we first feed the feature matrices of visual and acoustic modality into GAT-MLP layer for intra-modal context and inter-modal interaction encoding, and concatenate the visual and acoustic encoding results to obtain the v-a (visual-acoustic) feature matrix H va ; then we treat H va as the feature matrix of a new modality, and perform the same encoding operation between H va and the textual feature matrix to obtain the v-a-t (visual-acoustic-textual) feature matrix H vat ; finally, H vat and the feature matrix of shared subspace are encoded similarly to obtain the final feature matrix. The above steps can be formulated simply as follows:\n\nwhere H ′ is the final output of feature matrix; E va denotes the edge set consisting of E v intra , E a intra , and E va inter ; E vat and E ′ are also the edge sets that are created similarly to E va ; Θ va sep , Θ vat sep and Θ ′ are the trainable parameters, respectively; PairCC indicates GAT-MLP based PairCC function.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "5)",
      "text": "SingleGAT: We will present the graph attention network of this paper in this part. Graph Neural Networks (GNNs) typically involve two processes: aggregating information with the aggregation function and updating state with the combination function. Following that, we'll describe our SingleGAT (single-head graph attention) sublayer in terms of the aggrega-tion function AGG and combination function COM. The two processes can be formalized as follows:\n\nx agg,i = AGG({x j |w j ∈ N (w i )}; Θ agg ),\n\nwhere AGG and COM are the aggregation and combination functions, respectively; x i ∈ X denotes the feature vector of node w i , and w i ∈ V; x j is the feature vector of w i 's neighbor w j ; Θ agg and Θ com denote the learnable parameters. Aggregation: It is well known that computing the importance of neighbor information is crucial when GNNs aggregate information. So we utilize the attention mechanism to implement the aggregation function AGG. The output of aggregation is expressed as follows:\n\nwhere α ij is attention coefficient, as well as the edge weight between node w i and w j ; w j is the neighboring node of w i ; x j denotes the feature vector of w j , and x j ∈ X; W agg denotes the learnable parameter.\n\nGATv2 based edge weights: We use the attention module of GATv2  [39]  to learn edge weights for characterizing the relevance of diverse neighbor information. We define the attention coefficient α ij as follows:\n\nwhere σ denotes the non-linear activation function, such as LeakyReLU; ∥ denotes the concatenation operation; Θ att is the learnable parameter; x i is the feature representation of the current node w i ; both x j and x k are the representations of neighboring node of w i . In this work, the neighboring node is either an intra-modal contextual node or an inter-modal interactive node of w i .\n\nEmbedding of edge types: We assume that different types of edge/relation involve different implicit dependency information in the dialogue. Here are two conjectures: ① Suppose w j is an intra-modal contextual neighbor node of w i (w j is a long-distance contextual node). w j and w i may have similar semantics when they are uttered by the same speaker. At this moment, w j is more critical relative to others that have different speakers from w i . ② Suppose w j is the inter-modal interactive neighbor node of w i . When the semantics of w i does not match the ground-truth emotion label, w j can be semantically complementary to w i . w j is more important at this time relative to other neighbor nodes. Therefore, we encode the edge types as vector representations, and put them into the attention module to aid in the computation of the attention coefficient. We consider that the edge weight is affected not only by the nodes, but also by the edge types. The embedding of edge types, i.e., feature of edge type, can be formalized as follows:\n\nwhere ET = ET intra ∪ ET inter denotes the set of edge types, and DM is the number of edge types in a dialogue with D speakers and M modalities. The attention coefficient with the addition of edge feature is computed as follows:\n\nwhere et ij ∈ ET emb denotes the edge feature of between utterance node w i and w j .\n\nCombination: The combination function COM combines x agg,i with x i . We employ GRU as the combination function, which is inspired by GraphSage  [30]  but different from it. The output of the graph attention is expressed as follows:\n\nwhere x f wd com,i , x i and x agg,i are the output, input and hidden state of GRU, respectively; Θ f wd com is the trainable parameter. The neighbor information x agg,i (including intra-modal contextual information and inter-modal interactive information) is employed as the hidden state of GRU, and it may not be completely exploited. Therefore, we reverse the order of x i and x agg,i , i.e., x agg,i and x i are respectively utilized as the input and hidden state of GRU:\n\nwhere x rev com,i is the output, input and hidden state of GRU, and Θ rev com is the trainable parameter. The final output of the single-head graph attention SingleGAT as follows:\n\nBy calculating the average of multiple single-head graph attentions, we can obtain the following result:\n\nwhere x gat,i ∈ X gat is the output of the multi-head graph attention network, and K denotes the number of heads.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Multimodal Emotion Classifier",
      "text": "After encoding with the GAT-MLP based PairCC, the feature vector h i ∈ H ′ of utterance u i can be obtained. It is then fed to the fully connected layer to predict the emotion label ŷi for the utterance u i :\n\nWe employ cross-entropy loss along with L2-regularization as classification loss function to train the model:\n\nwhere N is the number of dialogues, n(i) is the number of utterances in dialogue i; y ij denotes the ground truth label of the j-th utterance in the i-th dialogue, p ij denotes the probability distribution of predicted emotion label of the j-th utterance in the i-th dialogue; λ is the L2-regularizer weight, and Θ re is the trainable parameter. Finally, combining the shared subspace loss L shr , separate subspace losses L ζ sep (ζ ∈ {a, v, t}) and classification loss L cls together, the final objective function is computed as:\n\nwhere β, γ a , γ v , γ t are the trade-off parameters.\n\nIV. EXPERIMENT A. Datasets and Evaluation Metrics 1) Datasets: We evaluate our GraphCFC model on two multimodal benchmark datasets: IEMOCAP  [40]  and MELD  [18] , which are subjected to raw utterance-level feature extraction according to MMGCN  [5] . The statistics of them are shown in TABLE  I . IEMOCAP is a multimodal dataset of two-way conversations from ten professional actors. It contains 151 conversations, a total of 7433 dyadic utterances. Emotion labels of IEMOCAP include Neutral, Happy, Sad, Angry, Frustrated and Excited. As in previous works  [2] ,  [5] , we utilize the first 80% of the data as the training set and the remaining data as the test set, with the 10% of the training set used as the validation set. IEMOCAP is one of the most popular datasets in ERC task, with high quality and multimodal information.\n\nMELD is a multimodal dataset, containing videos of multiparty conversations from Friends TV series. It involvs over 1433 conversations, a total of 13708 utterances by 304 speakers. Distinct from IEMOCAP, each conversation in MELD includes three or more speakers. Emotion labels include Anger, Disgust, Sadness, Joy, Neutral, Surprise and Fear. The conversations in this dataset involve many backgrounds knowledge, which makes it challenging to recognize the right emotion.\n\n2) Metrics: Following the previous methods  [2] ,  [5] , we chose weighted-average F1 score as the evaluation metric due to the class imbalanced problem. F1 score is reported for each class to allow for a more comprehensive comparison with the baselines. We also record the average accuracy score in addition to the weighted-average F1 score.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Baselines",
      "text": "To verify the effectiveness of our proposed GraphCFC model, we compare it with several previous baselines. The baselines include bc-LSTM  [11] , CMN  [12] , ICON  [13] , DialogueRNN  [3] , DialogueGCN  [2] , DialogueCRN  [4]  and MMGCN  [5] . The details of these models are listed as follows.\n\nbc-LSTM encodes context-aware information through a bidirectional LSTM network, but without taking speakerrelated information into account. CMN models utterance context through speaker-dependency GRUs, but it can only work when the conversation includes two speakers. ICON has improved CMN by modeling distinct speakers. A global GRU is utilized to model the variance of emotion status in a conversation. Nevertheless, ICON still can't be applied in scenario with more than two speakers. DialogueRNN leverages three GRUs to model information of speakers and sequence in conversations, which contain Global GRU, Speaker GRU and Emotion GRU. The goals of three GRUs are to extract context information, model identity information of the speakers and detect emotion of utterances, respectively. DialogueGCN focuses on the function of GCN, i.e., aggregating neighbor information, to improve the performance of ERC tasks. We extend DialogueGCN by directly concatenating features of each modality to implement multimodal setting. DialogueCRN extracts and integrates emotional clues by devising multi-turn reasoning modules to sufficiently model the situation-level and speaker-level context in a conversation. In order to achieve multimodal setting, we concatenate features of three modalities simply. MMGCN adopts a graph-based approach for multimodal feature fusion. MMGCN is currently significantly superior to most baselines for multimodal ERC, which provides a new idea for multimodal fusion.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Implementation Details",
      "text": "We implement the GraphCFC model through the PyTorch framework, and all experiments are executed on NVIDIA Tesla A100. The optimizer is AdamW, the L2 regularization parameter is 0.00001, and the Dropout rate is 0.1. For IEMOCAP dataset, the number of GAT-MLP layers is 5, the learning rate is 0.00001, the ratio of the speaker embedding µ is 1.0, and the batch size is 8. For MELD dataset, the number of GAT-MLP layers is 3, the learning rate is 0.00001, the ratio of the speaker embedding µ is 0.7, and the batch size is 32. We utilize the method proposed by Kendall et al.  [41]  to set the trade-off parameters (β, γ a , γ v and γ t ) of multiple loss functions as learnable parameters instead of setting them manually.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "In this section, we report and discuss the results of all comparative experiments and ablation studies. In addition, we provide three case studies on the IEMOCAP dataset at the end of this section.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Overall Performance",
      "text": "We compare our proposed GraphCFC with the baseline models on the IEMOCAP and MELD datasets. The overall performance of all models is shown in TABLE  II . Based on the experimental findings, we can see that the accuracy and weighted-average F1 score of our proposed model is 3.57% and 3.20% higher than those of the best baseline model (i.e., MMGCN) on the IEMOCAP dataset. The GraphCFC model achieves higher F1 scores than MMGCN in the most emotions when each emotion is observed separately. The F1 scores of Sad and Excited are respectively 84.99% and 78.86% on the IEMOCAP dataset, which are higher than those of other emotions. For Sad and Excited emotions, the F1 scores of GraphCFC are far higher than those of MMGCN, which are 7.46% and 6.82% higher than those of MMGCN, respectively. Overall, the GraphCFC model outperforms the others in terms of accuracy and weighted-average F1 scores. Therefore, we can conclude that our method can more adequately extract long-distance intra-modal contextual information and intermodal interactive information in comparison to the baselines such as MMGCN.\n\nWe note that while DialogueCRN can achieve excellent performance in uni-modal setting  [4] , direct concatenation of the results from multiple modalities is not as effective. One probable reason is that direct concatenation generates redundant information and fails to capture the inter-modal interactive information. GraphCFC, in contrast, extracts the interactive information through a GNN-based approach while also reducing redundant information, resulting in superior performances.\n\nAlthough GraphCFC model outperforms other approaches on the MELD dataset, its improvement was not very significant. It can be observed that the improvements in accuracy and weighted-average F1 scores of GraphCFC are 2.11% and 1.04%, respectively, relative to those of MMGCN. The reason for these results may be that the utterance sequences of a dialogue on the MELD dataset aren't from a continuous conversation in real scenes. Therefore, the graph-based models do not take advantage of their ability to capture contextual information. Another potential reason is that the MELD dataset contains a lot of background noise that is unrelated to emotion due to the camera setup and recording conditions. In addition, we observe that the F1 score of the Sadness class is low in the results of the MELD dataset. By looking at the distribution of classes in the MELD dataset, we find that the dataset suffers from the class-imbalanced problem. And the Sadness class belongs to the minority class, thus resulting in its low F1 score.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Comparison Under Various Modality Settings",
      "text": "TABLE  III  shows the performance of GraphCFC model in different modality settings. Overall, the performance of multimodal settings outperforms that of uni-modal settings. The textual modality has the best performance in the unimodal settings, whereas the visual modality has the lowest result. One probable explanation for the poor result is that the visual modality contains a lot of noise due to the effects of camera position, background, lighting, etc. The combination of textual and acoustic modalities produces the best performance in the two-modal settings, whereas the combination of visual and acoustic modalities produces the worst result. As expected, the fusion of acoustic, visual and textual modalities can improve the performance of GraphCFC.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Effect Of Various Components In Gat-Mlp Layer",
      "text": "We report the effects of MultiGAT and FeedForward in the GAT-MLP layer in TABLE IV. The performance of our model is noticeably degraded when the MultiGAT or FeedForward sublayer is not adopted. When the MultiGAT sublayer is not    utilized on the IEMOCAP dataset, the accuracy and weightedaverage F1 scores of GraphCFC decrease by 4.93% and 4.36%, respectively. The accuracy and F1 scores respectively decrease by 2.09% and 1.67% when the FeedForward sublayer is not applied. As a result, we can deduce that the effect of MultiGAT in the GAT-MLP layer is more significant than that of FeedForward. The effects of different numbers of GAT-MLP layer and skip connection on the GraphCFC model are shown in Fig.  4 . We can see that if we remove the skip connection, the performance of the model will drop sharply on the IEMOCAP dataset as the number of GAT-MLP layer increases when a certain threshold is exceeded. On the contrary, if we keep the skip connection, the performance of the proposed GraphCFC decreases slowly. Therefore, skip connection can help to mitigate the problem of over-smoothing to a certain extent.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Effect Of Multi-Subspace Loss",
      "text": "The impacts of the multi-subspace loss functions are seen in TABLE V. After eliminating the shared subspace loss or separate subspace loss, both accuracy and weighted-average F1 scores decline, as seen in the TABLE V. The experimental results suggest that setting the loss function in the multisubspace extractor can effectively improve the performance of our GraphCFC.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Effect Of Multi-Speaker And Edge Types",
      "text": "The influence of speakers and edge types on our GraphCFC model is seen in TABLE VI. The performance of GraphCFC will be compromised if the embedding of multi-speaker or edge types is not employed. The weighted-average F1 score drops to 65.91% on the IEMOCAP dataset when the embedding of edge types is not utilized, which amply proves our hypothesis that edge types affects the relevance of neighbor information. We note that without adding speaker information, the results of GraphCFC show only a slight degradation, which is still higher than the results of baseline models.\n\nThe phenomenon demonstrates that GrpahCFC is not heavily dependent on speaker and has a certain degree of generalization capability. Generally speaking, the performance of our proposed method can be improved by adding the embeddings of multi-speaker and edge types.\n\nF. Effect of the Past j and Future k Utterance Nodes\n\nAs shown in Fig.  5 , we discuss the effect of past j nodes and future k nodes on our proposed GraphCFC model. We set j and k to multiple combinations (the combination can be denoted as (j, k)), such as (0, 0), (2, 2), (4, 4),  (6, 6) , ...,  (40, 40) . From Fig.  5 , it can be concluded that the accuracy and weighted-average F1 scores increase on the IEMOCAP dataset with increasing values of j and k. When a certain threshold combination (i.e., (j, k) =  (18, 18) ) are reached, however, the accuracy and F1 scores gradually decrease. In particular, GraphCFC performs worst when the conversational context is not available (i.e., setting both j and k set to 0). Therefore, we can draw the conclusion that the conversational context is a crucial parameter for the proposed method.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "G. Overall Performance Of Three-Emotion",
      "text": "In this part, we conduct comparative experiments of the three-emotion. Prior to model training, we merge the original emotion labels into three categories (i.e., Positive, Neutral, and Negative), while the proposed GraphCFC is transformed into a three-classification model. Specifically, TABLE VII shows the statistics of the merged emotion labels. The experimental results of our three-emotion are recorded in TABLE VIII. We can find that the experimental results are similar to those of the previous experiments with six or sevenemotion, with improved accuracy and weighted-average F1 scores for all models. It can be seen that the accuracy and F1 scores of GraphCFC improve 0.61% and 1.20% relative to those of MMGCN on the MELD dataset, respectively. Similarly, there are slight improvements of accuracy and F1 scores on the IEMOCAP dataset. It may be attributable to the fact that the emotion labels are coarsened (similar emotions like Excited and Happy are merged) after converting the dataset into three-emotion labels, so most of the models are capable of performing the task of emotion classification easily.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "H. Case Studies",
      "text": "As shown in Fig.  6 , we conduct several case studies in this part. In text-modal ERC models such as DialogueGCN and DialogueCRN, several utterances with non-Neutral emotion labels, such as \"okay.\", \"yeah.\" or \"What's the meaning of that?\", are usually recognized as Neutral directly. In contrast, multimodal ERC models such as GraphCFC make integrated judgments based on multiple modalities, which, for example, may eventually be recognized as Sad. Therefore, visual and acoustic modalities can compensate for such lapses. Fig.  6a  depicts the above-mentioned case on the IEMOCAP dataset.\n\nFig.  6b  and Fig.  6c  show that the cases of emotional-shift on the IEMOCAP dataset. In Fig.  6b , when a speaker's emotion is Neutral for several preceding consecutive utterances, most of the models (e.g., MMGCN) tend to identify the speaker's next utterance as Neutral. In Fig.  6c , when a speaker's emotion was Neutral for several consecutive utterances, the majority of models trend towards recognizing the next utterance spoken by another speaker as Neutral. Unlike approaches such as MMGCN, our proposed GraphCFC can accurately identify the emotion of utterance as Excited in the above two cases.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose a directed Graph based Crossmodal Feature Complementation (GraphCFC) method for reducing the multimodal heterogeneity gap and compensating the inadequacies of earlier SOTA methods such as MMGCN. Concretely, we model the multimodal dialogue as a directed graph with variable context and extract distinct types of edges from the graph for graph attention learning, thus ensuring that GNNs can select accurately critical intra-modal contextual and",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows an instance of",
      "page": 1
    },
    {
      "caption": "Figure 1: An instance of a multimodal dialogue system. The utterances contain",
      "page": 2
    },
    {
      "caption": "Figure 2: The illustration of graph-based multimodal ERC, which includes uni-modal encoding, graph based cross-modal feature complementation and multimodal",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the overall archi-",
      "page": 4
    },
    {
      "caption": "Figure 2: , and are formulated as follows:",
      "page": 5
    },
    {
      "caption": "Figure 3: The structure of the designed GAT-MLP, where the Norm operation can be placed before MultiGAT and FeedForward. MultiGAT denotes the",
      "page": 6
    },
    {
      "caption": "Figure 3: , we design a new",
      "page": 6
    },
    {
      "caption": "Figure 2: , which mainly consists of GAT-MLP and concatenation",
      "page": 7
    },
    {
      "caption": "Figure 4: The effects of the number of GAT-MLP layer and skip connection on",
      "page": 10
    },
    {
      "caption": "Figure 5: , we discuss the effect of past j nodes",
      "page": 11
    },
    {
      "caption": "Figure 5: , it can be concluded that the accuracy",
      "page": 11
    },
    {
      "caption": "Figure 5: The effects of j nodes in the past and k nodes in the future on the",
      "page": 11
    },
    {
      "caption": "Figure 6: , we conduct several case studies in this",
      "page": 11
    },
    {
      "caption": "Figure 6: b and Fig. 6c show that the cases of emotional-shift on",
      "page": 11
    },
    {
      "caption": "Figure 6: b, when a speaker’s emotion",
      "page": 11
    },
    {
      "caption": "Figure 6: c, when a speaker’s emotion",
      "page": 11
    },
    {
      "caption": "Figure 6: The cases of ERC on the IEMOCAP. (a) An example shows that multi-modality can be used to compensate for the shortcoming of single-textual",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated",
          "MELD": "Neutral\nSurprise\nSadness\nJoy\nAnger"
        },
        {
          "Model": "bc-LSTM\nCMN\nICON\nDialogueRNN\nDialogueCRN\nDialogueGCN\nMMGCN",
          "IEMOCAP": "32.63\n70.34\n51.14\n63.44\n67.91\n61.06\n30.38\n62.41\n52.39\n59.83\n60.25\n60.69\n29.91\n64.57\n57.38\n63.04\n63.42\n60.81\n33.18\n78.80\n59.21\n65.28\n71.86\n58.91\n51.59\n74.54\n62.38\n67.25\n73.96\n59.97\n47.10\n80.88\n58.71\n66.08\n70.97\n61.21\n64.12\n45.45\n77.53\n61.99\n66.67\n72.04",
          "MELD": "75.66\n48.47\n22.06\n52.10\n44.39\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n76.79\n47.69\n20.41\n50.92\n45.52\n76.13\n46.55\n11.43\n49.47\n44.92\n75.97\n46.05\n19.60\n51.20\n40.83\n54.41\n75.16\n48.45\n25.71\n45.45"
        },
        {
          "Model": "GraphCFC",
          "IEMOCAP": "84.99\n64.70\n71.35\n78.86\n43.08\n63.70",
          "MELD": "76.98\n49.36\n26.89\n47.59\n51.88"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality Setting": "",
          "IEMOCAP": "Accuracy\nwa-F1",
          "MELD": "Accuracy\nwa-F1"
        },
        {
          "Modality Setting": "A\nV\nT",
          "IEMOCAP": "54.16\n53.85\n31.61\n27.67\n59.95\n60.09",
          "MELD": "47.55\n41.62\n47.59\n33.26\n60.77\n56.81"
        },
        {
          "Modality Setting": "A + V\nA + T\nV + T",
          "IEMOCAP": "54.17\n53.89\n64.20\n64.74\n63.15\n62.96",
          "MELD": "47.61\n41.67\n59.96\n57.46\n59.46\n57.29"
        },
        {
          "Modality Setting": "A + V + T",
          "IEMOCAP": "69.13\n68.91",
          "MELD": "61.42\n58.86"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lshr": "",
          "Lζ\nsep": "",
          "IEMOCAP": "Accuracy\nwa-F1",
          "MELD": "Accuracy\nwa-F1"
        },
        {
          "Lshr": "- w/o\n- w\n- w/o",
          "Lζ\nsep": "- w\n- w/o\n- w/o",
          "IEMOCAP": "68.70\n68.35\n67.53\n67.56\n68.70\n68.36",
          "MELD": "61.00\n58.39\n60.27\n57.99\n60.38\n58.09"
        },
        {
          "Lshr": "- w",
          "Lζ\nsep": "- w",
          "IEMOCAP": "69.13\n68.91",
          "MELD": "61.42\n58.86"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "IEMOCAP": "Positive\nNeutral\nNegative",
          "MELD": "Positive\nNeutral\nNegative"
        },
        {
          "Model": "bc-LSTM\nDialogueRNN\nDialogueCRN\nDialogueGCN\nMMGCN",
          "IEMOCAP": "90.58\n55.63\n84.04\n88.36\n57.99\n83.81\n79.39\n61.51\n83.09\n84.22\n56.88\n83.66\n64.21\n85.20\n83.73",
          "MELD": "36.97\n75.12\n61.46\n40.29\n74.95\n62.10\n40.80\n74.40\n62.87\n75.64\n32.92\n63.96\n43.32\n75.5\n65.57"
        },
        {
          "Model": "GraphCFC",
          "IEMOCAP": "84.35\n88.48\n62.03",
          "MELD": "50.66\n66.26\n75.12"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The social effects of emotions",
      "authors": [
        "G Van Kleef",
        "S Côté"
      ],
      "year": "2022",
      "venue": "Annual Review of Psychology"
    },
    {
      "citation_id": "2",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "3",
      "title": "DialogueRNN: An attentive RNN for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "DialogueCRN: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "5",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Speech landmark bigrams for depression detection from naturalistic smartphone speech",
      "authors": [
        "Z Huang",
        "J Epps",
        "D Joachim"
      ],
      "year": "2019",
      "venue": "Proceedings of ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "SemEval-2019 task 3: EmoContext contextual emotion detection in text",
      "authors": [
        "A Chatterjee",
        "K Narahari",
        "M Joshi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 13th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "8",
      "title": "Mining dual emotion for fake news detection",
      "authors": [
        "X Zhang",
        "J Cao",
        "X Li",
        "Q Sheng",
        "L Zhong",
        "K Shu"
      ],
      "year": "2021",
      "venue": "Proceedings of the Web Conference 2021, ser. WWW '21"
    },
    {
      "citation_id": "9",
      "title": "Automatic dialogue generation with expressed emotions",
      "authors": [
        "C Huang",
        "O Zaiane",
        "A Trabelsi",
        "N Dziri"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "10",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "13",
      "title": "ICON: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Deep multimodal representation learning: A survey",
      "authors": [
        "W Guo",
        "J Wang",
        "S Wang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "15",
      "title": "Misa: Modality-invariant and -specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "Deeper insights into graph convolutional networks for semi-supervised learning",
      "authors": [
        "Q Li",
        "Z Han",
        "X Wu"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "MIME: MIMicking emotions for empathetic response generation",
      "authors": [
        "N Majumder",
        "P Hong",
        "S Peng",
        "J Lu",
        "D Ghosal",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "18",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "19",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization"
    },
    {
      "citation_id": "21",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "22",
      "title": "HiGRU: Hierarchical gated recurrent units for utterance-level emotion recognition",
      "authors": [
        "W Jiao",
        "H Yang",
        "I King",
        "M Lyu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "23",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "24",
      "title": "Multimodal relational tensor network for sentiment and emotion classification",
      "authors": [
        "S Sahay",
        "S Kumar",
        "R Xia",
        "J Huang",
        "L Nachman"
      ],
      "year": "2018",
      "venue": "Multimodal relational tensor network for sentiment and emotion classification",
      "arxiv": "arXiv:1806.02923"
    },
    {
      "citation_id": "25",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Learning factorized multimodal representations"
    },
    {
      "citation_id": "26",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Z Sun",
        "P Sarma",
        "W Sethares",
        "Y Liang"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrusaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "29",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "J Kory"
      ],
      "year": "2015",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "30",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "31",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Liò",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Proceedings of International Conference on Learning Representations"
    },
    {
      "citation_id": "32",
      "title": "FastGCN: Fast learning with graph convolutional networks via importance sampling",
      "authors": [
        "J Chen",
        "T Ma",
        "C Xiao"
      ],
      "year": "2018",
      "venue": "Proceedings of International Conference on Learning Representations"
    },
    {
      "citation_id": "33",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "34",
      "title": "DeepGCNs: Making GCNs go as deep as CNNs",
      "authors": [
        "G Li",
        "M Mueller",
        "G Qian",
        "I Perez",
        "A Abualshour",
        "A Thabet",
        "B Ghanem"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "Proceedings of International Conference on Learning Representations"
    },
    {
      "citation_id": "36",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of 2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "37",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Transformer transducer: A streamable speech recognition model with transformer encoders and RNN-t loss",
      "authors": [
        "Q Zhang",
        "H Lu",
        "H Sak",
        "A Tripathi",
        "E Mcdermott",
        "S Koo",
        "S Kumar"
      ],
      "year": "2020",
      "venue": "Proceedings of 2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "How attentive are graph attention networks",
      "authors": [
        "S Brody",
        "U Alon",
        "E Yahav"
      ],
      "year": "2022",
      "venue": "Proceedings of International Conference on Learning Representations"
    },
    {
      "citation_id": "40",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "41",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "R Cipolla",
        "Y Gal",
        "A Kendall"
      ],
      "year": "2018",
      "venue": "Proceedings of 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}