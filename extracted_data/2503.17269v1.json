{
  "paper_id": "2503.17269v1",
  "title": "Recovering Pulse Waves From Video Using Deep Unrolling And Deep Equilibrium Models",
  "published": "2025-03-21T16:11:21Z",
  "authors": [
    "Vineet R Shenoy",
    "Suhas Lohit",
    "Hassan Mansour",
    "Rama Chellappa",
    "Tim K. Marks"
  ],
  "keywords": [
    "Heart Rate Estimation",
    "fixed-point methods",
    "algorithm unrolling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Camera-based monitoring of vital signs, also known as imaging photoplethysmography (iPPG), has seen applications in driver-monitoring, perfusion assessment in surgical settings, affective computing, and more. iPPG involves sensing the underlying cardiac pulse from video of the skin and estimating vital signs such as the heart rate or a full pulse waveform. Some previous iPPG methods impose model-based sparse priors on the pulse signals and use iterative optimization for pulse wave recovery, while others use end-to-end black-box deep learning methods. In contrast, we introduce methods that combine signal processing and deep learning methods in an inverse problem framework. Our methods estimate the underlying pulse signal and heart rate from facial video by learning deep-network-based denoising operators that leverage deep algorithm unfolding and deep equilibrium models. Experiments show that our methods can denoise an acquired signal from the face and infer the correct underlying pulse rate, achieving state-of-the-art heart rate estimation performance on well-known benchmarks, all with less than one-fifth the number of learnable parameters as the closest competing method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Physiological measurements using cameras  [1] ,  [2]  have emerged as an exciting research topic, with many recent techniques focusing on imaging photoplethysmography (iPPG) for heart rate estimation. In this task, given a camera recording of the skin (e.g., the face), the pulse wave is reconstructed and subsequently heart rate is determined-all without contact with the skin. The digital blood volume pulse signal, manifested as reflections of incident light on pulsatile blood flow, is weak in the presence of noise signals, and the main challenge is to separate the signal from the motion, lighting, and quantization noise. Pixel intensity changes due to the pulsatile signal are weak compared to those due to noise sources such as motion, lighting variations, and camera sensor quantization. The main challenge is to separate the signal from the noise caused by these and other factors. Developing robust solutions to iPPG waveform recovery could improve monitoring of sick patients  [3] , monitor drivers in automotive applications  [4] , and improve security in face recognition  [5] .\n\nEarly methods to solve this problem were modular and based on signal processing techniques; these methods cropped the face and defined one or more regions of interest, followed by a signal extraction technique. These include blind source separation methods  [6] ,  [7]  and later improvements used chrominance information  [8]  and skin reflection properties  [9] . Another class of model-driven methods was based on sparse optimization  [4] ,  [10] ; the authors assumed that the pulse signal was composed of a sparse set of frequency coefficients and that the noise was present in a sparse set of facial regions. The authors then solved for a pulse vector that is sparse in the frequency domain. While these hand-crafted signal-processing methods set a strong foundation for future work, they could not learn signal representations from data to improve pulse rate estimation, especially under challenging recording scenarios.\n\nAlmost all recent data-driven iPPG methods are based on deep learning. After initial work  [11]  modeled the nonlinear interactions between the pulse signal and noise, deep learning methods were further refined with temporal shift models  [12] , spatio-temporal models  [13] ,  [14] , and attention  [15] . Other deep neural network methods use meta-learning  [16] ,  [17] , efficient computation  [18] , and federated learning  [19] . With recent interest in learning without labels, unsupervised/selfsupervised methods such as  [20] -  [22]  have used contrastive learning and masked autoencoders to update network parameters without a supervisory signal.\n\nCompared to earlier iPPG methods, the deep-learning-based methods have a disadvantage in that they fundamentally lack interpretability. In this paper, we show that integrating both signal processing and learning-based techniques not only achieves strong iPPG performance, but also puts deep learning techniques on sound signal-processing foundations.\n\nTo this end, our work expands on the optimization-based signal recovery algorithms while integrating lightweight deep learning components that play specialized roles for pulse signal recovery. Similar to  [4] , we model pulse signal recovery as an optimization problem that minimizes a data fidelity term and a prior-promoting penalty term, and solve the resulting optimization problem using ISTA-style  [23]  forwardbackward splitting algorithms. However, using hand-crafted priors and their corresponding proximal operators does not always capture the complex and nonlinear interactions between frequencies in the pulse signal or the noise that is present in the environment. Therefore, we impose implicit priors on the heartbeat signal and the noise by training deep neural networks that function as denoising operators. Our approach therefore maintains the algorithmic structure of optimizationbased methods while benefiting from data-driven deep learning.\n\nWe apply our insights to develop three different algorithms. First, we present Unrolled iPPG, which learns the denois- ing operators of the signal and noise priors by \"unrolling\" proximal gradient descent  [24]  for a predetermined number of iterations, applying a single forward pass through the structure-promoting operators of each of the signal and noise at each iteration. A potential drawback of Unrolled iPPG is that it applies only a single denoising step in each of the unrolled iterations, and the number of iterations must be decided in advance. This limitation can be overcome using fixed-point theory and deep equilibrium (DEQ) models  [25] , which use fixed-point solvers to effectively iterate the unrolling steps until the output converges. Incorporating these ideas, we develop Deep Equilibrium-Proximal iPPG (DE-Prox-iPPG), which iterates both the gradient update and learned proximal/denoising steps through a solver until a fixed-point is reached  [26] .\n\nOur experiments show that this method, which makes fixedpoint assumptions on both the signal and noise, has its own weaknesses. Given that the pulsatile signal is a realization of the quasi-periodic nature of the beating heart, we assume it is sufficiently structured to be represented as a fixed point of some operator. However, the noise signal, which includes noise due to numerous sources including motion and illumination variations, may not admit a fixed-point representation. Consequently, we introduce Unrolled-DEQ-iPPG (UDEQ-iPPG).\n\nIn each unrolling iteration, Unrolled-DEQ-iPPG effectively applies the denoising operator on the pulse signal until a fixedpoint is achieved, but applies the operator on the noise only once through a single forward pass. We show that UDEQ-iPPG exceeds performance of both unrolling and fixed-point algorithms by integrating both techniques.\n\nExtensive experiments demonstrate that our models accurately estimate the underlying pulse signal from facial video, achieving new state-of-the-art heart rate estimation results on three publicly available datasets, and even generalizing to unseen datasets at test time.\n\nTo summarize, our main contributions are as follows:\n\n• We model iPPG pulse signal recovery and heart rate estimation as an optimization problem that minimizes the sum of a data fidelity term and an implicit structurepromoting prior. We then learn the denoising neural network operator corresponding to the implicit prior. • We present the first unrolling techniques for iPPG signal recovery. Using the ideas of proximal gradient descent, we alternate between gradient descent and neural network denoising on both the signal and noise to recover the pulsatile signal. This algorithm is called Unrolled iPPG. • We are the first to propose iPPG signal recovery through fixed-point methods and to incorporate deep equilibrium models into algorithms for pulse signal recovery and heart rate estimation. We do so by encapsulating both the gradient and denoising steps in an iteration map that runs until a fixed-point is obtained for both the pulse and noise signals and call this DE-Prox-iPPG. • We further propose UDEQ-iPPG, which integrates DE-Prox-iPPG and Unrolled iPPG. Assuming that the pulse signal can be represented as the fixed-point of a learned denoising operator, while the noise admits no such fixedpoint representation, we apply the denoiser on the pulse signal until fixed-point convergence, but we apply the denoiser on the noise a single time at each iteration in an unrolling framework. • We achieve state-of-the-art heart-rate estimation results on three publicly available datasets, while using less than 35% of the number of parameters of competing deep neural network methods.\n\nA preliminary version of Unrolled iPPG appeared in  [27] . We expand upon this work by: 1) Incorporating fixed-point methods for signal recovery (DE-Prox-iPPG); 2) showing that a combination of fixed-point recovery algorithms and traditional single-pass feed-forward neural networks effectively models the signal and noise (UDEQ-iPPG); 3) testing all three methods on two additional datasets; 4) performing crossdataset model evaluations; and 5) providing a more thorough empirical and statistical analysis of all of our methods.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work A. Imaging Ppg And Physiological Analysis",
      "text": "Pulse rate estimation algorithms can be broadly categorized into signal-processing-based (or model-based) algorithms and deep learning algorithms. Blind source separation (BSS) methods  [6] ,  [7] ,  [28] , a subset of the signal processing methods, assume that the intensity signal extracted from video is the weighted sum of a pulse signal and other source or error signals. By assuming the source signals are statistically independent,  [7]  uses independent component analysis (ICA) to extract the pulse signal, while  [6]  demixes signals along directions of maximum variance using principal components analysis (PCA). These methods were further developed to include skin reflection properties. The method of  [9]  does so by assuming all color variation in skin pixels is due to the pulsatile signal, and projects the extracted time series onto this color vector, the resultant signal of which is proportional to the underlying pulse. CHROM  [8]  first implements a skin tone correction, followed by a projection that eliminates specular reflection to extract only the pulse signal. POS  [29] , similar to CHROM, eliminates the specular reflection and performs skin tone correction, but defines a projection plane using a datadriven approach. Other methods have used low-rank  [30]  and sparsity-based methods. The method of  [4]  is most similar to our work; they define the extracted video signal as the sum of the pulse signal and noise, and assume the Fourier coefficients of the pulse signal and the noise signal to be sparse. They then solve a sparse optimization problem for the frequency coefficients and noise using proximal gradient descent. We propose to relax these sparsity assumptions by incorporating a learnable prior, which is realized through deep neural networks.\n\nDeep learning methods  [11] -  [18] ,  [21] ,  [31] -  [38]  show significant performance improvements over model-based approaches. Many of these methods accept RGB frames directly; PhysNet  [13]  inputs RGB frames to spatiotemporal neural networks to reconstruct the pulse signal. DeepPhys  [11]  uses the temporal difference of frames to eliminate the stationary skin reflection color and reconstruct the pulse wave. Both TS-CAN  [12]  and InverseCAN  [15]  extend DeepPhys, with the former including temporal shift modules  [39]  and the latter estimating corruption in the background to remove noise in the foreground and improve signal-to-noise ratio (SNR). Some recent work evaluates algorithms under stricter data constraints such as the few-shot scenario  [16] , while Effi-cientPhys  [18]  develops pulse signal estimation algorithms assuming a resource-constrained environment. Recent work  [20] ,  [21] ,  [33] ,  [40]  has developed neural methods with no supervisory training signal, showing good performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Deep Equilibrium Models",
      "text": "Conventional deep learning models learn by applying explicit functions (such as linear and convolutional operators, followed by nonlinearities) to inputs, building a computation graph along the way, and back-propagating through this graph to update parameters after a loss computation. Implicit models such as Neural ODEs  [41]  and Deep Equilibrium Models  [25]  do not build computation graphs; instead, they solve a nonlinear system prescribed by a criterion such as the endpoint of an ODE flow or the root of an equation, and backpropagate through this point. Thus, they decouple the choice of solver from the computation of the endpoint or root, and backpropagate through this point analytically and independently of the forward pass.\n\nDeep Equilibrium Models (DEQs)  [25]  approximate \"infinitely deep\" models by solving for the fixed point (or equilibrium point) z * of a function: z * = f θ (z * , x), where x is the input. The choice of solver for z * -whether it be Broyden's Method  [42] , Peaceman-Rachford splitting  [43] , Anderson Acceleration  [44] , or any other method-is independent of the backpropagation procedure. Once this point is found, the Implicit Function Theorem  [45]  can be used to compute the gradients of the parameters. These implicit models have shown competitive performance on word-level language modeling  [25]  as well as ImageNet classification and Cityscapes semantic segmentation  [46] . Further developments showed that these models can simultaneously perform forward inference as well as input optimization on problems such as generative modeling, adversarial training, and gradientbased meta-learning  [47] . Recent work has applied DEQs to optical flow estimation  [48] , diffusion models  [49] , facial landmark estimation  [50] , and imaging inverse problems  [26] ,  [51] -  [55] . In addition, significant research has explored the theoretical foundations of DEQs. After  [25]  demonstrated training instabilities in DEQs, inexact gradient methods  [56] ,  [57]  and regularization techniques  [58]  were developed that improved fixed-point convergence, while the work of  [59] ,  [60]  explored interval bound propagation methods to compute lower and upper bounds on the output of a layer. Convergence of DEQs were studied  [61] ,  [62]  using monotone operator theory, while  [63]  looked at self-supervised learning of DEQs from a learning theory perspective.\n\nWe are the first to apply deep equilibrium models to heart rate estimation. We will show that these formulations achieve state-of-the-art results on standard public datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Signal Model",
      "text": "The circulation of blood in the human body can be described as a pump-the heart-that rhythmically applies a force on a branched system of elastic tubes to move blood  [64] . Optical sensors such as those that perform photoplethysmography (PPG) capture blood volume changes by measuring transmission or reflection of illumination that is incident on the skin. While PPG measures this based on contact with the skin, we model the pulse signal based on non-contact imaging: reflected light from the skin that is captured via frames of an RGB camera.\n\nGiven a video segment with S video frames, C color channels, and height and width dimensions H × W pixels, assume that a time series Z ∈ R S×K has been extracted from K facial regions (described in Section V-C), where the set of K time series is assumed to capture a combination of the pulsatile signal and noise:\n\n(1) Fig.  2 . Our unrolling algorithm for both Unrolled iPPG and UDEQ-iPPG. We replace the proximal operators of proximal gradient descent by deep denoisers R θ R and Q θ Q on the frequency coefficients X and noise E, respectively. Unrolled iPPG implements a single forward pass through R θ R and Q θ Q , while UDEQ-iPPG effectively applies R to Xt until the output converges to a fixed-point.\n\nHere, Z ∈ R S×K is a time series of length S frames extracted from the image intensities of K facial regions of the video, F -1 ∈ C S×N is the oversampled inverse Fourier transform matrix, X ∈ C N ×K represents the frequency coefficients of the underlying pulse signal in the K face regions, and E ∈ R S×K is a non-pulsatile noise matrix that captures noise due to multiple sources such as specular reflections, motion, and camera quantization error. We aim to recover the pulse signal X and extract the noise E from the signal Z. We model this as the minimization of the sum of a data fidelity term and regularizing penalty function:\n\nwhere A = [F -1 I]. The data fidelity term, D, ensures consistency of the recovered signal with the measurements, while the regularization term imposes a prior on the signal and noise components. This optimization problem can be solved via proximal gradient descent, which alternates between a gradient update with respect to D and a proximal mapping operation corresponding to the regularizer ρ(X, E), such that,\n\nwhere \"(a)\" defines the gradient step and \"(b)\" defines the proximal step for an iteration t. The proximal operator is defined as\n\nWhen ρ(•, •) is explicitly defined, we may be able to calculate a closed form solution for the proximal operator. In this paper, we instead define the regularization term implicitly, and we present three paradigms that instantiate the proximal operators as denoising neural networks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Algorithms",
      "text": "Unlike previous work  [4] , which explicitly defines the regularizer ρ(X, E) as in (2), we assume that the penalty term is a complicated function that can not be written in closed form. In the framework of proximal gradient descent, ρ(X, E) is not minimized directly, but rather the minimization is realized through its proximal operator as shown in Equation (  4 ). In this paper, we approximate the proximal operator as a deep denoiser, proposing three methods: an unrolling algorithm (Section IV-A), a fixed-point algorithm (Section IV-B), and a combination of unrolling and fixed-point algorithms (Section IV-C).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Unrolling Algorithm: Unrolled Ippg",
      "text": "Previous work  [4]  defined explicit sparsity priors on the frequency coefficients of the pulse signal, making an assumption that the frequency coefficients are independent of each other. However, we hypothesize that sparsity (or joint sparsity) is not sufficiently expressive to characterize the inherent structure of the heartbeat signal. Rather, we explore how the complex, nonlinear combination of the frequency coefficients can be learned from data. Maintaining the algorithmic structure of proximal gradient descent, we \"unroll\"  [24]  the iteration map from Equation (2)\n\nwhere R θ R ( Xt+1 ) and Q θ Q ( Ẽt+1 ) are denoising functions implemented as neural networks in step (b). We graphically show the algorithm in Figure  2 . Similar to proximal gradient descent, the gradient descent step ensures the recovered signal's consistency with the measurements, while the denoising operators impose the learned signal prior. We unroll for T iterations, after which we compute a loss between the predicted signal and ground-truth (during training), or output the estimated pulse signal (during inference).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Fixed-Point Algorithm: De-Prox-Ippg",
      "text": "A potential drawback of Unrolled iPPG is that the gradient and denoising steps are applied for a predetermined number of iterations T , where T is a hyperparameter. It may be possible to apply these steps until convergence instead, obviating the need to choose T . Noting that proximal gradient descent can be viewed as a fixed-point iteration  [26] ,  [65] , we propose Deep Equilibrium Proximal Gradient Descent iPPG (DE-Prox-iPPG). This method, illustrated in Figure  3 , encapsulates the entire iteration map (i.e., gradient descent followed by denoising) in a Deep Equilibrium model (DEQ) and iterates these steps until a fixed point is found for both optimization variables. Compared to unrolling methods, we assume that we can find a joint fixed point for the frequency coefficients X * and noise E * . Therefore, we optimize for the fixed point of the concatenation of the two variables X * and E * , and define a deep equilibrium model f θ (•, •) that iterates through (a) gradient descent steps and (b) denoising operators until convergence:\n\nDE-Prox-iPPG computes the gradient and denoising steps until a fixed point of the global system is found. While Unrolled iPPG unrolls for T iterations, the number of DE-Prox-iPPG iterations varies depending on the input window, as well as on the joint fixed-point of the signal and noise variables.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Unrolling + Fixed-Point Algorithm: Udeq-Ippg",
      "text": "While DE-Prox-iPPG computes the fixed-point iteration for both the pulse signal and noise, we recognize that fixed-point optimization for both signals may be suboptimal. To address this, we propose Unrolled-DEQ-iPPG (UDEQ-iPPG), which is based on the following two assumptions: 1) Given that the pulsatile signal is a realization of the quasi-periodic nature of the beating heart, we assume it is sufficiently structured to be represented as a fixed point of some neural network-based denoising operator; and 2) we assume that the variation in the source of the noise, such as motion or illumination, does not admit a structure that would be appropriately represented as the fixed point of an operator. To that end, we instantiate R in Figure  2  as a DEQ but restrict Q to a single forward pass of a neural network, and we embed both operators in the Unrolled iPPG structure. The corresponding iteration map is summarized as follows:\n\nThe DEQ operator R(•, •) for UDEQ-iPPG is illustrated in Figure  4 . In each unrolling iteration, we input the gradientupdated frequency coefficients Xt+1 to the DEQ to produce the fixed-point variable X * using a fixed-point solver. On the other hand, only a single forward pass through Q occurs for the noise variable Ẽ. The overall procedure is unrolled over T iterations, similar to Unrolled iPPG.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Parameter Updates For Fixed-Point Networks",
      "text": "The fixed point in DE-Prox-iPPG can be found naïvely by repeated application of\n\nis below some tolerance. This method, unfortunately, builds large computation graphs in memory. Equivalently, we can find the root of\n\nSimilarly, for UDEQ-iPPG, we would like to find the root of h(X * , Xt+1 )) = X * -f θ (X * , Xt+1 ) = 0. This technique permits efficient root-solving algorithms such as Broyden's Method  [42]  or Anderson Acceleration  [44]  to solve for the fixed point.\n\nDuring training, we iterate for T iterations for UDEQ-iPPG, or until fixed-point convergence for DE-Prox-iPPG, as shown in Figures  2  and 3 , after which both models' parameters must be updated after computing some loss L. While θ Q (denoting the parameters of Q) in UDEQ-iPPG can be updated via the standard auto-differentiation modules, updating the parameters θ R in UDEQ-iPPG and both θ R , θ Q in DE-Prox-iPPG requires computing gradients with respect to the parameters. While a naïve implementation would backpropagate through iterations of the solver, a memory-efficient backpropagation scheme uses the Function Theorem (see  [25]  for a proof), shown below.\n\nTheorem 1. Implicit Function Theorem (IFT)  [25] ,  [45] ,  [48] . Given the fixed-point representation p * of the optimzation variables, and the corresponding loss L(Z, Z gt ), where Z = F -1 X * , the gradient of the DEQ flow is given by\n\nNote that for UDEQ-iPPG, p * = X * , while for DE-Prox-iPPG it is p * = X * E * . We compute the mean squared error (MSE) Loss between the final output Z = F -1 X * and the ground-truth PPG signal Z gt . Here, X * is X * T in UDEQ-iPPG (i.e. the output of the fixed-point operator at iteration T ) and is X * (the fixed point) in DE-Prox-iPPG. To ensure stability of the solution, we add a regularizing penalty function given by the Frobenius norm of the Jacobian with respect to the fixed-point p * , which is given by J f θ = ∂f θ (p * ) ∂p * , where J f θ ∈ R d×d and d is the length of the vector p *  [56] ,  [58] . To compute ∥J f θ ∥ F = (tr(J f θ J ⊤ f θ )) 1/2 , we use the Hutchinson estimator  [66] : Our final loss is composed of the mean squared error (MSE) between the recovered waveform and the ground-truth waveform plus the Jacobian regularization penalty:\n\nwhere λ determines the strength of regularization.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Pulse Rate Computation",
      "text": "The output of each of the aforementioned algorithms are the Fourier coefficients X ∈ C N ×K of the K regions from which we extracted a signal, where N is number of frequency bins. To find the pulse rate, we sum the power of the frequency coefficients across all regions for each bin:\n\nand the pulse rate is determined by the frequency bin that has the highest magnitude.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Implementation And Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "We evaluate our algorithm on three datasets, described below.\n\nMMSE-HR  [71] ,  [72] : The MMSE-HR dataset is a multimodal dataset with synchronized facial video and physiological monitoring. Facial video was captured at a resolution of 1040×1392 at 25 fps (frames per second) using the Di3D dynamic imaging system, while the blood pressure waveform was captured by a finger cuff using the Biopac NIBP100D recording at 1000 Hz (downsampled to 25Hz) and calibrated using an arm cuff. A total of 102 videos from 17 male and 23 female subjects were captured as various emotions (happiness, surprise, embarrassment, fear, etc.) were elicited, resulting in substantial head motion. We train on 10 s windows, shifting the time window by a stride of 2.4 s to obtain each new window. For evaluation on the MMSE-HR dataset, we follow the evaluation protocol of  [15] ,  [73]  concatenating three 10 s windows to obtain a 30 s window for evaluation. We perform leave-one-subject-out cross-validation and evaluate on 30 s, non-overlapping time windows.\n\nPURE  [74] : The PURE dataset consists of 10 subjects who perform 6 different motion tasks, from \"no motion\" to \"fast translation\" and \"rotation\". The camera recorded at 30 fps at a resolution of 640×480, while the corresponding pulse oximeter recorded at 60 Hz. The pulse oximeter was downsampled to match the rate of the video. We followed the train, validation, and test splits of  [31]  to evaluate our algorithm, using 30 s, non-overlapping time windows as in  [21]  during test time. We train on 10 s windows, shifting the time window by a stride of 2.4 s to obtain each new window.\n\nUBFC-rPPG  [75] : Captured using a webcam recording at 30 fps with a resolution of 640×480, the UBFC-rPPG dataset contains 43 videos of subjects playing a time-sensitive mathematical game to induce heart rate changes. The pulse signal was captured using a pulse oximeter finger clip sensor recording at 30 Hz. To evaluate our algorithm on this dataset, we follow the protocols of  [70] , which evaluated on nonoverlapping 10 s time windows per video, and averaged these windows to get a single heart rate estimate for each video. They then computed the pulse rate metrics over the 12 pulse rate estimates (corresponding to the 12 videos in the test set). We train on 10 s windows, shifting the time window by a stride of 2.4 s to obtain each new window.\n\nAn example of the images from each dataset is in Figure  5 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "To compute the pulse rate, we first multiply our pulse signal window by a Hanning window. Next, letting L = 100 × signal length, we compute the L-point FFT, and square the magnitudes of the frequency coefficients to get the power spectra. We sum the power for each frequency bin across all regions, and select the bin with the highest power as our pulse rate estimate. For pulse rate estimation, we report the mean absolute error (MAE) and root mean squared error (RMSE) between the predicted heart rate and ground-truth heart rate in a time window, measured in beats per minute as in  [2] . We define the MAE and RMSE as:\n\nwhere N is the number of time windows, R i is the groundtruth pulse-rate, and Ri is the predicted pulse rate in the time window. In addition, we measure the correlation coefficient ρ between the predicted and ground-truth heart-rate estimates as in  [2] . As it is also important to know what percentage of our estimates are sufficiently close to the ground-truth pulse rates, we use the PTE6 metric  [73] , defined as the percentage of the time that the estimated heart rate is within 6 bpm of the ground-truth heart rate:\n\nFor a fair comparison, we use 30 s time windows for evaluation on the MMSE-HR and PURE datasets as in  [21] , and 10 s time windows on the UBFC-rPPG dataset as in  [70] . Note that the results for  [21]  on the UBFC-rPPG dataset were reevaluated to match the evaluation protocol of other methods such as  [70]  on UBFC-rPPG.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Time Series Extraction",
      "text": "We generate the S-frame, K-channel time series Z ∈ R S×K from the video. To do so, we first detect the face using the FaceBoxes  [76]  and crop the face; next, we pass these crops to the LUVLi landmark detection  [77]  module, which generates 68 landmark points on the face as shown in Figure  1 . To extract a pulsatile signal from the skin, we first interpolate new landmarks from the initial 68 landmarks to obtain an additional 77 landmarks that span the cheeks, chin, and forehead. We then incorporate two design choices: 1) We segment the face into K = 5 super-regions, and average each RGB color channel's pixel intensity values in these regions as described in  [73] , and 2) we compute the ratio of the red color channel to green color channel as described in  [8] . Both of these steps improve signal strength and reduce motion noise. Doing this for all frames, we obtain a time series Z ∈ R S×K .\n\nAfter extracting the signals, each signal from the 5 spatial regions is convolved with a fifth-order Butterworth filter with cutoff frequencies of 0.7 Hz and 2.5 Hz as in  [15] . Finally, the signal is AC/DC normalized by dividing by the mean signal after subtracting the mean signal: ẑk = z k -µ k µ k , where z k is the signal in super-region k, and µ k is the mean of that same signal. These signals are segmented into windows of S samples and arranged as Z ∈ R S×K for heart rate estimation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Implementation Details",
      "text": "The architecture diagram for R and Q is shown in Figure  6 . Deep equilibrium models (DEQs) are instantiated as a collection of convolutions, linear operators, non-linearities, etc. organized as a single DEQ layer, and it is through this layer that the input is passed until convergence of a rootfinding algorithm. To ensure a fair comparison, we adopt the same architecture throughout. The DEQ models contain an extra MLP as an input injection network to ensure stability during training  [56] ,  [58] . Note that both R(•) and Q(•) have an identical architecture, with the only difference that the parameters of R(•) are complex (since R(•) operates in the frequency domain) while those of Q(•) are real.\n\nAll three models are trained using a batch size of 100. On the MMSE-HR dataset, we train for 10 epochs using UDEQ-iPPG and 25 epochs with DE-Prox-iPPG, with an initial learning rate for set to 3×10 -4 and reduced by half at the 10th epoch. For the PURE dataset, we train for 10 epochs on both models. On the UBFC dataset, we found using cross-validation that training for 45 epochs provided the optimal performance. The Jacobian regularization strength λ in Equation (  11 ) is set stochastically  [58]  during the training iterations using a random variable x ∼ Bernoulli(0.5), where λ(x) = 5 if x = 1, otherwise λ(x) = 0. All parameters were determined by cross validation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Experimental Results",
      "text": "1) Comparison to the State of the Art: The results on the MMSE-HR, PURE, and UBFC-rPPG are shown in Table  I . We see strong denoising performance for all three of our models. Our UDEQ-iPPG model yields new state-of-the-art results on both the MMSE-HR and PURE datasets, exhibiting better performance than both purely deep learning algorithms such as  [18] ,  [21]  and purely signal-processing, model-based methods such as  [4] ,  [7] ,  [8] . On the UBFC-rPPG dataset, UDEQ-iPPG nearly matches the performance of DualGAN  [70] , but does so with 50 times fewer trainable parameters. Our algorithms' strengths are shown qualitatively in Figure  7 , where the first Fig.  8 . Bland-Altman Analysis between predicted and ground-truth pulse rate estimates for the MMSE-HR¡ PURE, and UBFC-rPPG datasets using the UDEQ-iPPG algorithm. Each time window is represented by one point, which plots the difference between the predicted and ground-truth heart rates vs. the ground-truth heart rate. Note that the 95% limits of agreement, defined as the mean difference ± 1.96× the standard deviation, are within 5 bpm of the ground-truth heart rate on all datasets. column shows the input signals extracted from facial video in the time domain (top) and frequency domain (bottom), and the columns to the right of it compare the results of Unrolled iPPG, UDEQ-iPPG, and DE-Prox-iPPG. The orange signal is the ground-truth pulse signal obtained from the finger, while the black signal is the input signal and the blue signals are the outputs of our algorithms. The frequency domain representation of the input signal in Figure  7  shows a noisy signal extracted from the face, and all three algorithms attenuate the noise. However, only UDEQ-iPPG and DE-Prox-iPPG recover the correct peak frequency coefficients with high signal-to-noise ratio. This provides evidence that our assumption about the frequency coefficients of the heartbeat signal-that its frequency coefficients are well represented as the fixed point of a denoiser-is a valid assumption.\n\nDuring inference, our methods show rapid denoising and convergence to a fixed point, as shown in Figure  9 , where the orange signal is the frequency spectrum of the groundtruth signal, and the blue signal is the spectrum recovered by UDEQ-iPPG from facial video. We show the recovered signal over three unrolling iterations of UDEQ-iPPG. The blue signal in the top row shows Xt from Figure  2 , i.e., the signal after the gradient step but before the DEQ denoising step, while the the bottom row shows X t after R(•) operates upon it. Before the first iteration, the signal extracted from the face is noisy with an incorrect peak pulse-rate. In successive iterations, we notice the DEQ's ability to denoise the signal while the gradient step ensures consistency with the (noisy) extracted signal. By the end of three unrolling iterations, we successfully recover a cleaner signal.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "2) Statisical Analysis:",
      "text": "To confirm the validity of our method, we perform a modified Bland-Altman analysis (also known as the Tukey Mean-Difference plot) between the estimated pulse rates and the ground-truth rate pulse rates for the UDEQ-iPPG method. In Figure  8 , plot the difference between the predicted pulse rate and ground-truth pulse rate against the ground-truth pulse rate for the MMSE-HR, PURE, and UBFC-rPPG dataset. Each datapoint represents a single 30 s time window for the MMSE-HR dataset, a 10 s time window for the PURE dataset, and a 10 s time window for the UBFC-rPPG dataset. For all three datasets, our mean difference is close  to zero, showing our method's agreement with the groundtruth. Furthermore, the 95% limits of agreement, defined as the mean difference ± 1.96× the standard deviation, are less than or equal to 4.6 bpm from the ground-truth heart rate on all datasets. This shows strong performance, particularly over a range of pulse rates from 50 bpm to 130 bpm, confirming the effectiveness of our method.\n\nF. Ablation Studies 1) Model Class: DEQ versus Traditional Network: First, we look at the effect of implementing learned denoising operator as DEQs and traditional neural networks in UDEQ-iPPG. We examine the four algorithmic configurations for model type where R(•) ∈ {DEQ, NN}, Q(•) ∈ {DEQ, NN}, where \"DEQ\" is the fixed-point instantiation of the model in Figure  6 , while \"NN\" refers to a single application of the same model. The results are in Table  II . The results show Fig.  9 . We show the spectrum of our recovered signal from facial video (blue) and the ground truth signal (orange) over three unrolling iterations on the PURE dataset using UDEQ-iPPG. For each iteration, the blue signal in the top row corresponds to Xt before R(•) acts upon it, while the blue signal in the bottom row corresponds to Xt after R(•) as in Figure  2 . After multiple iterations, our algorithm captures the peak frequency and a harmonic while attenuating noise.\n\nthat the UDEQ-iPPG configuration, in which the signal X is modeled as the output of a fixed-point function (R = DEQ) and the noise E is modeled as unconstrained neural network (Q = NN), achieves state-of-the-art performance. The PURE dataset shows similar results; all model combinations perform well, but again the UDEQ-iPPG configuration performs best. On the UBFC-rPPG dataset, we see a clear distinction: when modeling the noise signal as a DEQ, our model struggles to converge. Both the Unrolled iPPG configuration (R = Q = NN) and the UDEQ-iPPG configuration (R = DEQ, Q = NN) show good performance. Our intuition is that the pulse signal has structure, while the noise is unstructured; therefore, instantiating R(•) as a DEQ and instantiating the noise Q as a traditional neural network learns best.\n\n2) Cross-Dataset Model Evaluation: Our methods' learned denoisers can generalize to unseen data; we show this in Table  III , in which we train and test on different datasets. UDEQ-iPPG has an MAE of less than 4 bpm in all scenarios, while Unrolled iPPG does well in most scenarios. We notice that UDEQ-iPPG transfers more effectively to other datasets, except in the case of MMSE-HR → PURE, when Unrolled iPPG performs better. Our algorithms transfer well to other datasets, though they suffer expected performance decreases when the domain shifts. We hypothesize that this happens because we learn the prior associated with each dataset. Since this prior is data-driven, it may not capture the recording scenarios, noise, etc. of another dataset. This is clearly seen when training on datasets with less motion/illumination noise (i.e. PURE, UBFC-rPPG) and testing on datasets with more noise (MMSE-HR).\n\n3) Mismatched Unrolling Iterations at Train and Test Time: One difference between UDEQ-iPPG and DE-Prox-iPPG is that UDEQ-iPPG is trained on a fixed number of unrolled iterations, while DE-Prox-iPPG is run until solver convergence. In Table  IV , we train Unrolled iPPG and UDEQ-iPPG on three unrolling iterations and train DE-Prox-iPPG until solver convergence, but limit the number of iterations at test time for all models. We see that UDEQ-iPPG's performance deteriorates when training and testing on a different number of iterations. In contrast, DE-Prox-iPPG maintains its performance, even after just one iteration. However, we see rapid two-step convergence for DE-Prox-iPPG; this may indicate that the joint fixed-point of pulse and noise signals may reside in a minimum that is suboptimal for each signal independently but optimal for them together. We explore this phenomenon further for UDEQ-iPPG. In Table  V , we train UDEQ-iPPG using four different unrolling iterations; we then test using that same set of iterations for each model. The diagonal for the table shows the results of training and testing on the same number of unrolling iterations. We notice that training and testing on three unrolling iterations provides the best performance, and report those numbers in the Table  I . However, we notice that training on five and ten unrolling iterations and testing on three unrolling iterations provides better performance than training and testing with five unrolling iterations, or training and testing with ten unrolling iterations. We posit that this performance discrepancy happens due to overfitting-at test time, unrolling for more iterations overfits to the training data at the expense of generalization performance.\n\n4) Modeling the noise explicitly: Finally, to determine the effect of explicitly modeling the noise, we examine training\n\nWe show the results in Table  VI . It is clear that Unrolled iPPG, DE-Prox-iPPG, and UDEQ-iPPG all benefit from modeling the noise in addition to the signal.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we model pulse signal recovery from facial video as an inverse problem with learnable priors and propose solutions using unrolling methods, fixed-point methods, and a combination of unrolling and fixed-point methods. In our first method, we apply successive gradient descent and neural network-based denoising steps on the pulse signal and noise for a fixed number of iterations-known commonly as \"unrolling\" proximal gradient descent-and call our method Unrolled iPPG. Instead of applying a fixed number of unrolling steps, Deep-Equilibrium-Proximal-iPPG (DE-Prox-iPPG), our second method, iterates both the gradient descent and denoising steps until fixed-point convergence. Finally, we combine unrolling and fixed-point methods in Unrolled-DEQ-iPPG, which applies the neural network-based denoising operator on the pulse signal until fixed-point convergence in each unrolling step. Our results show state-of-the-art performance using the UDEQ-iPPG, while all three methods show good performance using significantly fewer parameters than previous methods. Future work should explore methods to determine the learnable prior associated with our learned denoising operators, adding a layer of interpretability to these state-of-the-art algorithms.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our full pipeline. We pass the input video frames through a face and landmark detector, extrapolate additional facial landmarks, and obtain a time",
      "page": 2
    },
    {
      "caption": "Figure 2: Our unrolling algorithm for both Unrolled iPPG and UDEQ-iPPG. We replace the proximal operators of proximal gradient descent by deep denoisers",
      "page": 4
    },
    {
      "caption": "Figure 2: Similar to proximal gradient",
      "page": 4
    },
    {
      "caption": "Figure 3: Our DE-Prox-iPPG algorithmic paradigm. The video-extracted time series Z and the two concatenated optimization variables [X∗, E∗]T are passed",
      "page": 5
    },
    {
      "caption": "Figure 3: , encapsulates",
      "page": 5
    },
    {
      "caption": "Figure 2: as a DEQ but restrict Q to a single forward",
      "page": 5
    },
    {
      "caption": "Figure 4: In each unrolling iteration, we input the gradient-",
      "page": 5
    },
    {
      "caption": "Figure 4: The DEQ operator used in UDEQ-iPPG. At each unrolling iteration,",
      "page": 6
    },
    {
      "caption": "Figure 5: Image stills from the MMSE-HR dataset (top), PURE dataset (middle),",
      "page": 6
    },
    {
      "caption": "Figure 6: The architecture diagram of both R (complex-valued) and Q (real-",
      "page": 7
    },
    {
      "caption": "Figure 5: B. Evaluation Metrics",
      "page": 7
    },
    {
      "caption": "Figure 7: Comparing the input signal to results from Unrolled iPPG, UDEQ-iPPG, and DE-Prox-iPPG on the MMSE-HR dataset for one time window. The",
      "page": 8
    },
    {
      "caption": "Figure 7: , where the first",
      "page": 8
    },
    {
      "caption": "Figure 8: Bland-Altman Analysis between predicted and ground-truth pulse rate estimates for the MMSE-HR¡ PURE, and UBFC-rPPG datasets using the",
      "page": 9
    },
    {
      "caption": "Figure 2: , i.e., the signal after the",
      "page": 9
    },
    {
      "caption": "Figure 8: , plot the difference between",
      "page": 9
    },
    {
      "caption": "Figure 6: , while “NN” refers to a single application of the",
      "page": 9
    },
    {
      "caption": "Figure 9: We show the spectrum of our recovered signal from facial video (blue) and the ground truth signal (orange) over three unrolling iterations on the",
      "page": 10
    },
    {
      "caption": "Figure 2: After multiple iterations, our algorithm captures the peak frequency and a harmonic while attenuating",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Type": "Model-based",
          "Method": "ICA [6]\nCHROM [8]\nPOS [29]\nAutoSparsePPG(’20)\n[4]",
          "MMSE-HR\nρ ↑\nMAE↓\nRMSE↓": "5.44\n12.00\n-\n3.74\n8.11\n0.55\n3.90\n9.61\n-\n4.55\n14.42\n-",
          "PURE\nρ ↑\nMAE↓\nRMSE↓": "-\n-\n-\n2.07\n9.92\n-\n5.44\n12.00\n-\n-\n-\n-",
          "UBFC-rPPG\nρ ↑\nMAE ↓\nRMSE↓": "7.50\n14.41\n0.62\n2.37\n4.91\n0.89\n4.05\n8.75\n0.78\n-\n-\n-",
          "No. Params\n(×105) ↓": "-\n-\n-\n-"
        },
        {
          "Type": "Data-driven\nHybrid",
          "Method": "HR-CNN(’18)\n[31]\nSynRhythm(’18)\n[67]\nCAN(’18)\n[11]\nCVD (’20)\n[68]\nPulseGAN(’21)\n[69]\nInverseCAN(’21)\n[15]\nGideon(’21)\n[20]\nDualGAN(’22)\n[70]\nPhysformer(’22)\n[14]\nFederated(’22)\n[19]\nEfficientPhys-C(’23)\n[18]\nContrastphys+(’23)\n[21]\nUnrolled iPPG (Ours)\nDE-Prox-iPPG (Ours)\nUDEQ-iPPG (Ours)",
          "MMSE-HR\nρ ↑\nMAE↓\nRMSE↓": "-\n-\n-\n-\n-\n-\n4.06\n9.51\n-\n-\n6.04\n0.84\n-\n-\n-\n2.27\n4.90\n-\n-\n-\n-\n-\n-\n-\n2.84\n5.36\n-\n2.99\n-\n0.79\n2.91\n5.43\n0.86\n1.11\n3.83\n0.96\n1.11\n2.97\n0.97\n1.22\n2.80\n0.95\n0.95\n2.53\n0.98",
          "PURE\nρ ↑\nMAE↓\nRMSE↓": "1.84\n2.37\n-\n-\n-\n-\n-\n-\n-\n1.29\n2.01\n0.98\n-\n-\n-\n-\n-\n-\n2.10\n2.60\n0.99\n0.82\n1.31\n0.99\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.48\n0.98\n0.99\n0.41\n0.88\n0.99\n0.13\n0.28\n0.99\n0.18\n0.35\n0.99",
          "UBFC-rPPG\nρ ↑\nMAE ↓\nRMSE↓": "-\n-\n-\n5.59\n6.82\n0.72\n-\n-\n-\n2.19\n3.12\n0.99\n1.19\n2.10\n0.98\n-\n-\n-\n3.60\n4.60\n0.95\n0.44\n0.67\n0.99\n-\n-\n-\n2.00\n4.38\n0.93\n-\n-\n-\n0.99\n0.50\n0.84\n0.92\n1.84\n0.98\n2.86\n3.84\n0.97\n0.99\n0.46\n0.82",
          "No. Params\n(×105) ↓": "17.4\n-\n5.3\n123.3\n7.6\n10.7\n8.5\n76.8\n73.8\n5.3\n4.6\n8.5\n1.4\n1.4\n1.4"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "MMSE-HR",
          "Method": "—\nUDEQ-iPPG\n—\nUnrolled iPPG",
          "R(·) Q(·) MAE RMSE PTE6": "NN\nDEQ\nNN",
          "bpm ↓\nbpm ↓\n% ↑": "1.21\n3.02\n93.02\n0.95\n2.53\n94.57\n1.36\n3.2\n90.70\n1.12\n2.80\n93.02"
        },
        {
          "Dataset": "PURE",
          "Method": "—\nUDEQ-iPPG\n—\nUnrolled iPPG",
          "R(·) Q(·) MAE RMSE PTE6": "NN\nDEQ\nNN",
          "bpm ↓\nbpm ↓\n% ↑": "0.30\n0.80\n100\n0.18\n0.35\n100\n0.29\n0.63\n100\n0.41\n0.62\n100"
        },
        {
          "Dataset": "UBFC-rPPG",
          "Method": "—\nUDEQ-iPPG\n—\nUnrolled iPPG",
          "R(·) Q(·) MAE RMSE PTE6": "NN\nDEQ\nNN",
          "bpm ↓\nbpm ↓\n% ↑": "7.10\n10.10\n63.64\n0.46\n0.82\n100\n6.15\n9.45\n83.33\n0.92\n1.84\n90.91"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Train": "MMSE-HR\nPURE",
          "Test": "PURE\nMMSE-HR",
          "Model": "Unrolled iPPG\nUDEQ-iPPG",
          "MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "0.25\n0.58\n100\n0.44\n1.04\n100"
        },
        {
          "Train": "",
          "Test": "",
          "Model": "Unrolled iPPG\nUDEQ-iPPG",
          "MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "3.88\n11.54\n86.82\n2.38\n8.18\n89.92"
        },
        {
          "Train": "UBFC-rPPG MMSE-HR\nMMSE-HR",
          "Test": "UBFC-rPPG",
          "Model": "Unrolled iPPG\nUDEQ-iPPG",
          "MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "5.62\n10.23\n73.64\n3.72\n8.78\n85.01"
        },
        {
          "Train": "",
          "Test": "",
          "Model": "Unrolled iPPG\nUDEQ-iPPG",
          "MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "2.32\n8.46\n90.69\n0.42\n1.26\n100"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Test\niter": "1",
          "Method": "Unrolled iPPG\nUDEQ-iPPG\nDE-Prox-iPPG",
          "MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "3.44\n11.72\n89.14\n2.72\n10.20\n90.69\n1.28\n2.86\n93.80"
        },
        {
          "Test\niter": "2",
          "Method": "Unrolled iPPG\nUDEQ-iPPG\nDE-Prox-iPPG",
          "MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "3.18\n11.11\n89.14\n2.72\n9.81\n91.47\n1.22\n2.80\n93.80"
        },
        {
          "Test\niter": "3",
          "Method": "Unrolled iPPG\nUDEQ-iPPG\nDE-Prox-iPPG",
          "MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "1.11\n2.97\n93.53\n0.95\n2.53\n94.57\n1.22\n2.80\n93.80"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Test\nIterations": "1 iter."
        },
        {
          "Test\nIterations": "MAE\nRMSE\nPTE6\n(bpm) ↓\n(bpm) ↓\n(%)↑"
        },
        {
          "Test\nIterations": "1.51\n3.65\n90.69\n2.72\n10.20\n90.69\n2.61\n9.81\n90.69\n2.78\n10.00\n89.14"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "Unrolled iPPG",
          "Q(·)? MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "1.44\n3.87\n92.25\n1.11\n2.82\n93.02"
        },
        {
          "Method": "DE-Prox",
          "Q(·)? MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "1.50\n3.29\n90.70\n1.22\n2.80\n93.80"
        },
        {
          "Method": "UDEQ-iPPG",
          "Q(·)? MAE (bpm) ↓ RMSE (bpm) ↓ PTE6 (%) ↑": "1.70\n6.68\n92.24\n0.95\n2.53\n94.57"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Camera measurement of physiological vital signs",
      "authors": [
        "D Mcduff"
      ],
      "year": "2023",
      "venue": "ACM Computer Survey",
      "doi": "10.1145/3558518"
    },
    {
      "citation_id": "2",
      "title": "rppg-toolbox: Deep remote ppg toolbox",
      "authors": [
        "X Liu",
        "G Narayanswamy",
        "A Paruchuri",
        "X Zhang",
        "J Tang",
        "Y Zhang",
        "Y Wang",
        "S Sengupta",
        "S Patel",
        "D Mcduff"
      ],
      "year": "2022",
      "venue": "rppg-toolbox: Deep remote ppg toolbox",
      "arxiv": "arXiv:2210.00716"
    },
    {
      "citation_id": "3",
      "title": "Non-contact heart rate monitoring utilizing camera photoplethysmography in the neonatal intensive care unit-a pilot study",
      "authors": [
        "L Aarts",
        "V Jeanne",
        "J Cleary",
        "J Lieber",
        "S Nelson",
        "W Oetomo",
        "Verkruysse"
      ],
      "year": "2013",
      "venue": "Early human development"
    },
    {
      "citation_id": "4",
      "title": "Near-infrared imaging photoplethysmography during driving",
      "authors": [
        "E Nowara",
        "T Marks",
        "H Mansour",
        "A Veeraraghavan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "5",
      "title": "Remote photoplethysmography correspondence feature for 3d mask face presentation attack detection",
      "authors": [
        "S.-Q Liu",
        "X Lan",
        "P Yuen"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "6",
      "title": "Non-contact, automated cardiac pulse measurements using video imaging and blind source separation",
      "authors": [
        "M.-Z Poh",
        "D Mcduff",
        "R Picard"
      ],
      "year": "2010",
      "venue": "Optics express"
    },
    {
      "citation_id": "7",
      "title": "Measuring pulse rate with a webcam -a non-contact method for evaluating cardiac activity",
      "authors": [
        "M Lewandowska",
        "J Rumiński",
        "T Kocejko",
        "J Nowak"
      ],
      "year": "2011",
      "venue": "2011 Federated Conference on Computer Science and Information Systems (FedCSIS)"
    },
    {
      "citation_id": "8",
      "title": "Robust pulse rate from chrominance-based rppg",
      "authors": [
        "G De Haan",
        "V Jeanne"
      ],
      "year": "2013",
      "venue": "IEEE Trans. on Biomedical Eng"
    },
    {
      "citation_id": "9",
      "title": "Improved motion robustness of remote-ppg by using the blood volume pulse signature",
      "authors": [
        "G De Haan",
        "A Van Leest"
      ],
      "year": "2014",
      "venue": "Physiological Measurement",
      "doi": "10.1088/0967-3334/35/9/1913"
    },
    {
      "citation_id": "10",
      "title": "Sparseppg: Towards driver monitoring using camera-based vital signs estimation in near-infrared",
      "authors": [
        "E Magdalena Nowara",
        "T Marks",
        "H Mansour",
        "A Veeraraghavan"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "11",
      "title": "Deepphys: Video-based physiological measurement using convolutional attention networks",
      "authors": [
        "W Chen",
        "D Mcduff"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "12",
      "title": "Multi-task temporal shift attention networks for on-device contactless vitals measurement",
      "authors": [
        "X Liu",
        "J Fromm",
        "S Patel",
        "D Mcduff"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks",
      "authors": [
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Proc. BMVC"
    },
    {
      "citation_id": "14",
      "title": "Physformer: Facial video-based physiological measurement with temporal difference transformer",
      "authors": [
        "Z Yu",
        "Y Shen",
        "J Shi",
        "H Zhao",
        "P Torr",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "15",
      "title": "The benefit of distraction: Denoising camera-based physiological measurements using inverse attention",
      "authors": [
        "E Nowara",
        "D Mcduff",
        "A Veeraraghavan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "16",
      "title": "Metaphys: Few-shot adaptation for non-contact physiological measurement",
      "authors": [
        "X Liu",
        "Z Jiang",
        "J Fromm",
        "X Xu",
        "S Patel",
        "D Mcduff"
      ],
      "year": "2021",
      "venue": "Proc. of the Conf. on Health, Inference, and Learning, ser. CHIL '21",
      "doi": "10.1145/3450439.3451870"
    },
    {
      "citation_id": "17",
      "title": "Meta-rppg: Remote heart rate estimation using a transductive meta-learner",
      "authors": [
        "E Lee",
        "E Chen",
        "C.-Y Lee"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "18",
      "title": "Efficientphys: Enabling simple, fast and accurate camera-based cardiac measurement",
      "authors": [
        "X Liu",
        "B Hill",
        "Z Jiang",
        "S Patel",
        "D Mcduff"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "19",
      "title": "Federated remote physiological measurement with imperfect data",
      "authors": [
        "X Liu",
        "M Zhang",
        "Z Jiang",
        "S Patel",
        "D Mcduff"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "20",
      "title": "The way to my heart is through contrastive learning: Remote photoplethysmography from unlabelled video",
      "authors": [
        "J Gideon",
        "S Stent"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "21",
      "title": "Contrast-phys+: Unsupervised and weakly-supervised video-based remote physiological measurement via spatiotemporal contrast",
      "authors": [
        "Z Sun",
        "X Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Facial video-based remote physiological measurement via self-supervised learning",
      "authors": [
        "Z Yue",
        "M Shi",
        "S Ding"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "23",
      "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "authors": [
        "A Beck",
        "M Teboulle"
      ],
      "year": "2009",
      "venue": "SIAM Journal on Imaging Sciences",
      "doi": "10.1137/080716542"
    },
    {
      "citation_id": "24",
      "title": "Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing",
      "authors": [
        "V Monga",
        "Y Li",
        "Y Eldar"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "25",
      "title": "Deep equilibrium models",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Deep equilibrium architectures for inverse problems in imaging",
      "authors": [
        "D Gilton",
        "G Ongie",
        "R Willett"
      ],
      "venue": "IEEE Transactions on Computational Imaging"
    },
    {
      "citation_id": "27",
      "title": "Unrolled ippg: Video heart rate estimation via unrolling proximal gradient descent",
      "authors": [
        "V Shenoy",
        "T Marks",
        "H Mansour",
        "S Lohit"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "28",
      "title": "Heart rate and heart rate variability from single-channel video and ica integration of multiple signals",
      "authors": [
        "R Favilla",
        "V Zuccala",
        "G Coppini"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "29",
      "title": "Algorithmic principles of remote ppg",
      "authors": [
        "W Wang",
        "A Den",
        "S Brinker",
        "G Stuijk",
        "De Haan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "30",
      "title": "Self-adaptive matrix completion for heart rate estimation from face videos under realistic conditions",
      "authors": [
        "S Tulyakov",
        "X Alameda-Pineda",
        "E Ricci",
        "L Yin",
        "J Cohn",
        "N Sebe"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "31",
      "title": "Visual heart rate estimation with convolutional neural network",
      "authors": [
        "R Spetlik",
        "V Franc",
        "J Cech",
        "J Matas"
      ],
      "year": "2018",
      "venue": "British Machine Vision Conference"
    },
    {
      "citation_id": "32",
      "title": "Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks",
      "authors": [
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2019",
      "venue": "Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks",
      "arxiv": "arXiv:1905.02419"
    },
    {
      "citation_id": "33",
      "title": "Robust remote photoplethysmography estimation with environmental noise disentanglement",
      "authors": [
        "S.-Q Liu",
        "P Yuen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "34",
      "title": "3d convolutional neural networks for remote pulse rate measurement and mapping from facial video",
      "authors": [
        "F Bousefsaf",
        "A Pruski",
        "C Maaoui"
      ],
      "year": "2019",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "35",
      "title": "Eta-rppgnet: Effective time-domain attention network for remote heart rate measurement",
      "authors": [
        "M Hu",
        "F Qian",
        "D Guo",
        "X Wang",
        "L He",
        "F Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "36",
      "title": "Autohr: A strong end-toend baseline for remote heart rate measurement with neural searching",
      "authors": [
        "Z Yu",
        "X Li",
        "X Niu",
        "J Shi",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "37",
      "title": "Bvpnet: Video-to-bvp signal prediction for remote heart rate estimation",
      "authors": [
        "A Das",
        "H Lu",
        "H Han",
        "A Dantcheva",
        "S Shan",
        "X Chen"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "38",
      "title": "Dual-bridging with adversarial noise generation for domain adaptive rppg estimation",
      "authors": [
        "J Du",
        "S.-Q Liu",
        "B Zhang",
        "P Yuen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "39",
      "title": "Tsm: Temporal shift module for efficient video understanding",
      "authors": [
        "J Lin",
        "C Gan",
        "S Han"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "40",
      "title": "rppg-mae: Selfsupervised pretraining with masked autoencoders for remote physiological measurements",
      "authors": [
        "X Liu",
        "Y Zhang",
        "Z Yu",
        "H Lu",
        "H Yue",
        "J Yang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Neural ordinary differential equations",
      "authors": [
        "R Chen",
        "Y Rubanova",
        "J Bettencourt",
        "D Duvenaud"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "42",
      "title": "A class of methods for solving nonlinear simultaneous equations",
      "authors": [
        "C Broyden"
      ],
      "year": "1965",
      "venue": "Mathematics of Computation"
    },
    {
      "citation_id": "43",
      "title": "The numerical solution of parabolic and elliptic differential equations",
      "authors": [
        "D Peaceman",
        "H Rachford"
      ],
      "year": "1955",
      "venue": "Journal of the Society for Industrial and Applied Mathematics",
      "doi": "10.1137/0103003"
    },
    {
      "citation_id": "44",
      "title": "Iterative procedures for nonlinear integral equations",
      "authors": [
        "D Anderson"
      ],
      "year": "1965",
      "venue": "J. ACM",
      "doi": "10.1145/321296.321305"
    },
    {
      "citation_id": "45",
      "title": "The Implicit Function Theorem: History, Theory, and Applications",
      "authors": [
        "S Krantz",
        "H Parks"
      ],
      "year": "2012",
      "venue": "The Implicit Function Theorem: History, Theory, and Applications"
    },
    {
      "citation_id": "46",
      "title": "Multiscale deep equilibrium models",
      "authors": [
        "S Bai",
        "V Koltun",
        "J Kolter"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "Joint inference and input optimization in equilibrium networks",
      "authors": [
        "S Gurumurthy",
        "S Bai",
        "Z Manchester",
        "J Kolter"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "Deep equilibrium optical flow estimation",
      "authors": [
        "S Bai",
        "Z Geng",
        "Y Savani",
        "J Kolter"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "49",
      "title": "Deep equilibrium approaches to diffusion models",
      "authors": [
        "A Pokle",
        "Z Geng",
        "J Kolter"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "Recurrence without recurrence: Stable video landmark detection with deep equilibrium models",
      "authors": [
        "P Micaelli",
        "A Vahdat",
        "H Yin",
        "J Kautz",
        "P Molchanov"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "51",
      "title": "A highly interpretable deep equilibrium network for hyperspectral image deconvolution",
      "authors": [
        "A Gkillas",
        "D Ampeliotis",
        "K Berberidis"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "52",
      "title": "Equilibrium image denoising with implicit differentiation",
      "authors": [
        "Q Chen",
        "Y Wang",
        "Z Geng",
        "Y Wang",
        "J Yang",
        "Z Lin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "53",
      "title": "Online deep equilibrium learning for regularization by denoising",
      "authors": [
        "J Liu",
        "X Xu",
        "W Gan",
        "U Kamilov"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "54",
      "title": "Robustness of deep equilibrium architectures to changes in the measurement model",
      "authors": [
        "J Hu",
        "S Shoushtari",
        "Z Zou",
        "J Liu",
        "Z Sun",
        "U Kamilov"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "55",
      "title": "Deep equilibrium learning of explicit regularizers for imaging inverse problems",
      "authors": [
        "Z Zou",
        "J Liu",
        "B Wohlberg",
        "U Kamilov"
      ],
      "year": "2023",
      "venue": "IEEE Open J. Signal Process",
      "doi": "10.1109/OJSP.2023.3296036"
    },
    {
      "citation_id": "56",
      "title": "On training implicit models",
      "authors": [
        "Z Geng",
        "X.-Y Zhang",
        "S Bai",
        "Y Wang",
        "Z Lin"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "57",
      "title": "Jfb: Jacobian-free backpropagation for implicit networks",
      "authors": [
        "S Fung",
        "H Heaton",
        "Q Li",
        "D Mckenzie",
        "S Osher",
        "W Yin"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "58",
      "title": "Stabilizing equilibrium models by jacobian regularization",
      "authors": [
        "S Bai",
        "V Koltun",
        "Z Kolter"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning"
    },
    {
      "citation_id": "59",
      "title": "Certified robustness for deep equilibrium models via interval bound propagation",
      "authors": [
        "C Wei",
        "J Kolter"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "60",
      "title": "CerDEQ: Certifiable deep equilibrium model",
      "authors": [
        "M Li",
        "Y Wang",
        "Z Lin",
        "K Chaudhuri",
        "S Jegelka",
        "L Song",
        "C Szepesvari",
        "G Niu",
        "S Sabato"
      ],
      "year": "2022",
      "venue": "Proceedings of the 39th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "61",
      "title": "Estimating lipschitz constants of monotone deep equilibrium models",
      "authors": [
        "C Pabbaraju",
        "E Winston",
        "J Kolter"
      ],
      "year": "2021",
      "venue": "International conference on learning representations"
    },
    {
      "citation_id": "62",
      "title": "Monotone operator equilibrium networks",
      "authors": [
        "E Winston",
        "J Kolter"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "63",
      "title": "Self-supervised deep equilibrium models for inverse problems with theoretical guarantees",
      "authors": [
        "W Gan",
        "C Ying",
        "P Eshraghi",
        "T Wang",
        "C Eldeniz",
        "Y Hu",
        "J Liu",
        "Y Chen",
        "H An",
        "U Kamilov"
      ],
      "year": "2022",
      "venue": "Self-supervised deep equilibrium models for inverse problems with theoretical guarantees"
    },
    {
      "citation_id": "64",
      "title": "Mc-Donald's blood flow in arteries: theoretical, experimental and clinical principles",
      "authors": [
        "W Nichols",
        "M O'rourke",
        "E Edelman",
        "C Vlachopoulos"
      ],
      "year": "2022",
      "venue": "Mc-Donald's blood flow in arteries: theoretical, experimental and clinical principles"
    },
    {
      "citation_id": "65",
      "title": "Proximal algorithms",
      "authors": [
        "N Parikh",
        "S Boyd"
      ],
      "year": "2014",
      "venue": "Found. Trends Optim",
      "doi": "10.1561/2400000003"
    },
    {
      "citation_id": "66",
      "title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines",
      "authors": [
        "M Hutchinson"
      ],
      "year": "1989",
      "venue": "Communications in Statistics -Simulation and Computation",
      "doi": "10.1080/03610918908812806"
    },
    {
      "citation_id": "67",
      "title": "Synrhythm: Learning a deep heart rate estimator from general to specific",
      "authors": [
        "X Niu",
        "H Han",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "2018 24th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "68",
      "title": "Video-based remote physiological measurement via cross-verified feature disentangling",
      "authors": [
        "X Niu",
        "Z Yu",
        "H Han",
        "X Li",
        "S Shan",
        "G Zhao"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "69",
      "title": "Pulsegan: Learning to generate realistic pulse waveforms in remote photoplethysmography",
      "authors": [
        "R Song",
        "H Chen",
        "J Cheng",
        "C Li",
        "Y Liu",
        "X Chen"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "70",
      "title": "Dual-gan: Joint bvp and noise modeling for remote physiological measurement",
      "authors": [
        "H Lu",
        "H Han",
        "S Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "71",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang",
        "J Cohn",
        "Q Ji",
        "L Yin"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "72",
      "title": "Crossdomain au detection: Domains, learning approaches, and measures",
      "authors": [
        "I Ertugrul",
        "J Cohn",
        "L Jeni",
        "Z Zhang",
        "L Yin",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "Gesture Recognition"
    },
    {
      "citation_id": "73",
      "title": "Turnip: Time-series u-net with recurrence for nir imaging ppg",
      "authors": [
        "A Comas",
        "T Marks",
        "H Mansour",
        "S Lohit",
        "Y Ma",
        "X Liu"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "74",
      "title": "Non-contact video-based pulse rate measurement on a mobile service robot",
      "authors": [
        "R Stricker",
        "S Müller",
        "H.-M Groß"
      ],
      "year": "2014",
      "venue": "The 23rd IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "75",
      "title": "Unsupervised skin tissue segmentation for remote photoplethysmography",
      "authors": [
        "S Bobbia",
        "R Macwan",
        "Y Benezeth",
        "A Mansouri",
        "J Dubois"
      ],
      "year": "2019",
      "venue": "award Winning Papers from the 23rd Inter. Conf. on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "76",
      "title": "Faceboxes: A cpu real-time face detector with high accuracy",
      "authors": [
        "S Zhang",
        "X Zhu",
        "Z Lei",
        "H Shi",
        "X Wang",
        "S Li"
      ],
      "year": "2017",
      "venue": "Faceboxes: A cpu real-time face detector with high accuracy"
    },
    {
      "citation_id": "77",
      "title": "Luvli face alignment: Estimating landmarks' location, uncertainty, and visibility likelihood",
      "authors": [
        "A Kumar",
        "T Marks",
        "W Mou",
        "Y Wang",
        "M Jones",
        "A Cherian",
        "T Koike-Akino",
        "X Liu",
        "C Feng"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}