{
  "paper_id": "2303.04391v1",
  "title": "A Deep-Learning-Based Neural Decoding Framework For Emotional Brain-Computer Interfaces",
  "published": "2023-03-08T05:52:58Z",
  "authors": [
    "Xinming Wu",
    "Ji Dai"
  ],
  "keywords": [
    "deep learning",
    "confidence learning",
    "emotion decoding",
    "brain-computer interface",
    "Emo-Net"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Reading emotions precisely from segments of neural activity is crucial for the development of emotional brain-computer interfaces. Among all neural decoding algorithms, deep learning (DL) holds the potential to become the most promising one, yet progress has been limited in recent years. One possible reason is that the efficacy of DL strongly relies on training samples, yet the neural data used for training are often from non-human primates and mixed with plenty of noise, which in turn mislead the training of DL models. Given it is difficult to accurately determine animals' emotions from humans' perspective, we assume the dominant noise in neural data representing different emotions is the labeling error. Here, we report the development and application of a neural decoding framework called Emo-Net that consists of a confidence learning (CL) component and a DL component. The framework is fully data-driven and is capable of decoding emotions from multiple datasets obtained from behaving monkeys. In addition to improving the decoding ability, Emo-Net significantly improves the performance of the base DL models, making emotion recognition in animal models possible. In summary, this framework may inspire novel understandings of the neural basis of emotion and drive the realization of close-loop emotional brain-computer interfaces.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Decoding emotions from neural signals has long been popular in the fields of computational science, neuroscience, and psychology  (Glaser et al., 2020; Wan et al., 2021) . Traditionally, to enable the quantitative analysis and computational modeling of emotions, the neural representation of emotions is usually generated through cognitive and electrophysiological experiments. Recent advances in brain-computer interface (BCI) further enable us to collect neural signals from a broader scale, enriching the foundation for establishing computational models  (Onken et al., 2016; Shanechi, 2019; Degenhart et al., 2020; Steinmetz et al., 2021) .\n\nCurrently, BCIs can be classified as invasive (IBCIs) and non-invasive (Non-IBCIs).\n\nGenerally, IBCIs can collect signals from the deeper structure and generate cleaner signals than Non-IBCIs, thus can generate better accuracy when being used in neural decoding (e.g., motor imagination and language recovery)  (Anumanchipalli et al., 2018; Moses et al., 2018; Heelan et al., 2019; Gallego et al., 2020; Makin et al., 2020; Yu et al., 2020; Zhang et al., 2020a; Zhou et al., 2020; Tankus et al., 2021; Wen et al., 2021) . However, due to technological challenges, many matured BCI applications aiming to decode human emotions are Non-IBCIs-based (e.g., electroencephalogram, EEG)  (Li et al., 2017; Yang et al., 2018; Hasan and Kim, 2019; Li et al., 2020; Luo et al., 2020; He et al., 2022) . Given the advantages in signal quality, it remains intriguing to develop an IBCIs-based emotion decoder, especially from those discrete but informative spike trains  (Shanechi, 2019) .\n\nAmong machine learning strategies  (Xu et al., 2021) , some deep learning (DL) models have been performing well in natural language-based (e.g. text) emotion recognition. For example, by assigning neural data as texts of \"brain activity\" pieces, some DL models achieve up to 95% accuracy on many classification and reconstruction tasks  (Du et al., 2019; Zhang et al., 2019; Tankus et al., 2021; Ninenko et al., 2022; Wu et al., 2022) . However, DL models that perform well in neural responses-based emotion classification are still missing, partially due to the large uncertainty inhered in emotion-related neural signals. Because of the complexity of cognitive functions, emotion-related neural data normally consist of a higher order of noises than image or text data. In particular, the emotion stimulus and the evoked emotion in subjects may not be consistently matched all the time. In addition, a piece of neural signals may encode multiple emotions.\n\nOn the other hand, IBCIs normally collect data from animals, which are not capable of verbal communication  (Dai, 2021) . Therefore, there are inevitable errors within the definition of these subjects' emotions estimated by people, namely \"labeling errors\", which are difficult to correct or quantify. To identify incorrect labels, methods such as confident learning (CL), which is a model-agnostic theory and algorithm for identifying labeling errors, have been proposed  (Northcutt et al., 2021) . Additionally, animal experiments normally do not allow unlimited data collection, making the data samples not large enough for DL model training. However, it is not ideal to amplify data either as the labeling errors may spread or even amplify unexpectedly. Overall, further refined strategies are required to solve these issues and improve the performance of emotional BCIs.\n\nIn this study, based on investigating the relations between labels, stimuli, emotions, and neural responses, we present a hybrid framework that combines CL and DL for spike decoding of emotions from neural responses obtained from IBCIs. This framework is fully data-driven, with no special requirements for models involved in the training. In the following, we demonstrate the decoding capability of this framework, its ability to perform end-to-end mapping of neural responses to emotions, and its robustness to neural data obtained from IBCIs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Non-Human Primate Experiments And Neural Response Dataset",
      "text": "One adult male macaque monkey weighing 7.5 kg was used in the study. All experimental procedures were approved by the Institutional Animal Care and Use Committee (IACUC) at Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences following the guidelines stated in the Guide for Care and Use of Laboratory Animals  (Eighth Edition, 2011) .\n\nTo obtain neural responses from this experimental monkey, a recording chamber (Form-fitting, PEEK) and a micro-drive (SC32-42mm, both from Gray Matter Research, Montana) were implanted above the amygdala following the product manual  (Dotson et al., 2017) . During the experiment, the monkey was trained to sit quietly in a chair with the head fixed. Visual stimuli were presented on a monitor (VG248, ASUS) 57 cm in front of the subject. An eye tracker (iView X Hi-Speed Primate, SMI) was used to monitor the monkey's eye position. A Matlab-based toolbox MonkeyLogic (NIMH) was used for experimental control.\n\nVisual stimuli were faces of different species (human and monkey), different emotions (negative, positive, and neutral), and different spatial frequencies (broad, high, low, and a mix of high and low). Each stimulus was presented for 800 ms after the monkey acquired fixation for 500 ms (Figure  1A ).\n\nA 128-channel electrophysiological recording system (OmniPlex, Plexon Inc.) was used to monitor and record neural activities. Signals were filtered between 250 Hz and 5 kHz to identify spiking activity.\n\nSpikes were sorted offline using Kilosorter to identify single units. Specifically, we extracted spiking activities for each unit from 200 ms before to 1368 ms after the presentation of facial stimuli, then calculated the firing rate using a sliding window of 48 ms and a step size of 16 ms (Figure  1B ), from which a firing rate vector with a length of 96 was generated. The firing rate vectors were then arranged according to the position of the recording electrode, yielding a 64×96 matrix of firing rates (Figure  1C ), which was standardized and added a Gaussian noise with a mean of 1 and a variance of 0.005 to reduce the influence of sparsity on model performance. Lastly, to increase the sample size, the datasets were augmented by applying an 8% Dropout to the firing rate of each sample.\n\nBased on the three dimensions of facial stimuli (species, emotion, spatial frequency), we established four three-class datasets and one two-class dataset from the obtained samples (Table  1):   Table 1:",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "The Uncertainty Of Neural Response Samples",
      "text": "An emotion-evoked neural response sample with a response X and a label Y has two kinds of aleatoric uncertainty. One comes from the measurement of the neural response X. and the other comes from the label Y. In supervised learning, labels are used to train models, yet noises in labels increase the outcome uncertainty.\n\nAs illustrated in Figure  2 , since visual stimuli cannot consistently elicit specific emotional states in experimental animals, and it is difficult to accurately correlate each neural sequence with a specific emotion it leads to a significant number of label errors in such emotion-evoked neural data. These label errors can be divided into two types: stimuli failing to elicit the intended emotions and stimuli eliciting multiple emotions. The first type of error represents a clear case of mislabeling, which should be promptly corrected. The second category involves multi-label learning and can still be used for training, although it is preferable to supplement such labels as necessary. Functions used in Algorithm-1 are defined below:\n\nLoss reweight and prune function:\n\nOf which, μ and σ are the mean value and standard deviation of q, respectively.\n\nFunction (  1 ) is to standardize the uncertainty quality q and function (  2 ) is to normalize the results within 0 to 1.\n\nThe loss function is calculated by Cross Entropy, which is commonly used in classification models:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Decoding Performance",
      "text": "Firstly, we tested the performance of Emo-Net on five neural signal datasets we have established (Table  1 ) (Experiment 1) and presented the average accuracies and F1scores obtained through ten-fold cross-validation in Table  2 . Models running without the Emo-Net framework were taken as baselines, while models using the Emo-Net with loss reweight were labeled by \"Emo-R\", and models using the Emo-Net with data pruning were labeled by \"Emo-P\". Then, to mitigate the potential impact of labeling errors from the validation set of each fold on the accuracy of ten-fold, we employed CL to measure the uncertainty of emotion-evoked neural responses (Experiment 2). Specifically, we sampled randomly from the relatively reliable data and obtained an independent test set of balanced categories. The performance of each model running on these test sets were presented in Table  3 . These tests included the neural decoding performance in a machine learning model (RF), a Multi-Layer Perceptron (MLP), three different scales convolutional neural networks (CNNs) (MobileNet/VGG-16/ResNet-18)  (Simonyan and Zisserman, 2014; He et al., 2016; Howard et al., 2017) , and a state-of-the-art spike decoding models (SID)  (Zhang et al., 2020b) . The output layers of each model have been modified to fit the classification purpose.\n\nTables  2  and 3  show that the Emo-Net framework affected different models in different manners. Specifically, in Experiment 1, Emo-Net only slightly improved the performance of limited models in some datasets using an optimized algorithm either by loss reweight or data pruning (Table  2 ). In Experiment 2, models with the CLPembedded Emo-Net framework demonstrated a dramatically improved performance in neural decoding compared to the baseline in all tested datasets (Table  3 ). For example, the accuracy of model RestNet-18 increased from 70.4% to 85.8% after employing Emo-P when running on Dataset-2.\n\nAmong all tested models, after embedding the Emo-Net framework, the MLP and the ResNet-18 generated the best and second-best results, of which the accuracy reached 92.02% and 85.88%, respectively. Interestingly, even without complex CNNs, the MLP model generated better performance than other CNN models in decoding emotion-evoked neural responses, suggesting that such a simple artificial neural network is also capable of high decoding velocity and effective biotic simulation.\n\nWhen compared with the state-of-the-art model SID, even the simplest RF model",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Ablation Experiments",
      "text": "To further examine the contribution of the CLP in the Emo-Net framework in neural response-based emotion decoding, we conducted an ablation test by incrementally increasing the proportion of correctly-labeled samples in the training data. The tests were carried out on the independent test sets in Experiment 2 using the combination of Emo-Net, ResNet-18, and train data pruning, with a learning rate of 0.001, batch size of 256, and epochs of 200. The results on five datasets are presented in Figure  4A-E , which shows that the accuracy of Emo-Net gradually increased over the baseline as the ratio of noise-pruned data increased.\n\nFurther, to verify the necessity of reducing label uncertainty in emotion-evoked neural data, we conducted classification tests on a kinematics dataset  (Zhang et al., 2019) .\n\nCompared with the emotional dataset, kinematics-related data are typical \"heart-tohand\", where movement intentions can be clearly revealed in behaviors and rarely be mislabeled. As shown in Figure  4F , after pruning with CLP, the model's decoding performance for movement intentions was only slightly improved compared with the baseline (blue vs. yellow). As the proportion of pruned data increased, the negative impact enlarged and resulted in a gradual decrease in performing accuracy. From a structural perspective, the effectiveness of the CL+DL combination can be attributed to the robustness provided by CLP, which enables the DL model to perform effectively. Specifically, emotion-evoked neural response samples commonly have inaccurate or multiple labels, which leads to inaccurate decoding when running traditional DL models. In this study, we addressed this issue with two optimization strategies, named \"data pruning\" and \"loss reweight\". The first strategy assigns zero to the loss of uncertain samples, which effectively eliminates the impact of inaccurate labels and discards all information from unreliable samples. The second strategy incorporates the multiple-label information presented in unreliable samples. Both methods reduce aleatoric uncertainty and decrease the sample size while leading to an increase in epistemic uncertainty. In the context of emotion decoding, the benefits of reducing aleatoric uncertainty outweigh the potential negative effects of increased epistemic uncertainty, which underlines the effectiveness of the CL+DL combination.",
      "page_start": 15,
      "page_end": 17
    },
    {
      "section_name": "Data-Centered Optimization For Neural Decoding",
      "text": "Previously, to enhance the decoding performance, most neural decoding frameworks attempted to explore more powerful, biomimetic, and robust structures in their models  (Zhang et al., 2020a; Zhou et al., 2020) , or to find more general optimization strategies to improve the representation capabilities in all tasks  (Onken et al., 2016; Degenhart et al., 2020; Wen et al., 2021) . In contrast to these general approaches, we proposed a strategy to extract optimization algorithms from a data-centered perspective.\n\nOur study indicates that, in neural decoding, having more training data is not always beneficial. Properly balancing and removing uncertain data can lead to better performance of the model. Our method concentrates specifically on the scenario of emotion recognition. In other neural decoding scenarios, neural signal samples may have their characteristics. Therefore, in emotion-related research and the implementation of emotional BCIs (eBCIs), in addition to pursuing the latest machine learning models, we should also pay attention to optimization strategies that are characteristics-based.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Implication For Bcis",
      "text": "Although it is widely accepted that IBCIs are more accurate in terms of intent recognition, under current ethical and experimental constraints, spiking activity-based DL models perform poorly in emotion classification. Before IBCIs can be effectively applied to humans for emotion decoding, a neural decoding method, especially a spiking-based approach that is highly robust to uncertain data, is urgently required to be developed based on animal experiments.\n\nOur framework fills in this gap using current hardware and experimental settings and enables efficient emotion classification based on spike decoding. Furthermore, the DL component of this framework can be considered as a general module since its concept and implementation are straightforward, thus allowing great flexibility in model developments. The CL component is model-free and is responsible for providing robustness to the overall system. When combined with a DL model, an efficient and versatile CL component is a significant supplement in promoting emotion-related IBCIs studies.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Limitations And Future Studies",
      "text": "Our study focuses on optimizing the uncertainty inhered in neural signals that encode emotion-related information. However, a more in-depth examination of the two optimization strategies employed in Emo-Net (loss reweight vs. data pruning) has not been conducted. Additionally, the validation of the Emo-Net was only conducted in 5 datasets established by ourselves. Further tests should be performed on other thirdparty datasets to validate the transferability. Lastly, our evaluation of the framework was limited to its decoding capability. An extension to assess the encoding capability is preferable in the future.\n\nTo conclude, our study shows that in the neural decoding of emotions, the application of CLP in balancing uncertainty can yield an overperformance for the entire system.\n\nThe hybrid framework of CL+DL demonstrates superior performance in emotion classification and shows preliminary capability in identifying animal emotions in simple scenarios. Therefore, Emo-Net is a promising framework that may inspire novel understandings of the neural basis of emotion and drive the realization of closeloop eBCIs in the future.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The illustration of the monkey experiment acquiring emotion-evoked",
      "page": 5
    },
    {
      "caption": "Figure 1: B), from which a firing rate vector with a",
      "page": 6
    },
    {
      "caption": "Figure 1: C), which was standardized and added a Gaussian noise with a mean of 1 and a",
      "page": 6
    },
    {
      "caption": "Figure 2: An illustration of the generation of label errors in establishing emotion-",
      "page": 8
    },
    {
      "caption": "Figure 2: , since visual stimuli cannot consistently elicit specific",
      "page": 8
    },
    {
      "caption": "Figure 3: An illustration of the Emo-Net framework. The training data is initially",
      "page": 9
    },
    {
      "caption": "Figure 3: ). This hybrid framework can be incorporated before or during training.",
      "page": 9
    },
    {
      "caption": "Figure 4: A-E, which shows that the accuracy of Emo-Net gradually increased over the",
      "page": 15
    },
    {
      "caption": "Figure 4: F, after pruning with CLP, the model's decoding",
      "page": 15
    },
    {
      "caption": "Figure 4: Results of the ablation experiments on different datasets. The x-axis",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "1. Guangdong Provincial Key Laboratory of Brain Connectome and Behavior, CAS"
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "Key Laboratory of Brain Connectome and Manipulation,"
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "Brain Disease Institute (BCBDI), Shenzhen Institute of Advanced Technology, Chinese"
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "Academy of Sciences, Shenzhen, 518055, China."
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "2. Shenzhen-Hong Kong Institute of Brain Science-Shenzhen Fundamental Research"
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "Institutions, Shenzhen, 518055, China."
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "3.\nShenzhen Technological Research Center"
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "Shenzhen 518055, China"
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "4. University of Chinese Academy of Sciences, Beijing 100049, China"
        },
        {
          "Xinming Wu1,2, Ji Dai1,2,3,4,*": "* Corresponding author (Ji Dai, Email: ji.dai@siat.ac.cn; Tel: 18910119379)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Reading\nemotions\nprecisely\nfrom segments\nof\nneural\nactivity\nis\ncrucial\nfor\nthe"
        },
        {
          "Abstract": "development\nof\nemotional\nbrain-computer\ninterfaces. Among\nall\nneural\ndecoding"
        },
        {
          "Abstract": "algorithms, deep learning (DL) holds the potential to become the most promising one,"
        },
        {
          "Abstract": "yet progress has been limited in recent years. One possible reason is that\nthe efficacy"
        },
        {
          "Abstract": "of DL strongly relies on training samples, yet\nthe neural data used for\ntraining are"
        },
        {
          "Abstract": "often from non-human primates and mixed with plenty of noise, which in turn mislead"
        },
        {
          "Abstract": "the\ntraining of DL models. Given it\nis difficult\nto accurately determine\nanimals’"
        },
        {
          "Abstract": "emotions\nfrom humans’ perspective, we assume the dominant noise in neural data"
        },
        {
          "Abstract": "representing different emotions is the labeling error. Here, we report\nthe development"
        },
        {
          "Abstract": "and application of a neural decoding framework called Emo-Net\nthat consists of a"
        },
        {
          "Abstract": "confidence learning (CL) component and a DL component. The framework is fully"
        },
        {
          "Abstract": "data-driven and is capable of decoding emotions from multiple datasets obtained from"
        },
        {
          "Abstract": "behaving monkeys.\nIn\naddition\nto\nimproving\nthe\ndecoding\nability,\nEmo-Net"
        },
        {
          "Abstract": "significantly\nimproves\nthe\nperformance\nof\nthe\nbase DL models, making\nemotion"
        },
        {
          "Abstract": "recognition in animal models possible. In summary, this framework may inspire novel"
        },
        {
          "Abstract": "understandings of\nthe neural basis of emotion and drive the realization of close-loop"
        },
        {
          "Abstract": "emotional brain-computer interfaces."
        },
        {
          "Abstract": "confidence\nlearning,\nemotion decoding, brain-computer\nKeywords: deep learning,"
        },
        {
          "Abstract": "interface, Emo-Net"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Introduction": "Decoding\nemotions\nfrom neural\nsignals\nhas\nlong\nbeen\npopular\nin\nthe\nfields\nof"
        },
        {
          "Introduction": "computational science, neuroscience, and psychology (Glaser et al., 2020; Wan et al.,"
        },
        {
          "Introduction": "2021). Traditionally,\nto enable the quantitative analysis and computational modeling"
        },
        {
          "Introduction": "of\nemotions,\nthe\nneural\nrepresentation\nof\nemotions\nis\nusually\ngenerated\nthrough"
        },
        {
          "Introduction": "cognitive and electrophysiological experiments. Recent advances\nin brain-computer"
        },
        {
          "Introduction": "interface\n(BCI)\nfurther\nenable\nus\nto\ncollect\nneural\nsignals\nfrom a\nbroader\nscale,"
        },
        {
          "Introduction": "enriching the foundation for establishing computational models (Onken et al., 2016;"
        },
        {
          "Introduction": "Shanechi, 2019; Degenhart et al., 2020; Steinmetz et al., 2021)."
        },
        {
          "Introduction": "Currently, BCIs can be classified as invasive (IBCIs) and non-invasive (Non-IBCIs)."
        },
        {
          "Introduction": "Generally,\nIBCIs can collect\nsignals from the deeper structure and generate cleaner"
        },
        {
          "Introduction": "signals than Non-IBCIs,\nthus can generate better accuracy when being used in neural"
        },
        {
          "Introduction": "decoding (e.g., motor\nimagination and language\nrecovery)\n(Anumanchipalli\net\nal.,"
        },
        {
          "Introduction": "2018; Moses et al., 2018; Heelan et al., 2019; Gallego et al., 2020; Makin et al., 2020;"
        },
        {
          "Introduction": "Yu et al., 2020; Zhang et al., 2020a; Zhou et al., 2020; Tankus et al., 2021; Wen et al.,"
        },
        {
          "Introduction": "2021). However, due\nto technological\nchallenges, many matured BCI\napplications"
        },
        {
          "Introduction": "aiming to decode human emotions are Non-IBCIs-based (e.g., electroencephalogram,"
        },
        {
          "Introduction": "EEG) (Li et al., 2017; Yang et al., 2018; Hasan and Kim, 2019; Li et al., 2020; Luo et"
        },
        {
          "Introduction": "al., 2020; He et al., 2022). Given the advantages in signal quality, it remains intriguing"
        },
        {
          "Introduction": "to\ndevelop\nan\nIBCIs-based\nemotion\ndecoder,\nespecially\nfrom those\ndiscrete\nbut"
        },
        {
          "Introduction": "informative spike trains (Shanechi, 2019)."
        },
        {
          "Introduction": "Among machine\nlearning\nstrategies\n(Xu\net\nal.,\n2021),\nsome\ndeep\nlearning\n(DL)"
        },
        {
          "Introduction": "models\nhave\nbeen\nperforming well\nin\nnatural\nlanguage-based\n(e.g.\ntext)\nemotion"
        },
        {
          "Introduction": "recognition. For example, by assigning neural data as texts of \"brain activity\" pieces,"
        },
        {
          "Introduction": "some\nDL models\nachieve\nup\nto\n95% accuracy\non many\nclassification\nand"
        },
        {
          "Introduction": "reconstruction tasks (Du et al., 2019; Zhang et al., 2019; Tankus et al., 2021; Ninenko"
        },
        {
          "Introduction": "et\nal., 2022; Wu et\nal., 2022). However, DL models\nthat perform well\nin neural"
        },
        {
          "Introduction": "responses-based emotion classification are\nstill missing, partially due\nto the\nlarge"
        },
        {
          "Introduction": "uncertainty inhered in emotion-related neural signals. Because of\nthe complexity of"
        },
        {
          "Introduction": "cognitive functions, emotion-related neural data normally consist of a higher order of"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "emotion in subjects may not be consistently matched all\nthe time. In addition, a piece"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "of neural signals may encode multiple emotions."
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "On the other hand, IBCIs normally collect data from animals, which are not capable"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "of verbal communication (Dai, 2021). Therefore,\nthere are inevitable errors within the"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "definition of these subjects’ emotions estimated by people, namely “labeling errors”,"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "which are difficult to correct or quantify. To identify incorrect\nlabels, methods such as"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "confident\nlearning\n(CL), which\nis\na model-agnostic\ntheory\nand\nalgorithm for"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "identifying labeling errors, have been proposed (Northcutt et al., 2021). Additionally,"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "animal experiments normally do not allow unlimited data collection, making the data"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "samples not\nlarge enough for DL model\ntraining. However,\nit\nis not\nideal\nto amplify"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "data either as the labeling errors may spread or even amplify unexpectedly. Overall,"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "further\nrefined\nstrategies\nare\nrequired\nto\nsolve\nthese\nissues\nand\nimprove\nthe"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "performance of emotional BCIs."
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "In this study, based on investigating the relations between labels, stimuli, emotions,"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "and neural\nresponses, we present a hybrid framework that combines CL and DL for"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "spike\ndecoding\nof\nemotions\nfrom neural\nresponses\nobtained\nfrom IBCIs. This"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "framework is fully data-driven, with no special\nrequirements for models involved in"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "the\ntraining.\nIn\nthe\nfollowing, we\ndemonstrate\nthe\ndecoding\ncapability\nof\nthis"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "framework, its ability to perform end-to-end mapping of neural responses to emotions,"
        },
        {
          "noises\nthan image or\ntext data.\nIn particular,\nthe emotion stimulus and the evoked": "and its robustness to neural data obtained from IBCIs."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: The illustration of\nthe monkey experiment acquiring emotion-evoked": "(A) The experimental\nneural\nsignals using an invasive brain-machine interface."
        },
        {
          "Figure 1: The illustration of\nthe monkey experiment acquiring emotion-evoked": "paradigm.\n(B-C) The preprocessing of spiking signals and a single trial sample used"
        },
        {
          "Figure 1: The illustration of\nthe monkey experiment acquiring emotion-evoked": "for establishing datasets."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "Non-human primate experiments and neural response dataset"
        },
        {
          "Methods": "One\nadult male macaque monkey weighing\n7.5\nkg was\nused\nin\nthe\nstudy. All"
        },
        {
          "Methods": "experimental procedures were\napproved by the Institutional Animal Care\nand Use"
        },
        {
          "Methods": "Committee\n(IACUC)\nat\nShenzhen\nInstitutes\nof Advanced\nTechnology, Chinese"
        },
        {
          "Methods": "Academy of Sciences following the guidelines stated in the Guide for Care and Use of"
        },
        {
          "Methods": "Laboratory Animals (Eighth Edition, 2011)."
        },
        {
          "Methods": "To obtain\nneural\nresponses\nfrom this\nexperimental monkey,\na\nrecording\nchamber"
        },
        {
          "Methods": "(Form-fitting,\nPEEK)\nand\na micro-drive\n(SC32-42mm,\nboth\nfrom Gray Matter"
        },
        {
          "Methods": "Research, Montana) were\nimplanted\nabove\nthe\namygdala\nfollowing\nthe\nproduct"
        },
        {
          "Methods": "manual\n(Dotson et al., 2017). During the experiment,\nthe monkey was trained to sit"
        },
        {
          "Methods": "quietly in a chair with the head fixed. Visual\nstimuli were presented on a monitor"
        },
        {
          "Methods": "(VG248, ASUS) 57 cm in front of\nthe subject. An eye tracker\n(iView X Hi-Speed"
        },
        {
          "Methods": "Primate, SMI) was\nused\nto monitor\nthe monkey’s\neye\nposition. A Matlab-based"
        },
        {
          "Methods": "toolbox MonkeyLogic (NIMH) was used for experimental control."
        },
        {
          "Methods": "Visual\nstimuli were\nfaces\nof\ndifferent\nspecies\n(human\nand monkey),\ndifferent"
        },
        {
          "Methods": "emotions\n(negative, positive, and neutral), and different\nspatial\nfrequencies\n(broad,"
        },
        {
          "Methods": "high,\nlow, and a mix of high and low). Each stimulus was presented for 800 ms after"
        },
        {
          "Methods": "the monkey acquired fixation for 500 ms (Figure 1A)."
        },
        {
          "Methods": "A 128-channel electrophysiological\nrecording system (OmniPlex, Plexon Inc.) was"
        },
        {
          "Methods": "used to monitor and record neural activities. Signals were filtered between 250 Hz and"
        },
        {
          "Methods": "5 kHz to identify spiking activity."
        },
        {
          "Methods": "Spikes were sorted offline using Kilosorter\nto identify single units. Specifically, we"
        },
        {
          "Methods": "extracted spiking activities\nfor each unit\nfrom 200 ms before to 1368 ms after\nthe"
        },
        {
          "Methods": "presentation of facial stimuli, then calculated the firing rate using a sliding window of"
        },
        {
          "Methods": "48 ms and a step size of 16 ms (Figure 1B),\nfrom which a firing rate vector with a"
        },
        {
          "Methods": "length of 96 was generated. The firing rate vectors were then arranged according to"
        },
        {
          "Methods": "the position of the recording electrode, yielding a 64×96 matrix of firing rates (Figure"
        },
        {
          "Methods": "1C), which was\nstandardized and added a Gaussian noise with a mean of 1 and a"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "Type of"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "classification"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "Triple"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "classification"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "Binary"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "Classification"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": ""
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "Triple"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "classification"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": ""
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "Triple"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "classification"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": ""
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "Triple"
        },
        {
          "Table 1: A brief introduction of neural datasets used in the current study.": "classification"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "evoked neural datasets."
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "The uncertainty of neural response samples"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "An emotion-evoked neural response sample with a response X and a label Y has two"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "kinds\nof\naleatoric\nuncertainty. One\ncomes\nfrom the measurement\nof\nthe\nneural"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "response X. and the other comes from the label Y. In supervised learning,\nlabels are"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "used to train models, yet noises in labels increase the outcome uncertainty."
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "As\nillustrated\nin Figure\n2,\nsince\nvisual\nstimuli\ncannot\nconsistently\nelicit\nspecific"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "emotional\nstates\nin experimental animals, and it\nis difficult\nto accurately correlate"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "each neural sequence with a specific emotion it\nleads to a significant number of label"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "errors in such emotion-evoked neural data. These label errors can be divided into two"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "types:\nstimuli\nfailing to elicit\nthe\nintended emotions and stimuli\neliciting multiple"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "emotions. The first\ntype of error represents a clear case of mislabeling, which should"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "be promptly corrected. The second category involves multi-label learning and can still"
        },
        {
          "Figure 2: An illustration of the generation of label errors in establishing emotion-": "be used for training, although it is preferable to supplement such labels as necessary."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: An illustration of the Emo-Net framework. The training data is initially": "processed by a confident\nlearning component. This component utilizes an external"
        },
        {
          "Figure 3: An illustration of the Emo-Net framework. The training data is initially": "model θ2\nto estimate true labels and provide a ranked uncertainty, which is derived"
        },
        {
          "Figure 3: An illustration of the Emo-Net framework. The training data is initially": "from a combination of confidence thresholds and sparsity. After partitioning data,\nthe"
        },
        {
          "Figure 3: An illustration of the Emo-Net framework. The training data is initially": "weights of individual samples are re-adjusted and dirty data are pruned based on the"
        },
        {
          "Figure 3: An illustration of the Emo-Net framework. The training data is initially": "level of uncertainty at both individual and overall\nlevels. The cleaned data is then fed"
        },
        {
          "Figure 3: An illustration of the Emo-Net framework. The training data is initially": "into the deep learning classifier θ1\nto complete the remaining training process and"
        },
        {
          "Figure 3: An illustration of the Emo-Net framework. The training data is initially": "achieve emotion classification."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "these techniques": "source codes are publicly available on GitHub.",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "Algorithm-1: Prune/reweight training data",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "Input: training data X, training label Y, Emo-Net Net",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "# Compute Quality Matrix Q of Samples",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "Q ← Net.ComputeUncertainty(X, Y)",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "# Compute reweight factor /prune mask matrix W using Q according to functions (1)",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "and (2)",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "W ← ComputeWeightMatrix(Q)",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "# Select the minimum matrix w from W during every epoch",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        },
        {
          "these techniques": "for (x, y, w) in train epochs of (X, Y, W) do",
          "to reweigh the loss or prune the training data using PyTorch. All": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "W ← ComputeWeightMatrix(Q)": "# Select the minimum matrix w from W during every epoch"
        },
        {
          "W ← ComputeWeightMatrix(Q)": "for (x, y, w) in train epochs of (X, Y, W) do"
        },
        {
          "W ← ComputeWeightMatrix(Q)": "g ← 0"
        },
        {
          "W ← ComputeWeightMatrix(Q)": "y_hat = net(x)"
        },
        {
          "W ← ComputeWeightMatrix(Q)": "Compute g of (y_hat,y) according to function (3)"
        },
        {
          "W ← ComputeWeightMatrix(Q)": "g = w*g"
        },
        {
          "W ← ComputeWeightMatrix(Q)": "Update parameters of Net with g"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 1: ) (Experiment 1) and presented the average accuracies and F1-",
      "data": [
        {
          "of which, p is the real probability density function, p(xij)\nis 0 or 1, and q(xij)\nis the": "predicted probability given by model θ."
        },
        {
          "of which, p is the real probability density function, p(xij)\nis 0 or 1, and q(xij)\nis the": "Finally, the DL model θ is employed to learn the neural representations of the induced"
        },
        {
          "of which, p is the real probability density function, p(xij)\nis 0 or 1, and q(xij)\nis the": "emotions and complete\nthe classification of neural\nresponses\nto emotions,\nthereby"
        },
        {
          "of which, p is the real probability density function, p(xij)\nis 0 or 1, and q(xij)\nis the": "achieving emotion recognition. The learning rate of the DL model\nis uniformly set\nto"
        },
        {
          "of which, p is the real probability density function, p(xij)\nis 0 or 1, and q(xij)\nis the": "0.001, and the batch size is set to 256."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 2: ). In Experiment 2, models with the CLP-",
      "data": [
        {
          "signal-based emotion decoding.": "Dataset-1"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "Accura\nF1-"
        },
        {
          "signal-based emotion decoding.": "cy\nscore"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "0.34404"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "34.46%"
        },
        {
          "signal-based emotion decoding.": "(0.0114)"
        },
        {
          "signal-based emotion decoding.": "0.34791"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "34.90%"
        },
        {
          "signal-based emotion decoding.": "(0.0162)"
        },
        {
          "signal-based emotion decoding.": "0.49427"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "50.01%"
        },
        {
          "signal-based emotion decoding.": "(0.0129)"
        },
        {
          "signal-based emotion decoding.": "0.48979"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "49.13%"
        },
        {
          "signal-based emotion decoding.": "(0.0077)"
        },
        {
          "signal-based emotion decoding.": "0.49667"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "50.05%"
        },
        {
          "signal-based emotion decoding.": "(0.0185)"
        },
        {
          "signal-based emotion decoding.": "0.34418"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "34.82%"
        },
        {
          "signal-based emotion decoding.": "(0.0057)"
        },
        {
          "signal-based emotion decoding.": "0.34385"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "34.72%"
        },
        {
          "signal-based emotion decoding.": "(0.0055)"
        },
        {
          "signal-based emotion decoding.": "0.35484"
        },
        {
          "signal-based emotion decoding.": ""
        },
        {
          "signal-based emotion decoding.": "36.39%"
        },
        {
          "signal-based emotion decoding.": "(0.0133)"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 2: The performance of Emo-Net in ten-fold cross-validations on five neural",
      "data": [
        {
          "MobileNet": "",
          "0.47718": "",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "47.64%",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "(Baseline)",
          "0.47718": "(0.0102)",
          "0.62649": "(0.0057)",
          "0.49733": "(0.0073)",
          "0.42812": "(0.0077)",
          "0.43436": "(0.0101)"
        },
        {
          "MobileNet": "MobileNet",
          "0.47718": "0.47981",
          "0.62649": "0.61817",
          "0.49733": "0.49369",
          "0.42812": "0.43438",
          "0.43436": "0.43934"
        },
        {
          "MobileNet": "",
          "0.47718": "",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "48.18%",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "(Emo-R)",
          "0.47718": "(0.0085)",
          "0.62649": "(0.0085)",
          "0.49733": "(0.0068)",
          "0.42812": "(0.0098)",
          "0.43436": "(0.0094)"
        },
        {
          "MobileNet": "MobileNet",
          "0.47718": "0.48604",
          "0.62649": "0.62361",
          "0.49733": "0.51375",
          "0.42812": "0.38222",
          "0.43436": "0.38593"
        },
        {
          "MobileNet": "",
          "0.47718": "",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "48.81%",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "(Emo-P)",
          "0.47718": "(0.0126)",
          "0.62649": "(0.0116)",
          "0.49733": "(0.0122)",
          "0.42812": "(0.0153)",
          "0.43436": "(0.0173)"
        },
        {
          "MobileNet": "ResNet18",
          "0.47718": "0.47864",
          "0.62649": "0.66245",
          "0.49733": "0.53843",
          "0.42812": "0.44112",
          "0.43436": "0.46083"
        },
        {
          "MobileNet": "",
          "0.47718": "",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "47.94%",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "(Baseline)",
          "0.47718": "(0.0069)",
          "0.62649": "(0.0086)",
          "0.49733": "(0.0099)",
          "0.42812": "(0.0059)",
          "0.43436": "(0.0140)"
        },
        {
          "MobileNet": "ResNet18(",
          "0.47718": "0.48915",
          "0.62649": "0.6588",
          "0.49733": "0.5407",
          "0.42812": "0.44842",
          "0.43436": "0.45869"
        },
        {
          "MobileNet": "",
          "0.47718": "",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "49.01%",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "Emo-R)",
          "0.47718": "(0.0070)",
          "0.62649": "(0.0093)",
          "0.49733": "(0.0183)",
          "0.42812": "(0.0145)",
          "0.43436": "(0.0132)"
        },
        {
          "MobileNet": "ResNet18(",
          "0.47718": "0.48745",
          "0.62649": "0.66713",
          "0.49733": "0.54456",
          "0.42812": "0.38715",
          "0.43436": "0.39831"
        },
        {
          "MobileNet": "",
          "0.47718": "",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "48.90%",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "Emo-P)",
          "0.47718": "(0.0191)",
          "0.62649": "(0.0134)",
          "0.49733": "(0.0202)",
          "0.42812": "(0.0129)",
          "0.43436": "(0.0145)"
        },
        {
          "MobileNet": "",
          "0.47718": "0.3415",
          "0.62649": "0.50767",
          "0.49733": "0.34428",
          "0.42812": "0.35767",
          "0.43436": "0.34713"
        },
        {
          "MobileNet": "SID",
          "0.47718": "",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "34.41%",
          "0.62649": "",
          "0.49733": "",
          "0.42812": "",
          "0.43436": ""
        },
        {
          "MobileNet": "",
          "0.47718": "(0.0070)",
          "0.62649": "(0.0160)",
          "0.49733": "(0.0092)",
          "0.42812": "(0.0128)",
          "0.43436": "(0.0113)"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 3: The performance of Emo-Net tested on five datasets. “Emo-R” means a",
      "data": [
        {
          "Datasets": "/Models",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "Accur",
          "Dataset-2": "Accur",
          "Dataset-3": "Accur",
          "Dataset-4": "Accur",
          "Dataset-5": "F1-"
        },
        {
          "Datasets": "",
          "Dataset-1": "acy",
          "Dataset-2": "acy",
          "Dataset-3": "acy",
          "Dataset-4": "acy",
          "Dataset-5": "score"
        },
        {
          "Datasets": "RF",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "33.32%",
          "Dataset-2": "45.66%",
          "Dataset-3": "35.33%",
          "Dataset-4": "28.02%",
          "Dataset-5": "0.2703"
        },
        {
          "Datasets": "(Baseline)",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "RF",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "51.38%",
          "Dataset-2": "70.40%",
          "Dataset-3": "63.45%",
          "Dataset-4": "56.70%",
          "Dataset-5": "0.6459"
        },
        {
          "Datasets": "（Emo-P）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "MLP",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "56.38%",
          "Dataset-2": "74.44%",
          "Dataset-3": "64.35%",
          "Dataset-4": "51.95%",
          "Dataset-5": "0.5050"
        },
        {
          "Datasets": "(Baseline)",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "MLP",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "76.56%",
          "Dataset-2": "92.02%",
          "Dataset-3": "86.81%",
          "Dataset-4": "78.51%",
          "Dataset-5": "0.8236"
        },
        {
          "Datasets": "（Emo-P）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "MLP",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "64.71%",
          "Dataset-2": "87.44%",
          "Dataset-3": "70.31%",
          "Dataset-4": "66.41%",
          "Dataset-5": "0.6599"
        },
        {
          "Datasets": "（Emo-R）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "VGG16",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "34.11%",
          "Dataset-2": "51.22%",
          "Dataset-3": "37.30%",
          "Dataset-4": "35.93%",
          "Dataset-5": "0.3413"
        },
        {
          "Datasets": "(Baseline)",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "VGG16",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "37.76%",
          "Dataset-2": "52.02%",
          "Dataset-3": "40.82%",
          "Dataset-4": "36.58%",
          "Dataset-5": "0.4251"
        },
        {
          "Datasets": "（Emo-P）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "VGG16",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "38.41%",
          "Dataset-2": "53.34%",
          "Dataset-3": "39.94%",
          "Dataset-4": "39.32%",
          "Dataset-5": "0.4251"
        },
        {
          "Datasets": "（Emo-R）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "MobileNet",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "49.80%",
          "Dataset-2": "63.78%",
          "Dataset-3": "56.25%",
          "Dataset-4": "47.01%",
          "Dataset-5": "0.4593"
        },
        {
          "Datasets": "(Baseline)",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "MobileNet",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "51.56%",
          "Dataset-2": "65.18%",
          "Dataset-3": "55.85%",
          "Dataset-4": "47.13%",
          "Dataset-5": "0.4568"
        },
        {
          "Datasets": "(Emo-P)",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "MobileNet",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "54.82%",
          "Dataset-2": "68.69%",
          "Dataset-3": "59.47%",
          "Dataset-4": "50.91%",
          "Dataset-5": "0.4987"
        },
        {
          "Datasets": "(Emo-R)",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "ResNet18",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "55.00%",
          "Dataset-2": "70.40%",
          "Dataset-3": "64.50%",
          "Dataset-4": "51.00%",
          "Dataset-5": "0.4949"
        },
        {
          "Datasets": "(Baseline)",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "ResNet18",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "66.00%",
          "Dataset-2": "85.88%",
          "Dataset-3": "64.80%",
          "Dataset-4": "68.75%",
          "Dataset-5": "0.7373"
        },
        {
          "Datasets": "（Emo-P）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "ResNet18",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "56.97%",
          "Dataset-2": "77.46%",
          "Dataset-3": "65.33%",
          "Dataset-4": "57.42%",
          "Dataset-5": "0.5470"
        },
        {
          "Datasets": "（Emo-R）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "SID",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        },
        {
          "Datasets": "",
          "Dataset-1": "36.78%",
          "Dataset-2": "53.23%",
          "Dataset-3": "31.83%",
          "Dataset-4": "37.63%",
          "Dataset-5": "0.4251"
        },
        {
          "Datasets": "（Baseline）",
          "Dataset-1": "",
          "Dataset-2": "",
          "Dataset-3": "",
          "Dataset-4": "",
          "Dataset-5": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "using Emo-Net with training data being pruned. The F1 value is calculated using the"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "“macro”. Results that outperformed the baseline are highlighted in bold, and the best"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "and the second-best results are shown with underlines."
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "Ablation Experiments"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "To further examine the contribution of the CLP in the Emo-Net framework in neural"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "response-based emotion decoding, we\nconducted an ablation test by incrementally"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "increasing the proportion of correctly-labeled samples in the training data. The tests"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "were carried out on the independent\ntest sets in Experiment 2 using the combination"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "of Emo-Net, ResNet-18, and train data pruning, with a learning rate of 0.001, batch"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "size of 256, and epochs of 200. The results on five datasets are presented in Figure"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "4A-E, which\nshows\nthat\nthe\naccuracy\nof Emo-Net\ngradually\nincreased\nover\nthe"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "baseline as the ratio of noise-pruned data increased."
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "Further, to verify the necessity of reducing label uncertainty in emotion-evoked neural"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "data, we conducted classification tests on a kinematics dataset\n(Zhang et al., 2019)."
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "Compared with the emotional dataset, kinematics-related data are typical \"heart-to-"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "hand\", where movement\nintentions can be clearly revealed in behaviors and rarely be"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "mislabeled. As\nshown in Figure 4F, after pruning with CLP,\nthe model's decoding"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "performance for movement\nintentions was only slightly improved compared with the"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "baseline (blue vs. yellow). As the proportion of pruned data increased,\nthe negative"
        },
        {
          "model using the Emo-Net\nframework with loss\nreweight. “Emo-P” means a model": "impact enlarged and resulted in a gradual decrease in performing accuracy."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: Results of": "represents the pruning ratio and the y-axis represents the model accuracy. The blue",
          "the ablation experiments on different datasets. The x-axis": ""
        },
        {
          "Figure 4: Results of": "line shows\nthe classification accuracy of",
          "the ablation experiments on different datasets. The x-axis": "the model after pruning the training data"
        },
        {
          "Figure 4: Results of": "using CLP,\nthe\nyellow line",
          "the ablation experiments on different datasets. The x-axis": "shows\nthe\nrecognition\naccuracy\nof\nthe model\nafter"
        },
        {
          "Figure 4: Results of": "randomly removing the",
          "the ablation experiments on different datasets. The x-axis": "same proportion of\ntraining data\n(baseline),\nand the gray"
        },
        {
          "Figure 4: Results of": "rectangle denotes\nthe",
          "the ablation experiments on different datasets. The x-axis": "improvement of\naccuracies over\nthe baseline.\n(A-E) Results"
        },
        {
          "Figure 4: Results of": "tested on datasets 1-5. (F) Results tested on a movement intention dataset.",
          "the ablation experiments on different datasets. The x-axis": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Discussion": "In this\nstudy, we present a framework called Emo-Net\nto recognize emotions\nfrom"
        },
        {
          "Discussion": "neural data collected from monkeys.\nIn addition to achieving initial\nrecognition of"
        },
        {
          "Discussion": "animal emotions at high accuracy, Emo-Net shows an even better performance after"
        },
        {
          "Discussion": "optimizing the uncertainty in raw data. These together demonstrate the efficacy of the"
        },
        {
          "Discussion": "Emo-Net in neural responses-based emotion decoding."
        },
        {
          "Discussion": "The importance of CL in DL-based emotion decoding"
        },
        {
          "Discussion": "DL models alone perform poorly in the neural decoding of emotions as neither simple"
        },
        {
          "Discussion": "MLP nor convolutional models can generate satisfying baseline results.\nIn contrast,"
        },
        {
          "Discussion": "the performance of the Emo-Net framework indicates that, when combined with CL, a"
        },
        {
          "Discussion": "DL model outperforms the baseline in all decoding capability tests. On the other way"
        },
        {
          "Discussion": "round,\nthe decoding capability of the Emo-Net framework was impaired significantly"
        },
        {
          "Discussion": "in the absence of CLP. Therefore,\nthe employment of CLP is essential\nfor enhancing"
        },
        {
          "Discussion": "the decoding capability of Emo-Net."
        },
        {
          "Discussion": "From a structural perspective,\nthe effectiveness of\nthe CL+DL combination can be"
        },
        {
          "Discussion": "attributed to the robustness provided by CLP, which enables the DL model to perform"
        },
        {
          "Discussion": "effectively. Specifically,\nemotion-evoked neural\nresponse\nsamples\ncommonly have"
        },
        {
          "Discussion": "inaccurate\nor multiple\nlabels, which\nleads\nto\ninaccurate\ndecoding when\nrunning"
        },
        {
          "Discussion": "traditional DL models.\nIn this study, we addressed this issue with two optimization"
        },
        {
          "Discussion": "strategies, named “data pruning” and “loss reweight”. The first strategy assigns zero"
        },
        {
          "Discussion": "to the loss of uncertain samples, which effectively eliminates the impact of inaccurate"
        },
        {
          "Discussion": "labels\nand\ndiscards\nall\ninformation from unreliable\nsamples. The\nsecond\nstrategy"
        },
        {
          "Discussion": "incorporates\nthe multiple-label\ninformation\npresented\nin\nunreliable\nsamples. Both"
        },
        {
          "Discussion": "methods reduce aleatoric uncertainty and decrease the sample size while leading to an"
        },
        {
          "Discussion": "increase in epistemic uncertainty. In the context of emotion decoding,\nthe benefits of"
        },
        {
          "Discussion": "reducing aleatoric uncertainty outweigh the potential negative\neffects of\nincreased"
        },
        {
          "Discussion": "epistemic uncertainty, which underlines the effectiveness of the CL+DL combination."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data-Centered Optimization for neural decoding": "Previously,\nto enhance the decoding performance, most neural decoding frameworks"
        },
        {
          "Data-Centered Optimization for neural decoding": "attempted to explore more powerful, biomimetic, and robust structures in their models"
        },
        {
          "Data-Centered Optimization for neural decoding": "(Zhang\net\nal.,\n2020a; Zhou\net\nal.,\n2020),\nor\nto\nfind more\ngeneral\noptimization"
        },
        {
          "Data-Centered Optimization for neural decoding": "strategies to improve the representation capabilities in all\ntasks (Onken et al., 2016;"
        },
        {
          "Data-Centered Optimization for neural decoding": "Degenhart et al., 2020; Wen et al., 2021). In contrast\nto these general approaches, we"
        },
        {
          "Data-Centered Optimization for neural decoding": "proposed\na\nstrategy\nto\nextract\noptimization\nalgorithms\nfrom a\ndata-centered"
        },
        {
          "Data-Centered Optimization for neural decoding": "perspective."
        },
        {
          "Data-Centered Optimization for neural decoding": "Our study indicates that,\nin neural decoding, having more training data is not always"
        },
        {
          "Data-Centered Optimization for neural decoding": "beneficial.\nProperly\nbalancing\nand\nremoving\nuncertain\ndata\ncan\nlead\nto\nbetter"
        },
        {
          "Data-Centered Optimization for neural decoding": "performance of\nthe model. Our method concentrates\nspecifically on the scenario of"
        },
        {
          "Data-Centered Optimization for neural decoding": "emotion recognition.\nIn other neural decoding scenarios, neural signal samples may"
        },
        {
          "Data-Centered Optimization for neural decoding": "have\ntheir\ncharacteristics.\nTherefore,\nin\nemotion-related\nresearch\nand\nthe"
        },
        {
          "Data-Centered Optimization for neural decoding": "implementation of emotional BCIs (eBCIs), in addition to pursuing the latest machine"
        },
        {
          "Data-Centered Optimization for neural decoding": "learning models, we\nshould\nalso\npay\nattention\nto optimization\nstrategies\nthat\nare"
        },
        {
          "Data-Centered Optimization for neural decoding": "characteristics-based."
        },
        {
          "Data-Centered Optimization for neural decoding": "Implication for BCIs"
        },
        {
          "Data-Centered Optimization for neural decoding": "Although\nit\nis widely\naccepted\nthat\nIBCIs\nare more\naccurate\nin\nterms\nof\nintent"
        },
        {
          "Data-Centered Optimization for neural decoding": "recognition, under current ethical and experimental constraints, spiking activity-based"
        },
        {
          "Data-Centered Optimization for neural decoding": "DL models perform poorly in emotion classification. Before IBCIs can be effectively"
        },
        {
          "Data-Centered Optimization for neural decoding": "applied to humans\nfor\nemotion decoding,\na neural decoding method,\nespecially a"
        },
        {
          "Data-Centered Optimization for neural decoding": "spiking-based approach that\nis highly robust\nto uncertain data,\nis urgently required to"
        },
        {
          "Data-Centered Optimization for neural decoding": "be developed based on animal experiments."
        },
        {
          "Data-Centered Optimization for neural decoding": "Our framework fills in this gap using current hardware and experimental settings and"
        },
        {
          "Data-Centered Optimization for neural decoding": "enables efficient emotion classification based on spike decoding. Furthermore,\nthe DL"
        },
        {
          "Data-Centered Optimization for neural decoding": "component of this framework can be considered as a general module since its concept"
        },
        {
          "Data-Centered Optimization for neural decoding": "and\nimplementation\nare\nstraightforward,\nthus\nallowing\ngreat\nflexibility\nin model"
        },
        {
          "Data-Centered Optimization for neural decoding": "developments. The CL component\nis model-free\nand is\nresponsible\nfor providing"
        },
        {
          "Data-Centered Optimization for neural decoding": "robustness to the overall system. When combined with a DL model, an efficient and"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "IBCIs studies."
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "Limitations and future studies"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "Our study focuses on optimizing the uncertainty inhered in neural signals that encode"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "emotion-related\ninformation. However,\na more\nin-depth\nexamination\nof\nthe\ntwo"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "optimization strategies employed in Emo-Net (loss reweight vs. data pruning) has not"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "been conducted. Additionally,\nthe validation of the Emo-Net was only conducted in 5"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "datasets established by ourselves. Further\ntests should be performed on other\nthird-"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "party datasets to validate the transferability. Lastly, our evaluation of\nthe framework"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "was limited to its decoding capability. An extension to assess the encoding capability"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "is preferable in the future."
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "To conclude, our study shows that in the neural decoding of emotions, the application"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "of CLP in balancing uncertainty can yield an overperformance for\nthe entire system."
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "The hybrid\nframework of CL+DL demonstrates\nsuperior performance\nin emotion"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "classification and\nshows preliminary\ncapability in identifying\nanimal\nemotions\nin"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "simple\nscenarios. Therefore, Emo-Net\nis\na promising framework that may inspire"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "novel understandings of the neural basis of emotion and drive the realization of close-"
        },
        {
          "versatile CL component\nis\na\nsignificant\nsupplement\nin promoting emotion-related": "loop eBCIs in the future."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "Anumanchipalli, G.K., Chartier,\nJ., and Chang, E.F.\n(2018).\nIntelligible speech synthesis\nfrom neural"
        },
        {
          "References": "decoding of spoken sentences. Cold Spring Harbor Laboratory."
        },
        {
          "References": "Dai, J. (2021). Gaining a better understanding of nonhuman primate research. Science Bulletin 66(15),"
        },
        {
          "References": "1499-1501. doi: 10.1016/j.scib.2021.02.021."
        },
        {
          "References": "Degenhart, A.D., Bishop, W.E., Oby, E.R., Tyler-Kabara, E.C., Chase, S.M., Batista, A.P., et al.\n(2020)."
        },
        {
          "References": "Stabilization of a brain-computer\ninterface via the alignment of\nlow-dimensional spaces of"
        },
        {
          "References": "neural activity. Nat Biomed Eng 4(7), 672-685. doi: 10.1038/s41551-020-0542-9."
        },
        {
          "References": "Dotson, N.M., Hoffman, S.J., Goodell, B., and Gray, C.M. (2017). A Large-Scale Semi-Chronic Microdrive"
        },
        {
          "References": "Neuron\nRecording\nSystem\nfor\nNon-Human\nPrimates.\n769-782\ne762.\ndoi:\n96(4),"
        },
        {
          "References": "10.1016/j.neuron.2017.09.050."
        },
        {
          "References": "Du, C., Du, C., Huang, L., and He, H.\n(2019). Reconstructing Perceived Images From Human Brain"
        },
        {
          "References": "Activities With Bayesian Deep Multiview Learning.\nIEEE Trans Neural Netw Learn Syst 30(8),"
        },
        {
          "References": "2310-2323. doi: 10.1109/TNNLS.2018.2882456."
        },
        {
          "References": "Gallego, J.A., Perich, M.G., Chowdhury, R.H., Solla, S.A., and Miller, L.E. (2020). Long-term stability of"
        },
        {
          "References": "cortical population dynamics underlying consistent behavior. Nature Neuroscience 23(2), 260-"
        },
        {
          "References": "270."
        },
        {
          "References": "Glaser,\nJ.I., Benjamin, A.S., Chowdhury, R.H., Perich, M.G., Miller,\nL.E.,\nand Kording, K.P.\n(2020)."
        },
        {
          "References": "Machine Learning for Neural Decoding. eNeuro 7(4). doi: 10.1523/ENEURO.0506-19.2020."
        },
        {
          "References": "Hasan, M.J., and Kim,\nJ.M.\n(2019). A Hybrid Feature Pool-Based Emotional Stress State Detection"
        },
        {
          "References": "Algorithm Using EEG Signals. Brain Sciences 9(12)."
        },
        {
          "References": "He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep Residual Learning for Image Recognition. IEEE."
        },
        {
          "References": "He, Z., Zhong, Y., and Pan, J. (2022). An adversarial discriminative temporal convolutional network for"
        },
        {
          "References": "Comput\nBiol Med\nEEG-based\ncross-domain\nemotion\nrecognition.\n105048.\ndoi:\n141,"
        },
        {
          "References": "10.1016/j.compbiomed.2021.105048."
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "from spike-based neural population recordings in secondary auditory cortex of non-human"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "primates. Communications biology 2, 466."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., et al.\n(2017). MobileNets:"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Efficient Convolutional Neural Networks for Mobile Vision Applications."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Li, J., Qiu, S., Shen, Y., Liu, C., and He, H. (2020). Multisource Transfer Learning for Cross-Subject EEG"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Emotion Recognition. IEEE transactions on cybernetics 50(7), 3281-3293."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Li, J., Zhang, Z., and He, H. (2017). Hierarchical Convolutional Neural Networks for EEG-Based Emotion"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Recognition. Cognitive Computation 10(2), 368-380. doi: 10.1007/s12559-017-9533-x."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Luo, Y., Zhu, L.Z., Wan, Z.Y., and Lu, B.L. (2020). \"Data Augmentation for Enhancing EEG-based Emotion"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Recognition with Deep Generative Models\".)."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Makin, J.G., Moses, D.A., and Chang, E.F. (2020). Machine translation of cortical activity to text with an"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "encoder–decoder framework. Nature Neuroscience 23(4)."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Moses, D.A.,\nLeonard, M.K., and Chang, E.F.\n(2018). Real-time classification of auditory sentences"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Journal\nof\nNeural\nEngineering\nusing\nevoked\ncortical\nactivity\nin\nhumans.\n15(3),"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "036005.036001-036005.036009."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Ninenko,\nI., Kokorina, A., Egorov, M., Gupta, A., Weerakkodi Mudalige, N.D., Tsetserukou, D., et al."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "(2022). \"Novel method for lower limb rehabilitation based on brain-computer interface and"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "transcutaneous spinal cord electrical stimulation\",\nin: 2022 Fourth International Conference"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Neurotechnologies and Neurointerfaces (CNN).)."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Northcutt, C.G.,\nJiang,\nL., and Chuang,\nI.L.\n(2021). Confident\nLearning: Estimating Uncertainty\nin"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Dataset Labels. Journal of Artificial Intelligence Research 70, 1373-1411."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Onken, A., Liu,\nJ.K., Karunasekara, P.P., Delis,\nI., Gollisch, T., and Panzeri, S.\n(2016). Using Matrix and"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Tensor Factorizations for the Single-Trial Analysis of Population Spike Trains. PLoS Comput Biol"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "12(11), e1005189. doi: 10.1371/journal.pcbi.1005189."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Shanechi, M.M.\n(2019). Brain-machine interfaces from motor\nto mood. Nat Neurosci 22(10), 1554-"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "1564. doi: 10.1038/s41593-019-0488-y."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Simonyan, K., and Zisserman, A.\n(2014). Very Deep Convolutional Networks\nfor Large-Scale Image"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Recognition. Computer Science."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Steinmetz, N.A., Aydin, C., Lebedeva, A., Okun, M., Pachitariu, M., Bauza, M., et al. (2021). Neuropixels"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Science\n2.0:\nA miniaturized\nhigh-density\nprobe\nfor\nstable,\nlong-term brain\nrecordings."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "372(6539). doi: 10.1126/science.abf4588."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Tankus, A.,\nSolomon,\nL., Aharony,\nY.,\nFaust-Socher, A.,\nand Strauss,\nI.\n(2021). Machine\nlearning"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "algorithm for decoding multiple subthalamic spike trains for speech brain-machine interfaces."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "J Neural Eng 18(6). doi: 10.1088/1741-2552/ac3315."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Wan, Z., Yang, R., Huang, M., Zeng, N., and Liu, X. (2021). A review on transfer learning in EEG signal"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "analysis. Neurocomputing 421, 1-14."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Wen, S., Yin, A., Furlanello, T., Perich, M.G., Miller, L.E., and Itti, L. (2021). Rapid adaptation of brain-"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "computer\ninterfaces\nto new neuronal ensembles or participants via generative modelling."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Nat Biomed Eng. doi: 10.1038/s41551-021-00811-z."
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Wu, E.Q., Cao, Z., Xiong, P., Song, A., Zhu, L.-M., and Yu, M.\n(2022). Brain-Computer Interface Using"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Brain Power Map and Cognition Detection Network During Flight. IEEE/ASME Transactions on"
        },
        {
          "Heelan, C., Lee,\nJ., O'Shea, R., Lynch, L., Brandman, D., Truccolo, W., et al.\n(2019). Decoding speech": "Mechatronics 27(5), 3942-3952. doi: 10.1109/tmech.2022.3148141."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Innovation\n(Camb)\nparadigm\nfor\nscientific\nresearch.\n100179.\ndoi:\n2(4),"
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "10.1016/j.xinn.2021.100179."
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "Yang,",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Y., Wu, Q.M.J.,\nZheng, W.-L.,\nand Lu, B.-L.\n(2018).\nEEG-Based\nEmotion Recognition Using"
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "IEEE\nTransactions\non\nCognitive\nand\nHierarchical\nNetwork With\nSubnetwork\nNodes."
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Developmental Systems 10(2), 408-419. doi: 10.1109/tcds.2017.2685338."
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "Yu, Z.,",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Liu,\nJ.K.,\nJia,\nS., Zhang, Y., and Huang, T.\n(2020). Towards\nthe Next Generation of Retinal"
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Neuroprosthesis: Visual Computation with Spikes. 工程（英文）6(4), 13."
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "Zhang, R., Zong, Q., Dou, L., and Zhao, X. (2019). A novel hybrid deep learning scheme for four-class",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": ""
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "motor imagery classification. J Neural Eng 16(6), 066004. doi: 10.1088/1741-2552/ab3471."
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "Zhang, Y., Jia, S., Zheng, Y., Yu, Z., and Liu, J.K.",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "(2020a). Reconstruction of natural visual scenes from"
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "neural spikes with deep neural networks. Neural Networks 125."
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "Zhang, Y., Jia, S., Zheng, Y., Yu, Z., Tian, Y., Ma, S., et al. (2020b). Reconstruction of natural visual scenes",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": ""
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Neural\nNetw\nfrom\nneural\nspikes\nwith\ndeep\nneural\nnetworks.\n19-30.\ndoi:\n125,"
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "10.1016/j.neunet.2020.01.033."
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "Zhou, Q., Du, C., Li, D., Wang, H., Liu, J.K., and He, H. (2020). \"Simultaneous Neural Spike Encoding and",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": ""
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Joint\nDecoding Based on Cross-modal Dual Deep Generative Model\",\nin: 2020 International"
        },
        {
          "Xu, Y., Liu, X., Cao, X., Huang, C.,": "",
          "Liu, E., Qian, S., et al.\n(2021). Artificial\nintelligence: A powerful": "Conference on Neural Networks (IJCNN).)."
        }
      ],
      "page": 22
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Intelligible speech synthesis from neural decoding of spoken sentences",
      "authors": [
        "G Anumanchipalli",
        "J Chartier",
        "E Chang"
      ],
      "year": "2018",
      "venue": "Intelligible speech synthesis from neural decoding of spoken sentences"
    },
    {
      "citation_id": "2",
      "title": "Gaining a better understanding of nonhuman primate research",
      "authors": [
        "J Dai"
      ],
      "year": "2021",
      "venue": "Science Bulletin",
      "doi": "10.1016/j.scib.2021.02.021"
    },
    {
      "citation_id": "3",
      "title": "Stabilization of a brain-computer interface via the alignment of low-dimensional spaces of neural activity",
      "authors": [
        "A Degenhart",
        "W Bishop",
        "E Oby",
        "E Tyler-Kabara",
        "S Chase",
        "A Batista"
      ],
      "year": "2020",
      "venue": "Nat Biomed Eng",
      "doi": "10.1038/s41551-020-0542-9"
    },
    {
      "citation_id": "4",
      "title": "A Large-Scale Semi-Chronic Microdrive Recording System for Non-Human Primates",
      "authors": [
        "N Dotson",
        "S Hoffman",
        "B Goodell",
        "C Gray"
      ],
      "year": "2017",
      "venue": "Neuron",
      "doi": "10.1016/j.neuron.2017.09.050"
    },
    {
      "citation_id": "5",
      "title": "Reconstructing Perceived Images From Human Brain Activities With Bayesian Deep Multiview Learning",
      "authors": [
        "C Du",
        "C Du",
        "L Huang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Trans Neural Netw Learn Syst",
      "doi": "10.1109/TNNLS.2018.2882456"
    },
    {
      "citation_id": "6",
      "title": "Long-term stability of cortical population dynamics underlying consistent behavior",
      "authors": [
        "J Gallego",
        "M Perich",
        "R Chowdhury",
        "S Solla",
        "L Miller"
      ],
      "year": "2020",
      "venue": "Nature Neuroscience"
    },
    {
      "citation_id": "7",
      "title": "Machine Learning for Neural Decoding",
      "authors": [
        "J Glaser",
        "A Benjamin",
        "R Chowdhury",
        "M Perich",
        "L Miller",
        "K Kording"
      ],
      "year": "2020",
      "venue": "Machine Learning for Neural Decoding",
      "doi": "10.1523/ENEURO.0506-19.2020"
    },
    {
      "citation_id": "8",
      "title": "A Hybrid Feature Pool-Based Emotional Stress State Detection Algorithm Using EEG Signals",
      "authors": [
        "M Hasan",
        "J Kim"
      ],
      "year": "2019",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "9",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Deep Residual Learning for Image Recognition"
    },
    {
      "citation_id": "10",
      "title": "An adversarial discriminative temporal convolutional network for EEG-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Comput Biol Med",
      "doi": "10.1016/j.compbiomed.2021.105048"
    },
    {
      "citation_id": "11",
      "title": "Decoding speech from spike-based neural population recordings in secondary auditory cortex of non-human primates",
      "authors": [
        "C Heelan",
        "J Lee",
        "R O'shea",
        "L Lynch",
        "D Brandman",
        "W Truccolo"
      ],
      "year": "2019",
      "venue": "Communications biology"
    },
    {
      "citation_id": "12",
      "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand"
      ],
      "year": "2017",
      "venue": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
    },
    {
      "citation_id": "13",
      "title": "Multisource Transfer Learning for Cross-Subject EEG Emotion Recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y Shen",
        "C Liu",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "14",
      "title": "Hierarchical Convolutional Neural Networks for EEG-Based Emotion Recognition",
      "authors": [
        "J Li",
        "Z Zhang",
        "H He"
      ],
      "year": "2017",
      "venue": "Cognitive Computation",
      "doi": "10.1007/s12559-017-9533-x"
    },
    {
      "citation_id": "15",
      "title": "Data Augmentation for Enhancing EEG-based Emotion Recognition with Deep Generative Models",
      "authors": [
        "Y Luo",
        "L Zhu",
        "Z Wan",
        "B Lu"
      ],
      "year": "2020",
      "venue": "Data Augmentation for Enhancing EEG-based Emotion Recognition with Deep Generative Models"
    },
    {
      "citation_id": "16",
      "title": "Machine translation of cortical activity to text with an encoder-decoder framework",
      "authors": [
        "J Makin",
        "D Moses",
        "E Chang"
      ],
      "year": "2020",
      "venue": "Nature Neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Real-time classification of auditory sentences using evoked cortical activity in humans",
      "authors": [
        "D Moses",
        "M Leonard",
        "E Chang"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "18",
      "title": "Novel method for lower limb rehabilitation based on brain-computer interface and transcutaneous spinal cord electrical stimulation",
      "authors": [
        "I Ninenko",
        "A Kokorina",
        "M Egorov",
        "A Gupta",
        "N Weerakkodi Mudalige",
        "D Tsetserukou"
      ],
      "year": "2022",
      "venue": "2022 Fourth International Conference Neurotechnologies and Neurointerfaces (CNN)"
    },
    {
      "citation_id": "19",
      "title": "Confident Learning: Estimating Uncertainty in Dataset Labels",
      "authors": [
        "C Northcutt",
        "L Jiang",
        "I Chuang"
      ],
      "year": "2021",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "20",
      "title": "Using Matrix and Tensor Factorizations for the Single-Trial Analysis of Population Spike Trains",
      "authors": [
        "A Onken",
        "J Liu",
        "P Karunasekara",
        "I Delis",
        "T Gollisch",
        "S Panzeri"
      ],
      "year": "2016",
      "venue": "PLoS Comput Biol",
      "doi": "10.1371/journal.pcbi.1005189"
    },
    {
      "citation_id": "21",
      "title": "Brain-machine interfaces from motor to mood",
      "authors": [
        "M Shanechi"
      ],
      "year": "2019",
      "venue": "Nat Neurosci",
      "doi": "10.1038/s41593-019-0488-y"
    },
    {
      "citation_id": "22",
      "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Computer Science"
    },
    {
      "citation_id": "23",
      "title": "Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings",
      "authors": [
        "N Steinmetz",
        "C Aydin",
        "A Lebedeva",
        "M Okun",
        "M Pachitariu",
        "M Bauza"
      ],
      "year": "2021",
      "venue": "Science",
      "doi": "10.1126/science.abf4588"
    },
    {
      "citation_id": "24",
      "title": "Machine learning algorithm for decoding multiple subthalamic spike trains for speech brain-machine interfaces",
      "authors": [
        "A Tankus",
        "L Solomon",
        "Y Aharony",
        "A Faust-Socher",
        "I Strauss"
      ],
      "year": "2021",
      "venue": "J Neural Eng",
      "doi": "10.1088/1741-2552/ac3315"
    },
    {
      "citation_id": "25",
      "title": "A review on transfer learning in EEG signal analysis",
      "authors": [
        "Z Wan",
        "R Yang",
        "M Huang",
        "N Zeng",
        "X Liu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "26",
      "title": "Rapid adaptation of braincomputer interfaces to new neuronal ensembles or participants via generative modelling",
      "authors": [
        "S Wen",
        "A Yin",
        "T Furlanello",
        "M Perich",
        "L Miller",
        "L Itti"
      ],
      "year": "2021",
      "venue": "Nat Biomed Eng",
      "doi": "10.1038/s41551-021-00811-z"
    },
    {
      "citation_id": "27",
      "title": "Brain-Computer Interface Using Brain Power Map and Cognition Detection Network During Flight",
      "authors": [
        "E Wu",
        "Z Cao",
        "P Xiong",
        "A Song",
        "L.-M Zhu",
        "M Yu"
      ],
      "year": "2022",
      "venue": "IEEE/ASME Transactions on Mechatronics",
      "doi": "10.1109/tmech.2022.3148141"
    },
    {
      "citation_id": "28",
      "title": "Artificial intelligence: A powerful paradigm for scientific research",
      "authors": [
        "Y Xu",
        "X Liu",
        "X Cao",
        "C Huang",
        "E Liu",
        "S Qian"
      ],
      "year": "2021",
      "venue": "Innovation (Camb)",
      "doi": "10.1016/j.xinn.2021.100179"
    },
    {
      "citation_id": "29",
      "title": "EEG-Based Emotion Recognition Using Hierarchical Network With Subnetwork Nodes",
      "authors": [
        "Y Yang",
        "Q Wu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems",
      "doi": "10.1109/tcds.2017.2685338"
    },
    {
      "citation_id": "30",
      "title": "Towards the Next Generation of Retinal Neuroprosthesis: Visual Computation with Spikes",
      "authors": [
        "Z Yu",
        "J Liu",
        "S Jia",
        "Y Zhang",
        "T Huang"
      ],
      "year": "2020",
      "venue": "工程(英文)"
    },
    {
      "citation_id": "31",
      "title": "A novel hybrid deep learning scheme for four-class motor imagery classification",
      "authors": [
        "R Zhang",
        "Q Zong",
        "L Dou",
        "X Zhao"
      ],
      "year": "2019",
      "venue": "J Neural Eng",
      "doi": "10.1088/1741-2552/ab3471"
    },
    {
      "citation_id": "32",
      "title": "Reconstruction of natural visual scenes from neural spikes with deep neural networks",
      "authors": [
        "Y Zhang",
        "S Jia",
        "Y Zheng",
        "Z Yu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "33",
      "title": "Reconstruction of natural visual scenes from neural spikes with deep neural networks",
      "authors": [
        "Y Zhang",
        "S Jia",
        "Y Zheng",
        "Z Yu",
        "Y Tian",
        "S Ma"
      ],
      "year": "2020",
      "venue": "Neural Netw",
      "doi": "10.1016/j.neunet.2020.01.033"
    },
    {
      "citation_id": "34",
      "title": "Simultaneous Neural Spike Encoding and Decoding Based on Cross-modal Dual Deep Generative Model",
      "authors": [
        "Q Zhou",
        "C Du",
        "D Li",
        "H Wang",
        "J Liu",
        "H He"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    }
  ]
}