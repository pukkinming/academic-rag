{
  "paper_id": "2402.01579v2",
  "title": "Are Paralinguistic Representations All That Is Needed For Speech Emotion Recognition?",
  "published": "2024-02-02T17:17:42Z",
  "authors": [
    "Orchid Chetia Phukan",
    "Gautam Siddharth Kashyap",
    "Arun Balaji Buduru",
    "Rajesh Sharma"
  ],
  "keywords": [
    "Paralinguisitic Speech Processing",
    "Pre-Trained Models",
    "Speech Emotion Recognition",
    "TRILLsson"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Availability of representations from pre-trained models (PTMs) have facilitated substantial progress in speech emotion recognition (SER). Particularly, representations from PTM trained for paralinguistic speech processing have shown state-of-theart (SOTA) performance for SER. However, such paralinguistic PTM representations haven't been evaluated for SER in linguistic environments other than English. Also, paralinguistic PTM representations haven't been investigated in benchmarks such as SUPERB, EMO-SUPERB, ML-SUPERB for SER. This makes it difficult to access the efficacy of paralinguistic PTM representations for SER in multiple languages. To fill this gap, we perform a comprehensive comparative study of five SOTA PTM representations. Our results shows that paralinguistic PTM (TRILLsson) representations performs the best and this performance can be attributed to its effectiveness in capturing pitch, tone and other speech characteristics more effectively than other PTM representations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play a crucial role in human communication, influencing our behavior, decisions, and interactions. Speech emotion recognition (SER) as a task is designed to identify and understand these emotional cues conveyed through speech. By analyzing speech characteristics such as pitch, tone, intensity, and so on, SER models can accurately detect emotions such as happiness, sadness, anger, fear, and more. This holds immense significance across numerous domains, including human-computer interaction (HCI), healthcare, customer service, education, entertainment, as well as security.\n\nInitially, research around SER mostly revolved around using traditional statistical or handcrafted features  [1, 2] . However, with the wide-scale accessibility to pre-trained models (PTMs), the paradigm has completely shifted towards modeling SER with representations from PTMs  [3, 4] . The wide-scale and open availability of PTMs has led to sufficient development in SER. Representations from PTMs are provided as input for downstream modeling of SER. The main reason for their wide adaptation is their performance benefit and the ability to prevent training models from scratch. The superior performance of representations from PTMs for SER can be attributed to PTMs pre-training on diverse large-scale data, which, in return, provides meaningful representations for downstream SER.\n\nPrevious works have exploited various PTM representations for SER such as wav2vec2  [5] , wavLM  [6] , and so on. PTMs are trained for different tasks such as for general-purpose representation learning  [7] , speech recognition  [8] , paralinguis-tic tasks  [9] , etc. and with different pretext objectives as well as with different datasets. These PTMs are either trained in a single language or across multiple languages  [10] . These variabilities in the PTMs, leads to variability in the downstream SER performance with representations extracted from different PTMs. As such Morais et al.  [5]  have investigated representations from different PTMs for understanding the variability in the SER performance. Additionally, Phukan et al.  [11]  have evaluated various self-supervised PTM representations alongside speaker recognition PTM representations. Interestingly, their findings indicate that speaker recognition PTM representations tend to yield superior performance compared to selfsupervised PTM representations. Moreover, researchers have explored the applicability of different PTM representations for SER across multiple languages  [12, 13] .\n\nBenchmarks such as SUPERB  [14] , EMO-SUPERB  [15]  further assist researchers in validating various PTM representations for SER. However, previous investigative studies as well as the benchmarks haven't explored representations from paralinguistic PTM for SER in spite of its efficacy for state-of-theart (SOTA) performance in SER as shown by Shor et al.  [9] . However, Shor et al.  [9]  haven't evaluated the efficacy of representations from paralinguistic PTM for SER in languages other than English. In addition, ML-SUPERB  [16]  that evaluate PTM representations for multilingual tasks haven't included SER as a task yet. This leaves a gap for better understanding of representations from paralinguistic PTM for SER in multiple languages. So, to close this research gap, we perform a exhaustive comparative study of five PTM representations for SER consisting of representations from SOTA monolingual, multilingual, paralinguistic, as well as speaker recognition PTMs for better understanding of paralinguistic PTM representations capability for SER. Our main contributions are as follows: We are releasing the code 1 for future works to build upon our work for effective benchmarking of SER. There are four major sections in our work. Section 2 which discusses the PTMs whose representations are under consideration for our study. Section 3 presents the datasets, modeling, and its results. Lastly, Section 4 summarizes and concludes our study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pre-Trained Models",
      "text": "We use TRILLsson  [17]  as paralingustic PTM in our work. TRILLsson is built by teacher-student knowledge distillation from SOTA paralinguistic Conformer (CAP12)  [9] . TRILLsson is openly available while CAP12 is not. It achieves near SOTA performance in the Non-Semantic Speech (NOSS) benchmark. NOSS consists of various non-semantic tasks such as SER, speaker recognition, synthetic audio detection, etc. AudioSet and Libri-light dataset was used for distilling TRILLsson, while CAP12 is pre-trained on YT-U. Libri-light is a 60k hours English dataset, however, YT-U may contain data in multiple languages as it is a dataset of randomly collected audios from Youtube. Non-speech-related segments were removed from the collected audios and it resulted in around 900k hours unlabeled data. We use TRILLsson 2  available in Tensorflow Hub. The model aggregates over time and returns a vector of 1024 dimensional size for each input audio provided.\n\nWe use XLS-R  [18]  and Whisper  [19]  for multilingual PTMs. Both these PTMs are pre-trained in different manner, XLS-R in self-supervised while Whisper in weakly-supervised manner. XLS-R is pretrained on 436k hours data. Whisper is based on an encoder-decoder architecture and is trained to predict extensive volumes of audio transcriptions found on the internet. Whisper is pretrained on 680K hours encompassing 96 languages and also in multitask format. We remove the decoder and use the encoder to extract the representations. Also, we are the first work, according to best of our knowledge, to use Whisper encoder representations for multilingual SER. We use 0.3 billion parameters XLS-R 3  and whisper-base  4  version directly available in Hugginface.\n\nFor monolingual PTM, we consider WavLM 5    [7]  because of SOTA performance in SUPERB including SER. We include x-vector  6  as speaker recognition PTM in our study as previous researchers have shown the efficacy x-vector representations for SER  [20, 11] . X-vector  [21]  is a time-delay neural network, trained for speaker identification in supervised manner.\n\nThe last hidden states from XLS-R, Whisper, WavLM, and x-vector are extracted and converted into 1024, 512, 768, and 512-dimensional vectors for each audio file using average pooling. Sampling is performed at a rate of 16KHz for each audio file that is supplied as input to the PTMs.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "Crowd-Sourced Emotional Multimodal Actors Dataset (CREMA-D)  [22] : It is a benchmark gender-balance database in English, with 48 male and 43 female artists contributing a total of 7442 utterances. It acts as a valuable resource due to the variations in the speaker's ages and ethnicities. It consists of URDU  [24] : It is an urdu speech-emotion dataset. In total, there are 400 spoken expressions representing four emotions: anger, happiness, sadness, and neutral. These utterances come from 27 males and 11 females. The corpus comprises of genuine and unscripted emotional segments extracted from spontaneous discussions among various guests on a television talk show. German Emotional Speech Database (Emo-DB)  [25] : It is a German language dataset and consists of 535 utterances recorded from five male and five female actors. These actors were provided with a choice of ten unique scripts to deliver their lines. It contains seven emotions: anger, anxiety, boredom, disgust, happiness, neutral, and sadness. Acted Emotional Speech Dynamic Database (AESDD)  [26] : It is a Greek speech emotion dataset consisting of around 600 utterances spoken by 5 actors and comprising of five emotions: anger, disgust, fear, happiness, and sadness.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Downstream Modeling",
      "text": "We experiment with three downstream modeling approaches Support Vector Machine (SVM), Fully Connected Network (FCN), and CNN as these approaches have commonly used   by previous studies for various related speech processing tasks  [27, 11, 28] . The modeling approaches are shown in Figure  1  and 2. For SVM, we kept the hyperparameters that is given by default from Scikit-Learn library. For FCN, the extracted representations from the PTMs are directly passed to the dense layers and the number of neurons in each layer is given in Table  1 .\n\nFor CNN approach, we apply 1D-CNN on top of the extracted representations from the PTM followed by a maxpooling layer (Figure  2 ). 1D-CNN allows extraction of further important features. The output from the maxpooling layer is flattened and passed through FCN with the same architectural settings with the FCN given in Figure  1 . The softmax function is used as the activation function in the classification head i.e the output layer. It outputs the probabilities that signify different emotional states. We use Cross-entropy as the loss function and Tensorflow library for carrying out our experiments. All the models are trained in a 5-fold manner with different PTM representations. Four folds are kept for training and one is for test. Details regarding the hyperparameters kept during our experiments are provided in Table  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Results",
      "text": "The performance of the models trained on different PTM representations is shown in Table  2 . We see that models trained on TRILLsson representations performed the best across all the datasets with a sufficient margin than the other PTM representations. This demonstrates the reliability of TRILLsson representations and its capability to capture a wide range of speech characterisitcs including pitch, tone, intensity which play a significant role in influencing SER. XLS-R stands second after TRILLsson in CREMA-D, however, it fails in URDU, Emo-DB, and AESDD. WavLM representations perform comparably well despite WavLM being pre-trained only on English data. We can see mixed performance between both multilingual PTM representations, XLS-R performs better for CREMA-D and BAVED while Whisper on URDU, Emo-DB, and AESDD. This points out that the performance of the PTM representations depends on the downstream data distribution. We also plot t-SNE plots of the raw representations of various PTMs in Figure  3 . These figures support the results obtained as better clustering across emotions is seen for TRILLsson representations compared to other PTM representations. Among the downstream models, CNN performed the best. The confusion matrix for CREMA-D for the best score i.e CNN with TRILLsson representations is shown in Figure  4 .\n\nHowever, another important observation is the performance of x-vector representations. It is lower compared to TRILLsson representations but it is comparable to the performance of multilingual PTMs representations in certain languages and far better than them in some. This behavior could be due to the speaker recognition PTM which is able to capture certain speech characteristics present in speech that are helping for improved SER.\n\nWe also evaluated the PTM representations to see if dimension size has an influence on the performance. Table  3  presents the results obtained. We only experiment with CNN as CNN shows the best performance amongst the downstream networks. We linear project the representations of PTMs greater than 512-dimension i.e TRILLsson, wavLM, and XLS-R to 512-dimension, which is the dimension size of Whisper and xvector. We see there is bit drop in performance of the PTM representations when projected to a lower dimension. But, the TRILLsson representations still maintain the topmost position.\n\nx-vector and Whisper representations are not compared in Table 3 as their dimension is originally 512 and its comparison is already given in Table  2 .  We also compare our results to previous studies in Table  4 . We also experiment on an additional portugese dataset, emoUERJ  [32] . We attain SOTA accuracy and F1-score (Macro) on URDU, BAVED, AESDD, and emoUERJ datasets.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we performed a comprehensive comparative study of five SOTA PTM representations for investigating the effectiveness of paralingual PTM (TRILLsson) representations for SER in multiple languages. The PTMs considered in our study are SOTA in different benchmarks. Our results shows that representations from TRILLsson performed the best among all the PTM representations and this points out its efficacy in capturing essential speech components such as pitch, tone, intensity, important for SER. Models built on TRILLson representations shows SOTA performance across various benchmark datasets. The findings of our study will be instrumental in guiding the selection of appropriate representations for SER tasks. Moreover, they draw attention to the importance of incorporating paralinguistic PTM representations into various benchmarks for SER, thus facilitating future research endeavors in this domain.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Modeling Approach:",
      "page": 2
    },
    {
      "caption": "Figure 2: Modeling Approach: Convolution Neural Network",
      "page": 2
    },
    {
      "caption": "Figure 3: t-SNE plots of raw representations from various PTMs; Figure 3a, 3b, 3c, 3d shows the t-SNE plots for URDU; Figure 3e, 3f,",
      "page": 3
    },
    {
      "caption": "Figure 1: and 2. For SVM, we kept the hyperparameters that is given by",
      "page": 3
    },
    {
      "caption": "Figure 2: ). 1D-CNN allows extraction of further important fea-",
      "page": 3
    },
    {
      "caption": "Figure 1: The softmax function is used as",
      "page": 3
    },
    {
      "caption": "Figure 4: Confusion Matrix of CNN with TRILLsson represen-",
      "page": 4
    },
    {
      "caption": "Figure 3: These figures support the results obtained as better clustering",
      "page": 4
    },
    {
      "caption": "Figure 4: However, another important observation is the performance",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Hyperparameter Details",
      "page": 2
    },
    {
      "caption": "Table 2: Performance of Models trained on various PTM representations; All the scores are average of 5 folds and given in %; F1-Score",
      "page": 3
    },
    {
      "caption": "Table 3: Performance of Models after representations from various PTMs are projected to 512-dimension; All the scores are average",
      "page": 3
    },
    {
      "caption": "Table 1: For CNN approach, we apply 1D-CNN on top of the extracted",
      "page": 3
    },
    {
      "caption": "Table 1: Figure 4: Confusion Matrix of CNN with TRILLsson represen-",
      "page": 4
    },
    {
      "caption": "Table 2: We see that models trained",
      "page": 4
    },
    {
      "caption": "Table 3: presents the results obtained. We only experiment with CNN as",
      "page": 4
    },
    {
      "caption": "Table 2: Table 4: Comparison to SOTA works; CNN (TRILLsson) repre-",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in speech using mfcc and wavelet features",
      "authors": [
        "K Kishore",
        "P Satish"
      ],
      "year": "2013",
      "venue": "2013 3rd IEEE International Advance Computing Conference (IACC)"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "4",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "5",
      "title": "SUPERB: Speech Processing Universal PERformance Benchmark",
      "authors": [
        "S Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin",
        "T.-H Huang",
        "W.-C Tseng",
        "K Lee",
        "D.-R Liu",
        "Z Huang",
        "S Dong",
        "S.-W Li",
        "S Watanabe",
        "A Mohamed",
        "H Yi Lee"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using selfsupervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Evaluating self-supervised speech representations for speech emotion recognition",
      "authors": [
        "B Atmaja",
        "A Sasou"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "Wavlm: Largescale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "Universal paralinguistic speech representations using selfsupervised conformers",
      "authors": [
        "J Shor",
        "A Jansen",
        "W Han",
        "D Park",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "12",
      "title": "Transforming the Embeddings: A Lightweight Technique for Speech Emotion Recognition Tasks",
      "authors": [
        "O Chetia Phukan",
        "A Balaji",
        "R Buduru",
        "Sharma"
      ],
      "year": "2023",
      "venue": "Proc. INTER-SPEECH 2023"
    },
    {
      "citation_id": "13",
      "title": "Serab: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Decoding emotions: A comprehensive multilingual study of speech models for speech emotion recognition",
      "authors": [
        "A Singh",
        "A Gupta"
      ],
      "year": "2023",
      "venue": "Decoding emotions: A comprehensive multilingual study of speech models for speech emotion recognition",
      "arxiv": "arXiv:2308.08713"
    },
    {
      "citation_id": "15",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "16",
      "title": "Emo-superb: An in-depth look at speech emotion recognition",
      "authors": [
        "H Wu",
        "H.-C Chou",
        "K.-W Chang",
        "L Goncalves",
        "J Du",
        "J.-S Jang",
        "C.-C Lee",
        "H.-Y Lee"
      ],
      "year": "2024",
      "venue": "Emo-superb: An in-depth look at speech emotion recognition"
    },
    {
      "citation_id": "17",
      "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
      "authors": [
        "J Shi",
        "D Berrebbi",
        "W Chen",
        "E.-P Hu",
        "W.-P Huang",
        "H.-L Chung",
        "X Chang",
        "S.-W Li",
        "A Mohamed",
        "H Yi Lee",
        "S Watanabe"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "18",
      "title": "TRILLsson: Distilled Universal Paralinguistic Speech Representations",
      "authors": [
        "J Shor",
        "S Venugopalan"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "19",
      "title": "XLS-R: Selfsupervised Cross-lingual Speech Representation Learning at Scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino",
        "A Baevski",
        "A Conneau",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "21",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba",
        "N Chen",
        "N Dehak"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "24",
      "title": "Basic arabic vocal emotions dataset (baved)",
      "authors": [
        "A Aouf"
      ],
      "year": "2019",
      "venue": "Basic arabic vocal emotions dataset (baved)"
    },
    {
      "citation_id": "25",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International conference on frontiers of information technology (FIT)"
    },
    {
      "citation_id": "26",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "28",
      "title": "Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings",
      "authors": [
        "M Kodali",
        "S Kadiri",
        "P Alku"
      ],
      "year": "2023",
      "venue": "Proc. INTER-SPEECH 2023"
    },
    {
      "citation_id": "29",
      "title": "End to End Spoken Language Diarization with Wav2vec Embeddings",
      "authors": [
        "J Mishra",
        "J Patil",
        "A Chowdhury",
        "M Prasanna"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "30",
      "title": "Urdu speech emotion recognition using speech spectral features and deep learning techniques",
      "authors": [
        "S Taj",
        "G Shaikh",
        "S Hassan"
      ],
      "year": "2023",
      "venue": "2023 4th International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)"
    },
    {
      "citation_id": "31",
      "title": "Human-computer interaction with a realtime speech emotion recognition with ensembling techniques 1d convolution neural network and attention",
      "authors": [
        "W Alsabhan"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "32",
      "title": "Learning multilingual expressive speech representation for prosody prediction without parallel data",
      "authors": [
        "J Duret",
        "T Parcollet",
        "Y Est√®ve"
      ],
      "year": "2023",
      "venue": "Learning multilingual expressive speech representation for prosody prediction without parallel data",
      "arxiv": "arXiv:2306.17199"
    },
    {
      "citation_id": "33",
      "title": "emouerj: an emotional speech database in portuguese",
      "authors": [
        "R Germano",
        "M Tcheou",
        "F Da Rocha Henriques",
        "S Junior"
      ],
      "year": "2021",
      "venue": "emouerj: an emotional speech database in portuguese"
    }
  ]
}