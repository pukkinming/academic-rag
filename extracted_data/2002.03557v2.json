{
  "paper_id": "2002.03557v2",
  "title": "Multitask Emotion Recognition With Incomplete Labels",
  "published": "2020-02-10T05:32:12Z",
  "authors": [
    "Didan Deng",
    "Zhaokang Chen",
    "Bertram E. Shi"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We train a unified model to perform three tasks: facial action unit detection, expression classification, and valence-arousal estimation. We address two main challenges of learning the three tasks. First, most existing datasets are highly imbalanced. Second, most existing datasets do not contain labels for all three tasks. To tackle the first challenge, we apply data balancing techniques to experimental datasets. To tackle the second challenge, we propose an algorithm for the multitask model to learn from missing (incomplete) labels. This algorithm has two steps. We first train a teacher model to perform all three tasks, where each instance is trained by the ground truth label of its corresponding task. Secondly, we refer to the outputs of the teacher model as the soft labels. We use the soft labels and the ground truth to train the student model. We find that most of the student models outperform their teacher model on all the three tasks. Finally, we use model ensembling to boost performance further on the three tasks. Our code is publicly available 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Video emotion recognition is a longstanding problem studied by computer scientists and psychiatrists. It seeks to recognize people's emotional state automatically based on videos of their behaviour. Several taxonomies have been proposed to quantify human emotion. Facial Action Unit coding system was proposed by Ekman and Friesen  [12] . An action unit (AU) is a fundamental action of an individual muscle or group of muscles. Although the facial AU coding system can describe instantaneous changes in expression, it is quite hard to understand for non-experts. In the contrast, the seven basic emotions proposed by Ekman and Friesen  [11]  (anger, disgust, fear, happiness, sadness, surprise and neutral) are much easier to understand. Gunes and Pantic  [15]  proposed the continuous two-dimensional Valence-Arousal system. Valence describes how positive or negative the emotion is. Arousal describes how active or calm the person is. Most existing emotion datasets include only one set of labels  [3] ,  [10] ,  [14] . Very few datasets  [4]  contain two or more sets of labels, due to the high cost and time required to annotate. Due to these limitations, most past work in emotion recognition has focused on only one type of label e.g.  [9]  for valence-arousal.\n\nA multitask system that can label an input with all sets of labels is an important goal. Such a system would be more efficient, and would meet the demands of a wide range of applications. As Figure  1  shows, given a input facial video, the multitask system described here needs to 1 https://github.com/wtomin/multitask-Emotion-Recognition-with-Incomplete-Labels detect the presence or absence of eight action units (AUs), classify the emotion, and estimate the valence-arousal values, in each frame. In this paper, we address two key challenges of multitask emotion recognition: data imbalance and missing labels.\n\nThe problem of imbalanced data is very common in both single-label and multi-label emotion datasets. For example, the FER2013 dataset  [14]  consists of 35,887 facial images, each annotated with one of seven basic emotions. However, about 25% of the images are labeled with \"happy\", while only 1.5% are labeled with \"disgust\". The dramatic difference between the numbers of samples in the majority and minority classes leads to an overemphasis on the majority class, which hinders overall performance. We propose to deal with data imbalance by combining two methods: the introduction of more data, and re-sampling that over-samples minority classes and under-samples majority classes.\n\nThe problem of missing labels arises in multitask learning because most datasets are labelled for only one task. An intuitive solution is Binary Relevance (BR), which trains one classifier for each class in each task. However, BR fails to model the correlations between different labels. It is not efficient to train many classifiers, especially when the number of classes increases. To overcome these shortcomings, we propose to use a shared feature extractor for all tasks, and to use multiple heads on top of the feature extractor as classifiers. To better learn inter-task correlations with incomplete labels, we first train a teacher model with missing labels, and then use the ground truth and the outputs of the teacher model as supervision for a set of student models. Teacher-student networks are commonly used in knowledge distillation  [16] , which seeks to to compress model size and reduce inference time. However, in our case, the student model has the same structure and size as the teacher model. We hypothesize that a student exposed to a complete set of imperfect labels will learn better than a teacher exposed to an incomplete set of ground truth labels.\n\nOur primary contributions are:\n\n• We highlight the importance of data balancing for classification tasks in multitask learning. Surprisingly, we find that data balancing is not always beneficial for regression tasks. • We propose an to learn from missing labels using one teacher model and several student models. This algorithm is generic in the sense that we do not assume any particular structure (e.g. CNN or RNN) for the teacher/student networks,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Data Balancing",
      "text": "The data imbalance problem is very common in emotion datasets  [14] ,  [19] ,  [22] ,  [25] ,  [33] . Many methods have been proposed to solve the data imbalance problem. As a first priority, we should collect more data if possible. Where this is not possible, we can resample the data or generate synthetic samples. Resampling has been proved effective in dealing with imbalanced datasets  [13] . Since it is classifierindependent, it can be easily applied to many applications. Charte et al.  [8]  proposed a number of measures of the imbalance in multilabel datasets, such as the MeanIR (Mean Imbalance Ratio), as well as rebalancing algorithms for multilabel datasets.\n\nSome work has focused on algorithm adaptation methods, such as the cost-sensitive learning  [30] . The cost-sensitive SVM  [7]  incorporates the evaluation metrics (AUC and Gmean) into the objective function of SVM. The exampledependent cost-sensitive decision tree algorithm  [2]  takes example-dependent costs into account when training and pruning a tree. Such algorithm-specific modifications are useful in practice, but not flexible enough to be applied to a broader range of applications.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Learning From Missing Labels",
      "text": "To learn from missing labels, the most intuitive approach is to learn one classifier for each class, which is called as Binary Relevance (BR)  [28] . Another method replaces missing labels with negative labels  [5] ,  [31] . Although simple, this method impedes the model performance because many invisible positive ground truth labels are set to negative labels. There are some assumption-based methods to complete the training labels. For example, based on a low rank label matrix assumption, Cabral et al.  [6]  used matrix completion to fill in the missing labels. Based on the assumption that the missing labels are latent variables, Kapoor et al.  [17]  used Bayesian networks to infer missing labels. More specifically in emotion recognition, Kollias et al.  [21] ,  [24]  used a rulebased method to complete the training labels based on the co-occurrence between expression categories and AUs. Our method makes no hypotheses on the underlying relations over tasks. Instead, we use a data-driven teacher model to fill in the missing labels.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Knowledge Distillation",
      "text": "Hinton et al.  [16]  proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels. Knowledge Distillation has been proved effective in model compression, continual learning  [27]  and domain adaptation  [1] . However, its application to multitask learning with missing labels is under-researched.\n\nKnowledge distillation for regression is not as common as for classification. Some work in face alignment  [26] ,  [32]  has used the L1 or L2 distance as the distillation loss function.\n\nTo enable the use of knowledge distillation using the hyperparameter temperature for valence-arousal estimation, we transform the regression task to a classification task by discretizing the continuous values. Then we can use temperature to control the smoothness of the soft labels.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this paper, we design a network to perform three tasks simultaneously: facial AU detection, expression classification, and valence-arousal estimation. Facial AU detection is a multi-label classification problem, where the model detects the presence/absence of eight AUs (AU1, AU2, AU4, AU6, AU12, AU15, AU20, AU25), which are not mutually exclusive. Expression classification maps each frame to one of seven basic emotions. Valence-arousal estimation is a regression problem, where the model estimates two continuous scores in the interval [-1, 1].",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Balancing",
      "text": "The main dataset that we use is the Aff-wild2 dataset  [23] , which is an in-the-wild video dataset. There are three subdatasets in the Aff-wild2 dataset: one for facial AU detection, one for expression classification, and the other for valencearousal estimation. Each sub-dataset contains several videos, in which every frame is annotated with related labels. More details about this dataset can be found in  [20] .\n\nWe address the data imbalance problem in the Aff-wild2 dataset by importing external datasets followed by rebalancing.\n\nFor facial AU, we import the Denver Intensity of Spontaneous Facial Action (DISFA) dataset  [29] . By merging the DISFA dataset with the Aff-wild2 dataset, we enlarge the data size and also increase the number of samples in the minority classes. However, simply merging the two datasets does not solve the data imbalance problem. We use the ML-ROS algorithm  [8]  to oversample instances with positive minority labels. After applying ML-ROS, we find that the numbers of samples for each AU become closer. The facial AU distributions of the Aff-wild2 dataset, the DISFA dataset and the merged dataset are shown in Figure  2 (a) . As it  shows, the distribution of the dataset after ML-ROS is not fully balanced, which is caused by the co-occurrence of some minority labels and majority labels in the same instances.\n\nFor expression classification, we import the Expression in-the-Wild (ExpW) Dataset  [34] , which contains 91,795 images annotated with seven emotion categories. Since the number of images in the expression set of the Aff-wild2 dataset is 10 times larger than in the ExpW dataset, we downsample the Aff-wild2 dataset to make their sizes comparable. After merging the downsampled Aff-wild2 dataset and the ExpW dataset, we resample the samples to ensure the instances of each class have the same probability of appearing in one epoch. The distributions of seven emotion categories in the downsampled Aff-wild2 dataset, the ExpW dataset and the merged dataset are shown in Figure  2 (b) .\n\nFor valence-arousal estimation, we import the AFEW-VA dataset  [25]  which contains 30,051 frames annotated with both valence-arousal scores in [-10, 10], which are rescaled to [-1, 1]. We downsample the Aff-wild2 dataset by 5, and merge it with the AFEW-VA dataset. We then discretize the valence-arousal scores into 20 bins of the same width. We treat each bin as a category, and apply the oversampling/undersampling strategy as used in the expression recognition task. The distributions of the valence-arousal scores in the downsampled Aff-wild2 dataset, the AFEW-VA dataset and the merged dataset are shown in Figure  2  (c). The merged dataset is resampled to improve balance, but because of some rare cases, e.g., (V, A) = (1, -1), the distribution of the resampled dataset is not fully balanced.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Learning From Missing Labels",
      "text": "We denote the training dataset or a subset of the training dataset (i.e. a batch) by (X,Y ), where X is a set of input vectors and Y is a set of ground truth training labels. Although we wish to train a network to perform three tasks, we note that each data instance contains only a label for one task. Thus the entire dataset or batch consists of three subsets, (X,Y ) = {(X (i) ,Y (i) )} 3 i=1 . For convenience of notation, we assume each subset i includes an equal number N of instances within a batch, i.e. (X (i)\n\nwhere n indexes the instance. The batch size is 3 • N. It is straightforward to extend to the case where different subsets have different cardinality. For conciseness and simplicity of notation, we will often drop the indexing by instance, i.e refer to a label for task i by y (i) .\n\nThe inputs for all instances have the same dimensionality, independent of task. However, the ground truth labels for different tasks have different dimensionality. The label for the first task (facial AU detection) is y (1) ∈ {0, 1} 8 . Similarly, y (2) ∈ {0, 1} 7 (expression classification) and y (3) ∈ [-1, 1] 2 (valence-arousal estimation).\n\nAs each instance only has the label from one task, the intuitive way to train a unified multitask model is to only use label from that task for supervision. However, this training strategy does not capture inter-task correlations.\n\nTo capture these correlations, we propose the two-step algorithm shown in Figure  3 . In the first step, we train a single teacher model using only ground truth labels. In the second step, we replace the missing labels with soft labels derived from the output of the teacher model. We then use the ground truth and soft labels to train multiple student models.\n\nWe denote the output of a multitask network by f θ (•), where θ contains the model parameters, e.g. of the teacher or of the student network. Although each instance in the training data only has a label from one task, our model predicts the results for all the three tasks. We denote the output of the network for task j, by f\n\nθ (x (1) ) indicates the output of the network for task 2 (expression estimation) for an instance in the training dataset that has only task 1 (facial AU detection) labels. To avoid clutter, we will often refer to the output of the teacher network on task i by t (i) irrespective of what the input label is, i.e.\n\n) for some j ∈ {1, 2, 3} and similarly to the output of the student network on task i by s (i) , Generally speaking, the dimensionality of the teacher and student outputs for each task is the same as that of the ground truth. However, this is not true for the valencearousal estimation task (task 3), where as described below, we convert the regression problem to a classification problem to facilitate distillation of teacher knowledge to the student.\n\nThis architecture is quite generic, and can be applied to any network architecture placed inside the teacher and student blocks. Our main assumption is that the output layers of the teacher and student models are linear output layers. These can be converted into probabilities of each label or class using the element-wise logistic sigmoid σ (•) (e.g. for the facial AU detection task) or by a soft-max function for the multiclass classification. We define the soft-max function parameterized by temperature T and applied to a D dimensional vector y = {y d } D d=1 by\n\nIn training, the parameters of the teacher and student networks are obtained by minimizing different loss functions defined over each batch. These are are constructed by summing over different combinations of instance-wise loss functions which seek to bring the network output either closer to the ground truth, which we refer to as supervision loss functions, or closer to soft labels from the teacher, which we refer to as distillation loss functions. These are based on the binary cross entropy or cross entropy functions. For two vectors y = {y d } and z = {z d }, we define the total binary cross entropy by\n\nand the cross entropy to be\n\n1) Supervision loss functions: We choose different loss functions for data coming from different tasks.\n\nSince facial AU detection (task 1) is a multilabel classification problem, we use the total binary cross entropy loss across all AUs, defined as follows:\n\nL (1) (y (1) ,t (1) ) = BCE y (1) , σ (t (1) )\n\nSince expression classification is a multi-class classification problem, we use the categorical cross entropy loss, defined as follows: L (2) (y (2) ,t (2) ) = CE y (2) , SF(t (2) , 1)\n\nwhere setting the temperature parameter to 1 in the parameterized soft-max function results in the standard soft-max function.\n\nFor valence-arousal estimation, we combine classification and regression losses. Although valence-arousal estimation is a regression problem, we transform it into a classification task by discretizing the range [-1, 1] into 20 bins and representing each scalar dimension (valence or arousal) as a 20 dimensional one-hot vector. We denote the function transforming a scalar continuous label to a 20 dimensional discrete label by onehot(•). Thus, the categorical ground truth labels for valence and arousal are onehot(y 2 ). Correspondingly, the output of the final FC layer for valence-arousal estimation, t (3) is 40 dimensional.\n\nThe first 20 dimensions, t\n\n1 , corresponds to valence, and the second 20, t\n\n2 , corresponds to arousal. The classification loss can be computed for each instance as:\n\nTo compute the regression loss, we first transform each 20 dimensional network output to a continuous scalar value by taking the dot product of the vector of bin centers c and the standard softmax applied to the output, i.e.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "T(3)",
      "text": "i = c • SF(t\n\nThe regression loss is computed over each batch, using the Concordance Correlation Coefficient (CCC) between the scalar outputs and the scalar ground truth labels,\n\nwhere ρ yt is the correlation coefficient between the ground truth and the output and µ y , µ t , σ y , and σ t are the means and standard deviations computed over the batch. We compute two CCC's, one for valence, CCC 1 , and one for arousal\n\nWith a slight abuse of notation, we will define a perinstance loss function to be the sum of the L (3) class (y (3) ,t (3) ) and a portion of the negative CCC allocated to the instance.\n\n2) Distillation loss functions: For AU detection, the distillation loss we use is the binary cross entropy loss between the teacher model outputs and the student model outputs:\n\nH (1) (t (1) , s (1) ) = BCE σ (t (1) ), σ (s (1) )  (10)  This is identical to the supervision loss in Eq. (  4 ) with the ground truth replaced by the teacher output, and the teacher by the student.\n\nFor expression classification, we define the distillation loss to be H (2) (t (2) , s (2) ) = CE SF(t (2) , T ), SF(s (2) , T )  (11)  This is very similar to the supervision loss in Eq. (  5 ) with the ground truth replaced by the teacher output and the teacher by the student. The critical difference is that we set the temperature T to be a value a greater than one when computing the teacher output. This further softens the teacher output in order to better reveal the dark knowledge contained in the teacher network. Based on a grid search, we set T = 1.5.\n\nFor valence-arousal, we define the distillation loss to be\n\ni , T ), SF(s\n\nAs with the expression recognition distillation loss, this is very similar to the classification loss in Eq. (  6 ), with the critical difference being the increase in the temperature parameter, which we also set to T = 1.5.\n\n3) Batch-wise loss functions: Given a batch of data (X,Y ) = {{(x (i,n) , y (i,n) )} N n=1 } 3 i=1 , we define different loss functions for training the teacher and student netorks.\n\nWe denote the parameters of the teacher network by θ t . The teacher loss is defined based only on supervision loss functions.\n\nSince each instance only contains ground truth labels for one task, we use only the network output for that task and the corresponding supervised loss function in computing the loss function.\n\nWe denote the parameters of the student network by θ s . The student loss combines both supervision and distillation losses.\n\nThe parameter λ determines how much we weight the ground truth versus the teacher on tasks where the ground truth exists for that data instance. We set λ = 0.6 to weight the ground truth slightly more than the soft labels.\n\nWe note several key similarities and differences between the teacher (Eq. (  13 )) and student (Eq. (  14 )) loss functions. First, we find that both functions exploit ground truth knowledge when it is available (line 1 in both equations). However, due to the λ parameter, the student relies less on the ground truth. Instead, it also looks to the teacher for guidance (line 2 in Eq. (  14 )). Recall, however, that the student does not seek to blindly follow the teacher, but rather a softened version of the teacher's output due to the increased temperature parameter in the distillation loss functions. Second, for each data instance, the teacher network primarily updates only those parameters associated with the labelled task, since the loss function is only computed on the outputs of the network for that task. Although there may be some sharing of information between tasks through the use of a shared feature extractor as we describe later, the final decisions on each task are based only on the ground truth labelled data of that task. On the other hand, each instance contributes to the learning of all tasks by the student, both for the labelled task (lines 1 and 2 of Eq. (  14 )) and for the unlabelled tasks (line 3 of Eq. (  14 )) where the student relies upon the teacher for guidance. However, as noted above, it is guided by a softer version of the teacher's output due to the increased temperature parameter. This increased temperature may better reveal dark knowledge, enabling the student model to generalize better than the teacher.\n\nThe pseudo code for training the teacher and the student models is shown as Algorithm 1. Given a teacher model, we can repeat the student procedure in Algorithm 1 to obtain multiple student models. while not epoch end do (X,Y ) ← sampler(D)\n\nwhile not epoch end do (X,Y ) ← sampler(D)\n\n6: loss = F s (X,Y, θ t , θ s ) Eq. (  14 ) 7:\n\nWe studied the two different network architectures shown in Figure  4 : a CNN architecture and a CNN-RNN architecture. The CNN architecture considers each frame in isolation. It consists of a ResNet50 model, which functions as a shared feature extractor, followed by three MLPs stacked on top of the final ResNet50 conv layer: one for each task. The CNN-RNN architecture integrates information over time. It consists the same spatial feature extractor as that of the CNN architecture, followed by three bidirectional GRU layers to encode the temporal correlations over frames: one for each task. The GRU layers are followed by fully connected layers generating the final outputs. The input image size to both architectures is 112 × 112 × 3.\n\nWe trained both single-task CNN architectures (three different ResNet50 networks each followed by a single MLP) and a multitask CNN architecture (one ResNet50 network followed by three MLPs). We use the single-task networks to aim to compare the performance of the teacher model with and without the addition of external data and with and without data balancing. Training of the multitask architecture followed the procedures in Algorithm 1. Both teacher and student models used the exact same architecture. Multiple student models were ensembled.\n\nFor training CNN-RNN model, we only used data from the Aff-wild2 dataset to train the parameters of the GRU and FC layers after the spatial feature extractor, because the external databases we used did not always contain image sequences. The parameters of the spatial feature extractor were the same as those learned by the multitask CNN architecture trained on both the Aff-wild2 and the external datasets.  We used Adam  [18]  to optimize both architectures. The learning rate of Adam optimizer was initialized to 0.0001, and decreased by a factor of 10 after every 3 epochs. The teacher model was trained for 8 epochs in total, the student model for 3.\n\nWe used the same evaluation metrics as suggested in  [20] . For facial AU detection, the evaluation metric was 0.5 • F1 + 0.5 • Acc, where F1 denotes the unweighted F1 score for all 8 AUs, and Acc denotes the total accuracy. For expression classification, we used 0.67 • F1 + 0.33 • Acc as the metric, where F1 denotes the unweighted F1 score for 7 classes, and Acc is the total accuracy. For valence-arousal estimation, we evaluated with the Concordance Correlation Coefficient (CCC).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Cnn Results (Validation Set)",
      "text": "We trained single-task CNN models for all three tasks. Table  I  reports the results of the baseline network, after the addition of external data, and after data balancing. We find that both improve the performance of the single-task CNNs for facial AU detection and expression classification by a large margin. The improvement for the (valence-arousal) regression task is not obvious. However, given that it improved performance on the other tasks and did not degrade valence-arousal, we applied data balancing for the rest of the experiments.\n\nWe trained the multitask CNN models using Algorithm 1. Figure  5  compares the performance of the single-task CNN, the performance of the multitask teacher, the average performance of five students, and the performance of the fivestudent ensemble. Comparing multitask teacher and singletask CNN, we find that the multitask teacher model outperforms the single-task model for valence-arousal estimation, but not in facial AU detection and expression recognition. The average performance of the students exceeds that of the teacher on all tasks. We hypothesize that this is due to the   supervision by the soft but complete, rather than ground truth but only partial, labels for the students, and the potential \"dark knowledge\" in the softened probabilities. Ensembling the five students further improves performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Cnn-Rnn Results (Validation Set)",
      "text": "Figure  6  compares the performance of multitask CNN-RNN teacher, average student and student ensemble models with different sequence lengths during training: 32, 16 and 8 frames. Note that in all models the spatial feature extractor is fixed to that of the best CNN model to initialize the feature extractor in the CNN-RNN model. Only the parameters of the GRU and final FC layers differ.\n\nOn average, the students outperform the teacher model by a small margin for all sequence lengths and tasks, except facial AU detection with sequence length 32. We can further improve their performance by ensembling. The student ensemble gives the best performance in all cases, except for facial AU detection with sequence length 16 where it is surpassed by the average performance of the student models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Results On The Test Set",
      "text": "Table II reports the performance of several multitask models (teacher, best performing student, and student ensemble) on the test set. The results are consistent with those reported above for the validation set. First, the student generally outperforms the teacher. Second, the student ensemble generally gives the best performance. Third, the multitask CNN-RNN significantly outperforms the CNN on all three tasks, emphasizing the importance of temporal information for emotion recognition.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper has explored data balancing techniques and their application to multitask emotion recognition. We find that data balancing is beneficial for classification, but not necessarily for regression although it does not seem to hurt.  We also propose an teacher-student paradigm for learning multitask models in the presence of missing labels. This model is generic, and can be applied to many multitask scenarios aside from emotion recognition, as studied here.\n\nOur results show students generally outperform their teacher model on all tasks, and that ensembling students leads to the best performance. We suggest that although the soft yet complete labels provided by the teacher are not necessarily fully reliable, the increased supervision and dark knowledge they provide enabling the students to distill knowledge that enhances generalization.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows, given a input",
      "page": 1
    },
    {
      "caption": "Figure 1: The multitask system for simultaneous frame-by-",
      "page": 1
    },
    {
      "caption": "Figure 2: (a). As it",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison of different distributions.",
      "page": 3
    },
    {
      "caption": "Figure 2: (c). The merged dataset is resampled to improve balance,",
      "page": 3
    },
    {
      "caption": "Figure 3: In the ﬁrst step, we train a",
      "page": 3
    },
    {
      "caption": "Figure 3: Diagram of Algorithm 1. First, we train the teacher model with the teacher loss, which is shown in the top half of",
      "page": 4
    },
    {
      "caption": "Figure 4: a CNN architecture and a CNN-RNN architec-",
      "page": 6
    },
    {
      "caption": "Figure 4: The multitask CNN (a) and CNN-RNN (b) architec-",
      "page": 6
    },
    {
      "caption": "Figure 5: compares the performance of the single-task CNN,",
      "page": 6
    },
    {
      "caption": "Figure 5: Single and multitask CNN performance on the",
      "page": 7
    },
    {
      "caption": "Figure 6: compares the performance of multitask CNN-",
      "page": 7
    },
    {
      "caption": "Figure 6: Multitask CNN-RNN performance on the validation",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Architecture": "CNN",
          "Sequence\nLength": "NA",
          "Name": "Teacher",
          "facial AU": "0.562",
          "EXPR": "0.332",
          "Valence": "0.367",
          "Arousal": "0.412"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "",
          "Name": "Student0",
          "facial AU": "0.565",
          "EXPR": "0.372",
          "Valence": "0.421",
          "Arousal": "0.387"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "",
          "Name": "Ensemble",
          "facial AU": "0.576",
          "EXPR": "0.386",
          "Valence": "0.429",
          "Arousal": "0.414"
        },
        {
          "Architecture": "CNN-RNN",
          "Sequence\nLength": "32",
          "Name": "Teacher",
          "facial AU": "0.571",
          "EXPR": "0.395",
          "Valence": "0.425",
          "Arousal": "0.437"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "",
          "Name": "Student0",
          "facial AU": "0.587",
          "EXPR": "0.400",
          "Valence": "0.426",
          "Arousal": "0.452"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "",
          "Name": "Ensemble",
          "facial AU": "0.607",
          "EXPR": "0.405",
          "Valence": "0.440",
          "Arousal": "0.454"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "16",
          "Name": "Teacher",
          "facial AU": "0.600",
          "EXPR": "0.439",
          "Valence": "0.406",
          "Arousal": "0.384"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "",
          "Name": "Ensemble",
          "facial AU": "0.598",
          "EXPR": "0.440",
          "Valence": "0.425",
          "Arousal": "0.424"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "8",
          "Name": "Teacher",
          "facial AU": "0.585",
          "EXPR": "0.438",
          "Valence": "0.400",
          "Arousal": "0.420"
        },
        {
          "Architecture": "",
          "Sequence\nLength": "",
          "Name": "Ensemble",
          "facial AU": "0.599",
          "EXPR": "0.437",
          "Valence": "0.404",
          "Arousal": "0.436"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Domain adaptation of dnn acoustic models using knowledge distillation",
      "authors": [
        "T Asami",
        "R Masumura",
        "Y Yamaguchi",
        "H Masataki",
        "Y Aono"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "2",
      "title": "Example-dependent cost-sensitive decision trees",
      "authors": [
        "A Bahnsen",
        "D Aouada",
        "B Ottersten"
      ],
      "year": "2015",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "The omg-emotion behavior dataset",
      "authors": [
        "P Barros",
        "N Churamani",
        "E Lakomkin",
        "H Siqueira",
        "A Sutherland",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "2018 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "4",
      "title": "Emotionet challenge: Recognition of facial expressions of emotion in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "Q Feng",
        "Y Wang",
        "A Martinez"
      ],
      "year": "2017",
      "venue": "Emotionet challenge: Recognition of facial expressions of emotion in the wild",
      "arxiv": "arXiv:1703.01210"
    },
    {
      "citation_id": "5",
      "title": "Multi-label learning with incomplete class assignments",
      "authors": [
        "S Bucak",
        "R Jin",
        "A Jain"
      ],
      "year": "2011",
      "venue": "CVPR 2011"
    },
    {
      "citation_id": "6",
      "title": "Matrix completion for multi-label image classification",
      "authors": [
        "R Cabral",
        "F Torre",
        "J Costeira",
        "A Bernardino"
      ],
      "year": "2011",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "An optimized cost-sensitive svm for imbalanced data learning",
      "authors": [
        "P Cao",
        "D Zhao",
        "O Zaiane"
      ],
      "year": "2013",
      "venue": "Pacific-Asia conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "8",
      "title": "Addressing imbalance in multilabel classification: Measures and random resampling algorithms",
      "authors": [
        "F Charte",
        "A Rivera",
        "M Del Jesus",
        "F Herrera"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "9",
      "title": "Mimamo net: Integrating micro-and macro-motion for video emotion recognition",
      "authors": [
        "D Deng",
        "Z Chen",
        "Y Zhou",
        "B Shi"
      ],
      "year": "2019",
      "venue": "Mimamo net: Integrating micro-and macro-motion for video emotion recognition",
      "arxiv": "arXiv:1911.09784"
    },
    {
      "citation_id": "10",
      "title": "Video and image based emotion recognition challenges in the wild: Emotiw 2015",
      "authors": [
        "A Dhall",
        "O Ramana Murthy",
        "R Goecke",
        "J Joshi",
        "T Gedeon"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "11",
      "title": "Constants across cultures in the face and emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1971",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "12",
      "title": "Facial action coding system: a technique for the measurement of facial movement",
      "authors": [
        "E Friesen",
        "P Ekman"
      ],
      "year": "1978",
      "venue": "Facial action coding system: a technique for the measurement of facial movement"
    },
    {
      "citation_id": "13",
      "title": "On the effectiveness of preprocessing methods when dealing with different levels of class imbalance",
      "authors": [
        "V García",
        "J Sánchez",
        "R Mollineda"
      ],
      "year": "2012",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "14",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee"
      ],
      "year": "2013",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "15",
      "title": "Automatic, dimensional and continuous emotion recognition",
      "authors": [
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "International Journal of Synthetic Emotions (IJSE)"
    },
    {
      "citation_id": "16",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "17",
      "title": "Multilabel classification using bayesian compressed sensing",
      "authors": [
        "A Kapoor",
        "R Viswanathan",
        "P Jain"
      ],
      "year": "2012",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba",
        "Adam"
      ],
      "year": "2014",
      "venue": "A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "19",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "20",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Analysing affective behavior in the first abaw 2020 competition",
      "arxiv": "arXiv:2001.11409"
    },
    {
      "citation_id": "21",
      "title": "Face behavior\\a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior\\a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "22",
      "title": "Deep affect prediction in-thewild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "24",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "25",
      "title": "Afew-va database for valence and arousal estimation in-the-wild",
      "authors": [
        "J Kossaifi",
        "G Tzimiropoulos",
        "S Todorovic",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "26",
      "title": "Teacher and student joint learning for compact facial landmark detection network",
      "authors": [
        "H Lee",
        "W Baddar",
        "H Kim",
        "S Kim",
        "Y Ro"
      ],
      "year": "2018",
      "venue": "International Conference on Multimedia Modeling"
    },
    {
      "citation_id": "27",
      "title": "Learning without forgetting",
      "authors": [
        "Z Li",
        "D Hoiem"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "28",
      "title": "Binary relevance efficacy for multilabel classification",
      "authors": [
        "O Luaces",
        "J Díez",
        "J Barranquero",
        "J Del Coz",
        "A Bahamonde"
      ],
      "year": "2012",
      "venue": "Progress in Artificial Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Cost-sensitive boosting for classification of imbalanced data",
      "authors": [
        "Y Sun",
        "M Kamel",
        "A Wong",
        "Y Wang"
      ],
      "year": "2007",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Multi-label learning with weak label",
      "authors": [
        "Y.-Y Sun",
        "Y Zhang",
        "Z.-H Zhou"
      ],
      "year": "2010",
      "venue": "Twenty-fourth AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "32",
      "title": "Model distillation with knowledge transfer from face classification to alignment and verification",
      "authors": [
        "C Wang",
        "X Lan",
        "Y Zhang"
      ],
      "year": "2017",
      "venue": "Model distillation with knowledge transfer from face classification to alignment and verification",
      "arxiv": "arXiv:1709.02929"
    },
    {
      "citation_id": "33",
      "title": "Aff-wild: Valence and arousal'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "34",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    }
  ]
}