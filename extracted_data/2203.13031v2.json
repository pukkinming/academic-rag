{
  "paper_id": "2203.13031v2",
  "title": "Continuous Emotion Recognition Using Visual-Audio-Linguistic Information: A Technical Report For Abaw3",
  "published": "2022-03-24T12:18:06Z",
  "authors": [
    "Su Zhang",
    "Ruyi An",
    "Yi Ding",
    "Cuntai Guan"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose a cross-modal co-attention model for continuous emotion recognition using visual-audio-linguistic information. The model consists of four blocks. The visual, audio, and linguistic blocks are used to learn the spatial-temporal features of the multi-modal input. A coattention block is designed to fuse the learned features with the multi-head co-attention mechanism. The visual encoding from the visual block is concatenated with the attention feature to emphasize the visual information. To make full use of the data and alleviate over-fitting, crossvalidation is carried out on the training and validation set. The concordance correlation coefficient (CCC) centering is used to merge the results from each fold. The achieved CCC on the test set is 0.520 for valence and 0.602 for arousal, which significantly outperforms the baseline method with the corresponding CCC of 0.180 and 0.170 for valence and arousal, respectively. The code is available at https://github.com/sucv/ABAW3.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is the process of identifying human emotion. It plays a crucial role in behavioral modeling, human-computer interaction, and affective computing. By using the dimensional model  [1] , any emotional state can be taken as a point located in a continuous space, with the valence and arousal being the axes. Continuous emotion recognition seeks to map the N sequential data points into M sequential emotional state points, where M usually equals N . This report details our methodology for the valence-arousal estimation challenge from the third affective behavior analysis in-the-wild (ABAW3) workshop  [2] [3] [4] [5] [6] [7] [8] [9] [10] .\n\nOur work is an extension of the last year's attempt  [11]  on ABAW2  [3] . The audio-visual-linguistic information is exploited and combined by using a multi-head attentive feature fusion scheme. The network consists of three branches, fed by synchronous video frames, VGGish  [12]  features, and BERT features. The visual branch consists of a Resnet50 for spatial encoding and a temporal convolutional network (TCN)  [13]  for temporal encoding. The audio and linguistic branches each contain a TCN for temporal encoding. The three branches work in parallel and their outputs are sent to the attentive fusion block. The latter employs the co-attention mechanism of the three branches for feature fusion. To emphasize the dominance of the visual features, which we believe to have the strongest correlation with the label, the temporal feature outputted by the TCN of the visual branch is concatenated to the fused feature. A fully-connected layer is used for the regression. The 6-fold cross-validation is carried out to alleviate the over-fitting.\n\nThe remainder of the paper is arranged as follows. Section 2 details the model architecture including the visual, aural, linguistic, and attentive fusion blocks. Section 3 elaborates the implementation details including the data preprocessing, training settings, and post-processing. Section 4 provides the continuous emotion recognition results on the Aff-Wild2 database. Section 5 concludes the work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "The model architecture is illustrated in Fig.  1 . The visual block consists of a Resnet50 and a TCN. The resnet50 plays the role of backbone and is pre-trained on the MS-CELEB-1M dataset as a facial recognition task, it is then fine-tuned on the FER+  [14]  dataset. The Resnet50 spatially encodes each frame from a video frame sequence. The sequential spatial encodings are then stacked and fed to a TCN for temporal encoding. The audio and linguistic blocks each consist of a TCN. The Vggish  [12]  and BERT  [15]  features are the inputs, respectively.\n\nGiven the three temporal encodings from the visual, audio, and linguistic blocks, the attentive fusion block first maps the feature vectors to query, key, and value vectors by the following procedure. For the i-th branch, its encoder Figure  1 . The architecture of our proposed model. The model consists of four components, i.e., the visual, audio, linguistic, and coattention blocks. The visual block has a cascaded 2DCNN-TCN structure, and the audio and linguistic blocks each contain a TCN. The three branches yield three independent spatial-temporal feature vectors. They are then fed to the attentive fusion block. Three independent attention encoders are used. For the i-th branch, its encoder consists of three independent linear layers, they adjust the dimension of the feature vector producing a query Qi, a key Ki, and a value Vi. They are then regrouped and concatenated to form the cross-modal counterparts. For example, the cross-modal query Q = [Q1, Q2, Q3]. An attention score is obtained by Eq. 1.\n\nconsists of three independent linear layers, they adjust the dimension of the feature vector producing a query Q i , a key K i , and a value V i . They are then regrouped and concatenated to form the cross-modal counterparts. For example, the cross-modal query\n\nAfter which, the attention feature is calculated as:\n\nwhere d K = 32 is the dimension of the key K. After which, the attention feature is normalized and concatenated to the visual temporal encoding. Finally, a fully connected layer is employed to infer. Following the labeling scheme of the Aff-Wild2 database where the label frequency equals the video frame rate, each frame of the input video sequence is exactly corresponding to one label point.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Database",
      "text": "The ABAW3 competition uses the Aff-Wild2 database. The corpora of the valence-arousal estimation subchallenge includes 564 trials. The database is split into the training, validation and test sets. The partitioning is done in a subject independent manner so that any subject's data are included in only one partition. The partitioning produces 341, 71, and 152 trials for the training, validation, and test sets. Four experts annotate the videos using the method proposed in  [16] . In addition to the annotations (for the training and validation sets only) and the raw videos, the bounding boxes and landmarks for each raw video are also available to the participants.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing",
      "text": "The visual preprocessing is carried out as follows. The cropped-aligned image data provided by the organizer are used. All the images are resized to 48 × 48 × 3. Given a trial from the training or validation set, the length N is determined by the number of the rows of the annotation text file which does not include -5. For the test set, the length N is determined by the frame number of the raw video. A zero matrix B of size N × 48 × 48 × 3 is initialized and then iterated over the rows. For the i-th row of B, it is assigned as the i-th jpg image if it exists, otherwise doing nothing.\n\nThe audio preprocessing firstly converts all the videos to mono with a 16K sampling rate in wav format. The VG-Gish features are then extracted using the pretrained Vggish model  1  . The only change is that we specified the hop length to be 1/f ramerate of the raw video, in order to synchronize with other modalities and annotations.\n\nThe linguistic preprocessing is carried out as follows. The mono wav file obtained from the audio preprocessing is fed to a pretrained speech recognition model from the Vosk toolkit 2 , from which the recognized words and the word-level timestamp are obtained. The recognized words are then fed to a pretrained punctuation and capitalization model from the Nvidia Nemo toolkit 3 . After which a pretrained BERT model from the Pytorch library is employed to extract the word-level linguistic features. The linguistic features are obtained by summing together the last four layers of the BERT model  [17] . To synchronize, the word-level linguistic features are populated according to the timestamp of each word and each frame. Specifically, a word usually has a larger time span than that for a frame. Therefore, for one word, its feature is repetitively assigned to the time steps of all the frames within the time span.\n\nFor the valence-arousal labels, all the rows containing -5 are excluded. To ensure that the features have the same length as the corresponding trial, the feature matrices are either repeatedly padded (using the last feature points) or trimmed (starting from the rear), depending on whether the feature length is shorter than the trial length or not, respectively.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Expansion",
      "text": "The AffWild2 database employed by ABAW3 contains 341 and 71 trials in the training and validation sets, respectively. To clarify, a label txt file and its corresponding data are taken as a trial. Note that some videos include two subjects, resulting in two separated cropped-aligned image folders and label txt files, with different suffixes. They are each taken as two trials. Note that there are 10 fewer trials for the training set compared to its counterpart from ABAW2.\n\nTo make full use of the available data and alleviate overfitting, 6-fold cross-validation is employed. By evenly splitting the training set into 5 folds, we have 6 folds in total with a roughly equal trial amount, i.e., 68 × 4 + 69 + 71 trials. Note that the 0-th fold is exactly the original data partitioning. And there is no subject overlap across different folds. The CCC-centering is employed to merge the inference result on the test set.\n\nMoreover, during training and validation, the resampling window has a 33% overlap, resulting in 33% more data.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training",
      "text": "The batch size is 2. For each batch, the resampling window length and hop length are 300 and 200, respectively. I.e., the dataloader loads consecutive 300 feature points to form a minibatch, with a stride of 200. For any trials having feature points smaller than the window length, zero padding is employed. For visual data, the random flip, random crop with a size of 40 are employed for training and only the 2 https://alphacephei.com/vosk/models/vosk-model-en-us-0.22.zip 3 https://docs.nvidia.com/deeplearning/nemo/userguide/docs/en/main/nlp/punctuation and capitalization.html center crop is employed for validation. The data are then normalized so that mean = std = 0.5. For audio and linguistic data, they are normalized so that mean = 0 and std = 1.\n\nThe CCC loss is used as the loss function. The Adam optimizer with a weight decay of 0.001 is employed. The learning rate (LR) and minimal learning rate (MLR) are set to 1e -5 and 1e -7, respectively. The ReduceLROnPlateau scheduler with a patience of 5 and factor of 0.1 is employed based on the validation CCC. The maximal epoch number and early stopping counter are set to 100 and 10, respectively. Two groups of layers for the Resnet50 backbone are manually selected for further fine-tuning, which corresponds to the whole layer4 and the last three blocks of layer3.\n\nThe training strategy aims to alleviate the out-fitting issue. The Resnet50 backbone is initially fixed except for the output layer. The learning rate is linearly warmed up to 1e -5 when epoch ≤ 10. After that, the PyTorch Re-duceLROnPlateau steps in as the scheduler. The CCC on the validation set determines whether the training improves. If there were no improvements for 5 epochs, the scheduler would reduce the LR to 0.1•LR. When LR < 1e-7, layer4 of the backbone is unfrozen and the scheduler is reset. This process repeats until the last three blocks of layer3 are unfrozen. Finally, when once again LR < 1e -7, the training is done. Note that at the end of each epoch, the current best model state dictionary (i.e., the one with the greatest validation CCC) is loaded. By doing so, when there is no improvement on validation CCC, the training CCC cannot keep increasing and ends up with over-fitting.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Post-Processing",
      "text": "The post-processing consists of CCC-centering and clipping. Given the predictions from 6-fold cross-validation, the CCC-centering aims to yield the weighted prediction based on the inter-class correlation coefficient (ICC)  [18] . This technique has been widely used in many emotion recognition challenges  [18] [19] [20] [21]  to obtain the gold-standard labels from multiple raters, by which the bias and inconsistency among individual raters are compensated. The clipping ensures that the inference is truncated within the interval [-1, 1], i,e., any values larger or smaller than 1 or -1 are set to 1 or -1. respectively.\n\nWith all the predictions from the 6-fold cross-validation and CCC-centering, we have 5 submissions allowed. They are determined as follows. On the one hand, based on the experience from ABAW2, we find that the influence of the CCC-centering and clipping on the test CCC could be quite trivial (±0.001). On the other hand, we also tried other centering methods, such as the estimator weighted evaluator  [22]  and rater aligned annotation weighting  [23] . However, there is almost no visual difference among the three",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Result",
      "text": "The validation and test results of our method against the baseline are reported in Table  1 . The best results from all the teams are reported in Table  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We propose a cross-modal co-attention model for continuous emotion recognition using visual-audio-linguistic information. The model employs three parallel blocks to learn the feature from the three modalities. The learned features are then fused by the cross-modal co-attention block. Experiments are conducted on the Aff-Wild2 database. The achieved CCC on the test set is 0.520 for valence and 0.602 for arousal, which significantly outperforms the baseline method with the corresponding CCC of 0.180 and 0.170 for valence and arousal, respectively.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The visual",
      "page": 1
    },
    {
      "caption": "Figure 1: The architecture of our proposed model. The model consists of four components, i.e., the visual, audio, linguistic, and co-",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: The best results from all",
      "data": [
        {
          "Method": "Situ-RUCAIM3 [24]",
          "Valence": "0.606",
          "Arousal": "0.596",
          "Mean": "0.601"
        },
        {
          "Method": "Ours",
          "Valence": "0.520",
          "Arousal": "0.602",
          "Mean": "0.561"
        },
        {
          "Method": "PRL [25]",
          "Valence": "0.450",
          "Arousal": "0.445",
          "Mean": "0.448"
        },
        {
          "Method": "HSE-NN",
          "Valence": "0.417",
          "Arousal": "0.454",
          "Mean": "0.436"
        },
        {
          "Method": "AU-NO",
          "Valence": "0.418",
          "Arousal": "0.407",
          "Mean": "0.413"
        },
        {
          "Method": "LIVIA-2022",
          "Valence": "0.374",
          "Arousal": "0.363",
          "Mean": "0.369"
        },
        {
          "Method": "Netease Fuxi Virtual\nHuman [26]",
          "Valence": "0.300",
          "Arousal": "0.244",
          "Mean": "0.272"
        },
        {
          "Method": "Baseline [2]",
          "Valence": "0.180",
          "Arousal": "0.170",
          "Mean": "0.175"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Static and dynamic 3d facial expression recognition: A comprehensive survey",
      "authors": [
        "G Sandbach",
        "S Zafeiriou",
        "M Pantic",
        "L Yin"
      ],
      "year": "2012",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "2",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "3",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "4",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "venue": "Analysing affective behavior in the first abaw 2020 competition"
    },
    {
      "citation_id": "5",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "6",
      "title": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-thewild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "7",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "8",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond"
    },
    {
      "citation_id": "10",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference"
    },
    {
      "citation_id": "11",
      "title": "Continuous emotion recognition with audio-visual leader-follower attentive fusion",
      "authors": [
        "S Zhang",
        "Y Ding",
        "Z Wei",
        "C Guan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Cnn architectures for large-scale audio classification,\" in 2017 ieee international conference on acoustics, speech and signal processing (icassp)",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "Cnn architectures for large-scale audio classification,\" in 2017 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "13",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "arxiv": "arXiv:1803.01271"
    },
    {
      "citation_id": "14",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "15",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "16",
      "title": "feeltrace': An instrument for recording perceived emotion in real time",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "S Savvidou",
        "E Mcmahon",
        "M Sawey",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "17",
      "title": "Multimodal continuous dimensional emotion recognition using recurrent neural network and self-attention mechanism",
      "authors": [
        "L Sun",
        "Z Lian",
        "J Tao",
        "B Liu",
        "M Niu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop"
    },
    {
      "citation_id": "18",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "19",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international work"
    },
    {
      "citation_id": "20",
      "title": "Avec 2017: Real-life depression, and affect recognition workshop and challenge",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "J Gratch",
        "R Cowie",
        "S Scherer",
        "S Mozgai",
        "N Cummins",
        "M Schmitt",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "21",
      "title": "Avec 2018 workshop and challenge: Bipolar disorder and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "R Cowie",
        "H Kaya",
        "M Schmitt",
        "S Amiriparian",
        "N Cummins",
        "D Lalanne",
        "A Michaud"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on audio/visual emotion challenge and workshop"
    },
    {
      "citation_id": "22",
      "title": "Intelligent audio analysis",
      "authors": [
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Intelligent audio analysis"
    },
    {
      "citation_id": "23",
      "title": "Musetoolbox: The multimodal sentiment analysis continuous annotation fusion and discrete class transformation toolbox",
      "authors": [
        "L Stappen",
        "L Schumann",
        "B Sertolli",
        "A Baird",
        "B Weigell",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "24",
      "title": "Multi-modal emotion estimation for in-the-wild videos",
      "authors": [
        "L Meng",
        "Y Liu",
        "X Liu",
        "Z Huang",
        "W Jiang",
        "T Zhang",
        "Y Deng",
        "R Li",
        "Y Wu",
        "J Zhao"
      ],
      "year": "2022",
      "venue": "Multi-modal emotion estimation for in-the-wild videos",
      "arxiv": "arXiv:2203.13032"
    },
    {
      "citation_id": "25",
      "title": "An ensemble approach for facial expression analysis in video",
      "authors": [
        "H.-H Nguyen",
        "V.-T Huynh",
        "S.-H Kim"
      ],
      "year": "2022",
      "venue": "An ensemble approach for facial expression analysis in video",
      "arxiv": "arXiv:2203.12891"
    },
    {
      "citation_id": "26",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "Z Zhang",
        "F Qiu",
        "S Wang",
        "B Ma",
        "H Zeng",
        "R An",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Transformer-based multimodal information fusion for facial expression analysis",
      "arxiv": "arXiv:2203.12367"
    }
  ]
}