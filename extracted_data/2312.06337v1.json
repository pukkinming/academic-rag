{
  "paper_id": "2312.06337v1",
  "title": "Deep Imbalanced Learning For Multimodal Emotion Recognition In Conversations",
  "published": "2023-12-11T12:35:17Z",
  "authors": [
    "Tao Meng",
    "Yuntao Shou",
    "Wei Ai",
    "Nan Yin",
    "Keqin Li"
  ],
  "keywords": [
    "Data augmentation",
    "Data imbalance",
    "Feature fusion",
    "Graph neural network",
    "Multimodal emotion recognition in conversations"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The main task of Multimodal Emotion Recognition in Conversations (MERC) is to identify the emotions in modalities, e.g., text, audio, image and video, which is a significant development direction for realizing machine intelligence. However, many data in MERC naturally exhibit an imbalanced distribution of emotion categories, and researchers ignore the negative impact of imbalanced data on emotion recognition. To tackle this problem, we systematically analyze it from three aspects: data augmentation, loss sensitivity, and sampling strategy, and propose the Class Boundary Enhanced Representation Learning (CBERL) model. Concretely, we first design a multimodal generative adversarial network to address the imbalanced distribution of emotion categories in raw data. Secondly, a deep joint variational autoencoder is proposed to fuse complementary semantic information across modalities and obtain discriminative feature representations. Finally, we implement a multi-task graph neural network with mask reconstruction and classification optimization to solve the problem of overfitting and underfitting in class boundary learning, and achieve cross-modal emotion recognition. We have conducted extensive experiments on the IEMOCAP and MELD benchmark datasets, and the results show that CBERL has achieved a certain performance improvement in the effectiveness of emotion recognition. Especially on the minority class \"fear\" and \"disgust\" emotion labels, our model improves the accuracy and F1 value by 10% to 20%. Impact Statement-Multimodal emotion recognition in conversation plays an important role in human-computer interaction. However, existing methods ignore the data imbalance problem of multi-modal datasets. In this paper, we propose a Class Boundary Enhanced Representation Learning (CBERL) model. Since existing multi-modal emotion recognition datasets exhibit longtail distributions on different emotion categories, the proposed method can greatly alleviate the problem of data imbalance and ensure the accuracy of emotion recognition. In particular, our method can also achieve cross-modal semantic information fusion. Experimental results show that our method outperforms the SOTA methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "W ITH the continuous development of hardware re- sources and social media in recent decades, people have widely used multimodalities, e.g., text, audio, image  The ratios of the seven emotion labels to all, respectively. (b) Classification performance of some baseline models on the minority class labels \"fear\" and \"disgust\".\n\nand video to express their emotions or thoughts. The task of multimodal emotion recognition in Conversations (MERC) is to understand emotions in diverse modalities. It can be widely used in fields such as healthcare, conversation generation, and intelligent recommender systems, which has drawn increasing research attention  [1] ,  [2] ,  [3] . For example, in the field of intelligent recommendation, a machine can recommend things that may be most interesting to a consumer based on his changing mood. At the same time, the existence of large multimodal corpus datasets for instant chat software such as Weibo, Meta and Twitter can provide a data basis for MERC based on deep learning  [4] . However, these corpora naturally have a high class imbalance problem, i.e., most classes only contain a scarce number of samples, while a large number of samples belong to only a few classes  [5] ,  [6] .\n\nThe current mainstream MERC task mainly uses a graph neural network (GNN) for information fusion to enhance the effectiveness of emotion prediction  [7] ,  [8] , but they ignore the data imbalance problem. However, in the field of MERC, data imbalance is a widespread problem, which will hinder the model from learning the distribution law of the data, and resulting in the model failing to discern emotion class boundaries. Taking the popular multimodal benchmark dataset MELD shown in Fig.  1 (a) as an example, the \"fear\" and \"disgust\" emotion labels only account for 1.91% and 2.61% of the total labels, respectively, and all baseline models of F1 values are less than 11.2% on the \"fear\" and \"disgust\" emotion labels in Fig.  1(b ). These emotion classification results cannot meet practical needs. Similarly, this problem also exists on other multimodal benchmark datasets. Therefore, it is necessary to take the data imbalance problem as the starting points of the MERC model design.\n\nTo alleviate the data imbalance problem in deep learning, there are mainly three different research directions to optimizing the discriminative degree of class boundaries: dataaugmentation level  [9] ,  [10] , sampling-strategy level  [11] ,  [12] , and loss-sensitive level  [13] ,  [14] . Although these methods have achieved relatively good results in their respective fields, there is still a lack of systematic consideration for the data imbalance problem in MERC.\n\nThe research based on data augmentation aims to increase the number of minority class samples to improve the clarity of the model learned class boundaries. For example, Su et al.  [15]  proposed the Corpus-Aware Emotional CycleGAN (CAEmoCyGAN) method, which improves the distribution of generated data through a corpus-aware attention mechanism, so that the model can learn better class boundaries. However, the data generated by such methods still differ in distribution from the original data, especially in minority-class samples.\n\nThe research based on a sampling-strategy mainly focuses on balancing the ratio of minority-class samples to majority class samples by the sampling frequency. For example, Hamilton et al.  [16]  utilized GraphSAGE to randomly sample neighbor nodes and exploit their information to generate new node embedding representations. However, such sampling mechanisms may suffer from overfitting or underfitting  [17] ,  [18] .\n\nThe goal of research based on loss-sensitive function is to make the model learn the distribution of minority class samples by assigning a higher weight to the few samples in the loss function. For example, Li et al.  [14]  used Gradient Harmonizing Mechanism (GHM) to use a gradient density function to balance the model's weight for easy and hard-todistinguish samples. However, such methods are susceptible to interference from majority classes or noisy data samples  [19] ,  [20] ,  [21] .\n\nIn view of the above problems, how to systematically eliminate the negative impact of data imbalance in MERC from three aspects: data-augmentation, sampling-strategy, and losssensitive is still a challenging task. Therefore, we propose the Class Boundary Enhanced Representation Learning (CBERL) model to solve the data imbalance problem in MERC from these three aspects.\n\nThe proposed model, CBERL, will first use the data augmentation method of generative adversarial networks (GAN) to generate new samples, thereby providing a data basis for the subsequent model to learn discriminative class boundaries. In this model, we additionally add an identity loss and a classification loss to reduce the distribution difference between the generated and original data, and the generated and original labels, respectively.\n\nAfter data augmentation, we input the original and newly generated data into a Deep Joint Variational Autoencoder (DJVAE) with KL divergence for cross-modal feature fusion to capture complementary semantic information between different modalities and achieve effective feature dimensionality reduction. Then, the fused low-dimensional feature vector is input into Bi-LSTM to obtain feature representation of richer contextual semantic information.\n\nNext, we feed the obtained contextual feature vectors into our proposed Multi-task Graph Neural Network (MGNN). For the first subtask, to overcome the over-fitting or underfitting problems of the random sampling strategy in GNN to the minority class samples, MGNN first randomly performs a mask operation on some nodes in the network during the process of aggregating the information of surrounding neighbor nodes. Then, the remaining unmasked neighbor nodes are input into the graph convolutional network layer and the multilayer perceptron (MLP), and the predicted values of all neighbor nodes are obtained. Finally, the loss between the true and predicted values is computed to optimize the distribution representation of the class boundaries. For the second subtask, we construct an emotion classification model composed of multiple weak classifiers and add an adjustment factor to the loss function to enhance the model's propensity to learn the minority class samples. The underlying parameters are shared between the two subtasks of MGNN, which helps improve the model's generalization ability, thereby enhancing the performance of emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Our Contributions",
      "text": "Therefore, MERC should not only consider the feature fusion problem of text, audio, video and image modalities, but also generate a new architecture to solve the data imbalance problem. Inspired by the above problems, we propose a novel Class Boundary Enhanced Representation Learning (CBERL) model to obtain better emotion class boundaries. The main contributions of this paper are as follows:\n\n• A novel deep imbalanced learning architecture, named CBERL, is presented. CBERL can not only fuse semantic information across modalities, but also learn class boundaries for imbalanced data more accurately.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Emotion Recognition In Conversations",
      "text": "MERC has been widely used in various fields in real life, especially in intelligent dialogue recommendation, and has a high application value. The current mainstream methods mainly focus on three research directions: context-based emotion recognition  [22] , speaker-based emotion recognition  [23] , and speaker-distinguishing emotion recognition  [24] .\n\nIn context-based emotion recognition research, Nguyen et al.  [25]  adopted a deep neural network consisting of a dualstream autoencoder and a long short-term memory neural network (LSTM), which was able to perform emotion recognition by effectively integrating the conversational context. Qin et al.  [26]  achieved Deep Co-Interactive Relation Network (DCR-Net), which interacted with dialogue behaviors and emotion changes by combining BERT's bidirectional encoded representation  [27] .\n\nIn speaker-based emotion recognition research, Xing et al.  [28]  conducted the Adapted Dynamic Memory Network (A-DMN), which used a global recurrent neural network (RNN) to model the influence between speakers. However, A-DMN had poor memorization ability on overly long text sequences. Hazarika et al.  [29]  created the Conversational Memory Network (CMN), which creatively introduced an attention mechanism to obtain the importance of historical context to the current utterance, thereby simulating the dependencies between speakers. However, this method cannot model multi-dialogue relationships. Ghosal et al.  [30]  proposed DialogueGCN, which exploited the properties of graph convolutional neural networks (GCN) to construct a dynamic graph model that simulated interactions between speakers by using speakers as nodes of the graph and dependencies between speakers as edges. However, GCNs are prone to oversmoothing, which will cause the model to fail to extract deeper semantic information.\n\nIn emotion recognition based on distinguishing speakers, although CMN, ICON, DialogueGCN, and other models modeled the dependencies between different speakers, they did not distinguish who the speaker of the discourse was in the final emotion recognition process. To overcome this problem, Majumder et al.  [1]  introduced DialogueRNN. The model simultaneously considered the speaker information, the utterance context and the emotional information of multimodal features, and adopted three gated recurrent units (GRU), namely Party GRU, Global GRU and Emotion GRU, to capture the speaker state, global state of context, and affective state. For the utterance at the current time t, the global state of the context is updated by the context global state at the previous time t -1, the context representation at the current time t, and the current speaker's state at the previous time t -1. The speaker state was updated by the state of the current speaker at the previous time t -1, the representation of the current context, and the global state of the context at the previous time. The affective state was updated by the speaker's current state at time t and the affective state at the previous time t -1. Finally, emotion classification is performed with the obtained emotion state. Based on the above research, we used bidirectional LSTM (BiLSTM)  [31]  to model contextual semantic relations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Data Augmentation",
      "text": "The scarcity of datasets has always been an inevitable problem in deep learning and machine learning  [32] , making it difficult for deep neural network models to learn unbiased representations of real data, resulting in serious overfitting problems. Although regularization methods can alleviate the problem of model overfitting  [33] , this is not the most essential solution to the problem. Even the simplest machine learning model can achieve very good results when the dataset is large enough. Therefore, we will mainly consider data augmentation methods to improve the model's generalization ability.\n\nWang et al.  [34]  adopted Deep Generative Models (DGM), which differed from traditional data enhancement methods by adding Gaussian noise to the original data. DGM designed generative adversarial networks (GAN) and variational autoencoders (VAE) conditioned on different input vectors. DGM significantly outperformed traditional audio data enhancement methods. Kang et al.  [35]  created Independent Component Analysis-evolution (ICA-evolution), which selectively generated data matching the overall data distribution using a fitness function. ICA evolution inherited the idea of a genetic algorithm to enhance the data by crossover and mutation operations. However, this method tended to change the original distribution of the data. Su et al.  [15]  proposed Corpus-Aware Emotional CycleGAN (CAEmoCyGAN), which employed an unsupervised cross-corpus generative model to generate target data with rich semantic a corpus-aware attention mechanism to aggregate important source data information.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Feature Fusion Methods",
      "text": "Multimodal features have an important impact on emotion recognition. Feature fusion, as the primary method for information enhancement of multimodal features, has attracted much attention from researchers  [36] . Many studies have focused on capturing the differences between modalities to complement multimodal features, and many multimodal feature fusion methods have been successfully employed. Compared to decision-level fusion methods, feature fusion methods can fully use the advantages of multimodal features. Therefore, we will mainly summarize the research related to multimodal feature fusion.\n\nLiu et al.  [37]  introduced LFM, which used a low-rank tensor method to achieve dimensionality reduction of multi-modal features and improve the fusion efficiency. LFM achieved high performance on several different tasks. Zadeh et al.  [38]  used TFN. The method can learn the semantic information within and between modalities end-to-end. For semantic information extraction between modalities, TFN adopted the method of tensor fusion, which can simulate the interaction process between three modalities of text, audio, and video. TFN can effectively fuse multimodal features. Zhou et al.  [39]  provided Multiscale Feature Fusion and Enhancement (MFFENet), which introduced a spatial attention mechanism to fuse multi-scale features with global semantic information.\n\nMFFENet could assign higher attention weights to important feature vectors to obtain distinguishable features. Zadeh et al.  [7]  proposed DFG, which introduced a dynamic fusion graph model, which can achieve dynamic fusion of multimodal feature vectors, so that various modalities can play complementary roles.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Solutions For Imbalanced Data",
      "text": "Despite many vital advances in deep learning in recent decades, imbalanced data is still one of the challenges hindering the development of deep learning models  [40] . Therefore, researchers need to design a method to alleviate the problem of sample imbalance. In the existing research, there are mainly three solutions based on sample sampling, loss function, and model levels.\n\nIn a study based on the sample sampling level, Chawla et al.  [41]  utilized the Synthetic Minority Over-Sampling Technique (SMOTE) method. It increased the amount of data for a few samples by selecting k neighbors of each minority sample close to each other, then synthesizing each neighboring sample with the original sample manually into a new sample through an equation. However, this algorithm suffered from the marginalization of a small number of samples. Based on the SMOTE algorithm, Han et al.  [42]  proposed the Borderhne-SMOTE algorithm to increase the data volume of minority class samples by interpolating them in appropriate regions. This method solves the problem of marginalization of thesample distribution. DeepSMOTE can solve the problem of sample imbalance very well. At the level of loss-based function, Lin et al.  [13]  proposed Focal Loss, which added a parameter γ to weigh the loss, to balance the contribution of the simple classification samples and the complex classification samples to the loss. Li et al.  [14]  performed the Gradient Harmonizing Mechanism (GHM), which suppressed the classification results of both simple and difficult classification samples by means of a gradient density function. In model-level-based research, Wang et al.  [43]  proposed Deep-ensemble-level-based Interpretable Takagi-Sugeno-Kang Fuzzy Classifier (DE-TSK-FC), which divided the problem area layer by layer using several successive zero-order TSK fuzzy classifiers and then used K-Nearest Neighbor (KNN) for classification.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iii. Preliminary Information",
      "text": "In this section, we will use a mathematical language to define the MERC research in detail, and introduce the data preprocessing process of the benchmark dataset in this experiment, which consists of three important steps: (1) Word Embedding: We use the BERT  [27]  model to obtain word vector representations with rich semantic information.  (2)  Visual feature extraction: We use 3D-CNN  [44]  to obtain the feature vector of each frame in the video. (  3 ) Audio feature extraction: We use the openSMILE  [45]  open source toolkit to obtain sound signals with emotional changes in audio.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Multimodal Feature Extraction",
      "text": "The dialogue emotion recognition benchmark dataset selected in this paper contains three modal features: text, video and audio. The extraction methods of different modal features are different, and the semantic information they contain is also different. Next we describe the feature extraction process for each modality in detail.\n\n1) Text Features Extraction: How to obtain word vectors with rich semantic information from dialogue texts is a significant challenge in the field of natural language processing. To obtain richer contextual semantic information, this paper uses the BERT  [27]  pre-training model to extract and represent text features. We first use the Tokenizer method to segment the text and generate special tokens [SEP] and  [CLS] . Next, we build a mapping between words and indices by creating a dictionary. Finally, we input the mapped index data α ω spliced by word embeddings, and α ω ∈ R dω ,d ω = 100. Especially if the dimension of α ω is greater than 100, we do truncation. Otherwise we fill them up with 0.\n\n2) Visual Feature Extraction: The subtle changes in human facial expressions and visual signals, such as different gestures, dynamically reflect the emotional changes in the speaker's heart. Therefore, following the previous work  [1] ,  [46] ,  [47] , we use a supervised learning approach 3D-CNN  [44]  to recognize the speaker's behavior (i.e., human micro-expressions) to extract the semantic information contained in all samples of the video. Different from 2D-CNN in processing images, this paper will utilize 3D-CNN to extract visual-semantic features with temporal attributes in videos. 3D-CNN can capture the contextual semantic information of each frame. The input to the 3D-CNN network is a feature vector of size (channel,height,width,frame). Among them, the channel represents the number of channels in each video frame. In the IEMOCAP and MELD benchmark datasets, each video has three channels of RGB. width and height respectively represent the width and height of each frame in the video, and frame represents the total number of frames. The 3D-CNN used in this paper contains three modules. Each module consists of two 3D convolutional layers and a max-pooling layer. The 3D filter of the two convolutional layers are employed having dimensions (f m , f h , f w , f c ), where, f [m/h/w/c] represents the number of features maps, height, width and input channel, respectively. In addition, the kernel sizes of the two convolutional layers are 5 × 5 × 3 and 3 × 3 × 3, respectively. The size of the max pooling layer is 3×3×3. After two convolution operations, the max pooling operation is used for downsampling. After three modules, the obtained high level abstract features are input into the ReLU nonlinear transformation function to improve the expressiveness of the model. Finally, the feature vectors obtained after three convolution blocks and activation functions are input into the multi-layer perceptron for behavior recognition. The above process is to update the network parameters in a supervised manner. We take a hidden layer from the MLP as the final visual semantic feature representation α v , and\n\n3) Audio Feature Extraction: The sound signal in the audio contains the semantic information of the speaker's emotional fluctuation, which has an important impact on the process of the model's understanding of the speaker's emotional change. Therefore, following the previous work  [1] ,  [46] ,  [47] , this paper will leverage the openSMILE  [45]  open source toolkit to extract semantic information from audio, thereby enhancing the model's ability to understand emotion. OpenSMILE provides important statistical descriptors (e.g., MFCC, Melspectra, loudness, etc) for audio files. Therefore, we use the IS13 ComP arE1 extractor 1 in openSMILE to obtain a 6373-dimensional vector for each speech, which has rich semantic information. At the same time, we perform Z-score normalization on the speech feature vectors. Due to limited computing resources, we feed the obtained feature vectors into a multi-layer perceptron (MLP) for self-supervised learning, and use a hidden layer with 100 neurons as our final audio feature vectors α a , and α a ∈ R da , d a = 100.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. The Design Of The Cberl Structure",
      "text": "In this section, we detail the design of the CBERL structure. Fig.  2  visually shows the architecture of the CBERL model proposed in this paper. As shown, our model includes five key stages: (1) Data augmentation: For the multimodal dialogue emotion recognition scene, we will introduce the GAN method and optimize its loss function, adding the identity function as part of the loss function to enable the model to converge during training and generate new samples that conform to the original data distribution. In particular, we train a generator and a discriminator separately during the data augmentation stage. We input the newly generated samples together with the original samples into the subsequent CBERL model to achieve data balance; (2) Inter-modal feature fusion: We propose a multi-modal feature fusion method based on a Deep Joint Variational Autoencoder. Different from simple VAE, which 1 http://audeering.com/technology/opensmile only performs point-to-point mapping of raw data, inspired by the idea of joint probability distribution, we introduce KL divergence to estimate the underlying distribution law of raw data, so as to capture the characteristics of differences between modalities, and obtain a more discriminative representation of class boundaries; (3) Intra-modal context feature extraction: We use Bidirectional LSTM (Bi-LSTM) to extract contextual semantic information for the fused text, video and audio modal features; (4) Graph interaction: Increasing evidence shows aggregating the information of all neighboring nodes in the graph will prevent the model from learning the unbiased representation of data from a few classes of nodes. Therefore, the model CBERL utilized in this paper will mask some of the nodes in the graph, and then use GCN to aggregate the information of unmasked nodes. Finally the semantic information obtained from the aggregation will then perform the data reconstruction and emotion prediction tasks separately to improve the fifitting ability of GCN to the minority class nodes.\n\n(5) Emotion classifification: Unlike existing MERC methods that utilize fully-connected layers in emotion classifification to obtain the final emotion category, we propose a classifification optimization algorithm to make the model focus on hard-toclassify samples. In particular, for the four stages of Intermodal feature fusion, Intra-modal context feature extraction, Graph interaction, and emotion classification, we regard them as a whole for training. We will detail the design of each part in the CBERL model in the following sections.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "1) Data Augmentation:",
      "text": "To solve the problem of class distribution imbalance in the dataset in MERC, we first build a multi-source generator and discriminator for the application scenario of multimodal emotion recognition in conversations. The models then learn the underlying distributions of the Source to Target Generator Target Multimodal Features Target to Source Generator\n\nFig.  3 : GAN network consists of a generator, discriminator and emotion classifier. We used two generators for source-to-target and target-to-source transformations, respectively. The emotion classifier is used to specify the generator to generate target samples with specified emotion labels. multimodal data as they play against each other. Finally we increase the amount of data required by the model by sampling the data in the learned latent space.\n\nThe overview of the data augmentation method is presented in Fig.  3 , which consists of a generator G and a discriminator D. In this paper, we consider a bidirectional mapping function between source and target data, and use two generators, including a synthetic sample for going from source to target data (G S→T ) and a synthetic sample for going from target to source data (G T →S ).\n\nSpecifically, we use the original multimodal emotion dataset to pretrain the emotion classifier EC s , thus guiding the training direction of GAN. To synthesize new samples, we add an emotion state vector Z to the generator G T →S as its input. During model training, each source data corresponds to a specific target data, and their emotion labels should be consistent. We define the loss function between source data and target data as shown in Equation (1):\n\nwhere L C represents the loss between the data generated by the source-to-target and target-to-source generators and the true labels, and the loss between the data generated by the source-to-target generator and the true labels. k represents the sequence number of the sample, and Z k is a one-hot encoded vector identical to the emotion label of the source sample S k . Furthermore, to enable the generator to map the target sample to the emotion category specified by EC s when it is transformed into the source sample by the generator (G T →S ), we impose a constraint on the emotion state vectors as shown in Equation (2):\n\nAmong them, L RC represents the classification loss between the generated data T i and the real labels after passing through the target-to-source generator. Z r , y r is the one-hot encoding vector of emotion category r. Futhermore, we assume that G S→T is the generator for unimodal sources, and D T,F is the discriminator for unimodal targets. G S→T performs an encoding operation (Enc) on the noise data to generate samples that conform to the distribution law of the real data. D T,F maps the input data to the target domain through the decoding operation (Dec). The loss function of the generator is defined as shown in Equation (3):\n\nFinally, in order to ensure the consistency of the distribution law of the generated data and the original data, this paper also adds the identity loss function in the training process of the model, and its loss function is defined as shown in Equation (  4 ):\n\nwhere L identity represents the square error between the data generated by the source data S i through the target to the source generator and S i , and the square error between the data generated by the target data T i through the source to the target generator and T i . Therefore, the entire loss function L EmoGAN of the generative adversarial network used in this paper during the training process is defined as shown in Equation (  5 ):\n\n(5) Among them, λ 1 , λ 2 , λ 3 are the weights of L identity , L(G S→T , D T,F ) and L C + L RC loss functions, which are learnable network parameters.\n\nDuring model training, we use the Adam optimization algorithm to update the network parameters of the generator and discriminator. Among them, the update formula of the generator is defined as shown in Equation (  6 ):\n\nBesides, the update formula of the discriminator is defined as shown in Equation  (7) :\n\nAfter GAN is trained, this paper uses it to generate multimodal emotional samples that conform to the original data distribution law for data augmentation. In particular, GAN networks are trained separately.\n\n2) Inter-modal Feature Fusion: To capture complementary semantic information between modalities and fuse multimodal feature vectors with differences, we design a Deep Joint Variational Autoencoder (DJVAE). As shown in Fig.  4 , DJVAE consists of an encoder and a decoder. The encoder is used to map the data samples x into a low-dimensional feature space z ∈ Z, and the decoder is used to reconstruct the original data samples. The formula is defined as shown in Equation (  8 ):\n\nThen DJVAE obtains the optimal mapping relationship between the data samples and the low-dimensional feature space by minimizing the gap between the original data samples x and the reconstructed data samples. However, simple VAE cannot filter noise samples, but can only achieve point-topoint mapping between sample data and low-dimensional feature space through the mean square error (MSE Loss). Different from simple VAE, the DJVAE model proposed in this paper will introduce KL divergence to estimate the similarity between the encoder and decoder, so as to learn the latent semantic information of multimodal features. The formula for KL divergence is defined as shown in Equation (  9 ):\n\nwhere q Φ (z | x) represents the encoder's mapping of raw data samples to the latent feature space Z. p ϑ (z | x) as an approximation of the true posterior distribution.\n\nThe above formula can be deformed to obtain Equation  (10) :\n\nAt the same time, since the KL divergence is non-negative, we can get Equation  (11) :\n\n) Therefore, we can get the loss function of DJVAE as shown in Equation (  12 ):\n\nwhere q Φ (z | x) represents the encoder's mapping of raw data samples to the latent feature space Z. p ϑ (x | z) represents the decoder sampling data samples from the latent feature space Z. To simplify the calculation of KL divergence, we will use a standard normal distribution with mean 0 and variance 1.\n\n3) Intra-modal Context Feature Extraction: Intra-modal Context Feature Extraction: The utterances spoken by the speaker are arranged according to certain grammatical rules, and utterances composed of words in different sequences may have completely different meanings. In addition, like text features, the feature vectors of the two modalities of video and audio contain the semantic information of the time dimension, and the speaker may show different emotions at different times. More importantly, the semantic information of the above three modalities is transmitted in a particular order. Therefore, we use Bi-LSTM for contextual feature extraction within the modality. Each LSTM block consists of multiple basic LSTM cells, and each LSTM cell consists of an input gate, a forget gate, and an output gate  [48] .\n\nThe formula for the input gate is defined as shown in Equation (  13 ):\n\nwhere x t ∈ R d f is composed of three modal feature vectors of word vector α ω , video feature vector α v and audio feature vector α a after feature fusion. i t represents the input gate, which is used to process the input multimodal emotion feature vector. W i ∈ R d h ×d k is the weight matrix of the input gate, which is a learnable parameter. d h is the number of units in the LSTM hidden layer, d k = d f + d h , h t-1 represents the hidden layer state at time t -1. b i ∈ R d h is the bias vector of the input gate. Bi-LSTM is composed of the feature vector splicing of two hidden layers in opposite directions, and its formula is defined as shown in Equation (  14 ): 4) Graph Interaction Network: We use graph to construct the interaction between speakers to capture the contextual semantic information related to the speakers. However, datasets in MERC have data imbalance issues, which will cause the model to fail to learn unbiased representations of minority class nodes, or even treat them as outliers in the data. Therefore, in response to the above problems, we propose a multi-task graph neural network model, named MGNN, to alleviate the problem of unbalanced distribution. MGNN simultaneously performs two subtasks to improve the generalization ability of GCN: 1) data reconstruction; 2) emotion classification.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Encoder(Φ):Q Φ(Z|X) Decoder(Θ):P Θ (Z|X)",
      "text": "First, we construct a directed graph G = {V, ε, R, W }, where the node v i (v i ∈ V ) is composed of multimodal feature vector g i , and edge r ij (r ij ∈ ε) is composed of the relationship between node v i and node\n\nis the weight of the edge r ij , and r ∈ R represents the relation type of the edge.\n\nEdge Weights: Similarity attention mechanism is used to calculate the weights of edges in the graph, and aggregate neighbor information according to the calculated edge weights. We utilize Multilayer Perceptron (MLP) to calculate the similarity between node i and its surrounding neighbor j. The formula is defined as shown in Equation (  15 ):\n\nAmong them, W\n\nθ1 , W\n\nθ2 are the weight matrices of the t-th layer in the multilayer perceptron network, which are learnable parameters. In the experiments, we set W to 200 and 110, respectively. ⊕ represents the feature vector concatenation operation. ij ∈ {0, 1}, and ij = 0 means that there is no edge between node i and node j, and ij = 1 means that there is a directed edge between node i and node j. Next, we use the softmax function to get the attention score for each edge, as shown in Equation (  16 ):\n\nwhere M i is the set of surrounding neighbor nodes of node i. The larger w (t) ij is, the closer the interaction between node i and node j is.\n\nMessage passing: Due to the serious data imbalance problem in MERC, if the GCN operation is used to aggregate the information of all surrounding neighbor nodes, it will cause the model to be biased towards fitting the majority class nodes, while the minority class nodes are regarded as outliers in the data. Therefore, we consider it unnecessary to aggregate all neighbor nodes in the graph. As shown in Fig.  5 , to solve the above problems, we randomly perform mask operation on some neighbors, then use graph convolution operation to aggregate information for neighbor nodes that have not been masked, and then perform data reconstruction tasks. The formula for message passing is defined as shown in Equation (  17 ): Data reconstruction: We use MSE Loss to measure the difference between the reconstructed data and the original data. The formula is defined as shown in Equation (  18 ):\n\nwhere y i represents the multimodal feature vector of the original data, and ŷi represents the predicted multimodal feature vector. In general, the smaller L recon , the stronger the ability of model data reconstruction. We will introduce the specific implementation process of the sentiment classification task in detail in the next subsection.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "5) Emotion Classification:",
      "text": "In model training, compared with the majority class samples, the minority class samples have little influence on the model, which will cause the model to update the parameters in a direction that is beneficial to the majority class samples. However, it is also necessary for the model to be able to correctly classify the minority class samples. Therefore, we use an ensemble learning algorithm called Adaboost, which continuously optimizes the weight of utterance samples in the weak classifier and increases the weight of the minority class utterance samples in the classification process, thereby forming a strong classifier.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Model Training",
      "text": "The CBERL model makes the model pay more attention to indistinguishable samples by adding a conditioning factor (1 -P i,j [y i,j ]) γ to the cross-entropy loss function, and the L2 regularization method is used to prevent the model from overfitting, thereby providing guidance for guiding the updated direction of the model parameters. The loss is defined as shown in Equation  (19) :\n\nwhere N is the number of samples in the benchmark dataset, c(i) represents the number of utterances contained in sample i, and P i,j is the probability distribution of the emotion category in the j-th sentence in the i-th dialogue, which is a number in the range [0, 1]. γ is a constant greater than 0, λ is the weight decay coefficient, and θ is the set of all learnable parameters in the network.\n\nThen we can get the total loss function for model training as shown in Equation  (20) :\n\nAmong them, ξ 1 , ξ 2 represent the importance of data reconstruction and emotion classification tasks, respectively. In general, the smaller the loss function value, the better the effect of the model's emotion classification.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Experiments A. Benchmark Dataset Used",
      "text": "The MELD  [49]  and IEMOCAP  [50]  benchmark datasets are widely used by researchers in MERC research. Since the IEMOCAP benchmark dataset does not provide a separate validation set, we use 10% of the training data as the validation set for the model. Furthermore, in these two benchmark datasets, the ratio of training and testing sets is 80:20. In particular, the first four sessions of the dataset are used for training and the last session of the dataset is used for testing in the IEMOCAP dataset.\n\nThe IEMOCAP benchmark dataset is a multimodal corpus consisting of 12.46 hours of dynamic dialogues, containing three modalities: video, text and audio. Two speakers conduct each conversation in the dataset, and these conversations are divided into many small sentences. To reduce the subjectivity of data labeling, each sentence is jointly decided by at least three data labeling experts to decide the final labeling category. The IEMOCAP benchmark dataset contains a total of 6 discrete emotion categories, namely \"happy\", \"sad\", \"neutral\", \"angry\", \"excited\" and \"frustrated\". To compare effectively with the baseline model, we followed the work of  [1] ,  [29] ,  [38]  and used the dynamic dialogues of the first eight speakers as the training and validation for this experiment, and the dialogues of the remaining speakers as the testing set. To ensure strong robustness of the model, we use the validation set to fine-tune the model parameters.\n\nMELD is also a multimodal benchmark dataset commonly used in MERC research, which consists of 13.7 hours of dynamic dialogue and contains different dialogue scenes from the \"friends\" TV series. Different from the IEMOCAP benchmark dataset, each dialogue in the MELD benchmark dataset is conducted by at least two speakers. Each dialogue is divided into many small sentences, and each sentence is jointly determined by at least five data annotation experts to determine the final annotation category. The MELD benchmark dataset contains a total of 7 discrete emotion categories, namely \"anger\", \"disgust\", \"fear\", \"joy\", \"neutral\", \"sadness\", and \"surprise\".",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "In this section, we use the following four metrics to evaluate the performance of dialogue emotion recognition on the IEMOCAP and MELD benchmark datasets: 1) accuracy; 2) weighted average accuracy (WAA); 3) F1-score; 4) weighted average F1-score (WAF1).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Baseline Models",
      "text": "To validate the effectiveness of CBERL on MERC, the paper compared the following baseline models with our model: TextCNN  [51] , bc-LSTM  [52] , DialogueRNN  [1] , DialogueGCN  [30] , CT-Net  [23] , and LR-GCN  [53] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Implementation Details",
      "text": "In this paper, we divide the benchmark dataset for our experiments into training, testing, and validation sets, and use the effects of the validation set to fine-tune the hyperparameters. To optimize the parameters of the network, we use the Adam optimization scheme to reduce the loss and set the learning rate to 0.0003. We set the maximum number of iterations to train the model to be 60. To prevent the model from overfitting, we also use the dropout technique and L2 regularization, and set the dropout and weight decay coefficients to 0.5 and 0.00001, respectively. For hyperparameter settings, we set λ 1 = 1, λ 2 = 0.5, λ 3 = 0.5, ξ 1 = 0.3, ξ 2 = 0.7 respectively. Due to limited computing resources, we set the batch size to 32. Meanwhile, the sentence in the same batch of data must have the same length. For sentences of different lengths, we pad the sentences to make the sentences the same length. We conduct experiments on an Nvidia RTX 3090 server with 24G of video memory, and use the PyTorch deep learning framework to validate the effectiveness of CBERL.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Results And Discussion",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Comparison With Baseline Methods",
      "text": "We compare the proposed multimodal emotion recognition in conversations method CBERL with current baseline models. Tables  I  and II  show the recognition accuracy and F1 value of each category of CBERL and other baseline models on the IEMOCAP and MELD benchmark datasets, respectively.\n\nIEMOCAP: As shown in Table  I , compared with other baseline models, the CBERL has the best overall recognition performance on the IEMOCAP benchmark dataset, and the WAA and WAF1 values are 69.3% and 69.2%, respectively. CBERL proposes a method that combines data augmentation, multimodal feature fusion, and interaction between speakers for emotion recognition. Among other comparison algorithms, the effect of LR-GCN is second, with WAA and WAF1 values of 68.5% and 68.3%, respectively. We believe this is because LR-GCN comprehensively considers the interaction between speakers as well as the latent relationship between utterances. The emotion recognition effect of DialogueRNN and DialogueGCN is slightly worse than that of CBERL and LR-GCN, with WAA being 63.4% and 62.7%, respectively, and WAF1 being 62.7% and 64.1%, respectively. We think this is because DialogueRNN and DialogueGCN do not exploit the complementary semantic information between modalities. The emotion recognition effect of TextCNN and bc-LSTM is very poor, WAA is 20.4% ∼ 14.1% lower than other baseline models and CBERL, and WAF1 is 21.1% ∼ 14.3% lower than other baseline models and CBERL. We guess this is because TextCNN and bc-LSTM ignore the interaction between speakers and the emotional fusion between modalities.\n\nMELD: As shown in Table  II , CBERL has the best emotion recognition performance among all comparison algorithms, with WAA and WAF1 values of 67.7% and 66.8%, respectively. Specifically, our proposed model CBERL significantly improves emotion recognition performance on two minority classes \"fear\" and \"disgust\" labels. On the \"fear\" emotion label, CBERL achieves 25.0% and 22.2% values on WAA and WAF1, respectively. On the \"disgust\" emotion label, CBERL achieves 25.8% and 24.6% values on WAA and WAF1, respectively. Compared with other comparison algorithms, CBERL is",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Analysis Of The Experimental Results",
      "text": "To give the specific classification of CBERL on the benchmark datasets, we show the confusion matrices obtained by CBERL on the IEMOCAP and MELD benchmark datasets in Fig.  6 . On the IEMOCAP benchmark dataset, the semantic information learned by the model between the two emotions is similar due to the small discrimination between the emotions \"happy\" and \"excited\". Therefore, the model is prone to confusion about these two emotions. We can also observe the confusion matrix and find that the model is apt to misclassify the \"happy\" label as the \"excited\" label, and the \"excited\" class as the \"happy\" class. For the \"sad\" class emotion, the model has difficulty distinguishing it from the \"frustrated\" class emotion. In the trained multimodal corpus, all emotion categories have some relationship with the \"neutral\" emotion label so that the model may misclassify the \"neutral\" emotion as other categories and vice versa. Someone with an \"angry\" emotion may usually be accompanied by a \"frustrated\" emotion. Therefore, the model may learn this semantic information during training, causing the model to incorrectly classify the \"angry\" emotion as the \"frustrated\" emotion. We observed the confusion matrix on the MELD benchmark dataset and found that the model's recognition accuracy on the \"fear\" and \"disgust\" emotion labels improved significantly. Compared to other baseline models that barely recognize \"fear\" and \"disgust\" emotions, we believe this is mainly due to our increased amount of data for \"fear\" and \"disgust\" emotions, which effectively alleviates the imbalance in data distribution. The number of utterances belonging to the \"neutral\" emotion is the largest among all emotion categories, which makes the model biased towards learning feature representations for utterances with \"neutral\" emotion, which makes it easy for the model to misclassify other emotions as \"neutral\" emotion.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "C. Visualization",
      "text": "To compare the distribution of different emotions in the feature space more intuitively, we visualized the original multimodal emotion features and the emotion features learned by different networks from the IEMOCAP benchmark dataset. Specifically, we project the original emotion features as well as the learned high-dimensional emotion features into a twodimensional space. In this paper, we will use the t-SNE method  [54]  to visualize emotion features in the IEMOCAP benchmark dataset and color each point according to the emotion label.\n\nAs shown in Fig.  7 (a), we find that the distribution of the original multimodal data without network processing in the two-dimensional space is very messy, there is no distribution pattern among the various emotion categories, which are mutually fused. As shown in Fig.  7 (b) and (c), we find that the embedding representations learned by the bc-LSTM and DialogueRNN models are much better than the original data distribution. There is a boundary between utterances belonging to different emotion categories. However, since neither the bc-LSTM nor the DialogueRNN model considers the feature fusion between modalities and the interaction between speakers, the distinguishable boundaries are still blurred. As shown in Fig.  7(d) , the visualization of the embedding representation learned by CBERL is the best. After considering the three influencing factors of modeling contextual semantic information within modalities, fusion of semantic information between modalities, and emotional interactions between speakers, CBERL learns embedding representations with high intraclass similarity and inter-class variability, and different emotional labels are partitioned by different boundaries between them.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "D. Ablation Study",
      "text": "In this section, we perform ablation experiments on each part of our proposed model CBERL on the IEMOCAP benchmark dataset. The results are analyzed to judge the impact of each module of CBERL on the effect of emotion recognition. The specific results of the ablation experiments are shown in the Table  III . In particular, when using the adjustment factor γ, we set γ =3.\n\nWe conducted a total of 16 groups of ablation experiments to compare the effects of the algorithms. When only one module is chosen as a component of CBERL, we find that feature fusion has the greatest impact on the model, and the WAF1 value of emotion recognition can reach 67.1%. The effect of the graph node mask is second, and the WAF1 value is 65.9%. Furthermore, as shown in Table  I , the WAF1 value of DialogueGCN is 64.1%, and the experimental results demonstrate the effectiveness of graph node masks. The effects of γ and the Adaboost algorithm are relatively similar, with WAF1 values of 66.0% and 65.9%, respectively, which are lower than the effects of feature fusion and graph node masking. When selecting two modules as components of CBERL, the combination of feature fusion and graph node mask has the best effect on emotion recognition, with a WAF1 value of 68.5%. The combination of γ and the Adaboost algorithm has the lowest effect, with a W AF 1 value of only 66.2%, but slightly higher than only γ or the Adaboost algorithm. The emotion recognition effects of other composition methods are similar and are all higher than the emotion recognition effects of only a single module. When three modules are selected as the components of CBERL, the combination of feature fusion, graph node mask, and the Adaboost algorithm performs the best emotion recognition with a W AF 1 value of 69.0%. The emotion recognition effect of other composition methods is also higher than that of CBERL composed of two modules. When choosing four modules as components of CBERL, emotion recognition performed best in all ablation experiments, with a W AF 1 value of 69.2%. When none of the modules is selected (i.e. just Bi-LSTM and GCN modules with data augmentation) as a component of CBERL, the model has the worst emotion recognition performance, and the WAF1 value of emotion recognition is 64.1%. Experiments demonstrate the effectiveness of each module. In particular, when not using the Adaboost algorithm as our emotion classifier, we use a multilayer perceptron (MLP) as our emotion classifier.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "E. Analysis On Parameters",
      "text": "In this section, we analyze the effect of the adjustment factor γ in the cross-entropy loss function on the model performance. γ is an important hyperparameter of CBERL, which can adjust the weight of indistinguishable samples in the loss function, so that the model can focus on the classification of indistinguishable samples. Therefore, we designed several sets of comparative experiments to select the best γ value. Specifically, we choose γ values from the set M = {0, 1, 2, 3, 4, 5}. The experimental results obtained by our different values of γ on the IEMOCAP benchmark dataset are shown in Fig.  8 . As the value of γ increases from 0, CBERL achieves a certain degree of performance improvement. When γ = 3, the model achieves the best performance. The best WAA value of CBERL can reach 69.3%, and the WAF1 value can reach 69.2%. However, when γ > 3, the WAA value of CBERL will start to decrease. It indicates that too much concentration of the model on indistinguishable samples will make the model overfitting effect and will bring some redundant semantic information to the model, thus making it less effective in emotion recognition.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "F. Effectiveness Of Data Augmentation",
      "text": "To verify the impact of the data augmentation module on the experimental results, we performed an ablation experiment on the data augmentation module. As shown in Table  IV , We find that CBERL with a data augmentation module outperforms CBERL without a data augmentation module in emotion recognition on both IEMOCAP and MELD datasets. The performance improvement may be attributed that the data augmentation module can balance the data distribution relationship among different emotion categories, which is beneficial to enhance the representation learning of graphs. To verify whether the data augmentation module and the weighted cross entropy/masking strategy can improve the emotion recognition effect of minority emotions, we performed the ablation experiment of the data augmentation module and the weighted cross entropy/masking strategy on minority emotions (i.e., happy, fear, and disgust). As shown in Table  V , the emotion recognition effect of CBERL(N) on minority emotions is particularly poor, especially on fear and disgust. The emotion recognition effect of CBERL (A) on minority emotions is slightly improved compared to CBERL (N). The improved performance may be attributed to the adjustment factor forcing the model to focus on the classification of hard samples. The emotion recognition effect of CBERL (M) is better than that of CBERL (N) on minority emotions. We speculate that the masking mechanism can alleviate the longtail problem of graph node distribution while obtaining better node representation. Compared with CBERL(N), CBERL(M), and CBERL(A), CBERL(D) achieves the best emotion recognition performance on the minority class emotion. The performance improvement is attributed to the data augmentation module can optimize the data distribution of minority class",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "G. Emotion Distribution After Data Augmentation",
      "text": "As shown in Table  VI , we give the number of samples increased for each emotion category for the IEMOCAP and MELD datasets. To eliminate the long-tail problem, we try to keep the number of each emotion category as consistent as possible. However, the number of some emotion categories is too small, which leads to adding too much data to fail to increase the performance of the model. In addition, we also show the spatial distribution of different emotion categories after data augmentation. As shown in Fig.  9 , the original distributions of the IEMOCAP and MELD datasets are chaotic and indistinguishable, while the distributions between different emotion categories after data augmentation are discriminative. The data distribution enhanced by GAN can enhance the emotion recognition ability of subsequent models.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Vii. Conclusion And Future Work",
      "text": "This paper proposes the Class Boundary Enhanced Representation Learning (CBERL) model, a multimodal emotion recognition in conversation framework for dialogue emotion recognition tasks. CBERL extracts contextual semantic information within modalities and fuses complementary information between modalities. At the same time, CBERL also considers the problem of data distribution imbalance, and solves this problem from three levels of data augmentation, sampling strategy and loss-sensitive. Extensive experiments are conducted on two popular datasets, IEMOCAP and MELD, and compared with other models, CBERL achieves better classification accuracy and F1 value on multiple emotion categories. Furthermore, we demonstrate the necessity of modal feature fusion as well as address the data imbalance problem.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustrative example of imbalanced data distribution",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) as an example, the “fear” and “dis-",
      "page": 1
    },
    {
      "caption": "Figure 1: (b). These emotion classification results cannot meet",
      "page": 1
    },
    {
      "caption": "Figure 2: The framework of the Multi-task Graph Reinforcement Learning based on Feature Fusion (CBERL) model consists of",
      "page": 5
    },
    {
      "caption": "Figure 2: visually shows the architecture of the CBERL model",
      "page": 5
    },
    {
      "caption": "Figure 3: GAN network consists of a generator, discriminator and emotion classifier. We used two generators for source-to-target",
      "page": 6
    },
    {
      "caption": "Figure 3: , which consists of a generator G and a discriminator",
      "page": 6
    },
    {
      "caption": "Figure 4: The model architecture of Deep Joint Variational Autoencoder (DJVAE) consists of an encoder and decoder. DJVAE",
      "page": 8
    },
    {
      "caption": "Figure 5: , to solve the above problems, we randomly perform",
      "page": 8
    },
    {
      "caption": "Figure 5: The model architecture for graph masking and data reconstruction consists of graph structure generation, graph random",
      "page": 9
    },
    {
      "caption": "Figure 6: On the IEMOCAP benchmark dataset, the semantic",
      "page": 11
    },
    {
      "caption": "Figure 6: Confusion matrix learned by CBERL on the IEMOCAP",
      "page": 12
    },
    {
      "caption": "Figure 7: (a), we find that the distribution of the",
      "page": 12
    },
    {
      "caption": "Figure 7: (b) and (c), we find that",
      "page": 12
    },
    {
      "caption": "Figure 7: (d), the visualization of the embedding rep-",
      "page": 12
    },
    {
      "caption": "Figure 8: As the value of γ increases from 0, CBERL achieves a",
      "page": 12
    },
    {
      "caption": "Figure 7: Visualizing feature embeddings for the multimodal emotion on the IEMOCAP benchmark dataset. Each dot represents",
      "page": 13
    },
    {
      "caption": "Figure 8: We set different γ values to test their experimental",
      "page": 13
    },
    {
      "caption": "Figure 9: The original data distribution and the data distribution after data augmentation of the IEMOCAP and MELD datasets.",
      "page": 14
    },
    {
      "caption": "Figure 9: , the original",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "MELD": "Fear\nDisgust"
        },
        {
          "Methods": "",
          "MELD": "F1\nF1"
        },
        {
          "Methods": "CNN\nbc-LSTM\nDialogueRNN\nCTNet\nLR-GCN",
          "MELD": "3.7\n8.3\n5.4\n5.2\n1.2\n1.7\n10.0\n11.2\n0.0\n11.0"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)"
        },
        {
          "Methods": "",
          "IEMOCAP": "Acc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nWAA\nWAF1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nDialogueRNN\nDialogueGCN\nCT-Net\nLR-GCN\nCBERL",
          "IEMOCAP": "27.73\n29.81\n57.14\n53.83\n34.36\n40.13\n61.12\n52.47\n46.11\n50.09\n62.94\n55.78\n48.93\n48.17\n29.16\n34.49\n57.14\n60.81\n54.19\n51.80\n57.03\n56.75\n51.17\n57.98\n67.12\n58.97\n55.23\n54.98\n25.63\n33.11\n75.14\n78.85\n58.56\n59.24\n64.76\n65.23\n80.27\n71.85\n61.16\n58.97\n63.42\n62.74\n89.14\n84.45\n40.63\n42.71\n61.97\n63.54\n67.51\n64.14\n65.46\n63.08\n64.13\n66.90\n65.21\n64.14\n69.08\n65.82\n85.35\n78.74\n47.97\n51.36\n78.01\n79.94\n72.98\n67.21\n52.27\n58.83\n68.01\n67.55\n68.52\n68.35\n54.24\n55.51\n81.67\n79.14\n59.13\n63.84\n69.47\n69.02\n76.37\n74.05\n68.26\n68.91\n58.84\n67.34\n78.21\n71.19\n69.36\n69.27\n63.31\n72.84\n56.42\n60.75\n75.32\n73.51\n70.32\n70.77"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "MELD": "Neutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)"
        },
        {
          "Methods": "",
          "MELD": "Acc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nAcc.\nF1\nWAA\nWAF1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nDialogueRNN\nCT-Net\nLR-GCN\nCBERL",
          "MELD": "76.23\n74.91\n43.35\n45.51\n4.63\n3.71\n18.25\n21.17\n46.14\n49.47\n8.91\n8.36\n35.33\n34.51\n56.35\n55.01\n78.45\n73.84\n46.82\n47.71\n3.84\n5.46\n22.47\n25.19\n51.61\n51.34\n4.31\n5.23\n36.71\n38.44\n57.51\n55.94\n72.12\n73.54\n54.42\n49.47\n1.61\n1.23\n23.97\n23.83\n52.01\n50.74\n1.52\n1.73\n41.01\n41.54\n56.12\n55.97\n75.61\n77.45\n51.32\n52.76\n5.14\n10.09\n30.91\n32.56\n54.31\n56.08\n11.62\n11.27\n42.51\n44.65\n61.93\n60.57\n81.53\n55.45\n65.87\n80.81\n57.16\n0.00\n0.00\n36.33\n36.96\n62.21\n7.33\n11.01\n52.64\n54.74\n66.71\n65.67\n82.03\n57.91\n25.04\n22.23\n47.51\n41.36\n66.03\n25.81\n24.65\n53.75\n55.31\n67.78\n66.89\n81.45\n55.24\n65.67"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Omg: Towards effective graph classification against label noise",
      "authors": [
        "N Yin",
        "L Shen",
        "M Wang",
        "X Luo",
        "Z Luo",
        "D Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "3",
      "title": "Messages are never propagated alone: Collaborative hypergraph neural network for time-series forecasting",
      "authors": [
        "N Yin",
        "L Shen",
        "H Xiong",
        "B Gu",
        "C Chen",
        "X Hua",
        "S Liu",
        "X Luo"
      ],
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "5",
      "title": "Coco: A coupled contrastive framework for unsupervised domain adaptive graph classification",
      "authors": [
        "N Yin",
        "L Shen",
        "M Wang",
        "L Lan",
        "Z Ma",
        "C Chen",
        "X.-S Hua",
        "X Luo"
      ],
      "year": "2023",
      "venue": "Coco: A coupled contrastive framework for unsupervised domain adaptive graph classification",
      "arxiv": "arXiv:2306.04979"
    },
    {
      "citation_id": "6",
      "title": "Deal: An unsupervised domain adaptive framework for graph-level classification",
      "authors": [
        "N Yin",
        "L Shen",
        "B Li",
        "M Wang",
        "X Luo",
        "C Chen",
        "Z Luo",
        "X.-S Hua"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia, ser. MM '22"
    },
    {
      "citation_id": "7",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Pu"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Multimodal emotion recognition with capsule graph convolutional based representation fusion",
      "authors": [
        "J Liu",
        "S Chen",
        "L Wang",
        "Z Liu",
        "Y Fu",
        "L Guo",
        "J Dang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "On data augmentation for gan training",
      "authors": [
        "N.-T Tran",
        "V.-H Tran",
        "N.-B Nguyen",
        "T.-K Nguyen",
        "N.-M Cheung"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "10",
      "title": "Accurate scene text detection via scale-aware data augmentation and shape similarity constraint",
      "authors": [
        "P Dai",
        "Y Li",
        "H Zhang",
        "J Li",
        "X Cao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Deep point set resampling via gradient fields",
      "authors": [
        "H Chen",
        "B Du",
        "S Luo",
        "W Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "A resampling method based on filter designed by window function considering frequency aliasing",
      "authors": [
        "H Liu",
        "J Lin",
        "S Xu",
        "T Bi",
        "Y Lao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers"
    },
    {
      "citation_id": "13",
      "title": "Focal loss for dense object detection",
      "authors": [
        "T.-Y Lin",
        "P Goyal",
        "R Girshick",
        "K He",
        "P Dollár"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "Gradient harmonized single-stage detector",
      "authors": [
        "B Li",
        "Y Liu",
        "X Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Unsupervised cross-corpus speech emotion recognition using a multi-source cycle-gan",
      "authors": [
        "B.-H Su",
        "C.-C Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Inductive representation learning on large graphs",
      "authors": [
        "W Hamilton",
        "Z Ying",
        "J Leskovec"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "17",
      "title": "Object detection in medical images based on hierarchical transformer and mask mechanism",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "C Xie",
        "H Liu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "18",
      "title": "Prediction model of dow jones index based on lstm-adaboost",
      "authors": [
        "R Ying",
        "Y Shou",
        "C Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "19",
      "title": "A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "J Du",
        "H Liu",
        "K Li"
      ],
      "year": "2021",
      "venue": "A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition"
    },
    {
      "citation_id": "20",
      "title": "Graph information bottleneck for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng"
      ],
      "year": "2023",
      "venue": "Graph information bottleneck for remote sensing segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "21",
      "title": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "22",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "24",
      "title": "Gcb-net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "T Zhang",
        "X Wang",
        "X Xu",
        "C Chen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Deep auto-encoders with sequential learning for multimodal dimensional emotion recognition",
      "authors": [
        "D Nguyen",
        "D Nguyen",
        "R Zeng",
        "T Nguyen",
        "S Tran",
        "T Nguyen",
        "S Sridharan",
        "C Fookes"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "Dcr-net: A deep cointeractive relation network for joint dialog act recognition and sentiment classification",
      "authors": [
        "L Qin",
        "W Che",
        "Y Li",
        "M Ni",
        "T Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Pre-training with whole word masking for chinese bert",
      "authors": [
        "Y Cui",
        "W Che",
        "T Liu",
        "B Qin",
        "Z Yang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "S Xing",
        "S Mai",
        "H Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "30",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "31",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. ACL"
    },
    {
      "citation_id": "32",
      "title": "Improving speech emotion recognition with adversarial data augmentation network",
      "authors": [
        "L Yi",
        "M.-W Mak"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "33",
      "title": "Regularization on augmented data to diversify sparse representation for robust image classification",
      "authors": [
        "S Zeng",
        "B Zhang",
        "J Gou",
        "Y Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "34",
      "title": "Data augmentation using deep generative models for embedding based speaker recognition",
      "authors": [
        "S Wang",
        "Y Yang",
        "Z Wu",
        "Y Qian",
        "K Yu"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "35",
      "title": "Ica-evolution based data augmentation with ensemble deep neural networks using time and frequency kernels for emotion recognition from eeg-data",
      "authors": [
        "J.-S Kang",
        "S Kavuri",
        "M Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition from multimodal physiological signals using a regularized deep fusion of kernel machine",
      "authors": [
        "X Zhang",
        "J Liu",
        "J Shen",
        "S Li",
        "K Hou",
        "B Hu",
        "J Gao",
        "T Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "37",
      "title": "Efficient low-rank multimodal fusion with modalityspecific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Mffenet: Multiscale feature fusion and enhancement network for rgb-thermal urban road scene parsing",
      "authors": [
        "W Zhou",
        "X Lin",
        "J Lei",
        "L Yu",
        "J.-N Hwang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Fuzzy support vector machine with relative density information for classifying imbalanced data",
      "authors": [
        "H Yu",
        "C Sun",
        "X Yang",
        "S Zheng",
        "H Zou"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Fuzzy Systems"
    },
    {
      "citation_id": "41",
      "title": "Smote: synthetic minority over-sampling technique",
      "authors": [
        "N Chawla",
        "K Bowyer",
        "L Hall",
        "W Kegelmeyer"
      ],
      "year": "2002",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "42",
      "title": "Borderline-smote: a new oversampling method in imbalanced data sets learning",
      "authors": [
        "H Han",
        "W.-Y Wang",
        "B.-H Mao"
      ],
      "year": "2005",
      "venue": "International Conference on Intelligent Computing"
    },
    {
      "citation_id": "43",
      "title": "A deep-ensemble-level-based interpretable takagi-sugeno-kang fuzzy classifier for imbalanced data",
      "authors": [
        "G Wang",
        "T Zhou",
        "K.-S Choi",
        "J Lu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "44",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "45",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "46",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li",
        "F Tang",
        "M Zhao",
        "Y Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "47",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "48",
      "title": "Supervised sequence labelling with recurrent neural networks",
      "authors": [
        "A Graves",
        "A Graves"
      ],
      "year": "2012",
      "venue": "Supervised sequence labelling with recurrent neural networks"
    },
    {
      "citation_id": "49",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "50",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "51",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL"
    },
    {
      "citation_id": "52",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "53",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "54",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}