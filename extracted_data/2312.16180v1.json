{
  "paper_id": "2312.16180v1",
  "title": "Investigating Salient Representations And Label Variance In Dimensional Speech Emotion Analysis",
  "published": "2023-12-17T04:54:41Z",
  "authors": [
    "Vikramjit Mitra",
    "Jingping Nie",
    "Erdrin Azemi"
  ],
  "keywords": [
    "Salient representations",
    "speech emotion",
    "pre-trained model representations",
    "label variance",
    "robustness"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Representations derived from models such as BERT (Bidirectional Encoder Representations from Transformers) and Hu-BERT (Hidden units BERT), have helped to achieve stateof-the-art performance in dimensional speech emotion recognition. Despite their large dimensionality, and even though these representations are not tailored for emotion recognition tasks, they are frequently used to train large speech emotion models with high memory and computational costs. In this work, we show that there exist lower-dimensional subspaces within the these pre-trained representational spaces that offer a reduction in downstream model complexity without sacrificing performance on emotion estimation. In addition, we model label uncertainty in the form of grader opinion variance, and demonstrate that such information can improve the model's generalization capacity and robustness. Finally, we compare the robustness of the emotion models against acoustic degradations and observed that the reduced dimensional representations were able to retain the performance similar to the full-dimensional representations without significant regression in dimensional emotion performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech-based emotion estimation models aim to estimate the emotional state of a speaker from their speech utterances. Speech emotion models have garnered much attention to augment existing human-machine interaction systems, where such systems can not only recognize the words spoken by the speaker, but can also interpret how such words are expressed. Real-time speech-emotion models can help to improve human-computer interaction experiences, such as voice assistants  [1, 2]  and facilitate health/wellness applications such as health diagnoses  [3, 4] .\n\nSpeech emotion research has pursued two distinct definitions of emotion:  (1)  discrete emotions, and (2) dimensional emotions. Discrete emotions are categorized as, for example, fear, anger, joy, sadness, disgust, and surprise  [5] . These categories of emotions suffer from a lack of an agreed-upon lexicon of standard emotions, where the choice of emotions can vary  [5, 6] . Variation in discrete emotion definitions may result in annotation complexities, difficulty in realizing datasets with consistent emotion labels, and failure to include rare and/or complex emotional states. Dimensional emotion  [7]  helps to systematically represent the emotion space using a 3-dimensional model of Valence, Activation (or, Arousal) and Dominance, which maps discrete emotional states to a point in the 3-dimensional continuous space. In dimensional emotion, Activation reflects the energy in the voice, Valence indicates negativity versus positivity, and Dominance specifies how strong or submissive one may sound. Early studies on speech-based emotion detection focused on acted or elicited emotions  [8] , however, models trained with acted emotions often fail to generalize for spontaneous subtle emotions  [9] . Recently, attention has been given to datasets with spontaneous emotions  [10] , however, ground-truth labels for such data are often noisy due to varying degrees of grader agreement. Label uncertainty due to the variance in grader agreements is addressed by taking the mean of the grader decisions, however, the mean fails to account for examples that are challenging (where graders seem to mostly disagree with one another) compared to the samples that seem to be easier (where most graders agree). Modeling such uncertainty  [11]  can be useful in speech emotion modeling to account for audio samples that were perceptually difficult to annotate. In this work, we investigate incorporating label variance into emotion modeling and demonstrate a gain in performance compared to the model not using label variance.\n\nCombining lexical-and acoustic-based representations has been shown to boost model performance on emotion recognition from speech  [12, 13, 14, 15] . Recent studies have shown that using representations from pre-trained models can improve emotion recognition performance  [15, 14, 16, 17] . However, such pre-trained representations are high dimensional, which poses both computational and memory challenges for the downstream emotion modeling task. It is also an open question, whether all the dimensions in such a large dimensional representation are needed for emotion analysis tasks or if there exists a low-dimensional subspace that contains the relevant affect-specific information. In this work, we investigate obtaining a low-dimensional subspace of pre-trained representations for emotion modeling and demonstrate that reducing the input size does not result in significant arXiv:2312.16180v1 [cs.SD] 17 Dec 2023 regression of emotion model performance, but helps to reduce the model size substantially. Note that the focus of this work is to reduce the downstream emotion model's size, not the pre-trained model sizes. In this work, we demonstrate:\n\n1. Modeling label variance can help to achieve state-of-theart dimensional emotion estimation performance.\n\n2. Not all dimensions of the pre-trained model representation are needed for the emotion modeling task. 3. Model size can be reduced by reducing input feature dimension, without regression in model performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data",
      "text": "We have used the MSP-Podcast dataset (ver. 1.6)  [10, 18]  that contains speech spoken by English speakers. The speech segments contain single-speaker utterances with a duration of 3 to 11 seconds. The data contain manually assigned valence, activation and dominance scores (7-point Likert scores) from multiple graders. The data split is shown in Table  1 . To make our results comparable to literature  [13, 14] , we report results on Eval1.3 and Eval1.6 (see Table  1 ). To analyze the robustness of the emotion models, we add noise and reverberation to the Eval1.6 set at 3 SNR levels, resulting in three additional evaluation sets (see Table  1 ).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Representations & Acoustic Features",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Acoustic Representations From Hubert",
      "text": "We explore embeddings generated from HuBERT large, a pre-trained acoustic model  [19] , which was pre-trained on 60K hours of speech from the Libri-light dataset with 24 transformer layers and 1,024 embedding dimensions. In our study, we extracted the following embeddings: 1. HU BERT L embeddings from the 24 th layer of the pretrained HuBERT large model (no fine-tuning). 2. HU BERT A embeddings (24 th layer) from HU BERT L model fine-tuned on 100 hours of Librispeech data.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Lexical Representations From Bert",
      "text": "We have used pre-trained Bidirectional Encoder Representations from Transformers (BERT)  [20]  model to extract lexical embeddings for dimensional emotion model training. We rely on two pre-trained ASR models to estimate transcripts from speech: HU BERT A (mentioned above), and an inhouse ASR system. Given transcribed speech, we normalize the text (removing punctuation & considering all characters in lowercase) and use the resulting data as input to pre-trained BERT that generates 768-dimensional utterance-level embeddings from the 12 th layer.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Salient Representations",
      "text": "The pre-trained model embeddings have large dimensions. For example, both HU BERT L and HU BERT A have 1024 dimensions each, and when combined they result in a large input dimensional representation for the downstream emotion models. Prior studies  [14, 17]  have shown that when such large dimensional features are used to train speech emotion models, they result in small dimensional embedding space (in the range of ≈ 128 to 256). This finding indicates that dimensional emotion-relevant information in the input representations may reside in a subspace and the knowledge of that subspace can help to reduce the downstream model size. We investigate obtaining emotion-salient representations from the HU BERT L and HU BERT A representations, leveraging relationships between the input representation space and the targets. Prior work  [21]  has explored the input-output relationships of neurons to obtain neural saliency, and we use a similar idea to obtain the saliency of pre-trained model representations for dimensional emotion estimation. Let the k th representation of N dimensional HU BERT for an utterance y be represented by a vector H k,y = [X 1,k , . . . , X i,k , . . . , X M,k ], where M denotes the sequence length. Let the mean affect labels for y be L y ∈ {µ v,y , µ a,y , µ d,y }, where µ v,y , µ a,y , µ d,y represent the mean valence, activation and dominance scores for utterance y. H y,k in eq. 1 is obtained from H y,k by taking the mean across all the frames for utterance y. The cross-correlation based saliency (CCS) of k th dimension is given by\n\nwhere eq. 1 computes the absolute cross-correlation between labels L ∈ {µ v , µ a , µ d } and embeddings H k for dimension k for all utterances in the training set. γ k is the sum of the weighted cross-correlation between the k th dimension and all other dimensions, as shown in eq. 2\n\nwhere,\n\nIn our experiments we have used µ CCS given in eq. 3 to select salient dimensions in pre-trained representations. We also investigated the use of mutual information instead of cross-correlation to obtain saliency information, where the cross-correlation terms in eq. 1 and 2 are replaced with mutual information, resulting in mutual information based saliency (M IS). In addition, we have explored principal component analysis (P CA) based dimensionality reduction of the input representation to compare with the proposed saliency based dimensionality reduction approaches. Figure  1  shows the saliency scores from HU BERT L , across all three dimensional emotions, when sorted by saliency score (low to high from left to right). Note that around 20% of the HU BERT dimensions show salience ≤ 0.05, which can be removed for dimensional emotion modeling. Figure  3  shows the scatter plot of the saliency scores for HU BERT A representations (N=1024 dim.), across valence, activation and dominance, where each circle represents a HU BERT A dimension, and the color-coded filled circles represent the top 60% salient HU BERT A dimensions based on CCS. Train1.6 data (see Table  1 , where the performance on a heldout Valid1.6 set was used for model selection and early stopping. Concordance correlation coefficient (CCC)  [22]  is used as the loss function (L ccc ) (see eq. (  4 )), where L ccc is a combination (α = 1/3 and β = 1/3) of CCC's obtained from each of the three dimensional emotions. CCC is defined by eq. (  5 ), where µ x and µ y are the means, σ 2 x and σ 2 y are the corresponding variances for the estimated and ground-truth variables, and ρ is the correlation coefficient between those variables. The models are trained with a mini-batch size of 64 and a learning rate of 0.0005. The GRU network had two layers with 256 neurons each and an embedding size of 128, the convolution layer had a kernel size of 3.\n\n(5)   2 .\n\nWe investigated the model performance when: (i) incorporating target variance for both Eval1.3 and Eval1.6 test sets across all dimensions; (ii) reducing the model size as a consequence of input representation selection based on saliency (CIS); and (iii) under noise conditions as shown in Table  2 . Incorporating target variance helped to improve emotion estimation performance compared to the model not using it, where 1 to 2 % relative improvement (statistically significant) in CCC was observed across all dimensional emotions for both Eval1.3 and Eval1.6 test-sets. Removing 20%, 40%, and 6MB. This reductions in model sizes corresponded to relative decreases in CCC from 1 to 6% (statistically significant). Note that the impact of noise on the performance of models based on salient representations was minimal. Table  2  shows that for 16% reduction the model's performance is comparable, while for 32% and 49% reduction, there is some regression, compared to the baseline. When comparing the performance of models trained with full feature representations against those employing saliencybased feature selection (% Input Resps. = 80%, 60%, and 40%), it is observed that the mean squared error (MSE) for estimating three dimensions across various 7-point Likert score bins remains consistent despite the reduction in model size. Finally, we compared emotion estimation performance of models trained with 80% of pre-trained model dimensions, where the dimensionality reduction was performed with saliency-based (CCS and M IS) and P CA based approaches. Table  3  shows that both the saliency based approach (CCS and M IS) performed better than P CA. Saliency based approaches reduced dimension by removing less-salient input feature dimensions, while retaining the temporal structure of the residual features. On the other hand, P CA uses a singular value decomposition of the data to project it to a lower dimensional space and in that process does not retain the temporal structure of each of the feature dimensions. The lack of temporal structure in P CA based dimensionality reduction attributes to its lower performance (see table  3 ) when compared to the saliency based approaches. Note that both CCS and M IS based dimensionality reduction approaches performed similarly (see Table  3 ), where CCS performed better than M IS only for Eval1.6 activation and dominance estimation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusions",
      "text": "In this work, we investigated if it is possible to select salient representations from a pre-trained model for the task of dimensional emotion estimation from speech. We observed that such selection can result in substantial model size reduction, without significant regression in performance. We evaluated the model performance under unseen acoustic distortions, and observed that saliency-driven representation selection helped the reduced-sized models to perform as-good-as the large baseline models, without substantial regression in model's generalization capacity. In the future, we plan to investigate saliency-driven pruning of the downstream models, to investigate if the model size can be reduced even further without regression in performance and generalization capacity.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the saliency scores from HUBERTL, across all",
      "page": 3
    },
    {
      "caption": "Figure 1: HUBERT Large saliency plot",
      "page": 3
    },
    {
      "caption": "Figure 2: HUBERT ASR Saliency scatter plot across valence,",
      "page": 3
    },
    {
      "caption": "Figure 4: The model was trained with",
      "page": 3
    },
    {
      "caption": "Figure 3: Multi-modal emotion estimation model",
      "page": 3
    },
    {
      "caption": "Figure 3: with the following inputs: embeddings from (i) HuBERTL",
      "page": 3
    },
    {
      "caption": "Figure 4: Mean squared error (MSE) for valence estimation at",
      "page": 4
    },
    {
      "caption": "Figure 4: Note that MSE increased for",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Apple": "ABSTRACT"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "Representations derived from models such as BERT (Bidirec-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "tional Encoder Representations from Transformers) and Hu-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "BERT (Hidden units BERT), have helped to achieve state-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "of-the-art performance in dimensional speech emotion recog-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "nition. Despite their\nlarge dimensionality, and even though"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "these representations are not tailored for emotion recognition"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "tasks,\nthey are frequently used to train large speech emotion"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "models with high memory and computational costs.\nIn this"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "work, we show that\nthere exist\nlower-dimensional subspaces"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "within the these pre-trained representational spaces that offer"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "a reduction in downstream model complexity without sacri-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "ficing performance on emotion estimation.\nIn addition, we"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "model\nlabel uncertainty in the form of grader opinion vari-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "ance, and demonstrate that such information can improve the"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "model’s generalization capacity and robustness. Finally, we"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "compare the robustness of the emotion models against acous-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "tic degradations and observed that\nthe reduced dimensional"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "representations were able to retain the performance similar"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "to the full-dimensional representations without significant re-"
        },
        {
          "Apple": ""
        },
        {
          "Apple": "gression in dimensional emotion performance."
        },
        {
          "Apple": ""
        },
        {
          "Apple": "Index Terms— Salient representations, speech emotion,"
        },
        {
          "Apple": "pre-trained model representations, label variance, robustness"
        },
        {
          "Apple": ""
        },
        {
          "Apple": ""
        },
        {
          "Apple": "1.\nINTRODUCTION"
        },
        {
          "Apple": ""
        },
        {
          "Apple": ""
        },
        {
          "Apple": "Speech-based emotion estimation models aim to estimate the"
        },
        {
          "Apple": "emotional\nstate of a speaker\nfrom their\nspeech utterances."
        },
        {
          "Apple": "Speech\nemotion models\nhave\ngarnered much\nattention\nto"
        },
        {
          "Apple": "augment existing human-machine interaction systems, where"
        },
        {
          "Apple": "such systems can not only recognize the words\nspoken by"
        },
        {
          "Apple": "the speaker, but can also interpret how such words are ex-"
        },
        {
          "Apple": "pressed. Real-time speech-emotion models can help to im-"
        },
        {
          "Apple": "prove human-computer interaction experiences, such as voice"
        },
        {
          "Apple": "assistants\n[1, 2]\nand facilitate health/wellness\napplications"
        },
        {
          "Apple": "such as health diagnoses [3, 4]."
        },
        {
          "Apple": "Speech emotion research has pursued two distinct defini-"
        },
        {
          "Apple": "tions of emotion:\n(1) discrete emotions, and (2) dimensional"
        },
        {
          "Apple": "emotions. Discrete emotions are categorized as, for example,"
        },
        {
          "Apple": "fear, anger, joy, sadness, disgust, and surprise [5]. These cat-"
        },
        {
          "Apple": "egories of emotions suffer from a lack of an agreed-upon lex-"
        },
        {
          "Apple": "icon of standard emotions, where the choice of emotions can"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "tation space and the targets.\nPrior work [21] has explored"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "the\ninput-output\nrelationships of neurons\nto obtain neural"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "saliency, and we use a similar idea to obtain the saliency of"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "pre-trained model\nrepresentations\nfor dimensional emotion"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "estimation.\nLet\nthe kth\nrepresentation of N dimensional"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "HU BERT\ny\nfor\nan\nutterance\nbe\nrepresented\nby\na\nvec-"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "tor Hk,y = [X1,k, . . . , Xi,k, . . . , XM,k], where M denotes"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "the\nsequence\nlength.\nLet\nthe mean affect\nlabels\nfor y be"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "the\nLy ∈ {µv,y, µa,y, µd,y}, where µv,y, µa,y, µd,y represent"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "mean valence, activation and dominance scores for utterance"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "y. H y,k in eq.\n1 is obtained from Hy,k by taking the mean"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": ""
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "across all\nthe frames for utterance y.\nThe cross-correlation"
        },
        {
          "tions,\nleveraging relationships between the input\nrepresen-": "based saliency (CCS) of kth dimension is given by"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "regression of emotion model performance, but helps to reduce": "the model size substantially. Note that the focus of this work",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "house ASR system. Given transcribed speech, we normalize"
        },
        {
          "regression of emotion model performance, but helps to reduce": "is to reduce the downstream emotion model’s size, not\nthe",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "the text\n(removing punctuation & considering all characters"
        },
        {
          "regression of emotion model performance, but helps to reduce": "pre-trained model sizes. In this work, we demonstrate:",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "in lowercase) and use the resulting data as input to pre-trained"
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "BERT that generates 768-dimensional utterance-level embed-"
        },
        {
          "regression of emotion model performance, but helps to reduce": "1. Modeling label variance can help to achieve state-of-the-",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "dings from the 12th layer."
        },
        {
          "regression of emotion model performance, but helps to reduce": "art dimensional emotion estimation performance.",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "2. Not all dimensions of the pre-trained model representation",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "3.3.\nSalient representations"
        },
        {
          "regression of emotion model performance, but helps to reduce": "are needed for the emotion modeling task.",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "The pre-trained model embeddings have large dimensions."
        },
        {
          "regression of emotion model performance, but helps to reduce": "3. Model size can be reduced by reducing input\nfeature di-",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "mension, without regression in model performance.",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "For example, both HU BERTL and HU BERTA have 1024"
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "dimensions each, and when combined they result\nin a large"
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "input dimensional representation for the downstream emotion"
        },
        {
          "regression of emotion model performance, but helps to reduce": "2. DATA",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "models.\nPrior studies [14, 17] have shown that when such"
        },
        {
          "regression of emotion model performance, but helps to reduce": "We have used the MSP-Podcast dataset (ver. 1.6) [10, 18] that",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "large dimensional\nfeatures are used to train speech emotion"
        },
        {
          "regression of emotion model performance, but helps to reduce": "contains speech spoken by English speakers. The speech seg-",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "models,\nthey result\nin small dimensional embedding space"
        },
        {
          "regression of emotion model performance, but helps to reduce": "ments contain single-speaker utterances with a duration of 3",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "(in the range of ≈ 128 to 256).\nThis finding indicates that"
        },
        {
          "regression of emotion model performance, but helps to reduce": "to 11 seconds. The data contain manually assigned valence,",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "dimensional emotion-relevant\ninformation in the input\nrep-"
        },
        {
          "regression of emotion model performance, but helps to reduce": "activation and dominance scores (7-point Likert scores) from",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "resentations may reside\nin a\nsubspace\nand the knowledge"
        },
        {
          "regression of emotion model performance, but helps to reduce": "multiple graders. The data split is shown in Table 1. To make",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "of\nthat subspace can help to reduce the downstream model"
        },
        {
          "regression of emotion model performance, but helps to reduce": "our results comparable to literature [13, 14], we report results",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "size. We investigate obtaining emotion-salient\nrepresenta-"
        },
        {
          "regression of emotion model performance, but helps to reduce": "on Eval1.3 and Eval1.6 (see Table 1). To analyze the robust-",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "tions\nfrom the HU BERTL and HU BERTA representa-"
        },
        {
          "regression of emotion model performance, but helps to reduce": "ness of the emotion models, we add noise and reverberation",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "tions,\nleveraging relationships between the input\nrepresen-"
        },
        {
          "regression of emotion model performance, but helps to reduce": "to the Eval1.6 set at 3 SNR levels, resulting in three additional",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "tation space and the targets.\nPrior work [21] has explored"
        },
        {
          "regression of emotion model performance, but helps to reduce": "evaluation sets (see Table 1).",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "the\ninput-output\nrelationships of neurons\nto obtain neural"
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "saliency, and we use a similar idea to obtain the saliency of"
        },
        {
          "regression of emotion model performance, but helps to reduce": "Table 1. MSP-podcast data split and noise-degraded sets.",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "pre-trained model\nrepresentations\nfor dimensional emotion"
        },
        {
          "regression of emotion model performance, but helps to reduce": "Split\nHours\nDescription",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "estimation.\nLet\nthe kth\nrepresentation of N dimensional"
        },
        {
          "regression of emotion model performance, but helps to reduce": "Train1.6\n85\nTraining set",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "HU BERT\ny\nfor\nan\nutterance\nbe\nrepresented\nby\na\nvec-"
        },
        {
          "regression of emotion model performance, but helps to reduce": "Valid1.6\n15\nValidation set",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "tor Hk,y = [X1,k, . . . , Xi,k, . . . , XM,k], where M denotes"
        },
        {
          "regression of emotion model performance, but helps to reduce": "Eval1.3\n22\nPodcast1.3 evaluation set",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "the\nsequence\nlength.\nLet\nthe mean affect\nlabels\nfor y be"
        },
        {
          "regression of emotion model performance, but helps to reduce": "Eval1.6\n25\nPodcast1.6 evaluation set",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "the\nLy ∈ {µv,y, µa,y, µd,y}, where µv,y, µa,y, µd,y represent"
        },
        {
          "regression of emotion model performance, but helps to reduce": "25\nEval1.6 + noise within 20-30 dB\nEval1.625dB",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "mean valence, activation and dominance scores for utterance"
        },
        {
          "regression of emotion model performance, but helps to reduce": "25\nEval1.6 + noise within 10-20 dB\nEval1.615dB",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "y. H y,k in eq.\n1 is obtained from Hy,k by taking the mean"
        },
        {
          "regression of emotion model performance, but helps to reduce": "25\nEval1.6 + noise within 0-10 dB\nEval1.65dB",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "across all\nthe frames for utterance y.\nThe cross-correlation"
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "based saliency (CCS) of kth dimension is given by"
        },
        {
          "regression of emotion model performance, but helps to reduce": "3. REPRESENTATIONS & ACOUSTIC FEATURES",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "Cov(H k, L)"
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "(cid:13)(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)(cid:13)\n(1)\n+ γk\nSCCS ="
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "σHk σL"
        },
        {
          "regression of emotion model performance, but helps to reduce": "3.1. Acoustic Representations from HuBERT",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "where eq. 1 computes the absolute cross-correlation between"
        },
        {
          "regression of emotion model performance, but helps to reduce": "We explore embeddings generated from HuBERT large,\na",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "labels L ∈ {µv, µa, µd} and embeddings H k for dimension"
        },
        {
          "regression of emotion model performance, but helps to reduce": "pre-trained acoustic model\n[19], which was pre-trained on",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "k for all utterances in the training set.\nγk is the sum of the"
        },
        {
          "regression of emotion model performance, but helps to reduce": "60K hours of\nspeech from the Libri-light dataset with 24",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "weighted cross-correlation between the kth dimension and all"
        },
        {
          "regression of emotion model performance, but helps to reduce": "transformer layers and 1,024 embedding dimensions.\nIn our",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": ""
        },
        {
          "regression of emotion model performance, but helps to reduce": "",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "other dimensions, as shown in eq. 2"
        },
        {
          "regression of emotion model performance, but helps to reduce": "study, we extracted the following embeddings:",
          "and an in-\nfrom speech: HU BERTA (mentioned above),": "N(cid:88)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "also investigated the use of mutual\ninformation instead of": "cross-correlation to obtain saliency information, where the",
          "Train1.6 data (see Table 1, where the performance on a held-": "out Valid1.6 set was used for model selection and early stop-"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "cross-correlation terms\nin eq.\n1 and 2 are\nreplaced with",
          "Train1.6 data (see Table 1, where the performance on a held-": "ping. Concordance correlation coefficient (CCC) [22] is used"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "mutual\ninformation,\nresulting in mutual\ninformation based",
          "Train1.6 data (see Table 1, where the performance on a held-": "as the loss function (Lccc) (see eq. (4)), where Lccc is a com-"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "saliency (M IS).\nIn addition, we have\nexplored principal",
          "Train1.6 data (see Table 1, where the performance on a held-": "bination (α = 1/3 and β = 1/3) of CCC’s obtained from"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "component analysis (P CA) based dimensionality reduction",
          "Train1.6 data (see Table 1, where the performance on a held-": "each of the three dimensional emotions. CCC is defined by"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "of\nthe\ninput\nrepresentation to compare with the proposed",
          "Train1.6 data (see Table 1, where the performance on a held-": "eq.\n(5), where µx and µy are the means, σ2\nx and σ2\ny are the"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "saliency based dimensionality reduction approaches. Figure",
          "Train1.6 data (see Table 1, where the performance on a held-": "corresponding variances for\nthe estimated and ground-truth"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "1 shows\nthe saliency scores\nacross all\nfrom HU BERTL,",
          "Train1.6 data (see Table 1, where the performance on a held-": "variables, and ρ is the correlation coefficient between those"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "three dimensional emotions, when sorted by saliency score",
          "Train1.6 data (see Table 1, where the performance on a held-": "variables. The models are trained with a mini-batch size of"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "(low to high from left\nto right). Note that around 20% of the",
          "Train1.6 data (see Table 1, where the performance on a held-": "64 and a learning rate of 0.0005. The GRU network had two"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "HU BERT dimensions show salience ≤ 0.05, which can be",
          "Train1.6 data (see Table 1, where the performance on a held-": "layers with 256 neurons each and an embedding size of 128,"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "removed for dimensional emotion modeling. Figure 3 shows",
          "Train1.6 data (see Table 1, where the performance on a held-": "the convolution layer had a kernel size of 3."
        },
        {
          "also investigated the use of mutual\ninformation instead of": "the scatter plot of\nthe saliency scores for HU BERTA rep-",
          "Train1.6 data (see Table 1, where the performance on a held-": ""
        },
        {
          "also investigated the use of mutual\ninformation instead of": "",
          "Train1.6 data (see Table 1, where the performance on a held-": "(4)\nLccc = −(αCCCv + βCCCa + (1 − α − β)CCCd),"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "resentations\n(N=1024 dim.),\nacross valence,\nactivation and",
          "Train1.6 data (see Table 1, where the performance on a held-": ""
        },
        {
          "also investigated the use of mutual\ninformation instead of": "",
          "Train1.6 data (see Table 1, where the performance on a held-": "2ρσxσy"
        },
        {
          "also investigated the use of mutual\ninformation instead of": "dominance, where each circle represents a HU BERTA di-",
          "Train1.6 data (see Table 1, where the performance on a held-": ""
        },
        {
          "also investigated the use of mutual\ninformation instead of": "",
          "Train1.6 data (see Table 1, where the performance on a held-": "CCC ="
        },
        {
          "also investigated the use of mutual\ninformation instead of": "mension, and the color-coded filled circles represent\nthe top",
          "Train1.6 data (see Table 1, where the performance on a held-": "(5)\nσ2\nx + σ2\ny + (µx − µy)2 ."
        },
        {
          "also investigated the use of mutual\ninformation instead of": "60% salient HU BERTA dimensions based on CCS.",
          "Train1.6 data (see Table 1, where the performance on a held-": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: shows that both the saliency based",
      "data": [
        {
          "Table 2. Dimensional emotion estimation Concordance Cor-": "relation Coefficient (CCC) performance of models: (1) with-"
        },
        {
          "Table 2. Dimensional emotion estimation Concordance Cor-": "target variance (w/o) and with target variance (w),"
        },
        {
          "Table 2. Dimensional emotion estimation Concordance Cor-": "less salient\n(using CCS)\ninput"
        },
        {
          "Table 2. Dimensional emotion estimation Concordance Cor-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: shows that both the saliency based",
      "data": [
        {
          "out": "with removal of",
          "target variance (w/o) and with target variance (w),\n(ii)": "less salient\n(using CCS)\ninput\nrepresenta-"
        },
        {
          "out": "tions, and (iii) under noisy conditions",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "target\n% Input\n↓ % Rel."
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "variance\nReps.\nmodel size\nAct\nVal\nDom"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w/o\n100%\n-\n0.77\n0.68\n0.69"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n100%\n-\n0.78\n0.69\n0.70"
        },
        {
          "out": "Eval1.3",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n80%\n16%\n0.77\n0.68\n0.69"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n60%\n32%\n0.76\n0.67\n0.68"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n40%\n49%\n0.75\n0.66\n0.67"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w/o\n100%\n-\n0.74\n0.66\n0.66"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n100%\n-\n0.75\n0.67\n0.67"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "Eval1.6",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n80%\n16%\n0.75\n0.66\n0.67"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n60%\n32%\n0.74\n0.65\n0.66"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n40%\n49%\n0.73\n0.64\n0.65"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n100%\n-\n0.72\n0.66\n0.63"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n80%\n16%\n0.72\n0.65\n0.63"
        },
        {
          "out": "Eval1.625dB",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n60%\n32%\n0.72\n0.65\n0.62"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n40%\n49%\n0.70\n0.64\n0.60"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n100%\n-\n0.69\n0.64\n0.61"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n80%\n16%\n0.69\n0.64\n0.60"
        },
        {
          "out": "Eval1.615dB",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n60%\n32%\n0.68\n0.64\n0.60"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n40%\n49%\n0.66\n0.63\n0.56"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n100%\n-\n0.54\n0.57\n0.45"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n80%\n16%\n0.54\n0.57\n0.44"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "Eval1.65dB",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n60%\n32%\n0.51\n0.59\n0.43"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "w\n40%\n49%\n0.48\n0.59\n0.39"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "Table 3. Dimensional emotion estimation CCC after select-"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "ing 80% of the input representations using CCS, M IS and"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "P CA based approaches",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "Eval1.3\nEval1.6"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "System"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "act\nval\ndom\nact\nval\ndom"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "P CA\n0.75\n0.67\n0.67\n0.73\n0.65\n0.65"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "M IS\n0.77\n0.68\n0.69\n0.74\n0.66\n0.66"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "CCS\n0.77\n0.68\n0.69\n0.75\n0.66\n0.67"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "60% of input representations based on saliency (% Input Re-"
        },
        {
          "out": "sps.",
          "target variance (w/o) and with target variance (w),\n(ii)": "= 80%, 60%, and 40%)\ntranslated to a relative 16%,"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": ""
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "32%, and 49 % reduction in model size, where the baseline"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "model size was 15.6MB. This reductions in model sizes cor-"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "responded to relative decreases in CCC from 1 to 6% (sta-"
        },
        {
          "out": "tistically significant). Note that",
          "target variance (w/o) and with target variance (w),\n(ii)": "the impact of noise on the"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "performance of models based on salient representations was"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "minimal. Table 2 shows that for 16% reduction the model’s"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "performance is comparable, while for 32% and 49% reduc-"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "tion, there is some regression, compared to the baseline."
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "When comparing the performance of models trained with"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "full feature representations against those employing saliency-"
        },
        {
          "out": "based feature selection (% Input Resps.",
          "target variance (w/o) and with target variance (w),\n(ii)": "= 80%, 60%, and"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "40%), it is observed that the mean squared error (MSE) for es-"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "timating three dimensions across various 7-point Likert score"
        },
        {
          "out": "",
          "target variance (w/o) and with target variance (w),\n(ii)": "bins remains consistent despite the reduction in model size."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "Wilson,\n“Multi-modal\nlearning\nfor\nspeech\nemotion"
        },
        {
          "6. REFERENCES": "[1] V. Mitra, S. Booker, E. Marchi, D.S. Farrar, U.D. Peitz,",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "recognition: An analysis and comparison of asr out-"
        },
        {
          "6. REFERENCES": "B. Cheng, E. Teves, A. Mehta, and D. Naik,\n“Lever-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "In-\nputs with ground truth transcription.,”\nin Proc. of"
        },
        {
          "6. REFERENCES": "aging acoustic cues and paralinguistic embeddings\nto",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "terspeech, 2019, pp. 3302–3306."
        },
        {
          "6. REFERENCES": "Proc.\ndetect expression from voice,”\nInterspeech, pp.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[13] A. Ghriss,\nB. Yang, V. Rozgic,\nE.\nShriberg,\nand"
        },
        {
          "6. REFERENCES": "1651–1655, 2019.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "C. Wang,\n“Sentiment-aware automatic speech recog-"
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "nition pre-training for enhanced speech emotion recog-"
        },
        {
          "6. REFERENCES": "[2] V. Kowtha, V. Mitra, C. Bartels, E. Marchi, S. Booker,",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "nition,” Proc. of ICASSP, pp. 7347–7351, 2022."
        },
        {
          "6. REFERENCES": "W. Caruso, S. Kajarekar, and D. Naik, “Detecting emo-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "tion primitives from speech and their use in discerning",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[14] S. Srinivasan, Z. Huang, and K. Kirchhoff,\n“Represen-"
        },
        {
          "6. REFERENCES": "categorical emotions,” in Proc. of ICASSP. IEEE, 2020,",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "tation learning through cross-modal conditional teacher-"
        },
        {
          "6. REFERENCES": "pp. 7164–7168.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "student\ntraining for speech emotion recognition,”\npp."
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "6442–6446, 2022."
        },
        {
          "6. REFERENCES": "[3] B. Desmet and V. Hoste, “Emotion detection in suicide",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "notes,”\nExpert Systems with Applications, vol. 40, no.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[15] S.\nSiriwardhana,\nA.\nReis,\nR. Weerasekera,\nand"
        },
        {
          "6. REFERENCES": "16, pp. 6351–6358, 2013.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "S. Nanayakkara,\n“Jointly fine-tuning ”BERT-like” self"
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "supervised models to improve multimodal speech emo-"
        },
        {
          "6. REFERENCES": "[4] B. Stasak, J. Epps, N. Cummins, and R. Goecke,\n“An",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "tion recognition,”\narXiv preprint arXiv:2008.06682,"
        },
        {
          "6. REFERENCES": "investigation of emotional speech in depression classifi-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "2020."
        },
        {
          "6. REFERENCES": "cation.,” in Proc. of Interspeech, 2016, pp. 485–489.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[16] V. Mitra, H.Y.S. Chien, V. Kowtha,\nJ.Y. Cheng,\nand"
        },
        {
          "6. REFERENCES": "[5] P. Ekman, “An argument for basic emotions,” Cognition",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "E. Azemi,\n“Speech emotion:\nInvestigating model rep-"
        },
        {
          "6. REFERENCES": "& emotion, vol. 6, no. 3-4, pp. 169–200, 1992.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "resentations, multi-task learning and knowledge distilla-"
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "tion,” Proc. of Interspeech, 2022."
        },
        {
          "6. REFERENCES": "[6] A.S. Cowen and D. Keltner,\n“Self-report captures 27",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[17] V. Mitra, V. Kowtha, H.Y.S. Chien, E. Azemi,\nand"
        },
        {
          "6. REFERENCES": "distinct\ncategories of\nemotion bridged by continuous",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "C. Avendano,\n“Pre-trained model\nrepresentations and"
        },
        {
          "6. REFERENCES": "gradients,” Proc. of\nthe National Academy of Sciences,",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "their robustness against noise for speech emotion analy-"
        },
        {
          "6. REFERENCES": "vol. 114, no. 38, pp. E7900–E7909, 2017.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "sis,” in Proc. of ICASSP. IEEE, 2023, pp. 1–5."
        },
        {
          "6. REFERENCES": "[7]\nJ. Posner,\nJ.A. Russell, and B.S. Peterson,\n“The cir-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[18] R. Lotfian and C. Busso,\n“Building naturalistic emo-"
        },
        {
          "6. REFERENCES": "cumplex model of affect: An integrative approach to af-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "tionally balanced speech corpus by retrieving emotional"
        },
        {
          "6. REFERENCES": "fective neuroscience, cognitive development, and psy-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "IEEE Trans.\nspeech from existing podcast recordings,”"
        },
        {
          "6. REFERENCES": "chopathology,” Development and psychopathology, vol.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "on Affective Computing, vol. 10, no. 4, pp. 471–483,"
        },
        {
          "6. REFERENCES": "17, no. 3, pp. 715–734, 2005.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "2017."
        },
        {
          "6. REFERENCES": "[8] C.\nBusso, M.\nBulut,\nC.C.\nLee,\nA. Kazemzadeh,",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[19] W.N.\nHsu,\nB.\nBolte,\nY\n.H.\nTsai,\nK.\nLakhotia,"
        },
        {
          "6. REFERENCES": "E. Mower,\nJ.N. Kim, S.and Chang, S. Lee,\nand S.S.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "R. Salakhutdinov, and A. Mohamed,\n“HuBERT: Self-"
        },
        {
          "6. REFERENCES": "Narayanan,\n“Iemocap:\nInteractive\nemotional dyadic",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "supervised speech representation learning by masked"
        },
        {
          "6. REFERENCES": "Language\nresources and\nmotion capture database,”",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "prediction of hidden units,” IEEE/ACM Transactions on"
        },
        {
          "6. REFERENCES": "evaluation, vol. 42, no. 4, pp. 335–359, 2008.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "Audio, Speech, and Language Processing, vol. 29, pp."
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "3451–3460, 2021."
        },
        {
          "6. REFERENCES": "[9] E. Douglas-Cowie, L. Devillers, J.C. Martin, R. Cowie,",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "S. Savvidou, S. Abrilian,\nand C. Cox,\n“Multimodal",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[20]\nJ Devlin, Chang M.W., K. Lee,\nand K. Toutanova,"
        },
        {
          "6. REFERENCES": "databases of everyday emotion: Facing up to complex-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "“BERT: Pre-training of deep bidirectional\ntransformers"
        },
        {
          "6. REFERENCES": "ity,” in Proc. of Interspeech, 2005.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "for language understanding,”\nin Proc. of NAACL-HLT,"
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "2019, pp. 4171–4186."
        },
        {
          "6. REFERENCES": "[10] S. Mariooryad, R. Lotfian, and C. Busso,\n“Building a",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[21] V. Mitra and H. Franco,\n“Investigation and analysis"
        },
        {
          "6. REFERENCES": "naturalistic emotional\nspeech corpus by retrieving ex-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "of hyper and hypo neuron pruning to selectively update"
        },
        {
          "6. REFERENCES": "pressive behaviors\nfrom existing speech corpora,”\nin",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "neurons during unsupervised adaptation,” Digital Signal"
        },
        {
          "6. REFERENCES": "Proc. of Interspeech, 2014.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "Processing, vol. 99, pp. 102655, 2020."
        },
        {
          "6. REFERENCES": "[11] N.R. Prabhu, N. Lehmann-Willenbrock, and T. Gerk-",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "[22]\nI. Lawrence and K. Lin,\n“A concordance correlation"
        },
        {
          "6. REFERENCES": "mann,\n“Label uncertainty modeling and prediction for",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "coefficient\nto evaluate reproducibility,” Biometrics, pp."
        },
        {
          "6. REFERENCES": "speech emotion recognition using t-distributions,”\nin",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        },
        {
          "6. REFERENCES": "",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": "255–268, 1989."
        },
        {
          "6. REFERENCES": "Proc. of ACII. IEEE, 2022, pp. 1–8.",
          "[12] S. Sahu, V. Mitra, N. Seneviratne,\nand C.Y. Espy-": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Leveraging acoustic cues and paralinguistic embeddings to detect expression from voice",
      "authors": [
        "V Mitra",
        "S Booker",
        "E Marchi",
        "D Farrar",
        "U Peitz",
        "B Cheng",
        "E Teves",
        "A Mehta",
        "D Naik"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Detecting emotion primitives from speech and their use in discerning categorical emotions",
      "authors": [
        "V Kowtha",
        "V Mitra",
        "C Bartels",
        "E Marchi",
        "S Booker",
        "W Caruso",
        "S Kajarekar",
        "D Naik"
      ],
      "year": "2020",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Emotion detection in suicide notes",
      "authors": [
        "B Desmet",
        "V Hoste"
      ],
      "year": "2013",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "5",
      "title": "An investigation of emotional speech in depression classification",
      "authors": [
        "B Stasak",
        "J Epps",
        "N Cummins",
        "R Goecke"
      ],
      "year": "2016",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "6",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "7",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proc. of the National Academy of Sciences"
    },
    {
      "citation_id": "8",
      "title": "The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology",
      "authors": [
        "J Posner",
        "J Russell",
        "B Peterson"
      ],
      "year": "2005",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "J Kim",
        "S Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "10",
      "title": "Multimodal databases of everyday emotion: Facing up to complexity",
      "authors": [
        "E Douglas-Cowie",
        "L Devillers",
        "J Martin",
        "R Cowie",
        "S Savvidou",
        "S Abrilian",
        "C Cox"
      ],
      "year": "2005",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Building a naturalistic emotional speech corpus by retrieving expressive behaviors from existing speech corpora",
      "authors": [
        "S Mariooryad",
        "R Lotfian",
        "C Busso"
      ],
      "year": "2014",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Label uncertainty modeling and prediction for speech emotion recognition using t-distributions",
      "authors": [
        "N Prabhu",
        "N Lehmann-Willenbrock",
        "T Gerkmann"
      ],
      "year": "2022",
      "venue": "Proc. of ACII"
    },
    {
      "citation_id": "13",
      "title": "Multi-modal learning for speech emotion recognition: An analysis and comparison of asr outputs with ground truth transcription",
      "authors": [
        "S Sahu",
        "V Mitra",
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "year": "2019",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Sentiment-aware automatic speech recognition pre-training for enhanced speech emotion recognition",
      "authors": [
        "A Ghriss",
        "B Yang",
        "V Rozgic",
        "E Shriberg",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "15",
      "title": "Representation learning through cross-modal conditional teacherstudent training for speech emotion recognition",
      "authors": [
        "S Srinivasan",
        "Z Huang",
        "K Kirchhoff"
      ],
      "year": "2022",
      "venue": "Representation learning through cross-modal conditional teacherstudent training for speech emotion recognition"
    },
    {
      "citation_id": "16",
      "title": "Jointly fine-tuning \"BERT-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "Jointly fine-tuning \"BERT-like\" self supervised models to improve multimodal speech emotion recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion: Investigating model representations, multi-task learning and knowledge distillation",
      "authors": [
        "V Mitra",
        "H Chien",
        "V Kowtha",
        "J Cheng",
        "E Azemi"
      ],
      "year": "2022",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Pre-trained model representations and their robustness against noise for speech emotion analysis",
      "authors": [
        "V Mitra",
        "V Kowtha",
        "H Chien",
        "E Azemi",
        "C Avendano"
      ],
      "year": "2023",
      "venue": "Proc. of ICASSP"
    },
    {
      "citation_id": "19",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "HuBERT: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. of NAACL-HLT"
    },
    {
      "citation_id": "22",
      "title": "Investigation and analysis of hyper and hypo neuron pruning to selectively update neurons during unsupervised adaptation",
      "authors": [
        "V Mitra",
        "H Franco"
      ],
      "year": "2020",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    }
  ]
}