{
  "paper_id": "2504.03010v1",
  "title": "Emotion Recognition Using Convolutional Neural Networks",
  "published": "2025-04-03T20:08:32Z",
  "authors": [
    "Shaoyuan Xu",
    "Yang Cheng",
    "Qian Lin",
    "Jan P. Allebach"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion has an important role in daily life, as it helps people better communicate with and understand each other more efficiently. Facial expressions can be classified into 7 categories: angry, disgust, fear, happy, neutral, sad and surprise. How to detect and recognize these seven emotions has become a popular topic in the past decade. In this paper, we develop an emotion recognition system that can apply emotion recognition on both still images and real-time videos by using deep learning. We build our own emotion recognition classification and regression system from scratch, which includes dataset collection, data preprocessing , model training and testing. Given a certain image or a real-time video, our system is able to show the classification and regression results for all of the 7 emotions. The proposed system is tested on 2 different datasets, and achieved an accuracy of over 80%. Moreover, the result obtained from realtime testing proves the feasibility of implementing convolutional neural networks in real time to detect emotions accurately and efficiently.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As one of the most important features of human beings, emotion helps people communicate with and understand each other more efficiently. Therefore, detecting and recognizing various emotions has always been a popular topic. People detect emotions through different methods, such as voice intonation, body language and even electroencephalography  [1] . However, the most intuitive and practical way of detecting and recognizing emotions is still through facial expressions. In this paper, we propose a system to detect emotions by examining facial expressions. In our system, we follow the research work proposed by Paul Ekman  [2] , where the emotions are categorized into 7 classes: angry, disgust, fear, happy,neutral, sad and surprise, except that the category neutral is replaced with contempt.\n\nThere has been a lot of research work on emotion recognition, most of which uses traditional computer vision methods, such as LBP  [3] ; and machine learning classification methods, such as SVM  [4] . However, satisfying results could not be achieved due to the limitations of these methods, such as inadaptability to the change of facial muscles. Therefore, we have put much effort in investigating a new approach that take advantages of deep learning  1  .\n\nThere is also substantial research work done on emotion recognition using deep learning such as traditional model training methods using a specific network  [5] , or combining deep learn-ing with machine learning such as LBP  [6] . Although they obtain comparably high accuracy, there are two aspects that need to be improved. Firstly, most of them use traditional network structures such as VGG Net, Alex Net, or Google Net (including the improved versions of these network structures). This results in a large model size; so that it is extremely difficult to do real time emotion recognition. Secondly, most of the proposed systems only consider the classification scenario where the intensity information is missing in the results. But in practical usage, intensity information is as important as the classification result, because we want to know not only what emotions people have, but also the level of those emotions. In this paper, we solve these two problems by selecting an appropriate network structure for an accurate real-time emotion recognition. At the same time, we extend our classification results to the regression scenario so that the intensity information can be concluded from the results. We trained our emotion recognition model for both the classification scenario and the regression scenario. Figure  1  shows the flowchart of our emotion recognition project.\n\nThe paper is organized as follows. In the rest of Section 1, we introduce the seven classes of emotions and an overview of our approaches. Sections 2 and 3 describe how our emotion recognition system is trained using two different models, a classification model and a regression model. The conclusion is provided in Section 4.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Seven Classes Of Emotions",
      "text": "There are seven universal emotions: angry, disgust, fear, happy, neutral, sad and surprise. Examples of those emotions are shown in Figure 2  [7]  [8] and each emotion is described by some characteristics [9].\n\n• Angry: eyebrows are pulled down, upper eyelids are pulled up, lower eyelids are pulled up, margins of lips are rolled in and lips may be tightened.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition Classification Training",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Data Collection",
      "text": "Due to the lack of public datasets for emotion recognition tasks and the low quality of existing datasets, collecting enough datasets and examining them becomes the first challenging task. Firstly, there are 4 publicly available datasets: MUG-FED [10], CK+  [7]  [8], Japanese Female Facial Expression (Jaffe)  [11]  and KDEF  [12] . Figure  3  shows some sample images from these four datasets, and Table  1  contains the statistics. Secondly, we have collected our own dataset for testing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Preprocessing",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Cleaning",
      "text": "Since most of the public datasets contain raw images, very few of them can be directly used without further examination. Therefore, these datasets need to be cleaned in the first place.\n\nThere are 52 subjects in the MUG-FED Dataset. For each subject, it has 5-7 emotions and for each emotion, it has 3-7 attempts. And since all of the images are video frames, each emotion starts from neutral to the emotional expression of the strongest intensity and returns to neutral. Therefore, only the images that contain facial expressions of strong intensity should be selected. Table  2  shows the statistics of the MUG-FED Dataset after it is cleaned. It also provides us with 161 manually labeled images, which is used for validation. Besides these 4 datasets obtained from online sources, an additional 490 images were collected by us and are used to validate the model.\n\nEventually, there are 3 datasets for training and 3 datasets for validation, as shown in Table  3 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Face Alignment",
      "text": "Face alignment is another key step in dataset pre-processing.\n\nThe purpose is to remove potential uncertainties when applying our emotion recognition approach to real-time videos. For example, the position and the angle of the subject's head are changing as the video plays, which could affect the accuracy of the classification results if the face is not aligned in advance. With face alignment, the position of the head is aligned and the scale of the head is adjusted to have the same size, which eliminates the influence of any existing distortions on the recognition results.\n\nWe propose a novel face alignment algorithm that shows superior results compared to any existing method. Firstly, a face detector is used to detect the face in an image, then the Land Mark (LM) detector  [13]  is used to detect 68 landmark points of the face. A rotation matrix is then obtained based only the eye center coordinates. The traditional method uses the rotation matrix for face alignment. However, the resultant images can contain comparably useless background of large area and the eyes in different images are not at the same horizontal level, resulting in unsatisfying face classification results.\n\nWe improve the traditional face alignment by adding one more step. The aforementioned rotation matrix gets the coordinates of the 68 landmark points in the new coordinate system. Then we use the 1 st , 9 th and 17 th landmark points to get the left, bottom and right boundary of the face. The definition and the location of these facial landmark points are shown in Figure  4  [14] [15]  [16] . To get the top boundary, we stipulate that the length from the top boundary to the eye center is one-third of the height of the image. We use the boundary information to crop the original image into the one that contains smaller margins. Finally, the eyes of different images are adjusted to be at the same horizontal level. Figure  5  shows some sample images before and after face alignment. Note that all the images after face alignment are re-scaled to 128 × 128.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation",
      "text": "To increase the robustness of our model and to prevent it from being over-fitted, we apply data augmentation on the training dataset after face alignment. For each image, 7 images of different brightness and 28 images of different degree of blurring are created, resulting in a final training set with 1,148,812 images. Table  4  shows the statistics of the data augmentation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Training",
      "text": "In order to apply our emotion recognition system to real-time video, the model needs to be comparably small in size and fast in speed. We have tested several pre-traind models, such as the VGG-S  [17] , on real-time video with multiple rounds of finetuning. The validation accuracies are less than 60% which is far below our requirements. Moreover, the size of the VGG-S model is more than 500 MB which is too large to be implemented efficiently.\n\nTo reduce the model size, we modified the original VGG-S  [18]  model by reducing the kernel size and channel number as shown in Figure  6    [13] . Compared to the original VGG-S model, our model has a size of only 12.1 MB. It takes only 4.5 hours to train on more than 1 million images for 50,000 iterations. Besides a smaller size of the model, the validation accuracy obtained by using the new model reaches 85%, which is significantly higher than our previous results. The reason of getting higher accuracy",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition Regression Training",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Introduction",
      "text": "Although our emotion recognition classification model works well, it has its own drawback. And it is especially obvious when the classification model is applied in a real-time demo. Since our classification training dataset includes a lot of emotions that are not obvious or are of low intensity, this making them similar to one of the emotion categories in particular: neutral. This causes the prediction on the real-time video to be jittery, since in most cases, for example, a person does not need to express his or her happiness by a drop-jaw smile. And also, given an image or a frame of the video, our classification model can only tell if the facial expression is angry, disgust, fear, happy, neutral, sad or surprise; in other words, it is not able to tell the intensity of the emotion.\n\nTo solve these two problems, a regression model is used, where the ground truth labels become the intensities of the emotion, such as 20% happy and 80% neutral or 40% sad and 60% neutral. This additional information about the intensity of the emotion can be useful, especially in real-time videos.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "Among the four datasets collected from the online sources, only the MUG-FED Dataset is used because of the large number of images the dataset includes. However, the MUG-FED Dataset is more like an \"in-the-lab\" dataset, where all of the emotions included are standardized and all the images have the same background and consist of a purely frontal face. Since there are very few public in-the-wild datasets, especially for the task of emotion recognition, we collected our own dataset to train the model on a more \"in-the-wild\" dataset.\n\nUntil now, we have collected more than 7000 \"in-the-wild\" images containing facial expressions and we name this dataset as Emotion Intensity in the Wild Dataset. Table  5  shows the statistics of this dataset. It is worth noting that this dataset includes images containing heads at different angles, people with different races and ages, and backgrounds of different lighting conditions.\n\nTo train the regression model, we use both the MUG-FED and Emotion Intensity In the Wild datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Preprocessing",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Cleaning",
      "text": "Before an existing dataset obtained from online sources is used, it needs to be examined in the first place. As we introduced in the previous section, the images of the MUG-FED Dataset are consecutive frames obtained from videos. In a video for a specific emotion, the emotion starts from neutral to 100% facial expression and gradually returns to neutral. Therefore, for each attempt of expressing emotion, we select 9 images that contain facial expression intensities from 20% to 100% and back to 20% with an interval of 20%. An example is shown in Figure  7 . After dataset cleaning, we have collected 7,451 images for training and 981 images for validation for the MUG-FED Dataset. Each of these 7451 images is labeled with the intensity of the emotion.\n\nAnd for the Emotion Intensity In the Wild Dataset, after excluding some inappropriate images, we have 6141 images for training and 682 images for validation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Face Alignment",
      "text": "The procedure of face alignment is the same as the one introduced in the previous section. We utilized the 1 st , 9 th and 17 th landmark points to get the boundary of the faces, cropped them, aligned them and rescaled them to 128 × 128.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Augmentation",
      "text": "We experimented with two strategies of training, one with only the Emotion Intensity In The Wild Dataset, another with the combined dataset of Emotion Intensity In The Wild Dataset and the MUG-FED Dataset. The method of data augmentation remains the same, which includes changing brightness and blurring the images and gives the final training dataset, contains 6,141 images before data augmentation and 171,948 images after, and final validation dataset, containing 682 images before data augmentation and 19,096 images after for Emotion Intensity In The Wild Dataset. For the combined dataset, the final training dataset, containing 13,592 images before data augmentation and 380,576 images after, and the final validation dataset, containing 1,663 images before data augmentation and 46,564 images after, as shown in Table  7 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Training",
      "text": "The model framework used for the regression training is the same as for our classification model, except that the softmax loss function is replaced with the sigmoid cross entropy loss function.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Classification Results",
      "text": "As introduced in Section 3, the classification validation accuracy of the classification model is 85%. Figure  8  shows the validation confusion matrix for the validation dataset.\n\nIn order to test our classification model, we collected our own dataset which is called the HP Facial Expression Test Set, which contains 2443 images. The dataset was collected with 5 subjects doing 7 emotions while being video recorded and the images were selected from the video frames. Our model achieves an accuracy of 82% and takes only 13.68 seconds to test on the whole testing dataset (0.0056 s/image). This test was conducted on a workstation with an Nvidia Titan X GPU. Figure  9  shows some sample testing images and Figure  10  shows the testing confusion matrix.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Regression Results",
      "text": "The regression training also gets outstanding results. Figure  11  and Figure  12  shows the classification confusion matrices; and Table  6  shows the regression training results. Noting that, for the training and validation loss values, they are sigmoid cross entropy loss, the smaller the better and for the RMSE values, they represent the standard deviation of the prediction errors and are based on the datum that ranges from 0 to 1.\n\nAs indicated by the high accuracy which is around 77% and the small Root Mean Squared Error (RMSE) value which is below 0.13, our regression model performs well on the emotion recognition task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Real-Time Emotion Recognition",
      "text": "Currently, there are not many real-time emotion recognition frameworks, while the existing ones achieve only comparably low accuracy. However, our real-time demo version can detect people's frontal facial expressions accurately. Figure  13  shows some sample results from our real-time emotion recognition demo.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we apply emotion recognition using a deep learning method. The whole process includes data collection, data preprocessing, model training and model testing. Our contributions include:  • We built an emotion recognition framework from scratch. We first collected four public datasets and manually cleaned them. After that, we implemented data preprocessing, including labeling data, aligning faces and augmenting data.\n\nIn the training process, we designed our own model based on a VGG-S model but with a much smaller size, better accuracy and improved efficiency.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "• We Developed An Emotion Recognition Regression Training",
      "text": "framework to consider the intensity information of emotions. We collected our own Emotion Intensity In the Wild Dataset and defined a 5-level regression labeling scenario.\n\nOur experiment results show that the proposed system can recognize the emotion intensities with promising accuracies.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the flowchart",
      "page": 1
    },
    {
      "caption": "Figure 2: [7] [8] and each emotion is described by some charac-",
      "page": 1
    },
    {
      "caption": "Figure 1: Flowchart of Emotion Recognition Project.",
      "page": 2
    },
    {
      "caption": "Figure 2: Seven universal emotions: Neutral, Angry, Disgust,",
      "page": 2
    },
    {
      "caption": "Figure 3: shows some sample images from these four",
      "page": 2
    },
    {
      "caption": "Figure 3: Sample images of MUG-FED, CK+, Jaffe, and KDEF.",
      "page": 2
    },
    {
      "caption": "Figure 4: [14] [15] [16]. To get the top boundary, we stipulate that the",
      "page": 2
    },
    {
      "caption": "Figure 5: shows some sample images before and after",
      "page": 2
    },
    {
      "caption": "Figure 6: [13]. Compared to the original VGG-S model,",
      "page": 3
    },
    {
      "caption": "Figure 4: 68 points facial landmark system.",
      "page": 3
    },
    {
      "caption": "Figure 5: Comparison of images before and after face alignment.",
      "page": 4
    },
    {
      "caption": "Figure 7: After dataset",
      "page": 4
    },
    {
      "caption": "Figure 6: Framework of the classification model.",
      "page": 5
    },
    {
      "caption": "Figure 8: shows the validation",
      "page": 5
    },
    {
      "caption": "Figure 10: shows the testing con-",
      "page": 5
    },
    {
      "caption": "Figure 11: and Figure 12 shows the classification confusion matrices; and",
      "page": 5
    },
    {
      "caption": "Figure 13: shows some",
      "page": 5
    },
    {
      "caption": "Figure 7: Sample regression images from MUG-FED Dataset. These images are from one of the attempts that a subject does which have",
      "page": 6
    },
    {
      "caption": "Figure 8: Classification confusion matrix for our classification",
      "page": 6
    },
    {
      "caption": "Figure 9: Sample images from HP Facial Expression Test Set.",
      "page": 6
    },
    {
      "caption": "Figure 10: Classification confusion matrix for our self-collected",
      "page": 6
    },
    {
      "caption": "Figure 11: Classification confusion matrix for Emotion Intensity",
      "page": 7
    },
    {
      "caption": "Figure 12: Classification confusion matrix for the Combined",
      "page": 7
    },
    {
      "caption": "Figure 13: Sample result frames from real-time video demo for",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Database": "Extended\nCohn-\nKanade\nDataset\n(CK+)",
          "Facial Expression": "Angry,\nContempt,\nDisgust,\nFear,\nHappy,\nNeutral,\nSad and Surprise",
          "# of Subjects": "123",
          "# of Images": "593\nimage\nse-\nquences\n(327\nsequences\nhaving\ndiscrete\nemotion\nlabels)",
          "Gray/Color": "Mostly gray",
          "Size (pixel)": "640 × 490",
          "Ground Truth": "Facial\nex-\npression\nlabels\nand\nFACS labels",
          "Type": "Posed; spon-\ntaneous\nsmiles"
        },
        {
          "Database": "Japanese\nFemale\nFacial\nEx-\npression\n(Jaffe)",
          "Facial Expression": "Angry,\nDisgust,\nFear, Happy, Neu-\ntral,\nSad\nand\nSurprise",
          "# of Subjects": "10",
          "# of Images": "213 static images",
          "Gray/Color": "Gray",
          "Size (pixel)": "256 × 256",
          "Ground Truth": "Facial\nex-\npression\nlabels",
          "Type": "Posed"
        },
        {
          "Database": "Multimedia\nUnderstand-\ning\nGroup\n(MUG-FED)",
          "Facial Expression": "Angry,\nDisgust,\nFear, Happy, Neu-\ntral,\nSad\nand\nSurprise",
          "# of Subjects": "86",
          "# of Images": "1462\nsequences\nwith\nmore\nthan\n100K images",
          "Gray/Color": "Color",
          "Size (pixel)": "896 × 896",
          "Ground Truth": "Facial\nex-\npression and\nland\nmark\n(LM) labels",
          "Type": "Posed"
        },
        {
          "Database": "The\nKarolin-\nska\nDi-\nrected\nEmo-\ntional\nFaces\n(KDEF)",
          "Facial Expression": "Angry,\nDisgust,\nFear, Happy, Neu-\ntral,\nSad\nand\nSurprise",
          "# of Subjects": "70",
          "# of Images": "4900 images",
          "Gray/Color": "Color",
          "Size (pixel)": "562 × 762",
          "Ground Truth": "Facial\nex-\npression\nlabels",
          "Type": "Posed"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "Name",
          "Training": "MUG-FED,\nCK+\nand\nJaffe",
          "Validation": "MUG-FED (Manually Labeled by\nauthor), KDEF and Images of my-\nself"
        },
        {
          "Dataset": "Images\n# of",
          "Training": "41029",
          "Validation": "1867"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "20%",
          "Angry": "194",
          "Disgust": "147",
          "Fear": "93",
          "Happy": "236",
          "Neutral": "1093",
          "Sad": "210",
          "Surprise": "91",
          "Total (Without Neutral)": "971"
        },
        {
          "Dataset": "40%",
          "Angry": "323",
          "Disgust": "218",
          "Fear": "103",
          "Happy": "230",
          "Neutral": "",
          "Sad": "164",
          "Surprise": "218",
          "Total (Without Neutral)": "1256"
        },
        {
          "Dataset": "60%",
          "Angry": "221",
          "Disgust": "243",
          "Fear": "134",
          "Happy": "320",
          "Neutral": "",
          "Sad": "137",
          "Surprise": "279",
          "Total (Without Neutral)": "1334"
        },
        {
          "Dataset": "80%",
          "Angry": "207",
          "Disgust": "198",
          "Fear": "151",
          "Happy": "295",
          "Neutral": "",
          "Sad": "81",
          "Surprise": "316",
          "Total (Without Neutral)": "1248"
        },
        {
          "Dataset": "100%",
          "Angry": "227",
          "Disgust": "178",
          "Fear": "157",
          "Happy": "286",
          "Neutral": "",
          "Sad": "84",
          "Surprise": "420",
          "Total (Without Neutral)": "1352"
        },
        {
          "Dataset": "Total",
          "Angry": "1172",
          "Disgust": "984",
          "Fear": "638",
          "Happy": "1367",
          "Neutral": "1093",
          "Sad": "676",
          "Surprise": "1324",
          "Total (Without Neutral)": "6161 (7254 Total With Neutral)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "Emotion\nIntensity\nIn\nthe Wild\n(After data augmentation)",
          "Training": "6141\n(171948)",
          "Validation": "682\n(19096)"
        },
        {
          "Dataset": "Combined\nDataset\n(After data augmentation)",
          "Training": "13592\n(380576)",
          "Validation": "1663\n(46564)"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Article: Emotion recognition using speech and EEG signal a review",
      "authors": [
        "P Abhang",
        "S Rao",
        "B Gawali",
        "P Rokade"
      ],
      "year": "2011",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "2",
      "title": "Universals and cultural differences in the judgements of facial expressions of emotion",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1987",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "3",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2009",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "4",
      "title": "Page classification for print imaging pipeline",
      "authors": [
        "S Xu",
        "C Lu",
        "M Shaw",
        "P Bauer",
        "J Allebach"
      ],
      "year": "2017",
      "venue": "Color Imaging XXII: Displaying, Processing, Hardcopy, and Applications, (Part of IS&T Electronic Imaging"
    },
    {
      "citation_id": "5",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition in the wild via convolutional neural networks and mapped binary patterns",
      "authors": [
        "G Levi",
        "T Hassner"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, ICMI '15"
    },
    {
      "citation_id": "7",
      "title": "Comprehensive database for fa-emotional faces -KDEF",
      "authors": [
        "T Kanade",
        "J Cohn",
        "Y Tian"
      ],
      "year": "1998",
      "venue": "CD ROM from Department of Clinical Neuroscience"
    },
    {
      "citation_id": "8",
      "title": "CNN based facial landmark detection",
      "authors": [
        "R Mao",
        "Q Lin",
        "J Allebach"
      ],
      "year": "2018",
      "venue": "Imaging and Multimedia Analytics"
    },
    {
      "citation_id": "9",
      "title": "300 faces in-the-wild challenge: Database and results",
      "authors": [
        "C Sagonas",
        "E Antonakos",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "Image and Vision Computing (IMAVIS)"
    },
    {
      "citation_id": "10",
      "title": "A semiautomatic methodology for facial landmark annotation",
      "authors": [
        "C Sagonas",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of IEEE Int'l Conf. Computer Vision and Pattern Recognition (CVPR-W), 5th Workshop on Analysis and Modeling of Faces and Gestures (AMFG 2013)"
    },
    {
      "citation_id": "11",
      "title": "300 faces in-the-wild challenge: The first facial landmark localization challenge",
      "authors": [
        "C Sagonas",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Proceedings of IEEE Int'l Conf. on Computer Vision (ICCV-W)"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in the wild via convolutional neural networks and mapped binary patterns",
      "authors": [
        "G Levi",
        "T Hassner"
      ],
      "year": "2015",
      "venue": "Proc. ACM International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "13",
      "title": "Very deep convolutional networks for largescale image recognition",
      "authors": [
        "A Simonyan"
      ],
      "year": "2014",
      "venue": "CoRR"
    }
  ]
}