{
  "paper_id": "2309.01164v1",
  "title": "Noise Robust Speech Emotion Recognition With Signal-To-Noise Ratio Adapting Speech Enhancement",
  "published": "2023-09-03T13:00:04Z",
  "authors": [
    "Yu-Wen Chen",
    "Julia Hirschberg",
    "Yu Tsao"
  ],
  "keywords": [
    "Signal-to-noise-ratio level detection",
    "speech emotion recognition",
    "speech enhancement",
    "noise robustness"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) often experiences reduced performance due to background noise. In addition, making a prediction on signals with only background noise could undermine user trust in the system. In this study, we propose a Noise Robust Speech Emotion Recognition system, NRSER. NRSER employs speech enhancement (SE) to effectively reduce the noise in input signals. Then, the signal-to-noise-ratio (SNR)-level detection structure and waveform reconstitution strategy are introduced to reduce the negative impact of SE on speech signals with no or little background noise. Our experimental results show that NRSER can effectively improve the noise robustness of the SER system, including preventing the system from making emotion recognition on signals consisting solely of background noise. Moreover, the proposed SNR-level detection structure can be used individually for tasks such as data selection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) has many real-world applications, such as healthcare, stress monitoring, and marketing. However, in many use cases, users might not be speaking in a quiet environment. Therefore, the input signals are noisy, and a noisy input often leads to a significant drop in SER performance. Also, incorrect recognition of background noise may significantly reduce users' trust in the system; e.g., if a SER system recognizes different emotions while users change their environment without speaking, they might consider the system's recognition results unreliable. Hence, improving the noise robustness of SER is an essential research topic.\n\nPrevious studies have applied data augmentation to improve the noise robustness of SER. For example,  [1]  contaminated noises to the speech signals, and  [2]  used a parametric generative model to generate the noisy data. In addition, researchers have incorporated several speech enhancement (SE) techniques, which aims to improve the quality and intelligibility of speech signals. For example,  [3]  introduced the spec-tral subtraction and masking based SE;  [4]  applied the spectral subtraction, wiener filter, and MMSE;  [5]  investigated SE methods in cepstral and log-spectral domains; and  [6, 7, 8]  trained NN-based SE models. However, even though SE models can increase SER performance on noisy input speech signals, it also introduces artifacts and degrades SER performance on high signal-to-noise ratio (SNR) speech signals  [7] . Therefore, an automatic SNR-level detector is required to decide the suppression rule of SE  [9] . Recently, similar methods have been applied to SE on Automatic Speech Recognition (ASR)  [10, 11] ; however, the automatic SNR-level detector has not garnered sufficient attention in the integration of the SE model into SER.\n\nIn this study, we first investigate different strategies that apply SE to multi-task SER. Then, we propose a Noise Robust Speech Emotion Recognition (NRSER), which integrates the SE block, SNR-level detection block, and waveform reconstitution strategy. Our experimental results show that the proposed structure improves the system's performance on noisy speech signals without degrading its performance on signals with little or no background noise. Moreover, the proposed SNR-level detection structure can effectively distinguish between target noisy speech and signals that solely comprise background noise, which helps prevent the SER model from making predictions when the user is not speaking. Lastly, the SNR-level detection block can be used independently for other applications, such as data selection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Nrser",
      "text": "The NRSER system contains three NN blocks: SE, SNRlevel detection, and emotion recognition (abbreviated below as \"emotion block.\"). Figure  1  depicts our NRSER system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Se Block",
      "text": "In this study, we use the CMGAN  [12]  for SE. The CMGAN was pretrained using Voice Bank+DEMAND dataset and was fixed during the training of NRSER. Note that the CMGAN can be replaced with other SE models such as  [13, 14] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Snr-Level Detection Block",
      "text": "We propose a simply yet effective SNR-level detection structure that can work with various SE models without requiring any modifications to the SE models. The idea behind the design of the SNR-level detection structure is that, for the high SNR-level signals, the enhanced signal should closely resemble the original signal since there is little to no noise to be removed. Conversely, for low-SNR signals with prominent background noise, the enhanced signal will be more different from the original signal due to the noise. Building on this idea, the SNR-level block calculates the similarity between the original and enhanced input signals, and then a dense layer predicts the SNR-level scores.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Block",
      "text": "The self-supervised-learning (SSL)-based speech representation model aims to generate a general representation of the input signal. Previous studies have demonstrated the competitive generalizability and accessibility of using SSL models across various speech processing tasks, including SER  [15] . We fine-tune and extend a pretrained SSL model into a multitask SER system. Specifically, a SSL model, HuBERT  [16] , is finetuned by average-pooling the model's output embeddings and adding dense layers for arousal, valence, dominance, and emotion category recognition. In this study, we use hubert base due to the limitation of computing resources.\n\nThe hubert base can be substituted with larger SSL models, such as hubert-large, to achieve higher performance  [15] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Nrser Structure",
      "text": "First, an input waveform W in is fed to the SE block. Since the SE model we used is spectrogram-based, W in is converted to a spectrogram using the short-time Fourier transform (STFT).\n\nThe output of the SE model is an enhanced spectrogram, which is used for the SNR-level detection and waveform reconstitution. The SNR-level detection block calculates the cosine similarity between the original and enhanced spectro-grams in the time dimension. Then, a dense layer gives the SNR-level score S. The input waveform of the emotion block W re is reconstituted using the following equations:\n\nwhere W in is the original input waveform, W en is the corresponding enhanced waveform, and S ′ is the clamped SNRlevel score. In the ideal condition, the S ′ of a clean speech signal is 1, and the input of the emotion block W re is equal to the original input W in . W in is used for emotion recognition without preprocessing since it does not contain background noise. In contrast, S ′ of a noise signal is 0, and the input of emotion block W re is equal to the enhanced waveform W en . W in is discarded because it only contains noise information that might mislead the emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Nrser Training Phases",
      "text": "The training of the NRSER system includes three phases. First, we train the SNR-level detection block. The training data contains clean speech and background noise signals, where the objective of speech and noise signals is set to 1 and 0, respectively. Then, we train the emotion block with speech signals processed by the SE block. Finally, we simultaneously fine-tune the emotion block with the reconstituted waveforms and the SNR-level detection block with the same data as the previous phase.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "For emotion recognition, we adhere to the partitions defined in the MSP-PODCAST v1.10 dataset  [17]",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Training And System Hyperparameter",
      "text": "The sampling rate of all signals were set to 16kHz. The parameter setup of the STFT was a Hanning window with a window length of 400 and the hop length of 100. For the SE block, we used the open-sourced pretrained CMGAN  [12] . The weight of the SE block was fixed during the training and the inference steps. The SNR-level detection block was trained using a batch size of 32, an SGD optimizer with a learning rate of 0.0001 and a momentum of 0.9, and the mean square error (MSE) loss criterion. The emotion block was trained using a batch size of 8, an SGD optimizer with a learning rate of 0.0001 and a momentum of 0.9. The training loss of emotion block is a combination of the cross-entropy loss for the emotion category recognition and the concordance correlation coefficient (CCC) loss  [19]  for arousal, valence, and dominance recognition. The emotion category consists of ten primary emotions categories as defined in MSP-PODCAST  [17] . All models were trained using early stopping, with a patience of 2. The weights corresponding to the best validation scores during training were saved. The experimental results were the average of the same model and data retrained with different random seeds three times.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Performance On Speech Emotion Recognition",
      "text": "We compared NRSER performance with four representative configurations of the SER system (S-clean, S-noisy, S-en, and S-en'.) S-clean is the baseline system that only contains the emotion block and is trained with the MSP. S-noisy system is the S-clean system trained with additional MSP+AudioSet training data. S-en, S-en', and NRSER all incorporate SE into SER. S-en is S-clean using the SE as preprocessing in inference. Compared with S-en, the S-en' system uses the SE in both training and testing time. Table  1  summarized the configurations of SER systems. To evaluate the noise robustness of the systems, we test the systems on data with different noisy levels: MSP-PODCAST data (MSP), and MSP mixed with AudioSetval noise (MSP+AudioSet (SNR:12) and MSP+AudioSet (SNR:8).) The results are presented in Figure  2 . All systems performed worse with increasing noise levels (i.e., MSP > MSP+AudioSet (SNR:12) > MSP+AudioSet (SNR:8)). As the input signals became noisier, the performance of S-clean and S-en decreased significantly, while S-noisy, S-en', and NRSER exhibited greater noise robustness. S-clean generally performed better than S-en on MSP but worse than S-en on MSP+AudioSet, revealing that SE degrades SER performance on signals with no or little background noise. S-en' outperformed S-en in most cases since it was trained using enhanced signals. Because NRSER mitigated the effects of distortion caused by SE, it achieves better performance than S-en'. Overall, we observed that NRSER was less effective in recognizing arousal and dominance. One possible reason is that the SE process reduces the signal intensity while removing noise, potentially leading to the loss of information related to arousal and dominance. However, NRSER provided more specific information about the emotional experience, achieving the best performance in terms of F1 score for emotion category recognition and valence.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Analysis Of The Snr-Level Detection Block",
      "text": "To verify the effectiveness of the SNR-level detection block, we calculated the average SNR-level score S of signals at different SNR levels (Figure  3 ). The red diamonds indicate the average SNR-level score S of the training data, including the AudioSet-train and MSP-train. During training, MSPtrain was labeled as 1, whereas AudioSet-train was labeled as 0. The blue circles show the average S of unseen data for the tested SNR-level detection model. The numbers refer to the MSP (signal) to AudioSet (noise) ratio level for the MSP+AudioSet data. The results reflect that the SNRlevel detection block can recognize the interval levels even though it was trained using only the upper and lower bounds. Moreover, we tested the SNR-level detection block on the LibriSpeech test-clean set  [20] , in which the recordings have higher quality than the training data (MSP-train.) The result indicates that the SNR-level detection block can recognize that LibriSpeech signals have a higher SNR level than the MSP-train. Notably, this outcome was achieved even though all signals in MSP-train data were assigned the maximum ob- We also analyzed the SNR-level score in distinguishing noisy speech (MSP-test mixed with AudioSet-val data) and background noise signals (AudioSet-val). The experimental results show that setting threshold of 0.6 achieves more than 90% accuracy in our case. By using the SNR-level score as an indicator, NRSER can avoid attempting to provide emotion recognition results from signals with only background noise. Note that such attempts give users a clear sign of a system error and might diminish their trust in the system. In addition, the SNR-level detection block can be used in other speechprocessing tasks. For example, it can aid data selection by filtering out low-quality data from large crawled datasets.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Effectiveness Of The Waveform Reconstitution",
      "text": "We visualize the effectiveness of waveform reconstitution on signals with relatively higher clamped SNR-level score S ′ in Figure  4 , where W ori is an utterance in MSP-test, and W in is the utterance contaminated with additional noise (MSP+AudioSet). Because S ′ was calculated based on the similarity between the spectrogram with and without being processed by the SE block, an input signal receives a higher S ′ if the SE block makes less change to the input signal. There are two possible cases where an input signal received a high S ′ : (1) the input signal does not have background noise, and thus the SE block does not need to remove the noise; (2) the SE block could not remove the noise from the input signal. Figure  3  has already revealed case (1), that signals with higher SNR levels receive higher S ′ . Figure  4  demonstrates case  (2) , that W in has a high S ′ because the SE block fails to remove the noise. In both cases, the SER system did not benefit from the SE structure because the SE did not remove the noise but caused the distortion. However, since the SNRlevel score is high, the reconstituted waveform W re preserves the speech information in the W in and reduces the distortion caused by the SE structure.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We propose NRSER, a noise-robust SER system. Our experimental results confirm the effectiveness of integrating the SE block, SNR-level detection block, and waveform reconstitution strategy in improving the noise robustness of the SER system. The experimental results also demonstrate that training the SNR-level block with the upper and lower bounds of the objectives is sufficient for the model to learn the interval levels and recognize data that falls outside the boundary. Notably, the SE and emotion blocks can be substituted by alternatives that exhibit superior performance compared to the ones employed in this study, thereby achieving higher overall performance. Finally, we hope that our proposed noise-robust structure is not limited to emotion recognition but can also be applied to improve the noise robustness of other speechprocessing tasks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: depicts our NRSER system.",
      "page": 1
    },
    {
      "caption": "Figure 1: Proposed NRSER system, where d in dense layers refers to the output dimension.",
      "page": 2
    },
    {
      "caption": "Figure 2: All systems",
      "page": 3
    },
    {
      "caption": "Figure 3: ). The red diamonds indicate",
      "page": 3
    },
    {
      "caption": "Figure 2: SER performance.",
      "page": 4
    },
    {
      "caption": "Figure 3: Average SNR-level score S on signals with different",
      "page": 4
    },
    {
      "caption": "Figure 4: , where Wori is an utterance in MSP-test,",
      "page": 4
    },
    {
      "caption": "Figure 3: has already revealed case (1), that signals with",
      "page": 4
    },
    {
      "caption": "Figure 4: demonstrates",
      "page": 4
    },
    {
      "caption": "Figure 4: Visualization of the speech signals. The red circle",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "S-clean": "✓",
          "S-noisy": "✓",
          "S-en": "✓",
          "S-en’": "✓",
          "NRSER": "✓"
        },
        {
          "S-clean": "✗",
          "S-noisy": "✗",
          "S-en": "✗",
          "S-en’": "✗",
          "NRSER": "✓"
        },
        {
          "S-clean": "✗",
          "S-noisy": "✗",
          "S-en": "✗",
          "S-en’": "✓",
          "NRSER": "✓"
        },
        {
          "S-clean": "✗",
          "S-noisy": "✗",
          "S-en": "✓",
          "S-en’": "✓",
          "NRSER": "✓"
        },
        {
          "S-clean": "MSP",
          "S-noisy": "MSP,\nMSP+AudioSet",
          "S-en": "MPS",
          "S-en’": "MSP,\nMSP+AudioSet",
          "NRSER": "MSP,\nMSP+AudioSet\nAudioSet"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Head fusion: improving the accuracy and robustness of speech emotion recognition on the IEMOCAP and RAVDESS dataset",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Multiconditioning and data augmentation using generative noise model for speech emotion recognition in noisy conditions",
      "authors": [
        "Upasana Tiwari",
        "Meet Soni",
        "Rupayan Chakraborty",
        "Ashish Panda",
        "Sunil Kumar"
      ],
      "venue": "Proc. ICASSP 2020"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition under white noise",
      "authors": [
        "Chengwei Huang",
        "Chen Guoming",
        "Yu Hua",
        "Bao Yongqiang",
        "Zhao Li"
      ],
      "year": "2013",
      "venue": "Archives of Acoustics"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition in noisy environment",
      "authors": [
        "Farah Chenchah",
        "Zied Lachiri"
      ],
      "venue": "Proc. ATSIP 2016"
    },
    {
      "citation_id": "6",
      "title": "Spectral and cepstral audio noise reduction techniques in speech emotion recognition",
      "authors": [
        "Jouni Pohjalainen",
        "Fabien Fabien Ringeval",
        "Zixing Zhang",
        "Björn Schuller"
      ],
      "venue": "Proc. ACM-MM 2016"
    },
    {
      "citation_id": "7",
      "title": "Front-end feature compensation and denoising for noise robust speech emotion recognition",
      "authors": [
        "Rupayan Chakraborty",
        "Ashish Panda",
        "Meghna Pandharipande",
        "Sonal Joshi",
        "Sunil Kumar"
      ],
      "venue": "Proc. IN-TERSPEECH 2019"
    },
    {
      "citation_id": "8",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "Andreas Triantafyllopoulos",
        "Gil Keren",
        "Johannes Wagner",
        "Ingmar Steiner",
        "Björn Schuller"
      ],
      "venue": "Proc. INTER-SPEECH 2019"
    },
    {
      "citation_id": "9",
      "title": "Using speech enhancement preprocessing for speech emotion recognition in realistic noisy conditions",
      "authors": [
        "Hengshun Zhou",
        "Jun Du",
        "Yanhui Tu",
        "Chin-Hui Lee"
      ],
      "venue": "Proc. INTERSPEECH 2020"
    },
    {
      "citation_id": "10",
      "title": "Weighted speech distortion losses for neural-networkbased real-time speech enhancement",
      "authors": [
        "Yangyang Xia",
        "Sebastian Braun",
        "K Chandan",
        "Harishchandra Reddy",
        "Ross Dubey",
        "Ivan Cutler",
        "Tashev"
      ],
      "venue": "Proc. ICASSP 2020"
    },
    {
      "citation_id": "11",
      "title": "SNRi target training for joint speech enhancement and recognition",
      "authors": [
        "Yuma Koizumi",
        "Shigeki Karita",
        "Arun Narayanan",
        "Sankaran Panchapagesan",
        "Michiel Bacchiani"
      ],
      "venue": "Proc. INTERSPEECH 2022"
    },
    {
      "citation_id": "12",
      "title": "Learning to enhance or not: Neural network-based switching of enhanced and observed signals for overlapping speech recognition",
      "authors": [
        "Hiroshi Sato",
        "Tsubasa Ochiai",
        "Marc Delcroix",
        "Keisuke Kinoshita",
        "Naoyuki Kamo",
        "Takafumi Moriya"
      ],
      "venue": "Proc. ICASSP 2022"
    },
    {
      "citation_id": "13",
      "title": "CM-GAN: Conformer-based metric GAN for speech enhancement",
      "authors": [
        "Ruizhe Cao",
        "Sherif Abdulatif",
        "Bin Yang"
      ],
      "venue": "Proc. INTERSPEECH 2022"
    },
    {
      "citation_id": "14",
      "title": "DPCRN: Dual-path convolution recurrent network for single channel speech enhancement",
      "authors": [
        "Xiaohuai Le",
        "Hongsheng Chen",
        "Kai Chen",
        "Jing Lu"
      ],
      "venue": "Proc. INTER-SPEECH 2021"
    },
    {
      "citation_id": "15",
      "title": "Uformer: A unet based dilated complex & real dual-path conformer network for simultaneous speech enhancement and dereverberation",
      "authors": [
        "Yihui Fu",
        "Yun Liu",
        "Jingdong Li",
        "Dawei Luo",
        "Shubo Lv",
        "Yukai Jv",
        "Lei Xie"
      ],
      "venue": "Proc. ICASSP 2022"
    },
    {
      "citation_id": "16",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "Johannes Wagner",
        "Andreas Triantafyllopoulos",
        "Hagen Wierstorf",
        "Maximilian Schmitt",
        "Felix Burkhardt",
        "Florian Eyben",
        "Björn Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "17",
      "title": "HuBERT: self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "Reza Lotfian",
        "Carlos Busso"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "Jort F Gemmeke",
        "P Daniel",
        "Dylan Ellis",
        "Aren Freedman",
        "Wade Jansen",
        "R Channing Lawrence",
        "Manoj Moore",
        "Marvin Plakal",
        "Ritter"
      ],
      "venue": "Proc. ICASSP 2017"
    },
    {
      "citation_id": "20",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "Mao Li",
        "Bo Yang",
        "Joshua Levy",
        "Andreas Stolcke",
        "Viktor Rozgic",
        "Spyros Matsoukas",
        "Constantinos Papayiannis",
        "Daniel Bone",
        "Chao Wang"
      ],
      "venue": "Proc. ICASSP 2021"
    },
    {
      "citation_id": "21",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "venue": "Proc. ICASSP 2015"
    }
  ]
}