{
  "paper_id": "2411.09349v1",
  "title": "Paralbench: A Large-Scale Benchmark For Computational Paralinguistics Over Acoustic Foundation Models",
  "published": "2024-11-14T10:49:17Z",
  "authors": [
    "Zixing Zhang",
    "Weixiang Xu",
    "Zhongren Dong",
    "Kanglin Wang",
    "Yimeng Wu",
    "Jing Peng",
    "Runming Wang",
    "Dong-Yan Huang"
  ],
  "keywords": [
    "Computational paralinguistics",
    "acoustic foundation model",
    "non-verbal information extraction Emotion",
    "Sentiment",
    "... Medium-term: Sarcasm",
    "Stutter",
    "... Long-term: Gender",
    "Age",
    "Dialect"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Computational paralinguistics (ComParal) aims to develop algorithms and models to automatically detect, analyze, and interpret non-verbal information from speech communication, e. g., emotion, health state, age, and gender. Despite its rapid progress, it heavily depends on sophisticatedly designed models given specific paralinguistic tasks. Thus, the heterogeneity and diversity of ComParal models largely prevent the realistic implementation of ComParal models. Recently, with the advent of acoustic foundation models because of self-supervised learning, developing more generic models that can efficiently perceive a plethora of paralinguistic information has become an active topic in speech processing. However, it lacks a unified evaluation framework for a fair and consistent performance comparison. To bridge this gap, we conduct a large-scale benchmark, namely ParaLBench, which concentrates on standardizing the evaluation process of diverse paralinguistic tasks, including critical aspects of affective computing such as emotion recognition and emotion dimensions prediction, over different acoustic foundation models. This benchmark contains ten datasets with thirteen distinct paralinguistic tasks, covering short-, medium-and long-term characteristics. Each task is carried out on 14 acoustic foundation models under a unified evaluation framework, which allows for an unbiased methodological comparison and offers a grounded reference for the ComParal community. Based on the insights gained from ParaLBench, we also point out potential research directions, i. e., the cross-corpus generalizability, to propel Com-Paral research in the future. The code associated with this study will be available to foster the transparency and replicability of this work for succeeding researchers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Works",
      "text": "This section presents an overview of relevant tasks and applications in ComParal, summarizes its major advancements, and discusses commonly utilized features. Additionally, it highlights the role of benchmarking in improving evaluation methodologies in this field.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Foundations Of Computational Paralinguistics",
      "text": "Paralinguistic analysis, a component of the broader discipline known as \"alongside linguistics\"  [1] ,  [23] ,  [24] , investigates non-verbal elements that complement spoken language, aiming to elucidate their role in enhancing communication and information transmission. These elements encompass intonation  [25] ,  [26] , speech rate  [27] ,  [28] , and volume  [28] ,  [29] , which convey subtleties and encapsulate speaker attributes such as gender  [30] -  [32] , age  [33] -  [35] , emotion  [19] ,  [36] ,  [37] , and health status  [38] ,  [39] . Paralinguistic analysis thus enriches our understanding of human discourse, with diverse applications such as emotion recognition  [19] ,  [36] ,  [37] , detecting deceptive communication  [40] , speaker identification  [41] ,  [42] , enhancing human-computer interaction  [43] ,  [44] , and aiding healthcare diagnostics  [38] ,  [39] .\n\nHowever, synthesizing this multifaceted data into coherent frameworks for comprehensive interpretation remains a challenge. The development of ParaLBench is motivated by the need to address these complexities and the desire to create a unified platform that leverages the diverse aspects of paralinguistics for both scholarly investigation and practical application.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Machine Learning In Paralinguistic Analysis",
      "text": "Speech signals are rich in information pertaining to the speaker's identity, emotional state, and language habits, which can be discerned through analysis of intonation, speech rate, and timbre. The evolution of machine learning has propelled paralinguistic analysis from rudimentary rule-based and statistical methods to sophisticated deep learning techniques  [3] ,  [19] .\n\nInitially, paralinguistic analysis depended on symbolic techniques and statistical models, guided by expert-designed rules, to interpret diverse non-linguistic features  [45] -  [51] . Nonetheless, these methods were constrained in their ability to manage the intricate nature of speech data. Subsequent computational advancements, particularly through deep learning techniques such as convolutional neural networks (CNNs)  [52] -  [54]  and recurrent neural networks (RNNs)  [55] -  [57] , have exhibited remarkable proficiency in discerning significant patterns from speech signals.\n\nThe introduction of Transformers  [58] -  [60] , featuring their self-attention mechanisms, marked a significant milestone, as they enhanced speech analysis by effectively handling long-range temporal dependencies. Innovations like Speech-Former++  [19]  and HAFFormer  [3]  exemplify this progress, tackling computational challenges and refining the analysis of paralinguistic tasks.\n\nNotwithstanding these advancements, the field continues to grapple with challenges related to generalization, data availability, and model complexity. ParaLBench has emerged as an essential tool to confront these issues, providing a systematic platform for assessing model capabilities and performance in paralinguistic tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Acoustic Features For Paralinguistic Analysis",
      "text": "Acoustic features are crucial in paralinguistic analysis, disclosing information about emotion, gender, and age. Historically, research predominantly relied on handcrafted features such as MFCC  [61] ,  [62] . Innovations have broadened the feature repertoire, incorporating more advanced acoustic profiles like extended Geneva Acoustic Parameter Set (eGeMAPS)  [63]  and task-specific sets such as ComParE-2016  [64] ,  [65] .\n\nDespite the benefits, traditional handcraft features were not without their limitations, particularly in terms of adaptability to the complex and varied nature of speech signals. The advent of SSL models like BERT  [59]  and VIT  [66] , however, heralded a new era in acoustic feature extraction. Contemporary models such as wav2vec  [15]  and its successor, wav2vec 2.0  [67] , employ advanced techniques to auto-encode audio input, enhancing the precision and efficiency of paralinguistic analysis. This trend extends to even more recent models like HuBERT  [16] , which leverage offline clustering to improve alignment and predictive accuracy of speech representations.\n\nThe progression from handcrafted to self-supervised learning-based features marks not just methodological innovation but also a practical stride toward robust features capable of generalizing across linguistically varied datasets with reduced manual effort. Accordingly, ParaLBench will conduct a thorough evaluation of these models to guide future research.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Benchmark In Speech And Paralinguistic Analysis",
      "text": "Benchmarks play a pivotal role in standardizing evaluation by harmonizing datasets, methods, and models, thereby propelling research and development. Within the realm of speech processing, numerous benchmarks  [20] ,  [68] -  [73]  have been instituted to gauge the universality and adaptability of speech representation methods as technology evolves. For instance, the Speech processing Universal PERformance Benchmark (SUPERB)  [68]  offers a structured framework for systematically assessing the performance of speech representations across an array of tasks, encompassing content understanding, speaker identification, semantic analysis, and paralinguistic feature recognition. Although SUPERB has set a high standard, its predominant reliance on English language corpora limits its global applicability, leading to the emergence of benchmarks like IndicSUPERB  [69] , which focuses on Indian languages, and ML-SUPERB  [70] , extending to a broader linguistic spectrum that includes up to 143 languages, significantly enhancing cross-lingual evaluation capabilities and facilitating the improvement of model performance and generalizability in varied linguistic contexts. Additionally, a recent study  [20]  has expanded SUPERB by conducting extensive experiments to evaluate the performance of various SSL models on speech processing tasks, affirming the generalizability and efficacy of SSL models across diverse tasks.\n\nAdditionally, beyond the realm of general speech processing tasks, specialized benchmarks have emerged, specifically designed for computational paralinguistic tasks. For example, SERAB  [71]  scrutinizes utterance-level Speech Emotion Recognition (SER), with a particular emphasis on the application and generalization potential of diverse feature sets. In contrast to SERAB, which concentrates on features derived from deep neural networks, studies on SSL models, notably the paralinguistic model TRILLsson  [74] , have demonstrated their superior efficacy in SER tasks, highlighting TRILLsson's exceptional performance across multiple languages. Furthermore, MERBench  [72]  introduces a novel unified benchmark for multimodal emotion recognition, crafted to assist researchers by offering clear guidelines for both methodological advancement and thorough interaction with multimodal emotional data.\n\nThese benchmarks have propelled the field forward. Nevertheless, many have been confined to narrow research domains, such as single-language assessments or specific facets of paralinguistics like emotion recognition, resulting in a lack of comprehensive evaluation platforms that can cover the full range of ComParal tasks. Acknowledging these gaps in evaluation and comparison, ParaLBench has been developed not just as another benchmark, but as a holistic analytical toolkit crafted to evaluate a broad array of paralinguistic activities. ParaLBench's scope encompasses a variety of tasks, including speaker style recognition and accent detection, which are poised to significantly advance research and usher in a new era of progress in ComParal.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Paralbench Framework",
      "text": "In this section, we first describe the unified evaluation model structure, followed by the introduction of two handcrafted acoustic features and 14 widely used acoustic foundation models. Then, we present the paralinguistic tasks assessed in our study, and finally elucidate the criteria for evaluation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Model Structure",
      "text": "The unified evaluation framework is illustrated in Figure  2 , which comprises a pre-trained acoustic foundation model, a projection layer, a standard Transformer, and a taskspecific classifier. Specifically, the acoustic foundation model is generally pre-trained via certain SSL algorithms, such as Acoustic Foundation Model (wav2vec 2.0, HuBERT, WavLM, etc.)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A Standard Transformer",
      "text": "Task-Specific Classifier Projection Layer ( to 768 dim) Compared to previous methods  [20] ,  [68] , we introduced a standard Transformer to enhance the ability to capture semantic information and contextual understanding. The selfattention mechanism of the Transformer effectively focuses on important semantic information within the input sequence, which improves the richness of feature representation. Furthermore, this structure is flexible and efficient, simplifying the model architecture while enhancing its adaptability to downstream tasks.\n\nMathematically, this process can be described by the following formula:\n\nwhere x represents the input speech features given a speech sample, f (•) denotes the acoustic foundation model , h ∈ R L×h represents the extracted features, where L denotes the feature sequence length and h represents the dimension. W m ∈ R h×d is a learnable matrix that maps the feature to the Transformer input and output dimension d. h ∈ R L×d denotes the output of the Transformer. p ∈ R C represents the probability distribution over the output classes by the classifier. For handcraft features, the f (•) denotes handcrafted Feature Extractor, and h ∈ R h is obtained through f (•). Then, we directly feed h into the task-specific classifier, which consists of two dense layers, to obtain the final prediction:\n\nDuring training, we employ an end-to-end training approach.\n\nFor classification tasks, we utilize the cross-entropy loss function L CE as the objective function, while for regression tasks, we employ the mean absolute error (MAE) loss L M AE . L CE and L M AE are specifically defined as:\n\nwhere N is the number of samples, C represents the number of classes, and y i represents the normalized true label of sample i.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Acoustic Foundation Models",
      "text": "Nowadays, due to the development of SSL, various acoustic foundation models have emerged in the speech-processing society, with different neural network architectures, training objectives, and training data. Most of these models were extensively evaluated in ASR in previous years. Although these models are gradually and increasingly exploited in ComParal, it lacks a holistic and unified performance investigation of these foundation models in the application to ComParal. In the following, we will introduce two handcrafted feature sets and eight frequently used acoustic foundation models. As some of these models contain base, medium, and large versions, we investigated 14 acoustic foundation models in total. The details of these models and feature sets can be found in Table  I .\n\n1) Handcraft features: Early ComParal research typically involved designing handcrafted features to accommodate various tasks. Given the widespread utility of eGeMAPS (Extended GeMAPS)  [96]  and ComParE-2016 (Computational Paralinguistics Challenge 2016)  [3] ,  [10] ,  [65]  in the field of speech processing, we contrasted them with language-based models in this study. eGeMAPS captures emotion-relevant acoustic properties, while ComParE-2016 offers a broader suite for tasks like emotion and gender recognition. These features, accessible via the openSMILE toolkit, provide a rich tapestry of speech-based information.\n\n2) wav2vec: wav2vec  [15]  is an acoustic foundation model entirely composed of convolutional neural networks, which takes raw audio as input and is trained on a large amount of unlabeled audio data. Its training objective is to use a contrastive loss function to distinguish between real future audio samples and negative samples, thus obtaining a universal representation suitable for input into speech recognition systems. This foundational model is readily available on the open-source community Fairseq.   [77] , MELD  [78] , MEAD  [79] , Fairseq CMU-MOSEI  [80] , MSP-Podcast  [21]  wav2vec2-base  [  AudioCaps  [92] , Clotho  [93] , Huggingface LAION-Audio-630K  [91] , Audioset  [94]  data2vec-audio-base  [95]  SSL Transformer 93.16M LS-960  [75]  Huggingface\n\n3) wav2vec 2.0: wav2vec 2.0  [67]  represents an improved version of wav2vec. Its main improvements include: 1. Replacing the convolutional neural network with a Transformer, enhancing the model's ability to represent context. 2. Discretizing the output of the feature encoder z into a finite set of speech representations through quantization. 3. It masks the latent representation of the original waveform and solves contrastive learning tasks through quantized speech representations. Currently, wav2vec 2.0 has several derivative versions, such as wav2vec2-large-age-gender  [81]  fine-tuned on gender and age datasets, and wav2vec2-large-xlsr-53  [86]  fine-tuned on multilingual datasets. These foundational models are all available for download on the open-source community Huggingface.\n\n4) emotion2vec: emotion2vec  [76]  serves as a universal speech emotion representation model. This model is pre-trained on multiple open-source unlabeled emotion datasets through self-supervised online distillation, combining utterance-level loss and frame-level loss during pre-training. As a result, emotion2vec demonstrates consistent improvements across multiple speech emotion recognition datasets. This foundational model is conveniently accessible on the open-source community Fairseq. 5) HuBERT: HuBERT  [16]  utilizes an offline clustering step to generate pseudo-labels for pre-training, following a methodology akin to BERT. Specifically, the HuBERT model employs masked continuous speech features to predict predetermined cluster labels. This predictive loss is exclusively applied within the masked regions, compelling the model to acquire robust high-level representations and enabling accurate inference of the masked targets. Inspired by the DeepCluster  [97]  method in self-supervised visual learning, HuBERT leverages the masked prediction loss over speech sequences to encapsulate their sequential structure. Presently, this foundational model is available for download on the open-source community Huggingface.\n\n6) WavLM: WavLM  [17]  introduces a masking-based framework for speech denoising and prediction. Within this framework, certain inputs simulate noisy or overlapping speech with masks, aiming to predict pseudo-labels for the original speech within the masked regions, akin to HuBERT. The framework integrates masked speech prediction and denoising during pre-training. Consequently, the WavLM model not only learns ASR information through masked speech prediction but also acquires knowledge of non-ASR tasks via speech denoising modeling. Moreover, WavLM optimizes the model structures and training data of HuBERT and Wav2vec 2.0. Specifically, it incorporates gated relative position biases (grep) into the Transformer structure as the backbone, enhancing the model's ASR performance while maintaining nearly identical parameter count and training speed. Furthermore, to bolster model robustness and mitigate data mismatch issues, the unlabeled pre-training data is expanded to encompass 94k hours of public audio.\n\n7) CLAP: CLAP  [91] , similar to CLIP  [98] , employs the contrastive learning paradigm to train models on large-scale noisy data collected from the internet. The training process of CLAP involves encoding raw speech and text into highlevel representations using separate speech and text encoders. Genuine speech-text pairs are treated as positive samples, while the rest are considered negative, and contrastive loss is computed accordingly. CLAP uses the Transformer as the core architecture for the speech encoder and introduces a feature fusion scheme to handle variable-length audio inputs. In this study, we primarily extract speech features through CLAP's audio encoder.\n\n8) Whisper: Whisper  [18]  represents a significant advancement in audio modeling, released by OpenAI in 2022. It encompasses functionalities such as multilingual ASR, speech translation, and language identification. The Whisper model has showcased exceptional recognition performance, even with large amounts of weakly labeled data, without the necessity for complex models and tuning methods, particularly excelling in robustness and generalization. Diverging from pretrained speech models trained using unsupervised methods like Wav2vec 2.0, Whisper employs weakly supervised training, enabling direct multitask learning without the need for taskspecific fine-tuning. 9) Data2vec: Self-supervised learning has made progress in computer vision, natural language processing, and speech processing, but algorithms designed for a single modality obscure cross-modal learning mechanisms. Therefore, Data2vec  [95]  proposes enhancing the computational efficiency of self-supervised learning through contextualized target prediction, efficient data encoding, and fast convolutional decoders. It employs a unified learning objective but trains separate models for different modalities using Transformer architecture and different feature encoders. It utilizes a teacher model to create latent contextualized representations, which are regressed by a student model using masked examples. Contextualized targets capture sample information, enriching learning tasks and speeding up learning. Experimental results show efficiency improvements of 2 to 16 times with similar accuracy in tasks like image classification, speech recognition, and natural language understanding.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. The Paralbench Tasks",
      "text": "We categorize the tasks in ParaLBench using the time axis taxonomy method of computational psycholinguistics as outlined in  [1] ,  [23] . Based on the period of the impact of the paralinguistic information on the voice articulation, we group the paralinguistic information into long-, median-, and short-term groups. For example, the age and gender often alter the speech characteristics through years; while emotion often can transform the articulation behavior instantaneously, from few seconds to minutes. Traits requiring extended periods for determination are classified as \"long-term\", encompassing biological trait primitives, cultural traits, personality traits, and aspects of the speaker's personal style. States ascertainable in the short term are designated as \"short-term\", encompassing emotions and associated states such as stress, confidence, and uncertainty. \"Medium-term\" traits and states typically involve self-induced characteristics such as sleepiness, intoxication (e. g., alcohol poisoning), and health states, or may entail discrepant speech patterns such as sarcasm and lying. For detailed task taxonomies and corresponding datasets, please refer to Table  II .\n\n1) Emotion: SER  [7] ,  [19]  refers to automatically inferring the speaker's emotional state by analyzing both the acoustic features and linguistic content of the speech signal. This task holds diverse applications in affective computing, humancomputer interaction, customer service, and various other domains. For this task, we will utilize the MELD, IEMOCAP, and MSP-Podcast datasets.\n\n2) Emotion Dimensions: Arousal, Valence, and Dominance are key dimensions for describing emotions. Arousal measures the intensity or energy level of an emotion, ranging from calm to excited. Valence indicates the positive or negative nature of the emotion, from pleasant to unpleasant. Dominance reflects the sense of control or power one feels in a situation, varying from feeling submissive to dominant. Together, these dimensions provide a comprehensive framework for understanding and analyzing emotional states. For these tasks, we will utilize the IEMOCAP dataset.\n\n3) Sentiment: Sentiment analysis  [37]  involves automatically identifying and categorizing emotional tendencies in speech using machine learning techniques. Its primary objective is to determine the emotional polarity of speech as positive or negative. This task has applications in social media monitoring and product review analysis. We will utilize the MELD and CMU-MOSI datasets for this task.\n\n4) Sarcasm: The Sarcasm Detection Task  [99]  focuses on identifying sarcasm in audio, a form of expression conveying the speaker's opposite intention, critical in social media and sentiment analysis.\n\n5) Influenza: Influenza is a common respiratory disease caused by influenza viruses that lead to large outbreaks of infection and disease worldwide every year. The purpose of Influenza analysis  [100]  is to analyze and predict influenzarelated information through speech processing and machine learning technology.\n\n6) Stutter: Stuttering is a speech disorder marked by disruptions during speech, often involving repetition of syllables, words, and elongated sounds. The stutter analysis task aims to understand the features and patterns of stuttering  [101] . This analysis can reveal underlying mechanisms and influencing factors, ultimately providing more effective support for individuals who stutter.\n\n7) Depression: Depression detection  [102] ,  [103]  involves identifying depressive symptoms by analyzing an individual's emotional, behavioral, and cognitive data. The aim is to leverage machine learning to accurately detect signs of depression, enabling timely intervention and personalized treatment. For this task, we will utilize the DAIC-WOZ dataset.\n\n8) Gender: The Gender task  [32] ,  [35]  aims to identify and analyze the gender characteristics of speakers in speech. This task has wide applications in fields such as speech recognition, gender identification technology, and social media analysis, enabling us to better understand and utilize gender information in speech data. For this task, we will utilize the MSP-Podcast dataset.\n\n9) Age: The Age task  [35]  aims to analyze and identify the age characteristics of speakers through speech signals. This task has broad applications in fields such as speech recognition, age identification technology, and targeted advertising, enabling us to better understand and utilize age information in speech data. For this task, we will utilize the VCTK dataset.\n\n10) Accent: Accent refers to the unique pronunciation and speech rhythm of speakers, influenced by their native language, region, and culture. The Accent task  [104]  aims to identify and analyze these features using artificial intelligence. This task has applications in speech recognition, linguistics research, and cross-cultural communication. For this task, we will utilize the VCTK dataset.\n\n11) Dialect: Dialect refers to language variants used in specific regions or communities, encompassing differences in vocabulary, grammar, pronunciation, and other linguistic aspects. The Dialect analysis task  [105]  holds significant importance in fields such as speech recognition, cultural preservation, and sociolinguistic research. For this task, we will utilize the TIMIT dataset.\n\nBy leveraging these categorized tasks and datasets, ParaL-Bench strives to enhance ComParal research, facilitating a comprehensive understanding of human speech patterns and behaviors.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. The Unified Evaluation Metrics",
      "text": "Considering the diverse evaluation metrics used in prior approaches  [19] ,  [106] ,  [107] , we take unified evaluation metrics in ParaLBench to facilitate the comparison of various acoustic foundation models across different computational paralinguistic tasks. Simultaneously, we encourage future researchers to adhere to the same settings and evaluation criteria to ensure fair comparisons of their algorithms.\n\nFor classification tasks, we employ Weighted Accuracy (WA), Unweighted Accuracy (UA), and Weighted F1 Score (WF1) as evaluation metrics. The aforementioned metrics can be described as follows:\n\nwhere N c represents the number of samples in class c, Acc(c) and F 1(c) respectively denote the classification accuracy and F1 score of class S c . The Mean Absolute Error (MAE) is used as the evaluation metric for regression tasks and the formula can be described as follows:\n\nwhere N represents the number of samples, y i denotes the true labels, and ŷi represents the predicted values. Higher values of WA, UA, and WF1 indicate better performance, whereas lower values of MAE denote superior performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experimental Setup",
      "text": "In this section, we introduce the datasets used in ParaL-Bench, as shown by Table  III , as well as the method of dividing the datasets. Then we describe the implementation details.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Paralbench Corpus",
      "text": "In this study, the specific datasets chosen for evaluation tasks were primarily based on their representativeness, applicability, and ease of access. This ensures that the benchmarks established are not only relevant but also practical for future research in the field.\n\n1) MELD: The Multimodal EmotionLines Dataset  [78] [71]  is an extension of the EmotionLines dataset. It comprises 13,708 dialogue segments. The dataset is annotated with seven emotional states: Anger, Disgust, Sadness, Joy, Neutral, Surprise, and Fear. Additionally, MELD provides sentiment polarity annotations (positive, negative, and neutral) for each utterance. Similarly, we conduct experiments following the official dataset partitioning and report results on the test set.\n\n2) MSP-Podcast: MSP-Podcast  [21] , one of the largest speech-emotion corpora, contains speech segments from podcast recordings, annotated via crowdsourcing and regularly updated. For this study, we employ version v1.10 of the corpus, comprising 104,267 speech segments. The dataset is officially partitioned into Train, Development, Test1, and Test2 sets. Test results on Test1 will be disclosed following previously established methodologies  [112] . Annotations within the dataset encompass categorical labels (anger, happiness, sadness, disgust, surprised, fear, contempt, neutral, and others).\n\n3) CMU-MOSI: The Multimodal Corpus of Sentiment Intensity (CMU-MOSI)  [80]  dataset consists of 2,199 opinion video clips, each annotated with sentiment labels (Positive, Neutral, Negative). We follow the official dataset partitioning to train models using the training set, tune hyperparameters using the validation set, and ultimately select the best-performing model on the validation set for evaluation on the test set, reporting the results thereafter.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "4) Iemocap:",
      "text": "The Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [77]  dataset is a multimodal resource for emotion recognition. It consists of a series of session dialogue scenarios capturing interactions among multiple participants in various emotional states. Additionally, the dataset provides information on emotional dimensions e.g. Arousal, Valence, and Dominance. In this paper, we use the first four sessions as the training set and the last session as the test set to guarantee speaker independence. In emotion prediction, we utilize four types of data: angry, neutral, happy (& excited), and sad.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "5) Mustard:",
      "text": "The MUStARD  [108]  dataset is a multimodal video corpus designed for automatic sarcasm detection research, sourced from popular TV shows like \"Friends\", \"Golden Girls\", \"The Big Bang Theory\", and \"Sarcasmaholics Anonymous\". It comprises 690 audiovisual utterances labeled with sarcasm. We utilize the speaker-independent evaluation approach proposed in  [108] , using data from \"Golden Girls\", \"The Big Bang Theory\", and \"Sarcasmaholics Anonymous\" for training and data from \"Friends\" for testing.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "6) Flusense:",
      "text": "The FluSense  [109]  dataset is based on the Google Audioset  [94] , which contains 10-second audio clips from YouTube covering coughs, sneezes, speech, and various human activities and background noises. Following the method in  [100] , we divided the dataset into training and testing sets in a ratio of 0.8/0.2 and excluded categories with fewer than 100 samples. Therefore, we used audio samples from nine categories: other, breathe, cough, gasp, silence, sneeze, sniffle, speech, throat-clearing.\n\n7) SEP-28k: SEP-28k  [107]  is a dataset comprising over 28,000 clips labeled. The audio is sourced from public podcasts predominantly featuring individuals who stutter interviewing others who stutter. In this dataset, we conduct a binary classification task, distinguishing between stuttering and nonstuttering instances. The dataset is partitioned according to the approach outlined in  [107] , with 25k samples allocated for training, 2k for validation, and 1k for testing.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "8) Daic-Woz:",
      "text": "The DAIC-WOZ (Depression and Anxiety Interview Corpus -Wizard of Oz) dataset is a resource for studying depression and anxiety. It includes dialogues with virtual agents, capturing video, audio, and text data to simulate mental health assessment scenarios. The dataset assists researchers in evaluating emotional states and mental health, particularly for automated detection of depression and anxiety. 9) CSTR VCTK: CSTR VCTK  [111]  corpus contains speech data from 110 English speakers with different accents. The dataset comprises a total of 44,070 utterances, with each speaker reading approximately 400 sentences selected from inspired passages of newspapers, Rainbow passages, and speech accent archives. Since CSTR VCTK does not provide an official dataset partitioning, we employ previous methods  [113] ,  [114] , randomly split 90% of data for training and the remaining 10% for testing.\n\n10) TIMIT: The TIMIT  [84]  Acoustic-Phonetic Continuous Speech Corpus is a standard dataset used to evaluate the performance of automatic speech recognition systems. It comprises recordings of 10 phonetically rich sentences read by 630 speakers representing 8 different American English dialects. In our study, we conduct dialect classification experiments, following the dataset's official partitioning, training our models on the training set, and reporting results on the test set.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "B. Implementation Details",
      "text": "To fairly compare the performance of different acoustic foundation models, we strive to ensure that the models are trained under identical settings as much as possible. During training, we set the working dimension d of the Transformer in the baseline model to 768 and employed the AdamW optimizer to optimize model parameters. For deep features, we set the initial learning rate to 5×10 -4 , while for handcrafted features, we observed that a larger learning rate could lead to significant fluctuations in performance. Hence, we opt for 5 × 10 -5 as the initial learning rate. We utilize a polynomial decay strategy to dynamically adjust the learning rate throughout the entire training process. We set the maximum number of epochs to 60 and simultaneously set the weight decay to 10 -2 . To prevent overfitting, we introduced 0.1 Dropout rate in the standard Transformer layers and set it to 0.5 Dropout rate in the classification layer.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Results And Discussion",
      "text": "In this section, we present the results from large-scale experiments with thirteen paralinguistic tasks with 14 acoustic foundation models and two manually engineered feature sets, and provide detailed result analysis and discussion.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Full Benchmark Results",
      "text": "1) Handcrafted Features vs SSL Acoustic Foundation Models: Since eGeMAPS and ComParE-2016 are widely used as baselines for evaluating various tasks in computational paralinguistics, we employ them in ParaLBench for baseline comparisons. According to the results in Tables IV, VI, and VII, we find that the performance of acoustic foundation models surpasses that of handcrafted features in most tasks. Nonetheless, exceptions are noted. For example, in emotion and sentiment tasks, eGeMAPS performs comparably to, and sometimes even exceeds, some acoustic foundation models. This likely stems from eGeMAPS features being specifically designed for speech emotion recognition. Similarly, ComParE-2016 also achieves performance comparable to acoustic foundation models in specific tasks like gender prediction and sarcasm detection. These findings suggest that specially designed  handcrafted features can still compete with acoustic foundation models in certain scenarios.\n\nOverall, with the increase in training data for speech foundation models and the optimization of their structures, these models typically outperform handcrafted features. However, this superiority does not imply that handcrafted features lack advantages. For instance, handcrafted features demand fewer computational resources compared to acoustic foundation models, a crucial consideration in resource-constrained environments.\n\n2) Impact of Training Data Volume on Foundation Models: In deep learning, the axiom that \"there is no data like more data\" often holds true, a notion supported by the results achieved by various acoustic foundation models. Among models with similar capacities, such as wav2vec2-large, WavLMlarge, and HuBERT-large, differences in their training data volumes are illuminating. As shown in Table  III , these models are trained with varying data sizes. Disregarding other variables such as architectural specificities and training methodologies, it is observed that WavLM-large, benefiting from the most extensive training dataset, consistently outshines its peers in most tasks. Conversely, wav2vec2-large suffers in some areas when trained with only 960 hours of Librispeech data-less voluminous than its counterparts-underscoring the significance of data volume in bolstering the generalization capacity of speech foundation models for computational paralinguistic applications. Interestingly, we note instances where additional diversity in training data, such as including multiple languages as in the wav2vec2-large-xlsr-53 model, might detrimentally affect performance on tasks strictly in the English language. This highlights the nuanced interplay between data variety and task specificity.\n\nFurthermore, we observe that further pre-training on taskspecific datasets can improve acoustic foundation model performance for those tasks. For example, wav2vec2-large-agegender, compared to wav2vec2-large, undergoes additional pre-training on datasets related to age and gender, resulting in superior performance in gender and age prediction tasks. An-other example is emotion2vec, a foundation model specifically developed for speech emotion recognition. Its training data is primarily emotion-related, and despite having a parameter size comparable to other base models, emotion2vec outperforms most large models in emotion and sentiment tasks.\n\n3) Impact of Model Structure on Performance: The impact of model structure on performance is a critical consideration in the development of more general-purpose acoustic foundation models. Researchers have opted for various model architectures, showcasing exceptional performance in several general tasks, including automatic speech recognition  [16] ,  [18] , and speech translation  [18] . However, evidence supporting their effectiveness in a broader range of ComParal tasks is lacking.\n\nTable  I  provides detailed insights into the architectural foundations of various acoustic foundation models. With the exception of wav2vec-large, which uses a CNN-based architecture, the majority of models predominantly utilize Transformerbased structures. A more in-depth comparative analysis between wav2vec-large and wav2vec2-large models, sharing the same training methodology and data, reveals that the Transformer-incorporating wav2vec2-large model outperforms its CNN counterpart across most paralinguistic challenges. This observation suggests that the Transformer architecture excels in capturing paralinguistic nuances, possibly due to its proficiency in processing long-range sequential dependencies within speech data. Furthermore, models designated as 'large' consistently outperform their 'base' counterparts, indicating that architectural enhancements, whether in dimensionality or layer count, can significantly enhance performance. CLAP aligns text and speech representations through contrastive learning, while Whisper employs an autoregressive learning method to learn latent representations and undergoes supervised training on multiple tasks.\n\nComparing the results in all the tables mentioned above, we observe that HuBERT and WavLM, which employ clusteringbased methods (e.g., spectral clustering, hierarchical clustering), demonstrate the best generalization performance across multiple paralinguistic tasks. WavLM's exceptional performance in generalization can be partly attributed to its exposure to noise during the training phase. Similarly, Whisper, trained using an autoregressive method and supervised on multiple tasks, also exhibits strong generalization capabilities, achieving the best performance in tasks such as Sarcasm and Influenza detection. CLAP, which uses contrastive learning, shows stable results across various tasks. In contrast, data2vec, emotion2vec, and wav2vec 2.0 performed less competitively across a wider range of tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Acoustic Foundation Model On Different Terms",
      "text": "As shown in Table  IV , VI, and VII, we present the results of various tasks along the time axis, from short-term states to long-term traits. These findings delineate the inclinations of the acoustic foundation models across tasks situated at distinct junctures on the timeline, offering guidance for subsequent researchers in the selection of speech foundation models.\n\n1) Short-Term Task Performance Analysis: In the analysis of short-term task performance, we focus on states that manifest over a brief duration. Tables IV reflect the model performance in two such tasks: Emotion and Sentiment. While acoustic foundation models handle extensive datasets potentially containing emotion-specific information, their efficacy varies considerably. Notably, emotion2vec and WavLM-large demonstrate exceptional performance in shortterm tasks. Emotion2vec benefits from tailored datasets and architecture, enhancing its proficiency in emotion-centric paralinguistic learning, whereas WavLM-large, leveraging its extensive dataset and noise-augmented training regimen, excels in capturing transient states. Interestingly, wav2vec2-largeage-gender, through its gender-focused further training, exhibits notable enhancements in the Emotion (MSP-Podcast) and Sentiment (MELD) tasks, indicating a correlation between learning long-term attributes like gender and the recognition of short-term emotional states.\n\n2) Medium-Term Task Performance Analysis: In the analysis of medium-term task performance, we consider states that typically manifest between long-term and short-term durations, encompassing partially self-induced or temporary states, as well as potential anomalous speech. Within Table VII, we investigate three medium-term tasks: Sarcasm, Influenza, and Stutter. The Whisper-large model excels in the Sarcasm and Influenza tasks, likely due to its Encoder-Decoder Transformer design and skills honed through autoregressive training, features that align well with the requirements of medium-term tasks. Similarly, the WavLM-large model demonstrates its effectiveness in Stutter detection.\n\n3) Long-Term Task Performance Analysis: In our analysis of long-term task performance, we designate traits that humans develop over extended periods as long-term traits. Table VII primarily focuses on four tasks: Gender, Age, Accent, and Dialect. Interestingly, we observed that in these long-term tasks, some base models can even match or surpass large foundation models. Contrary to expectations, the best-performing model in the Age task is WavLM-base, not WavLM-large. Similarly, HuuBERT-base outperforms HuBERT-large in the Accent and Dialect tasks. Additionally, wav2vec2-base performs comparably to wav2vec2-large in the Age and Dialect tasks. This suggests that relatively smaller base models may be more suitable for long-term tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Cross-Corpus Analysis",
      "text": "This section aims to explore the generalization performance of acoustic foundation models in cross-corpus. Given the similarity of emotion labels, we selected the IEMOCAP, MELD, and MSP-Podcast datasets for cross-corpus experiments. However, these datasets differ in label categories, such as IEMOCAP (happiness, sadness, anger & excited, neutral), MSP-Podcast (anger, neutral, happiness, sadness, disgust), and MELD (anger, neutral, joy, sadness, disgust, surprise, fear). To address this issue, we selectively filter out certain labels from the test datasets. For instance, when testing the crosscorpus performance of a model trained on IEMOCAP using the MSP-Podcast dataset, we exclude instances labeled as disgust. Similarly, we remove instances labeled as disgust, surprise, and fear from the MELD dataset.\n\nFigure  3  presents the results of the speech foundation models in cross-corpus experiments. Due to space limitations, we selected ten different speech foundation models to showcase their cross-corpus performance. As shown in Figure  3 , emotion2vec-base, WavLM-base, and WavLM-large exhibit the best performance across multiple cross-corpus experiments, while wav2vec2-base and wav2vec2-large perform poorly in several experiments. By analyzing the performance of these models in Tables  IV  and V , we observe that models with strong performance on within-corpus tend to perform better in cross-corpus tasks, while those with weaker withincorpus performance tend to struggle. Another notable observation is that base models generally underperform compared to large models with more parameters in cross-corpus tasks.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "D. Efficient Fine-Tuning Using Lora",
      "text": "LoRA  [115]  (Low-Rank Adaptation) is an efficient finetuning technique. By introducing low-rank matrices for parameter adjustments, LoRA significantly reduces the number of parameters that need to be updated, thereby lowering the computational resources and storage requirements, especially when fine-tuning acoustic foundation models. Furthermore, LoRA only fine-tunes a subset of parameters, allowing it to maintain the capabilities of the pre-trained model while focusing on learning new tasks. Therefore, LoRA serves as an effective alternative to full fine-tuning.\n\nIn this section, we apply LoRA to acoustic foundation models, freezing the parameters of the foundation model and updating only the low-rank matrices of LoRA. For compatibility  and cost considerations, we tested four base models: data2vecaudio-base, HuBERT-base, wav2vec2-base, and WavLM-base.\n\nThe results are shown in Figure  4 . The experimental results indicate that after introducing LoRA, most speech foundation models show significant performance improvements in downstream tasks; however, WavLM-base experiences performance degradation in most tasks. This degradation may arise from several factors: first, the parameters of WavLM-base have already been thoroughly trained on large-scale data, so LoRA may not effectively capture the existing features and could introduce interference instead; second, the low-rank matrices of LoRA may not be well-suited to the specific architecture of WavLM-base, resulting in the loss of important information.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "E. The Effect Of Hidden States Of Each Layer",
      "text": "The architecture of most contemporary acoustic foundation models consists of multiple stacked Transformer layers. For example, the WavLM-base model has 12 layers, while the WavLM-Large model has 24. Research  [116] ,  [117]  has shown that different Transformer layers capture varying levels of semantic information, making it valuable to analyze their performance across paralinguistic tasks. Given that ParaLBench includes numerous such tasks and considering computational constraints, we selected the WavLM model, which has shown strong performance across multiple paralinguistic tasks, for testing. The results in Figure  5  illustrate the performance of different Transformer layers across various tasks, offering insights for optimizing speech foundation models in the future.\n\nThe results indicate that, whether in short, medium, or longterm paralinguistic tasks, the best performance does not come from the model's last hidden state but rather from the last few layers of the model. This trend is consistent in both the WavLM-base and WavLM-large versions, suggesting that deep hidden states in the model tend to capture more critical semantic features across different tasks and model sizes. This finding not only corroborates the theory that Transformer layers extract distinct features but also emphasizes the importance of leveraging the internal multi-layer features of the model in",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "F. Last Hidden State Vs. Fusion State",
      "text": "Considering that using only the last hidden state may overlook some task-relevant key information, we employed a learnable weighted-sum method  [20] ,  [68]  to fuse the hidden states from all layers into a single representation. The experimental results, as shown in Table  VIII , demonstrate that in most tasks, the fusion state generally outperforms the last hidden state. This indicates that fusion state from multiple layers helps capture and integrate richer task-relevant semantic information, thereby enhancing the model's overall performance in downstream tasks. However, it is worth noting that the fusion state does not always provide an advantage in every task. For example, in the DAI-WOZ and TIMIT datasets, the performance of the last hidden state surpasses that of the fusion states. This could be due to the potential introduction of noise when fusing multiple layers of hidden states in certain specific tasks or datasets, leading to suboptimal performance in downstream tasks. Therefore, the last hidden state still holds distinct advantages or value in these scenarios.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vi. Conclusion And Future Works",
      "text": "In this work, we introduce ParaLBench, a comprehensive benchmark for Computational Paralinguistics (ComParal) evaluation of diverse acoustic foundation models. This extensive benchmark spans thirteen distinct paralinguistic tasks across ten datasets and considers 14 widely recognized models in the speech processing domain. By establishing uniform testing criteria, ParaLBench aims to provide a fair and standardized means to evaluate the generalization, efficiency, and robustness of these models across various tasks. Our comprehensive experiments and subsequent analysis indicate the superiority of self-supervised learning-based speech foundation models over traditional handcrafted feature approaches in terms of both performance and generalizability. Additionally, the study demonstrates the influence of model architecture and data diversity on a model's effectiveness in capturing paralinguistic information. We further analyze the performance trends of speech foundation models relative to short-term, mediumterm, and long-term tasks, delineating paralinguistic tasks along a temporal continuum. With the intent to spur further research and enable uniform evaluations, we will make our code publicly available.\n\nIn the future, we will expand the benchmark to include more ComParal datasets and foundation models, enhancing its generalizability and usefulness. Additionally, we will conduct multilingual evaluations to assess model performance across diverse languages and incorporate multimodal and large language models into our research. These endeavors will not only strengthen ParaLBench but also provide valuable insights into the realm of ComParal and the capabilities of acoustic foundation models.",
      "page_start": 13,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Compared to linguistics that studies the text content, paralinguistics",
      "page": 1
    },
    {
      "caption": "Figure 1: The goal of ComParal",
      "page": 1
    },
    {
      "caption": "Figure 2: An evaluation diagram for ParaLBench. It first extracts deep repre-",
      "page": 4
    },
    {
      "caption": "Figure 3: presents the results of the speech foundation",
      "page": 11
    },
    {
      "caption": "Figure 3: , emotion2vec-base, WavLM-base, and WavLM-large",
      "page": 11
    },
    {
      "caption": "Figure 3: Cross-corpus results of the acoustic foundation model on three emotion datasets: IEMOCAP, MELD, and MSP-Podcast. The subtitles, such as",
      "page": 12
    },
    {
      "caption": "Figure 4: Performance comparison of acoustic foundation models with and without LoRA efficient fine-tuning.",
      "page": 12
    },
    {
      "caption": "Figure 4: The experimental results",
      "page": 12
    },
    {
      "caption": "Figure 5: illustrate the performance",
      "page": 12
    },
    {
      "caption": "Figure 5: The performance of the WavLM model layer feature across short, medium, and long-term datasets. The upper and lower figures display the Base and",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "without LoRA\nwith LoRA": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "(c) w",
          "Column_12": "av2",
          "Column_13": "ve",
          "Column_14": "c2",
          "Column_15": "-b",
          "Column_16": "ase",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "without LoRA\nwith LoRA": "without LoRA\nwith LoRA",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "without LoRA\nwith LoRA": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "n)\ncas",
          "Column_23": "",
          "Column_24": "r)\nTK(",
          "Column_25": "Acce",
          "Column_26": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "without LoRA\nwith LoRA": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "(d)",
          "Column_12": "Wa",
          "Column_13": "vLM",
          "Column_14": "-b",
          "Column_15": "a",
          "Column_16": "se",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "without LoRA\nwith LoRA": "without LoRA\nwith LoRA",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "",
          "Column_23": "",
          "Column_24": "",
          "Column_25": "",
          "Column_26": ""
        },
        {
          "without LoRA\nwith LoRA": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": "",
          "Column_13": "",
          "Column_14": "",
          "Column_15": "",
          "Column_16": "",
          "Column_17": "",
          "Column_18": "",
          "Column_19": "",
          "Column_20": "",
          "Column_21": "",
          "Column_22": "ion)\ncas",
          "Column_23": "",
          "Column_24": "r)\nTK(",
          "Column_25": "Acce",
          "Column_26": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "k(Stu"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": ")\nse(",
          "Column_2": "Infl"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": ")\n-28k",
          "Column_2": "(Stu"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "(Dep"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "P(Emo"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": ")\nse(",
          "Column_2": "Infl"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": ")\n(",
          "Column_2": "Depr"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "a",
          "Column_2": "st(E"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": ")\nSI(S"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "a",
          "Column_2": "st(E"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "SI",
          "Column_2": "(Se"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "P(Emo"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "D(E"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "n)\nD(S",
          "Column_2": "enti"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Paralinguistics in speech and language -State-of-the-art and the challenge",
      "authors": [
        "B Schuller",
        "S Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "2",
      "title": "Transformer encoder with multimodal multi-head attention for continuous affect recognition",
      "authors": [
        "H Chen",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "3",
      "title": "HAFFormer: A hierarchical attention-free framework for Alzheimer's disease detection from spontaneous speech",
      "authors": [
        "Z Dong",
        "Z Zhang",
        "W Xu",
        "J Han",
        "J Ou",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "COVID-19 response in low-and middle-income countries: Don't overlook the role of mobile phone communication",
      "authors": [
        "L Verhagen",
        "R De Groot",
        "C Lawrence",
        "J Taljaard",
        "M Cotton",
        "H Rabie"
      ],
      "year": "2020",
      "venue": "International Journal of Infectious Diseases"
    },
    {
      "citation_id": "5",
      "title": "Attention-augmented end-toend multi-task learning for emotion prediction from speech",
      "authors": [
        "Z Zhang",
        "B Wu",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Racial disparities in automated speech recognition",
      "authors": [
        "A Koenecke",
        "A Nam",
        "E Lake",
        "J Nudell",
        "M Quartey",
        "Z Mengesha",
        "C Toups",
        "J Rickford",
        "D Jurafsky",
        "S Goel"
      ],
      "year": "2020",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "7",
      "title": "Towards conditional adversarial training for predicting emotions from speech",
      "authors": [
        "J Han",
        "Z Zhang",
        "Z Ren",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "The INTERSPEECH 2009 emotion challenge",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner"
      ],
      "year": "2009",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "9",
      "title": "The ACM multimedia 2023 computational paralinguistics challenge: Emotion share & requests",
      "authors": [
        "B Schuller",
        "A Batliner",
        "S Amiriparian",
        "A Barnhill",
        "M Gerczuk",
        "A Triantafyllopoulos",
        "A Baird",
        "P Tzirakis",
        "C Gagne",
        "A Cowen",
        "N Lackovic",
        "M Caraty",
        "C Montacié"
      ],
      "year": "2023",
      "venue": "Proc. ACM International Conference on Multimedia (MM)"
    },
    {
      "citation_id": "10",
      "title": "Is deep learning a valid approach for inferring subjective self-disclosure in humanrobot interactions?",
      "authors": [
        "H Powell",
        "G Laban",
        "J George",
        "E Cross"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition with dual-sequence lstm architecture",
      "authors": [
        "J Wang",
        "M Xue",
        "R Culhane",
        "E Diao",
        "J Ding",
        "V Tarokh"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Quality-aware bag of modulation spectrum features for robust speech emotion recognition",
      "authors": [
        "S Kshirsagar",
        "T Falk"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "openxbow-introducing the passau opensource crossmodal bag-of-words toolkit",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "15",
      "title": "wav2vec: Unsupervised pre-training for speech recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "16",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "17",
      "title": "WavLM: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "19",
      "title": "SpeechFormer++: A hierarchical efficient framework for paralinguistic speech processing",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Pang",
        "L Du"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "A large-scale evaluation of speech foundation models",
      "authors": [
        "S.-W Yang",
        "H.-J Chang",
        "Z Huang",
        "A Liu",
        "C.-I Lai",
        "H Wu",
        "J Shi",
        "X Chang",
        "H.-S Tsai",
        "W.-C Huang"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "Y Wu",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A Dai",
        "A Hauth"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "23",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "B Schuller",
        "A Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "24",
      "title": "Complex paralinguistic analysis of speech: Predicting gender, emotions and deception in a hierarchical framework",
      "authors": [
        "A Velichko",
        "M Markitantov",
        "H Kaya",
        "A Karpov"
      ],
      "year": "2022",
      "venue": "Proc. International Speech Communication Association"
    },
    {
      "citation_id": "25",
      "title": "The intonational system of english",
      "authors": [
        "M Liberman"
      ],
      "year": "1975",
      "venue": "The intonational system of english"
    },
    {
      "citation_id": "26",
      "title": "QI-TTS: questioning intonation control for emotional speech synthesis",
      "authors": [
        "H Tang",
        "X Zhang",
        "J Wang",
        "N Cheng",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing ICASSP 2023"
    },
    {
      "citation_id": "27",
      "title": "Age differences in the effects of speaking rate on auditory, visual, and auditory-visual speech perception",
      "authors": [
        "M Sommers",
        "B Spehar",
        "N Tye-Murray",
        "J Myerson",
        "S Hale"
      ],
      "year": "2020",
      "venue": "Ear and hearing"
    },
    {
      "citation_id": "28",
      "title": "The influence of voice volume, pitch, and speech rate on progressive relaxation training: application of methods from speech pathology and audiology",
      "authors": [
        "G Knowlton",
        "K Larkin"
      ],
      "year": "2006",
      "venue": "Applied psychophysiology and biofeedback"
    },
    {
      "citation_id": "29",
      "title": "The effect of voice volume on the perception of personality",
      "authors": [
        "R Page",
        "J Balloun"
      ],
      "year": "1978",
      "venue": "The journal of social psychology"
    },
    {
      "citation_id": "30",
      "title": "Gender classification in speech recognition using fuzzy logic and neural network",
      "authors": [
        "K Meena",
        "K Subramaniam",
        "M Gomathy"
      ],
      "year": "2013",
      "venue": "International Arab Journal of Information Technology (IAJIT)"
    },
    {
      "citation_id": "31",
      "title": "Gender recognition system using speech signal",
      "authors": [
        "M Ali",
        "M Islam",
        "M Hossain"
      ],
      "year": "2012",
      "venue": "International Journal of Computer Science, Engineering and Information Technology (IJCSEIT)"
    },
    {
      "citation_id": "32",
      "title": "Speech gender classification using bidirectional long short term memory",
      "authors": [
        "R Alamsyah",
        "S Suyanto"
      ],
      "year": "2020",
      "venue": "Proc. International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)"
    },
    {
      "citation_id": "33",
      "title": "Emotion, age, and gender classification in children's speech by humans and machines",
      "authors": [
        "H Kaya",
        "A Salah",
        "A Karpov",
        "O Frolova",
        "A Grigorev",
        "E Lyakso"
      ],
      "year": "2017",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "34",
      "title": "A comparative study of gender and age classification in speech signals",
      "authors": [
        "M Sedaaghi"
      ],
      "year": "2009",
      "venue": "Iranian Journal of Electrical & Electronic Engineering"
    },
    {
      "citation_id": "35",
      "title": "Automatic speaker and age identification of children from raw speech using sincNet over ERB scale",
      "authors": [
        "K Radha",
        "M Bansal",
        "R Pachori"
      ],
      "year": "2024",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "36",
      "title": "Key-Sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing, (ICASSP), Virtual"
    },
    {
      "citation_id": "37",
      "title": "Customising general large language models for specialised emotion recognition tasks",
      "authors": [
        "L Peng",
        "Z Zhang",
        "T Pang",
        "J Han",
        "H Zhao",
        "H Chen",
        "B Schuller"
      ],
      "year": "2024",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Health condition monitoring of machines based on long short-term memory convolutional autoencoder",
      "authors": [
        "Z Ye",
        "J Yu"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "39",
      "title": "COVID-19 detection system using recurrent neural networks",
      "authors": [
        "A Hassan",
        "I Shahin",
        "M Alsabek"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Communications, Computing, Cybersecurity, and Informatics (CCCI)"
    },
    {
      "citation_id": "40",
      "title": "Deception detection with machine learning: A systematic review and statistical analysis",
      "authors": [
        "A Constâncio",
        "D Tsunoda",
        "H Silva",
        "J Silveira",
        "D Carvalho"
      ],
      "year": "2023",
      "venue": "Plos one"
    },
    {
      "citation_id": "41",
      "title": "Speaker identification through artificial intelligence techniques: A comprehensive review and research challenges",
      "authors": [
        "R Jahangir",
        "Y Teh",
        "H Nweke",
        "G Mujtaba",
        "M Al-Garadi",
        "I Ali"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "42",
      "title": "Speaker recognition based on deep learning: An overview",
      "authors": [
        "Z Bai",
        "X Zhang"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "43",
      "title": "Human-computer interaction for recognizing speech emotions using multilayer perceptron classifier",
      "authors": [
        "A Alnuaim",
        "M Zakariah",
        "P Shukla",
        "A Alhadlaq",
        "W Hatamleh",
        "H Tarazi",
        "R Sureshbabu",
        "R Ratna"
      ],
      "year": "2022",
      "venue": "Journal of Healthcare Engineering"
    },
    {
      "citation_id": "44",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "I Emary"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "45",
      "title": "Combination of statistical and rule-based approaches for spoken language understanding",
      "authors": [
        "Y Wang",
        "A Acero",
        "C Chelba",
        "B Frey",
        "L Wong"
      ],
      "year": "2002",
      "venue": "Proc. International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "46",
      "title": "Rule-based detection of speech features for automatic speech recognition",
      "authors": [
        "R Mori",
        "L Lam",
        "D Probst"
      ],
      "year": "1987",
      "venue": "Fundamentals in computer understanding: speech and vision"
    },
    {
      "citation_id": "47",
      "title": "Speech parameter generation from HMM using dynamic features",
      "authors": [
        "K Tokuda",
        "T Kobayashi",
        "S Imai"
      ],
      "year": "1995",
      "venue": "Proc. International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Hidden markov modelbased speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "Proc. International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Decision tree based depression classification from audio video and language information",
      "authors": [
        "L Yang",
        "D Jiang",
        "L He",
        "E Pei",
        "M Oveneke",
        "H Sahli"
      ],
      "year": "2016",
      "venue": "Proc. International Workshop on Audio/Visual Emotion Challenge (AVEC)"
    },
    {
      "citation_id": "50",
      "title": "Robust decision tree state tying for continuous speech recognition",
      "authors": [
        "W Reichl",
        "W Chou"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "51",
      "title": "Acoustic emotion recognition based on fusion of multiple feature-dependent deep boltzmann machines",
      "authors": [
        "K Poon-Feng",
        "D Huang",
        "M Dong",
        "H Li"
      ],
      "year": "2014",
      "venue": "Proc. International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "52",
      "title": "Representation learning with spectro-temporal-channel attention for speech emotion recognition",
      "authors": [
        "L Guo",
        "L Wang",
        "C Xu",
        "J Dang",
        "E Chng",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "Generalized dilated CNN models for depression detection using inverted vocal tract variables",
      "authors": [
        "N Seneviratne",
        "C Espy-Wilson"
      ],
      "year": "2021",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "54",
      "title": "Speech paralinguistic approach for detecting dementia using gated convolutional neural network",
      "authors": [
        "M Makiuchi",
        "T Warnita",
        "N Inoue",
        "K Shinoda",
        "M Yoshimura",
        "M Kitazawa",
        "K Funaki",
        "Y Eguchi",
        "T Kishimoto"
      ],
      "year": "2021",
      "venue": "IEICE Transactions on Information and Systems"
    },
    {
      "citation_id": "55",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "56",
      "title": "Significance of speaker embeddings and temporal context for depression detection",
      "authors": [
        "S Dumpala",
        "S Rodriguez",
        "S Rempel",
        "R Uher",
        "S Oore"
      ],
      "year": "2021",
      "venue": "Significance of speaker embeddings and temporal context for depression detection",
      "arxiv": "arXiv:2107.13969"
    },
    {
      "citation_id": "57",
      "title": "Speech agegender classification using long short-term memory",
      "authors": [
        "G Nitisara",
        "S Suyanto",
        "K Ramadhani"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Information and Communications Technology (ICOIACT)"
    },
    {
      "citation_id": "58",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "59",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "60",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "A Radford",
        "K Narasimhan",
        "T Salimans",
        "I Sutskever"
      ],
      "year": "2018",
      "venue": "Improving language understanding by generative pre-training"
    },
    {
      "citation_id": "61",
      "title": "Improved MFCC-based feature for robust speaker identification",
      "authors": [
        "Z Wu",
        "Z Cao"
      ],
      "year": "2005",
      "venue": "Tsinghua Science & Technology"
    },
    {
      "citation_id": "62",
      "title": "Analyzing noise robustness of MFCC and GFCC features in speaker identification",
      "authors": [
        "X Zhao",
        "D Wang"
      ],
      "year": "2013",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "63",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "64",
      "title": "Machine-learning analysis of voice samples recorded through smartphones: The combined effect of ageing and gender",
      "authors": [
        "F Asci",
        "G Costantini",
        "P Leo",
        "A Zampogna",
        "G Ruoppolo",
        "A Berardelli",
        "G Saggio",
        "A Suppa"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "65",
      "title": "Multiscale system for alzheimer's dementia recognition through spontaneous speech",
      "authors": [
        "E Edwards",
        "C Dognin",
        "B Bollepalli",
        "M Singh"
      ],
      "year": "2020",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "66",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "Proc. International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "67",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "68",
      "title": "SUPERB: speech processing universal performance benchmark",
      "authors": [
        "S Yang",
        "P Chi",
        "Y Chuang",
        "C Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G Lin",
        "T Huang",
        "W Tseng",
        "K Lee",
        "D Liu",
        "Z Huang",
        "S Dong",
        "S Li",
        "S Watanabe",
        "A Mohamed",
        "H Lee"
      ],
      "year": "2021",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "69",
      "title": "IndicSUPERB: A speech processing universal performance benchmark for indian languages",
      "authors": [
        "T Javed",
        "K Bhogale",
        "A Raman",
        "P Kumar",
        "A Kunchukuttan",
        "M Khapra"
      ],
      "year": "2023",
      "venue": "Proc. AAAI Conference on Artificial Intelligence (AAAI)"
    },
    {
      "citation_id": "70",
      "title": "ML-SUPERB: Multilingual speech universal performance benchmark",
      "authors": [
        "J Shi",
        "D Berrebbi",
        "W Chen",
        "H.-L Chung",
        "E.-P Hu",
        "W Huang",
        "X Chang",
        "S.-W Li",
        "A Mohamed",
        "H.-Y Lee"
      ],
      "year": "2023",
      "venue": "ML-SUPERB: Multilingual speech universal performance benchmark",
      "arxiv": "arXiv:2305.10615"
    },
    {
      "citation_id": "71",
      "title": "SERAB: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "72",
      "title": "How paralingual are paralinguistic representations? a case study in speech emotion recognition",
      "authors": [
        "O Phukan",
        "G Kashyap",
        "A Buduru",
        "R Sharma"
      ],
      "year": "2024",
      "venue": "How paralingual are paralinguistic representations? a case study in speech emotion recognition",
      "arxiv": "arXiv:2402.01579"
    },
    {
      "citation_id": "73",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren",
        "H Gu",
        "H Sun",
        "L Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "74",
      "title": "Trillsson: Distilled universal paralinguistic speech representations",
      "authors": [
        "J Shor",
        "S Venugopalan"
      ],
      "year": "2022",
      "venue": "Trillsson: Distilled universal paralinguistic speech representations",
      "arxiv": "arXiv:2203.00236"
    },
    {
      "citation_id": "75",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "76",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "77",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "78",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "79",
      "title": "MEAD: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "K Wang",
        "Q Wu",
        "L Song",
        "Z Yang",
        "W Wu",
        "C Qian",
        "R He",
        "Y Qiao",
        "C Loy"
      ],
      "year": "2020",
      "venue": "Proc. European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "80",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proc. Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "81",
      "title": "Speech-based age and gender prediction with transformers",
      "authors": [
        "F Burkhardt",
        "J Wagner",
        "H Wierstorf",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Prpc. Speech Communication"
    },
    {
      "citation_id": "82",
      "title": "A database of age and gender annotated telephone speech",
      "authors": [
        "F Burkhardt",
        "M Eckert",
        "W Johannsen",
        "J Stegmann"
      ],
      "year": "2010",
      "venue": "Proc. International Conference on Language Resources and Evaluation (LREC)"
    },
    {
      "citation_id": "83",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Kohler",
        "J Meyer",
        "M Henretty",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2020",
      "venue": "Proc. Language Resources and Evaluation Conference (LREC)"
    },
    {
      "citation_id": "84",
      "title": "Timit acoustic phonetic continuous speech corpus",
      "authors": [
        "J Garofolo"
      ],
      "year": "1993",
      "venue": "Timit acoustic phonetic continuous speech corpus"
    },
    {
      "citation_id": "85",
      "title": "Voxceleb: Largescale speaker verification in the wild",
      "authors": [
        "A Nagrani",
        "J Chung",
        "W Xie",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "86",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "87",
      "title": "MLS: A large-scale multilingual dataset for speech research",
      "authors": [
        "V Pratap",
        "Q Xu",
        "A Sriram",
        "G Synnaeve",
        "R Collobert"
      ],
      "year": "2020",
      "venue": "Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH)"
    },
    {
      "citation_id": "88",
      "title": "Libri-Light: A benchmark for ASR with limited or no supervision",
      "authors": [
        "J Kahn",
        "M Rivière",
        "W Zheng",
        "E Kharitonov",
        "Q Xu",
        "P Mazaré",
        "J Karadayi",
        "V Liptchinsky",
        "R Collobert",
        "C Fuegen",
        "T Likhomanenko",
        "G Synnaeve",
        "A Joulin",
        "A Mohamed",
        "E Dupoux"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "89",
      "title": "VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
      "authors": [
        "C Wang",
        "M Rivière",
        "A Lee",
        "A Wu",
        "C Talnikar",
        "D Haziza",
        "M Williamson",
        "J Pino",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "Proc. Association for Computational Linguistics and the International Joint Conference on Natural Language Processing (ACL/IJCNLP), Virtual Event"
    },
    {
      "citation_id": "90",
      "title": "GigaSpeech: An evolving, multi-domain ASR corpus with 10, 000 hours of transcribed audio",
      "authors": [
        "G Chen",
        "S Chai",
        "G Wang",
        "J Du",
        "W Zhang",
        "C Weng",
        "D Su",
        "D Povey",
        "J Trmal",
        "J Zhang",
        "M Jin",
        "S Khudanpur",
        "S Watanabe",
        "S Zhao",
        "W Zou",
        "X Li",
        "X Yao",
        "Y Wang",
        "Z You",
        "Z Yan"
      ],
      "year": "2021",
      "venue": "Proc. the International Speech Communication Association"
    },
    {
      "citation_id": "91",
      "title": "Large-Scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Y Wu",
        "K Chen",
        "T Zhang",
        "Y Hui",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2023",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "92",
      "title": "Clotho: an audio captioning dataset",
      "authors": [
        "K Drossos",
        "S Lipping",
        "T Virtanen"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "93",
      "title": "AudioCaps: Generating captions for audios in the wild",
      "authors": [
        "C Kim",
        "B Kim",
        "H Lee",
        "G Kim"
      ],
      "year": "2019",
      "venue": "Proc. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)"
    },
    {
      "citation_id": "94",
      "title": "Audio Set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "year": "2017",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "95",
      "title": "data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "96",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "97",
      "title": "Deep clustering for unsupervised learning of visual features",
      "authors": [
        "M Caron",
        "P Bojanowski",
        "A Joulin",
        "M Douze"
      ],
      "year": "2018",
      "venue": "Proc. European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "98",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proc. International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "99",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "100",
      "title": "CovNet: A transfer learning framework for automatic COVID-19 detection from crowdsourced cough sounds",
      "authors": [
        "Y Chang",
        "X Jing",
        "Z Ren",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Frontiers in Digital Health"
    },
    {
      "citation_id": "101",
      "title": "FluentNet: End-toend detection of stuttered speech disfluencies with deep learning",
      "authors": [
        "T Kourkounakis",
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "102",
      "title": "Audio based depression detection using convolutional autoencoder",
      "authors": [
        "S Sardari",
        "B Nakisa",
        "M Rastgoo",
        "P Eklund"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "103",
      "title": "Transformer-based multimodal feature enhancement networks for multimodal depression detection integrating video, audio and remote photoplethysmograph signals",
      "authors": [
        "H Fan",
        "X Zhang",
        "Y Xu",
        "J Fang",
        "S Zhang",
        "X Zhao",
        "J Yu"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "104",
      "title": "Language accent detection with cnn using sparse data from a crowd-sourced speech archive",
      "authors": [
        "V Mikhailava",
        "M Lesnichaia",
        "N Bogach",
        "I Lezhenin",
        "J Blake",
        "E Pyshkin"
      ],
      "year": "2022",
      "venue": "Mathematics"
    },
    {
      "citation_id": "105",
      "title": "Transformer-based arabic dialect identification",
      "authors": [
        "W Lin",
        "M Madhavi",
        "R Das",
        "H Li"
      ],
      "year": "2020",
      "venue": "Proc. International Conference on Asian Language Processing (IALP)"
    },
    {
      "citation_id": "106",
      "title": "Multimodal learning using optimal transport for sarcasm and humor detection",
      "authors": [
        "S Pramanick",
        "A Roy",
        "V Patel"
      ],
      "year": "2022",
      "venue": "Proc. Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "107",
      "title": "SEP-28k: A dataset for stuttering event detection from podcasts with people who stutter",
      "authors": [
        "C Lea",
        "V Mitra",
        "A Joshi",
        "S Kajarekar",
        "J Bigham"
      ],
      "year": "2021",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "108",
      "title": "Towards multimodal sarcasm detection (an obviously perfect paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proc. Association for Computational Linguistics (ACL)"
    },
    {
      "citation_id": "109",
      "title": "FluSense: A contactless syndromic surveillance platform for influenza-like illness in hospital waiting areas",
      "authors": [
        "F Hossain",
        "A Lover",
        "G Corey",
        "N Reich",
        "T Rahman"
      ],
      "year": "2020",
      "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol"
    },
    {
      "citation_id": "110",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "111",
      "title": "Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit",
      "authors": [
        "C Veaux",
        "J Yamagishi",
        "K Macdonald"
      ],
      "year": "2016",
      "venue": "Superseded-cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit",
      "doi": "10.7488/ds/2645"
    },
    {
      "citation_id": "112",
      "title": "Multi-view learning for speech emotion recognition with categorical emotion, categorical sentiment, and dimensional scores",
      "authors": [
        "D Tompkins",
        "D Emmanouilidou",
        "S Deshmukh",
        "B Elizalde"
      ],
      "year": "2023",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "113",
      "title": "Robust disentangled variational speech representation learning for zero-shot voice conversion",
      "authors": [
        "J Lian",
        "C Zhang",
        "D Yu"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), Virtual"
    },
    {
      "citation_id": "114",
      "title": "Vocbench: A neural vocoder benchmark for speech synthesis",
      "authors": [
        "E Albadawy",
        "A Gibiansky",
        "Q He",
        "J Wu",
        "M.-C Chang",
        "S Lyu"
      ],
      "year": "2022",
      "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "115",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "116",
      "title": "Deriving contextualised semantic features from BERT (and other transformer model) embeddings",
      "authors": [
        "J Turton",
        "R Smith",
        "D Vinson"
      ],
      "year": "2021",
      "venue": "Proc. Representation Learning for NLP"
    },
    {
      "citation_id": "117",
      "title": "What does a language-and-vision transformer see: The impact of semantic information on visual representations",
      "authors": [
        "N Ilinykh",
        "S Dobnik"
      ],
      "year": "2021",
      "venue": "Frontiers Artif. Intell",
      "doi": "10.3389/frai.2021.767971"
    },
    {
      "citation_id": "118",
      "title": "15-SM'22) received his master degree in physical electronics from the Beijing University of Posts and Telecommunications",
      "authors": [
        "Zixing Zhang"
      ],
      "venue": "15-SM'22) received his master degree in physical electronics from the Beijing University of Posts and Telecommunications"
    }
  ]
}