{
  "paper_id": "2408.03648v1",
  "title": "Hique: Hierarchical Question Embedding Network For Multimodal Depression Detection",
  "published": "2024-08-07T09:23:01Z",
  "authors": [
    "Juho Jung",
    "Chaewon Kang",
    "Jeewoo Yoon",
    "Seungbae Kim",
    "Jinyoung Han"
  ],
  "keywords": [
    "CCS Concepts",
    "Computing methodologies â†’ Artificial intelligence",
    "â€¢ Information systems â†’ Multimedia information systems",
    "Data mining Multimodal Depression Detection, Hierarchical Question Embedding, Clinical Interview ğ‘ªğ’ğ’ğ’”ğ’Šğ’…ğ’†ğ’“ ğ’’ğ’–ğ’†ğ’”ğ’•ğ’Šğ’ğ’-ğ’‚ğ’ğ’”ğ’˜ğ’†ğ’“ ğ’‘ğ’‚ğ’Šğ’“"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The utilization of automated depression detection significantly enhances early intervention for individuals experiencing depression. Despite numerous proposals on automated depression detection using recorded clinical interview videos, limited attention has been paid to considering the hierarchical structure of the interview questions. In clinical interviews for diagnosing depression, clinicians use a structured questionnaire that includes routine baseline questions and follow-up questions to assess the interviewee's condition. This paper introduces HiQuE (Hierarchical Question Embedding network), a novel depression detection framework that leverages the hierarchical relationship between primary and followup questions in clinical interviews. HiQuE can effectively capture the importance of each question in diagnosing depression by learning mutual information across multiple modalities. We conduct extensive experiments on the widely-used clinical interview data, DAIC-WOZ, where our model outperforms other state-of-the-art multimodal depression detection models and emotion recognition models, showcasing its clinical utility in depression detection.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The diagnosis of depression in clinical settings often involves the use of interview-based instruments  [66] , in which mental health experts conduct clinical interviews with patients, assessing their symptoms  [53, 66, 68, 89] . Due to the gradual and varied manifestation of depressive symptoms among individuals  [13, 17, 18, 48] , clinicians employ a structured interview process, which involves specific questionnaires and criteria, to detect a wide range of verbal and non-verbal symptoms of depression in patients' speech, behavior, facial expressions, and immediate responses during conversations  [46, 71, 79] .\n\nInterview-based diagnostic methods have proven highly effective in interpreting patient responses  [29, 53] . Mental health experts strategically incorporate follow-up questions in conjunction with primary questions to gather additional information from patients, thereby enhancing their understanding of the exhibited depressive symptoms  [68] . In a case where a response from an initial follow-up question is insufficient for diagnosis, further follow-up questions can be employed to synthesize the patient's responses. By employing hierarchical questions during clinical interviews, clinicians can obtain a comprehensive understanding of depressive signals and the patient's overall condition, leading to improved diagnostic accuracy  [17, 68] .\n\nUsing clinical interview data, many scholars have proposed methods that can detect depression by analyzing revealed verbal (e.g., textual) or non-verbal (e.g., visual or acoustic) signals. Some studies have delved into visual cues, encompassing facial expressions and head poses  [4, 5, 31, 51] . Additionally, a series of investigations has focused on acoustic and textual cues, with the goal of diagnosing depression based on linguistic patterns, vocal qualities, pitch, and loudness  [2, 43, 63, 82, 91] . Nevertheless, these approaches have treated the entire input sequence as a singular entity, disregarding the structured nature of clinical interviews. Given the use of structured questionnaires and conversational exchanges during these interviews, considering the interview data as a single input sequence can be less effective for depression detection in  clinical settings  [32, 87] . A few studies have sought to consider the conversations, including questions and answers during the clinical interviews  [11, 45, 52, 80, 81, 87] . However, their focus has primarily been on analyzing questions and answers in an interview without specifically modeling the relationship between primary and follow-up questions based on question types, which can be crucial in modeling and analyzing the structure of a clinical interview. Besides, there has been a lack of analysis on the interaction among multi-modalities in analyzing structured clinical interviews  [2, 19, 25, 37, 44, 59, 65, 87] ; different modalities can be different cues depending on questions and answer types.\n\nTo address these challenges, we propose HiQuE (Hierarchical Question Embedding network), a novel depression detection framework that leverages the hierarchical relationship between primary and follow-up questions in a clinical interview. Inspired by the clinical interview strategy employed by medical professionals for diagnosing depression, HiQuE incorporates a hierarchical embedding structure and interview-specific attention modules. These modules enable HiQuE to comprehensively assess the mutual information between multiple modalities within interviews, replicating the diagnostic approach used by clinicians. As illustrated in Figure  1 , the interview sequence is divided into primary questions and their corresponding follow-up questions. Using the question-aware module, HiQuE calculates the significance of each question and effectively enhances the mutual information across modalities using cross-modal attention, resulting in accurate depression diagnosis. The contributions of this study can be summarized as follows:\n\nâ€¢ To the best of our knowledge, HiQuE is the first attempt that analyzes the significance of all questions posed by the interviewer by explicitly categorizing them as primary and follow-up questions, considering their order and relationship. Our publicly available code 1  encompasses both the hierarchical question embedding process and the HiQuE. â€¢ This is the first interpretable multi-modal analysis conducted in a clinical interview context by analyzing both intra-modality and inter-modality attention scores. The quantitative evaluation of the interaction and importance of different modalities in depression detection provides deeper insights into the complex dynamics of clinical interviews. â€¢ HiQuE achieves the state-of-the-art performance on the DAIC-WOZ dataset, among other multimodal emotion recognition models as well as prior depression detection models that utilized the DAIC-WOZ dataset. HiQuE also demonstrates superior adaptation to the E-DAIC-WOZ dataset, highlighting its generalizability to unseen question scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Multimodal Expressions Of Major Depressive Disorder In Clinical Interviews",
      "text": "Researchers have identified distinctive features across various modalities, including acoustic patterns, visual characteristics, and language usage in clinical interviews. For instance, individuals with depression often exhibit specific acoustic features, such as slower speaking rates, lower pitch ranges, and reduced loudness  [6, 9, 20, 36, 75, 76, 79]  as well as visual features, including discernible facial expressions characterized by sadness, minimal head movement  [5] , unstable facial expressions  [14, 78] , and irregular eye-gazing patterns  [33, 40] . Moreover, they often reveal negative emotions in language, utilize a higher frequency of first-person pronouns, and exhibit intense focus on specific words  [3, 61, 85] . These findings highlight the crucial role of incorporating multiple modalities for effective depression detection to develop a comprehensive understanding of an individual's symptoms  [17, 18, 48] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Automatic Depression Detection",
      "text": "There have been a considerable number of proposals to detect depression by developing machine learning algorithms or natural language processing techniques  [41, 47] . Initially, a substantial efforts were dedicated to extracting representative features  [69, 70]  and creating single-modality models for depression detection  [30, 51, 77, 82, 83] . Furthermore, as Multimodal Sentiment Analysis (MSA)  [49, 67]  gained momentum with the recognition of various verbal and non-verbal symptoms of depression in psychological research, researchers made significant attempts to incorporate context-aware attention  [10]  and multimodal attention  [26]  to capture diverse  information across multiple modalities  [18, 23, 37, 62, 86, 88] . Recently, there have been attempts that analyze the word-sentence relations on interviewee's answers  [45, 59, 81, 87, 92]  as well as the correlation between question-answer pairs  [24, 52, 80] , which can be cued in identifying depression. Unfortunately, no research exists yet that explains how an attention score of modality manifests in a specific question or how the degree of modality reflection changes with the sequence of primary and follow-up questions and answers, which can be crucial in modeling and analyzing the structure of a clinical interview. To bridge this gap, we introduce the first interpretable multimodal depression detection framework that leverages the hierarchical relationship between primary and follow-up questions in a clinical interview.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Depression Detection Layer",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Clinical Interview Dataset",
      "text": "To train our proposed method for the depression detection task, we use the DAIC-WOZ dataset  [28] , which is a subset of the widely used dataset called Distress Analysis Interview Corpus (DAIC)  [74] . The DAIC-WOZ dataset comprises clinical interviews conducted to diagnose psychological distress disorders. These interviews involve Wizard-of-Oz interactions, where an AI virtual interviewer named Ellie is controlled by a human interviewer located remotely. The dataset consists of speech samples from 189 participants, including audio/visual features, raw audio files, and interview transcripts. Following the prescribed guidelines, we split the dataset into 107 training samples, 35 validation samples, and 47 test samples.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation With Random Sampling",
      "text": "The DAIC-WOZ dataset suffers from a significant class imbalance, with a higher proportion of non-depression samples. Some prior studies addressed this issue by employing data augmentation techniques like random masking  [7, 43, 65] . Inspired by these, we tripled the size of the depression dataset by randomly masking 10 out of 85 questions in each 85 Ã— ğ‘ question-embedded interview sequence, aligning it with the size of the non-depression dataset during training. Specifically, we first segmented the interview sequences into question-answer (Q-A) pairs based on timestamps, starting from the interviewer's question to the participant's response. Then, we randomly masked ten Q-A pairs per interview, corresponding to the interviewer's questions. Unused questions  , where ğ‘ ğ‘– contains the multi-modal inputs including audio, video, and text sequences; ğ‘‹ ğ‘ âˆˆ R ğ¿ ğ‘ Ã—ğ‘‘ ğ‘ , ğ‘‹ ğ‘£ âˆˆ R ğ¿ ğ‘£ Ã—ğ‘‘ ğ‘£ , and ğ‘‹ ğ‘¡ âˆˆ R ğ¿ ğ‘¡ Ã—ğ‘‘ ğ‘¡ , where ğ¿ ğ‘š represents the sequence length and ğ‘‘ indicates the feature dimension. Given the hierarchical structure of interview questions, we segment the interview sequence into question-answer pairs. Specifically, each input sequence is defined as ğ‘† = {ğ‘  ğ‘– } ğ‘› ğ‘–=1 , where ğ‘  ğ‘– = (ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘› ğ‘– , ğ‘ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ ğ‘– ). We then annotate the input sequence ğ‘† with corresponding hierarchical positions in the hierarchical question embedding process, denoted as Åœ = {(ğ‘  ğ‘– , ğ‘ğ‘œğ‘  ğ‘– )} ğ‘› ğ‘–=1 , where ğ‘ğ‘œğ‘  ğ‘– indicates the hierarchical position of the ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘› ğ‘– . Finally, the proposed model predicts an individual ğ‘ ğ‘– depression symptom Å· âˆˆ {ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™, ğ‘‘ğ‘’ğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘–ğ‘œğ‘›}.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Overall Architecture",
      "text": "The proposed method, HiQuE, as shown in Figure  2 , consists of three layers: (i) Question-Aware Module, (ii) Cross-Modal Attention, and (iii) Depression Detection. HiQuE categorizes interview sequences into main and follow-up questions using a hierarchical question embedding process. Audio, visual, and text features are extracted separately, and the Question-Aware Module generates Question-Aware representations for each feature. These are combined in the Cross-Modal Attention layer to create a final multimodal representation, which the Depression Detection layer uses to predict the presence of depression.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hierarchical Question Embedding Process",
      "text": "As depicted in Figure  3 , interviewer Ellie's questions are categorized into 85 topics based on content, following Gong et al.'s approach  [27] . Each question is associated with a specific topic code, such as labeling \"How has seeing a therapist affected you?\" as therapist_affect and \"Where are you from originally?\" as origin. These questions are further categorized into 66 primary and 19 follow-up questions based on content and order. For a complete list of the questions, please refer to Table  5 . Finally, we systematically tag each question based on its hierarchical order, specifically when a follow-up question follows a primary question or when a follow-up question follows a previous follow-up question. For instance, where the question sequence is \"What did you study at school?\", \"Are you still working in that?\", and \"How hard is that?\", the hierarchical order would be primary -follow-up -follow-up.\n\nAn overall process of hierarchical question embedding is depicted in Figure  3 . Interview sequences are represented as unimodal raw sequences ğ‘‹ ğ‘š , where ğ‘š denotes ğ‘šğ‘œğ‘‘ğ‘ğ‘™ğ‘–ğ‘¡ğ‘¦ âˆˆ {ğ‘, ğ‘£, ğ‘¡ }, respectively. Sequences ğ‘‹ ğ‘š are partitioned into segments ğ‘† = {ğ‘  ğ‘– } ğ‘› ğ‘–=1 based on question and answer boundaries. Note that the number of segments, ğ‘›, may vary for each sample due to differences in the type and number of questions employed during each interview. Then, each segment is split into a question segment and an answer segment;\n\nAfter partitioning, segments are labeled with topic codes corresponding to each question and given hierarchical position embeddings based on their relationships. Specifically, as shown in Figure  3 , we assign the previous question's Topic id to the follow-up question. These hierarchical positions are incorporated into the representation before feeding them into the model. This embedding process ensures uniform vector shapes by replacing unused questions with zero vectors, resulting in 85-dimensional representations for all samples.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Feature Extraction",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Audio Feature:",
      "text": "For audio feature extraction, we utilize the open-Source Media Interpretation by Large feature-space Extraction (openSMILE)  [22] , along with the extended Geneva Minimalistic Acoustic Parameter Set  [21] . These features encompass 88 functionals, including loudness, MFCCs, and other characteristics that aid in discerning emotions in speech. Consequently, each interviewee's audio features are represented as 85 Ã— 88-dimensional vectors, where 85 denotes the question embedding dimension. These audio features are then processed using a transformer encoder.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visual",
      "text": "Feature: Due to privacy concerns, the dataset only offers visual features extracted via the Constrained Local Neural Fields (CLNF) algorithm [8], a widely-used approach for facial landmark localization and face recognition. To address the variation in interview duration for each answer, we first extract 68 facial landmarks from each frame (at a rate of 1 frame per second) within each segment, considering their respective ğ‘¥ and ğ‘¦ coordinates. We then compute mean and variance vectors within each segment and concatenate the ğ‘¥ and ğ‘¦ coordinates. This results in 85 Ã— 272dimensional vectors per participant, with zero vectors used for segments where a face is not detected.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Text Feature:",
      "text": "For text feature extraction, we segment the interview transcripts into sections corresponding to individual answers for each question. We next leverage the pre-trained RoBERTa  [42]  to generate text features from each answer segment. Given the RoBERTa's strength in robustly capturing contextual information and semantic nuances in various NLP task  [38] , it demonstrated superior performance compared to other embedding methods and large language models (LLMs) as shown in Section 5.3. We extract features from the final layer, focusing on the [CLS] token, resulting in an 85 Ã— 768-dimensional vector for each answer, where 85 represents the dimensionality of the question embedding.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Question-Aware Module Layer",
      "text": "In Figure  2 , a transformer encoder with â„ multi-heads is utilized to capture attention between questionnaire responses. Initially, a stack of 1-dimensional convolutional layers is applied to process local information, converting varying shapes of HIQ Visual Rep. Subsequently, the question-aware self-attention mechanism guides the transformer encoder to focus on important segments and relationships among the question-embedded sequences. Given that each representation is embedded based on 85 questions, self-attention allows the model to focus on important questions within the questionembedded representation. As a result, this particular attention mechanism enables HiQuE to extract meaningful information, represented as Q-A ğ‘€ Rep; ğ‘€ âˆˆ {ğ´ğ‘¢ğ‘‘ğ‘–ğ‘œ, ğ‘‰ ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™,ğ‘‡ ğ‘’ğ‘¥ğ‘¡ }, in the form of 85 Ã— 85 matrices, for depression detection from each question. We analyze this unimodal attention score to identify the significant components of intra-modality. The same input is employed for self-attention as query (ğ‘„), key (ğ¾), and value (ğ‘‰ ) in the following equations:\n\nğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ (ğ‘„, ğ¾, ğ‘‰ ) = ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¡ğ‘’ (â„ğ‘’ğ‘ğ‘‘ 1 , ..., â„ğ‘’ğ‘ğ‘‘ â„ )\n\n(1)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cross-Modal Attention Layer",
      "text": "In Figure  2 , the multimodal transformer encoder with â„ multiheads integrates information from two modalities using a crossattention mechanism  [34] . This mechanism allows the model to discern crucial relationships between ğ‘š 1 and ğ‘š 2 modalities, with ğ‘š 1 serving as the source (query) and ğ‘š 2 as the target (key and value). Furthermore, since the information in the two modalities differs, we conduct bidirectional cross-attention between ğ‘š 1 and ğ‘š 2 (i.e., audio-visual, visual-text, and text-audio) to allow the model to learn relevant information across modalities as follows:\n\nGiven that the input to the cross-modal attention layer is ğ‘ˆ ğ‘š , ğ‘š âˆˆ ğ‘¡, ğ‘, ğ‘£ from the question-aware module layer, each input representation has a shape of 85 Ã— 85. This allows us to analyze the multimodal attention score to identify significant components between different modalities.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Depression Detection Layer",
      "text": "In the last stage, the ğ‘ğ‘¢ğ‘‘ğ‘–ğ‘œ-ğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™, ğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™-ğ‘¡ğ‘’ğ‘¥ğ‘¡, and ğ‘¡ğ‘’ğ‘¥ğ‘¡-ğ‘ğ‘¢ğ‘‘ğ‘–ğ‘œ cross-modal representations are transformed into a final multimodal representation after layer normalization, concatenation, and GAP (global average pooling), as follows:\n\nFinally, multimodal representation is fed into HiQuE's depression detection layer to detect depression as follows:\n\nwhere the HiQuE prediction layer comprises a fully connected layer and a dropout layer. Since the depression detection task can defined as a binary classification problem, we employed the cross entropy as the loss function as follows:\n\nwhere ğ‘ represents the batch size, ğ‘– is an index representing each example within the batch, ğ‘¦ ğ‘– is the actual label where 0 represents normal and 1 represents depression, and Å·ğ‘– is the softmax function that represents the model's prediction or probability.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "We use Tensorflow  [1]  to implement the proposed model. The dropout rate, batch size, epochs, and learning rate were set to 0.5, 8, 100, and 0.0002, respectively. The maximum sequence length was set to 85 since all sequences are embedded into 85 questions. All weights are randomly initialized in both HiQuE and baselines.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Methods",
      "text": "To evaluate the overall performance of the proposed model, we compare its performance against five state-of-the-art multimodal models for depression detection and emotion recognition as follows: (i) Tensor Fusion Network (TFN )  [90] , (ii) bidirectional LSTM / 1D CNN-based model (BiLSTM-1DCNN )  [41] , (iii) Multimodal Transformer (MulT )  [73] , (iv) MISA  [35] , and (v) D-vlog  [88] . Since these models were specifically designed to analyze multimodal fusion methods, we have categorized them as \"Modality-Aware\".\n\nWe further utilize seven context-aware multimodal models for depression detection and emotion recognition to compare the analysis of the hierarchical structure of clinical interviews: (i) bidirectional contextual LSTM (bc-LSTM)  [55] , (ii) Emotion Recognition  [64] , (iii) Sequence Modeling  [2] , (iv) Topic Modeling  [27] , (v) Contextaware deep learning (Context-Aware)  [37] , (vi) Speechformer  [11]  and (vii) GRU/BiLSTM-based  [65] . As these methods consider the context of interview questions and answers or focus on the topics of questions and the timing of their appearance during the interview, we have categorized them as \"Context-Aware\". Note that we extract multimodal features from the entire interview sequence to train the five modality-aware methods and seven context-aware methods. All models were trained on the same data partition to ensure fairness and evaluated using the hyperparameters that showed the best performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results",
      "text": "To provide a comprehensive assessment of the models' performance, particularly in the context of an imbalanced dataset (i.e., DAIC-WOZ), we report experimental results with various metrics including the weighted average and geometric mean scores (G-mean score). Table  1  shows the Macro Average precision / recall / F1-score, Weighted Average precision / recall / F1-score, and G-mean score of the baseline models and the proposed model, respectively.\n\nAs shown in Table  1 , HiQuE achieves the best depression detection with a macro average F1-score of 0.79, a weighted average F1score of 0.82, and a G-mean score of 0.790. As macro-average treats each class equally, while weighted-average gives weight based on class size, the result that HiQuE excels in both metrics highlights HiQuE's robustness and effectiveness against an imbalanced dataset. showcasing its ability to capture distinct depression indicators.\n\nAmong the baseline models, GRU/BiLSTM-based  [65]  achieves the highest performance with a macro average F1-score of 0.75, weighted average precision of 0.86, and G-mean score of 0.765. This underscores the effectiveness of analyzing speech characteristics We also find that MulT and D-vlog exhibit promising performance at 0.74 and 0.73 of the macro average F1-score, respectively. This suggests that employing a cross-attention mechanism to learn the relationship between multiple modalities helps the model learn important signals for depression detection.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Text Embedding Performance Comparison",
      "text": "We chose to use the pre-trained RoBERTa  [42]  as an encoder due to its higher performance as shown in in Table  2 , in comparison with other popular embedding techniques and large language models (LLMs). The high performance of RoBERTa is due to its robust representations and comprehensive contextual understanding. LLMs also showed a comparable performance as shown in Table  2 , but we decided not to use them due to practical challenges related to privacy and stability, particularly in mental health applications.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Generalization To Unseen Questions",
      "text": "To assess HiQuE's generalizability, we further utilized the E-DAIC-WOZ  [58]  and MIT Interview dataset  [50] . The E-DAIC-WOZ  [58]  comprises audio-visual recordings of semi-clinical interviews conducted in English, featuring numerous questions absent in the DAIC-WOZ dataset. However, it does not provide the transcript of interviewer's questions, making it difficult to determine the specific questions asked. The MIT Interview dataset  [50]  includes 138 interview videos of internship-seeking students from MIT, featuring facial expressions, language use, and prosodic cues. Moreover, it provides ground truth labels for stress level and job interview performance, rated by nine independent judges. This dataset encompasses multimodal features influencing mental states during job interviews  [50] .\n\nWe adapt our model to these datasets by extracting text from the audio using the whisper  [56]  and mapping unseen questions to the predefined list (Table  5 ) based on the BERT-score 2 similarity.  If a question's similarity falls below the average, we consider it as a new question and add it to the list. Baselines adopted the same encoder as HiQuE but without hierarchical question embedding. Table  3  shows that HiQuE outperforms the baselines across both datasets for the three different tasks: Depression Detection, Stress Level Prediction, and Job Interview Performance Prediction. This underscores HiQuE's effectiveness in detecting depression cues from clinical interviews, even with non-predefined questions. More importantly, the experimental result that HiQuE has shown promising performance not only in clinical interviews but also in job interviews highlights its usability in various real-world interview scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis 6.1 Analysis On Different Modalities",
      "text": "To analyze the importance of each modality (i.e., audio, visual, and text) for detecting depression, we compare the performance of models that are trained with different sets of modalities. For the unimodal models (i.e., A, V, T), we first simply utilize a hierarchical question embedding process followed by a question-aware module layer for each input modality. We then add global average pooling and fully connected layers with softmax activation function to generate predicted labels (i.e., depressed or not). As shown in Figure  4 , the model trained with text achieves the highest performance (0.71 of macro average F1-score) among the unimodal models. This implies that the text modality contains the most useful information in depression detection, which can be linked to the results of the prior Table  4 : The results of the ablation study on hierarchical question embedding process and model layers as illustrated in Figure  2 , along with augmentation methods with random sampling as described in Section 3.1. The term \"Q-A Module\" denotes the Question-Aware Module Layer, while \"C-M Attention\" represents the Cross-Modal Attention Layer, which is examined as part of the model components. \"Q.E.\" and \"H.Q.E.\" stand for Question Embedding and Hierarchical Question Embedding, respectively, and \"Aug.\" refers to the Augmentation.  studies  [12, 16, 59, 84] . For bimodal models (i.e., A+V, V+T, A+T), we first fuse two unimodal encoders via a cross-modal attention layer. We then add the same depression detection layer as unimodal models. Since the text feature contains the most useful information, we find that the bimodal models trained with text modality (i.e., A+T, V+T) show higher performance than the model trained without text modality (i.e., A+V). Also, we find that considering all modalities (i.e., A+V+T) significantly improves performance. This reveals that learning both verbal and non-verbal signals, as well as their relationships, is an effective way for depression detection.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Intermodal Interaction Analysis",
      "text": "By examining the attention score distributions across different modalities, as depicted in Figure  5 , we highlight the significance of each modality in depression detection. Notably, questions directly related to emotions or past experiences, such as \"Tell me about an event or something that you wish you could erase from your memory?\" or \"Tell me about the last time you felt really happy?\", had a significant impact on both audio and visual modalities, while those related to a current emotional state or past depression diagnosis, such as \"Have you been diagnosed with depression?\" or \"How have you been feeling lately?\", had the highest impact on text.\n\nWe also explore the impact of individual modalities (i.e., audio, visual, and text) when the model fails to make accurate predictions. Depressed patients are often misclassified as normal when interviewees exhibit cheerful tones or frequent laughter (resulting in high audio and visual attention scores, Figure  5 ), or when there are no clear indicators of depression during the interview. Conversely, our model tends to misclassify normal as depression when negative words are frequently used, particularly when participants express recent feelings of anxiety and depression. In these cases, as highlighted in Figure  5 , the text attention score predominantly influences the incorrect predictions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Hierarchical Question Embedding",
      "text": "To highlight the benefits of our proposed hierarchical question embedding process, we conducted an ablation study with three distinct cases: Non-Question Embedding (N.Q.E), only Question Embedding (Q.E.), and Hierarchical Question Embedding (H.Q.E.), as shown in Table  4 . In the case of N.Q.E, the entire interview sequence is treated as a single sequence for the depression detection model. Specifically, in this case, the input sequences are cropped from the beginning of the utterance to the end of the interview conversation. When only Q.E is applied, the interview is segmented into question-answer pairs. Notably, this procedure only divides the sequence into questions and aligns them with the respective question topics without incorporating hierarchical positional embedding. The improvement presented in Table  4  highlights that, for depression detection with clinical interviews, extracting useful information based on a question-driven approach is more effective than considering the entire interview sequence as a single sequence. In the last scenario H.Q.E, hierarchical position embedding is introduced following the question embedding procedure. To elaborate, after dividing the interview into question-answer pairs using question embedding, a hierarchical relationship (primary or follow-up) among the questions is tagged through hierarchical position embedding. As shown in Table  4 , our proposed hierarchical question embedding process effectively forces the model to capture hierarchical relationships and the importance of the questions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Model Components",
      "text": "To assess the effectiveness of each layer in the HiQuE, we conducted an ablation study on the Question-Aware Module Layer and the Cross-Modal Attention Layer. As shown in Table  4 , without \"Q-A Module Layer\", each audio, visual, and text representation undergoes hierarchical question embedding and feature extraction processes before entering the Cross-Modal Attention Layer, which incorporates bidirectional cross-attention (e.g.,    4 , the setting without \"Q-A Module Layer\" achieves a higher macro average F1-score and weighted average F1-score compared to the setting without \"C-M Attention Layer\". This reveals that in detecting depression, it is more important to learn relevant information and interactions between modalities than to analyze the relationships and importance of each question. Furthermore, Table  4  also presents the performances of our proposed model with and without data augmentation. The results confirm that data augmentation enhances performance by balancing the sizes of depression and non-depression cases in the training set.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Case Study",
      "text": "In this section, we present a case study on samples from our test set to assess the effectiveness of the decision-making process of HiQuE. Specifically, we examine the verbal (i.e., text) and non-verbal (i.e., audio and video) signals for the two cases: a depressed individual (381P) and a non-depressed individual (470P). Our analysis focuses on the distinct attributes of audio, text, and visual attention scores for each individual. For a fair comparison, we apply a normalization technique to the amplitude and time of the audio waves, allowing for unbiased and consistent analysis and comparison.\n\nFigure  6  showcases how the model integrates text, audio, and visual features during the decision-making process for each questionnaire response. In the case of the primary question \"What's one of your most memorable experiences?\", we observe that the depressed individual faces difficulties in providing a prompt response. He/she exhibits hesitation while reflecting on memorable experiences and ultimately struggles to provide a specific answer. In contrast, the non-depressed individual is more likely to respond immediately and accurately.\n\nBy analyzing audio, visual, and text attention scores to the followup question \"Can you tell me about that?\", we observe the comprehensive exploration and understanding of various responses exhibited by HiQuE in detecting depression. In the case of a nondepressed individual (470P), detailed explanations, expressions of excitement, and smiling faces are evident in the answer to the follow-up question. Note that HiQuE also gives the highest attention score 0.6 to visual features. Furthermore, apart from <laughter>, the audio waves display symmetrical patterns without irregular fluctuations, indicating a more wide range of tones and amplitudes. On the other hand, the depressed individual (381P) encounters difficulties recalling memorable experiences when responding to the follow-up question. Instead of positive recollections, this individual shares memories of regrettable past incidents. By examining the audio wave of the depressed individual, we observe unstable fluctuations in amplitude while his/her facial expressions remain neutral. For this reason, HiQuE assigns the highest attention score of 0.5 to text features, followed by attention scores of 0.3-0.4 for audio features.\n\nOur analysis of the attention scores for each modality during the model's diagnostic process demonstrates that HiQuE effectively incorporates the interview structure through its hierarchical question embedding layer. The case study provides further evidence that HiQuE successfully captures the sequential information of all questions and maximizes the mutual information between modalities by leveraging the question-aware module and the cross-modal attention layer.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we presented HiQuE, a novel hierarchical question embedding model for multimodal depression detection. HiQuE efficiently captures the hierarchical structure of questions in clinical interviews and explores the correlations between different modalities to extract valuable information for depression detection. Through a comprehensive case study, we confirmed that the HiQuE focuses on questions specifically related to depression and makes its final decision by utilizing attention scores. This approach allows the model to mimic the expertise of clinical professionals during clinical interviews, where the interaction of questionnaire responses plays a crucial role. Given HiQuE's demonstrated generalizability to unseen questions, future plans involve extending its applicability to additional speech-related tasks and exploring the advantages of hierarchical question embedding further. who's someone that's been a positive influence in your life Primary  (10)  how would your best friend describe you Primary  (11)  what are some things you don't really like about l_a Primary  (12)  what did you study at school Primary  (13)  is there anything you regret Primary  (14)  what's your dream job Primary  (15)  what do you enjoy about traveling Primary  (16)  what are you like when you don't sleep well Primary  (17)  what's one of your most memorable experiences Primary  (18)  tell me about the hardest decision you've ever had to make Primary  (19)  what are some things you like to do for fun Primary  (20)  tell me about a situation that you wish you had handled differently Primary  (21)  tell me about an event or something that you wish you could erase from your memory Primary  (22)  why did you move to l_a Primary  (23)  what are some things you wish you could change about yourself Primary  (24)  what would you say are some of your best qualities Primary  (25)  how often do you go back to your home town Primary  (26)  how long ago were you diagnosed Primary  (27)  what's something you feel guilty about Primary  (28)  when did you move to l_a Primary  (29)  how easy was it for you to get used to living in l_a Primary  (30)  when was the last time you felt really happy Primary  (31)  what's the hardest thing about being a parent Primary  (32)  do you still go to therapy now Primary  (33)  do you travel a lot Primary  (34)  have you ever served in the military Primary  (35)  when was the last time that happened Primary  (36)  what's the best thing about being a parent Primary  (37)  what are some things that make you really mad Primary  (38)  do you find it easy to be a parent Primary  (39)  what do you do now Primary  (40)  what were your symptoms Primary  (41)  tell me how you spend your ideal weekend Primary  (42)  what do you do when you are annoyed Primary  (43)  tell me about your kids Primary  (44)  tell me about a time when someone made you feel really badly about yourself Primary  (45)  what are some ways that you're different as a parent than your parents Primary  (46)  what do you think of today's kids Primary  (47)  do you feel down Primary  (48)  how do you like your living situation Primary  (49)  how are you doing today Primary  (50)  do you have roommates Primary  (51)  do you think that maybe you're being a little hard on yourself Primary  (52)  do you have disturbing thoughts Primary  (53)  where do you live Primary  (54)  what did you do after the military Primary  (55)  did you ever see combat Primary  (56)  why don't we talk about that later Primary  (57)  how did serving in the military change you Primary  (58)  have you noticed any changes in your behavior or thoughts lately Primary  (59)  have you been diagnosed with depression Primary  (60)  how easy is it for you to get a good night sleep Primary  (61)  how close are you to your family Primary  (62)  how have you been feeling lately Primary  (63)  do you consider yourself an introvert Primary  (64)  have you ever been diagnosed with p_t_s_d Primary  (65)  do you feel like therapy is useful Primary  (66)  what do you do to relax Primary  (67)  can you tell me about that Follow-up  (68)  why Follow-up  (69)  how hard is that Follow-up  (70)  what made you decide to do that Follow-up  (71)  are you still doing that Follow-up  (72)  what got you to seek help Follow-up  (73)  how do you cope with them Follow-up  (74)  how does it compare to l_a Follow-up  (75)  are you okay with this Follow-up  (76)  are they triggered by something Follow-up  (77)  are you happy you did that Follow-up  (78)  could you have done anything to avoid it Follow-up  (79)  has that gotten you in trouble Follow-up  (80)  how do you know them Follow-up  (81)  do you feel that way often Follow-up  (82)  did you think you had a problem before you found out Follow-up  (83)  why did you stop Follow-up  (84)  what's it like for you living with them Follow-up  (85)  can you give me an example of that Follow-up\n\n[8] Tadas Baltrusaitis, Peter Robinson, and Louis-Philippe Morency. 2013. Constrained local neural fields for robust facial landmark detection in the wild. In Proceedings of the IEEE international conference on computer vision workshops. 354-361.",
      "page_start": 9,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Previous research focused on learning the whole",
      "page": 2
    },
    {
      "caption": "Figure 2: An overall architecture of the HiQuEâ€™s multimodal depression detection process, where HIQ and Q-A indicate",
      "page": 3
    },
    {
      "caption": "Figure 3: Hierarchical Question Embedding Process.",
      "page": 3
    },
    {
      "caption": "Figure 2: , consists",
      "page": 3
    },
    {
      "caption": "Figure 3: , interviewer Ellieâ€™s questions are cat-",
      "page": 4
    },
    {
      "caption": "Figure 3: Interview sequences are represented as unimodal",
      "page": 4
    },
    {
      "caption": "Figure 3: , we assign",
      "page": 4
    },
    {
      "caption": "Figure 2: , a transformer encoder with â„multi-heads is utilized",
      "page": 4
    },
    {
      "caption": "Figure 2: , the multimodal transformer encoder with â„multi-",
      "page": 4
    },
    {
      "caption": "Figure 4: Performance comparisons between unimodal and",
      "page": 6
    },
    {
      "caption": "Figure 5: Distributions of attention scores across different",
      "page": 7
    },
    {
      "caption": "Figure 5: , we highlight the significance of",
      "page": 7
    },
    {
      "caption": "Figure 5: ), or when there",
      "page": 7
    },
    {
      "caption": "Figure 5: , the text attention score predominantly",
      "page": 7
    },
    {
      "caption": "Figure 6: A case analysis of a depression case (381P) and a non-depression case (470P). Specific audio, video, and text responses",
      "page": 8
    },
    {
      "caption": "Figure 2: Given that the Question-Aware",
      "page": 8
    },
    {
      "caption": "Figure 6: showcases how the model integrates text, audio, and",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "TFN [90]\nBiLSTM-1DCNN [41]\nMulT [73]\nMISA [35]\nD-vlog [88]",
          "Approach": "Modality-Aware\nModality-Aware\nModality-Aware\nModality-Aware\nModality-Aware",
          "Precision": "0.67\n0.65\n0.73\n0.74\n0.73",
          "Recall": "0.73\n0.61\n0.74\n0.77\n0.72",
          "F1-Score": "0.68\n0.62\n0.74\n0.74\n0.73",
          "WA* Prec. (â†‘)": "0.84\n0.77\n0.81\n0.86\n0.82",
          "WA* Rec. (â†‘)": "0.78\n0.71\n0.77\n0.77\n0.76",
          "WA* F1 (â†‘)": "0.81\n0.73\n0.77\n0.79\n0.77",
          "G-Mean (â†‘)": "0.699\n0.630\n0.735\n0.755\n0.725"
        },
        {
          "Method": "bc-LSTM [55]\nEmotion Recognition [64]\nSequence Modeling [2]\nTopic Modeling [27]\nContext Aware [37]\nSpeechformer [11]\nGRU/BiLSTM-based [65]",
          "Approach": "Context-Aware\nContext-Aware\nContext-Aware\nContext-Aware\nContext-Aware\nContext-Aware\nContext-Aware",
          "Precision": "0.59\n0.65\n0.67\n0.63\n0.71\n0.70\n0.75",
          "Recall": "0.60\n0.69\n0.71\n0.60\n0.71\n0.72\n0.78",
          "F1-Score": "0.59\n0.66\n0.70\n0.62\n0.71\n0.70\n0.75",
          "WA* Prec. (â†‘)": "0.77\n0.69\n0.85\n0.81\n0.85\n0.78\n0.86",
          "WA* Rec. (â†‘)": "0.69\n0.70\n0.73\n0.71\n0.73\n0.76\n0.77",
          "WA* F1 (â†‘)": "0.72\n0.71\n0.77\n0.74\n0.77\n0.76\n0.80",
          "G-Mean (â†‘)": "0.595\n0.670\n0.690\n0.615\n0.710\n0.710\n0.765"
        },
        {
          "Method": "HiQuE",
          "Approach": "Modality + Context",
          "Precision": "0.78",
          "Recall": "0.80",
          "F1-Score": "0.79",
          "WA* Prec. (â†‘)": "0.85",
          "WA* Rec. (â†‘)": "0.80",
          "WA* F1 (â†‘)": "0.82",
          "G-Mean (â†‘)": "0.790"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "E-DAIC-WOZ [58]": "Depression Detection",
          "MIT Interview dataset [50]": "Stress Level Prediction"
        },
        {
          "Methods": "",
          "E-DAIC-WOZ [58]": "Overall",
          "MIT Interview dataset [50]": "Overall"
        },
        {
          "Methods": "",
          "E-DAIC-WOZ [58]": "Pre.",
          "MIT Interview dataset [50]": "Pre."
        },
        {
          "Methods": "GRU/BiLSTM-based [65]\nD-vlog [88]\nMISA [35]\nMulT [35]",
          "E-DAIC-WOZ [58]": "0.67\n0.65\n0.62\n0.64",
          "MIT Interview dataset [50]": "0.70\n0.71\n0.69\n0.70"
        },
        {
          "Methods": "HiQuE",
          "E-DAIC-WOZ [58]": "0.71",
          "MIT Interview dataset [50]": "0.75"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: , our proposed hierarchical question embedding process",
      "data": [
        {
          "Ablation Settings": "Question Embedding",
          "Precision": "",
          "Recall": "",
          "F1-Score": "",
          "WA* F1 (â†‘)": ""
        },
        {
          "Ablation Settings": "Q.E.",
          "Precision": "",
          "Recall": "",
          "F1-Score": "",
          "WA* F1 (â†‘)": ""
        },
        {
          "Ablation Settings": "âœ—\nâœ“\nâœ“\nâœ“\nâœ“\nâœ“",
          "Precision": "0.74\n0.75\n0.73\n0.73\n0.75\n0.76",
          "Recall": "0.73\n0.76\n0.72\n0.75\n0.76\n0.77",
          "F1-Score": "0.73\n0.75\n0.72\n0.74\n0.76\n0.76",
          "WA* F1 (â†‘)": "0.75\n0.77\n0.74\n0.77\n0.79\n0.80"
        },
        {
          "Ablation Settings": "âœ“",
          "Precision": "0.78",
          "Recall": "0.80",
          "F1-Score": "0.79",
          "WA* F1 (â†‘)": "0.82"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Tensorflow: a system for large-scale machine learning",
      "authors": [
        "MartÃ­n Abadi",
        "Paul Barham",
        "Jianmin Chen",
        "Zhifeng Chen",
        "Andy Davis",
        "Jeffrey Dean",
        "Matthieu Devin",
        "Sanjay Ghemawat",
        "Geoffrey Irving",
        "Michael Isard"
      ],
      "year": "2016",
      "venue": "Osdi"
    },
    {
      "citation_id": "2",
      "title": "Detecting Depression with Audio/Text Sequence Modeling of Interviews",
      "authors": [
        "Al Tuka",
        "Mohammad Hanai",
        "James Ghassemi",
        "Glass"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "3",
      "title": "In an absolute state: Elevated use of absolutist words is a marker specific to anxiety, depression, and suicidal ideation",
      "authors": [
        "Mohammed Al",
        "Tom Johnstone"
      ],
      "year": "2018",
      "venue": "Clinical Psychological Science"
    },
    {
      "citation_id": "4",
      "title": "Multimodal depression detection: fusion analysis of paralinguistic, head pose and eye gaze behaviors",
      "authors": [
        "Sharifa Alghowinem",
        "Roland Goecke",
        "Michael Wagner",
        "Julien Epps",
        "Matthew Hyett",
        "Gordon Parker",
        "Michael Breakspear"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Head pose and movement analysis as an indicator of depression",
      "authors": [
        "Sharifa Alghowinem",
        "Roland Goecke",
        "Michael Wagner",
        "Gordon Parkerx",
        "Michael Breakspear"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "6",
      "title": "Reflections of depression in acoustic measures of the patient's speech",
      "authors": [
        "Murray Alpert",
        "Enrique Pouget",
        "Raul Silva"
      ],
      "year": "2001",
      "venue": "Journal of affective disorders"
    },
    {
      "citation_id": "7",
      "title": "Gender bias in depression detection using audio features",
      "authors": [
        "Andrew Bailey",
        "Mark Plumbley"
      ],
      "year": "2021",
      "venue": "2021 29th European Signal Processing Conference (EUSIPCO)"
    },
    {
      "citation_id": "8",
      "title": "Voice acoustical measurement of the severity of major depression",
      "authors": [
        "Michael Cannizzaro",
        "Brian Harel",
        "Nicole Reilly",
        "Phillip Chappell",
        "Peter Snyder"
      ],
      "year": "2004",
      "venue": "Brain and cognition"
    },
    {
      "citation_id": "9",
      "title": "Context-aware interactive attention for multi-modal sentiment and emotion analysis",
      "authors": [
        "Dushyant Singh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu",
        "Jianxin Pang",
        "Lan Du"
      ],
      "year": "2022",
      "venue": "Speechformer: A hierarchical efficient framework incorporating the characteristics of speech",
      "arxiv": "arXiv:2203.03812"
    },
    {
      "citation_id": "11",
      "title": "A textual-based featuring approach for depression detection using machine learning classifiers and social media texts",
      "authors": [
        "Raymond Chiong",
        "Gregorius Satia Budhi",
        "Sandeep Dhakal",
        "Fabian Chiong"
      ],
      "year": "2021",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "12",
      "title": "Diagnosis of depression by behavioural signals: a multimodal approach",
      "authors": [
        "Nicholas Cummins",
        "Jyoti Joshi",
        "Abhinav Dhall",
        "Vidhyasaharan Sethu",
        "Roland Goecke",
        "Julien Epps"
      ],
      "year": "2013",
      "venue": "Proceedings of the 3rd ACM"
    },
    {
      "citation_id": "13",
      "title": "A review of depression and suicide risk assessment using speech analysis",
      "authors": [
        "Nicholas Cummins",
        "Stefan Scherer",
        "Jarek Krajewski",
        "Sebastian Schnieder",
        "Julien Epps",
        "Thomas Quatieri"
      ],
      "year": "2015",
      "venue": "Speech communication"
    },
    {
      "citation_id": "14",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "15",
      "title": "Depression scale recognition from audio, visual and text analysis",
      "authors": [
        "Shubham Dham",
        "Anirudh Sharma",
        "Abhinav Dhall"
      ],
      "year": "2017",
      "venue": "Depression scale recognition from audio, visual and text analysis",
      "arxiv": "arXiv:1709.05865"
    },
    {
      "citation_id": "16",
      "title": "Dynamic multimodal measurement of depression severity using deep autoencoding",
      "authors": [
        "Hamdi DibeklioÄŸlu",
        "Zakia Hammal",
        "Jeffrey Cohn"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "17",
      "title": "Multimodal detection of depression in clinical interviews",
      "authors": [
        "Hamdi DibeklioÄŸlu",
        "Zakia Hammal",
        "Ying Yang",
        "Jeffrey Cohn"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on international conference on multimodal interaction"
    },
    {
      "citation_id": "18",
      "title": "A hierarchical depression detection model based on vocal and emotional cues",
      "authors": [
        "Yizhuo Dong",
        "Xinyu Yang"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "19",
      "title": "Vocal indicators of mood change in depression",
      "authors": [
        "Heiner Ellgring",
        "Klaus Scherer"
      ],
      "year": "1996",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "20",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "BjÃ¶rn Schuller",
        "Johan Sundberg",
        "Elisabeth AndrÃ©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "A multimodal fusion model with multi-level attention mechanism for depression detection",
      "authors": [
        "Ming Fang",
        "Siyu Peng",
        "Yujia Liang",
        "Chih-Cheng Hung",
        "Shuhua Liu"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "23",
      "title": "Transfer learning for depression screening from follow-up clinical interview questions",
      "authors": [
        "Ricardo Flores",
        "Ermal Tlachac",
        "Elke Toto",
        "Rundensteiner"
      ],
      "year": "2022",
      "venue": "Deep Learning Applications"
    },
    {
      "citation_id": "24",
      "title": "Depression screening using deep learning on follow-up questions in clinical interviews",
      "authors": [
        "Ricardo Flores",
        "Ermal Tlachac",
        "Elke Toto",
        "Rundensteiner"
      ],
      "year": "2021",
      "venue": "2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)"
    },
    {
      "citation_id": "25",
      "title": "Contextual inter-modal attention for multi-modal sentiment analysis",
      "authors": [
        "Deepanway Ghosal",
        "Shad Md",
        "Dushyant Akhtar",
        "Chauhan"
      ],
      "year": "2018",
      "venue": "proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "26",
      "title": "Topic modeling based multi-modal depression detection",
      "authors": [
        "Yuan Gong",
        "Christian Poellabauer"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "27",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "Jonathan Gratch",
        "Ron Artstein",
        "Gale Lucas",
        "Giota Stratou",
        "Stefan Scherer",
        "Angela Nazarian",
        "Rachel Wood",
        "Jill Boberg",
        "David Devault",
        "Stacy Marsella"
      ],
      "year": "2014",
      "venue": "The distress analysis interview corpus of human and computer interviews"
    },
    {
      "citation_id": "28",
      "title": "The Clinical Interview for Depression: a comprehensive review of studies and clinimetric properties",
      "authors": [
        "Jenny Guidi",
        "Giovanni Fava",
        "Per Bech",
        "Eugene Paykel"
      ],
      "year": "2010",
      "venue": "Psychotherapy and Psychosomatics"
    },
    {
      "citation_id": "29",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "Anmol Gulati",
        "James Qin",
        "Chung-Cheng Chiu",
        "Niki Parmar",
        "Yu Zhang",
        "Jiahui Yu",
        "Wei Han",
        "Shibo Wang",
        "Zhengdong Zhang",
        "Yonghui Wu"
      ],
      "year": "2020",
      "venue": "Conformer: Convolution-augmented transformer for speech recognition",
      "arxiv": "arXiv:2005.08100"
    },
    {
      "citation_id": "30",
      "title": "Automatic depression detection via learning and fusing features from visual cues",
      "authors": [
        "Yanrong Guo",
        "Chenyang Zhu",
        "Shijie Hao",
        "Richang Hong"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "31",
      "title": "What reveals about depression level? The role of multimodal features at the level of interview questions",
      "authors": [
        "Shan Guohou",
        "Lina Zhou",
        "Zhang Dongsong"
      ],
      "year": "2020",
      "venue": "Information & Management"
    },
    {
      "citation_id": "32",
      "title": "Multimodal prediction of affective dimensions and depression in humancomputer interactions",
      "authors": [
        "Rahul Gupta",
        "Nikolaos Malandrakis",
        "Bo Xiao",
        "Tanaya Guha",
        "Maarten Van Segbroeck",
        "Matthew Black",
        "Alexandros Potamianos",
        "Shrikanth Narayanan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th"
    },
    {
      "citation_id": "33",
      "title": "Humor knowledge enriched transformer for understanding multimodal humor",
      "authors": [
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Wasifur Rahman",
        "Amir Zadeh",
        "Rada Mihalcea",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Manic-depressive insanity and paranoia",
      "authors": [
        "Emil Kraepelin"
      ],
      "year": "1921",
      "venue": "Manic-depressive insanity and paranoia"
    },
    {
      "citation_id": "36",
      "title": "Context-aware deep learning for multi-modal depression detection",
      "authors": [
        "Genevieve Lam",
        "Huang Dongyan",
        "Weisi Lin"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "37",
      "title": "Learning Co-Speech Gesture for Multimodal Aphasia Type Detection",
      "authors": [
        "Daeun Lee",
        "Sejung Son",
        "Hyolim Jeon",
        "Seungbae Kim",
        "Jinyoung Han"
      ],
      "year": "2023",
      "venue": "Learning Co-Speech Gesture for Multimodal Aphasia Type Detection",
      "arxiv": "arXiv:2310.11710"
    },
    {
      "citation_id": "38",
      "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Ves Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "arxiv": "arXiv:1910.13461"
    },
    {
      "citation_id": "39",
      "title": "Eye movement indices in the study of depressive disorder. Shanghai Archives of",
      "authors": [
        "Yu Li",
        "Yangyang Xu",
        "Mengqing Xia",
        "Tianhong Zhang",
        "Junjie Wang",
        "Xu Liu",
        "Yongguang He",
        "Jijun Wang"
      ],
      "year": "2016",
      "venue": "Psychiatry"
    },
    {
      "citation_id": "40",
      "title": "Towards automatic depression detection: A BiLSTM/1D CNN-based model",
      "authors": [
        "Lin Lin",
        "Xuri Chen",
        "Ying Shen",
        "Lin Zhang"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "41",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "42",
      "title": "Depaudionet: An efficient deep model for audio based depression classification",
      "authors": [
        "Xingchen Ma",
        "Hongyu Yang",
        "Qiang Chen",
        "Di Huang",
        "Yunhong Wang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on audio/visual emotion challenge"
    },
    {
      "citation_id": "43",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "44",
      "title": "A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews",
      "authors": [
        "Adria Mallol-Ragolta",
        "Ziping Zhao",
        "Lukas Stappen",
        "Nicholas Cummins",
        "BjÃ¶rn Schuller"
      ],
      "year": "2019",
      "venue": "Interspeech",
      "doi": "10.21437/Interspeech.2019-2036"
    },
    {
      "citation_id": "45",
      "title": "Clinician-identified depression in community settings: concordance with structured-interview diagnoses",
      "authors": [
        "Ramin Mojtabai"
      ],
      "year": "2013",
      "venue": "Psychotherapy and psychosomatics"
    },
    {
      "citation_id": "46",
      "title": "A cross-modal review of indicators for depression detection systems",
      "authors": [
        "Michelle Morales",
        "Stefan Scherer",
        "Rivka Levitan"
      ],
      "year": "2017",
      "venue": "Proceedings of the fourth workshop on computational linguistics and clinical psychology-From linguistic signal to clinical reality"
    },
    {
      "citation_id": "47",
      "title": "Speech vs. text: A comparative analysis of features for depression detection systems",
      "authors": [
        "Renee Morales",
        "Rivka Levitan"
      ],
      "year": "2016",
      "venue": "2016 IEEE spoken language technology workshop (SLT)"
    },
    {
      "citation_id": "48",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on multimodal interfaces"
    },
    {
      "citation_id": "49",
      "title": "Automated analysis and prediction of job interview performance",
      "authors": [
        "Iftekhar Naim",
        "Iftekhar Md",
        "Daniel Tanveer",
        "Mohammed Gildea",
        "Hoque"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "50",
      "title": "A review on depression detection and diagnoses based on visual facial cues",
      "authors": [
        "Sana Nasser",
        "Ivan Hashim",
        "Wisam H Ali"
      ],
      "year": "2020",
      "venue": "2020 3rd International Conference on Engineering Technology and its Applications (IICETA)"
    },
    {
      "citation_id": "51",
      "title": "Hcag: A hierarchical context-aware graph attention model for depression detection",
      "authors": [
        "Meng Niu",
        "Kai Chen",
        "Qingcai Chen",
        "Lufeng Yang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "52",
      "title": "The Clinical Interview for Depression: development, reliability and validity",
      "authors": [
        "Paykel"
      ],
      "year": "1985",
      "venue": "Journal of affective disorders"
    },
    {
      "citation_id": "53",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "54",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "55",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "56",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "57",
      "title": "AVEC 2019 workshop and challenge: state-of-mind, detecting depression with AI, and cross-cultural affect recognition",
      "authors": [
        "Fabien Ringeval",
        "BjÃ¶rn Schuller",
        "Michel Valstar",
        "Nicholas Cummins",
        "Roddy Cowie",
        "Leili Tavabi",
        "Maximilian Schmitt",
        "Sina Alisamir",
        "Shahin Amiriparian",
        "Eva-Maria Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "58",
      "title": "Detecting Depression with Word-Level Multimodal Fusion",
      "authors": [
        "Morteza Rohanian",
        "Julian Hough",
        "Matthew Purver"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "59",
      "title": "Code llama: Open foundation models for code",
      "authors": [
        "Jonas Baptiste Roziere",
        "Fabian Gehring",
        "Sten Gloeckle",
        "Itai Sootla",
        "Gat",
        "Ellen Xiaoqing",
        "Yossi Tan",
        "Jingyu Adi",
        "Tal Liu",
        "JÃ©rÃ©my Remez",
        "Rapin"
      ],
      "year": "2023",
      "venue": "Code llama: Open foundation models for code",
      "arxiv": "arXiv:2308.12950"
    },
    {
      "citation_id": "60",
      "title": "Language use of depressed and depression-vulnerable college students",
      "authors": [
        "Stephanie Rude",
        "Eva-Maria Gortner",
        "James Pennebaker"
      ],
      "year": "2004",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "61",
      "title": "DepressNet: A Multimodal Hierarchical Attention Mechanism approach for Depression Detection",
      "authors": [
        "Guramritpal Singh Saggu",
        "Keshav Gupta",
        "Ciro Kv Arya",
        "Rodriguez"
      ],
      "year": "2022",
      "venue": "Int. J. Eng. Sci"
    },
    {
      "citation_id": "62",
      "title": "Audio based depression detection using Convolutional Autoencoder",
      "authors": [
        "Sara Sardari",
        "Bahareh Nakisa",
        "Mohammed Naim Rastgoo",
        "Peter Eklund"
      ],
      "year": "2022",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "63",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "64",
      "title": "Automatic depression detection: An emotional audio-textual corpus and a gru/bilstm-based model",
      "authors": [
        "Ying Shen",
        "Huiyu Yang",
        "Lin Lin"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "65",
      "title": "The diagnosis of depression: current and emerging methods",
      "authors": [
        "Katie M Smith",
        "Perry Renshaw",
        "John Bilello"
      ],
      "year": "2013",
      "venue": "Comprehensive psychiatry"
    },
    {
      "citation_id": "66",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "Mohammad Soleymani",
        "David Garcia",
        "Brendan Jou",
        "BjÃ¶rn Schuller",
        "Shih-Fu Chang",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "67",
      "title": "Comparison of self-report and structured clinical interview in the identification of depression",
      "authors": [
        "Amanda L Stuart",
        "Julie Pasco",
        "Felice Jacka",
        "Sharon Brennan",
        "Michael Berk",
        "Lana Williams"
      ],
      "year": "2014",
      "venue": "Comprehensive psychiatry"
    },
    {
      "citation_id": "68",
      "title": "Automatic detection of depression in speech using gaussian mixture modeling with factor analysis",
      "authors": [
        "Douglas Sturim",
        "Pedro Torres-Carrasquillo",
        "Thomas Quatieri",
        "Nicolas Malyska",
        "Alan Mccree"
      ],
      "year": "2011",
      "venue": "Twelfth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "69",
      "title": "A random forest regression method with selected-text feature for depression assessment",
      "authors": [
        "Bo Sun",
        "Yinghui Zhang",
        "Jun He",
        "Lejun Yu",
        "Qihua Xu",
        "Dongliang Li",
        "Zhaoying Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the 7th annual workshop on Audio/Visual emotion challenge"
    },
    {
      "citation_id": "70",
      "title": "Assessing depression in youth: relation between the Children's Depression Inventory and a structured interview",
      "authors": [
        "Benedikte Timbremont",
        "Caroline Braet",
        "Laura Dreessen"
      ],
      "year": "2004",
      "venue": "Journal of Clinical Child and Adolescent Psychology"
    },
    {
      "citation_id": "71",
      "title": "Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava"
      ],
      "year": "2023",
      "venue": "Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "72",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "73",
      "title": "Avec 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "Michel Valstar",
        "Jonathan Gratch",
        "BjÃ¶rn Schuller",
        "Fabien Ringeval",
        "Denis Lalanne",
        "Mercedes Torres",
        "Stefan Scherer",
        "Giota Stratou",
        "Roddy Cowie",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th international workshop on"
    },
    {
      "citation_id": "74",
      "title": "Examination of the sensitivity of acoustic-phonetic parameters of speech to depression",
      "authors": [
        "KlÃ¡ra Vicsi",
        "DÃ¡vid SztahÃ³",
        "GÃ¡bor Kiss"
      ],
      "year": "2012",
      "venue": "2012 IEEE 3rd International Conference on Cognitive Infocommunications (CogInfoCom)"
    },
    {
      "citation_id": "75",
      "title": "Acoustic differences between healthy and depressed people: a cross-situation study",
      "authors": [
        "Jingying Wang",
        "Lei Zhang",
        "Tianli Liu",
        "Wei Pan",
        "Bin Hu",
        "Tingshao Zhu"
      ],
      "year": "2019",
      "venue": "BMC psychiatry"
    },
    {
      "citation_id": "76",
      "title": "Transformer-based acoustic modeling for hybrid speech recognition",
      "authors": [
        "Yongqiang Wang",
        "Abdelrahman Mohamed",
        "Due Le",
        "Chunxi Liu",
        "Alex Xiao",
        "Jay Mahadeokar",
        "Hongzhao Huang",
        "Andros Tjandra",
        "Xiaohui Zhang",
        "Frank Zhang"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "77",
      "title": "Nonverbal cues for depression",
      "authors": [
        "Peter Waxer"
      ],
      "year": "1974",
      "venue": "Journal of Abnormal Psychology"
    },
    {
      "citation_id": "78",
      "title": "Articulation rate in psychotherapeutic dialogues for depression: patients and therapists",
      "authors": [
        "Laurence White",
        "Hannah Grimes"
      ],
      "year": "2022",
      "venue": "depression"
    },
    {
      "citation_id": "79",
      "title": "Detecting depression using vocal, facial and semantic communication cues",
      "authors": [
        "Elizabeth James R Williamson",
        "Miriam Godoy",
        "Adrianne Cha",
        "Pooya Schwarzentruber",
        "Youngjune Khorrami",
        "Hsiang-Tsung Gwon",
        "Charlie Kung",
        "Thomas Dagli",
        "Quatieri"
      ],
      "year": "2016",
      "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "80",
      "title": "Affective conditioning on hierarchical networks applied to depression detection from transcribed clinical interviews",
      "authors": [
        "Georgios Danai Xezonaki",
        "Paraskevopoulos"
      ],
      "year": "2020",
      "venue": "Affective conditioning on hierarchical networks applied to depression detection from transcribed clinical interviews",
      "arxiv": "arXiv:2006.08336"
    },
    {
      "citation_id": "81",
      "title": "Review on automated depression detection from audio visual clue using sentiment analysis",
      "authors": [
        "Uma Yadav",
        "Ashish Sharma"
      ],
      "year": "2021",
      "venue": "2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC)"
    },
    {
      "citation_id": "82",
      "title": "A novel automated depression detection technique using text transcript",
      "authors": [
        "Uma Yadav",
        "Ashish Sharma"
      ],
      "year": "2023",
      "venue": "International Journal of Imaging Systems and Technology"
    },
    {
      "citation_id": "83",
      "title": "Mtag: Modal-temporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "Jianing Yang",
        "Yongxin Wang",
        "Ruitao Yi",
        "Yuying Zhu",
        "Azaan Rehman",
        "Amir Zadeh",
        "Soujanya Poria",
        "Louis-Philippe Morency"
      ],
      "year": "2020",
      "venue": "Mtag: Modal-temporal attention graph for unaligned human multimodal language sequences",
      "arxiv": "arXiv:2010.11985"
    },
    {
      "citation_id": "84",
      "title": "Detecting depression severity from vocal prosody",
      "authors": [
        "Ying Yang",
        "Catherine Fairbairn",
        "Jeffrey Cohn"
      ],
      "year": "2012",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "85",
      "title": "Multi-modal depression detection based on emotional audio and evaluation text",
      "authors": [
        "Jiayu Ye",
        "Yanhong Yu",
        "Qingxiang Wang",
        "Wentao Li",
        "Hu Liang",
        "Yunshao Zheng",
        "Gang Fu"
      ],
      "year": "2021",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "86",
      "title": "A multi-modal hierarchical recurrent neural network for depression detection",
      "authors": [
        "Cong Shi Yin",
        "Heyan Liang",
        "Shangfei Ding",
        "Wang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "87",
      "title": "D-vlog: Multimodal Vlog Dataset for Depression Detection",
      "authors": [
        "Jeewoo Yoon",
        "Chaewon Kang",
        "Seungbae Kim",
        "Jinyoung Han"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "88",
      "title": "Research on the clinical interview",
      "authors": [
        "Gerald Young",
        "John D O' Brien",
        "Elane Gutterman",
        "Patricia Cohen"
      ],
      "year": "1987",
      "venue": "Journal of the American Academy of Child & Adolescent Psychiatry"
    },
    {
      "citation_id": "89",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "90",
      "title": "Depa: Selfsupervised audio embedding for depression detection",
      "authors": [
        "Pingyue Zhang",
        "Mengyue Wu",
        "Heinrich Dinkel",
        "Kai Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "91",
      "title": "Hierarchical Convolutional Attention Network for Depression Detection on Social Media and Its Impact During Pandemic",
      "authors": [
        "Imran Hamad Zogan",
        "Shoaib Razzak",
        "Guandong Jameel",
        "Xu"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    }
  ]
}