{
  "paper_id": "2505.23018v3",
  "title": "Emotiontalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations",
  "published": "2025-05-29T02:56:08Z",
  "authors": [
    "Haoqin Sun",
    "Xuechen Wang",
    "Jinghua Zhao",
    "Shiwan Zhao",
    "Jiaming Zhou",
    "Hui Wang",
    "Jiabei He",
    "Aobo Kong",
    "Xi Yang",
    "Yequan Wang",
    "Yonghua Lin",
    "Yong Qin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In recent years, emotion recognition plays a critical role in applications such as human-computer interaction, mental health monitoring, and sentiment analysis. While datasets for emotion analysis in languages such as English have proliferated, there remains a pressing need for high-quality, comprehensive datasets tailored to the unique linguistic, cultural, and multimodal characteristics of Chinese. In this work, we propose EmotionTalk, an interactive Chinese multimodal emotion dataset with rich annotations. This dataset provides multimodal information from 19 actors participating in dyadic conversational settings, incorporating acoustic, visual, and textual modalities. It includes 23.6 hours of speech (19,250 utterances), annotations for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative, neutral, weakly positive, and positive) and 4-dimensional speech captions (speaker, speaking style, emotion and overall). The dataset is well-suited for research on unimodal and multimodal emotion recognition, missing modality challenges, and speech captioning tasks. To our knowledge, it represents the first high-quality and versatile Chinese dialogue multimodal emotion dataset, which is a valuable contribution to research on cross-cultural emotion analysis and recognition. Additionally, we conduct experiments on EmotionTalk to demonstrate the effectiveness and quality of the dataset. It will be open-source and freely available for all academic purposes. The dataset and codes will be made available at: https://github.com/NKU-HLT/EmotionTalk.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition (MMER) has become a key focus in artificial intelligence, integrating speech, vision, and text to capture the complexity of human emotions. It drives advancements in applications like virtual assistants, online education, and mental health monitoring. However, most research relies on English datasets, with Chinese resources remaining scarce. Existing datasets often face issues such as low quality, limited scale, and incomplete modalities, hindering model performance. Therefore, the development of a high-quality Chinese multimodal emotion recognition dataset is of critical importance to advance research in this field.\n\nTraditional emotion recognition tasks include unimodal / multimodal emotion recognition on isolated utterances  [28, 29, 44, 45]  and conversational emotion recognition  [42, 43] . The former relies on a single modality or integrates multimodal information for emotion recognition. For example, MISA  [16] utilizes modality-invariant and modality-specific representations to fuse multimodal information. FDMER  [57]  extends MISA by incorporating tailored constraints and adversarial learning strategies to effectively capture multimodal information. The latter focuses on analyzing emotional changes by examining context and emotional evolution. DialogueRNN  [34]  extracts emotional information from conversations by modeling the speaker, context, and emotions within the dialogue. DialogueGCN  [13]  and MMGCN  [52]  leverage graph-based networks to model the dependencies within dialogues. As emotion recognition research continues to advance, researchers introduce new tasks such as emotion recognition in missing modality scenarios  [67, 66, 46]  and emotion caption  [54, 26] , driven by evolving applications and practical requirements. TATE  [66]  utilizes a Tag-Assisted Transformer Encoder network to guide the model in focusing on different missing cases by encoding specific tags for the missing modalities. As for the emotion captioning task, it is first proposed by SECap  [54] , which employs the SSL model and LLaMA to generate emotion descriptions.\n\nHowever, these studies use different datasets, and while they perform well in their respective experiments, directly comparing their performance remains challenging. This is mainly due to significant differences in dataset scale, annotation methods, modality combinations, and dialogue structures, which affect model applicability and generalization. For instance, popular multimodal benchmarks like IEMOCAP  [3] , MELD  [37] , CMU-MOSEI  [65] , and CH-SIMS  [61]  have been widely used but are primarily in English, with varying emotion category definitions and annotation standards, limiting cross-lingual and cross-cultural applicability. Specifically, IEMOCAP, one of the most widely used emotion recognition datasets, has only 5,531 usable samples, totaling around 7 hours of data. Meanwhile, the CH-SIMS dataset integrates multiple modalities but lacks clear emotional discrete label definitions and dialogue contexts. Additionally, in emotion captioning, most research relies on unpublished datasets, leading to a lack of a standardized, open benchmark, which hinders reproducibility and broader application.\n\nDue to the vast availability of resources on the internet, obtaining these resources has become relatively easy, and previous Chinese multimodal emotion datasets have largely relied on publicly available data. However, these datasets still have limitations in terms of quality and task coverage, and cannot fully meet the demands of multimodal emotion research. Therefore, constructing a high-quality, comprehensive emotion dataset is particularly important. Such a dataset would not only address the gaps in existing resources but also provide a unified benchmark for emotion-related tasks, contributing to the advancement of research in the field of affective computing. In this paper, we construct an large-scale interactive Chinese multimodal emotion dataset with fine-grained labels and emotional speaking style captions, EmotionTalk, in which the data are contributed by 19 professional actors, ensuring the naturalness and authenticity of the emotion expression. The dataset is in the form of dialogues, containing 23.6 hours of data and 19,250 utterances, along with corresponding labels that support various emotion tasks, including 7 discrete labels, 5 dimensional labels, and 4 caption labels. To the best of our knowledge, EmotionTalk is the first large-scale, comprehensive, recorded interactive Chinese multimodal emotion dataset. We further conduct experiments on unimodal emotion recognition, multimodal emotion recognition, and emotion caption tasks to validate the effectiveness and applicability of the constructed dataset. These experiments not only demonstrate the dataset's performance across different emotion tasks but also highlight its potential to support diverse model development and evaluation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Datasets",
      "text": "Table  1  presents the datasets which are commonly used in the field of multimodal emotion recognition, all of which consist of video, audio and text modalities.\n\nEnglish Datasets: The CMU-MOSEI  [65]  and MELD  [37]  datasets provide large-scale multimodal data sourced from YouTube and TV shows, covering tasks such as discrete emotion classification and continuous sentiment intensity prediction. These datasets are advantageous due to their rich emotional labeling, but they are primarily derived from entertainment content, where emotional expressions tend to be exaggerated. As such, they may not fully capture the natural emotional expressions encountered in real-life situations. In contrast, the CREMA-D  [5] , RAVDESS  [32] , IEMOCAP  [3]  and MSP-IMPROV  [4]  datasets are based on actor performances and emotion training, with IEMOCAP and MSP-IMPROV consist of conversational data, whereas CREMA-D and RAVDESS record non- dialogue data. These datasets offer higher-quality emotional data. However, due to their scripted nature, the limitations of the dialogue scripts can affect the actors' performances, leading to emotional expressions that may feel unnatural or overly theatrical.\n\nChinese Datasets: Currently, there have been some preliminary research efforts in the field of multimodal emotion datasets based on Mandarin For example, the CH-SIMS  [61]  and MER-MULTI  [24]  dataset use five continuous emotion labels and six discrete emotion labels respectively, making it suitable for multimodal sentiment analysis on isolated utterances spoken in Mandarin. However, both of them lack dialogue scenarios, overlooking the emotional changes multi-turn interactions. In contrast, datasets like M 3 ED  [68]  and MC-EIU ch  [27]  have made progress in terms of dialogue-level data, making it possible for supporting multimodal emotion recognition in conversations. Moreover, M 3 ED and MC-EIU ch have been significant progress regarding the scale of the data.\n\nOverall, the existing Mandarin multimodal emotion datasets still have gaps compared to the English datasets. Most of the Mandarin datasets focus on relatively simple emotion labels or data sourced from the internet, which limits both data quality and emotional annotation. As a result, the mentioned datasets struggle to support some specific emotion tasks, such as missing modality scenarios and emotion captioning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Methods",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Multimodal emotion recognition typically involves identifying the speaker's emotions from an isolated utterance or a dialogue. The methods of multimodal feature fusion play a crucial role in utterance-level emotion recognition. Yang et al.  [58]  propose a context representation module and a self-adaptive path selection module, obtaining final integrated multimodal features. Fan et al.  [11]  design an attention aggregation network and a auxiliary uni-modal classifier to align shared emotional information across modalities. Design an attention aggregation network and a auxiliary uni-modal classifier to align shared emotional information across modalities. Ma et al.  [33]  propose a transformer-based model with self-distillation for conversational emotion recognition. The framework could capture intra-and inter-modal interactions and obtain more expressive context-sensitive features. Hu et al.  [20]  propose a graph-based dynamic fusion network to reduce redundancy in multimodal interactions and model the context features in a conversation. In addition, since discrete emotion labels are unable to capture the complexity of emotional intensity, some work focuses more on continuous multimodal emotion recognition. For instance, Yang et al.  [59]  propose a multimodal emotion analysis framework based on contrastive learning guided by sentiment intensity. The effectiveness has been validated on the CMU-MOSEI and CH-SIMS datasets, which include labels related to emotional intensity.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Captioning",
      "text": "To break through the limitations of traditional emotion recognition and capture richer emotional information, the task of emotion captioning has gradually emerged. For example, Xu et al.  [54]  propose a speech emotion captioning framework, effectively discrbing speech emotions with the help of LLaMA  [48]  and HuBERT  [18] . Liang et al.  [25]  design a network named AlignCap to align speech emotion captioning to human preferences. This approach improves the generalization on unseen speech, obtaining stronger performance to other zero-shot emotion captioning methods. In addition, captioning-related tasks have also found some applications in TTS. For an instance, Kawamura et al.  [21]  uses speaker captions and speaking style captions, which contain emotional information, to train a TTS system. This TTS system demonstrates higher naturalness and word accuracy with the help of speaking style and speaker identity prompts.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Description",
      "text": "In this section, we introduce a large-scale, comprehensive, recorded interactive Chinese multimodal emotion dataset, EmotionTalk. We describe data Collection, annotation and statistics in detail.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Collection",
      "text": "Compared to other Chinese multimodal datasets, our data is recorded by professional actors from the drama department, ensuring more authentic and natural emotional expressions, thus better simulating spontaneous emotional behavior in real-world environments. At the same time, this process is more challenging and time-consuming.\n\nTo ensure data diversity, we create situational scripts that simulate real-life interpersonal interactions in a dialogue format. The scripts are inspired by television plotlines or generated by large language models (LLMs) and then manually reviewed for quality. Each dialogue scene features two characters, with multiple rounds of interaction designed to capture the dynamic changes in emotions over time. The scripts include varying emotional intensities, ranging from lighthearted conversations to intense emotional conflicts, fully showcasing the diversity of emotional expression.\n\nThe scripts cover multiple life themes, such as friendship, family, workplace, and patient-caregiver interactions. The friendship theme includes dialogues about joy, arguments, and reconciliation, highlighting support and conflict. The workplace theme addresses complex emotions like collaboration, competition, pressure, and misunderstandings. Each theme is intricately designed, with the family theme encompassing scenarios like family arguments, holiday reunions, and farewells, reflecting emotions like warmth, anger, and sadness. The language style is tailored to each theme: friendly dialogues are casual and natural, while workplace conversations are more formal and serious, effectively simulating real-world language environments and inspiring authentic emotional performances from the actors, thereby enhancing the script's emotional depth and immersion. It is important to note that actors are not constrained by the script itself. Instead, they are encouraged to express their authentic emotions based on the theme and subject matter. Furthermore, considering that performers cannot sustain a single emotion or continuously portray the same emotion for extended periods, each dialogue lasts approximately 2 minutes.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Annotation",
      "text": "To ensure the high quality and diversity of the dataset, we design a rigorous data annotation process, incorporating multi-dimensional annotations for emotion categories, emotion intensity, and emotional speaking style caption. The detailed annotation process is outlined below: Emotion Category: For each sample, we design a multi-step annotation process with cross-validation by N (N = 5) annotators. The emotion category annotation is based on the basic emotion theory commonly used in psychological research and covers K (K = 7) widely recognized emotion categories: happiness, surprise, sadness, disgust, anger, fear, and neutral. To prevent interference between different modalities and avoid potential confusion, we follow the annotation principles of CH-SIMS, requiring annotators to only view the information from the current modality without performing simultaneous annotations across multiple modalities. The annotation process follows a predefined sequence, starting with text, followed by audio, silent video, and finally multimodal integration. Each emotion annotation consists of a emotion category y i and a confidence score c i , of which is set to 0.1, 0.3, 0.5, 0.7, and 0.9, to quantify the annotator's confidence in their judgment. The formula for calculating the weighted confidence score x k , k = {1, . . . , K} for each category of a sample is as follows:\n\nwhere k represents the emotion category, N k is the number of annotations for category k, I(y i = k) is an indicator function, which equals 1 if the label y i assigned by annotator i is equal to category k, and 0 otherwise.\n\nThus, the final emotion category y is calculated as follows:\n\nwhere argmax represents selecting the category k that corresponds to the maximum weighted confidence x k as the final category label.\n\nFor annotations with low confidence scores or inconsistent annotations, expert reviewers are consulted to make final determinations, ensuring the accuracy and reliability of the annotations.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion Intensity:",
      "text": "To more accurately quantify the intensity of emotional expressions, we have designed a multimodalbased emotion intensity annotation process aimed at quantitatively labeling the emotional polarity (positive, negative, neutral) and its intensity in utterances. For each audio clip, five annotators will be assigned, and each annotator will evaluate the emotional state as -2 (strongly negative), -1 (weakly negative), 0 (neutral), 1 (weakly positive), or 2 (strongly positive). The annotation results from the five annotators are then averaged to obtain a continuous label that contains emotion intensity information. The final labeling results will be one of the following values: {-2.0, -1. Emotional Speaking Style Caption: The most crucial aspect is that we design an innovative speech annotation process aimed at comprehensively describing the emotional and expressive features in speech. This annotation system covers four distinct dimensions: speaker caption, speaking style caption, emotion caption, and emotional speaking style caption. In this process, we first annotate the speaker's voice quality, focusing on features such as warmth, hoarseness, and clarity. Next, the speaking style caption emphasizes annotating speech rate, intonation, stress, and pauses, highlighting the individual's linguistic habits in communication. Emotion caption details the type and intensity of emotions conveyed in the speech. Finally, the integrated caption combines the features from the speaker, speaking style, and emotion dimensions to generate a comprehensive and accurate semantic summary. To further enhance the diversity and expressive capability of the annotations, we employ a LLM to expand the integrated descriptions, generating five semantically consistent but stylistically varied versions, thus ensuring data diversity and flexibility.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Statistics",
      "text": "The dataset is composed of 744 dialogues, with   For the multi-modal video modality incorporating audio, neutral emotions constitute the largest category, with 6,853 instances, representing 35.6% of the dataset. Subsequently, angry and happy emotions are the next most frequent, occurring in 3,698 instances (19.2%) and 3,592 instances (18.7%), respectively. The remaining emotional categories are surprised (1,661 instances, 8.6%), disgusted (1,371 instances, 7.1%), sad (1,153 instances, 6.0%), and fearful (922 instances, 4.8%). In summary, the distribution is characterized by a significant proportion of neutral, angry, and happy emotions.\n\nConcerning the continuous emotion labels within the multi-modal data, our analysis demonstrates a marked dominance of negative sentiments. Weakly negative and negative emotions together comprise 78.4% of the total (49.1% and 29.3%, respectively). Neutral sentiments account for 11.5%, while positive sentiments (weakly positive at 8.2% and positive at 2.0%) represent a smaller portion at 10.2%. This distribution underscores a notable bias toward negative emotional expressions in the dataset, which may be attributed to the inherent characteristics of the video content or the nature of user responses.\n\nFleiss' Kappa is a statistical measure used to assess the agreement among multiple raters or evaluators when categorizing items into distinct categories. Fleiss' Kappa can be applied to situations with more than two raters. The formula for Fleiss' Kappa is as follows:  In our dataset, the Fleiss' Kappa value for the audio data is 0.79, for the text data is 0.66, for the video data without audio is 0.73, and for the video data with audio is 0.78. These Fleiss' Kappa values indicate good agreement across all modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we evaluate our dataset across a variety of tasks, including unimodal emotion recognition, multimodal emotion recognition, multimodal emotion analysis, and emotional speaking style captioning. We build our experimental pipeline upon the MerBench  [24] , which provides a standardized setup for benchmarking multimodal models.\n\nSpecifically, in the continuous setting, we focus on a binary classification task that distinguishes between positive and negative emotions, where samples with scores below 0 are labeled as negative, and those above 0 as positive. For the first three tasks, accuracy (ACC) is used as the primary evaluation metric, while for the speaker emotion-style captioning task, we adopt BLEU 4 , ROUGE L , METEOR, SPIDEr, FENSE, BERTScore and CLAPScore for evaluation. To facilitate reproducibility, we document all experimental settings in Appendix B.2, including hyperparameter tuning strategies, optimizer selection, and the values of all key training parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Unimodal Emotion Recognition",
      "text": "This section reports the emotion recognition performance of different feature extractors on the corresponding modalities, as shown in Table  2 .  Feature Extractor: To assess the performance of our dataset, we employ a comprehensive suite of pre-trained baseline models across different modalities. Specifically, for the speech modality, we utilize Wav2Vec 2.0  [1] , HuBERT  [19] , WavLM  [6] , and Whisper  [39] . For the text modality, our selection includes Vicuna-7B  [7] , LERT  [8] , DeBERTa  [17] , BERT  [9] , Sentence-BERT  [40] , BLOOM-7B  [53] , RoBERTa  [30] , ChatGLM2  [10]  and Baichuan-7B  [55] . For the visual modality, we adopt Data2Vec  [2] , VideoMAE  [47] , EVA-02  [12] , CLIP  [38] , and DINOv2  [35] .\n\nBased on the comparative performance of these encoders shown in Table  2 , we aim to provide guidance for modality-specific feature selection in downstream emotion recognition tasks. Given that the EmotionTalk dataset provides independent unimodal annotations, we conduct two experimental settings to investigate the capability of unimodal representations in emotion recognition. In the first setting, we utilize ground-truth unimodal labels to evaluate each model's ability to perform unimodal emotion classification. In the second setting, we adopt multimodal labels instead, to assess whether a single modality alone can reliably infer the speaker's actual emotional state.\n\nSeveral key findings emerge from these experiments. First, for the same unimodal classification task, models consistently achieve better performance when trained and evaluated with unimodal labels than with multimodal labels. This suggests that models are effective at capturing the modality-specific emotional cues. However, these results do not necessarily reflect the speaker's actual emotional state, as unimodal annotations may be biased or incomplete. This indicates that unimodal information remains a valuable signal for emotion recognition, though it is inherently limited in expressiveness and scope. Consequently, relying solely on unimodal representations is insufficient for accurately capturing complex emotional states, reinforcing the importance of multimodal fusion in emotion understanding.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Multimodal Emotion Recognition / Sentiment Analysis",
      "text": "Table  3  presents the performance of various multimodal fusion algorithms on the EmotionTalk dataset using the optimal encoder from each modality-HuBERT-Base (speech), Baichuan-7B (text), and CLIP-Large (visual). The fusion methods are categorized into frame-level (e.g., MFN  [64] , GMFN  [65] , MCTN  [36] , MFM  [49] , and MulT  [50] ) and utterance-level (e.g., TFN  [63] , LMF  [31] , MISA  [16] , MMIM  [14] , and the Attention mechanism  [51] ) strategies, enabling a comparative analysis of their effectiveness in multimodal emotion recognition.\n\nSeveral important observations can be drawn. First, utterance-level fusion methods generally outperform frame-level approaches in both the four-class and full-class emotion classification settings.\n\nFor instance, LMF achieves the highest score in the Multimodal(Four) setting (83.04%) and also yields the best average performance (75.53%), indicating that aligning features at the utterance level better captures the holistic emotional state. Similarly, attention-based fusion also performs competitively, with an average score of 75.14%, suggesting the advantage of adaptive weighting across modalities. Moreover, due to the limited scale of emotion datasets, complex fusion algorithms are prone to overfitting. In contrast, simple yet effective fusion strategies often achieve relatively better performance.\n\nTable  4  reports the multimodal emotion recognition results using the top four models from each modality, selected based on unimodal performance in Table  2 . All combinations adopt the LMF algorithm for fusion. Among the configurations, the combination of RoBERTa-Base (text), HuBERT-Large (speech), and Dinov2-Giant (visual) achieves the best overall performance, with the highest score in the Discrete (Four) setting (83.23%) and the highest average (81.87%). Notably, different model combinations yield comparable performance on the continuous labels, while their results on discrete tasks vary considerably, underscoring the impact of feature selection. These findings confirm that even under the same fusion strategy, the choice of multimodal features can significantly affect the overall performance of multimodal fusion.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotional Speaker Style Captioning",
      "text": "Table  5  presents a comprehensive comparison of three decoder architectures-Transformer-based, GPT-2, and Qwen-2-across multiple captioning dimensions (Speaker, Style, Emotion, and Overall), evaluated using a suite of standard automatic metrics. Qwen-2 outperforms other models across all four tasks, demonstrating its effectiveness in producing captions that preserve both emotional nuance and stylistic diversity. The strong BERTScore suggests that its generation aligns closely with human references at the semantic level, beyond surface-level lexical similarity. Although Qwen-2 achieved the best overall performance, GPT-2 performed notably well on the speaker-focused task, obtaining the highest ROUGE L (0.430) and CLAPScore (0.899). In contrast, the Transformer-based decoder showed weaker overall results but maintained basic structural coherence and content coverage, as reflected in its ROUGE L and SPIDEr scores. Overall, these results highlight Qwen-2's robustness in capturing fine-grained stylistic and emotional cues, crucial for emotional speaking style captioning.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we construct an interactive Chinese multimodal sentiment dataset, EmotionTalk, filling the gap in Chinese multimodal emotion research that lacks high-quality recorded data and emotional speaking style captions dataset. The dataset includes 23.6 hours of multimodal data recorded by 19 professional actors from the drama department, ensuring the authenticity and naturalness of emotional expressions. EmotionTalk is the first interactive multimodal emotion dataset in Chinese with fine-grained labels and emotional speaking style caption annotations, making it a valuable resource for affective computing community. In addition, we conduct experiments on unimodal emotion recognition, multimodal emotion recognition and emotional speaking style caption tasks to validate the quality of the dataset. The dataset is freely available to the academic community, aiming to promote further development in the fields of affective computing and human-computer interaction, and to provide valuable resource support for related research.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "A Datasheets For Datasets A.1 Dataset Snapshots",
      "text": "The dataset comprises 744 dialogues, encompassing a total of 19,250 utterances for each unimodal modality-text, audio, and video. The audio data span approximately 23.6 hours, with an average duration of 4.4 seconds per utterance.\n\nEach utterance is stored as an individual JSON file following a unique naming convention in the format: \"<group_No>_<session_No>_<Speaker_id>_<Utt_No>.json\". Corresponding audio and video files are named identically, with the extensions \".wav\" and \".mp4\" respectively: \"<group_No>_<session_No>_<Speaker_id>_<Utt_No>.wav\" and \"<group_No>_<session_No>_<Speaker_id>_<Utt_No>.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.2 Data Format",
      "text": "Each utterance in the EmotionTalk dataset is associated with a corresponding \".jsonl\" file, which contains detailed sample-level annotations. These annotations include not only the basic information such as the emotion label, speaker identity, and transcript, but also rich metadata that describes the expressive characteristics of the utterance. The detailed annotation fields are listed in Table  6 .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A.3 Data Distribution",
      "text": "In this study, to make full use of the data and ensure both effective model training and fair evaluation, the dataset is divided into training, validation, and test sets in a approximate ratio of 8:1:1. Specifically, 80% of the data is used for training the model to learn effective feature representations, 10% is allocated for validation to assist in model selection and prevent overfitting during training, and the remaining 10% is reserved as the test set to evaluate the model's generalization performance. When splitting the dataset, we make effort to ensure that the data distribution of each category remains consistent across the training, validation, and test sets. A detailed information of the the distribution across different subsets is presented in the Table  7 . Relative path to the audio file. 1 The emotion categories include: happiness, surprise, sadness, disgust, anger, fear, and neutral. 2 The 5-dimensional sentiment labels include: -2 (strongly negative), -1 (weakly negative), 0 (neutral), 1 (weakly positive), or 2 (strongly positive). 3 The computation method is detailed in Section 3.2 Annotation. To comprehensively evaluate the proposed dataset, we conduct extensive experiments on three tasks: unimodal emotion recognition, multimodal emotion recognition / sentiment analysis and emotional speaker style captioning. For the unimodal and multimodal emotion recognition tasks, we employ a range of state-of-the-art models as feature extractors to obtain representations from each modality. Then, we select several high-quality features as the foundation for multimodal fusion. For the emotional speaker style captioning task, we utilize three types of decoders, including transformer, GPT-2 and Qwen-2, to assess the quality and utility of the dataset. The details of the models are provided in Table  8 .",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "B.2 Hyperparameters And Computing Resources",
      "text": "We provide open access to both the data and the code used in our experiments. The full experimental code is available at https://github.com/NKU-HLT/EmotionTalk. Key training hyperparameters",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C Annotation Website",
      "text": "To improve the efficiency of the annotation process, we conduct data annotation and quality assessment on a data platform. As shown in the Fig  3 , this platform supports the annotation of various tasks such as speech emotion recognition and emotional speaking style captioning.  In the task of Emotion Prediction in Conversation (EPC), we observe clear performance differences across modalities and models  [41] [42] [43] , as shown in Table  12 . Speech-based models consistently outperform text-based ones, highlighting the importance of vocal information in anticipating upcoming emotional states. Among the speech-only models, EAMT  [43]   In Emotion Recognition in Conversation (ERC), a similar modality gap is evident, with speech-based models outperforming their text-based versions by a large margin  [15, 60, 34, 13] , as shown in Table  13 . DialoguGCN  [13]  achieves the best performance among all models with 67.51% accuracy, leveraging its graph-based structure to effectively capture speaker interactions and contextual flow.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "D Extra Experiment Results And Analysis",
      "text": "DialogueRNN  [34]  and CMN  [15]  also perform competitively in the speech modality (66.34% and 66.37%, respectively), while ICON  [60]  slightly lags behind at 65.31%. In contrast, their textbased counterparts yield significantly lower results-DialoguGCN at 49.75% and DialogueRNN at 49.63%-underscoring the limitations of relying solely on lexical information for emotion recognition. Notably, DialoguGCN consistently outperforms the other models across both modalities, suggesting its architectural advantage in handling complex conversational dynamics.\n\nAll models are trained using the Adam optimizer with an initial learning rate of 1e-4 and a weight decay of 1e-5. The batch size is set to 16, and models are trained for a maximum of 30 epochs (except for DialogueRNN, which is trained for 100 epochs). A StepLR scheduler is employed to decay the learning rate by a factor of 0.1 every 10 epochs in ERC.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "E Ethics Statement And License",
      "text": "This study is conducted in accordance with rigorous ethical guidelines to ensure the protection of participants' rights and well-being. All recordings take place in a quiet indoor environment, where professional actors engage in natural, emotionally diverse, and logically coherent dialogues based on predefined emotional themes and content outlines. The annotation cost for each data sample is 0.2 RMB.\n\nTo preserve participant privacy, all data are anonymized by removing personal identifiers and replacing them with coded labels. The dataset is released under the CC BY-NC 4.0 license, which prohibits commercial use and supports ethical research practices. Data are securely stored, and access is restricted to authorized researchers for academic purposes only.\n\nIn conclusion, this study demonstrates a strong commitment to ethical standards, encompassing informed consent, the protection of personal privacy, appropriate compensation, and the responsible dissemination of data, thereby safeguarding participant rights and supporting ethical scientific advancement.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "F Accessibility",
      "text": "The EmotionTalk dataset will be released soon.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "G Impact G.1 Positive Impact",
      "text": "This paper introduces EmotionTalk, first interactive multimodal emotion dataset in Chinese, with fine-grained labels and emotional speaking style caption annotations. EmotionTalk addressing the lack of high-quality Chinese datasets in the field of multimodal emotion research. We conduct a series of experiments on unimodal, multimodal, and emotional speaking style captioning tasks to assess the quality of the dataset. The EmotionTalk dataset has been publicly released with the goal of advancing research in affective computing.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "G.2 Negative Impact",
      "text": "Although the EmotionTalk dataset comprises 23.6 hours of recordings, its scale remains subject to further enhancement. Additionally, there exists a problem of the imbalanced category distribution in our dataset. Overall, the neutral emotion category contains the largest number of samples, while the fearful emotion category has the fewest. While this distribution is more consistent with real-world emotional occurrences, it can still constrain the model's ability to accurately identify certain emotion categories.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "H Limitations",
      "text": "Although the dataset includes 23.6 hours of multimodal data and holds significant value for MMER, its scale is still relatively small compared to the large datasets used in fields such as speech synthesis and speech recognition. This limits the applicability of the dataset in large-scale data processing and model training. Additionally, the dataset is recorded by 19 professional actors from different regions. While this ensures the naturalness and diversity of emotional expression, the relatively small number of participants and the uneven geographical distribution may affect the representativeness of the data. In the future, expanding the number of participants and incorporating samples from a broader range of regions and cultural backgrounds will help further enrich the dataset's diversity, enhance its adaptability to different emotional expressions and cultural contexts, and thereby improve the effectiveness and applicability of sentiment analysis models in a wider range of real-world scenarios.",
      "page_start": 20,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a, neutral instances dominate, with 9,378 samples accounting for 48.7% of the total. Angry",
      "page": 5
    },
    {
      "caption": "Figure 1: b, in the text modality, 7,903 instances are labeled as neutral, accounting for 41.1%",
      "page": 5
    },
    {
      "caption": "Figure 1: Data distribution across different modalities.",
      "page": 6
    },
    {
      "caption": "Figure 1: c and 1d, respectively. In the video modality without audio, neutral",
      "page": 6
    },
    {
      "caption": "Figure 2: (a) Examples of audio file samples.",
      "page": 15
    },
    {
      "caption": "Figure 2: Snapshots of audio and video samples in the EmotionTalk dataset. All files are named",
      "page": 15
    },
    {
      "caption": "Figure 3: Overview of the annotation platform interface.",
      "page": 18
    },
    {
      "caption": "Figure 3: , this platform supports the annotation of various",
      "page": 19
    }
  ],
  "tables": [
    {
      "caption": "Table 1: presents the datasets which are commonly used in the field of multimodal emotion recognition,",
      "page": 2
    },
    {
      "caption": "Table 1: Summary of multimodal emotion datasets.",
      "page": 3
    },
    {
      "caption": "Table 2: We report unimodal results for the EmotionTalk dataset. Four means that only four emotion",
      "page": 7
    },
    {
      "caption": "Table 3: We report multimodal results for the EmotionTalk dataset. Four means that only four emotion",
      "page": 8
    },
    {
      "caption": "Table 4: “Top4” indicates that we select the top 4 models for each modality (their ranking is based on",
      "page": 8
    },
    {
      "caption": "Table 2: ). We utilize the LMF for multimodal fusion.",
      "page": 8
    },
    {
      "caption": "Table 2: , we aim to provide",
      "page": 8
    },
    {
      "caption": "Table 3: presents the performance of various multimodal fusion algorithms on the EmotionTalk",
      "page": 8
    },
    {
      "caption": "Table 5: Automatic captioning results. All methods use Hubert as the speech encoder.",
      "page": 9
    },
    {
      "caption": "Table 4: reports the multimodal emotion recognition results using the top four models from each",
      "page": 9
    },
    {
      "caption": "Table 2: All combinations adopt the LMF",
      "page": 9
    },
    {
      "caption": "Table 5: presents a comprehensive comparison of three decoder architectures—Transformer-based,",
      "page": 9
    },
    {
      "caption": "Table 7: Table 6: Description of Sample-Level Annotations",
      "page": 16
    },
    {
      "caption": "Table 7: Statistics of the data distribution across the training, validation, and test sets.",
      "page": 16
    },
    {
      "caption": "Table 9: , Table 10, and Table 11. All models are trained using",
      "page": 17
    },
    {
      "caption": "Table 8: An overview of the models employed across different tasks.",
      "page": 17
    },
    {
      "caption": "Table 9: Training hyperparameters used for the unimodal and multimodal models in Table 2 on the",
      "page": 17
    },
    {
      "caption": "Table 10: Key training hyperparameters used for each multimodal model in Table 3 on the Emo-",
      "page": 18
    },
    {
      "caption": "Table 11: Training hyperparameters for each decoder in Table 5.",
      "page": 18
    },
    {
      "caption": "Table 12: Accuracy (%) comparison of different models across modalities (Speech, Text, and",
      "page": 19
    },
    {
      "caption": "Table 13: Accuracy (%) comparison of different models across modalities (Speech and Text) on the",
      "page": 19
    },
    {
      "caption": "Table 12: Speech-based models consistently",
      "page": 19
    },
    {
      "caption": "Table 13: DialoguGCN [13] achieves the best performance among all models with 67.51% accuracy,",
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "year": "2022",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "6",
      "title": "Wavlm: Large-scale self-supervised pretraining for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Ziqing Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality"
    },
    {
      "citation_id": "8",
      "title": "Lert: A linguistically-motivated pre-trained language model",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Shijin Wang",
        "Ting Liu"
      ],
      "year": "2022",
      "venue": "Lert: A linguistically-motivated pre-trained language model",
      "arxiv": "arXiv:2211.05344"
    },
    {
      "citation_id": "9",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies"
    },
    {
      "citation_id": "10",
      "title": "General language model pretraining with autoregressive blank infilling",
      "authors": [
        "Zhengxiao Du",
        "Yujie Qian",
        "Xiao Liu",
        "Ming Ding",
        "Jiezhong Qiu",
        "Zhilin Yang",
        "Jie Tang",
        "Glm"
      ],
      "year": "2021",
      "venue": "General language model pretraining with autoregressive blank infilling",
      "arxiv": "arXiv:2103.10360"
    },
    {
      "citation_id": "11",
      "title": "Atta-net: attention aggregation network for audio-visual emotion recognition",
      "authors": [
        "Ruijia Fan",
        "Hong Liu",
        "Yidi Li",
        "Peini Guo",
        "Guoquan Wang",
        "Ti Wang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "12",
      "title": "Eva-02: A visual representation for neon genesis",
      "authors": [
        "Yuxin Fang",
        "Quan Sun",
        "Xinggang Wang",
        "Tiejun Huang",
        "Xinlong Wang",
        "Yue Cao"
      ],
      "year": "2024",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "13",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "14",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "arxiv": "arXiv:2109.00412"
    },
    {
      "citation_id": "15",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "16",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "17",
      "title": "Deberta: Decoding-enhanced bert with disentangled attention",
      "authors": [
        "Pengcheng He",
        "Xiaodong Liu",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "year": "2020",
      "venue": "Deberta: Decoding-enhanced bert with disentangled attention",
      "arxiv": "arXiv:2006.03654"
    },
    {
      "citation_id": "18",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc",
      "doi": "10.1109/TASLP.2021.3122291"
    },
    {
      "citation_id": "19",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "20",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP43922.2022.9747397"
    },
    {
      "citation_id": "21",
      "title": "Libritts-p: A corpus with speaking style and speaker identity prompts for textto-speech and style captioning",
      "authors": [
        "Masaya Kawamura",
        "Ryuichi Yamamoto",
        "Yuma Shirahata",
        "Takuya Hasumi",
        "Kentaro Tachibana"
      ],
      "year": "2024",
      "venue": "Libritts-p: A corpus with speaking style and speaker identity prompts for textto-speech and style captioning"
    },
    {
      "citation_id": "22",
      "title": "Gpt2: Empirical slant delay model for radio space geodetic techniques",
      "authors": [
        "Klemens Lagler",
        "Michael Schindelegger",
        "Johannes Böhm",
        "Hana Krásná",
        "Tobias Nilsson"
      ],
      "year": "2013",
      "venue": "Geophysical research letters"
    },
    {
      "citation_id": "23",
      "title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "Mike Lewis",
        "Yinhan Liu",
        "Naman Goyal",
        "Marjan Ghazvininejad",
        "Abdelrahman Mohamed",
        "Omer Levy",
        "Ves Stoyanov",
        "Luke Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "arxiv": "arXiv:1910.13461"
    },
    {
      "citation_id": "24",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "25",
      "title": "AlignCap: Aligning speech emotion captioning to human preferences",
      "authors": [
        "Ziqi Liang",
        "Haoxiang Shi",
        "Hanhui Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.224"
    },
    {
      "citation_id": "26",
      "title": "Aligncap: Aligning speech emotion captioning to human preferences",
      "authors": [
        "Ziqi Liang",
        "Haoxiang Shi",
        "Hanhui Chen"
      ],
      "year": "2024",
      "venue": "Aligncap: Aligning speech emotion captioning to human preferences",
      "arxiv": "arXiv:2410.19134"
    },
    {
      "citation_id": "27",
      "title": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Xiaofen Xing",
        "Björn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "Emotion and intent joint understanding in multimodal conversation: A benchmarking dataset",
      "arxiv": "arXiv:2407.02751"
    },
    {
      "citation_id": "28",
      "title": "Discriminative feature representation based on cascaded attention network with adversarial joint loss for speech emotion recognition",
      "authors": [
        "Yang Liu",
        "Haoqin Sun",
        "Wenbo Guan",
        "Yuqi Xia",
        "Zhen Zhao"
      ],
      "year": "2022",
      "venue": "Interspeech 2022",
      "doi": "10.21437/Interspeech.2022-11480"
    },
    {
      "citation_id": "29",
      "title": "Multi-level knowledge distillation for speech emotion recognition in noisy conditions",
      "authors": [
        "Yang Liu",
        "Haoqin Sun",
        "Geng Chen",
        "Qingyue Wang",
        "Zhen Zhao",
        "Xugang Lu",
        "Longbiao Wang"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech 2023"
    },
    {
      "citation_id": "30",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "31",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "32",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "33",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "Hui Ma",
        "Jian Wang",
        "Hongfei Lin",
        "Bo Zhang",
        "Yijia Zhang",
        "Bo Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2023.3271019"
    },
    {
      "citation_id": "34",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "35",
      "title": "Learning robust visual features without supervision",
      "authors": [
        "Maxime Oquab",
        "Timothée Darcet",
        "Théo Moutakanni",
        "Huy Vo",
        "Marc Szafraniec",
        "Vasil Khalidov",
        "Pierre Fernandez",
        "Daniel Haziza",
        "Francisco Massa",
        "Alaaeldin El-Nouby"
      ],
      "year": "2023",
      "venue": "Learning robust visual features without supervision",
      "arxiv": "arXiv:2304.07193"
    },
    {
      "citation_id": "36",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "37",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "38",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "39",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "40",
      "title": "Sentence-bert: Sentence embeddings using siamese bertnetworks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Sentence-bert: Sentence embeddings using siamese bertnetworks",
      "arxiv": "arXiv:1908.10084"
    },
    {
      "citation_id": "41",
      "title": "Audio-visual emotion forecasting: Characterizing and predicting future emotion using deep learning",
      "authors": [
        "Sadat Shahriar",
        "Yelin Kim"
      ],
      "year": "2019",
      "venue": "2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)"
    },
    {
      "citation_id": "42",
      "title": "Dimensional emotion prediction based on interactive context in conversation",
      "authors": [
        "Xiaohan Shi",
        "Sixia Li",
        "Jianwu Dang"
      ],
      "year": "2020",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "43",
      "title": "Emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
      "authors": [
        "Xiaohan Shi",
        "Xingfeng Li",
        "Tomoki Toda"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "44",
      "title": "Iterative prototype refinement for ambiguous speech emotion recognition",
      "authors": [
        "Haoqin Sun",
        "Shiwan Zhao",
        "Xiangyu Kong",
        "Xuechen Wang",
        "Hui Wang",
        "Jiaming Zhou",
        "Yong Qin"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech 2024"
    },
    {
      "citation_id": "45",
      "title": "Finegrained disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Haoqin Sun",
        "Shiwan Zhao",
        "Xuechen Wang",
        "Wenjia Zeng",
        "Yong Chen",
        "Yong Qin"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "46",
      "title": "Enhancing emotion recognition in incomplete data: A novel cross-modal alignment, reconstruction, and refinement framework",
      "authors": [
        "Haoqin Sun",
        "Shiwan Zhao",
        "Shaokai Li",
        "Xiangyu Kong",
        "Xuechen Wang",
        "Jiaming Zhou",
        "Aobo Kong",
        "Yong Chen",
        "Wenjia Zeng",
        "Yong Qin"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "47",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "48",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar",
        "Aurelien Rodriguez",
        "Armand Joulin",
        "Edouard Grave",
        "Guillaume Lample"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models"
    },
    {
      "citation_id": "49",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Paul Liang",
        "Amir Zadeh",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "50",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting"
    },
    {
      "citation_id": "51",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "52",
      "title": "Multi-modal graph convolution network for personalized recommendation of microvideo",
      "authors": [
        "Yinwei Wei",
        "Xiang Wang",
        "Liqiang Nie",
        "Xiangnan He",
        "Richang Hong",
        "Tat-Seng Chua",
        "Mmgcn"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "53",
      "title": "A 176b-parameter open-access multilingual language model",
      "authors": [
        "Bigscience Workshop",
        "Le Teven",
        "Angela Scao",
        "Christopher Fan",
        "Ellie Akiki",
        "Suzana Pavlick",
        "Daniel Ilić",
        "Roman Hesslow",
        "Alexandra Castagné",
        "François Sasha Luccioni",
        "Yvon"
      ],
      "year": "2022",
      "venue": "A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "54",
      "title": "Secap: Speech emotion captioning with large language model",
      "authors": [
        "Yaoxun Xu",
        "Hangting Chen",
        "Jianwei Yu",
        "Qiaochu Huang",
        "Zhiyong Wu",
        "Shi-Xiong Zhang",
        "Guangzhi Li",
        "Yi Luo",
        "Rongzhi Gu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "55",
      "title": "Open large-scale language models",
      "authors": [
        "Aiyuan Yang",
        "Bin Xiao",
        "Bingning Wang",
        "Borong Zhang",
        "Ce Bian",
        "Chenxu Chao Yin",
        "Da Lv",
        "Dian Pan",
        "Dong Wang",
        "Yan"
      ],
      "year": "2023",
      "venue": "Open large-scale language models",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "56",
      "title": "Qwen2 technical report",
      "authors": [
        "An Yang",
        "Baosong Yang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Zhou",
        "Chengpeng Li",
        "Chengyuan Li",
        "Dayiheng Liu",
        "Fei Huang"
      ],
      "year": "2024",
      "venue": "Qwen2 technical report"
    },
    {
      "citation_id": "57",
      "title": "Disentangled representation learning for multimodal emotion recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "58",
      "title": "Self-adaptive context and modal-interaction modeling for multimodal emotion recognition",
      "authors": [
        "Haozhe Yang",
        "Xianqiang Gao",
        "Jianlong Wu",
        "Tian Gan",
        "Ning Ding",
        "Feijun Jiang",
        "Liqiang Nie"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023",
      "doi": "10.18653/v1/2023.findings-acl.390"
    },
    {
      "citation_id": "59",
      "title": "CLGSI: A multimodal sentiment analysis framework based on contrastive learning guided by sentiment intensity",
      "authors": [
        "Yang Yang",
        "Xunde Dong",
        "Yupeng Qiang"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2024",
      "doi": "10.18653/v1/2024.findings-naacl.135"
    },
    {
      "citation_id": "60",
      "title": "An interaction-aware attention network for speech emotion recognition in spoken dialogs",
      "authors": [
        "Sung-Lin Yeh",
        "Yun-Shao Lin",
        "Chi-Chun Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "61",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "62",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "63",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "64",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "65",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "66",
      "title": "Tag-assisted multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Jiandian Zeng",
        "Tianyi Liu",
        "Jiantao Zhou"
      ],
      "year": "2022",
      "venue": "Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "67",
      "title": "Mitigating inconsistencies in multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Jiandian Zeng",
        "Jiantao Zhou",
        "Tianyi Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "68",
      "title": "M3ed: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "M3ed: Multi-modal multi-scene multi-label emotional dialogue database",
      "arxiv": "arXiv:2205.10237"
    }
  ]
}