{
  "paper_id": "2409.16081v1",
  "title": "Online Multi-Level Contrastive Representation Distillation For Cross-Subject Fnirs Emotion Recognition",
  "published": "2024-09-24T13:30:15Z",
  "authors": [
    "Zhili Lai",
    "Chunmei Qing",
    "Junpeng Tan",
    "Wanxiang Luo",
    "Xiangmin Xu"
  ],
  "keywords": [
    "fNIRS",
    "Emotion Recognition",
    "Online Knowledge Distillation",
    "Contrastive Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Utilizing functional near-infrared spectroscopy (fNIRS) signals for emotion recognition is a significant advancement in understanding human emotions. However, due to the lack of artificial intelligence data and algorithms in this field, current research faces the following challenges: 1) The portable wearable devices have higher requirements for lightweight models; 2) The objective differences of physiology and psychology among different subjects aggravate the difficulty of emotion recognition. To address these challenges, we propose a novel cross-subject fNIRS emotion recognition method, called the Online Multi-level Contrastive Representation Distillation framework (OMCRD). Specifically, OMCRD is a framework designed for mutual learning among multiple lightweight student networks. It utilizes multi-level fNIRS feature extractor for each sub-network and conducts multi-view sentimental mining using physiological signals. The proposed Inter-Subject Interaction Contrastive Representation (IS-ICR) facilitates knowledge transfer for interactions between student models, enhancing cross-subject emotion recognition performance. The optimal student network can be selected and deployed on a wearable device. Some experimental results demonstrate that OMCRD achieves state-of-the-art results in emotional perception and affective imagery tasks. \n CCS CONCEPTS • Computing methodologies → Artificial intelligence; • Humancentered computing → HCI design and evaluation methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is an important research task in affective computing, which aims to represent and explain human mental states by acquiring psychological or non-psychological signals. Accurate emotion recognition can help us better sense a person's brain activity and thoughts. Given contemporary society's challenging employment and living conditions, emotion recognition technology is increasingly prevalent across diverse industries. Among them, common applications include health diagnostics, safe driving, medical services, and human-computer interaction.  [1] [2] [3] [4] [5] .\n\nTo this end, commonly processed data for affective computing include facial videos, body movements, speech, text scales, and physiological signals  [6] [7] [8] [9] . Comparing these data sources, physiological signals come from the human brain nervous system, less influenced by subjective consciousness or deception, and offer more dependable insights into human mood shifts. Functional Near-Infrared Spectroscopy (fNIRS) stands out as a potential method for neural signal capture and visualization, to create a real-time communication link between the human brain and external devices. fNIRS signals indicate brain activity by tracking alterations in oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR) levels across various brain areas. Two primary features exist in the fNIRS signal: region-level features representing activation levels in different regions, and channel-level features showing the overall change in blood oxygen concentration across time.\n\nWhile fNIRS has been crucial in measuring physiological signals, there remain numerous challenges in advancing fNIRS data research. Current deep learning-based fNIRS research primarily concentrates on distinct tasks, like Brain-Computer Interface (BCI)  [10]  and mental health diagnosis  [11] , while exploration in fNIRS emotion recognition is still in the early stages of development. With the rapid development of portable wearable physiological devices, lightweight network models can maintain reasonable measurement accuracy while adapting to the resource constraints of wearable devices and application requirements. Finally, the fNIRS recordings related to emotions exhibit substantial inter-subject variabilities due to the physiological differences among individuals. This is a considerable challenge for cross-subject emotion recognition.\n\nTo address the issues mentioned above, this paper proposes a novel Online Multi-level Contrastive Representation Distillation framework for fNIRS emotion recognition, named OMCRD. We explore and experiment with fNIRS emotion recognition to improve deficiencies in this field. Specifically, the OMCRD framework employs a one-stage online multi-student network mutual Knowledge Distillation (KD) strategy with the multi-level (region-level and channel-level) fNIRS feature extractor, which exploits the mutual learning between multiple lightweight student networks during training to eliminate the dependence on complex teacher models. Furthermore, we also propose the Inter-Subject Interaction Contrastive Representation loss (IS-ICR). \"Inter-Subject\" means that fNIRS signals collected from different subjects facing the same stimulus are treated as the same class, facilitating the learning of their similarities. \"Interaction\" refers to establishing collaborative learning relationships across peer networks. Different networks can communicate with each other and optimize parameters during training. This ensures that each network learns additional contrastive representation knowledge from other peer networks. This approach facilitates the deployment of the final best-performing student model to a wearable fNIRS physiological monitoring device.\n\nIn summary, our primary contributions are as follows:\n\n(1) We propose a novel fNIRS emotion recognition method (OM-CRD), which is an online multi-level contrastive representation distillation framework with the multiple student networks for learning multi-view fNIRS features and reducing model complexity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "In this segment, to enhance comprehension of the proposed approach, we mainly introduce several related works: physiological signals emotion recognition, knowledge distillation, and contrastive learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Physiological Signals Emotion Recognition",
      "text": "Recently, physiological signals emotion recognition has gradually become a research hotspot in affective computing. Unlike emotion recognition of behavioral modalities  [6, 8, 12] , Physiological signals are challenging to disguise and cannot be intentionally or consciously controlled. Physiological signals include electroencephalogram (EEG)  [9, 13] , electrodermal activity (EDA)  [14] , electrocardiogram (ECG)  [15, 16] , Galvanic Skin Response (GSR)  [17] . They are frequently used in emotion recognition systems because they can mimic the real physiological changes of human emotions. Like, Xiao et al.  [16]  enhanced emotion recognition performance by acquiring spatial-temporal representations of various ECG regions and implementing a dynamic weight allocation layer to modify the influence of each ECG region. To ensure the recognition of emotional states in a long-time series, Susanto et al.  [17]  analyzed GSR signals based on a 1D convolutional neural network and a residual bidirectional gated loop unit. Meanwhile, Yu et al.  [14]  investigated the performance of different deep neural networks on a subject-independent EDA-based emotion classification task. Gong et al.  [9]  proposed a spatio-temporal two-stream fusion network based on an attention mechanism for EEG-based emotion recognition.\n\nNotably, as an emerging non-invasive brain imaging technology, fNIRS has gradually gained attention in emotion recognition research due to its advantages of flexibility, ease of operation, and low cost. Si et al.  [18]  combined the CNN branch and the statistical branch to construct a dual-branch joint network for cross-subject fNIRS emotion recognition, which is the first introduction of deep learning techniques in the field. Chen et al.  [19]  presented a pioneering wearable bimodal system. The system combined fNIRS and EEG technology and used the temporal convolutional network to identify implicit emotional states in real-time. fNIRS is crucial for processing emotional brain responses and shows promise in emotion recognition. However, due to minimal AI research and challenges in feature extraction, algorithm accuracy is currently very low. Implementing a multi-student network strategy for optimizing training with multi-view features could be beneficial.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Knowledge Distillation",
      "text": "The Knowledge Distillation (KD) method was initially proposed by Hinton et al.  [21] , focusing on distilling knowledge from a large teacher model to improve a smaller student network. KD methods facilitate the deployment of large-parameter models in practical applications. In the KD research based on physiological signals, Gu et al.  [22]  designed a novel frame-level teacher-student framework, achieving the best performance in subject-independent EEG emotion recognition. Wang et al.  [23]  attempted to distill the knowledge from ResNet34 into a smaller model ResNet8 in EEG emotion recognition tasks to achieve performance improvement and model compression. Liu et al.  [24]  proposed a cross-modal (GSR and EEG) knowledge distillation framework, effectively transferring heterogeneous and interactive knowledge from multimodal to unimodal GSR models, enhancing emotion recognition performance while reducing dependence on EEG signals.\n\nHowever, these methods employ an offline learning strategy and require pre-training of a suitable and powerful \"teacher\" model. Moreover, this two-stage training process is very time-consuming, which is challenging in real-time fNIRS emotion recognition tasks. Compared to traditional KD, online KD aims to improve performance by leveraging collaborative learning across multiple student networks. Online KD employs one-stage training and does not rely on a pre-trained teacher model. Therefore, inspired by the success of online KD in the image domain  [25] [26] [27] [28] , we design a novel online distillation framework to ensure knowledge transfer and interaction optimization among multi-student networks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Learning",
      "text": "The idea of contrastive learning is to learn which data pairs are similar or different, thereby acquiring the general features of the dataset. It is a form of self-supervised learning algorithm. It has achieved outstanding performance in diverse fields, including Computer Vision (CV)  [29] , Natural Language Processing (NLP)  [30] , and bioinformatics  [31] . Khosla et al  [32]  extended contrastive loss to supervised environments, effectively using label information to extract more discriminative representations. Recently, contrastive learning has also been gradually applied to physiological signals studies. For instance, Soltanieh et al  [33]  investigated the effectiveness of various data augmentation techniques for contrastive self-supervised learning of ECG signals. This enables the model to enhance learning of the generalized ECG representation, thereby enhancing the accuracy of arrhythmia detection. Kalanadhabhatta et al.  [11]  introduced a multi-task supervised contrastive learning method to extract fNIRS, GSR, and facial video embeddings for early childhood mental disorder identification. Ensuring highquality brain signal acquisition with portable devices involves addressing the cross-subject effects and device parameters. Shen et al.  [34]  introduced a contrastive learning-based approach for crosssubject EEG emotion recognition to enhance generalization ability. Wang et al.  [35]  utilized contrastive learning for cross-subject cognitive workload prediction using fNIRS signals. Limited research exists on cross-subject emotion recognition in human-computer interaction based on fNIRS, with low algorithm recognition rates and deployment challenges. To address the problem of cross-subject fNIRS emotion recognition, we propose a cross-subject multi-level contrastive learning strategy. It enables the model to aggregate region-level or channel-level representations of different subjects belonging to the same class at training time, while optimizing the complementary properties between peer models. As shown in Figure  1 , each peer/student network (•) consists of a multi-level fNIRS feature extractor (•) and a linear classifier (•). The fNIRS signal is mapped to the logit vector through the network , i.e.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology 3.1 Overview Of The Proposed Framework",
      "text": "Specifically, the input is processed by the extractor to obtain the region-level embedding and the channel-level embedding ℎ , which are concatenated and then fed into . For the intermediate feature embeddings and ℎ , we introduce two projection modules (•) and ℎ (•) respectively. The process is as follows:\n\nwhere is composed of a linear layer and the 2 -normalization to linearly transform these embeddings into contrastive embeddings , ℎ ∈ R , and is the embedding size. The embeddings and ℎ are used for the calculation of the proposed IS-ICR.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training And Deployment.",
      "text": "During training, ( 2) peer networks are optimized together. Notably, to learn diverse representations, all identical networks { } =1 are assigned different weights initially, a crucial factor for mutual learning success. In the evaluation phase, projection modules are omitted, allowing the assessment of each network independently. The optimal performing network can be selected for final deployment. As the structure of the preserved network is identical to the others, there are no extra computational expenses during testing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multi-Level Fnirs Feature Extractor",
      "text": "The fNIRS signal has two important features: region-level features and channel-level features. Region-level features represent the spatial correlation between different fNIRS signal channels. Over a while, changes in blood oxygen concentration from different channels often exhibit different dependencies, such as the higher correlation between channels from similar functional brain regions or neighboring spatial locations. Channel-level features represent the temporal continuity of blood oxygen concentration changes within multiple fNIRS signal channels. We designed two feature extractors to capture these two types of features. One is a hybrid feature extractor based on CNN+LSTM, and the other is a feature extractor based on Transformers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Cnn+Lstm Extractor.",
      "text": "For the hybrid feature extractor with CNN and LSTM, we design a CNN module (•) to extract region-level features . The mainly consists of two 1D convolutional layers (with kernel sizes of 50 and 10, strides of 10 and 2, and output channels of 32 and 16, respectively) and a fully connected layer. Simultaneously, we also design an LSTM module (•) containing two LSTM layers (with 64 hidden units) to extract channel-level features ℎ . The formula is defined as follows:\n\nThe resulting representation vectors and ℎ serve as inputs to the linear classifier. It is worth noting that to accommodate the CNN+LSTM feature extractor, the input is ∈ R 2 × , where is the number of HbO (HbR) channels and is the signal length.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Transformer Extractor.",
      "text": "For the Transformer-based feature extractor, with ∈ R 2× × as input. Inspired by iTransformer  [36] , each fNIRS channel's complete sequence serves as a token for region-level modeling. Moreover, building on the idea in Informer  [37] , the information from multiple fNIRS channels at the same timestamp acts as a token for capturing channel-level feature information.\n\nSpecifically, the input is first reshaped into ∈ R × (2• ) and ℎ ∈ R × (2• ) . Then and ℎ are fed into the region-level encoding branch (•) and the channel-level encoding branch (•), respectively. The process is as follows:\n\nBoth encoding branches consist of a linear projection layer, positional encoding, the Transformer encoder  [38] , and a Global Average Pooling (GAP) layer  [39]  (see figure  2  for details).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Learning Objectives",
      "text": "Given a sample set D = {( , )} =1 containing instances (sample from different subjects) from classes, where ∈ {1, 2, ..., }. D as the input to networks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Learning From Labels.",
      "text": "Each network is optimized by Cross-Entropy loss (CE) between probability distributions and hard labels. The probability of class for sample given by the -th network is calculated as:\n\nwhere the logit from is the input of the \"softmax\" layer. Therefore, the CE loss of the -th network is computed as:\n\nwhere = ∈ {0, 1} represents the indicator function, which takes 1 when = holds, and 0 otherwise. Overall, CE loss of networks is:\n\n3.3.2 Distillation from so labels.\n\nWe simply construct an \"online teacher\" by softening the true label with temperature . The probability distribution of the soft label contains higher quality logit knowledge. It is as follows:\n\nwhere , ∈ {0, 1} is the -th element in the one-hot encoding of label . The soft probability of class corresponding to sample from network is:\n\nTo measure the alignment between the predictions of the network and the soft labels, while also improving generalization, we adopt the Kullback Leibler (KL) Divergence. The KL loss of is calculated as follows:\n\n) Thus, the total KL loss of networks is calculated as follows:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Inter-Subject Interaction Contrastive Representation Loss.",
      "text": "During network training, contrastive learning techniques can push positive pairs from the same class as the anchor closer together in the embedding space, while pushing negative pairs from different classes further apart. To this end, we propose a novel Inter-Subject Interaction Contrastive Representation Loss (IS-ICR). This loss is designed to enhance the representation similarity between samples of the same class from different subjects (as shown in Figure  3 ) while capturing the general distribution of the same class of emotions. Notably, IS-ICR also makes full use of the information interaction between various peer networks to learn better representations through cross-network interaction.  Obtain the embeddings and logits of each peer network;\n\n5:\n\nSoften labels and probability distributions;\n\n6:\n\nCompute the classification loss L (Eq.(  10 )); 7:\n\nCompute the distillation loss L (Eq.(  14 ));\n\n8:\n\nCompute two contrastive losses L and L ℎ (Eq.(  15 ));\n\n9:\n\nCompute the final loss function (Eq.(  18 )); 10:\n\nUpdate the parameters { } =1 by the AdamW optimizer.\n\n11: end for 12: / * Testing * / 13: Select the best performing model in { } =1 for deployment.\n\nGiven two networks and for illustration, where , ∈ {1, 2, • • • , }, ≠ . The embeddings generated from D are { } =1 and { } =1 , respectively. From the perspective of to (as shown in Figure  3 ), given the anchor embedding 1 of instance 1 from , and the embeddings in the contrastive embeddings { } =1 that is of the same class as 1 are considered as positive embeddings, otherwise as negative embeddings. Since the embeddings are preprocessed through 2 -normalization, the dot product measures the similarity distribution between the anchor and contrastive embeddings. The formula is defined as follows:\n\nwhere , denote the labels of anchor sample from and contrastive samples and from , respectively. represents the number of samples whose label is in D. is the temperature coefficient. Intuitively, each network can benefit from Eq.(  15 ) by gaining additional comparative knowledge from other networks. Generic inter-subject fNIRS representations that capture specific emotions using network clusters can help generalize to unseen emotional stimuli or unseen subjects to predict emotional categories. When scaling to networks, the overall loss function can be listed as follows:\n\nConsidering region-level and channel-level representations, two losses L and L ℎ are obtained.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Overall Loss.",
      "text": "The overall loss function L is a weighted sum of the loss terms mentioned above, which is illustrated as follows:  where 2 is used to balance the contribution of soft labels to the total loss, and and are the weighting coefficients for the regionlevel and channel-level contrastive representation losses, respectively.\n\nThe entire algorithmic process is shown in Algorithm 1. It is worth noting that OMCRD follows a one-stage training approach and does not require the pre-trained teacher model.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments 4.1 Implementation Details",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Datasets.",
      "text": "We conducted fNIRS emotion analysis on the NEMO dataset  [20] . The dataset records the brain activity of 31 subjects during two tasks: Emotional perception (Empe) and Affective imagery (Afim). For Empe, subjects viewed images from the International Affective Picture System (IAPS) database  [40]  passively. Afim participants evaluated emotional scenarios based on textual descriptions for subjective valence and arousal levels. As shown in Figure  4 , The fNIRS recordings were collected using a 24-channel device at 50 Hz sampling rate and grouped into four categories based on valence and arousal. High-Arousal Positive-Valence (HAPV), and High-Arousal Negative-Valence (HANV), Low-Arousal Positive-Valence (LAPV), Low-Arousal Negative-Valence (LANV). The dataset contains 1203 records for the Empe task and 720 records for the Afim task, as detailed in Table  1 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Details.",
      "text": "The dataset of around 31 participants (subjects numbered 26 and 41 completed only half of the experiment) is divided into training and testing subsets following an 8:2 proportion, iterated 5 times. To simulate real-world application scenarios, the samples used in the testing phase should come from subjects not seen during the training phase. We train the models using the AdamW  [41]  optimizer with decay parameters 1 =0.9 and 2 =0.999, and chose cosine learning rate decay  [42]  as the learning rate scheduler. To improve the model's generalization ability, we adopt a label smoothing  [43]  strategy. For the architecture that includes only 2 peer networks, we trained for 60 epochs and defined the weight decay as 2. For the architecture that provides for more than 2 peer networks, epochs is 90 and the weight decay is 3. , , and are set as 0.1, 0.2, and 0.2. For the Empe and Afim tasks, is set to 2 and 5, respectively. The learning rates for the CNN+LSTM and Transformer feature extractors are set to 0.00005 and 0.0002, respectively. The batch size for all experiments is 64. We use a specific sampler to ensure that each batch contains 16 samples from each class (4 classes), which is key to the effectiveness of IS-ICR. Our model is implemented using Python 3.10 and PyTorch 2.1, and it is trained on an NVIDIA RTX 3080Ti (16GB).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Quantitative Results And Analysis",
      "text": "To demonstrate the effectiveness of the proposed OMCRD, we compare it with several classic online distillation frameworks, including DML  [25] , KDCL  [44] , and MCL  [45] . Unless stated otherwise, we adopt the 3 peer networks ( = 3) design. All frameworks are   2  and 3 , it is evident that the proposed OMCRD achieves the best results in emotion recognition across different tasks.\n\nThe experimental results for both feature extractors yield three main conclusions. Due to mutual learning among networks, all frameworks show improvement compared to the baseline (peer networks are independent of each other). For example, the CNN+LSTM-based DML, despite its relatively poor performance, still sees an average improvement of 1.29% and 1.32% on the Empe and Afim. Secondly, various online knowledge distillation frameworks exhibit different adaptability levels with different extractors. With the Transformer extractor, MCL achieves suboptimal average accuracy rates of 33.17% and 32.83% on the Empe and Afim tasks, respectively, but fails to achieve the desired performance on the CNN+LSTM extractor.\n\nThirdly, OMCRD excels across various feature extractors and datasets, showcasing the effectiveness and robustness of the proposed framework. Specifically, OMCRD proposes a contrastive representation learning method for cross-subject generalization, which employs a multi-peer model to capture similar features across subjects for the same emotion. Alignment of similar emotion distributions is encouraged through cross-network learning. In contrast, MCL and KDCL focus only on the knowledge of logit distribution, and these methods ignore the information of intermediate features. Suffering from incomplete information transfer, their classification performance is relatively low. For DML, each peer network learns from each other by aligning the output with other peer networks. For KDCL, the original inputs are first augmented with different random seeds separately to enhance the network's invariance to input perturbations. The predictions of the peer models are also effectively integrated to generate a soft target to ensure that multiple sub-networks benefit from collaborative learning. In addition, MCL proposes Interactive Contrastive Learning (ICL) based on Vanilla Contrastive Learning (VCL). ICL maximizes the lower bound on the mutual information between two networks by consolidating inter-network representation information. Meanwhile, distributions can be considered as soft labels to guide other distributions. However, MCL ignores the fNIRS region-level and channel-level representation information between subjects, which limits its performance. In summary, the proposed OMCRD improves   4  show metrics for training and inference phases. Experiments reveal faster inference computation with compression ratios of 67.88% and 68.71%. These findings highlight the framework's ability to compress models through online knowledge distillation, enhancing performance on wearable devices in resource-constrained settings.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of Loss Items.",
      "text": "We conduct additional ablation experiments for two tasks on the Nemo dataset to understand better the contribution of different loss terms in our proposed framework. Specifically, several different variants are designed in the same experimental setup:\n\n• w/o L , L ℎ : Using only region-level representations.\n\n• w/o L , L : Using only channel-level representations.\n\n• w/o L : Distillation without logits knowledge. As shown in Table  5 , each loss term in the proposed OMCRD is indispensable. These loss terms convey valuable logits or feature knowledge. The peer models learn high-quality soft label knowledge through L . L and L ℎ effectively transfer multi-level representations information of fNIRS signals between different models, which helps information interaction between peers.  Figure  5  illustrates that combining several peer networks can markedly surpass the basic model's performance. This outcome confirms our hypothesis that sharing contrastive knowledge among peer networks enhances overall generalization ability. Nonetheless, with an increasing number of peers in the queue, the improvement diminishes gradually, reaching a point of stabilization or even demonstrating negative growth. The main experiment has produced impressive results using three student networks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Visualization",
      "text": "To further illustrate the effectiveness of the proposed OMCRD, we visualize the raw and trained features of five subjects through t-SNE (Figure  6 ). We represented different emotion categories with four colors and used five shapes to distinguish samples with different subjects. Figure  6  (a) shows the scattered distribution of mean features extracted from the raw fNIRS signals in the t-SNE embedding space. In contrast, Figure  6  (b) illustrates a clustering phenomenon in the distribution of various categories following the OCM-CRD, although not completely separable. Specifically, fNIRS data from different subjects under the same emotional stimulus exhibit more similar feature distributions. Notably, the blue data points are farther from the orange data points in the embedding space compared to the green (or pink) data points. This validates that the IS-ICR contrastive learning strategy pulls the less similar features further apart. Overall, the proposed method effectively mitigates inter-subject differences without losing emotion separability, thereby facilitating cross-subject emotion recognition. This is beneficial for deploying the model onto wearable physiological monitoring devices to predict emotion categories for unseen subjects.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we propose a novel method (OMCRD) for lightweight the fNIRS emotion recognition model. The framework allows multiple students to jointly optimize the network and teach cross-subject knowledge at region and channel levels interactively. Furthermore, the IS-ICR loss boosts the similarity between identical emotional representations from various individuals, notably improving the model's capability for cross-subject adaptive recognition. Our framework shows effective emotion recognition, robustness across subjects, and deployability in extensive experiments. The proposed OMCRD is a general framework for multi-channel fNIRS sequences that can be extended to large-scale models in fields like mental health diagnosis and cognitive assessment.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources",
      "page": 3
    },
    {
      "caption": "Figure 1: , each peer/student network 푓(·) consists",
      "page": 3
    },
    {
      "caption": "Figure 2: The architecture of the encoding branch unit in the",
      "page": 4
    },
    {
      "caption": "Figure 3: ) while capturing the general distribution of the same",
      "page": 5
    },
    {
      "caption": "Figure 3: Overview of the proposed IS-ICR. 푓푎and 푓푏repre-",
      "page": 5
    },
    {
      "caption": "Figure 3: ), given the anchor embedding 푣1푎of instance 푥1 from 푓푎,",
      "page": 5
    },
    {
      "caption": "Figure 4: The channel location of fNIRS. Orange circles are",
      "page": 6
    },
    {
      "caption": "Figure 4: , The fNIRS recordings were collected using a 24-channel device",
      "page": 6
    },
    {
      "caption": "Figure 5: Ablation study results with varying numbers of",
      "page": 7
    },
    {
      "caption": "Figure 5: illustrates that combining several peer networks can",
      "page": 8
    },
    {
      "caption": "Figure 6: ). We represented diﬀerent emotion categories with",
      "page": 8
    },
    {
      "caption": "Figure 6: (a) shows the scattered distribution of mean",
      "page": 8
    },
    {
      "caption": "Figure 6: (b) illustrates a clustering phenom-",
      "page": 8
    },
    {
      "caption": "Figure 6: (a) t-SNE results for the original data. (b) t-SNE",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Online Multi-level Contrastive Representation Distillation for": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for": "Zhili Lai"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for": "School of Electronic and Information"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for": "Engineering, South China University"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for": "of Technology"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for": "Guangzhou, China"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for": "202321012469@mail.scut.edu.cn"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of Technology": "Guangzhou, China",
          "of Technology & Pazhou Lab\nof Technology": "Guangzhou, China\nGuangzhou, China"
        },
        {
          "of Technology": "202321012469@mail.scut.edu.cn",
          "of Technology & Pazhou Lab\nof Technology": "qchm@scut.edu.cn\ntjeepscut@gmail.com"
        },
        {
          "of Technology": "Wanxiang Luo",
          "of Technology & Pazhou Lab\nof Technology": "Xiangmin Xu"
        },
        {
          "of Technology": "School of Electronic and Information",
          "of Technology & Pazhou Lab\nof Technology": "School of Future Technology, South"
        },
        {
          "of Technology": "Engineering, South China University",
          "of Technology & Pazhou Lab\nof Technology": "China University of Technology &"
        },
        {
          "of Technology": "of Technology",
          "of Technology & Pazhou Lab\nof Technology": "Pazhou Lab"
        },
        {
          "of Technology": "Guangzhou, China",
          "of Technology & Pazhou Lab\nof Technology": "Guangzhou, China"
        },
        {
          "of Technology": "wanxiangluo@foxmail.com",
          "of Technology & Pazhou Lab\nof Technology": "xmxu@scut.edu.cn"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Guangzhou, China": "xmxu@scut.edu.cn"
        },
        {
          "Guangzhou, China": "KEYWORDS"
        },
        {
          "Guangzhou, China": "fNIRS, Emotion Recognition, Online Knowledge Distillation, Con-"
        },
        {
          "Guangzhou, China": "trastive Learning"
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "ACM Reference Format:"
        },
        {
          "Guangzhou, China": "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu."
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "2024. Online Multi-level Contrastive Representation Distillation for Cross-"
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "Subject fNIRS Emotion Recognition. In Proceedings of the 1st International"
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "Workshop on Brain-Computer Interfaces (BCI) for Multimedia Understanding"
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "(BCIMM ’24), October 28, 2024, Melbourne, VIC, Australia. ACM, New York,"
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "NY, USA, 9 pages. https://doi.org/10.1145/3688862.3689110"
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "1\nINTRODUCTION"
        },
        {
          "Guangzhou, China": ""
        },
        {
          "Guangzhou, China": "Emotion recognition is an important research task in aﬀective com-"
        },
        {
          "Guangzhou, China": "puting, which aims to represent and explain human mental states"
        },
        {
          "Guangzhou, China": "by acquiring psychological or non-psychological signals. Accurate"
        },
        {
          "Guangzhou, China": "emotion recognition can help us better sense a person’s brain ac-"
        },
        {
          "Guangzhou, China": "tivity and thoughts. Given contemporary society’s challenging em-"
        },
        {
          "Guangzhou, China": "ployment and living conditions, emotion recognition technology"
        },
        {
          "Guangzhou, China": "is increasingly prevalent across diverse industries. Among them,"
        },
        {
          "Guangzhou, China": "common applications include health diagnostics, safe driving, med-"
        },
        {
          "Guangzhou, China": "ical services, and human-computer interaction.[1–5]."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to a wearable fNIRS physiological monitoring device.": "In summary, our primary contributions are as follows:",
          "oneering wearable bimodal system. The system combined fNIRS": "and EEG technology and used the temporal convolutional network"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "",
          "oneering wearable bimodal system. The system combined fNIRS": "to identify implicit emotional states in real-time.\nfNIRS is crucial"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "(1) We propose a novel fNIRS emotion recognition method (OM-",
          "oneering wearable bimodal system. The system combined fNIRS": "for processing emotional brain responses and shows promise in"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "CRD), which is an online multi-level contrastive representa-",
          "oneering wearable bimodal system. The system combined fNIRS": "emotion recognition. However, due to minimal AI\nresearch and"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "tion distillation framework with the multiple student net-",
          "oneering wearable bimodal system. The system combined fNIRS": "challenges in feature extraction, algorithm accuracy is currently"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "works for learning multi-view fNIRS features and reducing",
          "oneering wearable bimodal system. The system combined fNIRS": "very low. Implementing a multi-student network strategy for opti-"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "model complexity. To our knowledge,\nit is the ﬁrst work to",
          "oneering wearable bimodal system. The system combined fNIRS": "mizing training with multi-view features could be beneﬁcial."
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "apply knowledge distillation in fNIRS emotion recognition.",
          "oneering wearable bimodal system. The system combined fNIRS": ""
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "(2) A novel\nInter-Subject\nInteraction Contrastive Representa-",
          "oneering wearable bimodal system. The system combined fNIRS": ""
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "",
          "oneering wearable bimodal system. The system combined fNIRS": "2.2\nKnowledge Distillation"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "tion loss (IS-ICR)\nis proposed, which is a multi-level con-",
          "oneering wearable bimodal system. The system combined fNIRS": ""
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "trastive representation learning.\nIS-ICR eﬀectively enables",
          "oneering wearable bimodal system. The system combined fNIRS": "The Knowledge Distillation (KD) method was initially proposed by"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "diﬀerent student models to obtain region-level and channel-",
          "oneering wearable bimodal system. The system combined fNIRS": "Hinton et al. [21],\nfocusing on distilling knowledge from a large"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "level knowledge across subjects and optimizes multi-view",
          "oneering wearable bimodal system. The system combined fNIRS": "teacher model to improve a smaller student network. KD methods"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "features between student models.",
          "oneering wearable bimodal system. The system combined fNIRS": "facilitate the deployment of large-parameter models in practical ap-"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "(3) Extensive experiments are conducted on a publicly available",
          "oneering wearable bimodal system. The system combined fNIRS": "plications. In the KD research based on physiological signals, Gu"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "fNIRS dataset\nto assess the proposed method. The results",
          "oneering wearable bimodal system. The system combined fNIRS": "et al. [22] designed a novel frame-level teacher-student framework,"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "demonstrated the eﬀectiveness and robustness of\nthe pro-",
          "oneering wearable bimodal system. The system combined fNIRS": "achieving the best performance in subject-independent EEG emo-"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "posed framework.",
          "oneering wearable bimodal system. The system combined fNIRS": "tion recognition. Wang et al. [23] attempted to distill\nthe knowl-"
        },
        {
          "to a wearable fNIRS physiological monitoring device.": "",
          "oneering wearable bimodal system. The system combined fNIRS": "edge from ResNet34 into a smaller model ResNet8 in EEG emotion"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "While fNIRS has been crucial\nin measuring physiological sig-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "signals emotion recognition, knowledge distillation, and contrastive"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "nals, there remain numerous challenges in advancing fNIRS data",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "learning."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "research. Current deep learning-based fNIRS research primarily",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "concentrates on distinct tasks, like Brain-Computer Interface (BCI)",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "2.1\nPhysiological Signals Emotion Recognition"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[10] and mental health diagnosis [11], while exploration in fNIRS",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "emotion recognition is still in the early stages of development. With",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Recently, physiological signals emotion recognition has gradually"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "the rapid development of portable wearable physiological devices,",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "become a research hotspot in aﬀective computing. Unlike emotion"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "lightweight network models can maintain reasonable measurement",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "recognition of behavioral modalities [6, 8, 12], Physiological sig-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "accuracy while adapting to the resource constraints of wearable de-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "nals are challenging to disguise and cannot be intentionally or con-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "vices and application requirements. Finally, the fNIRS recordings",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "sciously controlled. Physiological signals include electroencephalo-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "related to emotions exhibit substantial\ninter-subject variabilities",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "gram (EEG) [9, 13], electrodermal activity (EDA) [14], electrocar-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "due to the physiological diﬀerences among individuals. This is a",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "diogram (ECG) [15, 16], Galvanic Skin Response (GSR) [17]. They"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "considerable challenge for cross-subject emotion recognition.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "are frequently used in emotion recognition systems because they"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "To address the issues mentioned above,\nthis paper proposes a",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "can mimic the real physiological changes of human emotions. Like,"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "novel Online Multi-level Contrastive Representation Distillation",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Xiao et al. [16] enhanced emotion recognition performance by ac-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "framework for fNIRS emotion recognition, named OMCRD. We ex-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "quiring spatial-temporal representations of various ECG regions"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "plore and experiment with fNIRS emotion recognition to improve",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "and implementing a dynamic weight allocation layer\nto modify"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "deﬁciencies in this ﬁeld. Speciﬁcally, the OMCRD framework em-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "the inﬂuence of each ECG region. To ensure the recognition of"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "ploys a one-stage online multi-student network mutual Knowledge",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "emotional states in a long-time series, Susanto et al. [17] analyzed"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Distillation (KD)\nstrategy with the multi-level\n(region-level and",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "GSR signals based on a 1D convolutional neural network and a"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "channel-level)\nfNIRS feature extractor, which exploits the mutual",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "residual bidirectional gated loop unit. Meanwhile, Yu et al. [14] in-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "learning between multiple lightweight\nstudent networks during",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "vestigated the performance of diﬀerent deep neural networks on a"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "training to eliminate the dependence on complex teacher models.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "subject-independent EDA-based emotion classiﬁcation task. Gong"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Furthermore, we also propose the Inter-Subject\nInteraction Con-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "et al. [9] proposed a spatio-temporal two-stream fusion network"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "trastive Representation loss (IS-ICR).\n\"Inter-Subject\" means\nthat",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "based on an attention mechanism for EEG-based emotion recogni-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "fNIRS signals collected from diﬀerent subjects facing the same stim-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "tion."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "ulus are treated as the same class, facilitating the learning of their",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Notably, as an emerging non-invasive brain imaging technol-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "similarities. \"Interaction\" refers to establishing collaborative learn-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "ogy, fNIRS has gradually gained attention in emotion recognition"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "ing relationships across peer networks. Diﬀerent networks can com-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "research due to its advantages of ﬂexibility, ease of operation, and"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "municate with each other and optimize parameters during training.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "low cost. Si et al. [18] combined the CNN branch and the statistical"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "This ensures that each network learns additional contrastive repre-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "branch to construct a dual-branch joint network for cross-subject"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "sentation knowledge from other peer networks. This approach fa-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "fNIRS emotion recognition, which is the ﬁrst introduction of deep"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "cilitates the deployment of the ﬁnal best-performing student model",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "learning techniques in the ﬁeld. Chen et al. [19] presented a pi-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "to a wearable fNIRS physiological monitoring device.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "oneering wearable bimodal system. The system combined fNIRS"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "In summary, our primary contributions are as follows:",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "and EEG technology and used the temporal convolutional network"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "to identify implicit emotional states in real-time.\nfNIRS is crucial"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "(1) We propose a novel fNIRS emotion recognition method (OM-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "for processing emotional brain responses and shows promise in"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "CRD), which is an online multi-level contrastive representa-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "emotion recognition. However, due to minimal AI\nresearch and"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "tion distillation framework with the multiple student net-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "challenges in feature extraction, algorithm accuracy is currently"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "works for learning multi-view fNIRS features and reducing",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "very low. Implementing a multi-student network strategy for opti-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "model complexity. To our knowledge,\nit is the ﬁrst work to",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "mizing training with multi-view features could be beneﬁcial."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "apply knowledge distillation in fNIRS emotion recognition.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "(2) A novel\nInter-Subject\nInteraction Contrastive Representa-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "2.2\nKnowledge Distillation"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "tion loss (IS-ICR)\nis proposed, which is a multi-level con-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "trastive representation learning.\nIS-ICR eﬀectively enables",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "The Knowledge Distillation (KD) method was initially proposed by"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "diﬀerent student models to obtain region-level and channel-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Hinton et al. [21],\nfocusing on distilling knowledge from a large"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "level knowledge across subjects and optimizes multi-view",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "teacher model to improve a smaller student network. KD methods"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "features between student models.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "facilitate the deployment of large-parameter models in practical ap-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "(3) Extensive experiments are conducted on a publicly available",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "plications. In the KD research based on physiological signals, Gu"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "fNIRS dataset\nto assess the proposed method. The results",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "et al. [22] designed a novel frame-level teacher-student framework,"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "demonstrated the eﬀectiveness and robustness of\nthe pro-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "achieving the best performance in subject-independent EEG emo-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "posed framework.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "tion recognition. Wang et al. [23] attempted to distill\nthe knowl-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "edge from ResNet34 into a smaller model ResNet8 in EEG emotion"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "recognition tasks to achieve performance improvement and model"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "2\nRELATED WORK",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "compression. Liu et al. [24] proposed a cross-modal (GSR and EEG)"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "In this segment,\nto enhance comprehension of\nthe proposed ap-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "knowledge distillation framework, eﬀectively transferring hetero-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "proach, we mainly introduce several related works: physiological",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "geneous and interactive knowledge from multimodal to unimodal"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "are taken from [20]. (b) and (c) show two diﬀerent types of multi-level fNIRS feature extractors."
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "GSR models, enhancing emotion recognition performance while"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "reducing dependence on EEG signals."
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "However, these methods employ an oﬄine learning strategy and"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "require pre-training of a suitable and powerful\n\"teacher\" model."
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "Moreover, this two-stage training process is very time-consuming,"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "which is challenging in real-time fNIRS emotion recognition tasks."
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "Compared to traditional KD, online KD aims to improve perfor-"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "mance by leveraging collaborative learning across multiple student"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "networks. Online KD employs one-stage training and does not rely"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "on a pre-trained teacher model. Therefore, inspired by the success"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "of online KD in the image domain [25–28], we design a novel on-"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "line distillation framework to ensure knowledge transfer and inter-"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "action optimization among multi-student networks."
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "2.3\nContrastive Learning"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "The idea of contrastive learning is to learn which data pairs are"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "similar or diﬀerent,\nthereby acquiring the general\nfeatures of the"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "dataset.\nIt\nis a form of self-supervised learning algorithm.\nIt has"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "achieved outstanding performance in diverse ﬁelds, including Com-"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "puter Vision (CV) [29], Natural Language Processing (NLP) [30],"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "and bioinformatics [31]. Khosla et al [32] extended contrastive loss"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "to supervised environments, eﬀectively using label information to"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "extract more discriminative representations. Recently, contrastive"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "learning has also been gradually applied to physiological signals"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "studies. For\ninstance, Soltanieh et al\n[33]\ninvestigated the eﬀec-"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "tiveness of various data augmentation techniques for contrastive"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "self-supervised learning of ECG signals. This enables the model to"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "enhance learning of the generalized ECG representation, thereby"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "enhancing the accuracy of arrhythmia detection. Kalanadhabhatta"
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": ""
        },
        {
          "Figure 1: (a) Overview of the proposed OMCRD for fNIRS emotion recognition. The pictures of emotional stimulus sources": "et al.\n[11]\nintroduced a multi-task supervised contrastive learn-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "(3)\n𝒗𝑐ℎ = 𝜙𝑐ℎ (𝒆𝑐ℎ)",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "encoding branch 𝜑𝑅𝐸𝐵 (·) and the channel-level encoding branch"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝜑𝐶𝐸𝐵 (·), respectively. The process is as follows:"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "where 𝜙 is composed of a linear layer and the 𝑙2-normalization to",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "linearly transform these embeddings into contrastive embeddings",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "(6)\n𝒆𝑟𝑔 = 𝜑𝑅𝐸𝐵 (𝒙𝑟𝑔)"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "𝒗𝑟𝑔, 𝒗𝑐ℎ ∈ R𝑑 , and 𝑑 is the embedding size. The embeddings 𝒗𝑟𝑔",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "and 𝒗𝑐ℎ are used for the calculation of the proposed IS-ICR.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "(7)\n𝒆𝑐ℎ = 𝜑𝐶𝐸𝐵 (𝒙𝑐ℎ)"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Both encoding branches consist of a linear projection layer, posi-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "3.1.2\nTraining and deployment.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "tional encoding, the Transformer encoder [38], and a Global Aver-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "During training, 𝑀 (𝑀 > 2) peer networks are optimized to-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "age Pooling (GAP) layer [39] (see ﬁgure 2 for details)."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "gether. Notably, to learn diverse representations, all\nidentical net-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "works {𝑓𝑚 }𝑀\n𝑚=1 are assigned diﬀerent weights initially, a crucial",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "factor for mutual\nlearning success.\nIn the evaluation phase, pro-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "(cid:2)\n(cid:1)"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Transformer\nGlobal Average"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Linear"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "jection modules 𝜙 are omitted, allowing the assessment of each",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Encoder\nPooling (GAP)"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "network independently. The optimal performing network can be",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Positional"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Encoding"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "selected for ﬁnal deployment. As the structure of the preserved net-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "work is identical\nto the others, there are no extra computational",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "expenses during testing.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Multi-head"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Norm\nNorm\nMLP"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Attention"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "3.2\nMulti-level fNIRS Feature Extractor",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "The fNIRS signal has two important features: region-level features",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Figure 2: The architecture of the encoding branch unit in the"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "and channel-level features. Region-level features represent the spa-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Transformer extractor."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "tial correlation between diﬀerent\nfNIRS signal channels. Over a",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "while, changes in blood oxygen concentration from diﬀerent chan-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "nels often exhibit diﬀerent dependencies, such as the higher cor-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "relation between channels from similar functional brain regions",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "3.3\nLearning Objectives"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "or neighboring spatial locations. Channel-level features represent",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Given a sample set D = {(𝒙𝑖, 𝑦𝑖)}𝑁\n𝑖=1 containing 𝑁 instances (sam-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "the temporal continuity of blood oxygen concentration changes",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "ple from diﬀerent subjects) from 𝐶 classes, where 𝑦𝑖 ∈ {1, 2, ..., 𝐶 }."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "within multiple fNIRS signal channels. We designed two feature",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "D as the input to 𝑀 networks."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "extractors to capture these two types of features. One is a hybrid",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "feature extractor based on CNN+LSTM, and the other is a feature",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "3.3.1\nLearning from labels."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "extractor based on Transformers.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Each network is optimized by Cross-Entropy loss (CE) between"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "probability distributions and hard labels. The probability of class 𝑐"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "3.2.1\nCNN+LSTM extractor.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "for sample 𝒙𝑖 given by the 𝑚-th network is calculated as:"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "For the hybrid feature extractor with CNN and LSTM, we design",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝐶"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "a CNN module 𝜑𝐶𝑁 𝑁 (·)\nto extract region-level\nfeatures 𝒆𝑟𝑔. The",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑝𝑐\n𝑒𝑥𝑝 (𝑧𝑖,𝑐\n(8)\n𝑚 )/[\n𝑚 )]\n𝑚 (𝒙𝑖 ) = 𝑒𝑥𝑝 (𝑧𝑖,𝑐"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "𝜑𝐶𝑁 𝑁 mainly consists of two 1D convolutional layers (with kernel",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑐=1"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "sizes of 50 and 10, strides of 10 and 2, and output channels of 32",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Õ"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "and 16, respectively) and a fully connected layer. Simultaneously,",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "the \"softmax\"\nlayer.\nwhere the logit 𝒛𝑚 from 𝑓𝑚 is the input of"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "we also design an LSTM module 𝜑𝐿𝑆𝑇 𝑀 (·) containing two LSTM",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Therefore, the CE loss of the 𝑚-th network is computed as:"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "layers (with 64 hidden units) to extract channel-level\nfeatures 𝒆𝑐ℎ.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑁\n𝐶"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "The formula is deﬁned as follows:",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "L𝑚\n= −\n𝑝𝑐"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "1 𝑁\n(9)\n𝐼𝑦𝑖 =𝑐 log\n𝑚 (𝒙𝑖 )\n𝐶𝐸"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑖=1\n𝑐=1"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "(4)\n𝒆𝑟𝑔 = 𝜑𝐶𝑁 𝑁 (𝒙)",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Õ\nÕ"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "(cid:0)\n(cid:1)"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "where 𝐼𝑦𝑖 =𝑐 ∈ {0, 1} represents the indicator function, which takes"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "(5)\n𝒆𝑐ℎ = 𝜑𝐿𝑆𝑇 𝑀 (𝒙)",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "1 when 𝑦𝑖 = 𝑐 holds, and 0 otherwise. Overall, CE loss of 𝑀 net-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "The resulting representation vectors 𝒆𝑟𝑔 and 𝒆𝑐ℎ serve as inputs",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "works is:"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑀"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "to the linear classiﬁer. It is worth noting that to accommodate the",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "L𝑚\n(10)\nL𝐶𝐸 ="
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "CNN+LSTM feature extractor, the input is 𝒙 ∈ R2𝑛×𝑇 , where 𝑛 is",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝐶𝐸"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑚=1"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "the number of HbO (HbR) channels and 𝑇 is the signal length.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Õ"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "3.3.2\nDistillation from soft labels."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "3.2.2\nTransformer extractor.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "We simply construct an \"online teacher\" by softening the true"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "For the Transformer-based feature extractor, with 𝒙 ∈ R2×𝑛×𝑇",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "label with temperature 𝑇 . The probability distribution of the soft"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "as input. Inspired by iTransformer [36], each fNIRS channel’s com-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "label contains higher quality logit knowledge. It is as follows:"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "plete sequence serves as a token for region-level modeling. More-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝐶"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "over, building on the idea in Informer [37], the information from",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑒𝑥𝑝 (𝑦𝑖,𝑐/𝑇 )]\n(11)\n𝑝𝑐 (𝑦𝑖) = 𝑒𝑥𝑝 (𝑦𝑖,𝑐/𝑇 )/["
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "multiple fNIRS channels at the same timestamp acts as a token for",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "𝑐=1"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "capturing channel-level feature information.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Õ"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "the input 𝒙 is ﬁrst reshaped into 𝒙𝑟𝑔 ∈ R𝑛× (2·𝑇 )",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Speciﬁcally,",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "where 𝑦𝑖,𝑐 ∈ {0, 1} is the 𝑐-th element in the one-hot encoding 𝒚𝑖"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "and 𝒙𝑐ℎ ∈ R𝑇 × (2·𝑛) . Then 𝒙𝑟𝑔 and 𝒙𝑐ℎ are fed into the region-level",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "of label 𝑦𝑖. The soft probability of class 𝑐 corresponding to sample"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝒙𝑖\nfrom network 𝑓𝑚 is:",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Algorithm 1 The Proposed OMCRD Algorithm"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝐶",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Input: Training data D = {(𝒙𝑖, 𝑦𝑖)}𝑁"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "𝑖=1 from diﬀerent subjects."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝑝𝑐\n𝑒𝑥𝑝 (𝑧𝑖,𝑐\n(12)\n𝑚 /𝑇 )/[\n𝑚 /𝑇 )]\n𝑚 (𝒙𝑖 ) = 𝑒𝑥𝑝 (𝑧𝑖,𝑐",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Output: Trained models {𝑓𝑚 }𝑀\n𝑚=1 with parameters {𝜃𝑚 }𝑀\n𝑚=1."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝑐=1",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "Õ",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "1:\n/∗Training∗/"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "To measure the alignment between the predictions of the network",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "2:\nInitialisation: Randomly initialise parameters"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "labels, while also improving generalization, we\n𝑓𝑚 and the soft",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "3:\nfor 1 → 𝐸𝑝𝑜𝑐ℎ𝑚𝑎𝑥 do"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "adopt the Kullback Leibler (KL) Divergence. The KL loss of\n𝑓𝑚 is",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "4:\nObtain the embeddings and logits of each peer network;"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "calculated as follows:",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "5:\nSoften labels and probability distributions;"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝑁\n𝐶",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "6:\nCompute the classiﬁcation loss L𝐶𝐸 (Eq.(10));"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝑝𝑐 (𝑦𝑖)",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "= 1\nL𝑚",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝑝𝑐 (𝑦𝑖) log\n𝑝𝑐 (𝑦𝑖)k ˜𝑝𝑐\n= 𝐷𝐾𝐿\n𝑚 (𝒙𝑖 )",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "7:\nCompute the distillation loss L𝐾𝐿 (Eq.(14));"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝐾𝐿\n𝑁\n𝑝𝑐\n𝑚 (𝒙𝑖)",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝑖=1\n𝑐=1",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Compute two contrastive losses L𝑟𝑔"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "Õ\nÕ",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "8:\n𝐶𝑅 and L𝑐ℎ\n𝐶𝑅 (Eq.(15));"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "(cid:0)\n(cid:1)\n(13)",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "9:\nCompute the ﬁnal loss function (Eq.(18));"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "Thus, the total KL loss of 𝑀 networks is calculated as follows:",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Update the parameters {𝜃𝑚 }𝑀"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "10:\n𝑚=1 by the AdamW optimizer."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "𝑀",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": ""
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition": "",
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "11:\nend for"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Õ": ""
        },
        {
          "Õ": "3.3.3\nInter-subject interaction contrastive representation loss."
        },
        {
          "Õ": "During network training, contrastive learning techniques can"
        },
        {
          "Õ": "push positive pairs from the same class as the anchor closer to-"
        },
        {
          "Õ": ""
        },
        {
          "Õ": "gether in the embedding space, while pushing negative pairs from"
        },
        {
          "Õ": ""
        },
        {
          "Õ": ""
        },
        {
          "Õ": "diﬀerent classes\nfurther apart. To this end, we propose a novel"
        },
        {
          "Õ": ""
        },
        {
          "Õ": "Inter-Subject Interaction Contrastive Representation Loss (IS-ICR)."
        },
        {
          "Õ": "This loss is designed to enhance the representation similarity be-"
        },
        {
          "Õ": "tween samples of the same class from diﬀerent subjects (as shown"
        },
        {
          "Õ": ""
        },
        {
          "Õ": "in Figure 3) while capturing the general distribution of the same"
        },
        {
          "Õ": ""
        },
        {
          "Õ": "class of emotions. Notably, IS-ICR also makes full use of the infor-"
        },
        {
          "Õ": ""
        },
        {
          "Õ": "mation interaction between various peer networks to learn better"
        },
        {
          "Õ": ""
        },
        {
          "Õ": "representations through cross-network interaction."
        },
        {
          "Õ": ""
        },
        {
          "Õ": ""
        },
        {
          "Õ": "Class:"
        },
        {
          "Õ": "Subject #1"
        },
        {
          "Õ": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Comparison of online knowledge distillation meth-": "ods based on CNN+LSTM extractor."
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "Methods"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "Baseline"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "DML [25]"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "KDCL [44]"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "MCL [45]"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "Ours"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "Baseline"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "DML [25]"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "KDCL [44]"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "MCL [45]"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": "Ours"
        },
        {
          "Table 2: Comparison of online knowledge distillation meth-": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Aﬁm": "",
          "KDCL [44]": "MCL [45]",
          "35.03": "31.94",
          "34.03": "33.75",
          "34.72": "34.72",
          "31.94": "32.64",
          "33.53": "33.14"
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "Ours",
          "35.03": "35.42",
          "34.03": "34.03",
          "34.72": "35.42",
          "31.94": "34.03",
          "33.53": "35.00"
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "Table 3: Comparison of online knowledge distillation meth-",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "ods based on Transformer extractor.",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "",
          "34.03": "",
          "34.72": "Testing Conditions",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "Datasets",
          "KDCL [44]": "Methods",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "Fold 1",
          "34.03": "Fold 2",
          "34.72": "Fold 3",
          "31.94": "Fold 5",
          "33.53": "Avg."
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "",
          "35.03": "",
          "34.03": "",
          "34.72": "",
          "31.94": "",
          "33.53": ""
        },
        {
          "Aﬁm": "",
          "KDCL [44]": "Baseline",
          "35.03": "31.67",
          "34.03": "31.28",
          "34.72": "32.52",
          "31.94": "33.75",
          "33.53": "32.18"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Computational performance and parameter cost re-": "sults. # Compress denotes the compression ratio.",
          "Table 5: Results of ablation study on loss terms.": ""
        },
        {
          "Table 4: Computational performance and parameter cost re-": "",
          "Table 5: Results of ablation study on loss terms.": "CNN+LSTM\nTransformer"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "",
          "Table 5: Results of ablation study on loss terms.": "Variants"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "CNN+LSTM\nTransformer",
          "Table 5: Results of ablation study on loss terms.": ""
        },
        {
          "Table 4: Computational performance and parameter cost re-": "",
          "Table 5: Results of ablation study on loss terms.": "Empe\nAﬁm\nEmpe\nAﬁm"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "Metric",
          "Table 5: Results of ablation study on loss terms.": ""
        },
        {
          "Table 4: Computational performance and parameter cost re-": "Training\nInference\nTraining\nInference",
          "Table 5: Results of ablation study on loss terms.": ""
        },
        {
          "Table 4: Computational performance and parameter cost re-": "",
          "Table 5: Results of ablation study on loss terms.": "Baseline\n31.13\n31.66\n32.18\n29.66"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "Param. (K)\n515.29\n165.49\n306.73\n95.97",
          "Table 5: Results of ablation study on loss terms.": "32.83\n33.20\n32.89\n32.55\nw/o L𝐾𝐿, L𝑐ℎ"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "",
          "Table 5: Results of ablation study on loss terms.": "𝐶𝑅"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "MACs (M)\n13.3\n4.43\n36.21\n12.07",
          "Table 5: Results of ablation study on loss terms.": "32.92\n32.98\n33.26\n32.64\nw/o L𝐾𝐿, L𝑟𝑔"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "",
          "Table 5: Results of ablation study on loss terms.": "𝐶𝑅"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "FLOPs (M)\n250.11\n83.37\n73.25\n24.42",
          "Table 5: Results of ablation study on loss terms.": "33.08\n34.28\n33.75\n32.98\nw/o L𝐾𝐿"
        },
        {
          "Table 4: Computational performance and parameter cost re-": "# Compress\n67.88%\n68.71%",
          "Table 5: Results of ablation study on loss terms.": "Completed\n34.08\n35.00\n34.24\n34.01"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "# Compress\n67.88%\n68.71%",
          "33.08\nw/o L𝐾𝐿": "Completed\n34.08",
          "34.28": "35.00",
          "33.75": "34.24",
          "32.98": "34.01"
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "the performance over other online distillation frameworks by ap-",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "proximately 1.1%-1.5%. Therefore, OMCRD demonstrates high per-",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "experimented with several times based on CNN+LSTM and Trans-",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "formance and robustness in sentiment recognition tasks.",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "former feature extractors. The experimental results are all evalu-",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "Furthermore, we assess the OMCRD based on parameters, com-",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "ated in terms of accuracy, with bold results indicating the best and",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "putational speed, and compression ratio. Results in Table 4 show",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "underlined results representing the second-best results. As shown",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "metrics for training and inference phases. Experiments reveal faster",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "in Tables 2 and 3, it is evident that the proposed OMCRD achieves",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "inference computation with compression ratios of 67.88% and 68.71%.",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "the best results in emotion recognition across diﬀerent tasks.",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "These ﬁndings highlight the framework’s ability to compress mod-",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "The\nexperimental\nresults\nfor\nboth\nfeature\nextractors",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "els through online knowledge distillation, enhancing performance",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "three main conclusions. Due to mutual learning among networks,",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "on wearable devices in resource-constrained settings.",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "all\nframeworks\nshow improvement\ncompared\nto\nthe",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "(peer networks are independent of each other). For example,",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "Ablation Studies",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "CNN+LSTM-based DML, despite its relatively poor performance,",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "Impact of loss items.",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "still sees an average improvement of 1.29% and 1.32% on the Empe",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "We conduct additional ablation experiments for two tasks on",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "and Aﬁm. Secondly, various online knowledge distillation frame-",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "the Nemo dataset to understand better the contribution of diﬀer-",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "works exhibit diﬀerent adaptability levels with diﬀerent extractors.",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "ent loss terms in our proposed framework. Speciﬁcally, several dif-",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "With the Transformer extractor, MCL achieves suboptimal average",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "",
          "33.08\nw/o L𝐾𝐿": "ferent variants are designed in the same experimental setup:",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "accuracy rates of 33.17% and 32.83% on the Empe and Aﬁm tasks,",
          "33.08\nw/o L𝐾𝐿": "",
          "34.28": "",
          "33.75": "",
          "32.98": ""
        },
        {
          "FLOPs (M)\n250.11\n83.37\n73.25\n24.42": "respectively, but\nfails to achieve the desired performance on the",
          "33.08\nw/o L𝐾𝐿": "• w/o L𝐾𝐿, L𝑐ℎ",
          "34.28": "𝐶𝑅: Using only region-level representations.",
          "33.75": "",
          "32.98": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": ""
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "MCL and KDCL focus only on the knowledge of\nlogit distribu-"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "tion, and these methods ignore the information of\nintermediate"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": ""
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "features. Suﬀering from incomplete information transfer, their clas-"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "siﬁcation performance is relatively low. For DML, each peer net-"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "work learns from each other by aligning the output with other"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "peer networks. For KDCL, the original\ninputs are ﬁrst augmented"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": ""
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "with diﬀerent random seeds separately to enhance the network’s"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "invariance to input perturbations. The predictions of the peer mod-"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "els are also eﬀectively integrated to generate a soft\ntarget\nto en-"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": ""
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": "sure that multiple sub-networks beneﬁt from collaborative learn-"
        },
        {
          "tions is encouraged through cross-network learning.\nIn contrast,": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "4.3.2\nNumber of peers.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "model’s capability for cross-subject adaptive recognition. Our frame-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Figure 5 illustrates that combining several peer networks can",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "work shows eﬀective emotion recognition, robustness across sub-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "markedly surpass the basic model’s performance. This outcome",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "jects, and deployability in extensive experiments. The proposed"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "conﬁrms our hypothesis that sharing contrastive knowledge among",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "OMCRD is a general framework for multi-channel fNIRS sequences"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "peer networks enhances overall generalization ability. Nonethe-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "that can be extended to large-scale models in ﬁelds like mental"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "less, with an increasing number of peers in the queue, the improve-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "health diagnosis and cognitive assessment."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "ment diminishes gradually, reaching a point of stabilization or even",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "demonstrating negative growth. The main experiment has produced",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "ACKNOWLEDGMENTS"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "impressive results using three student networks.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "This work is partially supported by the following grants: National"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Natural Science Foundation of China (61972163, U1801262), Natu-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "4.4\nVisualization",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "ral Science Foundation of Guangdong Province (2022A1515011555,"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "To further illustrate the eﬀectiveness of the proposed OMCRD, we",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "2023A1515012568), Guangdong Provincial Key Laboratory of Hu-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "visualize the raw and trained features of ﬁve subjects through t-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "man Digital Twin (2022B1212010004)."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "SNE (Figure 6). We represented diﬀerent emotion categories with",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "four colors and used ﬁve shapes to distinguish samples with diﬀer-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "REFERENCES"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "ent subjects. Figure 6 (a) shows the scattered distribution of mean",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[1] Roddy Cowie, Ellen Douglas-Cowie, Nicolas Tsapatsoulis, George Votsis, Ste-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "features extracted from the raw fNIRS signals in the t-SNE embed-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "fanos Kollias, Winfried Fellenz, and John G Taylor. 2001. Emotion recognition"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "ding space. In contrast, Figure 6 (b) illustrates a clustering phenom-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "in human-computer interaction.\nIEEE Signal processing magazine 18, 1 (2001),"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "enon in the distribution of various categories following the OCM-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "32–80."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[2] Bea Waelbers, Stefano Bromuri, and Alexander P Henkel. 2022. Comparing neu-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "CRD, although not completely separable. Speciﬁcally, fNIRS data",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "ral networks for speech emotion recognition in customer service interactions."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "from diﬀerent subjects under the same emotional stimulus exhibit",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "In 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 1–8."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[3] Değer Ayata, Yusuf Yaslan, and Mustafa E Kamasak. 2020. Emotion recognition"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "more similar\nfeature distributions. Notably,\nthe blue data points",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "from multimodal physiological signals for emotion aware healthcare systems."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "are farther from the orange data points in the embedding space",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Journal of Medical and Biological Engineering 40 (2020), 149–157."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "compared to the green (or pink) data points. This validates that",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[4] Maochun Huang, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. 2023. Context-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Based Adaptive Multimodal Fusion Network for Continuous Frame-Level Sen-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "the IS-ICR contrastive learning strategy pulls the less similar fea-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "timent Prediction.\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "tures further apart. Overall, the proposed method eﬀectively miti-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "cessing (2023)."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "gates inter-subject diﬀerences without losing emotion separability,",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[5] Ruihan Chen,\nJunpeng\nTan,\nZhijing\nYang, Xiaojun\nYang, Qingyun Dai,"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Yongqiang Cheng, and Liang Lin. 2024. DPHANet: Discriminative Parallel and"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "thereby facilitating cross-subject emotion recognition. This is ben-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Hierarchical Attention Network for Natural Language Video Localization.\nIEEE"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "eﬁcial for deploying the model onto wearable physiological moni-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Transactions on Multimedia (2024)."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[6] Dongri Yang, Abeer Alsadoon, PW Chandana Prasad, Ashutosh Kumar Singh,"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "toring devices to predict emotion categories for unseen subjects.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "and Amr Elchouemi. 2018. An emotion recognition model based on facial recog-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "nition in virtual\nlearning environment.\nProcedia Computer Science 125 (2018),"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "2–10."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[7]\nJinglun Cen, Chunmei Qing, Haochun Ou, Xiangmin Xu, and Junpeng Tan."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "2024. MASANet: Multi-Aspect Semantic Auxiliary Network for Visual Senti-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "ment Analysis.\nIEEE Transactions on Aﬀective Computing (2024)."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[8]\nSiddique Latif, Rajib Rana, Sara Khalifa, Raja Jurdak,\nJunaid Qadir, and Björn"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Schuller. 2021. Survey of deep representation learning for speech emotion recog-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "nition.\nIEEE Transactions on Aﬀective Computing 14, 2 (2021), 1634–1654."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[9]\nPeiliang Gong, Ziyu Jia, Pengpai Wang, Yueying Zhou, and Daoqiang Zhang."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "2023. ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Net-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "work for EEG-Based Emotion Recognition. In Proceedings of the 31st ACM Inter-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "national Conference on Multimedia. 883–892."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[10] Youngchul Kwak, Woo-Jin Song, and Seong-Eun Kim. 2022.\nFGANet:\nfNIRS-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "guided attention network for hybrid EEG-fNIRS brain-computer interfaces. IEEE"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Transactions on Neural Systems and Rehabilitation Engineering 30 (2022), 329–"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "339."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[11] Manasa Kalanadhabhatta, Adrelys Mateo Santana, Deepak Ganesan, Tauhidur"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Rahman, and Adam Grabell. 2022. Extracting Multimodal Embeddings via Su-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Figure 6:\n(a)\nt-SNE results for the original data.\n(b)\nt-SNE",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "pervised Contrastive Learning for Psychological Screening. In 2022 10th Interna-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "results of OMCRD (using fold 1 data). Diﬀerent colors are",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "tional Conference on Aﬀective Computing and Intelligent Interaction (ACII). IEEE,"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "classes: Green for LANV, Orange for HANV, Blue for LAPV,",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "1–8."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[12]\nStefano Piana, Alessandra Staglianò, Francesca Odone, and Antonio Camurri."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "and Pink for HAPV. Diﬀerent markers are various subjects.",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "2016. Adaptive body gesture representation for automatic emotion recognition."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "ACM Transactions on Interactive Intelligent Systems (TiiS) 6, 1 (2016), 1–31."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[13]\nShuaiqi Liu, Yingying Zhao, Yanling An, Jie Zhao, Shui-Hua Wang, and Jingwen"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Yan. 2023.\nGLFANet: A global\nto local\nfeature aggregation network for EEG"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "emotion recognition. Biomedical Signal Processing and Control 85 (2023), 104799."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "5\nCONCLUSION",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[14] Dian Yu and Shouqian Sun. 2020. A systematic exploration of deep neural net-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "In this study, we propose a novel method (OMCRD) for lightweight",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "works for EDA-based emotion recognition.\nInformation 11, 4 (2020), 212."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[15] Yu-Liang Hsu, Jeen-Shing Wang, Wei-Chun Chiang, and Chien-Han Hung. 2017."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "the fNIRS emotion recognition model. The framework allows mul-",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Automatic ECG-based emotion recognition in music listening. IEEE Transactions"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "tiple students to jointly optimize the network and teach cross-subject",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "on Aﬀective Computing 11, 1 (2017), 85–99."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "[16]\nShuo Xiao, Xiaojing Qiu, Chaogang Tang, and Zhenzhen Huang. 2023. A Spatial-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "knowledge at region and channel levels interactively. Furthermore,",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "Temporal ECG Emotion Recognition Model Based on Dynamic Feature Fusion."
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "the IS-ICR loss boosts the similarity between identical emotional",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": ""
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Sig-"
        },
        {
          "BCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "representations from various individuals, notably improving the",
          "Zhili Lai, Chunmei Qing, Junpeng Tan, Wanxiang Luo, and Xiangmin Xu": "nal Processing (ICASSP). IEEE, 1–5."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[17]\nImam Yogie Susanto, Tse-Yu Pan, Chien-Wen Chen, Min-Chun Hu, and Wen-\n[31] Xianggen Liu, Yunan Luo, Pengyong Li, Sen Song, and Jian Peng. 2021. Deep"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Huang Cheng. 2020.\nEmotion recognition from galvanic skin response signal\ngeometric representations for modeling eﬀects of mutations on protein-protein"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "based on deep hybrid neural networks. In Proceedings of the 2020 International\nbinding aﬃnity. PLoS computational biology 17, 8 (2021), e1009284."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Conference on Multimedia Retrieval. 341–345.\n[32]\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian,"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[18] Xiaopeng Si, Huang He, Jiayue Yu, and Dong Ming. 2023. Cross-subject emotion\nPhillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.\nSupervised"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "recognition brain–computer interface based on fNIRS and DBJNet. Cyborg and\ncontrastive learning. Advances in neural information processing systems 33 (2020),"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Bionic Systems 4 (2023), 0045.\n18661–18673."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[19]\nJiafa Chen, Kaiwei Yu, Fei Wang, Zhengxian Zhou, Yifei Bi, Songlin Zhuang, and\n[33]\nSahar Soltanieh, Ali Etemad, and Javad Hashemi. 2022.\nAnalysis of augmen-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Dawei Zhang. 2024.\nTemporal convolutional network-enhanced real-time im-\ntations for contrastive ecg representation learning.\nIn 2022 International Joint"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "plicit emotion recognition with an innovative wearable fNIRS-EEG dual-modal\nConference on Neural Networks (IJCNN). IEEE, 1–10."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "system. Electronics 13, 7 (2024), 1310.\n[34] Xinke Shen, Xianggen Liu, Xin Hu, Dan Zhang, and Sen Song. 2022.\nCon-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[20] Michiel Spapé, Kalle Mäkelä, and Tuukka Ruotsalo. 2023. NEMO: A Database\ntrastive learning of subject-invariant EEG representations for cross-subject emo-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "for Emotion Analysis Using Functional Near-infrared Spectroscopy.\nIEEE Trans-\ntion recognition.\nIEEE Transactions on Aﬀective Computing 14, 3 (2022), 2496–"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "actions on Aﬀective Computing (2023).\n2511."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[21] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. 2015. Distilling the knowledge in\n[35]\nJiyang Wang, Ayse Altay, and Senem Velipasalar. 2024. Block-As-Domain Adap-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "a neural network. arXiv preprint arXiv:1503.02531 (2015).\ntation for Workload Prediction from fNIRS Data. arXiv preprint arXiv:2405.00213"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[22] Tianhao Gu, Zhe Wang, Xinlei Xu, Dongdong Li, Hai Yang, and Wenli Du. 2022.\n(2024)."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Frame-level teacher–student learning with data privacy for EEG emotion recog-\n[36] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "nition.\nIEEE transactions on neural networks and learning systems 34, 12 (2022),\nMingsheng Long. 2023.\niTransformer: Inverted Transformers Are Eﬀective for"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "11021–11028.\nTime Series Forecasting. In The Twelfth International Conference on Learning Rep-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[23]\nPai Wang, Chunyong Guo, Shuangqiang Xie, Xiang Qiao, Lili Mao, and Xiany-\nresentations."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "ong Fu. 2022.\nEEG emotion recognition based on knowledge distillation op-\n[37] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "timized residual networks.\nIn 2022 IEEE 6th Advanced Information Technology,\nand Wancai Zhang. 2021.\nInformer: Beyond eﬃcient\ntransformer for long se-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Electronic and Automation Control Conference (IAEAC). IEEE, 574–581.\nquence time-series forecasting. In Proceedings of the AAAI conference on artiﬁcial"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[24] Yucheng Liu, Ziyu Jia, and Haichao Wang. 2023.\nEmotionkd: a cross-modal\nintelligence, Vol. 35. 11106–11115."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "knowledge distillation framework for emotion recognition based on physiologi-\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones,"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "cal signals. In Proceedings of the 31st ACM International Conference on Multimedia.\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "6122–6131.\nneed. Advances in neural information processing systems 30 (2017)."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[25] Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. 2018. Deep\n[39] Min Lin, Qiang Chen, and Shuicheng Yan. 2013. Network in network.\narXiv"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "mutual\nlearning.\nIn Proceedings of\nthe IEEE conference on computer vision and\npreprint arXiv:1312.4400 (2013)."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "pattern recognition. 4320–4328.\n[40]\nPeter Lang and Margaret M Bradley. 2007. The International Aﬀective Picture"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[26] Xiatian Zhu, Shaogang Gong, et al. 2018. Knowledge distillation by on-the-ﬂy\nSystem (IAPS) in the study of emotion and attention. Handbook of emotion elic-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "native ensemble. Advances in neural information processing systems 31 (2018).\nitation and assessment 29 (2007), 70–73."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[41]\nIlya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization.\n[27] Defang Chen, Jian-Ping Mei, Can Wang, Yan Feng, and Chun Chen. 2020. Online"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "In International Conference on Learning Representations.\nknowledge distillation with diverse peers. In Proceedings of the AAAI conference"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[42]\nIlya Loshchilov and Frank Hutter. 2016. SGDR: Stochastic Gradient Descent with\non artiﬁcial intelligence, Vol. 34. 3430–3437."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Warm Restarts. In International Conference on Learning Representations.\n[28] Guile Wu and Shaogang Gong. 2021.\nPeer collaborative learning for online"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[43] Rafael Müller, Simon Kornblith, and Geoﬀrey E Hinton. 2019. When does label\nknowledge distillation.\nIn Proceedings of\nthe AAAI Conference on artiﬁcial\nin-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "smoothing help? Advances in neural information processing systems 32 (2019).\ntelligence, Vol. 35. 10302–10310."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[44] Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu,\n[29] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoﬀrey Hinton. 2020."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "and Ping Luo. 2020. Online knowledge distillation via collaborative learning. In\nA simple framework for contrastive learning of visual representations. In Inter-"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\nnational conference on machine learning. PMLR, 1597–1607."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "tion. 11020–11029.\n[30]\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader. 2020.\nDeclutr: Deep"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "[45] Chuanguang Yang, Zhulin An, Linhang Cai, and Yongjun Xu. 2022. Mutual con-\ncontrastive learning for unsupervised textual representations.\narXiv preprint"
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "trastive learning for visual representation learning. In Proceedings of the AAAI\narXiv:2006.03659 (2020)."
        },
        {
          "Online Multi-level Contrastive Representation Distillation for Cross-Subject fNIRS Emotion Recognition\nBCIMM ’24, October 28, 2024, Melbourne, VIC, Australia.": "Conference on Artiﬁcial Intelligence, Vol. 36. 3045–3053."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "Comparing neural networks for speech emotion recognition in customer service interactions",
      "authors": [
        "Bea Waelbers",
        "Stefano Bromuri",
        "Alexander Henkel"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition from multimodal physiological signals for emotion aware healthcare systems",
      "authors": [
        "Değer Ayata",
        "Yusuf Yaslan",
        "Mustafa Kamasak"
      ],
      "year": "2020",
      "venue": "Journal of Medical and Biological Engineering"
    },
    {
      "citation_id": "4",
      "title": "Context-Based Adaptive Multimodal Fusion Network for Continuous Frame-Level Sentiment Prediction",
      "authors": [
        "Maochun Huang",
        "Chunmei Qing",
        "Junpeng Tan",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "DPHANet: Discriminative Parallel and Hierarchical Attention Network for Natural Language Video Localization",
      "authors": [
        "Ruihan Chen",
        "Junpeng Tan",
        "Zhijing Yang",
        "Xiaojun Yang",
        "Qingyun Dai",
        "Yongqiang Cheng",
        "Liang Lin"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "An emotion recognition model based on facial recognition in virtual learning environment",
      "authors": [
        "Dongri Yang",
        "Abeer Alsadoon",
        "Chandana Pw",
        "Ashutosh Prasad",
        "Amr Singh",
        "Elchouemi"
      ],
      "year": "2018",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "7",
      "title": "MASANet: Multi-Aspect Semantic Auxiliary Network for Visual Sentiment Analysis",
      "authors": [
        "Jinglun Cen",
        "Chunmei Qing",
        "Haochun Ou",
        "Xiangmin Xu",
        "Junpeng Tan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "Siddique Latif",
        "Rajib Rana",
        "Sara Khalifa",
        "Raja Jurdak",
        "Junaid Qadir",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "ASTDF-Net: Attention-Based Spatial-Temporal Dual-Stream Fusion Network for EEG-Based Emotion Recognition",
      "authors": [
        "Peiliang Gong",
        "Ziyu Jia",
        "Pengpai Wang",
        "Yueying Zhou",
        "Daoqiang Zhang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "FGANet: fNIRSguided attention network for hybrid EEG-fNIRS brain-computer interfaces",
      "authors": [
        "Youngchul Kwak",
        "Woo-Jin",
        "Seong-Eun Song",
        "Kim"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "11",
      "title": "Extracting Multimodal Embeddings via Supervised Contrastive Learning for Psychological Screening",
      "authors": [
        "Manasa Kalanadhabhatta",
        "Mateo Adrelys",
        "Deepak Santana",
        "Tauhidur Ganesan",
        "Adam Rahman",
        "Grabell"
      ],
      "year": "2022",
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "12",
      "title": "Adaptive body gesture representation for automatic emotion recognition",
      "authors": [
        "Stefano Piana",
        "Alessandra Staglianò",
        "Francesca Odone",
        "Antonio Camurri"
      ],
      "year": "2016",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "13",
      "title": "GLFANet: A global to local feature aggregation network for EEG emotion recognition",
      "authors": [
        "Shuaiqi Liu",
        "Yingying Zhao",
        "Yanling An",
        "Jie Zhao",
        "Shui-Hua Wang",
        "Jingwen Yan"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "14",
      "title": "A systematic exploration of deep neural networks for EDA-based emotion recognition",
      "authors": [
        "Dian Yu",
        "Shouqian Sun"
      ],
      "year": "2020",
      "venue": "Information"
    },
    {
      "citation_id": "15",
      "title": "Automatic ECG-based emotion recognition in music listening",
      "authors": [
        "Yu-Liang Hsu",
        "Jeen-Shing Wang",
        "Wei-Chun Chiang",
        "Chien-Han Hung"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "A Spatial-Temporal ECG Emotion Recognition Model Based on Dynamic Feature Fusion",
      "authors": [
        "Shuo Xiao",
        "Xiaojing Qiu",
        "Chaogang Tang",
        "Zhenzhen Huang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition from galvanic skin response signal based on deep hybrid neural networks",
      "authors": [
        "Yogie Imam",
        "Tse-Yu Susanto",
        "Chien-Wen Pan",
        "Min-Chun Chen",
        "Wen-Huang Hu",
        "Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "18",
      "title": "Cross-subject emotion recognition brain-computer interface based on fNIRS and DBJNet",
      "authors": [
        "Xiaopeng Si",
        "Huang He",
        "Jiayue Yu",
        "Dong Ming"
      ],
      "year": "2023",
      "venue": "Cyborg and Bionic Systems"
    },
    {
      "citation_id": "19",
      "title": "Temporal convolutional network-enhanced real-time implicit emotion recognition with an innovative wearable fNIRS-EEG dual-modal system",
      "authors": [
        "Jiafa Chen",
        "Kaiwei Yu",
        "Fei Wang",
        "Zhengxian Zhou",
        "Yifei Bi",
        "Songlin Zhuang",
        "Dawei Zhang"
      ],
      "year": "2024",
      "venue": "Electronics"
    },
    {
      "citation_id": "20",
      "title": "NEMO: A Database for Emotion Analysis Using Functional Near-infrared Spectroscopy",
      "authors": [
        "Michiel Spapé",
        "Kalle Mäkelä",
        "Tuukka Ruotsalo"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "22",
      "title": "Frame-level teacher-student learning with data privacy for EEG emotion recognition",
      "authors": [
        "Tianhao Gu",
        "Zhe Wang",
        "Xinlei Xu",
        "Dongdong Li",
        "Hai Yang",
        "Wenli Du"
      ],
      "year": "2022",
      "venue": "Frame-level teacher-student learning with data privacy for EEG emotion recognition"
    },
    {
      "citation_id": "23",
      "title": "EEG emotion recognition based on knowledge distillation optimized residual networks",
      "authors": [
        "Pai Wang",
        "Chunyong Guo",
        "Shuangqiang Xie",
        "Xiang Qiao",
        "Lili Mao",
        "Xianyong Fu"
      ],
      "year": "2022",
      "venue": "2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)"
    },
    {
      "citation_id": "24",
      "title": "Emotionkd: a cross-modal knowledge distillation framework for emotion recognition based on physiological signals",
      "authors": [
        "Yucheng Liu",
        "Ziyu Jia",
        "Haichao Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Deep mutual learning",
      "authors": [
        "Ying Zhang",
        "Tao Xiang",
        "Timothy Hospedales",
        "Huchuan Lu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Knowledge distillation by on-the-fly native ensemble",
      "authors": [
        "Xiatian Zhu",
        "Shaogang Gong"
      ],
      "year": "2018",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Online knowledge distillation with diverse peers",
      "authors": [
        "Defang Chen",
        "Jian-Ping Mei",
        "Can Wang",
        "Yan Feng",
        "Chun Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "28",
      "title": "Peer collaborative learning for online knowledge distillation",
      "authors": [
        "Guile Wu",
        "Shaogang Gong"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on artificial intelligence"
    },
    {
      "citation_id": "29",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "30",
      "title": "Declutr: Deep contrastive learning for unsupervised textual representations",
      "authors": [
        "John Giorgi",
        "Osvald Nitski",
        "Bo Wang",
        "Gary Bader"
      ],
      "year": "2020",
      "venue": "Declutr: Deep contrastive learning for unsupervised textual representations",
      "arxiv": "arXiv:2006.03659"
    },
    {
      "citation_id": "31",
      "title": "Deep geometric representations for modeling effects of mutations on protein-protein binding affinity",
      "authors": [
        "Xianggen Liu",
        "Yunan Luo",
        "Pengyong Li",
        "Sen Song",
        "Jian Peng"
      ],
      "year": "2021",
      "venue": "PLoS computational biology"
    },
    {
      "citation_id": "32",
      "title": "Supervised contrastive learning",
      "authors": [
        "Prannay Khosla",
        "Piotr Teterwak",
        "Chen Wang",
        "Aaron Sarna",
        "Yonglong Tian",
        "Phillip Isola",
        "Aaron Maschinot",
        "Ce Liu",
        "Dilip Krishnan"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Analysis of augmentations for contrastive ecg representation learning",
      "authors": [
        "Sahar Soltanieh",
        "Ali Etemad",
        "Javad Hashemi"
      ],
      "year": "2022",
      "venue": "2022 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "34",
      "title": "Contrastive learning of subject-invariant EEG representations for cross-subject emotion recognition",
      "authors": [
        "Xinke Shen",
        "Xianggen Liu",
        "Xin Hu",
        "Dan Zhang",
        "Sen Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Block-As-Domain Adaptation for Workload Prediction from fNIRS Data",
      "authors": [
        "Jiyang Wang",
        "Ayse Altay",
        "Senem Velipasalar"
      ],
      "year": "2024",
      "venue": "Block-As-Domain Adaptation for Workload Prediction from fNIRS Data",
      "arxiv": "arXiv:2405.00213"
    },
    {
      "citation_id": "36",
      "title": "iTransformer: Inverted Transformers Are Effective for Time Series Forecasting",
      "authors": [
        "Yong Liu",
        "Tengge Hu",
        "Haoran Zhang",
        "Haixu Wu",
        "Shiyu Wang",
        "Lintao Ma",
        "Mingsheng Long"
      ],
      "year": "2023",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "37",
      "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
      "authors": [
        "Haoyi Zhou",
        "Shanghang Zhang",
        "Jieqi Peng",
        "Shuai Zhang",
        "Jianxin Li",
        "Hui Xiong",
        "Wancai Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "38",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "39",
      "title": "Network in network",
      "authors": [
        "Min Lin",
        "Qiang Chen",
        "Shuicheng Yan"
      ],
      "year": "2013",
      "venue": "Network in network",
      "arxiv": "arXiv:1312.4400"
    },
    {
      "citation_id": "40",
      "title": "The International Affective Picture System (IAPS) in the study of emotion and attention. Handbook of emotion elicitation and assessment",
      "authors": [
        "Peter Lang",
        "Margaret Bradley"
      ],
      "year": "2007",
      "venue": "The International Affective Picture System (IAPS) in the study of emotion and attention. Handbook of emotion elicitation and assessment"
    },
    {
      "citation_id": "41",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "42",
      "title": "SGDR: Stochastic Gradient Descent with Warm Restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2016",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "43",
      "title": "When does label smoothing help? Advances in neural information processing systems",
      "authors": [
        "Rafael Müller",
        "Simon Kornblith",
        "Geoffrey Hinton"
      ],
      "year": "2019",
      "venue": "When does label smoothing help? Advances in neural information processing systems"
    },
    {
      "citation_id": "44",
      "title": "Online knowledge distillation via collaborative learning",
      "authors": [
        "Qiushan Guo",
        "Xinjiang Wang",
        "Yichao Wu",
        "Zhipeng Yu",
        "Ding Liang",
        "Xiaolin Hu",
        "Ping Luo"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Mutual contrastive learning for visual representation learning",
      "authors": [
        "Chuanguang Yang",
        "Zhulin An",
        "Linhang Cai",
        "Yongjun Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}