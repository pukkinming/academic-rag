{
  "paper_id": "2003.01277v3",
  "title": "The Effect Of Silence Feature In Dimensional Speech Emotion Recognition",
  "published": "2020-03-03T01:17:07Z",
  "authors": [
    "Bagus Tris Atmaja",
    "Masato Akagi"
  ],
  "keywords": [
    "speech emotion recognition",
    "dimensional emotion",
    "silence feature",
    "silence threshold",
    "affective computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Silence is a part of human-to-human communication, which can be a clue for human emotion perception. For automatic emotion recognition by a computer, it is not clear whether silence is useful to determine human emotion within a speech. This paper presents an investigation of the effect of using silence feature in dimensional emotion recognition. Since the silence feature is extracted per utterance, we grouped the silence feature with high statistical functions from a set of acoustic features. The result reveals that the silence features affect the arousal dimension more than other emotion dimensions. The proper choice of a threshold factor in the calculation of silence feature improved the performance of dimensional speech emotion recognition performance, in terms of a concordance correlation coefficient. On the other side, improper choice of that factor leads to a decrease in performance by using the same architecture.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "One of the elements of human to computer communication is the perception, which is implemented as automatic recognition in computers. Perception is the application's ability to consume, organize, and classify information about the user's physical and digital, and current and historical context. Perceptual data includes things like location, date, time, mood, expression, environment, physiological responses, connected applications, networks, and nearby devices  [1] . Due to this difference with human communication, especially on processing the data, the processing mechanism to obtain perceptual data on human-to-machine communication may be different from human-to-human communication.\n\nEmotion is one of human perceptions. The difference between emotion and mood is that emotions are short-lived feelings that come from a known cause, while moods are feelings that are longer lasting than emotions and often without apparent cause  [2] . Emotions can range from happy, ecstatic, sad, and prideful in the category, while moods are either positive or negative. Emotion also can be described in a degree of valence, arousal, and dominance. Other researchers used liking  [3]  and expectancy  [4]  as additional dimensions or attributes to those dimensional emotions.\n\nValence (V) is the pleasantness of the stimulus [pleasure (P)], ranges from positive (extreme happy) to negative (extreme unhappy). In other words, it is also known as \"sentiment\" or \"semantic orientation\"  [5] . Arousal or activation (A) is the intensity of emotion provoked by the stimulus, ranges from sleepiness to excitement. The dominance (D) or power dimension refers to the degree of power or sense of control over the emotion  [6] . This three-dimensional emotion model is known as VAD or PAD model  [7] .\n\nThe concept of verbal communication is by conveying verbal words. However, some researchers reported that the use of non-verbal words, i.e., pause or silence, is needed for better human communication. Adding pause to emotional speech affects the recognition rate by human participants. Furthermore, silence and other disfluencies are not only useful for human communication but also can be effective cues for the computer to recognize human emotion  [8] .\n\nAn investigation on how speech pause length influences how listeners ascribe emotional states to the speaker has been done by authors in  [9] . The author manipulated the length of speech pauses to create five variants of all passages. The participants were asked to rate the emotionality of these passages by indicating on a 16 point scale how angry, sad, disgusted, happy, surprised, scared, positive, and heated the speaker could have been. The data reveal that the length of silent pauses influences listeners in attributing emotional category to the speaker. Their findings argue that pauses play a relevant role in ascribing emotions and that this phenomenon might be partly independent of language.\n\nDifferent from human to human communication, human to machine communication (or human-machine interaction, HMI) is a form of communication where humans interact with a variety of devices like sensors and actuators, or generally the computer. Although the silence aforementioned is useful for human emotion perception, it is still unclear whether it is useful or not for human to machine communication. One of the clue for this question is a study by Tian et al.  [8] ,  [4] , which used disfluencies and other non-verbal vocalizations as features for speech emotion recognition. Their results indicated that disfluencies and non-verbal vocalizations provide useful information overlooked by the other two types of features for emotion recognition: lexical and acoustic features. However, instead of using silences or pauses, they used filler pauses, fillers, stutters, laughter, breath, and sigh within an utterance to extract those features.\n\nInstead of using silence feature, Atmaja and Akagi  [10, 11]  removed silence within speech and extract acoustic features from the speech region after silence removal. Their results show an improvement of emotion category detection on an emotional speech dataset by utilizing silence removal and attention model. However, this method may slightly corrupts the speech fluency, because it generated a context of audio samples artificially.\n\nThe contribution of this paper is the investigation of the use of silence as a feature in automatic dimensional speech emotion recognition (SER). For each utterance, a number of frames are calculated and checked whether those frames can be categorized as silence. The fraction of the number of silence frames over total frames is measured as a silence feature. This silence feature is grouped with high statistical function (HSF), i.e., mean and standard deviation, of an acoustic feature set as the input to speech emotion recognition system. The comparison of HSF with and without silence feature can be used to determine the effect of silence feature on dimensional speech emotion recognition. The measure of comparison was given by the concordance correlation coefficient (CCC)  [12] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Acoustic And Silence Features",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Acoustic Feature Set",
      "text": "Acoustic features are the input to an SER system. One of the acoustic feature sets proposed for SER is called Geneva Minimalistic Acoustic Parameter Set (GeMAPS), which is developed by Eyben et al.  [13] . Those acoustic features extracted on frame-based processing are often called as Low-Level Descriptors (LLD). This frame-based processing is common in other speech processing applications. Other researchers  [14]  proposed to ex tract functional features on certain lengths, e.g., 100 ms, 1 s, or per utterance/turn depend on the given labels. These functional features is often called as High-Level Statistical Functions (HSF). The reason for using HSF is to roughly describe the temporal and contour variations of different LLDs during certain period/utterance  [15] . Assuming that emotional content lies temporal variations rather than LLDs, HSFs may give a more accurate performance in determining emotional state from speech. Schmitt et al. suggested that using mean and standard deviation (std) from a set of acoustic features (GeMAPS) performed better than LLDs on Table  1 . GeMAPS feature  [13]  and its functionals used for dimensional SER in this research.\n\nLLDs loudness, alpha ratio, hammarberg index, spectral slope 0-500 Hz, spectral slope 500-1500 Hz, spectral flux, 4 MFCCs, F0, jitter, shimmer, Harmonics-to-Noise Ratio (HNR), Harmonic difference H1-H2, Harmonic difference H1-A3, F1, F1 bandwidth, F1 amplitude, F2, F2 amplitude, F3, and F3 amplitude. HSFs mean (of LLDs), standard deviation (of LLDs), silence speech emotion recognition  [16] . We used these mean and std features, which are extracted per utterance from LLDs in GeMAPS feature set (2 × 23 features). To add those functionals, we proposed to use a silence feature, which is also extracted per utterance. The computation of a silence feature is explained below.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Silence Feature",
      "text": "Silence, in this paper, is defined as the portion of the silence frames compared to the total frames in an utterance. In human communication, this portion of silence in speaking depends on the speaker's emotion. For example, a happy speaker may have fewer silences (or pauses) than a sad speaker. The portion of silence in an utterance can be calculated as\n\nwhere N s is the number of frames to be categorized as silence (silence frames), and N t is the number of total frames within an utterance. To be categorized as silence, a frame is checked whether it is less than a threshold, which is a multiplication of a factor with a root mean square (RMS) energy (X rms ). Mathematically, it can be formulated\n\nand X rms is defined as\n\nThese equations are similar to what is proposed in  [17] . The author of that paper used a fixed threshold, while we evaluated some factors of α to find the best factor for silence feature in speech emotion recognition. The equation 1 to calculate the silence feature is also similar to the calculation of the disfluency feature proposed in  [4] . In that paper, the author divides the total duration of disfluency over the total utterance length on n words. Fig.  1  shows the calculation of our silence feature. If X rms from a frame is below the th, then it is categorized as silence and follow the calculation of the equation 1.\n\n... The \"Interactive Emotional dyadic MOtion CAPture\" (IEMOCAP) database, collected by the Speech Analysis and Interpretation Laboratory (SAIL) at the University of Southern California (USC) was used to investigate the effect of silence feature on dimensional SER. This dataset consists of multimodal measurement of speech and gesture, including markers of the face, head, and hands, which provide detailed information about facial expressions and hand movements during a dyadic conversation. Among those modalities, only speech utterance is used. The total utterances are 10039 turns with three emotion attributes: arousal, valence, and dominance. The average turn duration is 4.5 s with average 11.4 words per turn. The annotations are rated by at least two evaluators per utterance. The evaluators were USC students. We used emotion dimensions scores averaged from those two annotators as gold-standard labels in the experiments. The detail of that pilot study for developing the dataset can be found in  [18] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Speech Emotion Recognition System",
      "text": "SER is an attempt to make the computer recognize emotional states in speech. A deep neural network (DNN)-based SER is the common approach in recent days. Among numerous DNN methods, convolutional neural network (CNN) and LSTM are the most common  [14] ,  [15] . We choose an LSTM-based dimensional SER due to its simplicity and the hardware support (CuDNN  [19] ). This architecture is a modification from the previous LSTM-based SER system reported in  [20]  by en-larging the size of networks and using different parameters for multitask learning.\n\nFor the input features, three sets of acoustic features are evaluated. These features are GeMAPS feature set (baseline); mean and std of GeMAPS (mean+std); and mean, std, and silence (mean+std+silence) features. The features in GeMAPS are extracted in 25 ms and 10 ms of the window and hop lengths using openSMILE feature extraction toolkit  [21] . Mean, std, and silence are extracted per utterance. The silence feature is extracted per utterance from 2048 samples of time frame length (128 ms) and 512 samples of hop length (32 ms) with 16000 Hz of sampling frequency. The implementation of silence feature computation was performed using Li-bROSA python package  [22] . Those features are evaluated to the same architecture, shown in Fig.  2 , which is implemented using Keras toolkit  [23] . Each frame shown in that figure represents a time frame to calculate X rms and to check whether it is a silence (if it is greater than th) or not.\n\nThe first layer on the dimensional SER system on that figure is the batch normalization layer. This layer is intended to accelerate deep network training, as suggested in  [24] . The size of the batch normalization layer depends on the input features. GeMAPS has the size of the nodes of (3409 × 23) for IEMOCAP dataset, mean+std has a size of (1 × 46), and mean+std+silence has a size of (1×47). After a batch normalization layer, we stacked three LSTM layers (unidirectional, 512 nodes each) and flattened the output of the last LSTM layer. Three dense layers with each size of 1 are connected to Flatten layer to predict the degree of valence, arousal, and dominance. The degree of those emotion dimensions is a floating-point value ranges from [-1, 1], converted from the original 5-point scale. The total size of the networks (trainable parameters) depends on the input features, about 10 million for GeMAPS input, and about 5 million for mean+std and mean+std+silence inputs.\n\nFor each input feature set, a number of 100 epochs were performed with earlystopping callbacks with a number of 10 patiences. This means, if the training process did not find an improvement of performance after 10 epochs, it will stop and save that best model for evaluation. To obtain a consistent/same result on each run, the same fixed random number is initiated at the top of the SER computer programs.\n\nTo measure the performance, a correlation measure, namely CCC, is used. This CCC is a measure of relation between prediction and true dimensional emotion degree (valence, arousal, dominance), which penalizes the score if the prediction shifts the true value. Instead of using a single value, we measure CCC for each emotion dimension. This method enables us to analyze which emotion dimension relates to specific features. The cumulative performance for all three dimensions can be given in an average of three CCC scores. The fair comparison can be performed between mean+std and mean+std+silence feature inputs, as it only has a difference in input size by a single value (46 vs. 47).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Effect Of Silence Feature On Dimensional Ser",
      "text": "Although it is stated previously that the fair comparison could be made by comparing results from mean+std vs. mean+std+silence, for the sake of research continuity, the result from the previous reported result  [20]  and GeMAPS feature are presented as baselines. Both kinds of research used the same SER architecture and the same input with different size of network (64 vs. 512 nodes for each LSTM layer). By using larger networks and different multitasking coefficients, an improvement of arousal has been obtained on GeMAPS feature input, while the CCC scores of both valences and dominance are similar. Our approach adopted multitask learning to train simultaneously valence, arousal, and dominance from  [25] . Here, the coefficients (weighting factors) used for valence, arousal, and dominance are 0.1, 0.5, and 0.4, respectively. Table  2  shows the obtained CCC score for each emotion dimension and its average score from different methods.\n\nUsing HSFs of LLDs from GeMAPS, i.e., mean and std of 23 acoustic features, an improvement of valence was obtained. However, the CCC score of arousal and dominance decreased, although the average CCC score remains the same. This type of input feature (mean+std) has a smaller number of dimensions (1 × 46) compared to GeMAPS feature (3409 × 23). The size of the network of input with mean+std also about half of the network of GeMAPS input.\n\nOn the last method in Table  2 , a silence feature was combined with std+mean resulting (1 × 47) of input size. This small modification leads to improvements in valence and arousal among other methods. A CCC score for this mean+std+silence input for dominance has decreased compared to GeMAPS, but slightly higher than mean+std. Both CCC scores on valence and arousal improved with 6% and 17% relative improvement. This result suggests that the silence feature affects arousal (activeness of speech) more than other dimensions. This finding may follow that humans tend to use more pauses in speech when they are sad and fewer pauses when they are happy.\n\nTo extend this investigation, evaluation of the silence threshold factor (α) was performed and discussed below. Most studies on silent pauses used threshold as one of the objects of study  [26] ,  [27] . Those studies categorized thresholds in silent pause into two groups: low threshold (200 ms) and high threshold (2000 ms). However, the definition of the threshold used here is different from those researches. The threshold in this research is defined as the upper-bound of RMS energy of a frame to be categorized as silence (equation (2)).\n\nThe silence threshold factor (α) in equation (  2 ) plays an important role in determining whether a frame belongs to the silence category. To investigate the effect of this factor on dimensional SER performance, we variate the α to 0.4, 0.3, 0.2, and 0.1. The result obtained in the previous Table  2  with mean+std+silence input was obtained using α = 0.3.\n\nFig.  3  shows the example of an utterance, its X rms of corresponding frame, X rms , and three lines of threshold using different silence threshold factors. As shown in that figure, using α = 0.4 may result in an incorrect decision to include speech as silence. However, using a low silence threshold factor, e.g., α = 0.1, leads to a smaller number of silence frames due to a tight filter. An evaluation to choose the proper factor is needed to obtain the optimal silence feature for dimensional SER.\n\nFig.  4  shows the effect of changing the silence threshold factor to the CCC score of valence, arousal, and dominance. Using a higher factor will impact on increasing the number of silence frames. On the other side, using a smaller factor will decrease the possibility to count a frame as a silence. As can be seen in that figure, the best CCC score was obtained using α = 0. These results also suggest that using improper silence threshold factor will decrease the performance of dimensional SER, especially on the arousal dimension.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we investigate the effect of using silence feature on the dimensional speech emotion recognition. The result reveals that using mean+std+silence features affects the CCC score of predicted emotion degree compared to mean+std features. Using a proper factor of silence threshold, a remarkable improvement of CCC scores was obtained, particularly on arousal (activation) dimension. This can be explained that passiveness or activeness in speech, which reflected by number of pauses/silences in speech, contribute to arousal degree, as expected. On the other side, the use of improper silence threshold may decrease the performance of arousal. Using a fixed random number to initiate the computation of dimensional speech emotion recognition (same number for both mean+std and mean+std+silence for all architectures), the consistent results were obtained to support that finding on effect of silence on dimensional speech emotion recognition. There are some issues which need to be confirmed for the future research. Although we obtained improvements in all emotion dimensions by using mean+std+silence from mean+std, the relationship between silence features with valence and dominance dimensions needs to be verified. The relation between positive and negative emotion dimensions with silence features is also meriting further study., e.g., more silences features with more valence, arousal, and dominance.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the calculation of",
      "page": 2
    },
    {
      "caption": "Figure 1: The moving frame to calculate a silence feature",
      "page": 3
    },
    {
      "caption": "Figure 2: , which is implemented",
      "page": 3
    },
    {
      "caption": "Figure 2: Structure of dimensional SER system to investigate the effect of silence features; the number inside the bracket represents",
      "page": 4
    },
    {
      "caption": "Figure 3: shows the example of an utterance, its Xrms of cor-",
      "page": 4
    },
    {
      "caption": "Figure 4: shows the effect of changing the silence threshold",
      "page": 4
    },
    {
      "caption": "Figure 4: also supports the ﬁnding that",
      "page": 4
    },
    {
      "caption": "Figure 3: RMS energy of corresponding frames with Xrms and",
      "page": 5
    },
    {
      "caption": "Figure 4: Evaluation of different silence threshold factor (α) and",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: GeMAPS feature [13] and its functionals used for",
      "page": 2
    },
    {
      "caption": "Table 2: shows the obtained CCC score for each emo-",
      "page": 4
    },
    {
      "caption": "Table 2: , a silence feature was",
      "page": 4
    },
    {
      "caption": "Table 2: Results of dimensional emotion recognition by vari-",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "The humanto-machine communication model",
      "authors": [
        "J Sukis",
        "L Lawrence"
      ],
      "year": "2018",
      "venue": "The humanto-machine communication model"
    },
    {
      "citation_id": "3",
      "title": "Psychological models of emotion",
      "authors": [
        "K Scherer"
      ],
      "year": "2000",
      "venue": "The neuropsychology of emotion"
    },
    {
      "citation_id": "4",
      "title": "Avec 2019 workshop and challenge: state-of-mind, detecting depression with ai, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "5",
      "title": "Word-level emotion recognition using high-level features",
      "authors": [
        "J Moore",
        "L Tian",
        "C Lai"
      ],
      "year": "2014",
      "venue": "International Conference on Intelligent Text Processing and Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "Speech and language processing",
      "authors": [
        "D Jurafsky",
        "J Martin"
      ],
      "year": "2018",
      "venue": "Retrieved March"
    },
    {
      "citation_id": "7",
      "title": "Automatic, dimensional and continuous emotion recognition",
      "authors": [
        "H Gunes",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "International Journal of Synthetic Emotions (IJSE)"
    },
    {
      "citation_id": "8",
      "title": "An approach to environmental psychology",
      "authors": [
        "A Mehrabian",
        "J Russell"
      ],
      "year": "1974",
      "venue": "An approach to environmental psychology"
    },
    {
      "citation_id": "9",
      "title": "Recognizing emotions in dialogues with disfluencies and non-verbal vocalisations",
      "authors": [
        "L Tian",
        "C Lai",
        "J Moore"
      ],
      "year": "2015",
      "venue": "Proceedings of the 4th Interdisciplinary Workshop on Laughter and Other Non-verbal Vocalisations in Speech"
    },
    {
      "citation_id": "10",
      "title": "Ascribing emotions depending on pause length in native and foreign language speech",
      "authors": [
        "E Tisljár-Szabó",
        "C Pléh"
      ],
      "year": "2014",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition based on speech segment using lstm with attention model",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Signals and Systems (ICSigSys)"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using speech feature and word embedding",
      "authors": [
        "B Atmaja",
        "K Shirai",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
    },
    {
      "citation_id": "13",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "L Lawrence I-Kuei"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "14",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Continuous emotion recognition in speech-do we need recurrence?",
      "authors": [
        "M Schmitt",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Training"
    },
    {
      "citation_id": "16",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Deep recurrent neural networks for emotion recognition in speech",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings DAGA"
    },
    {
      "citation_id": "18",
      "title": "Multimodal speech emotion recognition and ambiguity resolution",
      "authors": [
        "G Sahu"
      ],
      "year": "2019",
      "venue": "Multimodal speech emotion recognition and ambiguity resolution",
      "arxiv": "arXiv:1904.06022"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "cudnn: Efficient primitives for deep learning",
      "authors": [
        "S Chetlur",
        "C Woolley",
        "P Vandermersch",
        "J Cohen",
        "J Tran",
        "B Catanzaro",
        "E Shelhamer"
      ],
      "year": "2014",
      "venue": "cudnn: Efficient primitives for deep learning",
      "arxiv": "arXiv:1410.0759"
    },
    {
      "citation_id": "21",
      "title": "RNN-based dimensional speech emotion recognition",
      "authors": [
        "B Atmaja",
        "R Elbarougy",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "ASJ Autum Meeting"
    },
    {
      "citation_id": "22",
      "title": "Recent developments in opensmile, the munich opensource multimedia feature extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Gross",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "B Mcfee",
        "V Lostanlen",
        "M Mcvicar",
        "A Metsai",
        "S Balke",
        "C Thom",
        "C Raffel",
        "D Lee",
        "F Zalkow",
        "K Lee"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "24",
      "title": "Keras",
      "authors": [
        "F Chollet"
      ],
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "25",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "32nd International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Multitask learning and multistage fusion for dimensional audiovisual emotion recognition",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2020",
      "venue": "ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing Proceedings"
    },
    {
      "citation_id": "27",
      "title": "A large-scale multilingual study of silent pause duration",
      "authors": [
        "E Campione",
        "J Véronis"
      ],
      "year": "2002",
      "venue": "Speech prosody 2002, international conference"
    },
    {
      "citation_id": "28",
      "title": "Silent and filled pauses and speech planning in first and second language production",
      "authors": [
        "R Rose"
      ],
      "year": "2017",
      "venue": "Silent and filled pauses and speech planning in first and second language production"
    }
  ]
}