{
  "paper_id": "2312.10381v3",
  "title": "Secap: Speech Emotion Captioning With Large Language Model",
  "published": "2023-12-16T08:33:10Z",
  "authors": [
    "Yaoxun Xu",
    "Hangting Chen",
    "Jianwei Yu",
    "Qiaochu Huang",
    "Zhiyong Wu",
    "Shixiong Zhang",
    "Guangzhi Li",
    "Yi Luo",
    "Rongzhi Gu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotions are crucial in human communication and are extensively used in fields like speech synthesis and natural language understanding. Most prior studies, such as speech emotion recognition, have categorized speech emotions into a fixed set of classes. Yet, emotions expressed in human speech are often complex, and categorizing them into predefined groups can be insufficient to adequately represent speech emotions. On the contrary, describing speech emotions directly by means of natural language may be a more effective approach. Regrettably, there are not many studies available that have focused on this direction. Therefore, this paper proposes a speech emotion captioning framework named SECap, aiming at effectively describing speech emotions using natural language. Owing to the impressive capabilities of large language models in language comprehension and text generation, SECap employs LLaMA as the text decoder to allow the production of coherent speech emotion captions. In addition, SECap leverages HuBERT as the audio encoder to extract general speech features and Q-Former as the Bridge-Net to provide LLaMA with emotion-related speech features. To accomplish this, Q-Former utilizes mutual information learning to disentangle emotion-related speech features and speech contents, while implementing contrastive learning to extract more emotion-related speech features. The results of objective and subjective evaluations demonstrate that: 1) the SE-Cap framework outperforms the HTSAT-BART baseline in all objective evaluations; 2) SECap can generate high-quality speech emotion captions that attain performance on par with human annotators in subjective mean opinion score tests.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech communication plays a pivotal role in people's daily life in terms of transmitting information and establishing connections. As one of the core carriers of interpersonal communication, speech not only undertakes the function of verbal communication but also deeply involves the transmission of emotions and intentions. Recognizing and interpreting speech emotions precisely is crucial for enhancing communication effects. Therefore, how to extract the speaker's emotional information accurately from speech has gradually become an important topic in the field of speech processing.\n\nPrevious research has typically approached speech emotion acquisition as a categorization task, known as speech emotion recognition (SER)  (El Ayadi, Kamel et al. 2011; Nwe, Foo, and De Silva 2003; Jiang et al. 2019) , where emotions like fear and happiness are assigned to discrete categories. In recent years, the performance of such SER tasks has made great progress thanks to the emergence of innovative model architectures.\n\nHowever, traditional SER exhibits limitations, because single-word labels often lack nuances, failing to convey detailed emotional information like intensity and fluctuations. Speech emotions are typically multifaceted, encompassing diverse affective states (e.g., simultaneous happiness and nervousness) within one utterance. Classifying speech into a single emotion category may inadequately capture authentic emotion. Additionally, the inherently subjective perception of emotions leads to potential variability in emotion classification among individuals interpreting complicated speech. Considering the limitations of speech emotion classification, employing natural language sentences rather than labels could be a promising strategy to describe speech emotions more precisely. Motivated by the recent progress of the Automated Audio Captioning (AAC) task  (Han et al. 2021; Chen et al. 2020; Ye et al. 2021 ) which employs natural language to describe acoustic events in audio, we present the Speech Emotion Captioning (SEC) task and propose an innovative SECap framework, comprising an audio encoder, a Bridge-Net, and a text decoder, to characterize human speech emotions using natural language. To our knowledge, this is among the pioneering works in this direction.\n\nIn the SEC task, there are two primary challenges to address: firstly, how to extract the emotion-related speech features from the original speech inputs; and secondly, how to generate high-quality, human-like speech emotion descriptions. For the first challenge, limited speech data with emotion captions makes training the audio encoder from scratch challenging. Inspired by the success of pre-trained model in SER  (Mohamed and Aly 2021 ) tasks, we utilize HuBERT  (Hsu et al. 2021 ) as SECap's audio encoder for robust speech feature extraction. However, directly using the frame-level HuBERT features can be computationally heavy. To address this, inspired by BLIP-2  (Li et al. 2023) , we employ Q-Former as Bridge-Net to compress HuBERT features. While both acoustic and content information within HuBERT fea- tures are related to speech emotion, acoustic information is typically more directly related to speech emotion, and content information can be easily obtained through transcription. Therefore, in the Bridge-Net, we aim to separately extract emotion-related acoustic information from HuBERT features while eliminating content information. Thus, we employ Speech-Caption Contrastive Learning and Speech-Transcription Mutual Information Learning to train Bridge-Net to better extract emotion-related acoustic information.\n\nFor the second challenge, due to the advances in large language models (LLMs) and their impressive natural language understanding capabilities, such as GPT-4 (OpenAI 2023), we employ LLaMA  (Touvron et al. 2023 ) as text decoder for generating fluent and coherent speech emotion captions based on Q-Former-extracted speech features. Concurrently, we use LLaMA to guide Q-Former training, enabling better projection of speech emotion features into LLaMA, ultimately yielding higher-quality speech emotion captions.\n\nAs for evaluation, we design both subjective and objective evaluation metrics based on the AAC task to better assess the quality of speech emotion captions generated by SE-Cap. To facilitate a more effective comparison, we choose the HTSAT-BART model  (Mei et al. 2023) , which performs exceptionally well in the AAC task, as our baseline. Experimental results demonstrate that SECap outperforms the HTSAT-BART model across all objective metrics. In the subjective mean opinion score (MOS) test, the quality of speech emotion captions generated by SECap surpasses that of human labels  (i.e., 3.77 vs. 3.39 MOS score)  and are on par with human annotations (i.e., 3.77 vs. 3.85 MOS score). Our main contributions are as follows:\n\n• We propose the task of Speech Emotion Captioning (SEC), which, to our knowledge, stands among the pioneering efforts to characterize speech emotions using natural language. Open-source models like BLOOM  (Scao et al. 2022)  and ChatGLM  (Du et al. 2022 ) have fostered growth in the LLM community. Also, researchers explore LLMs' performance in multimodal interactions, aspiring for models capable of managing audio, vision, and text modalities, reflecting human daily interactions. The first approach uses LLMs as task orchestrators, connecting downstream models like Au-dioGPT  (Huang et al. 2023)  and HuggingGPT  (Shen et al. 2023 ) for specialized tasks. The second approach positions LLMs as multitask processors, mapping modal tasks to a unified space. For example, BLIP-2 maps images to text space using Q-Former, while Video-LLaMA (Zhang, Li, and Bing 2023) maps audio and vision modalities via Q-Former.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Automated Audio Captioning",
      "text": "Automated audio captioning (AAC)  (Xu, Wu, and Yu 2022; Koh, Fuzhao, and Siong 2022 ) is a crucial task in the audio domain, describing ambient sounds using natural language. Unlike audio tagging  (Xu et al. 2017)  or sound event detection  (Bilen et al. 2020) , AAC requires identifying specific events and describing them naturally. After the emergence of this task, the encoder-decoder (Sutskever, Vinyals, and Le 2014) framework has been the dominant solution to this problem. Methods like AudioClip  (Guzhov et al. 2022 ) and CLAP  (Wu et al. 2023 ) employ contrastive learning to map audio and text, boosting the encoder-decoder connection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "Inspired by the AAC task, SECap employs encoder-decoder architecture, as illustrated in Figure  1 . The audio encoder extracts speech features, and Bridge-Net extracts emotionrelated speech features and transforms them into the text decoder's feature space. The text decoder then generates speech emotion captions based on these features. In this section, we will begin by providing an overview of the SECap structure. Following that, we will elaborate on the two key aspects of the Bridge-Net design for obtaining emotion-related representations. Lastly, we will describe the overall training process of SECap.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "As illustrated in Figure  2 , SECap utilizes a HuBERTbased audio encoder, a Q-Former-based Bridge-Net, and a LLaMA-based text decoder.\n\nThe HuBERT is to derive speech embedding for its powerful speech feature extraction capability. However, framelevel HuBERT features can lead to heavy computation cost. We employ Q-Former-based Bridge-Net to compress features. Meanwhile, acoustic information is more directly associated with speech emotion, while content information is obtainable from transcriptions. Thus, the Bridge-Net is used to extract emotion-related acoustic information and eliminate content information. We employ LLaMA as the text decoder for generating speech emotion captions, leveraging its exceptional language comprehension capabilities. Aligning with LLaMA's input format, we position L-Embedding between the \"BOS\" and a prompt. This method constrains LLaMA's output space via the prompt, yielding more accurate speech emotion captions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Q-Former",
      "text": "Owing to the redundancy of HuBERT speech features, Q-Former is designed and adopted to compress and extract emotion-related speech features which consists of self-attention, cross-attention, and linear layers. Q-queries are learnable parameters for extracting speech embedding.\n\nLet q ∈ R nq×dq represent the Q-queries, where n q is the number of Q-queries, d q is the dimension of Q-queries and S ∈ R ns×Ts×ds represent the speech embedding, where n s is the batch size, T s is the number of time steps, and d s is the dimension of speech embedding. We first input the Qqueries q ∈ R nq×dq into the self-attention mechanism:\n\nwhere\n\nwhere A cross ∈ R ns×nq×dv represents the cross-attention output, while W q ∈ R dv×d k , W k ∈ R ds×d k , and W v ∈ R ds×dv are the learnable weight matrices for queries, keys, and values in the cross-attention mechanism. This approach enables the attention mechanism to retrieve features related to Q-queries within the speech embedding. Specifically, the output of the Q-Former, denoted as the Q-Embedding Q e ∈ R ns×nq×dq , maintains a fixed length that is independent of the length of the input speech. This fixedlength representation leads to improved generalization performance across speech inputs of varying lengths.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Obtain Emotion-Related Representations",
      "text": "To provide LLaMA with more content-unrelated and emotion-related speech features, we simultaneously incorporate both the human-labeled speech emotion captions and the transcriptions. As depicted in Figure  3 , these are passed through a Q-Former that is largely consistent with the original, except for the absence of the cross-attention module. This process yields C-Embedding Q c ∈ R ns×Tc×dq and T-Embedding Q t ∈ R ns×Tt×dq , where T c and T t denote the length of the caption and the transcription, respectively. We employ Speech-Transcription Mutual Information Learning to disentangle speech features from speech content. Additionally, Speech-Caption Contrastive Learning is utilized to extract more emotion-related speech features.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech-Transcription Mutual Information Learning (Stmil)",
      "text": "The speech content can potentially impact emotion assessment, for example, expressing joyous statements calmly. To minimize the correlation between speech features and content, thereby mitigating the speech content's impact on LLaMA's speech emotion caption generation, we propose Speech-Transcription Mutual Information Learning. As illustrated in Figure  3 , we introduce both Speech Embedding and Trans Embedding into the Q-Former simultaneously, yielding Q-Embedding Q e and T-Embedding Q t . This enables the comparison of speech and its content within a unified representation space. To evaluate the correlation between Q e and Q t , we adopt mutual information I(Q t ; Q e ) as the metric:\n\nqt∈Qt qe∈Qe p(q t , q e ) log p(q t , q e ) p(q t )p(q e ) (3)\n\nwhere p(q t , q e ) represents the joint probability distribution of Q t and Q e , and p(q t ) and p(q e ) denote the marginal probability distributions of Q t and Q e , respectively. However, direct computation of mutual information between Q e and Q t is infeasible due to their unknown, highdimensional nature. While prior methods such as MINE  (Belghazi et al. 2018 ) and infoNCE (Van Den Oord, Vinyals et al. 2017) can estimate the lower bound of mutual information, they are not suitable for controlling the minimization process. Following vCLUB  (Cheng et al. 2020 ), we use Equation (4) to estimate the upper bound of mutual information and employ it as a loss function to reduce the correlation between speech features and content.\n\nThe equation includes conditional probabilities q(y i |x i ) and q(y j |x i ), representing the probabilities of the i-th and j-th Q e samples given the i-th Q t sample. The logarithm captures the dissimilarity between Q e conditioned on Q t , and summing over all pairwise combinations provides the upper bound mutual information measure between Q e and Q t .\n\nSpeech-Caption Contrastive Learning (SCCL) Due to the high dimensionality and redundancy in speech representations, speech features contain abundant information such as content and background noise, with only a fraction being emotion-related. To alleviate the complexity of processing speech features by LLaMA, we aim for Q-Former to extract features highly correlated with speech emotion caption, consequently bridging the gap between speech features and text modality. As illustrated in Figure  3 , our objective is to minimize the distance between Q e and C-Embedding Q c , prompting Q-Former to extract more emotion-related features and progressively approach the text modality. Drawing inspiration from CLAP  (Wu et al. 2023) , we employ a contrastive learning approach to accurately represent distances between Q e of distinct speech samples, ensuring that speech with similar emotions yield closer Q e distances, while those with dissimilar emotions result in farther Q e distances.\n\nTo mitigate the influence of similar emotions in negative samples during contrastive learning, we partition the dataset into N distinct categories based on human-labeled speech emotion labels. This guarantees substantial differences in speech emotion captions across categories, thereby enhancing the model's discriminative capacity throughout the learning process. During each training step, we select K speech-caption pairs from each of the N sets, ensuring that for each Q e (referred to as e i ), there is 1 corresponding Q c (referred to as d i ),(K -1) Q c with similar emotions (referred to as p i ), and (N K -K) Q c with dissimilar emotions (referred to as u i ).\n\nWe opt to use cosine similarity S to measure the distance between Q e and Q c . For enhanced contrastive learning, we design the training method as follows:\n\nwhere the weighting coefficients w 1 , w 2 , and w 3 control the contribution of each term in the loss function. The threshold value m is the margin to control the distance between speech feature Q e and irrelevant speech emotion caption feature Q c .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Process",
      "text": "To enhance speech emotion caption generation with LLaMA, we devise a two-stage training process. The first stage compresses HuBERT-extracted speech features to obtain emotion-relevant attributes, while the subsequent stage aligns these features with LLaMA's representation space.\n\nIn the first training stage, we combine STMIL and SCCL for collaborative training as in Figure  3 , while keeping the HuBERT model frozen. Inspired by BLIP-2, we initialize the Q-Former using pre-trained parameters from BERT base  (Devlin et al. 2019) . Specifically, the training loss is:\n\nwhere the weighting coefficients w T 1 and w T 2 control the contribution of STMIL and SCCL.\n\nIn the second training stage, we fine-tune the Q-Former and the projection layer to effectively integrate Q-Formerextracted speech features into LLaMA. Meanwhile, the parameters of LLaMA and HuBERT remain frozen. We insert a \"BOS\" token before the L-Embedding to align with the inference format. To improve SECap's generalization ability, we devise 30 semantically akin sentences, each instructing to \"portray the speaker's emotion in a single Chinese sentence.\" During training, we randomly choose a sentence to concatenate after the L-Embedding. Subsequently, we append the human-labeled speech caption C after the prompt and employ the teacher-forcing approach to enable LLaMA generate caption Ĉ. Cross-entropy loss (CELoss) is then adopted as the training objective:",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset",
      "text": "Due to the lack of publicly available SEC datasets, we utilize an internal dataset called EMOSpeech. EMOSpeech dataset consists of 5 female and 2 male speakers, totaling 41.6 hours of speech covering 30526 sentences, sampled at a rate of 24kHz. Each speech in EMOSpeech has three to five humanlabeled speech emotion captions and human-labeled speech emotion labels provided by different annotators, along with its corresponding transcription.\n\nAs for labeling, we begin with 50 sample audio clips for independent annotator labeling and hold a discussion session for annotators to review annotations and establish standardized rules based on collective input. The annotation process has three levels: identifying overall emotion using a single word, describing emotion intensity, and providing a comprehensive sentence considering emotion, volume, and speech rate. With these guidelines, annotators consistently label the dataset. To ensure annotation quality, we conduct consistency checks by randomly selecting 5 out of every 100 clips for review by other annotators, upholding high standards throughout the dataset construction.\n\nUpon constructing the EMOSpeech dataset, we randomly select 600 sentences for testing, 600 sentences for validation, and the remaining 29,326 sentences for training 2  .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Evaluation Metric",
      "text": "Since there is no existing method to evaluate speech emotion captions, we devise both objective and subjective evaluation methods based on the nature of the SEC task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Objective Evaluation",
      "text": "In this study, we initially adopt objective evaluation metrics for the AAC task, containing BLEU 1  (Papineni et al. 2002) , BLEU 4 , METEOR  (Banerjee and Lavie 2005) , ROUGE l  (Lin 2004) , CIDEr  (Wang and Chan 2019), and SPICE (Anderson et al. 2016 ). However, these metrics primarily focus on word-level matching. To more effectively assess the similarity between two Chinese emotion captions at the sentence level, we employ sentence similarity evaluation metrics in conjunction with the aforementioned criteria. The first model  (Ming 2022 ) is based on MACBERT  (Cui et al. 2021)  and trained on Chinese STS-B  (Cer et al. 2017) , while the second model  (Reimers and Gurevych 2019)  is finetuned on Tencent Cloud. Their evaluation indicators are denoted as SIM 1 and SIM 2 , respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Subjective Evaluation",
      "text": "In the subjective scoring method, we develop a three-stage scoring criterion to reduce variability due to evaluators' inconsistent understanding of emotions. The first step involves determining whether the generated sentence describes an emotion. The second step assesses if the generated sentence, when summarized into an emotion, matches the speech. The third step evaluates whether the generated sentence aligns with the speech in terms of the emotion's intensity.\n\nTo be specific, we have devised a scoring method similar to the Mean Opinion Score (MOS) used in Text-to-Speech systems, with ratings ranging from 1 to 5, where 1 represents the worst and 5 represents the best.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Analysis Experiment Setup",
      "text": "Our experiments are conducted exclusively on the EMO-Speech dataset. We choose the HuBERT-large model, pretrained on the 10k-hour WenetSpeech  (Zhang et al. 2022 ) L subset, as the audio encoder. Due to the original LLaMA's limited proficiency in understanding Chinese, we choose an enhanced version of LLaMA  (Cui, Yang, and Yao 2023)  finetuned with Chinese datasets as the text decoder 3  .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Performance Analysis",
      "text": "This experiment aims to demonstrate the effectiveness of SECap. Given speech content's impact on emotion, we incorporate transcriptions as additional input and design multiple comparison groups for thorough analysis. Specifically, when incorporating transcriptions, we either use the raw transcriptions processed through the LLaMA tokenizer or apply the T-Embedding processed through a projection layer as an extra input, which is then concatenated between BOS Emb and L-Embedding. Subsequently, we append the Prompt Emb and feed it into LLaMA. In some of the comparison groups, speech features are not used, the former processed transcriptions are directly concatenated between BOS Emb and Prompt Emb. Objective Evaluation Table  2  illustrates consistent trends in most metrics across various experiments. Therefore, our analysis will primarily focus on the insights obtained from the two Chinese sentence similarity models.\n\nAs depicted in Table  2 , when incorporating only speech features, the proposed SECap (#6) surpasses the HTSAT-BART (#1) baseline across all objective metrics, signifying its ability to generate more natural, human-like speech emotion captions compared to the HTSAT-BART model.\n\nCompared to raw transcription (#2), employing T-Embedding (#3) improves SIM values by 16.66% and 178.11%. Since LLaMA has not been previously trained on the EMOSpeech dataset, it lacks prior knowledge of the dataset's captions, resulting in unconstrained output space. However, T-Embedding imposes greater constraints, extracting more emotion-related features, and resulting in relatively accurate speech emotion captions.\n\nUsing only Q-Embedding (#6), we observe a 9.18% and 13.29% SIM value increase compared to relying solely on T-Embedding (#3). As identical sentences can convey different emotions, relying only on speech content (i.e. transcription) may not sufficiently reflect speech emotions, while speech signals better represent speech emotions. Upon incorporating Q-Embedding and the raw transcription (#4), the SIM values relatively decrease by 3.77% and 4.27% compared to only using Q-Embedding (#6). However, replacing the raw transcription (#4) with T-Embedding (#5) shows a relative improvement of 0.61% and 3.73% in SIM values. Despite this increase, the SIM values remain lower than those obtained when using only Q-Embedding (#6).\n\nConsistent with text-only modality, T-Embedding outperforms raw transcription in extracting emotion features from transcription, providing greater constraints for LLaMA while reducing conflicts with Q-Embedding. However, integrating both audio and text modalities into the model may increase the difficulty for LLaMA in processing the information, as text and audio could contain similar, unrelated, or even contradictory information. Consequently, LLaMA must balance these features, potentially impeding its ability to optimally leverage information from both modalities and affecting the model's assessment of speech emotion.\n\nSubjective Evaluation In the subjective experiment, we randomly select a test set of 50 sentences. We apply all the methods listed in Table  2  to generate corresponding speech emotion captions. To enable a more comprehensive compar-ison, we also include human-labeled speech emotion labels (Human Label) and emotion categories (SER Model Label) identified by a competitive pre-trained Chinese SER model from HuggingFace 4  . We request evaluators to score the nine types of texts according to the Subjective Evaluation details provided in Evaluation Metric section. 15 evaluators participate in the tests, with the results presented in Figure  4 . As seen in Figure  4 , human-labeled speech emotion captions surpass both human-labeled speech emotion labels and SER Model Labels. This outcome aligns with the SEC task's aim to represent emotions more comprehensively and accurately within a single sentence. Notably, the best SECap model outperforms human-labeled speech emotion labels and is on par with human-labeled speech emotion captions.\n\nFurthermore, Figure  4  reveals that SECap's performance is suboptimal with solely raw transcription. However, other SECap input methods outperform human-labeled emotion labels and the baseline in subjective evaluation metrics, demonstrating SECap's ability to generate suitable speech emotion captions deemed more representative of emotions. However, employing both Q-Embedding and T-Embedding as input generates superior subjective evaluation outcomes compared to using solely Q-Embedding.\n\nWe suppose that objective metrics, relying on predefined rules, may differ from subjective evaluations based on human perception and understanding. Discrepancies can arise as evaluators focus on intricate details, such as emotional expression's naturalness and contextual information, which are challenging for objective metrics to capture. Additionally, we observed that evaluators initially concentrated on speech content. In cases of content-emotion conflicts, evaluators tended to assign higher scores to content-relevant captions. For example, uttering \"I'm feeling terrible today\" in a flat tone with only speech embedding might yield a caption describing the flat tone, while incorporating text embedding could result in a caption combining sadness and flatness.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study On Different Model Components",
      "text": "This experiment aims to explore the effect of different model components on generating speech emotion captions. Considering that HTSAT-BART differs from SECap in both the audio encoder, the Bridge-Net, and the text decoder, to better analyze each component, we construct the model using different audio encoders, text decoders, and Bridge-Nets, respectively. In the previous experiment, we find that the two text similarity models exhibit the same trend despite different experiments. Therefore, in this experiment as well as subsequent experiments, we only use the first text similarity model introduced in the Evaluation Metric section for the evaluation of objective indicators.  Similarly, substituting the audio encoder with HuBERT while maintaining the text decoder results in a 7.26% SIM value increase, suggesting HuBERT's robust speech feature extraction abilities compared to HTSAT. Replacing both components yields a 15.10% SIM value increase. Furthermore, exchanging the linear layer with Q-Former significantly enhances high-quality speech emotion caption generation, accompanied by a 4.85% SIM value increase.\n\nEvidently, HuBERT is more suitable for speech feature extraction compared to HTSAT, and LLaMA demonstrates superior text comprehension and generation capabilities compared to BART. By further extracting speech features using the Q-Former, speech features that better align with emotional aspects can be conveyed to LLaMA, ultimately resulting in more accurate speech emotion captions.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison Of Training Methods",
      "text": "This experiment seeks to investigate the impact of various training methods for the Q-Former on generating speech emotion captions. While maintaining the audio encoder as HuBERT and the text decoder as LLaMA, we conduct a series of comparisons, including whether to use STMIL or SCCL in the first training stage, and whether to freeze the Q-Former in the second training stage. Table  4  indicates that incorporating STMIL or SCCL individually during the first training stage results in a 2.17% and 3.14% SIM value increase, respectively, compared to omitting this stage, suggesting that disentangling content information or extracting extra emotion-related speech features enhances caption quality. Moreover, using both methods simultaneously yields a 6.92% SIM value increase. Without both STMIL and SCCL, Q-Former lacks speech feature insight, and limited EMOSpeech risks overfitting. Compared to employing either method alone, employing both STMIL and SCCL leads to the rise of SIM values by 4.65% and 3.67%, respectively, highlighting that concurrent utilization of both methods bolsters speech feature extraction, generating more precise speech emotion captions.\n\nUpon finalizing the initial training phase, a 19.50% SIM value reduction is observed by freezing Q-Former and solely training the projection layer, compared to the unfrozen Q-Former. As LLaMA lacks involvement in guiding Q-Former's training, the extracted features might inadequately align with LLaMA's input, and the projection layer's adaptability fails to offset this misalignment.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "To better represent speech emotions, we introduce an innovative task called speech emotion captioning, which uses natural language descriptions rather than singular labels to characterize speech emotions. Our proposed model, SECap, integrates a HuBERT-based audio encoder, a LLaMA-based text decoder, and a Q-Former-based Bridge-Net. The Q-Former effectively disentangles speech features and speech content information through Speech-Transcription Mutual Information Learning, while extracting more emotion-related speech features via Speech-Caption Contrastive Learning. Impressively, SECap generates high-quality speech emotion captions, with its performance on par with human annotators. This pioneering task and method provide a fresh perspective on speech emotion understanding, fostering a more comprehensive approach to analyzing and interpreting speech emotional expressions.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of Speech Emotion Recognition",
      "page": 2
    },
    {
      "caption": "Figure 1: The audio encoder",
      "page": 2
    },
    {
      "caption": "Figure 2: Framework of the proposed SECap",
      "page": 3
    },
    {
      "caption": "Figure 2: , SECap utilizes a HuBERT-",
      "page": 3
    },
    {
      "caption": "Figure 3: , these are passed",
      "page": 3
    },
    {
      "caption": "Figure 3: , we introduce both Speech Embed-",
      "page": 3
    },
    {
      "caption": "Figure 3: The figure presents Q-Former decoupling audio representation and content information using Speech-Transcription",
      "page": 4
    },
    {
      "caption": "Figure 3: , our objective is to",
      "page": 4
    },
    {
      "caption": "Figure 3: , while keeping the",
      "page": 4
    },
    {
      "caption": "Figure 4: Figure 4: Subjective experiment results on performance",
      "page": 6
    },
    {
      "caption": "Figure 4: , human-labeled speech emotion cap-",
      "page": 6
    },
    {
      "caption": "Figure 4: reveals that SECap’s performance",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Objective experiment results on performance analysis. “Raw Trans” denotes raw transcription of speech.“Q-Emb”",
      "data": [
        {
          "Model": "",
          "ID": "",
          "Input Modality": "Text\nAudio",
          "SIM1\nSIM2": "",
          "METEOR\nCIDEr\nSPICE\nBLEU1\nBLUE4\nROUGEl": ""
        },
        {
          "Model": "HTSAT-BART",
          "ID": "#1",
          "Input Modality": "—\nQ-Emb",
          "SIM1\nSIM2": "59.62\n53.19",
          "METEOR\nCIDEr\nSPICE\nBLEU1\nBLUE4\nROUGEl": "32.74\n3.05\n14.61\n23.64\n2.21\n2.17"
        },
        {
          "Model": "SECap",
          "ID": "#2\n#3\n#4\n#5\n#6",
          "Input Modality": "Raw Trans\n—\nT-Emb\n—\nRaw Trans\nQ-Emb\nT-Emb\nQ-Emb\n—\nQ-Emb",
          "SIM1\nSIM2": "56.49\n22.38\n65.90\n62.24\n69.24\n67.50\n69.66\n70.02\n71.95\n70.51",
          "METEOR\nCIDEr\nSPICE\nBLEU1\nBLUE4\nROUGEl": "0.014\n0.00\n2.67\n4.38\n0.00\n0.00\n25.97\n4.28\n15.98\n23.37\n18.37\n2.58\n29.59\n5.36\n16.99\n25.16\n28.51\n5.77\n33.62\n7.25\n18.44\n27.18\n33.82\n5.96\n36.08\n8.12\n19.30\n28.49\n34.81\n6.49"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Spice: Semantic propositional image caption evaluation",
      "authors": [
        "P Anderson",
        "B Fernando",
        "M Johnson",
        "S Gould"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "2",
      "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "authors": [
        "S Banerjee",
        "A Lavie"
      ],
      "year": "2005",
      "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization"
    },
    {
      "citation_id": "3",
      "title": "Mutual information neural estimation",
      "authors": [
        "M Belghazi",
        "A Baratin",
        "S Rajeshwar",
        "S Ozair",
        "Y Bengio",
        "A Courville",
        "D Hjelm"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "4",
      "title": "A framework for the robust evaluation of sound event detection",
      "authors": [
        "C Bilen",
        "G Ferroni",
        "F Tuveri",
        "J Azcarreta",
        "S Krstulović"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Gpt-neox-20b: An open-source autoregressive language model",
      "authors": [
        "S Black",
        "S Biderman",
        "E Hallahan",
        "Q Anthony",
        "L Gao",
        "L Golding",
        "H He",
        "C Leahy",
        "K Mcdonell",
        "J Phang"
      ],
      "year": "2022",
      "venue": "Gpt-neox-20b: An open-source autoregressive language model",
      "arxiv": "arXiv:2204.06745"
    },
    {
      "citation_id": "6",
      "title": "SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
      "authors": [
        "D Cer",
        "M Diab",
        "E Agirre",
        "I Lopez-Gazpio",
        "L Specia"
      ],
      "year": "2017",
      "venue": "Proceedings of the 11th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "7",
      "title": "Optimizing speech emotion recognition using manta-ray based feature selection",
      "authors": [
        "S Chattopadhyay",
        "A Dey",
        "H Basak"
      ],
      "year": "2020",
      "venue": "Optimizing speech emotion recognition using manta-ray based feature selection",
      "arxiv": "arXiv:2009.08909"
    },
    {
      "citation_id": "8",
      "title": "Audio Captioning Based on Transformer and Pre-Trained CNN. In DCASE",
      "authors": [
        "K Chen",
        "Y Wu",
        "Z Wang",
        "X Zhang",
        "F Nian",
        "S Li",
        "X Shao"
      ],
      "year": "2020",
      "venue": "Audio Captioning Based on Transformer and Pre-Trained CNN. In DCASE"
    },
    {
      "citation_id": "9",
      "title": "Club: A contrastive log-ratio upper bound of mutual information",
      "authors": [
        "P Cheng",
        "W Hao",
        "S Dai",
        "J Liu",
        "Z Gan"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "10",
      "title": "Pre-training with whole word masking for chinese bert",
      "authors": [
        "Y Cui",
        "W Che",
        "T Liu",
        "B Qin",
        "Z Yang"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "Efficient and effective text encoding for chinese llama and alpaca",
      "authors": [
        "Y Cui",
        "Z Yang",
        "X Yao"
      ],
      "year": "2023",
      "venue": "Efficient and effective text encoding for chinese llama and alpaca",
      "arxiv": "arXiv:2304.08177"
    },
    {
      "citation_id": "12",
      "title": "Speaker dependent speech emotion recognition using MFCC and Support Vector Machine",
      "authors": [
        "P Dahake",
        "K Shaw"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Automatic Control and Dynamic Optimization Techniques (ICACDOT)"
    },
    {
      "citation_id": "13",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "14",
      "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
      "authors": [
        "Z Du",
        "Y Qian",
        "X Liu",
        "M Ding",
        "J Qiu",
        "Z Yang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M El Ayadi",
        "M Kamel"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "16",
      "title": "Automated Audio Captioning with Weakly Supervised Pre-Training and Word Selection Methods",
      "authors": [
        "A Guzhov",
        "F Raue",
        "J Hees",
        "A Dengel",
        "W Yuan",
        "D Liu",
        "X Li",
        "Z Yang"
      ],
      "year": "2021",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Hubert: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
      "authors": [
        "R Huang",
        "M Li",
        "D Yang",
        "J Shi",
        "X Chang",
        "Z Ye",
        "Y Wu",
        "Z Hong",
        "J Huang",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Audiogpt: Understanding and generating speech, music, sound, and talking head",
      "arxiv": "arXiv:2304.12995"
    },
    {
      "citation_id": "19",
      "title": "Parallelized convolutional recurrent neural network with spectral features for speech emotion recognition",
      "authors": [
        "P Jiang",
        "H Fu",
        "H Tao",
        "P Lei",
        "L Zhao"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using deep learning techniques: A review",
      "authors": [
        "R Khalil",
        "E Jones",
        "M Babar",
        "T Jan",
        "M Zafar",
        "T Alhussain"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Automated audio captioning using transfer learning and reconstruction latent space similarity regularization",
      "authors": [
        "A Koh",
        "X Fuzhao",
        "C Siong"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Large language models are zero-shot reasoners",
      "authors": [
        "T Kojima",
        "S Gu",
        "M Reid",
        "Y Matsuo",
        "Y Iwasawa"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "arxiv": "arXiv:2301.12597"
    },
    {
      "citation_id": "24",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "Y Mei",
        "J.-W Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "26",
      "title": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "authors": [
        "X Mei",
        "C Meng",
        "H Liu",
        "Q Kong",
        "T Ko",
        "C Zhao",
        "M Plumbley",
        "Y Zou",
        "W Wang"
      ],
      "year": "2023",
      "venue": "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research",
      "arxiv": "arXiv:2303.17395"
    },
    {
      "citation_id": "27",
      "title": "A Tool for Text to Vector",
      "authors": [
        "X Ming"
      ],
      "year": "2022",
      "venue": "A Tool for Text to Vector"
    },
    {
      "citation_id": "28",
      "title": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "authors": [
        "O Mohamed",
        "S Aly"
      ],
      "year": "2021",
      "venue": "Arabic speech emotion recognition employing wav2vec2. 0 and hubert based on baved dataset",
      "arxiv": "arXiv:2110.04425"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "30",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Openai ; Papineni",
        "S Roukos",
        "T Ward",
        "W.-J Zhu"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "31",
      "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
      "authors": [
        "N Reimers",
        "I Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "32",
      "title": "A 176b-parameter open-access multilingual language model",
      "authors": [
        "T Scao",
        "A Fan",
        "C Akiki",
        "E Pavlick",
        "S Ilić",
        "D Hesslow",
        "R Castagné",
        "A Luccioni",
        "F Yvon",
        "M Gallé"
      ],
      "year": "2022",
      "venue": "A 176b-parameter open-access multilingual language model",
      "arxiv": "arXiv:2211.05100"
    },
    {
      "citation_id": "33",
      "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
      "authors": [
        "Y Shen",
        "K Song",
        "X Tan",
        "D Li",
        "W Lu",
        "Y Zhuang",
        "K Singhal",
        "S Azizi",
        "T Tu",
        "S Mahdavi",
        "J Wei",
        "H Chung",
        "N Scales",
        "A Tanwani",
        "H Cole-Lewis",
        "S Pfohl"
      ],
      "year": "2023",
      "venue": "Nature",
      "arxiv": "arXiv:2303.17580"
    },
    {
      "citation_id": "34",
      "title": "Sequence to Sequence Learning with Neural Networks",
      "authors": [
        "I Sutskever",
        "O Vinyals",
        "Q Le",
        "Z Ghahramani",
        "M Welling",
        "C Cortes",
        "N Lawrence",
        "Weinberger"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "Neural discrete representation learning",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2017",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "36",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "37",
      "title": "Describing like humans: on diversity in image captioning",
      "authors": [
        "Q Wang",
        "A Chan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Large-scale contrastive languageaudio pretraining with feature fusion and keyword-tocaption augmentation",
      "authors": [
        "Y Wu",
        "K Chen",
        "T Zhang",
        "Y Hui",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "39",
      "title": "A comprehensive survey of automated audio captioning",
      "authors": [
        "X Xu",
        "M Wu",
        "K Yu"
      ],
      "year": "2022",
      "venue": "A comprehensive survey of automated audio captioning",
      "arxiv": "arXiv:2205.05357"
    },
    {
      "citation_id": "40",
      "title": "Unsupervised feature learning based on deep models for environmental audio tagging",
      "authors": [
        "Y Xu",
        "Q Huang",
        "W Wang",
        "P Foster",
        "S Sigtia",
        "P Jackson",
        "M Plumbley"
      ],
      "year": "2017",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Improving the Performance of Automated Audio Captioning via Integrating the Acoustic and Semantic Information",
      "authors": [
        "Z Ye",
        "H Wang",
        "D Yang",
        "Y Zou",
        "F Font",
        "A Mesaros",
        "D Ellis",
        "E Fonseca",
        "M Fuentes",
        "B Elizalde",
        "B Zhang",
        "H Lv",
        "P Guo",
        "Q Shao",
        "C Yang",
        "L Xie",
        "X Xu",
        "H Bu",
        "X Chen",
        "C Zeng"
      ],
      "year": "2021",
      "venue": "Proceedings of the 6th Workshop on Detection and Classification of Acoustic Scenes and Events 2021 (DCASE 2021)"
    },
    {
      "citation_id": "42",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "H Zhang",
        "X Li",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    }
  ]
}