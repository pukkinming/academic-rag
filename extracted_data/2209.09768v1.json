{
  "paper_id": "2209.09768v1",
  "title": "An Efficient End-To-End Transformer With Progressive Tri-Modal Attention For Multi-Modal Emotion Recognition",
  "published": "2022-09-20T14:51:38Z",
  "authors": [
    "Yang Wu",
    "Pai Peng",
    "Zhenyu Zhang",
    "Yanyan Zhao",
    "Bing Qin"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent works on multi-modal emotion recognition move towards end-to-end models, which can extract the task-specific features supervised by the target task compared with the two-phase pipeline. However, previous methods only model the feature interactions between the textual and either acoustic and visual modalities, ignoring capturing the feature interactions between the acoustic and visual modalities. In this paper, we propose the multi-modal end-to-end transformer (ME2ET), which can effectively model the trimodal features interaction among the textual, acoustic, and visual modalities at the low-level and high-level. At the low-level, we propose the progressive tri-modal attention, which can model the tri-modal feature interactions by adopting a two-pass strategy and can further leverage such interactions to significantly reduce the computation and memory complexity through reducing the input token length. At the high-level, we introduce the tri-modal feature fusion layer to explicitly aggregate the semantic representations of three modalities. The experimental results on the CMU-MOSEI and IEMOCAP datasets show that ME2ET achieves the state-of-the-art performance. The further in-depth analysis demonstrates the effectiveness, efficiency, and interpretability of the proposed progressive tri-modal attention, which can help our model to achieve better performance while significantly reducing the computation and memory cost. Our code will be publicly available.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multi-modal emotion recognition aims at detecting emotion in the utterance, which consists of three modal inputs, including textual, visual, and acoustic. It has gained increasing attention from people since it is vital for many downstream applications such as natural human-machine interaction and mental healthcare. Most of the existing studies  (Liu et al., 2018; Tsai et al., 2019; Rahman et al., 2020)  generally use a two-phase pipeline, first extracting uni-modal features from input data and then performing the proposed multi-modal feature fusion method on the features. Different from this line of works, some works train the feature extraction and multi-modal feature fusion modules together in an end-to-end manner since they consider that this approach enables the model to extract the task-specific features supervised by the emotion prediction loss.  Dai et al. (2021)  propose the MESM model, which takes the CNN blocks as the feature extraction module and uses the cross-modal attention to capture bimodal feature interactions. However, MESM only captures the features interactions between the textual and either acoustic or visual modalities, ignoring leveraging the feature interactions between the acoustic and visual modalities, which is very useful for understanding the sentiment semantics  (Zadeh et al., 2017; Tsai et al., 2019) .\n\nTo address this problem, we propose the multi-modal end-to-end transformer (ME2ET) shown in Figure  1 , which can capture the tri-modal feature interactions among the textual, acoustic, and visual modalities effectively at the low-level and highlevel. At the high-level, we propose the tri-modal feature fusion layer to sufficiently fuse the three uni-modal semantic representations. At the lowlevel, we propose the progressive tri-modal attention. The main idea of it is to generate fewer but more informative visual/acoustic tokens based on the input visual/acoustic tokens by leveraging the tri-modal feature interactions. Through reducing the length of the input tokens, we can significantly reduce the computation and memory complexity.\n\nIf the length of the input tokens is N and the length of the generated tokens is K(K N ), which is a hyper-parameter, we can reduce the computation and memory complexity of the self-attention block in the visual/acoustic transformer from O(N 2 ) to O(K 2 ). More details about the complexity analysis are given in Appendix A.\n\nTo be more specific, we adopt a simple two-pass strategy to capture the tri-modal feature interactions in the proposed progressively tri-modal attention. In the first pass, considering the visual data contains more noise as some parts of the face may be missing, we utilize the textual representation to attend the input visual tokens and generate fewer tokens, which are then passed into the visual transformer and get the preliminary visual representation. In the second pass, we use not only the textual but also visual representations to attend the acoustic tokens and feed the outputs into the acoustic transformer producing the acoustic representation. Subsequently, we perform the attention mechanism on the original visual tokens again using both the textual and acoustic representations and get the final visual representation. In this way, our model can obtain the semantic uni-modal representations effectively by leveraging the tri-modal feature interactions.\n\nWe conduct extensive experiments on CMU-MOSEI  (Bagher Zadeh et al., 2018)  and IEMO-CAP  (Busso et al., 2008) . The experimental results show that our model surpasses the baselines and achieves the state-of-the-art performance, which demonstrates the effectiveness of our model. We further analyze the contribution of each part of our model, and the results indicate that the progressive tri-modal attention and tri-modal feature fusion layer are necessary for our model, which can capture the tri-modal feature interactions at the low-level and high-level. Moreover, the in-depth analysis of the progressive tri-modal attention including the ablation study, computation and memory analysis, and visualization analysis shows its effectiveness, efficiency, and interpretability.\n\nThe main contributions of this work are as follows:\n\n• We propose the progressive tri-modal attention, which can help our model to achieve better performance while significantly reducing the computation and memory cost by fully exploring the tri-modal feature interactions.\n\n• We introduce the multi-modal end-to-end transformer (ME2ET), which is a simple and efficient purely transformer-based multimodal emotion recognition framework. To facilitate further research, we explore different patch construction methods for the acoustic and visual raw data and analyze their effects.\n\n• We evaluate ME2ET on two public datasets and ME2ET obtains the state-of-the-art results.\n\nWe also conduct an in-depth analysis to show the effectiveness, efficiency and interpretability of ME2ET.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "There are two lines of works conducted on utterance-level multi-modal emotion recognition. One line of works adopts a two-phase pipeline, first extracting features and then fusing them.\n\nMulT  (Tsai et al., 2019)  uses the cross-modal attention to capture the bi-modal interactions.\n\nEmoEmbs  (Dai et al., 2020)  leverages the crossmodal emotion embeddings for multi-modal emotion recognition.  Lv et al. (2021)  introduce the message hub to explore the inter-modal interactions. The other line of works trains the whole model in an end-to-end manner since they consider the extracted features may not be suitable for the target task and can not be fine-tuned, which may lead to sub-optimal performance. MESM  (Dai et al., 2021)  applies the VGG blocks to extract the visual and acoustic features and proposes the crossmodal attention to make the model focus on the important features. In this paper, we focus on the end-to-end multi-modal emotion recognition and propose the multi-modal end-to-end transformer (ME2ET). There are mainly three differences between our model and MESM. (1) MESM fails to leverage the feature interactions between the visual and acoustic modalities. To fully leverage tri-modal information, we propose the progressive tri-modal attention and tri-modal feature fusion layer to capture the tri-modal feature interactions;\n\n(2) we directly apply the vision transformer to process the raw audio and video data while MESM utilizes CNNs as the encoders considering its strong capacity of capturing the global intra-modal dependencies. Moreover, our model can easily benefit from the emerging vision transformer field, such as adopting stronger pre-trained transformers to boost model performance;\n\n(3) the proposed progressive tri-modal attention is different from the bi-modal attention adopted by MESM. The main idea of our proposed attention is to generate fewer but more useful tokens based on the input tokens. But the core idea of the cross-modal attention utilized by MESM is masking the less important area.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Transformer Network",
      "text": "Inspired by the recent successes of Transformer in NLP, multiple works try utilizing Transformer for image understanding. ViT  (Dosovitskiy et al., 2021)  is the first work attempting to apply a standard Transformer directly to images, which first splits an image into patches and then passes them to the transformer model. DeiT  (Touvron et al., 2021)  takes a convnet model as the teacher and learns knowledge from it by reproducing the label predicted by the teacher. Besides, the vision transformer can also be adapted for audio classification  (Gong et al., 2021) . Considering the huge computation and memory cost of Transformer, some transformer variants are proposed. Reformer  (Kitaev et al., 2020)  replaces dot-product attention in self-attention by one that uses locality-sensitive hashing and reduces the time and memory complexity.  Ryoo et al. (2021)  insert the TokenLearner layer into the middle of the vision transformer to reduce the token number. T2T-ViT  (Yuan et al., 2021)  utilizes a layer-wise Tokens-to-Token module to model the local structure information of the input image, which also reduces the length of tokens. TR-BERT  (Ye et al., 2021)  learns to dynamically select important tokens.\n\nOur work, in contrast to these works, explicitly leverages the tri-modal feature interaction to transform a long sequence of tokens into fewer but more informative tokens, which not only enhances the model performance but also significantly reduces the computation and memory cost.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Approach",
      "text": "In this section, we introduce the proposed multimodal end-to-end transformer (ME2ET) in detail, which is shown in Figure  2 . ME2ET adopts the progressive tri-modal attention to leverage the trimodal feature interactions at the low-level to generate fewer but more useful tokens based on the input tokens, which significantly reduces the memory and computation cost. In addition, ME2ET also uses the tri-modal feature fusion layer to model the tri-modal feature interactions at the high-level.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background",
      "text": "We first introduce the transformer architecture  (Vaswani et al., 2017) , which consists of two basic components: multi-head self-attention (MSA) and feed-forward network (FFN).\n\nMSA Given an input token sequence x ∈ R l×d , we first obtain the queries, keys, and values by Equation  1 . And then, for each element, we compute the similarity between it and all tokens and obtain the attention weights. Finally, we apply the weights to the values and get the outputs.\n\nwhere\n\nMSA is an extension of the self-attention (SA) introduced above. Specifically, We run k different self-attention functions, and project their concatenated outputs, resulting in the final results.\n\nFFN FFN consists of two linear layers with a GELU  (Hendrycks and Gimpel, 2016)  activation in between. Given an input sequence z ∈ R l×d , the outputs are calculated by Equation  4 .\n\nwhere The transformer architecture can be described as follows  1  .\n\nwhere t, e pos , and LN denote the input tokens, the position embeddings, and the LayerNorm layer  (Ba et al., 2016)  respectively.\n\nWe can use the transformer model to encode the input token embeddings t, producing z L , where L is the layer number. Generally, we take the representation of x cls as the output.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Token Construction",
      "text": "Visual Tokens To apply the transformer architecture to the visual input, we transform the raw data into tokens, which can be passed to the transformer model. Given a sequence of face images x v ∈ R J×H×W ×C , we split each image in them into a sequence of patches x v p ∈ R P ×P ×C without overlap, and then concatenate the patches, resulting in Q patches, where Q = JHW/P 2 is the number of patches. J, H, W , and C are the image number, the height of the face image, the width of the face image, and the channel number of the face image respectively. We then map each patch into a 1D token embedding using a linear projection layer, producing Q tokens t v ∈ R Q×d .\n\nAcoustic Tokens For the acoustic input, we first convert the input audio waveform into the spectrogram x a ∈ R F ×T ×1 and then split it into rectangular patches x a p ∈ R F ×2×1 in the temporal order to keep the timing information, where F is the dimension of features and T is the frame number. Finally, we apply a linear layer to transform the patches into tokens, producing M tokens t a ∈ R M ×d .\n\nTextual Tokens We construct the textual tokens following BERT (Kenton and Toutanova, 2019). We then pass the textual tokens to BERT and obtain the representation of [CLS], denoted as v l .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Progressive Tri-Modal Attention",
      "text": "To capture tri-modal feature interactions at the lowlevel, we propose the progressive tri-modal attention, which consists of two passes. In the first pass, we obtain the preliminary visual representation by leveraging the textual information. Note that, we do not directly take this representation as the final visual representation, as we consider that incorporating the acoustic information can further improve the visual representation. Specifically, the preliminary visual representation is obtained as follows. Given the visual tokens t v ∈ R Q×d and acoustic tokens t a ∈ R M ×d , instead of passing the transformed tokens directly into the transformer block, we first use the proposed attention mechanism to generate new visual tokens z v ∈ R K×d leveraging the textual information and the token number K is a hyper-parameter and is smaller than Q and M . This approach can significantly reduce the computation and memory complexity. Then we pass the z v to the visual transformer and obtain the preliminary visual representation v one .\n\nwhere v l is obtained by repeating v l , W lv ∈ R 2•d×K and b lv ∈ R K .\n\nIn the second pass, we utilize the textual representation v l and the preliminary visual representation v one to guide the model to address the important acoustic tokens and obtain the acoustic representation a.\n\nwhere v l , v v are obtained by repeating v l and v one respectively, W la ∈ R 3•d×K , and b la ∈ R K . Subsequently, we perform the attention over the original visual tokens again since in the first pass the acoustic information is not utilized, which is useful for selecting informative visual tokens. The generated tokens are then passed into the same visual transformer used in the first pass, producing the refined visual representation v. Finally, we get the visual representation v and acoustic representation a by leveraging the tri-modal feature interactions.\n\nwhere v l , v a are obtained by repeating v l and a respectively, W lv ∈ R 3•d×K , and b lv ∈ R K .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multi-Modal Feature Fusion",
      "text": "To capture the tri-modal feature interactions at the high-level, we propose the tri-modal feature fusion layer to fuse the semantic representations obtained by the transformer models and predict the results.\n\nFinally, we apply the decision fusion layer to aggregate the predicted results and generate the final prediction label.   (Busso et al., 2008) . We use the reorganized datasets released by the previous work  (Dai et al., 2021)  since the original datasets do not provide the aligned raw data. IEMOCAP consists of 7,380 annotated utterances. Each utterance in this dataset consists of three modalities: an audio file, a text transcript, and a sequence of sampled image frames, and each utterance is labeled with an emotion label from the set {angry, happy, excited, sad, frustrated, and neutral}. The CMU-MOSEI dataset consists of 20,477 annotated utterances. Each utterance consists of three modal inputs including an audio clip, a text transcript, and a sequence of sampled image frames, and each utterance is annotated with multiple emotion labels from the set {happy, sad, angry, fearful, disgusted, and surprised}. The statistics of the two adopted datasets are described in Appendix B. Following prior works  (Dai et al., 2021) , we use the Accuracy (Acc.) and F1-score to evaluate the models on IEMOCAP. For the CMU-MOSEI dataset, we take the weighted Accuracy (WAcc.) and F1-score as the metrics.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Details",
      "text": "We use Adam (Kingma and Ba, 2014) as the optimizer. We use the binary cross-entropy loss to train our model on both datasets. The learning rate is set to 1e-4. The epoch number and batch size are set to 40 and 8 respectively. The token number K is set to 256. The max lengths of the visual and textual tokens are set to 576 and 300 respectively. The Our experiments are run on a Tesla V100S GPU.\n\nFor the hyper-parameters of the transformer models, d, k, d h , and L are 768, 12, 64, and 12 respectively. In practice, we use the pre-trained weights of DeiT to initialize our visual transformer and acoustic transformer. For the visual modality, we adopt MTCNN  (Zhang et al., 2016)  to detect faces from the input images. The image resolution of the obtained face image is 128 by 128. We split the face images into 16 × 16 patches following DeiT  (Touvron et al., 2021) . For the acoustic modality, we convert the input audio waveform into a sequence of 128-dimensional log-Mel filterbank (fbank) features computed with a 25ms Hamming window every 10ms and split it into 128 × 2 patches following AST  (Gong et al., 2021) . We also analyse the effects of different patch construction methods on our proposed model in Appendix C.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "We compare our proposed model with the two lines of baselines. One line of models adopts a two-phase pipeline. LF-LSTM first uses LSTMs to encode the input features and then fuses them.\n\nLF-TRANS uses the transformer models to encode the input features and fuses them for prediction. EmoEmbs  (Dai et al., 2020)  leverages the cross-modal emotion embeddings for multi-modal emotion recognition. MulT  (Tsai et al., 2019)  utilizes the cross-modal attention to fuse multi-modal features. This line of models takes the extracted features as the inputs. The other baselines are trained in an end-to-end manner. FE2E  (Dai et al., 2021)  first uses two VGG models to encode the visual and acoustic inputs and then utilizes AL-BERT  (Lan et al., 2020)  to encode the textual input. MESM  (Dai et al., 2021)  utilizes the cross-modal sparse CNN block to capture the bi-modal interactions based on FE2E. FE2E-BERT and MESM-BERT take BERT as the textual encoder. These four baselines take the raw data as the input.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experimental Results",
      "text": "We compare our model with baselines, and the experimental results are shown in Table  1  and Table  2 . We can see from the results that our model surpasses all baseline models on two datasets. We attribute the success to strong model capacity and effective feature fusion. Specifically, our model uses three transformer models to model the raw input data, which can capture the global intra-modal dependencies. Moreover, we propose the progres- sive tri-modal attention and tri-modal feature fusion layer to model the tri-modal feature interactions, which enables the model to predict labels by leveraging the tri-modal information effectively.\n\nComparing with the two-phase pipeline models, we find that the end-to-end models can achieve better performance, and we attribute it to that the endto-end models can extract more task-discriminative features supervised by the target task loss. The comparison between MESM and FE2E shows that although MESM utilizes the bi-modal attention to capture the interactions between textual and acoustic/visual based on FE2E, MESM obtains worse results than FE2E on two datasets. This observation indicates that MESM fails to balance the computation cost with the model performance. Besides, we replace the ALBERT models of the end-to-end baselines with the BERT models for a fair comparison. But FE2E-BERT and MESM-BERT are not clearly superior to FE2E and MSEM, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Analysis",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct the ablation study to distinguish the contribution of each part. There are several variants of our model. ME2ET is our proposed full model. ME2ET w/o Attention does not use the progressive tri-modal attention to capture tri-modal interactions, which uses three transformer models separately to encode the inputs and applies the feature fusion and decision fusion layers to predict the result. ME2ET w/o Two-pass does not perform the attention over the visual tokens in the second pass to obtain the refined visual representation. ME2ET w/o Feature Fusion does not use the tri-modal feature fusion layer. The result of the ablation study is shown in Table  3 . We observe that ablating the progressive tri-modal attention hurts the model performance, which demonstrates the importance of our proposed attention. Comparing ME2ET w/o Two-pass with ME2ET, we find that performing the attention over the visual tokens again by leveraging the tri-modal information is useful for model prediction. We also observe that utilizing the proposed tri-modal feature fusion layer can boost the model performance, which can enable ME2ET to model the tri-modal feature interactions at high-level.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Computation And Memory Analysis",
      "text": "Computation Efficiency. Firstly, we adopt the number of float-point operations (FLOPs) as our metric to measure the computation complexity.\n\nThe performance of ME2ET w/o Attention and ME2ET with different token number K on IEMO-CAP is shown in Figure  3 . Comparing ME2ET with ME2ET w/o Attention, we can find that our models not only significantly reduce the computation complexity but also obtain better results than ME2ET w/o Attention. Specifically, when the token number K is set to 32, our proposed model achieves better performance than ME2ET w/o Attention while only requiring about 3 times less computation. We also observe that our model obtains better results as we increase the token number. This is in line with our expectations because a larger K enables the model to address the important information from more perspectives and also increases the model capacity. Secondly, in order to more accurately evaluate the speeds of models, we analyze the computation time performance of ME2ET and ME2ET w/o Attention and show the result in Figure  4 (a). We can see that ME2ET can obtain 1.2x∼1.6x speedup over ME2ET w/o Attention depending on the selected token number K, which indicates that ME2ET is computation-efficient.\n\nMemory Efficiency. We list the GPU memory required for ME2ET and ME2ET w/o Attention in Figure  4 (b). We can observe that with our proposed tri-modal attention, ME2ET only requires 48%∼77% GPU memory while achieves better performance. Specifically, when the token number K is set to 32, ME2ET only uses 48% of the memory required by ME2ET w/o Attention. This analysis shows that ME2ET is memory-efficient.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Visualization Analysis",
      "text": "To have an intuitive understanding of our proposed model, we visualize the attention weights of our progressive tri-modal attention. Specifically, we average the attention weights on the input tokens produced by different attention heads and use the obtained attention map to highlight the important regions. Based on the visualization result for the visual modality in Figure  5 , we observe that the proposed progressive tri-modal attention successfully addresses the important image regions. In the first pass, our model only uses the textual and visual information and fails to address the important patches. In the second pass, our model pays more attention to the raised lip corners by capturing the tri-modal feature interactions, which are useful for predicting emotion. As for the acoustic modality, we show an example in Figure  6 . The speaker stresses the word \"Fine\" to express her angry emotion and the model places more attention on the region corresponding to the word \"Fine\", which is in line with our expectation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose the multi-modal end-toend transformer (ME2ET), which utilizes the progressive tri-modal attention and tri-modal feature fusion layer to capture the tri-modal feature interactions at the low-level and high-level. We conduct extensive experiments on two datasets to evaluate our proposed model and the results demonstrate its effectiveness. The computation and memory analysis shows that ME2ET is computation and memoryefficient. With the proposed progressive tri-modal attention, ME2ET can achieve better performance by fully leveraging the tri-modal feature interactions, while obtaining 1.2x∼1.6x speedup and saving 23%∼52% GPU memory during training. The visualization analysis shows the interpretability of our model, which can successfully address the informative tokens. For future work, we would like to explore building a unified transformer model for multi-modal emotion recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A Complexity Analysis",
      "text": "The memory and time complexity of the selfattention and FFN blocks in Transformer are O(N 2 ) and O(N ) respectively  (Kitaev et al., 2020) . The self-attention scales quadratically with the length N , which makes it hard to directly apply Transformer to the raw audio and video. Given an utterance length of x seconds, the lengths of the visual and acoustic tokens are 128 • x and 100 • x. Let us take an example. Given an utterance length of 10s, the audio can be transformed into a very long sequence of acoustic tokens of which length is about 1000. If we directly feed them into the acoustic transformer, the computation and memory cost is very high. To address this problem, we propose the progressive tri-modal attention. The main idea of it is to fully leverage the tri-modal information to reduce the length of the input tokens. We utilize it to transform a long sequence ( N tokens) into a fixed-length sequence (K tokens, K N ) and reduce the memory and time complexity of the selfattention and FFN blocks from O(N 2 ) and O(N ) to O(K 2 ) and O(K) respectively.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B Datasets",
      "text": "We show the statistics of the two adopted datasets in Table  4 . The average duration of the utterances in CMU-MOSEI is much longer than IEMOCAP. Thus, we set the max lengths of the input acoustic tokens to 1024 for MOSEI and 512 for IEMOCAP. As for the visual modality, we only sample some key image frames from the video following the previous work  (Dai et al., 2021)  and set the max length of the input visual tokens to 576 for both datasets.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D Computation And Gpu Memory Evaluation",
      "text": "We benchmark by running a forward and backward pass. Each measurement is an average over 100 runs on an Tesla V100S 32GB. The code is implemented in the PyTorch framework v1.8.1  (Paszke et al., 2019) . The batch size is set to 8. The max lengths of the acoustic, visual, and textual tokens are set to 512, 576, and 300 respectively.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of ME2ET and the proposed",
      "page": 1
    },
    {
      "caption": "Figure 1: , which can capture the tri-modal feature",
      "page": 2
    },
    {
      "caption": "Figure 2: ME2ET adopts the",
      "page": 3
    },
    {
      "caption": "Figure 2: An illustration of the proposed ME2ET model. ME2ET uses the progressive tri-modal attention and the",
      "page": 4
    },
    {
      "caption": "Figure 3: Comparison of ME2ET w/o Attention and",
      "page": 7
    },
    {
      "caption": "Figure 4: Computation time (a) and GPU memory re-",
      "page": 7
    },
    {
      "caption": "Figure 3: Comparing ME2ET",
      "page": 7
    },
    {
      "caption": "Figure 5: Visualization of the progressive tri-modal at-",
      "page": 8
    },
    {
      "caption": "Figure 4: (a). We can see that ME2ET can obtain",
      "page": 8
    },
    {
      "caption": "Figure 4: (b). We can observe that with our pro-",
      "page": 8
    },
    {
      "caption": "Figure 5: , we observe that the",
      "page": 8
    },
    {
      "caption": "Figure 6: Visualization of the progressive tri-modal at-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Results on the IEMOCAP dataset. †indicates the results are copied from (Dai et al., 2021). The best",
      "data": [
        {
          "Pipeline": "End-to-End",
          "LF-LSTM†\nLF-TRANS†\nEmoEmbs†\nMulT†": "FE2E†\nMESM†\nFE2E-BERT\nMESM-BERT",
          "71.2\n49.4\n79.3\n57.2\n68.2\n51.5\n67.2\n37.6\n66.5\n47.0\n78.2\n54.0\n71.8\n49.5\n81.9\n50.7\n85.3\n57.3\n60.5\n49.3\n85.2\n37.6\n72.4\n49.7\n87.4\n57.4\n78.8\n50.3\n65.9\n48.9\n73.5\n58.3\n68.5\n52.0\n69.6\n38.3\n73.6\n48.7\n80.8\n53.0\n72.0\n49.8\n77.9\n60.7\n76.9\n58.0\n72.4\n57.0\n80.0\n46.8\n74.9\n53.7\n83.5\n65.4\n77.6\n56.9": "88.7\n63.9\n89.1\n61.9\n71.2\n57.8\n90.0\n44.8\n79.1\n58.4\n89.1\n65.7\n84.5\n58.8\n47.3\n88.2\n62.8\n88.3\n61.2\n74.9\n58.4\n89.5\n77.0\n52.0\n88.6\n62.2\n84.4\n57.4\n90.6\n64.4\n87.3\n59.9\n72.4\n59.8\n90.6\n43.4\n77.3\n54.0\n90.0\n65.2\n84.7\n57.8\n90.7\n86.6\n63.0\n89.2\n57.6\n75.6\n58.5\n43.1\n77.5\n54.6\n89.2\n65.3\n84.8\n57.0"
        },
        {
          "Pipeline": "End-to-End",
          "LF-LSTM†\nLF-TRANS†\nEmoEmbs†\nMulT†": "Ours",
          "71.2\n49.4\n79.3\n57.2\n68.2\n51.5\n67.2\n37.6\n66.5\n47.0\n78.2\n54.0\n71.8\n49.5\n81.9\n50.7\n85.3\n57.3\n60.5\n49.3\n85.2\n37.6\n72.4\n49.7\n87.4\n57.4\n78.8\n50.3\n65.9\n48.9\n73.5\n58.3\n68.5\n52.0\n69.6\n38.3\n73.6\n48.7\n80.8\n53.0\n72.0\n49.8\n77.9\n60.7\n76.9\n58.0\n72.4\n57.0\n80.0\n46.8\n74.9\n53.7\n83.5\n65.4\n77.6\n56.9": "65.9\n89.2\n63.9\n79.2\n60.7\n78.8\n58.7\n92.4\n73.8\n86.5\n61.3\n89.8\n90.0\n44.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Results on the IEMOCAP dataset. †indicates the results are copied from (Dai et al., 2021). The best",
      "data": [
        {
          "Pipeline": "End-to-End",
          "LF-LSTM†\nLF-TRANS†\nEmoEmbs†\nMulT†": "FE2E†\nMESM†\nFE2E-BERT\nMESM-BERT",
          "64.5\n47.1\n70.5\n49.8\n61.7\n22.2\n61.3\n73.2\n63.4\n47.2\n57.1\n20.6\n63.1\n43.3\n65.3\n47.7\n74.4\n51.9\n62.1\n24.0\n60.6\n72.9\n60.1\n45.5\n62.1\n24.2\n64.1\n44.4\n66.8\n49.4\n69.6\n48.7\n63.8\n23.4\n61.2\n71.9\n60.5\n47.5\n63.3\n24.0\n64.2\n44.2\n67.2\n75.4\n64.9\n47.5\n71.6\n49.3\n62.9\n25.3\n64.0\n48.3\n61.4\n25.6\n65.4\n45.2": "77.7\n57.1\n66.7\n29.1\n67.0\n49.6\n63.8\n26.8\n65.4\n72.6\n65.2\n49.0\n67.6\n47.4\n66.8\n49.3\n75.6\n56.4\n65.8\n28.9\n64.1\n72.3\n63.0\n46.6\n65.7\n27.2\n66.8\n46.8\n66.8\n49.4\n72.8\n55.0\n66.2\n29.1\n66.5\n71.4\n64.4\n48.4\n62.5\n28.1\n66.5\n46.9\n66.5\n49.2\n77.6\n54.0\n69.2\n28.4\n62.7\n72.2\n63.6\n47.7\n60.5\n26.3\n66.7\n46.3"
        },
        {
          "Pipeline": "End-to-End",
          "LF-LSTM†\nLF-TRANS†\nEmoEmbs†\nMulT†": "Ours",
          "64.5\n47.1\n70.5\n49.8\n61.7\n22.2\n61.3\n73.2\n63.4\n47.2\n57.1\n20.6\n63.1\n43.3\n65.3\n47.7\n74.4\n51.9\n62.1\n24.0\n60.6\n72.9\n60.1\n45.5\n62.1\n24.2\n64.1\n44.4\n66.8\n49.4\n69.6\n48.7\n63.8\n23.4\n61.2\n71.9\n60.5\n47.5\n63.3\n24.0\n64.2\n44.2\n67.2\n75.4\n64.9\n47.5\n71.6\n49.3\n62.9\n25.3\n64.0\n48.3\n61.4\n25.6\n65.4\n45.2": "67.9\n51.1\n69.3\n29.3\n66.2\n50.0\n68.3\n48.0\n76.4\n56.4\n66.4\n73.2\n63.3\n27.7"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ME2ET\n(K=256)": ""
        },
        {
          "ME2ET\n(K=256)": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "IEMOCAP",
          "Label": "Anger\nExcited\nFrustrated\nHappy\nNeutral\nSad",
          "Avg. duration(s)": "4.51\n4.78\n4.71\n4.34\n3.90\n5.50",
          "Train\nValid\nTest": "757\n112\n234\n736\n92\n213\n1,298\n180\n371\n398\n62\n135\n1,214\n173\n321\n759\n118\n207"
        },
        {
          "Dataset": "CMU-MOSEI",
          "Label": "Anger\nDisgusted\nFear\nHappy\nSad\nSurprised",
          "Avg. duration(s)": "23.24\n23.54\n28.82\n24.12\n24.07\n25.95",
          "Train\nValid\nTest": "3,267\n318\n1,015\n2,738\n273\n744\n1,263\n169\n371\n7,587\n945\n2,220\n4,026\n509\n1,066\n1,465\n197\n393"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Layer normalization",
      "authors": [
        "Jimmy Lei Ba",
        "Jamie Ryan Kiros",
        "Geoffrey Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "2",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "3",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Samuel Cahyawijaya",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "5",
      "title": "Modality-transferable emotion embeddings for low-resource multimodal emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Zihan Liu",
        "Tiezheng Yu",
        "Pascale Fung"
      ],
      "year": "2020",
      "venue": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "6",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly",
        "Jakob Uszkoreit",
        "Neil Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "7",
      "title": "AST: Audio Spectrogram Transformer",
      "authors": [
        "Yuan Gong",
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021",
      "doi": "10.21437/Interspeech.2021-698"
    },
    {
      "citation_id": "8",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "9",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "10",
      "title": "Reformer: The efficient transformer",
      "authors": [
        "Nikita Kitaev",
        "Lukasz Kaiser",
        "Anselm Levskaya"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "11",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Zhenzhong Lan",
        "Mingda Chen",
        "Sebastian Goodman",
        "Kevin Gimpel",
        "Piyush Sharma",
        "Radu Soricut"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "12",
      "title": "Efficient lowrank multimodal fusion with modality-specific factors",
      "authors": [
        "Zhun Liu",
        "Ying Shen",
        "Varun Bharadhwaj Lakshminarasimhan",
        "Paul Liang",
        "Amirali Bagher Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1209"
    },
    {
      "citation_id": "13",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Fengmao Lv",
        "Xiang Chen",
        "Yanyong Huang",
        "Lixin Duan",
        "Guosheng Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Amirali Bagher Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.214"
    },
    {
      "citation_id": "16",
      "title": "Mostafa Dehghani, and Anelia Angelova. 2021. Tokenlearner: What can 8 learned tokens do for images and videos? arXiv preprint",
      "authors": [
        "Michael S Ryoo",
        "Anurag Piergiovanni",
        "Arnab"
      ],
      "venue": "Mostafa Dehghani, and Anelia Angelova. 2021. Tokenlearner: What can 8 learned tokens do for images and videos? arXiv preprint",
      "arxiv": "arXiv:2106.11297"
    },
    {
      "citation_id": "17",
      "title": "Training data-efficient image transformers & distillation through attention",
      "authors": [
        "Hugo Touvron",
        "Matthieu Cord",
        "Matthijs Douze",
        "Francisco Massa",
        "Alexandre Sablayrolles",
        "Hervé Jégou"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "18",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1656"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "TR-BERT: Dynamic token reduction for accelerating BERT inference",
      "authors": [
        "Deming Ye",
        "Yankai Lin",
        "Yufei Huang",
        "Maosong Sun"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.463"
    },
    {
      "citation_id": "21",
      "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
      "authors": [
        "Li Yuan",
        "Yunpeng Chen",
        "Tao Wang",
        "Weihao Yu",
        "Yujun Shi",
        "Zi-Hang Jiang",
        "Francis Tay",
        "Jiashi Feng",
        "Shuicheng Yan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D17-1115"
    },
    {
      "citation_id": "23",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    }
  ]
}