{
  "paper_id": "2508.17502v1",
  "title": "Social-Mae: A Transformer-Based Multimodal Autoencoder For Face And Voice",
  "published": "2025-08-24T19:49:48Z",
  "authors": [
    "Hugo Bohy",
    "Minh Tran",
    "Kevin El Haddad",
    "Thierry Dutoit",
    "Mohammad Soleymani"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human social behaviors are inherently multimodal necessitating the development of powerful audiovisual models for their perception. In this paper, we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE), which is pre-trained on audiovisual social data. Specifically, we modify CAV-MAE to receive a larger number of frames as input and pre-train it on a large dataset of human social interaction (VoxCeleb2) in a self-supervised manner. We demonstrate the effectiveness of this model by finetuning and evaluating the model on different social and affective downstream tasks, namely, emotion recognition, laughter detection and apparent personality estimation. The model achieves state-of-the-art results on multimodal emotion recognition and laughter recognition and competitive results for apparent personality estimation, demonstrating the effectiveness of indomain self-supervised pre-training.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Human emotions and social behaviors are expressed and perceived through multiple modalities. While verbal communication can provide information on a person's communicative intent and emotions, non-verbal communication has shown to be equally or even more important  [26] . Socially intelligent systems require multimodal methods allowing them to perceive human social and expressive behaviors. Understanding expressions and social behaviors can be achieved by analyzing audiovisual modalities, i.e., face, body and voice. Although unimodal approaches, e.g., vision from facial expression or audio for tracking arousal, can reach a high performance  [5] ,  [18] , fusing two modalities increases the efficiency and robustness of multimodal systems  [29] ,  [10] . Information from different modalities can be congruent and reinforce each other for more effective communication. For instance, a smiling face aligned with a cheerful tone reinforce the expression of happiness. Information from different modalities can also be complementary, improving clarity and reducing uncertainty, such as a confident tone coupled with a smiling face. There is also a possibility of interaction between modalities generating new meanings from contradictory information from different modalities, e.g., expression of irony with conflicting face and voice behaviors.\n\nIn supervised learning, the availability of labeled data is often limited by laborious annotation. A common solution is to fine-tune a pre-trained model, i.e., to use models that have previously been trained on a larger dataset of similar nature  [8] ,  [35] ,  [22] . Self-supervised learning has been proposed to leverage large-scale unlabeled datasets to pretrain a model by pre-training models using a pretext task. One such pretext task is autoencoding, i.e., encoding input information into an often more compact latent representation and decoding it back to the original space. The encoder module of an autoencoder can be re-used as a pre-trained model with powerful and discriminative representations to be fine-tuned or re-used for downstream tasks.\n\nPast work extensively explored the natural interactions between audio and visual signals for representation learning  [32] ,  [33] ,  [31] ,  [30] ,  [1] ,  [11] ,  [37]  through self-supervision with a variety of pretext tasks. Synthesis-based strategies  [37] ,  [32] ,  [33]  have been proposed, where audio and visual signals are artificially combined to facilitate learning crossmodal associations. Alignment-based methods  [27] ,  [31] ,  [2] ,  [12] , on the other hand, focus on aligning signals from both modalities in time or space, aiming to extract meaningful correlations between them. Another line of research involves the application of masked autoencoding (MAE)  [14] , where the model learns to reconstruct the missing portions of either the audio or visual input, fostering representation learning through learning the structure of the data. Recently, two models, namely MAViL  [25]  and CAV-MAE  [20] , have explored the combination of MAE with contrastive learning and demonstrated state-of-the-art (SOTA) performance on audiovisual classification. Adding contrastive learning allows the models to learn inter-modality representations.\n\nDespite the popularity of emotion and social behavior perception, datasets for such tasks are often limited in size due to the high cost of labeling. Most existing audiovisual methods are based either on transfer learning with models trained on out-of-domain data, e.g., AudioSet  [13] , or trained from scratch. However, the desired input data should contain human faces and voices. Existing audiovisual encoders, e.g.,  [19] , also lack the temporal fidelity in the visual domain. In contrast, expressive behaviors in the human face are rather dynamic and fast-moving. There is limited work, e.g.,  [38] , on audiovisual encoders suitable for the automatic perception of human emotional and communicative behaviors. 979-8-3503-9494-8/24/$31.00 ©2024 IEEE Fig.  1 : Social-MAE model for voice and face analysis in videos. The model is pre-trained to reconstruct audio and visual modalities from masked portions of their corresponding input, narrowing the difference between each modality representation.\n\nIn this paper, we present Social-MAE, a pre-trained audiovisual model based on Masked Autoencoder. We aim to adapt a self-supervised method with superior results on audio event recognition for the audiovisual understanding of human social behaviors. We evaluate our model against several baselines on three different social and affective tasks: emotion recognition, laughter detection and apparent personality estimation. The main contributions of this work are as follows.\n\n• We present Social-MAE, a model based on CAV-MAE architecture adapted to social context by pre-training on a large-scale social dataset; • To develop Social-MAE, we modify CAV-MAE to accept multiple frames providing higher temporal fidelity at visual input; • Our experiments demonstrate the importance of indomain pre-training for affective and social tasks. Our model reaches or outperforms SOTA models on relevant tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Method",
      "text": "In this section, we present Social-MAE (Fig.  1 ), an adapted version of CAV-MAE that focuses on voice and face. The model is composed of two modality-specific encoders followed by a joint encoder module and a joint decoder module. Each module relies on a set of Transformer layers  [39]  made of an attention block, a feed-forward network, residual connections and layer normalization  [3] . We describe the pre-processing pipeline in Sec. II-A, the model overview in Sec. II-B and the self-supervised training in Sec. II-C.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. Audiovisual Tokenization",
      "text": "The architecture follows a mid-fusion scheme: both audio and video are first encoded in two separate branches for several encoder layers before merging into a joint encoder. Audio data are pre-processed as in CAV-MAE: we convert the input audio waveform into a sequence of 128-dimensional log Mel filterbank features computed with a 25 ms Hamming window and an overlap of 10 ms. We pad or crop the length of the input to keep 1024 audio frames, resulting in a 128 × 1024 spectrogram. The spectrogram is processed as an image that we split in N 16 × 16 non-overlapping patches. Each patch is projected with a linear layer to a 1dimensional embedding of size 768, referred to as a token. We add a trainable positional embedding to each token to provide information about the token order.\n\nVisual inputs differ from CAV-MAE as they consist of eight randomly selected frames as proposed in  [25]  rather than single frame. Each frame is an RGB image of the face bounding box scaled to 224 × 224 pixels, resulting in a 8 × 224 × 224 × 3 video input. We split the video into N 2 × 16 × 16 patches with no overlap, flatten and project with a linear layer into tokens of size 768. A trainable positional embedding is added to each token as well. Another trainable parameter provides information about each token's modality and weights the modality's importance. After adding positional and modality embeddings, a random mask with a rate of p% is applied to the input tokens, providing the model only with (1-p)% of the original audio and/or video sequence.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Model Description",
      "text": "This section presents an overview of the autoencoder architecture as described in  [20] . The model first processes an input sequence in separate encoders, each leveraging unimodal information. The modality encoders are stacks of 11 Transformer layers that aim to encode internal patterns in the input sequence. The joint encoder comprises a single Transformer layer on top of the modality encoders. Each modality is processed by the respective encoder followed by the joint encoder either individually or concatenated with the second modality depending on the targeted loss. The layer normalization on top of the joint encoder differs for audio, video and multimodal processing. The weights of the joint encoder are shared regardless of its input modality, as it was shown that weight sharing lightens the model without degrading performance  [28] . The unimodal tokens are averaged following the average pooling method, while the multimodal tokens are fed to the joint decoder, which is a stack of 8 Transformer layers. It aims to retrieve the original video and audio from an input sequence made of the encoded tokens and a learnable token M repeated at masked positions. The reconstruction loss is the distance between tokens M at the output of the decoder and their corresponding original tokens.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Self-Supervised Pre-Training",
      "text": "We adapted the pre-trained CAV-MAE model by training with self-supervision on the VoxCeleb2 dataset  [7] . VoxCeleb2 is an audiovisual dataset that contains over a million utterances from more than 6,000 speakers of 145 different nationalities. It provides a wide range of languages, accents, ethnicities and ages from real-world recordings. As self-supervised pre-training often requires vast amounts of data, we chose VoxCeleb2, as a suitable large and diverse audiovisual dataset with social content.\n\nThe learning phase relies on the weighted combination of contrastive and reconstruction loss that provides complementary information. For an input sequence of N pairs of audio and video tokens a i , v i , the contrastive loss L c is computed on modality averaged tokens c a i , c v i and aims to leverage relevant inter-modal information by following a LogSoftmax loss. The reconstruction loss L r evaluates the model's ability to reconstruct masked tokens x mask i from the tokens at the output of the decoder M i with an MSE loss. The final loss is the weighted sum of the contrastive and the reconstruction losses:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiments And Results",
      "text": "We pre-trained our Social-MAE during 25 epochs with a learning rate starting at 10 -4 and decreasing at a decay rate of 0.5 every 5 epochs with a masking ratio p=75%. For comparison, we also pre-trained CAV-MAE (as it uses 1 frame instead of 8) following the same settings. Both models were initialized on CAV-MAE scale+ weights pretrained on AudioSet-2M with self-supervision. We report visual zero-shot reconstruction in Fig.  2  using pre-trained Social-MAE on two downstream task datasets: CREMA-D  [6]  and ChaLearn First Impressions (FI)  [36] . The model is able to provide a convincing output on previously unseen data. Most reconstruction errors, although not obvious at first sight, come from the most dynamic areas of the face, such as the eyes or lips.\n\nFor downstream tasks, we remove the decoder from the architecture and replace it with a randomly initialized linear layer. We evaluate our pre-trained model by fine-tuning it on three different social and affective tasks: emotions recognition on CREMA-D  [6] , personality traits regression on ChaLearn FI  [36]  and smiles and laughter detection on NDC-ME  [23] . For each task, we describe the dataset, the finetuning pipeline and the evaluation metrics to compare CAV-MAE and Social-MAE models against published baselines, following their experimental settings for consistency.\n\nA. Emotion Recognition 1) Experimental setup: This task is evaluated on the Crowd-sourced Emotional Multimodal Actors Dataset (CREMA-D), containing 7,442 clips from 48 male and 43 female actors (20-74 years old). Each actor spoke 12 sentences using one of six emotions (anger, disgust, fear, happiness, sadness and neutral) ranging from 763 to 2204 utterances per emotion. Fine-tuning requires no masking on audio and visual tokens. We fine-tune pre-trained Social-MAE as well as our pre-trained version of CAV-MAE for 20 epochs using a mini-batch size of 8, learning rates at 10 -4 and 10 -5 for the encoders and the head respectively and we use the Cross-Entropy Loss.\n\n2) Baselines: a) UAVM:  [19]  presented UAVM, a unified audiovisual framework for classification. The model uses pre-trained CNN-based feature extractors on log Mel filterbanks and multi-frame visual inputs that are fed to Transformer layers.\n\nb) AuxFormer:  [16]  proposed AuxFormer, a multimodal model that fuses audio and visual tokens through Transformer inputs. The model also processes separate modalities through auxiliary networks. The model loss is a weighted combination of the network losses. Audio inputs are low-level descriptors from OpenSmile  [9]  toolkit, and visual inputs are face clips processed by pre-trained VGGface architecture  [34] .\n\nc) VAVL:  [17]  proposed an audiovisual model, named Versatile AudioVisual Learning (VAVL), which relies on the Conformer architecture  [21] . Each modality input flows through a separate encoder followed by a shared-weight conformer. Audio inputs are high-dimensional features from Wav2vec2.0  [24]  and visual inputs are face clips processed into emotional feature representations. 3) Results and discussion: Table  I  reports the F1-score with micro and macro averaging techniques. Social-MAE outperforms previously published methods for audiovisual classification. The micro F1 score shows the global accuracy, and the macro F1 score shows the unweighted average accuracy across each class, so the macro F1 score can be influenced by class imbalance. Since classes in CREMA-D range from 763 utterances (Sadness) to 2204 utterances (Neutral), we interpret the similarities between the macro and micro F1-scores reached by our pre-trained models as their ability to recognize emotions regardless of their prevalence.\n\nAdapted CAV-MAE competes for best audio-only classification against VAVL model. Social-MAE rivals the best baseline for visual classification. The performance is impressive when you consider that the former processes 8 frames and the latter processes high-level features from all input frames. We also find it interesting that adapted CAV-MAE is able to outperform multi-frame baselines AuxFormer and UAVM on  B. Personality Trait Prediction 1) Experimental setup: We evaluate personliaty prediction with the First Impressions (FI) dataset, a collection of 10,000 in-the-wild videos, in average 15s long. Videos are annotated with apparent personality traits known as big-5  [15] : Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. Fine-tuning requires no masking on audio and visual tokens. We fine-tuned both models presented in Sec. II-C for 10 epochs using a mini-batch size of 8, an encoder learning rate of 1e-4 and the classification head learning rate of 1e-5. We use a Mean Absolute Error loss and our accuracy metric is 1 -Mean Absolute Error.\n\nWe compare our fine-tuned CAV-MAE and Social-MAE to the best team of the challenge associated to the dataset: NJU-LAMDA  [40] , a model pre-trained on VGG-face. The audio input is log Mel filterbank and the visual input is the deep features from 100 frames. They train their model in 100 epochs for the audio stream and 3 epochs for the pre-trained visual stream, with a mini-batch of 128. 2) Results and discussion: Table  II  shows the accuracy of each personality trait on ChaLearn First Impressions dataset as well as the mean accuracy. Social-MAE shows a performance of 90.32% on average. While the accuracy is lower than the baseline, it remains impressive considering it was trained for only 10 epochs and with a smaller minibatch size. We can also observe that processing multiple frames simultaneously (Social-MAE) demonstrates better regressions on four out of five traits compared to the single frame method (CAV-MAE).  2) Results and discussion: Table  III  shows that both self-supervised methods reach higher F1-scores than the supervised baseline. Using multiple frames instead of one significantly improves the performance of the visual modality while slightly improving that of the multimodal classification.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "C. Smiles And",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusions",
      "text": "In this paper, we presented Social-MAE, our pre-trained audiovisual Masked AutoEncoder on audiovisual social data. We modified existing CAV-MAE to accept multiple frames on a large human social behavior dataset. We evaluated our model on three relevant downstream tasks, demonstrating its effectiveness in achieving state-of-the-art results in audiovisual emotion recognition with a 0.837 F1 score and laughter detection with a 0.776 F1 score. With this work, we demonstrated the significance of in-domain adaptation of a large multimodal model trained through self-supervised pretraining. The proposed pre-trained encoder can be easily finetuned for other audiovisual social behavior understanding tasks, enabling more robust and performant models for perceiving human behaviors.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Social-MAE model for voice and face analysis in videos. The model is pre-trained to reconstruct audio and visual",
      "page": 2
    },
    {
      "caption": "Figure 2: using pre-trained",
      "page": 3
    },
    {
      "caption": "Figure 2: Social-MAE visual zero-shot reconstruction on CREMA-D and ChaLearn First Impressions datasets. The first row",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Self-supervised learning of audio-visual objects from video",
      "authors": [
        "T Afouras",
        "A Owens",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "2",
      "title": "Look, listen and learn",
      "authors": [
        "R Arandjelovic",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "3",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "Layer normalization",
      "arxiv": "arXiv:1607.06450"
    },
    {
      "citation_id": "4",
      "title": "A New Perspective on Smiling and Laughter Detection: Intensity Levels Matter",
      "authors": [
        "H Bohy",
        "K Haddad",
        "T Dutoit"
      ],
      "venue": "2022 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "5",
      "title": "Marlin: Masked autoencoder for facial video representation learning",
      "authors": [
        "Z Cai",
        "S Ghosh",
        "K Stefanov",
        "A Dhall",
        "J Cai",
        "H Rezatofighi",
        "R Haffari",
        "M Hayat"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Voxceleb2: Deep speaker recognition",
      "authors": [
        "J Chung",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Voxceleb2: Deep speaker recognition",
      "arxiv": "arXiv:1806.05622"
    },
    {
      "citation_id": "8",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "9",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, MM '10"
    },
    {
      "citation_id": "10",
      "title": "Large scale audiovisual learning of sounds with weakly labeled data",
      "authors": [
        "H Fayek",
        "A Kumar"
      ],
      "year": "2020",
      "venue": "Large scale audiovisual learning of sounds with weakly labeled data",
      "arxiv": "arXiv:2006.01595"
    },
    {
      "citation_id": "11",
      "title": "Rolling-unrolling lstms for action anticipation from first-person video",
      "authors": [
        "A Furnari",
        "G Farinella"
      ],
      "year": "2020",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "12",
      "title": "Visualechoes: Spatial image representation learning through echolocation",
      "authors": [
        "R Gao",
        "C Chen",
        "Z Al-Halah",
        "C Schissler",
        "K Grauman"
      ],
      "year": "2020",
      "venue": "Computer Vision-ECCV 2020: 16th European Conference"
    },
    {
      "citation_id": "13",
      "title": "Audio set: An ontology and human-labeled dataset for audio events",
      "authors": [
        "J Gemmeke",
        "D Ellis",
        "D Freedman",
        "A Jansen",
        "W Lawrence",
        "R Moore",
        "M Plakal",
        "M Ritter"
      ],
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "14",
      "title": "Audiovisual masked autoencoders",
      "authors": [
        "M.-I Georgescu",
        "E Fonseca",
        "R Ionescu",
        "M Lucic",
        "C Schmid",
        "A Arnab"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "An Alternative \"Description of Personality\": The Big-Five Factor Structure",
      "authors": [
        "L Goldberg"
      ],
      "venue": "Personality and Personality Disorders"
    },
    {
      "citation_id": "16",
      "title": "AuxFormer: Robust Approach to Audiovisual Emotion Recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Versatile audio-visual learning for handling single and multi modalities in emotion regression and classification tasks",
      "authors": [
        "L Goncalves",
        "S.-G Leem",
        "W.-C Lin",
        "B Sisman",
        "C Busso"
      ],
      "year": "2023",
      "venue": "Versatile audio-visual learning for handling single and multi modalities in emotion regression and classification tasks",
      "arxiv": "arXiv:2305.07216"
    },
    {
      "citation_id": "18",
      "title": "Ast: Audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "Y.-A Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "Ast: Audio spectrogram transformer",
      "arxiv": "arXiv:2104.01778"
    },
    {
      "citation_id": "19",
      "title": "Uavm: Towards unifying audio and visual models",
      "authors": [
        "Y Gong",
        "A Liu",
        "A Rouditchenko",
        "J Glass"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "20",
      "title": "Contrastive audio-visual masked autoencoder",
      "authors": [
        "Y Gong",
        "A Rouditchenko",
        "A Liu",
        "D Harwath",
        "L Karlinsky",
        "H Kuehne",
        "J Glass"
      ],
      "year": "2022",
      "venue": "Contrastive audio-visual masked autoencoder",
      "arxiv": "arXiv:2210.07839"
    },
    {
      "citation_id": "21",
      "title": "Conformer: Convolution-augmented transformer for speech recognition",
      "authors": [
        "A Gulati",
        "J Qin",
        "C.-C Chiu",
        "N Parmar",
        "Y Zhang",
        "J Yu",
        "W Han",
        "S Wang",
        "Z Zhang",
        "Y Wu"
      ],
      "year": "2020",
      "venue": "Conformer: Convolution-augmented transformer for speech recognition",
      "arxiv": "arXiv:2005.08100"
    },
    {
      "citation_id": "22",
      "title": "Neighborhood Attention Transformer",
      "authors": [
        "A Hassani",
        "S Walton",
        "J Li",
        "S Li",
        "H Shi"
      ],
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "23",
      "title": "A Dyadic Conversation Dataset on Moral Emotions",
      "authors": [
        "L Heron",
        "J Kim",
        "M Lee",
        "K Haddad",
        "S Dupont",
        "T Dutoit",
        "K Truong"
      ],
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "24",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "25",
      "title": "Mavil: Masked audio-video learners",
      "authors": [
        "P.-Y Huang",
        "V Sharma",
        "H Xu",
        "C Ryali",
        "Y Li",
        "S.-W Li",
        "G Ghosh",
        "J Malik",
        "C Feichtenhofer"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Nonverbal communication in human interaction",
      "authors": [
        "M Knapp",
        "J Hall",
        "T Horgan"
      ],
      "year": "1978",
      "venue": "Nonverbal communication in human interaction"
    },
    {
      "citation_id": "27",
      "title": "Cooperative learning of audio and video models from self-supervised synchronization",
      "authors": [
        "B Korbar",
        "D Tran",
        "L Torresani"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Parameter efficient multimodal transformers for video representation learning",
      "authors": [
        "S Lee",
        "Y Yu",
        "G Kim",
        "T Breuel",
        "J Kautz",
        "Y Song"
      ],
      "year": "2020",
      "venue": "Parameter efficient multimodal transformers for video representation learning",
      "arxiv": "arXiv:2012.04124"
    },
    {
      "citation_id": "29",
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "A Nagrani",
        "S Yang",
        "A Arnab",
        "A Jansen",
        "C Schmid",
        "C Sun"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "30",
      "title": "You2me: Inferring body pose in egocentric video via first and second person interactions",
      "authors": [
        "E Ng",
        "D Xiang",
        "H Joo",
        "K Grauman"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Audio-visual scene analysis with selfsupervised multisensory features",
      "authors": [
        "A Owens",
        "A Efros"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "32",
      "title": "Visually indicated sounds",
      "authors": [
        "A Owens",
        "P Isola",
        "J Mcdermott",
        "A Torralba",
        "E Adelson",
        "W Freeman"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "33",
      "title": "Ambient sound provides supervision for visual learning",
      "authors": [
        "A Owens",
        "J Wu",
        "J Mcdermott",
        "W Freeman",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "34",
      "title": "Deep Face Recognition",
      "authors": [
        "O Parkhi",
        "A Vedaldi",
        "A Zisserman"
      ],
      "venue": "Procedings of the British Machine Vision Conference 2015"
    },
    {
      "citation_id": "35",
      "title": "Scalable diffusion models with transformers",
      "authors": [
        "W Peebles",
        "S Xie"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "36",
      "title": "ChaLearn LAP 2016: First Round Challenge on First Impressions -Dataset and Results",
      "authors": [
        "V Ponce-López",
        "B Chen",
        "M Oliu",
        "C Corneanu",
        "A Clapés",
        "I Guyon",
        "X Baró",
        "H Escalante",
        "S Escalera"
      ],
      "venue": "Computer Vision -ECCV 2016 Workshops"
    },
    {
      "citation_id": "37",
      "title": "Learning lip-based audio-visual speaker embeddings with av-hubert",
      "authors": [
        "B Shi",
        "A Mohamed",
        "W.-N Hsu"
      ],
      "year": "2022",
      "venue": "Learning lip-based audio-visual speaker embeddings with av-hubert",
      "arxiv": "arXiv:2205.07180"
    },
    {
      "citation_id": "38",
      "title": "Saaml: A framework for semi-supervised affective adaptation via metric learning",
      "authors": [
        "M Tran",
        "Y Kim",
        "C.-C Su",
        "C.-H Kuo",
        "M Soleymani"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia, MM '23"
    },
    {
      "citation_id": "39",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "40",
      "title": "Deep Bimodal Regression for Apparent Personality Analysis",
      "authors": [
        "C.-L Zhang",
        "H Zhang",
        "X.-S Wei",
        "J Wu"
      ],
      "venue": "Computer Vision -ECCV 2016 Workshops"
    }
  ]
}