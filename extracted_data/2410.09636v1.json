{
  "paper_id": "2410.09636v1",
  "title": "Can We Estimate Purchase Intention Based On Zero-Shot Speech Emotion Recognition?",
  "published": "2024-10-12T20:25:16Z",
  "authors": [
    "Ryotaro Nagase",
    "Takashi Sumiyoshi",
    "Natsuo Yamashita",
    "Kota Dohi",
    "Yohei Kawaguchi"
  ],
  "keywords": [
    "Speech emotion recognition",
    "contrastive language-audio pre-training",
    "zero-shot",
    "purchase intention"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper proposes a zero-shot speech emotion recognition (SER) method that estimates emotions not previously defined in the SER model training. Conventional methods are limited to recognizing emotions defined by a single word. Moreover, we have the motivation to recognize unknown bipolar emotions such as \"I want to buy -I do not want to buy.\" In order to allow the model to define classes using sentences freely and to estimate unknown bipolar emotions, our proposed method expands upon the contrastive language-audio pre-training (CLAP) framework by introducing multi-class and multi-task settings. We also focus on purchase intention as a bipolar emotion and investigate the model's performance to zero-shot estimate it. This study is the first attempt to estimate purchase intention from speech directly. Experiments confirm that the results of zero-shot estimation by the proposed method are at the same level as those of the model trained by supervised learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) is a technique to estimate emotions conveyed by speech. This technique can be applied to the speech analysis of call centers  [1] , spoken dialogue agent  [2] , e-learning system  [3] , and mental health analysis  [4] , etc.\n\nMany researchers have proposed various methods to improve the performance of SER. Especially, in recent years, there are many methods using deep learning such as SER with self-supervised learning model  [5] ,  [6] . In these methods, the model of SER is often trained by supervised-learning. Therefore, the trained model can recognize predefined seen emotions, while it is difficult to recognize unseen emotions. To alleviate this limitation, previous studies have proposed methods of zero-shot SER that can recognize undefined emotions. For example, X. Xu, et al. proposed the method using auditory affective descriptors (AAD) to transfer knowledge from the seen domain to the unseen domain  [7] , and the method using reconstructed semantic prototypes and data augmentation for the unseen domain  [8] .\n\nThe challenge of zero-shot SER is how to define the classes of emotions. Conventional methods do not assume the estimation of emotion classes that cannot be defined by a single word. Therefore, it is difficult to recognize the emotions expressed by text, such as \"the feeling of willing to buy (purchase intention).\" In order to realize SER capable of zeroshot recognizing various emotions, it is necessary to have a framework that can freely define classes during the training and estimation phase. We also have the motivation to realize the zero-shot estimation of bipolar emotions, such as \"I want to buy -I do not want to buy.\"\n\nTo meet these requirements, in this study, we propose a new contrastive language-audio pre-training (CLAP) method that can perform zero-shot estimation of bipolar emotions. CLAP can decide the emotion freely because it can represent the classification class by the sentence. In our method, we define the multi-class emotions with a bipolar sub-class and train the model with CLAP for each emotion. We use six basic emotions with a bipolar sub-class as the multi-class emotions. We expect this model trained by the proposed method to correctly classify the emotions using the knowledge of six basic emotions, even if the estimated class is an unknown bipolar emotion. We also focus on purchase intention as a bipolar emotion and experiment on whether the model trained by the proposed method can zero-shot estimate purchase intention. The contributions of this study are as follows.\n\n• This is the first method for zero-shot SER capable of estimating bipolar emotions, such as purchase intention. • This study is the first time to directly estimate purchase intention from speech. This paper proceeds as follows. In Section2, we describe the method of the basic CLAP, proposed method with CLAP, and the data augmentation. In Section3, we explain the setup for the experiment and show the results. Finally, in Section4, we present our conclusions and future work.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Methodology A. Clap",
      "text": "CLAP is the training scheme that calculates the similarity or dissimilarity between acoustic and linguistic embeddings  [9] . This method makes it possible for us to define the categories during the estimation phase and classify speech into unseen categories. Moreover, the model can estimate more various categories because CLAP can define them with the sentence.\n\nThe overviews of CLAP during the training or estimation phase are shown in Figures  1  and 2 , respectively. The inputs are audio and text of categories. Let x ∈ R N ×T , y ∈ R N ×L be the audio and the text, respectively, where T and L are the length of them, and N is the size of mini-batch. Note that x, y have correspondence in mini-batch.\n\nThe inputs are converted to the embeddings by the audio and text encoders. The audio encoder f a (•) and text encoder f t (•)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Estimated Calss",
      "text": "Fig.  2 . Overview of clap during the estimation phase are given in Equation 1 and 2, respectively. Let X ∈ R N ×Da , Y ∈ R N ×Dt be the audio embeddings and text embeddings, where D a , D t are the number of dimensions of each encoder's output. Moreover, Pooling is applied to average or select the embeddings from each encoder.\n\nThose embeddings are input into the linear projection, which are given in Equation 3 and 4. Let E a ∈ R N ×D , E t ∈ R N ×D be the fixed embeddings of D dimensions from each linear projection. Also, Linear a (•), Linear t (•) are the linear projection for audio and text, respectively.\n\nDuring the training phase, we calculate the similarity matrix M ∈ R N ×N given in Equation  5 . Let τ be the temperature parameter.\n\nAfter that, we calculate and optimize the symmetric loss function  [10]  L(•) given in Equation  6 , where CE(•) is the function of cross entropy, and M ∈ R N ×N is the ground truth matrix. Optimizing this loss function, we can train the model to minimize the loss between the ground truth and the estimated values and maximize the loss between the incorrect answer and the estimated values.\n\nDuring the estimation phase, we defined the categorical classes by words or sentences. Then, we input the speech and texts represented the categories into the encoder and linear projection corresponding to each modalities, and obtain fixedlength vectors. Finally, the similarity between speech and text is calculated using the fixed-length vectors, and the class indicated by the text of the highest similarity is the estimation result.\n\nThis method has been applied to SER. For example, in the original paper on CLAP, zero-shot SER was performed using the CLAP model trained with environmental sound datasets  [9] . Y. Pan et al. also proposed GEmo-CLAP for gender and emotion classification and improved the accuracy rate of four emotion classifications that were not zero-shot  [11] . However, there is still no research on recognizing emotion-related classes such as purchase intention using the model of zero-shot SER trained with emotional speech datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Zero-Shot Ser With Multi-Class Multi-Task Clap",
      "text": "We propose the training method of the zero-shot SER for multi-class emotional speeches, which is called multi-class multi-task CLAP. The overview of the proposed method is shown in Figure  3 .\n\nDuring the training phase, the model outputs the similarity matrix of the multi-class prepared for each emotion. After calculating the contrastive loss with the ground truth matrix in each emotion, we summarize and optimize them. Equation  7 formulates the calculation of the loss L all . Let K be the number of emotion classes, k be the index of an emotion class, and M k ∈ R N ×N , Mk ∈ R N ×N be the similarity matrix and ground truth matrics in each emotion, respectively. During the estimation phase, we also define the categories and classify speeches as before. The model trained by the proposed method can recognize multi-class emotions with the sub-class. Therefore, we expect the proposed method will enable the zero-shot SER to estimate emotions such as unknown bipolar emotions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. The Data Augmentation Of Multi-Class By Paraphrasing",
      "text": "We extend the dataset by paraphrasing the text indicating the multi-class to increase the vocabulary and grammar of text associated with speech. Paraphrasing was performed using GPT4, a large-scale language model provided by OpenAI 1 . The template of prompts given to GPT4 is shown in Table  I . GPT4 performs better in English than Japanese  [12] , so the prompt is written in English. The prompt includes the text of multi-class and the forbidden word. If the forbidden word is included in the output, we modify it by hand.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Experiments",
      "text": "We experimented with the binary classification task of whether purchase intention from speech. In the following, we explain the experimental setup and results.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset",
      "text": "Before the experiment, we created the internal dataset which has annotated emotional utterances in Japanese. First, we collected emotional utterances by role-playing conversations. We prepared six scenarios related to conversations between salespeople and customers. Five Japanese speakers talked freely for each scenario. Note that we did not prepare sentences for reading. The sampling rate of the recorded utterance was 44,100 Hz. The recorded data were separated into sessions for each scenario, and we obtained the dataset consisting of six sessions. Afterward, three annotators listened to collected utterances and scored the intensity values of the six emotions (pleasant-unpleasant, aroused-sleepy, dominantsubmissive, credible-doubtful, interested-indifferent, positivenegative) on a seven-point scale (1.very low-7.very high). These emotions were assigned according to the related work  [13] . In addition, these annotators scored the intensity values indicating the purchase intention on a seven-point scale for only customer's data at corrected utterances.\n\n1 https://openai.com/gpt-4 Finally, we arranged the labels for the classification task. The distribution of scores of each annotator was normalized by the mean and variance of the distribution of scores given by all annotators. The normalized labels exclude the most frequent value and sets it as a threshold. The score below the threshold are relabeled as negative labels (0) and the score above the threshold are relabeled as positive labels  (1) . The number of utterances in two classes of six emotions was 2,598, and the number of utterances with the two classes of the purchase intention was 940. We converted the sampling rate of each utterance to 16,000Hz.\n\nDuring the training and estimation phase using the proposed method, we used descriptions that explain the emotions corresponding to the scores aggregated above. Table  II  shows the correspondence between the class scores of each emotion and the descriptions of each emotion. In addition, we experimented with the augmented data by paraphrasing, which was 11 times the number of the original dataset. In the evaluation, we used one of the six sessions as the testing data and others as the training data.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Models And Metrics",
      "text": "We explain the model structure of the proposed method. The audio encoder consisted of Japanese HuBERT  [14]  and linear projection. The output dimensions of them were 768 and 512, respectively. Before passing embeddings into linear projection, we averaged the audio embeddings obtained from HuBERT in the time direction. The text encoder consisted of Japanese DistilBERT  [15]  and linear projection. The output dimensions of them were also 768 and 512, respectively. Before passing embeddings into linear projection, we only used class token from the embeddings obtained from DistilBERT. We used the pre-trained parameters of HuBERT 2  and DistilBERT 3  provided by Hugging Face  [16] .\n\nThe baseline in this paper was the model to estimate purchase intention by supervised learning instead of CLAP. This model structure was a combination of HuBERT, linear projection, and one fully connected layer, and we used the pretrained HuBERT in the same way as the proposed method. The threshold for binary classification on the baseline is defined by the Youden index of the receiver operatorating characteristic (ROC) curve. The number of epochs was 300, the batch size was 64, the learning rate was 0.000001, and the optimization method was Adam  [17] . The loss functions of the baseline and the proposed method were binary cross entropy loss and symmetric cross entropy loss, respectively.\n\nWe used weighted accuracy (WA), unweighted accuracy (UA), and recall of each category as the evaluation metrics. Note that WA is the total number of correct answers to the total data, and UA is the average recall of each category. The higher these metrics are, the more correctly the performance can estimate purchase intention. Moreover, we calculated the frequency rate of matching the random value and the correct answer as a chance rate. In the evaluation of the proposed method, we used the three texts. They all indicated the purchase intention, while the words used in the sentence differed.\n\nWe used these texts to compare the proposed method with the chance rate and baseline.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Results",
      "text": "Table  III  shows WA, UA, and recall of each category. The result of the random was about 50% for both WA and UA. Comparing the result of the random with the baseline, WA was 24.5%, and UA was 19.8% higher in the baseline result than in the random. These results indicate that the baseline model does not make a random estimation. It also shows that it is possible to estimate purchase intention from speech using deep learning. Comparing the result of the random with the proposed methods, WA and UA were higher in all proposed methods than in the random. These results show that the zeroshot estimation by the proposed method is not random, and these methods can train the model to estimate unknown bipolar emotions.\n\nWithout data augmentation, the results show that the recall with purchase intention \"Yes\" using text (1) is higher in the proposed method than in the baseline. The UA using text (1) and the recall with purchase intention \"Yes\" using text (3) were similar to the baseline result. On the other hand, the recalls with purchase intention \"No\" using text (1) and \"Yes\" using text (2) were lower than 60%. It indicates that the model trained by the proposed method cannot represent the correspondence well between the semantic information of text and speech under some conditions.\n\nAll evaluation scores with data augmentation are improved overall than those without data augmentation. Those scores equal the baseline, especially when using text (3). This indicates that data augmentation make it possible for the model to train the correspondence between text and speech semantic information relatively more easy. These results indicate that the zero-shot SER trained by the proposed method can estimate purchase intention that is unseen class by data augmentation of paraphrasing and appropriate creation of text indicating classes.\n\nWe also investigated the significant difference between the evaluation scores of the baseline and the proposed method using text (3) through a sign test. If there was no significant difference between them, we could assume that the number of samples that were correct by the baseline and incorrect by the proposed method was the same as those that were correct by the proposed method and incorrect by the baseline. We performed a two-tailed sign test based on the above hypothesis.\n\nThe p-value p about the baseline and the proposed method using text (3) without data augmentation was less than 0.05 (p < 0.05). On the other hand, the p-value p about the baseline and the proposed method using text (3) with data augmentation was more than 0.05 (p > 0.05). These results indicated that the significant difference between the baseline and the proposed method decreased with data augmentation. In other words, when using text (3) along with data augmentation, the proposed method can provide zero-shot estimates comparable to the supervised method.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Conclusion",
      "text": "This study proposed a new method for zero-shot speech emotion recognition. The proposed method is a multi-class multi-task CLAP that can recognize unknown bipolar emotions. We found that zero-shot SER trained by the proposed method with paraphrasing data augmentation can be used to estimate purchase intention with the same accuracy as a supervised learning model. In the future, we would like to expand speech data with multi-class labels and perform zero-shot inference for generously related classes other than purchase intention.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of clap during the training phase",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of clap during the estimation phase",
      "page": 2
    },
    {
      "caption": "Figure 3: Overview of the proposed method",
      "page": 2
    },
    {
      "caption": "Figure 3: During the training phase, the model outputs the similarity",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "...": ""
        },
        {
          "...": ""
        },
        {
          "...": ".\n.\n."
        },
        {
          "...": "..."
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Call redistribution for a call center based on speech emotion recognition",
      "authors": [
        "M Bojanić",
        "V Delić",
        "A Karpov"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "2",
      "title": "Alexa, My love: Analyzing reviews of Amazon Echo",
      "authors": [
        "Y Gao",
        "Z Pan",
        "H Wang",
        "G Chen"
      ],
      "venue": "Proc"
    },
    {
      "citation_id": "3",
      "title": "Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation",
      "year": "2018",
      "venue": "Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition in e-learning system based on affective computing",
      "authors": [
        "W Li",
        "Y Zhang",
        "Y Fu"
      ],
      "year": "2007",
      "venue": "Proc. ICNC 2007 -Third International Conference on Natural Computation"
    },
    {
      "citation_id": "5",
      "title": "LSSED: A large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "W Fan",
        "X Xu",
        "X Xing",
        "W Chen",
        "D Huang"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "8",
      "title": "Autonomous emotion learning in speech: A view of zero-shot speech emotion recognition",
      "authors": [
        "X Xu",
        "J Deng",
        "N Cummins",
        "Z Zhang",
        "L Zhao",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH 2019 -20 th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "9",
      "title": "Zero-shot speech emotion recognition using generative learning with reconstructed prototypes",
      "authors": [
        "X Xu",
        "J Deng",
        "Z Zhang",
        "Z Yang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "CLAP: Learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proc. ICML'21 -38 th International Conference on Machine Learning (ICML)"
    },
    {
      "citation_id": "12",
      "title": "GEmo-CLAP: Gender-attribute-enhanced contrastive language-audio pretraining for accurate speech emotion recognition",
      "authors": [
        "Y Pan",
        "Y Hu",
        "Y Yang",
        "W Fei",
        "J Yao",
        "H Lu",
        "L Ma",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "GEmo-CLAP: Gender-attribute-enhanced contrastive language-audio pretraining for accurate speech emotion recognition"
    },
    {
      "citation_id": "13",
      "title": "GPT-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report"
    },
    {
      "citation_id": "14",
      "title": "Constructing a spoken dialogue corpus for studying paralinguistic information in expressive conversation and analyzing its statistical/acoustic characteristics",
      "authors": [
        "H Mori",
        "T Satake",
        "M Nakamura",
        "H Kasuya"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "15",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "16",
      "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": "2019",
      "venue": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
    },
    {
      "citation_id": "17",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "Q Lhoest",
        "A Rush"
      ],
      "year": "2020",
      "venue": "Proc. EMNLP 2020 -2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    },
    {
      "citation_id": "18",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization"
    }
  ]
}