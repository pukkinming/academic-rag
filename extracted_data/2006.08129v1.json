{
  "paper_id": "2006.08129v1",
  "title": "Emotion Recognition In Audio And Video Using Deep Neural Networks",
  "published": "2020-06-15T04:50:18Z",
  "authors": [
    "Mandeep Singh",
    "Yuan Fang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Humans are able to comprehend information from multiple domains for e.g. speech, text and visual. With advancement of deep learning technology there has been significant improvement of speech recognition. Recognizing emotion from speech is important aspect and with deep learning technology emotion recognition has improved in accuracy and latency. There are still many challenges to improve accuracy. In this work, we attempt to explore different neural networks to improve accuracy of emotion recognition. With different architectures explored, we find (CNN+RNN) + 3DCNN multi-model architecture which processes audio spectrograms and corresponding video frames giving emotion prediction accuracy of 54.0% among 4 emotions and 71.75% among 3 emotions using IEMOCAP[2] dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is an important ability for good interpersonal relations and plays an important role in an effective interpersonal communications. Recognizing emotions, however, could be hard; even for human beings, the ability of emotion recognition varies among persons.\n\nThe aim of this work is to recognize emotions in audios and audios+videos using deep neural networks. In this work, we attempt to understand bottlenecks in existing architecture and input data, and explored novel ways on top of existing architectures to increase emotion recognition accuracy.\n\nThe dataset we used is IEMOCAP  [2] , which contains 12 hours audiovisual data of 10 people(5 females, 5 males) speaking in anger, happiness, excitement, sadness, frustration, fear, surprise, other and neutral state.\n\nOur work mainly consists of two stages. First, we build neural networks to recognize emotions in audios by replicating and expanding upon the work of  [13] . The input of the models are the audio spectrograms converted from the audio of an actor speaking a sentence, and the models give one output which is the emotion the actor has when saying that sentence. The models only predict one of the four different emotions, e.g. happiness, anger, sadness, and neutral state, which were chosen for comparison with  [13] . The deep learning architectures we explored were CNN, CNN + RNN, CNN + LSTM.\n\nAfter achieving a comparably good accuracy on audios comparing with  [13] , we build models which predict emotions using audio spectrogram and video frames in a video, since we believe video frames contain additional emotionrelated information that can help us to achieve a better emotion prediction performance. The inputs of the models are the audio spectrogram and video frames, which are converted and extracted from the sound and images of a video recording an actor speaking one sentence. The output of the models is still one of the four selected emotions mentioned above. Inspired by the work of  [14] , we explore the model made of two sub-networks; the first sub-network is a 3D CNN which takes in the video frames, and the second one is a CNN+RNN which takes in the audio spectrogram, and the last layer of the two sub-networks are concatenated and followed by a fully connected layer that output the prediction.\n\nThe metric we use for evaluation is the overall accuracy for both the audio and audio+video models.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Emotion recognition is an important research area that many researchers work on in recent years using various methods. Using speech signals  [9] , facial expression  [4] , and physiological changes  [7]  are some of the common approaches researchers arise to approach the emotion recognition problem. In this work, we will use audio spectrograms and video frames to do emotion recognition.\n\nIt has been shown, emotion recognition accuracy can be improved with statistical learning of low-level features (frequency & signal power intensity) by different layers of deep learning network. Mel-scale spectrograms for speech recognition was demonstrated to be useful in  [3] . There has been state of the art speech recognition method that uses linearly spaced audio spectrograms as described in  [1]  Figure  1 . Example of audio spectrogram of anger emotion. Original time scale without noise cleanup.  [5] . Our work related to emotion recognition using audio spectrogram follows the approach described in  [13] . Audio spectrogram is an image of audio signal which consists of 3 main components namely: 1. Time on x-axis. 2. frequency on y-axis. 3. power intensity on the colorbar scale which can be in decibels(dB) as shown in Fig.  1 .  [12]  covers machine learning methods to extract temporal features from the audio signals. The goodness in machine learning models is, it's training & prediction latency is good but, prediction accuracy is low. The CNN model that uses audio spectrograms to detect emotion has better prediction accuracy compared to machine learning model.\n\nComparing the CNN network used in  [13]  &  [14]  for training using audio spectrograms,  [13]  uses wider kernel window size with zero padding while  [14]  uses smaller window size and no zero padding. With wider kernel window size we are able to see larger vision of the input which allows for more expressive power. In order to avoid loosing features use of zero padding becomes important. The zero padding decreases as the number of CNN layers increase in the architeture used in  [13] .  [14]  avoids adding zero padding in order to not consume extra virtual zeroenergy coefficients which are not useful in extracting local features. One drawback that we see in  [14]  is that it does not compare performance between audio model & audio+video model being used. One goodness observed in  [14]  is that it does not do noise removal from the audio input data while  [13]  uses noise removal techniques in the audio spectrogram before training the model.\n\nTo achieve better prediction accuracy, a natural progression of emotion recognition using audio sprectrogram is to include facial features extracted from the video frames.  [11]  &  [6]  implements facial emotion recognition using images and video frames respectively but, without audio.  [14]  &  [8]  implements neural network architecture which processes audio spectrogram & video frames to recognize emotion. Both  [14]  and  [8]  implement a self-supervised model for cooperative learning of audio & video models on different dataset.  [8]  further does a supervised learning on the pre-trained model to do classification. The model come up by  [14]  and  [8]  are very similar; both are two-stream models that contains one part for audio data, and one part for video data. The only difference is the way the kernel size, layer number, input data dimension are set. These hyperparamters are set differently because their input data is different.  [14]  tends to use smaller input size, and kernel size because its input images only capture mouth, which doesn't contain as much information as the image which captures the movement of a person used in  [8] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset & Features",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "The dataset we use is IEMOCAP  [2]  corpora as it is the best known comprehensibly labeled public corpus of emotion speech by actors.  [10]  uses this IEMOCAP dataset to generate state of the art results at the time. IEMOCAP contains 12 hours audio and visual data of conversations of two persons (1 female and 1 male for one conversation, and there are 5 females and 5 males in total), where each sentence in conversations is labelled with one emotion-anger, happiness, excitement, sadness, frustration, fear, surprise, other or neutral state.\n\n3.2. Data pre-processing 3.2.1 Audio Data Pre-processing IEMOCAP data corpus contain audio wav files with various time length and with marking of actual emotion label for corresponding time segment. The audio wav files in IEMO-CAP are generated at a sample rate of 22KHz. The audio spectrogram is extracted from the wav file using librosa  1 python package with a sample rate of 44KHz. 44KHz sample rate was used because as per Nyquist-Shannon sampling theorem 2  in order to fully recover a signal the sampling frequency should at least be twice the signal frequency. The audio signal frequency ranges from 20Hz to 20KHz. Hence, 44KHz sampling rate is commonly used rate for sampling. The spectrograms were generated in 2 segments which are: 1. Original time length of utterance of sentence or emotion.  is summarized in Table  1 . Model training is done on these data segments separately.\n\nIn order to get rid of the background noise we applied bandpass filter 3 4 between 1Hz to 30KHz. Denoising or noise cleanup of the input audio signal for data augmentation is also followed by  [1] . Sentence utterances that are less than 3 second are padded with noise to maintain uniformity in noise frequency and noise amplitude w.r.t noise in other parts of the signal. Initially zero padding was also experimented with to have 3 second time scale and then noise is added with signal to noise ratio (SNR) of 1 through out the signal time length but, this resulted in distorting the original audio signal. The resulting signal is then denoised. Denoising helps in making the frequency, time scale and amplitude features of the input audio signal to be more visible in the hopes of getting better prediction accuracy per emotion. All the audio spectrograms are generated with same colorbar intensity scale (+/-60dB) to maintain uniformity of the spectrogram across the board among different emotions. This is similar to normalization of data. As seen in Fig.  2  after denoising only the signal that contains actual information remain with high power intensity or signal amplitude. Other regions in the spectrogram remains with low power 3 https://timsainburg.com/noise-reduction-python.html 4 https://github.com/julieeF/CS231N-Project/blob/master/load single wav file.py intensity relative to where there is actual signal of interest. Compared to Fig.  1  where some signal intensity is observed throughout the time scale, which is actually the noise. The spectrogram images generated are of size 200x300 pixels.\n\nTotal count of 3 second audio spectrograms among 4 different emotions is summarized in Table. At first, we started off with audio spectrograms that contains xy axis and colorbar scale but, we removed the scales after learning that including axis & scale could be contributing negatively to prediction accuracy.\n\nTo observe class accuracy improvement, input audio spectrograms were data augmented by cropping and rotation. Each image was cropped by 10 pixels from the top and resized back to 200x300 pixels. This cropping is done to simulate frequency change in the emotion by small amount. Similarly, each image was also rotated by +/-10 degrees. This rotation also simulates frequency change but it also shifts the time scale. Augmenting data that changes time scale is not preferred hence the rotation was done to a very small degree of 10 degrees. With cropping and rotation, total count of data used for training becomes 19200. The model training was done separately with original images and images with data augmentation for comparison.Horizontal flip of images were avoided as this means flipping the timescale and enacts a person speaking in reverse, which will lead to lower model prediction accuracy.\n\nModel training on audio spectrogram that contains full time length, and not 3 second, was done separately. For given 3 second audio spectrogram, it was replaced with the full time length spectrogram, thus maintaining data count for balancing.\n\nVisual analysis of around 100 audio spectrograms were done. It was observed that maximum frequency observed among all these spectrograms is around 8KHz. This means around 60% of the spectrogram image is blue which does not carry any information from emotion perspective. All the input audio spectrograms were cropped from the top by 60% and resized back to 200X300 pixels. An ideal method would be to generate spectrograms specifying fixed frequency scale if the frequency range is known prior.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Video Data Pre-Processing",
      "text": "Since our work also include implementing video model to see room for improvement in prediction accuracy of emotion recognition, we also did video data pre-processing. For the video data, we first clipped each video file into sentences according to how we processed audio files. This ensured that we are querying that part of the video file that corresponds to given audio spectrogram. Then we extracted 20 images per 3 second from each video avi file that corresponds to 3 sec audio spectrogram. The video contains both the actors in the frames hence the frames were cropped accordingly from left or right to only capture the actor whose emotion is being recorded. We then cropped the video frames further to cover the actor's face/head. The final resolution of video frames is 60x100. One limitation with the dataset is that, in the video the actors are not speaking facing the camera, therefore full facial expression corresponding to a given emotion are not visible.\n\nWhile processing the extraction of audio spectrograms and video frames, it was observed that the memory usage on the machine was more than 12GB. This lead to machine crashes. Therefore, to extract data, each audio and video file was processed individually in batch. Python script 5 was launched individually through unix shell script.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methods & Model Architecture",
      "text": "In this section, we will talk about the models we have built for emotion recognition in audios in the 'Audio Models' subsection, and models for emotion recognition in au-dios+videos in 'Audio+Video Models' subsection.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Models",
      "text": "By replicating and expanding upon the network architecture used in  [13]  we formulate three different models. The first model is a CNN model, which consists of three 2D convolutional layers and maxpooling layers followed by two fully connected layers, as shown in Fig.  3 . The second architecture we build is that we add a LSTM layer after the convolutional layers in the CNN model we have built, and we will call this model CNN+LSTM in this work. In the third model, we replace the LSTM layer with a vanilla RNN 5 https://github.com/julieeF/CS231N-Project/blob/master/load single Video.py\n\nwhere N is the number of data in the dataset, x n c is the true class's score of the n-th data point, x j is the j-th class's score of the n-th input data. Minimizing the cross entropy loss will force our model to learn the emotion-related features from the audio spectrogram because when the loss will be minimum only when for a datapoint, the score of the true class is remarkably larger than the score of all other classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio+Video Models",
      "text": "Inspired by the work of  [14] , our audio+video model is a two-stream network that consists of two sub-networks as shown in Fig.  4(a) . The first sub-network is the audio model, which we choose to use the best-performing audio model we have built-CNN & RNN as shown in Fig.  3 . The architecture of the first sub-network is the same as the audio model except that it dumps the original output layer in order to get high-level features of audio spectrograms, as shown in Fig.  4 (CNN+RNN). The second sub-network is the video model, and it is made of four 3D convolutional layers, and three 3D maxpooling layers, followed by two fully connected layers, as shown in Fig.  4 (3D CNN). Finally, the last layer of the two sub-networks are concatenated together, followed by one output layer, as shown in Fig.  4(a) .\n\nWe train this audio+video model using two different methods-semi-supervised training and supervised training. For semi-supervised training method, we first pre-train our model using video frames and audio spectrogram from the same video and from different videos, as shown in Fig.  4(b) . This forces the model to learn the correlation between the visual and auditive elements of a video. The input of the pre-training process has three distinct typespositive (the audio spectrogram and video frames are from the same video); hard negative (the audio spectrogram and video frames are from different videos with different emotions); super hard negative (the audio spectrogram and video frames are from different videos with the same emotion). The loss function we use for pre-training is the con-trastive loss.\n\nis the number of datapoints in the dataset, v n , a n are the video frames and audio spectrogram of the n-the datapoint, f v , f a are the video and audio sub-networks, y n is one if the video frames and audio spectrogram are from the same video, and zero otherwise.η is the margin hyperparameter. f v (v n ) -f a (a n ) 2 should be small when the video frames and audio spectrogram are from the same video, and large when they come from different videos. Therefore, by minimizing the contrastive loss, the audio and video models are forced to output similar values when their inputs are from the same video, and very disctint values when they are not. This allows the model to learn the connection between audio and visual elements from the same video.\n\nAfter pre-training is done, we do supervised learning on the pre-trained model where the input is the audio spectrogram and video frames of a video and output is the emotion predicted, as shown in Fig.  4(a) . The loss of our model is the cross entropy, and the formula is the same as in Equation.1.\n\nThe second training method is that we do supervised training directly on the model without pre-training process.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments & Results",
      "text": "For model evaluation, prediction accuracy is the key metric used. For results comparison, the accuracy was compared with accuracy reported in  [13] . Since we balanced the data count therefore the overall accuracy and class accuracy as reported in  [13]  are mathematically equal terms in our work. Our work aimed to achieve prediction accuracy of around 60% considering 4 emotions.\n\nWe trained the model on all 4 segmentation of dataset generated and observed that results on data with original time scale and without noise cleanup gives the best accuracy. The results reported are based on this dataset. Spectrograms with noise removed theoretically sounds promising but it did not work due to 2 possible reasons. First, the algorithm used to remove noise reduces signal amplitude which may lead to some feature suppression. An algorithm that amplifies the signal back need to be explored. Some techniques for e.g. subtracting noise from the signal and multiplying final signal with a constant was explored but they all resulted in signal distortion. Secondly, having noise in the spectrogram simulates real scenario and during model training noise could indirectly acts as a regularizer.  [14]  also does not remove noise from the input audio spectrograms.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hyperparameters",
      "text": "We started off with prediction on 4 emotions and most of the work, results and analysis is based off of these 4 emotions. Our validation accuracy did not go beyond 54.00% and we saw overfitting during model training beyond this point. This lead us to experiment with various hyperparameters in the optimizer and in the network model layers for e.g. kernel size, size of input and output in each layer, dropout, batchnorm, data augmentation, l1 & l2 regularization.\n\nAdam optimizer was used with learning rate of 1e-4 to train the model as this gave best accuracy. We experimented with 1e-3 & 1e-5 and observed the model did not train well with these settings. It was observed weight decay(parameter that controls l2 regularization) of 0.01 in Adam optimizer improved the accuracy by 1%. Weight decay values of 0.005 and 0.02 were also experimented with but, did not help. All other parameters was kept default in the optimizer.\n\nEnabling l1 regularization, data augmentation of rotation and cropping, batchnorm resulted in no change or improvement in accuracy. This is possibly due to that the model learned all the features it can from the available data based on the model architecture.\n\nTuning of dropout probability was also experimented with and optimal value of 0.2 for the last fully connected layer and 0.1 for the dropout in RNN layer was obtained.\n\nThe input & output dimensions in the audio network layers was doubled & quadrupled which resulted in accuracy improvement of 1-2%. Increasing the input output dimensions in the layers also resulted in high memory usage during model training. We attempted to extend this learning on the video network but due to limited memory of 12GB on the machine we were unable to carry out this experiment. This leads us to strongly believe that there is room for improvement that needs more experimentation on a machine with large memory. The accuracy improvement is also evident from different model architectures we used starting from CNN, CNN+RNN and CNN+RNN+3DCNN which actually is having more parameters in the model to learn features better.\n\n80% of total data points were used for training and rest for validation. Batch of 64 data points per iteration is used to train the model. Higher batch count resulted in long iteration time and high memory usage thus, 64 was picked.\n\nUsing normalization in image transformation with mean of [0.485, 0.456, 0.406] and standard deviation of [0.229, 0.224, 0.225] on all images improved accuracy by 0.37%.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Validation Set Accuracy",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Loss & Classification Accuracy History On Cnn+Rnn+3Dcnn",
      "text": "Fig.  5  is the contrastive loss curve obtained during self supervision model training. We ran self supervision model with 5 epochs and 10 epochs separately and fed the learned weights in these 2 experiments into CNN+RNN+3DCNN model for classification training. We observed the self supervised model run with 5 epochs gave better classification accuracy by 0.5% compared to the self supervised model run with 10 epochs. This could be attributed to overfitting of weights learned in self supervised model when run with 10 epochs. Fig.  6  is the softmax/cross entropy loss curve obtained on the best model which is CNN+RNN+3DCNN. Since the loss is reported per iteration hence it appears noisy but we observed that per epoch it is decreasing on logarithmic scale. Fig.  7  is the classification accuracy history on best model which is CNN+RNN+3DCNN. We obtained best validation accuracy of 71.75% considering 3 emotions(sad, anger, neutral).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Confusion Matrix On Cnn+Rnn & Cnn+Rnn+3Dcnn",
      "text": "Fig.  8  is the confusion matrix obtained with CNN+RNN. From this confusion matrix we see only happy emotion is predicted poorly compared to other emotions. This led us to explore CNN+RNN & CNN+RNN+3DCNN architecture only on 3 emotions (instead of 4) to understand if we do see performance improvement when switching from audio only inputs to audio+video inputs. Fig.  9  is the confusion matrix obtained with the best model which is CNN+RNN+3DCNN.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results Analysis",
      "text": "From Table  3 , considering 4 emotions, we can see that CNN+RNN is the best performing architecture and data  augmentation doesn't improve the accuracy. CNN does not work well comparing with CNN+RNN because CNN has the same architecture as the first few layers of CNN+RNN, and is comparably simple. Therefore, CNN+RNN will learn features of higher-level and performs better compared with CNN. For CNN+LSTM, it does have a more complex architecture, however, when we were tuning the hyperparams, we found out that accuracy improved slightly by increasing dropout probability in CNN+LSTM , indicating that CNN+LSTM could be overly complex for our dataset and training purpose. Also, adding model complexity requires more careful hyperparameters tuning, and since CNN+RNN is giving a relatively good performance compared with  [13] , we decided not to bother with adjusting CNN+LSTM.\n\nFrom Table  3 , it also evident that CNN+RNN+3DCNN architecture which uses video frames along with audio spectrogram is the best considering 3 emotions but, the accuracy did not improve significantly to CNN+RNN. This is due to the fact that the cropping window to focus on the face/head to recognize facial emotion was large as the actors are not facing the camera and they moved during their speech. Auto detecting face/head with detection model and then cropping based on the bounding box would be ideal and accuracy is expected to increase significantly. Considering 4 emotions, CNN+RNN+3DCNN performed worse compared to CNN+RNN is because the model prediction accuracy for happy emotion itself is bad due its low data count, therefore when the video frames are used which are only using facial expression from the side only confuses the model to learn poorly. Data augmentation does not increase the validation accuracy and even slightly makes the model perform worse could be due to that the image generated by cropping and rotation loses some emotion-related features, since it alters the frequency and time scale. Which is similar to altering the pitch of the audio and reversing the audio of a sentence, and could confuse the model.\n\nFrom confusion matrix, we observed that happiness prediction is low compared to other emotions. One possible reason for this is, happiness data set count is very low compared to other emotions, and over-sampling by repetition the happiness data set is not enough. More dataset of happiness is expected to improve happiness prediction accuracy.\n\nComparing our results with  [13] , we lag their class accuracy by 5.4% but, comparing the overall accuracy considering 3 emotions our work achieved accuracy of 71.75% which is better by 2.95%.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion/Future Work",
      "text": "Our work demonstrated emotion recognition using audio spectrograms through various deep neural networks like CNN, CNN+RNN & CNN+LSTM on IEMOCAP  [2]  dataset.\n\nWe then explored combining audio with video to achieve better performance accuracy through CNN+RNN+3DCNN. We demostrated that CNN+RNN+3DCNN performs better as it learns emotion features from the audio signal(CNN+RNN) and also learns emotion features from facial expression in video frames(3DCNN) thus complementing each other.\n\nTo further improve the accuracy of our model we plan to explore more and touch on various aspects. We want to explore more noise removal algorithms and generate audio spectrograms without noise in them. This work will help in analyzing if removing noise actually helps or it acts as a regularizer and we don't need to remove noise from the spectrograms. We also want to explore, if there are multiple people speaking how the model predicts the emotion in such scenarios. Next, we want to explore auto cropping around the face/head from video frames. We strongly believe it will significantly improve the prediction accuracy. As far as data augmentation is concerned, even though none of direct data augmentation methods proved to be useful but, adding signal with very low amplitude and varying frequency onto the speech signal and then generating audio spectrogram from the resulting signal will create unique data points and help in getting rid of model overfitting. If there were machines/GPUs with more memory we wanted to experiment with increasing input and output dimensions in each layer in the network to obtain optimal point. There is definitely a room to get better accuracy using this method. We then want to experiment with prediction latency among different models and there architecture size. We also wanted to experiment more with CNN+LSTM network and fine tune it to see what is the best accuracy we can achieve with this model. We did try transfer learning using ResNet18 but didn't achieve good results. Need to do more experimentation on how to transfer learn using existing models. Lastly, try the model on all 12 emotions in the dataset and understand bottlenecks and come up with neural network solutions that can predict with high accuracy. We would like to thank the CS231N Teaching Staff for guiding us through the project. We also want to thank Google Cloud Platform and Google Colaboratory for providing us free resources to carry out experimentation involved in this work.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Link To Github Code",
      "text": "",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Example of audio spectrogram of anger emotion. Origi-",
      "page": 2
    },
    {
      "caption": "Figure 1: [12] cov-",
      "page": 2
    },
    {
      "caption": "Figure 2: Example of audio spectrogram of anger emotion. 3 sec",
      "page": 3
    },
    {
      "caption": "Figure 2: after denoising only the signal that contains actual informa-",
      "page": 3
    },
    {
      "caption": "Figure 1: where some signal intensity is observed",
      "page": 3
    },
    {
      "caption": "Figure 3: Audio model architectures.",
      "page": 4
    },
    {
      "caption": "Figure 4: Audio+Video model architectures.",
      "page": 5
    },
    {
      "caption": "Figure 5: Contrastive loss history curve on CNN+RNN+3DCNN.",
      "page": 6
    },
    {
      "caption": "Figure 5: is the contrastive loss curve obtained during self",
      "page": 6
    },
    {
      "caption": "Figure 6: is the softmax/cross entropy loss curve obtained",
      "page": 6
    },
    {
      "caption": "Figure 7: is the classiﬁcation accuracy history on best model",
      "page": 6
    },
    {
      "caption": "Figure 6: Loss history curve on CNN+RNN+3DCNN. The curve",
      "page": 7
    },
    {
      "caption": "Figure 7: Classiﬁcation accuracy history on CNN+RNN+3DCNN",
      "page": 7
    },
    {
      "caption": "Figure 8: is the confusion matrix obtained with CNN+RNN.",
      "page": 7
    },
    {
      "caption": "Figure 8: Confusion matrix of true class vs.",
      "page": 7
    },
    {
      "caption": "Figure 9: Confusion matrix of true class vs.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DatasetsegmentationType": "Originaltimelengthofutterance\nClipeachutteranceinto3secondclips\nOriginaltimelengthofutterance\nClipeachutteranceinto3secondclips",
          "NoiseCleanup": "No\nNo\nYes\nYes",
          "Name": "DSI\nDSII\nDSIII\nDSIV"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotion": "Happy\nSad\nAnger\nNeutral",
          "Countofdatapoints": "786\n1752\n1458\n2118"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Validation set accuracy over CNN, CNN+LSTM,",
      "data": [
        {
          "Architecture": "CNN\nCNN\nCNN+LSTM\nCNN+LSTM\nCNN+RNN\nCNN+RNN\nCNN+RNN+3DCNN\nCNN+RNN+3DCNN",
          "Accuracy(%)": "52.23\n51.90\n39.77\n39.65\n54.00\n70.25\n51.94\n71.75",
          "DataAug.": "No\nYes\nNo\nYes\nNo\nNo\nNo\nNo",
          "Emotion": "H,S,A,N\nH,S,A,N\nH,S,A,N\nH,S,A,N\nH,S,A,N\nS,A,N\nH,S,A,N\nS,A,N"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep speech 2: Endto-end speech recognition in english and mandarin",
      "authors": [
        "D Amodei",
        "R Anubhai",
        "E Battenberg",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "J Chen",
        "M Chrzanowski",
        "A Coates",
        "G Diamos",
        "E Elsen",
        "J Engel",
        "L Fan",
        "C Fougner",
        "T Han",
        "A Hannun",
        "B Jun",
        "P Legresley",
        "L Lin",
        "S Narang",
        "A Ng",
        "S Ozair",
        "R Prenger",
        "J Raiman",
        "S Satheesh",
        "D Seetapun",
        "S Sengupta",
        "Y Wang",
        "Z Wang",
        "C Wang",
        "B Xiao",
        "D Yogatama",
        "J Zhan",
        "Z Zhu"
      ],
      "year": "2015",
      "venue": "Deep speech 2: Endto-end speech recognition in english and mandarin"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database"
    },
    {
      "citation_id": "3",
      "title": "A tutorial survey of architectures, algorithms, and applications for deep learning",
      "authors": [
        "L Deng"
      ],
      "year": "2014",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition: facial components associated with various emotions",
      "authors": [
        "K Gouta",
        "M Miyamoto"
      ],
      "year": "2000",
      "venue": "Shinrigaku kenkyu: The Japanese journal of psychology"
    },
    {
      "citation_id": "5",
      "title": "Deep speech: Scaling up end-to-end speech recognition",
      "authors": [
        "A Hannun",
        "C Case",
        "J Casper",
        "B Catanzaro",
        "G Diamos",
        "E Elsen",
        "R Prenger",
        "S Satheesh",
        "S Sengupta",
        "A Coates",
        "A Ng"
      ],
      "year": "2014",
      "venue": "Deep speech: Scaling up end-to-end speech recognition"
    },
    {
      "citation_id": "6",
      "title": "Facial emotion recognition from videos using deep convolutional neural networks",
      "authors": [
        "W Hashim Abdulsalam",
        "M Salam"
      ],
      "venue": "Facial emotion recognition from videos using deep convolutional neural networks"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "J Kim",
        "E André"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Co-training of audio and video representations from self-supervised temporal synchronization",
      "authors": [
        "B Korbar",
        "D Tran",
        "L Torresani"
      ],
      "year": "2018",
      "venue": "Co-training of audio and video representations from self-supervised temporal synchronization"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition by speech signals",
      "authors": [
        "O.-W Kwon",
        "K Chan",
        "J Hao",
        "T.-W Lee"
      ],
      "year": "2003",
      "venue": "Eighth European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "10",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "11",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "A Abdolrashidi"
      ],
      "year": "2019",
      "venue": "Deep-emotion: Facial expression recognition using attentional convolutional network"
    },
    {
      "citation_id": "12",
      "title": "Multimodal speech emotion recognition and ambiguity resolution",
      "authors": [
        "G Sahu"
      ],
      "year": "2019",
      "venue": "Multimodal speech emotion recognition and ambiguity resolution"
    },
    {
      "citation_id": "13",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Efficient emotion recognition from speech using deep learning on spectrograms"
    },
    {
      "citation_id": "14",
      "title": "Dawson. Coupled 3d convolutional neural networks for audiovisual recognition",
      "authors": [
        "A Torfi",
        "S Iranmanesh",
        "N Nasrabadi"
      ],
      "year": "2017",
      "venue": "Dawson. Coupled 3d convolutional neural networks for audiovisual recognition"
    }
  ]
}