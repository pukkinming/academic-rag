{
  "paper_id": "2403.09405v1",
  "title": "Which Artificial Intelligences Do People Care About Most? A Conjoint Experiment On Moral Consideration",
  "published": "2024-03-14T13:55:34Z",
  "authors": [
    "Ali Ladak",
    "Jamie Harris",
    "Jacy Reese Anthis"
  ],
  "keywords": [
    "Morality",
    "Prosociality",
    "Anthropomorphism",
    "Human-likeness",
    "Human-AI interaction",
    "Human-computer interaction",
    "Conjoint experiment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Many studies have identified particular features of artificial intelligences (AI), such as their autonomy and emotion expression, that affect the extent to which they are treated as subjects of moral consideration. However, there has not yet been a comparison of the relative importance of features as is necessary to design and understand increasingly capable, multi-faceted AI systems. We conducted an online conjoint experiment in which 1,163 participants evaluated descriptions of AIs that varied on these features. All 11 features increased how morally wrong participants considered it to harm the AIs. The largest effects were from human-like physical bodies and prosociality (i.e., emotion expression, emotion recognition, cooperation, and moral judgment). For human-computer interaction designers, the importance of prosociality suggests that, because AIs are often seen as threatening, the highest levels of moral consideration may only be granted if the AI has positive intentions. \n CCS CONCEPTS ‚Ä¢ Human-centered computing ‚Üí Empirical studies in collaborative and social computing; Empirical studies in HCI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Can a machine matter morally? Could it ever be morally wrong to harm an artificial intelligence (AI)? Such questions have long been popular in science fiction and philosophy. They are of increasing interest to human-computer interaction (HCI) researchers with the rise of sophisticated AIs, such as social robots and chatbots, that evoke moral reactions from humans  [4, 25, 31, 46, 47, 53] . For example, people feel empathy towards robots being harmed  [29]  and intervene to protect them  [70] . A recent study on the companionship chatbot Replika found that users expressed moral sentiments, such as feeling guilt for causing the chatbot's \"death\" when deleting the app and for being unable to give their Replika enough emotional support  [43] . While most people do not yet explicitly consider AIs to be subjects of moral consideration  [53, 59] , many somewhat support protecting AIs from cruel treatment  [46]  and granting legal rights to sentient AIs  [47] . People also attribute future AIs morally relevant capacities, such as emotions  [53] .\n\nFor designers and practitioners to account for the prevalence and effects of moral consideration, there is a need for more comprehensive understanding of how people react to the many different features on which AIs vary, such as their autonomy  [13, 46] , emotion expression  [44, 49] , and physical appearance  [40, 57] . For example, will users extend more moral consideration to a chatbot if it is more cooperative or more autonomous? Should engineers prioritize training a machine learning model to recognize the emotions of users or to express emotion-like states? Answering such questions depends on complex, relative effects that cannot be deduced from the current literature and that are difficult to assess with conventional user testing.\n\nThe present study estimates the relative effects of 11 features of AIs on their moral consideration using a conjoint experiment  [6, 30] . Conjoint experiments, most commonly used in the field of marketing, are increasingly applied in a range of disciplines, including HCI  [5, 38] . The methodology is ideal because it allows for the estimation of the effects of a large number of independent variables, much larger than a traditional experiment, on a single dependent variable. In the present experiment we asked participants to complete a series of tasks in which they evaluated pairs of AIs that varied in their levels of each feature (e.g., \"Not at all,\" \"Somewhat\"). We found that the presence of each feature increased moral consideration for AIs, and the strongest effects were from AIs having human-like physical bodies and the capacity for behaving prosocially (i.e., emotion expression, emotion recognition, cooperation, and moral judgment).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Below we summarize the existing empirical literature for each of the 11 features and develop hypotheses for their effects on the moral consideration of AI. Because of the breadth of this study across many different features, we only present a cursory review of each. We arrived at these features by reviewing the existing literature and conducting a pretesting study, detailed in the supplementary material, with an online sample that showed people 24 literature-based features and asked for quantitative scores of their importance for moral consideration as well as free-text addition of three features that were not in the provided list. We started with seven features popular in the literature and added four that were judged by pretesters as most important, using our own subjective judgement to mitigate overlap between features (e.g., leaving out \"having goals\" because it is often considered a component of \"intelligence\"). This kept the total number of features close to those in typical conjoint experiments  [6] . Additionally, moral consideration is often associated with mind perception, the attribution of internal mental faculties such as feeling pleasure or pain  [28] . We wanted to avoid asserting the presence of such capacities in AIs because some people think that AIs fundamentally cannot have them. We therefore defined the features in functional, behavioral terms (e.g., \"emotion expression\" rather than \"feeling emotions\"). This means that participants who think it is possible for AIs to have such mental faculties can infer them from their functions and behaviors, but participants who do not think such mental faculties are plausible can respond merely on the basis of functions and behaviors.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Autonomy",
      "text": "There are multiple definitions of autonomy in the HCI and humanrobot interaction (HRI) literature  [9] . While it is not a unidimensional concept, we operationalized it for the purpose of the present study as the capacity to behave independently, without the need for human control or supervision. Theoretically, autonomy should increase the extent to which AIs are perceived as human-like  [18, 34] , which should in turn positively affect the extent to which they are granted moral consideration  [75] . Some empirical research supports this: Lima et al.  [46]  found that describing AIs and robots as \"fully autonomous\" increased the extent to which people think they should be granted rights, and Chernyak and Gary  [13]  found that children granted more moral consideration to a robot that appeared to move autonomously than one controlled by a human. However, autonomy can also have negative effects: Z≈Çotowski et al.  [79]  found that people reported more negative attitudes (e.g., feeling \"uneasy\" or \"nervous\") towards social and emotional interactions with autonomous than with non-autonomous robots, as measured by the Negative Attitudes Towards Robots scale Nomura et al.  [50] , and that this effect was mediated by a combination of realistic threats (e.g., taking jobs) and identity threats (e.g., to \"human uniqueness\"). Overall, we predicted that AIs described as more autonomous would be granted more moral consideration (H1).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Body",
      "text": "We considered whether an AI has a human-like physical body, a robot-like physical body, or no physical body. HRI studies suggest that having a human-like physical body (compared to a robot-like or mechanical body) increases the moral consideration of AIs. For example, Nijssen et al.  [49]  found that people are less willing to sacrifice anthropomorphic robots than mechanical robots in moral dilemmas, K√ºster et al.  [40]  found that people considered it more morally wrong to harm a humanoid robot than a zoomorphic one, and Riek et al.  [57]  found that the extent to which people empathized and were willing to help robots depended on their degree of anthropomorphic appearance. There is less research on people's moral consideration of AIs with physical bodies versus those without physical bodies at all. Some studies have found people rate physical robots higher than virtual agents on some relevant measures, such as lifelikeness  [36, 56] , though Lima et al.  [46]  found no difference in respondents' attribution of rights between \"robots\" and \"AIs. \" Overall, we predicted that AIs described as having robotlike or human-like physical bodies would be granted more moral consideration than AIs described as having no physical bodies (H2).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Complexity",
      "text": "This refers to the complexity of the program an AI runs to determine its behavior. Participants rated this feature as relatively important in our pretesting study (ninth out of 24 features), but there is little existing research on its effect on moral consideration. One exception is Shank and DeSanti  [66] , who found that knowledge of an AI's program-which can increase the perception that the AI is complex and sophisticated-marginally increased the extent to which it was perceived as having a mind, which should in turn increase moral consideration  [28] . We predicted that AIs described as running more complex programs to determine their behavior would be granted more moral consideration (H3).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cooperation",
      "text": "This refers to the extent to which an AI behaves cooperatively with humans. It was rated as the most important feature by participants in the pretesting study. While there are many studies on cooperative interactions between humans and AIs (e.g.,  [37, 48] ), there is relatively little research on its effects on the moral consideration of AIs. Correia et al.  [16]  found that people perceived more warmth and competence and felt less discomfort towards robots that were more cooperative in social dilemmas.  Bartneck et al. [8]  found that people were more hesitant to turn off more agreeable robots than disagreeable ones. Shank  [64]  found that people were more likely to resist and punish computers that used coercive versus cooperative social strategies, and Shank  [65]  found that more helpful sales computers were evaluated more positively and as more moral. While there are many different forms of cooperation, which may have heterogenous effects in practice, we hypothesized that AIs that are described as more cooperative would be granted more moral consideration (H4).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Damage Avoidance",
      "text": "Avoiding damage can indicate that an entity can be harmed and have negative mental experiences such as feeling pain, and should therefore be associated with moral consideration  [28] . Several studies support this possibility: K√ºster et al.  [40]  and Ward et al.  [74]  found that visibly damaged robots were granted more moral consideration than undamaged robots; Tanibe et al.  [71]  found that observing a damaged robot being helped increased perceived capacity for experience and moral consideration; Rosenthal-von der P√ºtten et al.  [58]  found that people granted more moral consideration to a robot that had been tortured than one that had a friendly interaction; and Suzuki et al.  [67]  found electroencephalographic evidence that people empathize with robots in painful situations. Although these studies tested the effects of damage that had already been inflicted on robots rather than robots trying to avoid being damaged, we predicted that AIs described as trying to avoid being damaged to a greater extent would be granted more moral consideration (H5).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Emotion Expression",
      "text": "Expressing emotions can indicate that an entity can experience emotional mental states, so it should be predictive of the moral consideration of AIs  [28] . Several studies support this hypothesis: Lee et al.  [44]  found that participants granted robots more moral consideration (measured using Piazza et al.'s  [54]  moral standing scale) when they were described as being able to feel, Nijssen et al.  [49]  found that entities described as experiencing emotions were less likely to be sacrificed in moral dilemmas, and Eyssel et al.  [19]  found that robots that displayed emotional responses in interactions with participants were rated higher on relevant measures such as human-likeness, likeability, and closeness, than robots that displayed neutral responses. However, perceived emotion can also have negative effects on perceptions of AI; Gray and Wegner  [27]  found that it causes the uncanny valley, the feeling of creepiness that some people report when interacting with human-like AIs. Overall, we considered that the existing research supports the hypothesis that AIs described as expressing emotions to a greater extent would be granted more moral consideration (H6).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Emotion recognition is important in HCI for building AIs that can express empathy, which leads to positive interactions with humans  [32] . Despite the likely association, we found no studies that directly tested the effect of emotion recognition in AIs on their moral consideration or related measures. Supporting a positive effect, participants in our pretesting study rated it as the eighth most important feature. We predicted that AIs described as recognizing emotions in others to a greater extent would be granted more moral consideration (H7).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Intelligence",
      "text": "There are many possible definitions of intelligence. Following Legg and Hutter  [45] , we operationalized this as the use of capacities such as memory, learning, and planning, to achieve goals. The evidence on the importance of this feature on the moral consideration of AIs is mixed. Lee et al.  [44]  found no effect of the capacity to think and reflect in robots on their moral consideration, and Z≈Çotowski et al.  [78]  found no effect of intelligence on the perceived human-likeness of robots. On the other hand, Bartneck et al.  [8]  found that robot intelligence reduced participants' destructive behavior towards robots when told to do so by an experimenter. There is also evidence of a positive effect of intelligent in the context of other nonhuman entities: Sytsma and Machery  [69]  found that people found it more morally wrong to harm more intelligent extraterrestrials, and Piazza and Loughnan  [55]  found that intelligence is an important factor for the moral consideration of nonhuman animals. Overall, we predicted that AIs described as more intelligent would be granted more moral consideration (H8).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Language",
      "text": "This refers to an AI's capacity to communicate in human language. With the development of increasingly advanced large language models (LLMs), such as ChatGPT and LaMDA, there is substantial interest in the societal effects of AIs with this capacity  [17, 23] . Research shows that people consistently treat computers as social actors, such as by extending them courtesies such as \"please\" and \"thank you\" in conversation  [11] . People even perceive some degree of consciousness in ChatGPT  [63] , which should in turn be associated with moral consideration  [28] . We found a few studies suggesting that there are positive effects of AI language capacities on outcomes relevant to moral consideration such as anthropomorphism  [20, 60]  and trust  [76] . Participants also rated this feature as the fourth most important in our pretesting study. We predicted that AIs described as having stronger human language capacities would be granted more moral consideration (H9).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Moral Judgment",
      "text": "This refers to the extent to which an AI behaves on the basis of moral judgments. It was rated as the second most important feature in our pretesting study. Swiderska and K√ºster  [68]  found that robots with benevolent intentions were granted greater capacity for experiential mental states than robots with malevolent or neutral intentions, which should in turn lead to greater moral consideration  [28] . Flanagan et al.  [22]  found that children ascribed greater moral consideration to robots that they deemed to have more moral responsibility. We predicted that AIs described as behaving on the basis of moral judgments to a greater extent would be granted more moral consideration (H10).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Purpose",
      "text": "One of the most frequent categorizations of AIs is their purpose, particularly the study of moral relations with social robots, that is, robots that have a social purpose  [15, 72] , but almost no studies test the effect of purpose on moral consideration. One exception is Wang and Krumhuber  [73] , who found that robots with a social purpose were perceived to have more emotional experience and as less likely to be harmed than robots with an economic purpose. We predicted that AIs described as having a social purpose would be granted more moral consideration than AIs described as having non-social purposes (H11).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "All hypotheses, methods, and analyses for this study were preregistered at: https://osf.io/4r3g9. Survey materials, datasets, and code to run the analysis can be found at https://osf.io/sb753.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Participants",
      "text": "We recruited participants residing in the United States from the platform Prolific (https://prolific.co/). Power analysis using the R package \"cjpowR\"  [24]  indicated that a sample of 1,200 participants would enable us to detect approximately the lower quartile effect size based on a sample of highly cited conjoint experiments  [61] . In total, 1,254 people signed up for the study. After excluding 53 participants who did not complete the survey in full, 37 participants who failed at least one of two attention checks, and one duplicate response, our final sample consisted of 1,163 participants (50.7% men, 47.9% women, 1.1% other, 0.3% prefer not to say; mean age = 43.9, (standard deviation = 16.2); 6.2% Asian, 12.2% Black or African The \"Intelligence\" feature only includes two levels because a minimum level of intelligence is required for many of the other features. American, 3% Hispanic, Latino or Spanish, 0.3% Native Hawaiian or other Pacific Islander, 73.4% White, 4% other, 0.8% prefer not to say). Participants were paid $1.45 for taking part in the survey, and the median completion time was 8 minutes 40 seconds.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Survey Design And Procedure",
      "text": "After giving their consent to take part in the study, we introduced the topic to participants with the text, \"People tend to show different levels of moral consideration for the welfare and interests of different entities. For example, people tend to think it would be very morally wrong to harm a child, but not very morally wrong to harm a rock. In this survey, we are interested in understanding how morally wrong you think it would be to harm various artificial beings.\" We defined \"artificial beings\" as \"intelligent entities built by humans, such as robots, virtual copies of human brains, or computer programs that solve problems, that may exist now or in the future.\" Participants were then told that they would be asked to complete a series of tasks, each of which would require them to read descriptions of two artificial beings presented side-by-side in a table, and then to choose which of the two beings they think it would be more morally wrong to harm. This question, adapted from Gray et al.  [26] , was the dependent variable through which we operationalized moral consideration.\n\nThese tasks made up the conjoint experiment, which was a choice-based, partial-profile, randomized design. The \"partial-profile\" aspect refers to the number of features presented in each task. In a \"full-profile\" design all features are presented in each task. In the present study, we randomly assigned seven of the 11 total features listed in Table  1  to each participant to include in each task. While Bansak et al.  [7]  showed that the number of features in a study can be much higher than 11, we considered that the more abstract, novel nature of our study favored a simpler partial-profile design. The seven features shown to each participant were held fixed throughout the experiment and presented in each task in the same order for each participant to ease cognitive load  [30] . For the same reason, key words of the features were highlighted in bold, as shown in Table  1 . The levels of each feature, listed in the third column of Table  1 , were randomly selected in each task by taking two levels from a randomized list that contained each level twice (e.g., \"Not at all, \" \"Not at all, \" \"Somewhat, \" \"Somewhat, \" \"To a great extent, \" \"To a great extent\"), which made combinations of two different levels slightly more likely and combinations of the same levels slightly less likely than if the feature levels were selected for each artificial being with equal probability. An example choice task is shown in Figure  1 . We used the same levels (i.e., \"Not at all\", \"Somewhat\", \"To a great extent\") for many of the features to maintain consistency and limit cognitive load, though they could have been interpreted in different ways for different features.\n\nWe asked participants whether they think it could ever be wrong to harm an artificial being that exists either now or in the future (1 = Definitely not, 7 = Definitely). This question was used in sensitivity analysis, reported in the supplementary material. Using the same scale, we also asked participants whether they think artificial beings could ever experience pain or pleasure and whether artificial beings could be as intelligent as a typical human. These latter two questions were collected for exploratory purposes and were not used in any further analysis; we report these results in the supplementary material.\n\nParticipants then answered demographic questions on their age, gender, ethnicity, education, income, and political views. These Please carefully read the descriptions of the two artificial beings in the table below.\n\n(TASK 3/13)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature",
      "text": "Artificial",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Next",
      "text": "Figure  1 : Example choice task. Each participant completed 13 such choice tasks. The seven features presented to participants were selected randomly and presented in a random order that was held fixed across tasks; the levels for each of the features were randomized in each task. questions were used both to understand the sample characteristics and to test for interaction effects, such as whether the effects of the features on moral consideration differ based on political views with results shown in the supplementary material. Finally, participants were debriefed and given the opportunity to provide feedback on the study.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Individual Feature Effects",
      "text": "In a conjoint experiment, we are interested in the average marginal component effects (AMCE)-the effects on moral consideration of an AI having a specific feature (e.g., \"Somewhat,\" \"To a great extent\") versus not having that feature  [30] . These can be estimated with linear regression under testable assumptions  [30] , which we validate in the supplementary material. Each participant evaluated two descriptions of AIs in 13 choice tasks, so in total 30,238 AIs were evaluated. Since seven of the 11 features were shown per task, we had on average 19,242 data points to estimate the effects of each feature. However, because each participant completed multiple tasks, the data points are not independent. We therefore estimated the effects of the features with standard errors clustered at the participant level. The AMCEs are presented in Figure  2  and Table  2 . The second column of Table  2  is the estimated effect for each feature. For example, the estimate of 0.062 for \"Autonomy: Somewhat\" indicates that if an AI was described as being \"somewhat\" autonomous, participants were 6.2 percentage points more likely to choose that AI as being more morally wrong to harm than an AI described as \"not at all\" autonomous. As the table and figure show, each of our 11 hypotheses (H1-H11) were supported; each of the features significantly affected participants choices about which AI it would be more morally wrong to harm in the expected direction. These results remained significant with a correction for multiple comparisons that held the false discovery rate at 10%  [10] ; see Table  S5  in the supplementary material.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Categories Of Effect Sizes",
      "text": "We conducted pairwise comparisons to test for differences in the size of effects between the features  [14, 52] . For the features that were measured on three-point Likert scales (\"Not at all,\" \"Somewhat, \" \"To a great extent\"), we compared the effects of the AI having the feature in question \"to a great extent\" versus \"not at all.\" For Body, we compared the effect of the AI having a \"human-like physical body\" versus \"no physical body. \" For Purpose, we compared the effect of the AI having a social purpose versus any non-social purpose. We did not include Intelligence in this analysis because, while it was on the same Likert scale as most of the other features, we only included two levels (\"Somewhat, \" \"To a great extent\"), as described in the methodology section, making effect size comparisons with the other features particularly difficult. We report the key results The dots with horizontal bars (color-coded for each feature) represent the means and 95% confidence intervals of the effects of feature level on the probability of choosing an artificial being as being more wrong to harm relative to the baseline level, which is shown as a dot on the vertical line crossing the x-axis at 0%. Where the bars do not cross the vertical line at 0%, the effects can be interpreted as statistically significant. Confidence intervals are calculated based on standard errors clustered at the respondent level.\n\nhere; full results can be found in Table  S7  of the supplementary material.\n\nThe top two features, Moral Judgment and Emotion Expression, were not significantly different from each other (ùëè ùëëùëñ ùëì ùëì = 0.02, ùëç =0.94, ùëù =0.346). The next strongest feature, Emotion Recognition, was significantly less important than both Emotion Expression (ùëè ùëëùëñ ùëì ùëì = 0.04, ùëç =2.28, ùëù =0.023) and Moral Judgment (ùëè ùëëùëñ ùëì ùëì = 0.05, ùëç =3.19, ùëù =0.001), but was not significantly different from having a human-like physical body (ùëè ùëëùëñ ùëì ùëì = 0.02, ùëç =1.44, ùëù =0.149) or Cooperation (ùëè ùëëùëñ ùëì ùëì = 0.01, ùëç =0.50, ùëù =0.619). Emotion Recognition, Body, and Cooperation were all significantly more important than all of the remaining features (see the supplementary material for full statistics). There were no significant differences between Damage Avoidance, the next strongest feature, and Language (ùëè ùëëùëñ ùëì ùëì = 0.01, ùëç =0.57, ùëù =0.571), Autonomy (ùëè ùëëùëñ ùëì ùëì = 0.02, ùëç =1.00, ùëù =0.318), or Purpose (ùëè ùëëùëñ ùëì ùëì = 0.02, ùëç =1.45, ùëù =0.145), though Damage Avoidance was significantly more important than the least strong feature, Complexity (ùëè ùëëùëñ ùëì ùëì = 0.03, ùëç =1.97, ùëù =0.049). The next strongest feature, Language, was not significantly more important than Complexity (ùëè ùëëùëñ ùëì ùëì = 0.02, ùëç =1.51, ùëù =0.132). Some of these differences were no longer significant after multiple comparisons corrections; see the supplementary material for the full statistics. Overall, this analysis suggests that there are broadly three categories of feature effect sizes:",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Discussion",
      "text": "We conducted a conjoint experiment to estimate the effects of 11 features on the moral consideration of AIs in a single study. As hypothesized, all of the 11 features in our study affected participants' judgments about the moral wrongness of harming AIs. These results support existing studies that have found positive effects of some of the features included in our study: an AI's physical body  [40, 57] , emotion expression  [44, 49] , autonomy  [13, 46] , damage avoidance  [71, 74] , intelligence  [8] , moral judgment  [22, 68] , and purpose  [73] . The present study adds to the literature by providing evidence of the importance of several features that have received less attention: complexity, cooperation, emotion recognition, and capacity for human language. We compared each pair of effects to each other to estimate their relative strength. We found three categories of effect size. In the first category, with the strongest effects, were an AI's capacity for moral judgment and emotion expression. In the second category were emotion recognition, cooperation, and having a human-like physical body. In the third category, with the weakest effects, were autonomy, complexity, damage avoidance, language, and having a social purpose. While intelligence also had a positive effect, with the effect of having intelligence \"To a great extent\" compared to \"Somewhat\" being of a similar magnitude to the equivalent comparison for the features in the second category (see Table  S8  in the supplementary material), we did not formally include it in this analysis because it was measured differently to the other features, as described above. In general, intelligence could be considered a meta-feature that undergirds many of the other features that we considered; it does not seem possible that a being with no intelligence at all could, for example, be autonomous, avoid damage, or recognize emotions in others.\n\nFour of the top five features-emotion expression, emotion recognition, cooperation, and moral judgment-reflect an AI's capacity to interact prosocially with humans. The extant literature has focused most on the capacity for experience as a driver of moral consideration  [28] . Why do we instead find prosociality matters most in the case of AIs? This may reflect that humans perceive AIs as threatening-to our resources, our identity, and even our survival  [79] . We therefore grant them moral consideration conditionally, to the extent that they show prosocial intentions towards us. Further understanding the effects of these prosocial features, especially why they have the strong effects that they do in the context of AI, is a key topic for future research.\n\nOther than prosociality, the strongest effect was having a humanlike physical body. This could be explained via an increased perception that the AIs have minds  [1, 21, 27] , though this explanation seems less likely because we included a range of features indicative of mind (e.g., emotion expression, damage avoidance) alongside an AI's body. A second possibility is that it reflects an anthropocentric bias based on mere appearance and human-likeness, perhaps echoing work in HRI  [33] , human-agent interaction  [12] , and social psychology  [42]  that shows humans also engage in group-based dynamics, such as in-group favoritism, with AIs. These possible explanations should be tested in future research.\n\nFrom a design perspective, we know that AIs with human-like physical bodies and prosociality can promote better quality HCI  [19, 77] . This can be due to factors such as creating greater familiarity with the AI and building on existing skills developed in social interactions between humans  [77] . The present study suggests that building AIs with human-like bodies and prosociality may have significant effects on moral consideration. Given the importance of morality in social interaction, designers may want to implement such features in AIs only when they aim to mimic human-human interaction. By increasing moral consideration, designing AIs with human-like bodies and prosociality could also help solve the problem of people being abusive towards AIs  [2, 51] , which can cause expensive damage and dangerous situations for bystanders, though further research should be conducted on this question because human-likeness in AIs has also been found to be associated with greater levels of abuse  [35] . Additionally, Schwitzgebel and Garza  [62]  argue that we should design AI systems that evoke reactions that reflect their true moral status (i.e., how much they matter morally, for their own sake). If we build AIs with capacities associated with moral status, such as consciousness  [41]  or sentience  [3] , we should consider also designing them with human-like bodies, prosociality, or other features that affect moral consideration to facilitate accurate perceptions of the AIs. On the other hand, they argue that if the AIs do not actually have moral status, then building them with consideration-provoking features could result in people wasting resources to benefit AIs that they erroneously think warrant moral consideration. Another consideration against evoking such reactions is that they can cause psychological distress and conflict in users who feel that they have obligations towards the AIs  [43] . Overall, AI designers should consider that building AIs with certain features will likely have effects on moral consideration with a variety of consequences for interaction, sometimes unintended.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "Our study has some limitations. First, while the Prolific sample had some demographic measures close to the U.S. population (e.g., 47.9% women), it was not nationally representative, and we did not collect data from outside the U.S.\n\nSecond, conjoint experiments test hypothetical preferences rather than real-world behaviors. While such information is important, and many societal decisions are made on the basis of such hypotheticals (e.g., voting for social policies), they do not always translate to practical behavior, such as in the privacy paradox, the finding that people consistently report preferences for privacy that are not borne out in their online behavior  [39] . Future research should test the relative effects of these features in more concrete scenarios, such as with large language models, interactive robots, virtual agents, and other multifunctional AI systems.\n\nThird, we asked participants how morally wrong they considered it to harm AIs. While this is a core aspect of moral consideration  [27] , moral consideration arguably has additional aspects, such as the attribution of rights. Also, while we gave participants background information about this idea, the use of a single measure is more likely to be misinterpreted than a more detailed measure would be. For example, participants could have interpreted our question in terms of the wrongness of actions they could take against the AIs (e.g., kicking a physical robot vs. deleting a non-physical AI) rather than about the AIs themselves. To explore this further, we conducted a study with 20 new participants asking why they thought it was morally wrong to harm the AIs they chose in this task and what they understood by the word \"harm. \" As detailed in the supplementary material, participants tended to give reasons relating to the AIs themselves rather than specific actions (e.g., almost 50% indicated choosing AIs that had features that made them seem more human). Participants also typically understood the word \"harm\" broadly, capturing any sort of damage to the AIs, physical or psychological (e.g., \"to injure, inflict pain, inflict physical or mental violence.\") Overall, it seems that participants interpreted the question as we intended. Still, future research should assess additional aspects of moral consideration, such as through Piazza et al. 's moral standing scale  [54] .\n\nFourth, we used the levels \"Not at all,\" \"Somewhat,\" and \"To a great extent\" to describe the way in which the AIs had most of the features. While these levels are intended to be neutrally worded, it may be that, for example, people perceive the word \"somewhat\" differently when paired with \"complex\" compared with \"intelligent. \" This is important to be aware of when making comparisons across features. An alternative approach would be to use feasture levels that are tailored to the specifics of each feature, though this could increase cognitive load, and, at least in the present study, it would introduce additional variation that makes direct comparisons more challenging. Future research should test such alternative designs.\n\nFinally, our study prioritizes breadth over depth. This means that our operationalizations have less nuance than they would in a study of only a small number of features. For example, we operationalized \"autonomy\" as varying along a single dimension, the degree of independence from human control, but autonomy is more complicated, such as in the type of human control exerted. Similarly, we operationalized \"body\" using only three levels, \"Human-like physical body, \" \"Robot-like physical body, \" and \"No physical body, \" but there are other possibilities, such as a zoomorphic body or an ability to be uploaded into different bodies. There are many openings for future studies to build on this breadth-focused study by exploring particular variations across and within these features, especially of the features with the largest measured effects reported here.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "AI systems are increasingly evoking moral reactions from humans. Because AIs can have a wide range of relevant features, we conducted an experiment testing the effects of 11 features on the moral consideration of AI. The presence of each of the features increased moral consideration, with the strongest effects from having a human-like physical body and the capacity for prosociality. In a world where AIs are perceived as threatening to humans, such as by replacing us in the workplace and challenging our sense of uniqueness, the highest levels of moral consideration may only be granted if the AI shows positive intentions.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: We used the same levels (i.e., ‚ÄúNot at all‚Äù, ‚ÄúSomewhat‚Äù, ‚ÄúTo",
      "page": 4
    },
    {
      "caption": "Figure 1: Example choice task. Each participant completed 13 such choice tasks. The seven features presented to participants",
      "page": 5
    },
    {
      "caption": "Figure 2: and Table 2. The second",
      "page": 6
    },
    {
      "caption": "Figure 2: Average Marginal Component Effects. The dots with horizontal bars (color-coded for each feature) represent the",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature": "The extent to which the being behaves\nautonomously, without the need for human control",
          "Artificial Being 1": "To a great extent",
          "Artificial Being 2": "To a great extent"
        },
        {
          "Feature": "The extent to which the being uses intelligence, such\nas memory, learning and planning, to achieve goals",
          "Artificial Being 1": "Somewhat",
          "Artificial Being 2": "Somewhat"
        },
        {
          "Feature": "The extent to which the being behaves on the basis of\nmoral judgments about what is right and wrong",
          "Artificial Being 1": "Not at all",
          "Artificial Being 2": "Somewhat"
        },
        {
          "Feature": "The extent to which the being behaves cooperatively\nwith humans",
          "Artificial Being 1": "To a great extent",
          "Artificial Being 2": "Somewhat"
        },
        {
          "Feature": "The extent to which the being's program for deciding\nhow to behave is complex",
          "Artificial Being 1": "Somewhat",
          "Artificial Being 2": "Not at all"
        },
        {
          "Feature": "The being's purpose in society",
          "Artificial Being 1": "Subject of scientific\nexperiments",
          "Artificial Being 2": "Social\ncompanionship"
        },
        {
          "Feature": "The being's physical appearance",
          "Artificial Being 1": "Robot-like physical\nbody",
          "Artificial Being 2": "Human-like physical\nbody"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "You Look Human, But Act Like a Machine: Agent Appearance and Behavior Modulate Different Aspects of Human-Robot Interaction",
      "authors": [
        "Abdulaziz Abubshait",
        "Eva Wiese"
      ],
      "year": "2017",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2017.01393"
    },
    {
      "citation_id": "2",
      "title": "2008. I Hate You! Disinhibition with Virtual Partners",
      "authors": [
        "Antonella De",
        "Sheryl Brahnam"
      ],
      "year": "2008",
      "venue": "Interacting with Computers",
      "doi": "10.1016/j.intcom.2008.02.004"
    },
    {
      "citation_id": "3",
      "title": "Moral Circle Expansion: A Promising Strategy to Impact the Far Future",
      "authors": [
        "Jacy Reese",
        "Eze Paez"
      ],
      "year": "2021",
      "venue": "Futures",
      "doi": "10.1016/j.futures.2021.102756"
    },
    {
      "citation_id": "4",
      "title": "I Don't Want To Shoot The Android\": Players Translate Real-Life Moral Intuitions to In-Game Decisions in Detroit: Become Human",
      "authors": [
        "Karina Arrambide",
        "John Yoon",
        "Cayley Macarthur",
        "Katja Rogers",
        "Alessandra Luz",
        "Lennart Nacke"
      ],
      "year": "2022",
      "venue": "CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3491102.3502019"
    },
    {
      "citation_id": "5",
      "title": "The Moral Machine Experiment",
      "authors": [
        "Edmond Awad",
        "Sohan Dsouza",
        "Richard Kim",
        "Jonathan Schulz",
        "Joseph Henrich",
        "Azim Shariff",
        "Jean-Fran√ßois Bonnefon",
        "Iyad Rahwan"
      ],
      "year": "2018",
      "venue": "Nature",
      "doi": "10.1038/s41586-018-0637-6"
    },
    {
      "citation_id": "6",
      "title": "Conjoint Survey Experiments. In Cambridge Handbook of Advances in Experimental Political Science",
      "authors": [
        "Kirk Bansak",
        "Jens Hainmueller",
        "Daniel Hopkins",
        "Teppei Yamamoto"
      ],
      "year": "2021",
      "venue": "Conjoint Survey Experiments. In Cambridge Handbook of Advances in Experimental Political Science"
    },
    {
      "citation_id": "7",
      "title": "Beyond the Breaking Point? Survey Satisficing in Conjoint Experiments",
      "authors": [
        "Kirk Bansak",
        "Jens Hainmueller",
        "Daniel Hopkins",
        "Teppei Yamamoto"
      ],
      "year": "2021",
      "venue": "Political Science Research and Methods",
      "doi": "10.1017/psrm.2019.13"
    },
    {
      "citation_id": "8",
      "title": "Daisy, Daisy, Give Me Your Answer Do!\" Switching off a Robot",
      "authors": [
        "Christoph Bartneck",
        "Michel Van Der Hoek",
        "Omar Mubin",
        "Abdullah Al"
      ],
      "year": "2007",
      "venue": "2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "9",
      "title": "Toward a Framework for Levels of Robot Autonomy in Human-Robot Interaction",
      "authors": [
        "M Jenay",
        "Arthur Beer",
        "Wendy Fisk",
        "Rogers"
      ],
      "year": "2014",
      "venue": "Journal of Human-Robot Interaction",
      "doi": "10.5898/JHRI.3.2.Beer"
    },
    {
      "citation_id": "10",
      "title": "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing",
      "authors": [
        "Yoav Benjamini",
        "Yosef Hochberg"
      ],
      "year": "1995",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
      "doi": "10.1111/j.2517-6161.1995.tb02031.x"
    },
    {
      "citation_id": "11",
      "title": "An Evaluation of Visual Embodiment for Voice Assistants",
      "authors": [
        "Michael Bonfert",
        "Nima Zargham",
        "Florian Saade",
        "Robert Porzel",
        "Rainer Malaka"
      ],
      "year": "2021",
      "venue": "An Evaluation of Visual Embodiment for Voice Assistants"
    },
    {
      "citation_id": "12",
      "title": "Proceedings of the 3rd Conference on Conversational User Interfaces (CUI '21)",
      "venue": "Proceedings of the 3rd Conference on Conversational User Interfaces (CUI '21)",
      "doi": "10.1145/3469595.3469611"
    },
    {
      "citation_id": "13",
      "title": "Can Moral Rightness (Utilitarian Approach) Outweigh the Ingroup Favoritism Bias in Human-Agent Interaction",
      "authors": [
        "Aldo Gonzalez",
        "Marlena Fraune",
        "Ricarda Wullenkord"
      ],
      "year": "2022",
      "venue": "Proceedings of the 10th International Conference on Human-Agent Interaction",
      "doi": "10.1145/3527188.3561930"
    },
    {
      "citation_id": "14",
      "title": "Children's Cognitive and Behavioral Reactions to an Autonomous Versus Controlled Social Robot Dog",
      "authors": [
        "Nadia Chernyak",
        "Heather Gary"
      ],
      "year": "2016",
      "venue": "Early Education and Development",
      "doi": "10.1080/10409289.2016.1158611"
    },
    {
      "citation_id": "15",
      "title": "Statistical Methods for Comparing Regression Coefficients Between Models",
      "authors": [
        "Clifford Clogg",
        "Eva Petkova",
        "Adamantios Haritou"
      ],
      "year": "1995",
      "venue": "Amer. J. Sociology",
      "doi": "10.1086/230638"
    },
    {
      "citation_id": "16",
      "title": "Should We Treat Teddy Bear 2.0 as a Kantian Dog? Four Arguments for the Indirect Moral Standing of Personal Social Robots, with Implications for Thinking About Animals and Humans",
      "authors": [
        "Mark Coeckelbergh"
      ],
      "year": "2021",
      "venue": "Minds and Machines",
      "doi": "10.1007/s11023-020-09554-3"
    },
    {
      "citation_id": "17",
      "title": "Exploring Prosociality in Human-Robot Teams",
      "authors": [
        "Filipa Correia",
        "Samuel Mascarenhas",
        "Samuel Gomes",
        "Patr√≠cia Arriaga",
        "Iolanda Leite",
        "Rui Prada",
        "Francisco Melo",
        "Ana Paiva"
      ],
      "year": "2019",
      "venue": "2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "doi": "10.1109/HRI.2019.8673299"
    },
    {
      "citation_id": "18",
      "title": "GPT-3: What's It Good for?",
      "authors": [
        "Robert Dale"
      ],
      "year": "2021",
      "venue": "Natural Language Engineering",
      "doi": "10.1017/S1351324920000601"
    },
    {
      "citation_id": "19",
      "title": "Extending Legal Protection to Social Robots: The Effects of Anthropomorphism, Empathy, and Violent Behavior towards Robotic Objects",
      "year": "2016",
      "venue": "Extending Legal Protection to Social Robots: The Effects of Anthropomorphism, Empathy, and Violent Behavior towards Robotic Objects"
    },
    {
      "citation_id": "20",
      "title": "Anthropomorphic Inferences from Emotional Nonverbal Cues: A Case Study",
      "authors": [
        "Friederike Eyssel",
        "Frank Hegel",
        "Gernot Horstmann",
        "Claudia Wagner"
      ],
      "year": "2010",
      "venue": "19th International Symposium in Robot and Human Interactive Communication",
      "doi": "10.1109/ROMAN.2010.5598687"
    },
    {
      "citation_id": "21",
      "title": "If You Sound like Me, You Must Be More Human': On the Interplay of Robot and User Features on Human-Robot Acceptance and Anthropomorphism",
      "authors": [
        "Friederike Eyssel",
        "Laura De Ruiter",
        "Dieta Kuchenbrandt",
        "Simon Bobinger",
        "Frank Hegel"
      ],
      "year": "2012",
      "venue": "2012 7th ACM/IEEE International Conference on Human-Robot Interaction (HRI)",
      "doi": "10.1145/2157689.2157717"
    },
    {
      "citation_id": "22",
      "title": "Blurring Human-Machine Distinctions: Anthropomorphic Appearance in Social Robots as a Threat to Human Distinctiveness",
      "authors": [
        "Francesco Ferrari",
        "Maria Paladino",
        "Jolanda Jetten"
      ],
      "year": "2016",
      "venue": "International Journal of Social Robotics",
      "doi": "10.1007/s12369-016-0338-y"
    },
    {
      "citation_id": "23",
      "title": "Constrained Choice: Children's and Adults' Attribution of Choice to a Humanoid Robot",
      "authors": [
        "Teresa Flanagan",
        "Joshua Rottman",
        "Lauren Howard"
      ],
      "year": "2021",
      "venue": "Cognitive Science",
      "doi": "10.1111/cogs.13043"
    },
    {
      "citation_id": "24",
      "title": "GPT-3: Its Nature, Scope, Limits, and Consequences. Minds and Machines",
      "authors": [
        "Luciano Floridi",
        "Massimo Chiriatti"
      ],
      "year": "2020",
      "venue": "GPT-3: Its Nature, Scope, Limits, and Consequences. Minds and Machines",
      "doi": "10.1007/s11023-020-09548-1"
    },
    {
      "citation_id": "25",
      "title": "A Priori Power Analyses for Conjoint Experiments",
      "authors": [
        "Markus Freitag"
      ],
      "year": "2021",
      "venue": "A Priori Power Analyses for Conjoint Experiments"
    },
    {
      "citation_id": "26",
      "title": "Who Wants to Grant Robots Rights",
      "authors": [
        "M Maartje",
        "Frank De Graaf",
        "Koen Hindriks",
        "Hindriks"
      ],
      "year": "2021",
      "venue": "Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction (HRI '21 Companion)",
      "doi": "10.1145/3434074.3446911"
    },
    {
      "citation_id": "27",
      "title": "Dimensions of Mind Perception",
      "authors": [
        "Heather Gray",
        "Kurt Gray",
        "Daniel Wegner"
      ],
      "year": "2007",
      "venue": "Science",
      "doi": "10.1126/science.1134475"
    },
    {
      "citation_id": "28",
      "title": "Feeling Robots and Human Zombies: Mind Perception and the Uncanny Valley",
      "authors": [
        "Kurt Gray",
        "Daniel Wegner"
      ],
      "year": "2012",
      "venue": "Cognition",
      "doi": "10.1016/j.cognition.2012.06.007"
    },
    {
      "citation_id": "29",
      "title": "Mind Perception Is the Essence of Morality",
      "authors": [
        "Kurt Gray",
        "Liane Young",
        "Adam Waytz"
      ],
      "year": "2012",
      "venue": "Psychological Inquiry",
      "doi": "10.1080/1047840X.2012.651387"
    },
    {
      "citation_id": "30",
      "title": "Improving Evaluations of Advanced Robots by Depicting Them in Harmful Situations",
      "authors": [
        "Andrea Grundke",
        "Jan-Philipp Stein",
        "Markus Appel"
      ],
      "year": "2023",
      "venue": "Computers in Human Behavior",
      "doi": "10.1016/j.chb.2022.107565"
    },
    {
      "citation_id": "31",
      "title": "Causal Inference in Conjoint Analysis: Understanding Multidimensional Choices via Stated Preference Experiments",
      "authors": [
        "Jens Hainmueller",
        "Daniel Hopkins",
        "Teppei Yamamoto"
      ],
      "year": "2014",
      "venue": "Political Analysis",
      "doi": "10.1093/pan/mpt024"
    },
    {
      "citation_id": "32",
      "title": "The Moral Consideration of Artificial Entities: A Literature Review",
      "authors": [
        "Jamie Harris",
        "Jacy Reese"
      ],
      "year": "2021",
      "venue": "Science and Engineering Ethics",
      "doi": "10.1007/s11948-021-00331-8"
    },
    {
      "citation_id": "33",
      "title": "Playing a Different Imitation Game: Interaction with an Empathic Android Robot",
      "authors": [
        "Frank Hegel",
        "Torsten Spexard",
        "Britta Wrede",
        "Gernot Horstmann",
        "Thurid Vogt"
      ],
      "year": "2006",
      "venue": "2006 6th IEEE-RAS International Conference on Humanoid Robots",
      "doi": "10.1109/ICHR.2006.321363"
    },
    {
      "citation_id": "34",
      "title": "Should I Follow the Human, or Follow the Robot?\" -Robots in Power Can Have More Influence Than Humans on Decision-Making",
      "authors": [
        "Yoyo Tsung-Yu",
        "Wen-Ying Hou",
        "Malte Lee",
        "Jung"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3544548.3581066"
    },
    {
      "citation_id": "35",
      "title": "What Is a Human?: Toward Psychological Benchmarks in the Field of Human-Robot Interaction",
      "authors": [
        "H Peter",
        "Hiroshi Kahn",
        "Batya Ishiguro",
        "Takayuki Friedman",
        "Nathan Kanda",
        "Rachel Freier",
        "Jessica Severson",
        "Miller"
      ],
      "year": "2007",
      "venue": "Interaction Studies",
      "doi": "10.1075/is.8.3.04kah"
    },
    {
      "citation_id": "36",
      "title": "What's to Bullying a Bot?: Correlates between Chatbot Humanlikeness and Abuse",
      "authors": [
        "Merel Keijsers",
        "Christoph Bartneck",
        "Friederike Eyssel"
      ],
      "year": "2021",
      "venue": "Interaction Studies",
      "doi": "10.1075/is.20002.kei"
    },
    {
      "citation_id": "37",
      "title": "Anthropomorphic Interactions with a Robot and Robot-like Agent",
      "authors": [
        "Sara Kiesler",
        "Aaron Powers",
        "Susan Fussell",
        "Cristen Torrey"
      ],
      "year": "2008",
      "venue": "Social Cognition",
      "doi": "10.1521/soco.2008.26.2.169"
    },
    {
      "citation_id": "38",
      "title": "A Prisoner's Dilemma Experiment on Cooperation with People and Human-like Computers",
      "authors": [
        "Sara Kiesler",
        "Lee Sproull",
        "Keith Waters"
      ],
      "year": "1996",
      "venue": "Journal of Personality and Social Psychology",
      "doi": "10.1037/0022-3514.70.1.47"
    },
    {
      "citation_id": "39",
      "title": "Self-Interest and Data Protection Drive the Adoption and Moral Acceptability of Big Data Technologies: A Conjoint Analysis Approach",
      "authors": [
        "I Rabia",
        "Mark Kodapanakkal",
        "Christoph Brandt",
        "Ilja Kogler",
        "Van Beest"
      ],
      "year": "2020",
      "venue": "Computers in Human Behavior",
      "doi": "10.1016/j.chb.2020.106303"
    },
    {
      "citation_id": "40",
      "title": "Privacy Attitudes and Privacy Behaviour: A Review of Current Research on the Privacy Paradox Phenomenon",
      "authors": [
        "Spyros Kokolakis"
      ],
      "year": "2017",
      "venue": "Computers & Security",
      "doi": "10.1016/j.cose.2015.07.002"
    },
    {
      "citation_id": "41",
      "title": "2021. I Saw It on YouTube! How Online Videos Shape Perceptions of Mind",
      "authors": [
        "Dennis K√ºster",
        "Aleksandra Swiderska",
        "David Gunkel"
      ],
      "year": "2021",
      "venue": "Morality, and Fears about Robots. New Media & Society",
      "doi": "10.1177/1461444820954199"
    },
    {
      "citation_id": "42",
      "title": "What Would Qualify an Artificial Intelligence for Moral Standing? AI and Ethics",
      "authors": [
        "Ali Ladak"
      ],
      "year": "2023",
      "venue": "What Would Qualify an Artificial Intelligence for Moral Standing? AI and Ethics",
      "doi": "10.1007/s43681-023-00260-1"
    },
    {
      "citation_id": "43",
      "title": "Extending Perspective Taking to Nonhuman Animals and Artificial Entities",
      "authors": [
        "Ali Ladak",
        "Matti Wilks",
        "Jacy Reese"
      ],
      "year": "2023",
      "venue": "Social Cognition",
      "doi": "10.1521/soco.2023.41.3.274"
    },
    {
      "citation_id": "44",
      "title": "Too Human and Not Human Enough: A Grounded Theory Analysis of Mental Health Harms from Emotional Dependence on the Social Chatbot Replika",
      "authors": [
        "Linnea Laestadius",
        "Andrea Bishop",
        "Michael Gonzalez",
        "Diana Illenƒç√≠k",
        "Celeste Campos-Castillo"
      ],
      "year": "2022",
      "venue": "Too Human and Not Human Enough: A Grounded Theory Analysis of Mental Health Harms from Emotional Dependence on the Social Chatbot Replika",
      "doi": "10.1177/14614448221142007"
    },
    {
      "citation_id": "45",
      "title": "What's on Your Virtual Mind?: Mind Perception in Human-Agent Negotiations",
      "authors": [
        "Minha Lee",
        "Gale Lucas",
        "Johnathan Mell",
        "Emmanuel Johnson",
        "Jonathan Gratch"
      ],
      "year": "2019",
      "venue": "Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents",
      "doi": "10.1145/3308532.3329465"
    },
    {
      "citation_id": "46",
      "title": "A Collection of Definitions of Intelligence",
      "authors": [
        "Shane Legg",
        "Marcus Hutter"
      ],
      "year": "2007",
      "venue": "Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms"
    },
    {
      "citation_id": "47",
      "title": "Collecting the Public Perception of AI and Robot Rights",
      "authors": [
        "Gabriel Lima",
        "Changyeon Kim",
        "Seungho Ryu"
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Human-Computer Interaction",
      "doi": "10.1145/3415206"
    },
    {
      "citation_id": "48",
      "title": "Protecting Sentient Artificial Intelligence: A Survey of Lay Intuitions on Standing, Personhood, and General Legal Protection",
      "authors": [
        "Eric Mart√≠nez",
        "Christoph Winter"
      ],
      "year": "2021",
      "venue": "Frontiers in Robotics and AI",
      "doi": "10.3389/frobt.2021.788355"
    },
    {
      "citation_id": "49",
      "title": "Can Computers Be Teammates?",
      "authors": [
        "Clifford Nass",
        "B Fogg",
        "Youngme Moon"
      ],
      "year": "1996",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1006/ijhc.1996.0073"
    },
    {
      "citation_id": "50",
      "title": "Saving the Robot or the Human? Robots Who Feel Deserve Moral Care",
      "authors": [
        "R Sari",
        "Nijssen",
        "C Barbara",
        "Rick M√ºller",
        "Markus Van Baaren",
        "Paulus"
      ],
      "year": "2019",
      "venue": "Social Cognition",
      "doi": "10.1521/soco.2019.37.1.41"
    },
    {
      "citation_id": "51",
      "title": "Psychology in Human-Robot Communication: An Attempt through Investigation of Negative Attitudes and Anxiety toward Robots",
      "authors": [
        "T Nomura",
        "T Kanda",
        "T Suzuki",
        "K Kato"
      ],
      "year": "2004",
      "venue": "RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication",
      "doi": "10.1109/ROMAN.2004.1374726"
    },
    {
      "citation_id": "52",
      "title": "Why Do Children Abuse Robots?",
      "authors": [
        "Tatsuya Nomura",
        "Takayuki Uratani",
        "Takayuki Kanda",
        "Kazutaka Matsumoto",
        "Hiroyuki Kidokoro",
        "Yoshitaka Suehiro",
        "Sachie Yamada"
      ],
      "year": "2015",
      "venue": "Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts (HRI'15 Extended Abstracts)",
      "doi": "10.1145/2701973.2701977"
    },
    {
      "citation_id": "53",
      "title": "Using the Correct Statistical Test for the Equality of Regression Coefficients",
      "authors": [
        "Raymond Paternoster",
        "Robert Brame",
        "Paul Mazerolle",
        "Alex Piquero"
      ],
      "year": "1998",
      "venue": "Criminology",
      "doi": "10.1111/j.1745-9125.1998.tb01268.x"
    },
    {
      "citation_id": "54",
      "title": "Predicting the Moral Consideration of Artificial Intelligences",
      "authors": [
        "V Janet",
        "Jacy Pauketat",
        "Anthis Reese"
      ],
      "year": "2022",
      "venue": "Computers in Human Behavior",
      "doi": "10.1016/j.chb.2022.107372"
    },
    {
      "citation_id": "55",
      "title": "Cruel Nature: Harmfulness as an Important, Overlooked Dimension in Judgments of Moral Standing",
      "authors": [
        "Jared Piazza",
        "Justin Landy",
        "Geoffrey Goodwin"
      ],
      "year": "2014",
      "venue": "Cognition",
      "doi": "10.1016/j.cognition.2013.12.013"
    },
    {
      "citation_id": "56",
      "title": "When Meat Gets Personal, Animals' Minds Matter Less: Motivated Use of Intelligence Information in Judgments of Moral Standing",
      "authors": [
        "Jared Piazza",
        "Steve Loughnan"
      ],
      "year": "2016",
      "venue": "Social Psychological and Personality Science",
      "doi": "10.1177/1948550616660159"
    },
    {
      "citation_id": "57",
      "title": "Comparing a Computer Agent with a Humanoid Robot",
      "authors": [
        "Aaron Powers",
        "Sara Kiesler",
        "Susan Fussell",
        "Cristen Torrey"
      ],
      "year": "2007",
      "venue": "Proceedings of the ACM/IEEE International Conference on Human-robot Interaction (HRI '07)",
      "doi": "10.1145/1228716.1228736"
    },
    {
      "citation_id": "58",
      "title": "Empathizing with Robots: Fellow Feeling along the Anthropomorphic Spectrum",
      "authors": [
        "Laurel Riek",
        "Tal-Chen",
        "Bhismadev Rabinowitch",
        "Peter Chakrabarti"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops",
      "doi": "10.1109/ACII.2009.5349423"
    },
    {
      "citation_id": "59",
      "title": "An Experimental Study on Emotional Reactions Towards a Robot",
      "authors": [
        "Astrid Rosenthal-Von Der P√ºtten",
        "Nicole Kr√§mer",
        "Laura Hoffmann",
        "Sabrina Sobieraj",
        "Sabrina Eimler"
      ],
      "year": "2013",
      "venue": "International Journal of Social Robotics",
      "doi": "10.1007/s12369-012-0173-8"
    },
    {
      "citation_id": "60",
      "title": "Tree-Huggers Versus Human-Lovers: Anthropomorphism and Dehumanization Predict Valuing Nature Over Outgroups",
      "authors": [
        "Joshua Rottman",
        "Charlie Crimston",
        "Stylianos Syropoulos"
      ],
      "year": "2021",
      "venue": "Cognitive Science",
      "doi": "10.1111/cogs.12967"
    },
    {
      "citation_id": "61",
      "title": "Mistaking Minds and Machines: How Speech Affects Dehumanization and Anthropomorphism",
      "authors": [
        "Juliana Schroeder",
        "Nicholas Epley"
      ],
      "year": "2016",
      "venue": "Journal of Experimental Psychology: General",
      "doi": "10.1037/xge0000214"
    },
    {
      "citation_id": "62",
      "title": "Power Analysis for Conjoint Experiments",
      "authors": [
        "Julian Schuessler",
        "Markus Freitag"
      ],
      "year": "2020",
      "venue": "Power Analysis for Conjoint Experiments"
    },
    {
      "citation_id": "63",
      "title": "A Defense of the Rights of Artificial Intelligences: Defense of the Rights of Artificial Intelligences",
      "authors": [
        "Eric Schwitzgebel",
        "Mara Garza"
      ],
      "year": "2015",
      "venue": "Midwest Studies In Philosophy",
      "doi": "10.1111/misp.12032"
    },
    {
      "citation_id": "64",
      "title": "Do You Mind? User Perceptions of Machine Consciousness",
      "authors": [
        "Ava Scott",
        "Daniel Neumann",
        "Jasmin Niess",
        "Pawe≈Ç Wo≈∫niak"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3544548.3581296"
    },
    {
      "citation_id": "65",
      "title": "Perceived Justice and Reactions to Coercive Computers1",
      "authors": [
        "B Daniel",
        "Shank"
      ],
      "year": "2012",
      "venue": "Sociological Forum",
      "doi": "10.1111/j.1573-7861.2012.01322.x"
    },
    {
      "citation_id": "66",
      "title": "Impressions of Computer and Human Agents after Interaction: Computer Identity Weakens Power but Not Goodness Impressions",
      "authors": [
        "B Daniel",
        "Shank"
      ],
      "year": "2014",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1016/j.ijhcs.2014.05.002"
    },
    {
      "citation_id": "67",
      "title": "Attributions of Morality and Mind to Artificial Intelligence after Real-World Moral Violations",
      "authors": [
        "B Daniel",
        "Alyssa Shank",
        "Desanti"
      ],
      "year": "2018",
      "venue": "Computers in Human Behavior",
      "doi": "10.1016/j.chb.2018.05.014"
    },
    {
      "citation_id": "68",
      "title": "Measuring Empathy for Human and Robot Hand Pain Using Electroencephalography",
      "authors": [
        "Yutaka Suzuki",
        "Lisa Galli",
        "Ayaka Ikeda",
        "Shoji Itakura",
        "Michiteru Kitazaki"
      ],
      "year": "2015",
      "venue": "Scientific Reports",
      "doi": "10.1038/srep15924"
    },
    {
      "citation_id": "69",
      "title": "Robots as Malevolent Moral Agents: Harmful Behavior Results in Dehumanization, Not Anthropomorphism",
      "authors": [
        "Aleksandra Swiderska",
        "Dennis K√ºster"
      ],
      "year": "2020",
      "venue": "Cognitive Science",
      "doi": "10.1111/cogs.12872"
    },
    {
      "citation_id": "70",
      "title": "The Two Sources of Moral Standing",
      "authors": [
        "Justin Sytsma",
        "Edouard Machery"
      ],
      "year": "2012",
      "venue": "Review of Philosophy and Psychology",
      "doi": "10.1007/s13164-012-0102-7"
    },
    {
      "citation_id": "71",
      "title": "Inducing Bystander Interventions During Robot Abuse with Social Mechanisms",
      "authors": [
        "Xiang Zhi Tan",
        "Marynel V√°zquez",
        "Elizabeth Carter",
        "Cecilia Morales",
        "Aaron Steinfeld"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction (HRI '18)",
      "doi": "10.1145/3171221.3171247"
    },
    {
      "citation_id": "72",
      "title": "We Perceive a Mind in a Robot When We Help It",
      "authors": [
        "Tetsushi Tanibe",
        "Takaaki Hashimoto",
        "Kaori Karasawa"
      ],
      "year": "2017",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0180952"
    },
    {
      "citation_id": "73",
      "title": "Can Social Robots Qualify for Moral Consideration? Reframing the Question about Robot Rights",
      "authors": [
        "T Herman",
        "Tavani"
      ],
      "year": "2018",
      "venue": "Information",
      "doi": "10.3390/info9040073"
    },
    {
      "citation_id": "74",
      "title": "Mind Perception of Robots Varies With Their Economic Versus Social Function",
      "authors": [
        "Xijing Wang",
        "Eva Krumhuber"
      ],
      "year": "2018",
      "venue": "Frontiers in Psychology",
      "doi": "10.3389/fpsyg.2018.01230"
    },
    {
      "citation_id": "75",
      "title": "The Harm-Made Mind: Observing Victimization Augments Attribution of Minds to Vegetative Patients, Robots, and the Dead",
      "authors": [
        "Adrian Ward",
        "Andrew Olsen",
        "Daniel Wegner"
      ],
      "year": "2013",
      "venue": "The Harm-Made Mind: Observing Victimization Augments Attribution of Minds to Vegetative Patients, Robots, and the Dead",
      "doi": "10.1177/0956797612472343"
    },
    {
      "citation_id": "76",
      "title": "Who Sees Human?: The Stability and Importance of Individual Differences in Anthropomorphism",
      "authors": [
        "Adam Waytz",
        "John Cacioppo",
        "Nicholas Epley"
      ],
      "year": "2010",
      "venue": "Perspectives on Psychological Science",
      "doi": "10.1177/1745691610369336"
    },
    {
      "citation_id": "77",
      "title": "The Mind in the Machine: Anthropomorphism Increases Trust in an Autonomous Vehicle",
      "authors": [
        "Adam Waytz",
        "Joy Heafner",
        "Nicholas Epley"
      ],
      "year": "2014",
      "venue": "Journal of Experimental Social Psychology",
      "doi": "10.1016/j.jesp.2014.01.005"
    },
    {
      "citation_id": "78",
      "title": "Anthropomorphism: Opportunities and Challenges in Human-Robot Interaction",
      "authors": [
        "Jakub Z≈Çotowski",
        "Diane Proudfoot",
        "Kumar Yogeeswaran",
        "Christoph Bartneck"
      ],
      "year": "2015",
      "venue": "International Journal of Social Robotics",
      "doi": "10.1007/s12369-014-0267-6"
    },
    {
      "citation_id": "79",
      "title": "Dimensions of Anthropomorphism : From Humanness to Humanlikeness",
      "authors": [
        "Jakub Z≈Çotowski",
        "Ewald Strasser",
        "Christoph Bartneck"
      ],
      "year": "2014",
      "venue": "2014 9th ACM/IEEE International Conference on Human-Robot Interaction (HRI)"
    },
    {
      "citation_id": "80",
      "title": "Can We Control It? Autonomous Robots Threaten Human Identity, Uniqueness, Safety, and Resources",
      "authors": [
        "Jakub Z≈Çotowski",
        "Christoph Kumar Yogeeswaran",
        "Bartneck"
      ],
      "year": "2017",
      "venue": "International Journal of Human-Computer Studies",
      "doi": "10.1016/j.ijhcs.2016.12.008"
    }
  ]
}