{
  "paper_id": "2509.18570v1",
  "title": "Harmonifuse: A Component-Selective And Prompt-Adaptive Framework For Multi-Task Speech Language Modeling",
  "published": "2025-09-23T02:53:38Z",
  "authors": [
    "Yuke Si",
    "Runyan Yang",
    "Yingying Gao",
    "Junlan Feng",
    "Chao Deng",
    "Shilei Zhang"
  ],
  "keywords": [
    "Multi-task Learning",
    "Speech Emotion Recognition",
    "Automatic Speech Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recent advances in large language models have facilitated the development of unified speech language models (SLMs) capable of supporting multiple speech tasks within a shared architecture. However, tasks such as automatic speech recognition (ASR) and speech emotion recognition (SER) rely on distinct types of information: ASR primarily depends on linguistic content, whereas SER requires the integration of both linguistic and paralinguistic cues. Existing multitask SLMs typically adopt naive parameter sharing or prompt-based conditioning without explicitly modeling the differences in information composition required by each task. Such designs risk task interference and performance degradation, especially under limited data conditions. To address these limitations, we propose HarmoniFuse, a component-selective and promptadaptive framework for multi-task speech language modeling. HarmoniFuse is designed to harmonize heterogeneous task demands by selecting and fusing task-relevant components of speech representations. Specifically, it integrates a gated speech encoder to extract task-specific acoustic features and a prompt-adaptive dynamic fusion module to aggregate transformer layers based on task characteristics. In addition, a batch-interleaved training strategy enables leveraging separate ASR and SER datasets without requiring joint annotation. Experimental results demonstrate that HarmoniFuse improves both ASR and SER performance, offering a scalable and robust solution for multitask speech understanding under realistic data constraints.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Benefiting from the rapid evolution of large language models (LLM), large-scale speech understanding systems have increasingly adopted a unified multitask processing paradigm  [1, 2, 3, 4, 5, 6] . These works demonstrate that shared encoderdecoder architectures can handle diverse speech tasks through † Equal contribution * Corresponding author multitask joint training, usually including automatic speech recognition (ASR), language identification, and speech emotion recognition (SER), etc. These unified multitask frameworks offer significant advantages by reducing deployment overhead, enhancing cross-task generalization, and enabling holistic speech understanding. Current large speech language models (SLMs) commonly adopt a brute-force multitask training strategy, leveraging hundreds of thousands of hours' annotated data to implicitly resolve task conflicts across diverse objectives  [7] . Some models further incorporate prompt-based decoding mechanisms to distinguish between tasks during inference  [4, 6] , providing a lightweight form of task conditioning. However, these approaches typically lack explicit architectural mechanisms for modeling the divergent nature of tasks like ASR and SER. Moreover, the effectiveness of this data-driven strategy hinges on the availability of large-scale multi-label datasets, which are especially costly and scarce for affective tasks such as SER. This limits the scalability and applicability of current approaches in scenarios where the available data remains insufficient for large-scale model training.\n\nIn contrast, prior work on ASR-SER integration has largely been conducted in small-scale models, where researchers have proposed strategies to mitigate task interference. Classical approaches include cascaded pipelines that apply ASR followed by text-based SER  [8, 9] , but such designs suffer from ASR error propagation and disconnect between acoustic and emotional features. More recently, some SER-related studies have introduced representation disentanglement  [10, 11]  and multi-stream attention mechanisms  [12, 13, 14]  to separately model affective and linguistic cues. While these methods show promise in controlled settings, they are typically developed for specific tasks and lightweight model architectures, and have not yet been applied to large-scale SLMs with unified multitask capabilities.\n\nTo address the above challenges, we propose Harmoni-Fuse, a component-selective and prompt-adaptive framework for multi-task speech language modeling. The core insight is that effective multi-task learning for divergent tasks such as ASR and SER requires more than parameter sharing. It demands task-aware, dynamic control over how information is extracted, routed, and fused within the model. Harmoni-Fuse selectively emphasizes task-specific components based on task prompt, such as linguistic details for ASR and a combination of linguistic and paralinguistic cues for SER.\n\nHarmoniFuse introduces three key innovations. First, a speech encoder selectively emphasizes task-relevant components from raw audio based on task characteristics. Second, a prompt-adaptive layer fusion module dynamically aggregate transformer layers based on task prompts and input samples, allowing the decoder to flexibly shift attention between linguistic precision and paralinguistic expressiveness. Third, a batch-interleaved training strategy is introduced to overcome the scarcity of jointly annotated ASR-SER data, allowing the model to alternate between independent ASR and SER samples during training. This design leverages separate task-specific datasets while still promoting cross-task learning. Together, these innovations provide fine-grained control over task-aware information flow, effectively mitigating task interference, improving generalization across diverse tasks, and enabling robust multi-task speech understanding in dataconstrained conditions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "As shown in Fig.  1 , HarmoniFuse consists of four components: (1) a speech encoder for extracting continuous acoustic representations from raw audio, (2) a multi-modal transformer language model for task-aware contextual processing, (3) a prompt-adaptive dynamic layer fusion module, and (4) task-specific output layers for downstream ASR or SER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Speech Encoder With Gated Layer Fusion",
      "text": "The first stage of HarmoniFuse converts raw waveforms into high-level speech representations. We employ a pre-trained 24-layer WavLM model  [15]  as the speech encoder due to its strong capability in capturing both phonetic content and prosodic cues across multiple layers. Given an input speech waveform, the speech encoder extracts a sequence of layer hidden states: H 1 , H 2 , . . . , H 24 ∈ R T0×d0 , where T 0 is the representation sequence length and d 0 is the WavLM model hidden dimension. To enable task-aware fusion across layers, we introduce a learnable weighting mechanism over all layers instead of using only the static layer:\n\nwhere w i is the learnable weight parameter of the i-th layer, jointly optimized with other model parameters during endto-end training. This layer fusion mechanism allows the encoder to dynamically emphasize different layer combinations based on task characteristics, thereby avoiding over-reliance on shallow or top-level features. It also enhances training efficiency and task specialization without requiring separate finetuning for each task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Modal Transformer Lm",
      "text": "The second stage processes fused speech representations using a transformer-based language model. Inspired by recent multitask prompting strategies  [16, 6] , we design a structured input format comprising three main parts: (1) speech representation H fused , (2) a task prompt token or token sequence (e.g., \"speech recognition\" for ASR or \"emotion recognition\" for SER), and (3) a \"BOS\" (beginning-of-sentence) token that signals the model to begin generation. These are concatenated and fed into a stack of M Transformer encoder layers. Given the input sequence, the model produces each layer's contextual representations R m , m ∈ {1, 2, . . . , M }:\n\nwhere T is the sequence length and d is the hidden dimension.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prompt-Adaptive Dynamic Layer Fusion",
      "text": "To enable task-specific adaptation, we introduce a dynamic layer fusion module on top of the SLM. For a given speech task τ , at each output time step t, we use a set of weights to fuse the model's layer-wise representation:\n\nwhere each α τ m,t ∈ [0, 1] represents the importance of layer m. The final task-conditioned representation is computed as:\n\nSpecifically, α τ m for each layer is decomposed into two parts:\n\nwhere β is a hyperpameter set to 0.5, σ (•) denotes the sigmoid function, λ τ m is a learnable task-and layer-specific scalar parameter, and FFN (•; Θ m ) is a feed-forward network specific to layers with hidden dimention d ′ = d/4, taking hidden representation r τ m,t as input. The first part of α τ m,t\n\nencodes the preference directly related to the task, while the second part captures the relevance of speech representations as well as task prompts.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Task-Specific Output Layers",
      "text": "The final stage includes task-specific heads that project the fused transformer representation to output spaces for ASR or SER.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Asr Head (Autoregressive Sequence Generation)",
      "text": "For ASR, the model generates output tokens in an autoregressive manner. At each decoding step t, the head computes a probability distribution over the vocabulary using a linear projection followed by a softmax:\n\nwhere r ASR t is the output fused hidden state of the Transformer at step t, and W ASR , b ASR are learnable parameters. The loss is computed using cross-entropy with teacher forcing during training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ser Head (Classification)",
      "text": "For SER, the SLM predict only the emotion label (i.e. T = 1). So we use the fused hidden state r SER 1 of the Transformer to predict the label. The head applies a fully connected layer:\n\nwhere W SER , b SER are task-specific learnable parameters. The loss is computed using categorical cross-entropy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "For the ASR task, we utilize the widely adopted 960 hours LibriSpeech dataset  [17]  as our primary training corpus. To enhance the robustness of speech recognition across variable speaking styles, we apply 3-fold speed perturbation at ratio 0.9, 1.0 and 1.1  [18] .\n\nFor the SER task, we adopt the IEMOCAP dataset  [19]  as the primary supervised corpus. To scale up the data for large model training, we further incorporate unlabeled speech from the Emilia dataset  [20] , which was originally designed for speech synthesis and features rich emotional expressiveness. We follow the Emotion2Vec framework  [21]  to automatically generate emotion pseudo-labels for Emilia. To ensure label compatibility with IEMOCAP, we perform emotion category alignment and retain four prototypical emotions: happy, sad, angry, and neutral  [21, 22] . From the full Emilia corpus, we select approximately 1,800 hours of English utterances with a relatively balanced distribution across these four emotion classes. This pseudo-labeled subset of Emilia is then combined with IEMOCAP to form the training data. Instead of performing 5-fold cross-validation as commonly done for IEMOCAP in smaller-scale setups, we directly adopt the session 5 as a fixed held-out test set, in line with several existing works where high computational cost limits repeated training across folds  [22] . The remainder of IEMOCAP data is used for training and validation, providing a rigorous and reproducible evaluation while avoiding unnecessary retraining.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model And Training Settings",
      "text": "The multi-modal transformer language model in Harmoni-Fuse consists of 12 transformer blocks, each with 12 attention heads, a hidden size of 768, and a feed-forward network size of 2,048. The total number of trainable parameters is approximately 160 million. Training is performed on 4 NVIDIA A800 GPUs (80 GB each). One training batch on each GPU contains 400 seconds of speech, and we apply gradient accumulation over 4 steps to simulate a larger effective batch size. Optimization is conducted using the Adam optimizer with an initial warm-up of 2,000 steps, followed by linear decay. We train the model for 20 epochs, and select the checkpoint with the lowest validation loss for final evaluation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "We evaluate HarmoniFuse on two downstream tasks: SER and ASR. For SER, we report unweighted accuracy (UA) and weighted accuracy (WA), which measure performance by averaging over classes and by accounting for class distribution, respectively. For ASR, we report word error rate (WER) on both the \"clean\" and \"other\" subsets of the LibriSpeech test set. The results are presented in Tables  1  and 2 . Single-task performance: gated layer aggregation enhances encoder representation learning. Table  1  shows the performance of different encoder layer selections in singletask settings. Prior studies have shown that different layers of pre-trained speech models like wav2vec 2.0 and HuBERT capture distinct types of information  [23, 24] . They showed that lower and middle layers tend to preserve acoustic and paralinguistic features, while top layers encode more linguistic information. Our experiments corroborate these findings.\n\nSpecifically, we observe that using the 12th layer of the WavLM encoder yields the higher accuracy for SER (75.01% on UA and 73.65% on WA), likely because this layer best preserves emotional cues such as pitch, rhythm, and tone. In contrast, the 24th (final) layer achieves the lowest WER for ASR (4.5% on clean, 6.0% on other), consistent with its stronger focus on semantic content. To move beyond fixed-layer selection, we introduce a learnable gating mechanism to aggregate information across all encoder layers. This approach outperforms both single-layer baselines, improving SER to 75.31% (UA) and 75.66% (WA) and reducing ASR WER to 3.8% (clean) and 5.1% (other), showing that gated speech encoder is beneficial even in single-task scenarios.\n\nMulti-task performance: handling conflicts between SER and ASR. Table  2  reports performance under multi-task learning with task prompts. It shows that the shared encoder alone cannot ensure effective multi-task learning, and the model exhibits trade-off results without the prompt-adaptive dynamic layer fusion. Specifically, the model using the 12th layer performs reasonably well on SER (75.51% on UA and 74.94% on WA) but poorly on ASR (6.1% WER on clean and 12.1% on other), while the 24th layer does the opposite. The gated speech encoder attempts to compromise, but still fails to reach the performance of the best single-task models, suggesting that static fusion cannot resolve task interference.\n\nIn contrast, HarmoniFuse introduces prompt-adaptive dynamic layer fusion, allowing the model to dynamically aggregate different layers depending on the task. This significantly improves both tasks: SER accuracy rises to 76.51% (UA) and 76.31% (WA), and ASR WER drops to 3.5% (clean) and 5.1% (other). Notably, both ASR and SER performances surpasses the single-task oracle baselines (WER 3.8% on clean and 5.1% on other for ASR, UA 75.31% and WA 75.66% for SER), confirming that our method effectively balances taskspecific needs without requiring separate models.\n\nTo further analyze the model's behavior, we visualize the    5 ) in Figure  2 . For SER, the model assigns higher importance to both the highest and mid-level layers (e.g., peaks at index 8 and 12), indicating a strong reliance on both prosodic and linguistic components. In contrast, the ASR task emphasizes deeper layers, more concentrating on refined linguistic component. It validates that HarmoniFuse learns task-discriminative representations by selecting the most relevant layers for each task. Rather than forcing a compromise, the model routes information according to the unique demands of each objective.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we presented HarmoniFuse, a componentselective and prompt-adaptive framework designed to harmonize divergent objectives in multitask speech understanding, specifically focusing on ASR and SER tasks. The proposed framework incorporates three key innovations: a gated speech encoder, prompt-adaptive dynamic layer fusion, and batchinterleaved training strategy. These components jointly address longstanding challenges in multitask learning, including representation conflicts, limited joint annotations, and intertask interference. Experimental results demonstrate that it achieves strong performance across both tasks by dynamically coordinating linguistic and paralinguistic components, highlighting the benefits of task prompt-aware fusion in large speech language models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , HarmoniFuse consists of four compo-",
      "page": 2
    },
    {
      "caption": "Figure 1: Overall architecture of proposed HarmoniFuse, featur-",
      "page": 2
    },
    {
      "caption": "Figure 2: Comparison of layer-weights distribution between",
      "page": 4
    },
    {
      "caption": "Figure 2: For SER, the model assigns higher importance to both the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Experimental results on single-task models.",
      "page": 4
    },
    {
      "caption": "Table 1: shows the",
      "page": 4
    },
    {
      "caption": "Table 2: reports performance under multi-task",
      "page": 4
    },
    {
      "caption": "Table 2: Experimental results on multi-task with task prompt.",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Unified-modal encoder-decoder pretraining for spoken language processing",
      "authors": [
        "Junyi Ao",
        "Rui Wang",
        "Long Zhou",
        "Chengyi Wang",
        "Shuo Ren",
        "Yu Wu",
        "Shujie Liu",
        "Tom Ko",
        "Qing Li",
        "Yu Zhang"
      ],
      "year": "2021",
      "venue": "Speecht",
      "arxiv": "arXiv:2110.07205"
    },
    {
      "citation_id": "3",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "4",
      "title": "Pengi: An audio language model for audio tasks",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Rita Singh",
        "Huaming Wang"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "6",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "7",
      "title": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "authors": [
        "Qian Keyu An",
        "Chong Chen",
        "Zhihao Deng",
        "Changfeng Du",
        "Zhifu Gao",
        "Yue Gao",
        "Ting Gu",
        "Hangrui He",
        "Kai Hu",
        "Hu"
      ],
      "year": "2024",
      "venue": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "arxiv": "arXiv:2407.04051"
    },
    {
      "citation_id": "8",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "9",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with asr transcripts: A comprehensive study on word error rate and fusion techniques",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "11",
      "title": "Frontend attributes disentanglement for speech emotion recognition",
      "authors": [
        "Yu-Xuan Xi",
        "Yan Song",
        "Li-Rong Dai",
        "Ian Mcloughlin",
        "Lin Liu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Disentanglement network: Disentangle the emotional features from acoustic features for speech emotion recognition",
      "authors": [
        "Zhichen Yuan",
        "Shuzhen Cl Philip Chen",
        "Tong Li",
        "Zhang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using dual-stream representation and cross-attention fusion",
      "authors": [
        "Shaode Yu",
        "Jiajian Meng",
        "Wenqing Fan",
        "Ye Chen",
        "Bing Zhu",
        "Hang Yu",
        "Yaoqin Xie",
        "Qiurui Sun"
      ],
      "year": "2024",
      "venue": "Electronics"
    },
    {
      "citation_id": "14",
      "title": "Memocmt: multimodal emotion recognition using cross-modal transformerbased feature fusion",
      "authors": [
        "Mustaqeem Khan",
        "Phuong-Nam Tran",
        "Nhat Pham",
        "Abdulmotaleb Saddik",
        "Alice Othmani"
      ],
      "year": "2025",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "Heqing Zou",
        "Yuke Si",
        "Chen Chen",
        "Deepu Rajan",
        "Eng Siong"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "16",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Polyspeech: Exploring unified multitask speech models for competitiveness with single-task models",
      "authors": [
        "Runyan Yang",
        "Huibao Yang",
        "Xiqing Zhang",
        "Tiantian Ye",
        "Ying Liu",
        "Yingying Gao",
        "Shilei Zhang",
        "Chao Deng",
        "Junlan Feng"
      ],
      "year": "2024",
      "venue": "Polyspeech: Exploring unified multitask speech models for competitiveness with single-task models",
      "arxiv": "arXiv:2406.07801"
    },
    {
      "citation_id": "18",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "19",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "Tom Ko",
        "Vijayaditya Peddinti",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "INTERSPEECH-2015"
    },
    {
      "citation_id": "20",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "21",
      "title": "Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi"
      ],
      "year": "2024",
      "venue": "2024 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "22",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "23",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "24",
      "title": "A layer-wise analysis of mandarin and english suprasegmentals in ssl speech models",
      "authors": [
        "Antón De",
        "La Fuente",
        "Dan Jurafsky"
      ],
      "year": "2024",
      "venue": "A layer-wise analysis of mandarin and english suprasegmentals in ssl speech models",
      "arxiv": "arXiv:2408.13678"
    },
    {
      "citation_id": "25",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    }
  ]
}