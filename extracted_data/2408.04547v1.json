{
  "paper_id": "2408.04547v1",
  "title": "Emotional Cues Extraction And Fusion For Multi-Modal Emotion Prediction And Recognition In Conversation",
  "published": "2024-08-08T15:55:35Z",
  "authors": [
    "Haoxiang Shi",
    "Ziqi Liang",
    "Jun Yu"
  ],
  "keywords": [
    "emotion prediction in conversation",
    "emotion recognition",
    "multi-modal fusion † Equal Contribution"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Prediction in Conversation (EPC) aims to forecast the emotions of forthcoming utterances by utilizing preceding dialogues. Previous EPC approaches relied on simple context modeling for emotion extraction, overlooking fine-grained emotion cues at the word level. Additionally, prior works failed to account for the intrinsic differences between modalities, resulting in redundant information. To overcome these limitations, we propose an emotional cues extraction and fusion network, which consists of two stages: a modality-specific learning stage that utilizes word-level labels and prosody learning to construct emotion embedding spaces for each modality, and a two-step fusion stage for integrating multi-modal features. Moreover, the emotion features extracted by our model are also applicable to the Emotion Recognition in Conversation (ERC) task. Experimental results validate the efficacy of the proposed method, demonstrating superior performance on both IEMO-CAP and MELD datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play an important role in human communication  [1, 2] . Proper understanding and application help us communicate better and establish connections. In human-machine dialogue, it is also crucial to understand the user's emotional changes. Emotion prediction can enable intelligent agents to achieve more friendly and considerate interactions, improving user experience and human-machine interaction efficiency.\n\nEmotion prediction in conversation (EPC) involves forecasting the speaker's future emotional state using preceding contextual cues  [3] . Previous research have focused on modeling historical context information in sessions to improve the accuracy of EPC task  [4, 5, 6] . Shi et al.  [5]  performed subsequent emotion prediction by correlating the contextual speaking information of bilateral speakers instead of a single speaker. In their subsequent work  [7] , Shi et al. further considered the importance of multi-turn dialogues and modeled them, improving EPC task accuracy. However, word-level associations often constitute the core of conversational dynamics. Thus, to facilitate comprehensive comprehension and subsequent predictions, modeling contextual word-level relationships is imperative.\n\nIn addition, regarding the utilization of multi-modal information, previous EPC approaches applied identical modeling techniques to both text and audio modalities without considering their distinct characteristics. The text modality typically encapsulates clear content and character information, whereas the audio modality includes not only the spoken content but also valuable speaker-specific and prosodic cues  [8] . Therefore, it is crucial to extract and enhance distinctive features within each modality prior to their fusion. Furthermore, during modal fusion, prior models  [9, 10]  often directly concatenated modalities, thereby overlooking the potential interactions and complementary information between them.\n\nTo address these challenges, we propose a multi-step fusion model based on enhanced intra-modal emotional information to predict the emotions of multi-party conversations. Specifically, the model consists of two stages. In the first stage, we perform intra-modal emotional cues perception and enhancement. For text, we introduce a knowledge-based word relation tagging module, which helps the model pay attention to word-level emotional features and improve emotional perception capabilities by constructing a word-level importance matrix within the conversation. As for speech, we design a prosody enhancement module aimed at detecting prosodic changes to better predict emotional trends in conversations. In the second stage, we employ a two-step fusion method to integrate multimodal features. Initially, we fuse the intra-modal features obtained in the first stage to generate the preliminary fusion representation. Subsequently, the representation fused again with the mel-spectrogram extracted from the audio waveform to incorporate spectral domain emotional features and generate the final fused emotional representation. Additionally, we conducted experiments on the Emotion Recognition in Conversation (ERC) task to validate the effectiveness and multi-task applicability of our model's emotional feature extraction and fusion. The main contribution of this work can be summarized as follows:\n\n• We propose a novel method to complete the EPC task, which perceives the intra-modal features through word-level relation tagging and prosody enhancement modules. • We introduce a novel multi-modal fusion method that leverages the spectral domain features of audio for two-step multimodal information fusion. • We conduct experiments on both EPC and ERC tasks, experimental results demonstrate that our model achieves leading performance in two tasks. speaker pair (un+1, sn+1) by given the historical dialogues {(u1, s1), (u2, s2), ..., (un, sn)}.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "Emotion Recognition in Conversation (ERC) In the same conversation D, differnt from EPC, ERC task requires identifying the emotion category labels E = {e1, e2, ..., eN } of the dialogue {(u1, s1), (u2, s2), ..., (uN , sN )}.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview",
      "text": "The architecture of our model, as depicted in Figure  1 , is primarily composed of three parts: the intra-modal emotion perception part, the two-step multi-modal fusion part, and the final emotion classification part. Specifically, in the intra-modal emotion perception part, we design a Knowledge-based Word Relation Tagging (KWRT) module and a Prosody Enhancement (PE) module to address the differences in information between modalities. In the two-step multi-modal fusion part, we first fuse the text and audio features enhanced in the first part, then combine the initial fusion features with the mel-spectrogram extracted from the audio to generate the final multi-modal fusion representation. Finally, we obtain the prediction and recognition results by designing classifiers for the EPC and ERC tasks in the emotion classification part.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Knowledge-Based Word Relation Tagging (Kwrt)",
      "text": "Given a conversation, we first transform it into a sequence based on utterances and speaker information, and input it into RoBERTa  [11]  to discern the dynamics and interdependencies among speakers within the conversation. Specifically, we distinguish different speakers through different special tokens, such as [s1] and [s2]. Then we get a conversation sequence x containing speakers and corresponding utterances. Now assume that the number of tokens in x is N , and then we use RoBERTa to encode the utterance sequence: ht = RoBERT a(x), where ht ∈ R dxN and d is the output dimension of RoBERTa.\n\nOur goal is to explore word-level relationships between contextual dialogues, inspired by  [12, 13] , we design a word relation tagging method to obtain the importance level of words, thereby enabling the model to extract emotional cues. Taking the example of the 3-turn dialogues, we first concatenate the three segments of dialogue to construct a word-level matrix, as shown in Figure  2 . We exclude function words, retaining only content words, as only the word-level relationships between content words contain emotional content information. We use two word-level tags to assess word importance comprehensively: word recurrence and word relations. For example, we use \"0/H\" to represent the relationship between the word pair \"asleep-kangaroo\". Here, '0' indicates the frequency of word recurrence. If two words are identical and located offdiagonally, they are labeled as '1', indicating that the words have appeared at least twice in a dialogue; otherwise, they are labeled as '0'. Meanwhile, 'H' represents the lexical relationship between them. As emotional cues within dialogues alone cannot provide knowledge beyond the context, we use the common-sense knowledge base ConceptNet  [14]  as an external source to tag relationships between word pairs. We select three main relations among 38 pairs, namely \"IsA,\" \"HasContext,\" and \"Causes,\" which may contain emotional cues, and we use 'I', 'H' and 'C' to represent them, respectively. It should be noted that word pairs containing function words and those on the diagonal are marked as \"0/N\". At this point, by assigning word-level labels, we obtain a word-level recurrence frequency matrix Mrec and a word-level relationship matrix M rel . Here, K denotes the total number of words in the dialogue. We then superimpose the two matrices to obtain the final word-level importance matrix M , as shown in Figure  1b , where {Mrec, M rel , M } ∈ R KxK .\n\nFinally, we combine the word-level importance matrix M with utterance-level featuresh:\n\nwhere Ft ∈ R dxK is the final utterance representation. The operation Squeeze(M ) compresses the dimensions of the M matrix into R 1xK since each row in M corresponds to the importance score vector of a word.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Prosody Enhancement (Pe)",
      "text": "In previous emotion prediction models  [15, 16] , features were often extracted from audio using the same processing methods as text and then fused with text features. However, these approaches did not account for the unique information inherent in each modality  [8] . In the text modality, we consider the semantic connections between content words. However, the expression of sentiments associated with function words, such as \"what\" or \"where\" in interrogative sentences, is often conveyed through the prosody in audio. Therefore, we design a prosody enhancement module to extract and amplify emotional cues from audio features. Specifically, we first extract preliminary audio features ha using a pre-trained audio model and a feature extraction module. We then use ha to extract emotional cues via the fine-tuned prosody encoder as described in  [17] . Similar to the text modality, we derive the final audio emotional feature representation by integrating the prosody information with the preliminary audio features:\n\nwhere Enc() stands for prosody encoder, LN () represents the Layer Normalization.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Two-Step Multi-Modal Fusion Layer (Tmf)",
      "text": "To integrate the bimodal features of text and audio, we adopt a two-step fusion strategy. Initially, we utilize a Transformer  [18]  to integrate multi-modal emotional features. The audio features are treated as the query matrix, while the text modal features are considered as the key-value matrix, enabling the propagation of information between modalities, resulting in preliminary complementary fusion features, denoted as Ft,a.\n\nBesides the pitch contour, the mel-spectrogram also carries prosodic information  [19] . Therefore, to further leverage the frequency domain information of audio, we design a multilayer multi-modal fusion module (MFM) utilizing the melspectrogram, as depicted in Figure  1a . We employ pre-trained large-scale language  [20]  and vision  [21]  Transformer models to process the fusion and spectral domain information, represented as T rans l and T ransv, respectively. Initially, according to  [22] , we initialize a trainable bridge vector, denoted as v br , to facilitate the multi-modal fusion process. For instance, taking the spectral domain features Fm as the initial input, we concatenate v br with Fm, and subsequently feed them into a unimodal transformer layer to derive the preliminary feature representation:\n\nwhere ⊕ stands for the concatenate operation. Then, the obtained bridge vector vbr with spectral domain information is mapped to the feature space of the Ft,a through an MLP layer, and concatenated with the Ft,a. The concatenated vectors are then fed into another Transformer layer to obtain the final fused features Fm→t,a:\n\nSymmetrically, using Ft,a as the initial input and applying the same operations, we can obtain the fused feature Ft,a→m:\n\nUpon completing the multi-modal fusion, we employ Fm→t,a and Ft,a→m as inputs for two distinct linear classifiers and then average the logits for final classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets And Baselines",
      "text": "We conduct experiments on two datasets: IEMOCAP  [23]  and MELD  [24] . IEMOCAP database is a widely used corpus in affective computing. It contains approximately 12 hours of audiovisual recordings and is designed for two-person dialogs. Each conversation in IEMOCAP has been segmented into utterances with the continuous label in the Valence-Arousal dimension and category label in categories such as anger, happiness, sadness, and neutrality. MELD is a popular dataset for tasks involving the analysis of emotions expressed by multiple speakers. It encompasses a collection of over 1400 dialogues and 13,000 speech instances extracted from the television show F riends. Emotion annotation in the dataset includes: neutral, happiness, surprise, sadness, anger, disgust, and f ear.\n\nTo showcase the efficacy of our proposed model across both unimodal and multi-modal scenarios, we selected the following baseline models for EPC task: • DEP(2020)  [5]  incorporated the content from multiple speakers to enhance contextual information modeling. • EAMT(2023)  [7]  further considered multi-turn conversations and speaker information. For ERC task, we selected several state-of-the-art models: • MM-DFN(2022)  [25]  designed a new graph-based dynamic fusion module to fuse multi-modal context features. • SCFA(2023)  [26]  proposed a novel speaker-aware crossmodal fusion architecture. • DF-ERC(2023)  [27]  developed a disentanglement mechanism to decouple and model multi-modal features and the contextual aspects of conversations.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation",
      "text": "In this study, we utilize pre-trained wavLM  [28]  and RoBERTa models to extract 768-dimensional features from audio and text, respectively. The feature extraction module comprises twolayer Transformers, each with 8 heads and embeddings of 1024 dimensions. The Transformer in fusion-stage employs the same configuration. To ensure consistent output dimensions across modalities, we map the output of the prosody enhancement module to 1024 dimensions. Furthermore, we fine-tune the prosody encoder according to the configuration detailed in  [17] .\n\nWithin the TMF, we set the length of the v br to 4 and stack two MFM blocks. The network is trained using the Adam optimizer, with a batch size of 32 and a learning rate of 0.0001.   showcase the prediction results on specific emotion categories, we selected four emotions and constructed a confusion matrix based on IEMOCAP, as shown in Figure  3a . The results predicted by our model are mostly concentrated on the diagonal, good performance in emotion prediction. Emotion Recognition To validate the model's performance on ERC tasks, we conducted experiments on the same datasets, using accuracy (Acc) and weighted F1 (W-F1) metrics, as shown in Table  2 . Our model achieves comparable performance across all three modal combinations. Specifically, on MELD, our model outperforms SCFA's Acc by 4.50% under the single text modality, although it is slightly lower than DF-ERC by 1.35%. In the audio modality, our model performs comparably to the best-performing SCFA, surpassing DF-ERC by 2.14%. In the multi-modal experiment, our model demonstrates performance comparable to MM-DFN and SCFA, indicating its strong emotion perception capabilities. Additionally, similar to the EPC task, we constructed a confusion matrix based on the IEMO-CAP, as shown in Figure  3b . Our model continued to exhibit accurate performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted ablation experiments on the EPC task by removing the KWRT, PE, and TMF modules to demonstrate the effectiveness of our proposed components, as indicated in Table  3 . For example, on the IEMOCAP dataset, we first removed the KWRT module and directly used RoBERTa and the feature extraction module to output text features. UAR and M-F1 dropped by 0.93% and 0.89%, respectively, showing that the word-level annotation information in KWRT improves the emotional perception of text modalities. Next, we removed the PE module, resulting in UAR and W-F1 dropping by 0.75% and 0.72%. This indicates that although audio features contain emotional information, removing PE weakens the model's ability to perceive prosody. Finally, we removed the two-stage multi-modal fusion module based on the mel-spectrogram and retained only the preliminary fusion in the first stage, reducing UAR and M-F1 by 0.88% and 0.84%, respectively. This demonstrates that the new fusion method based on the mel-spectrogram enhances the model's ability to learn prosody information in the spectrum domain and improves the prediction effect.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "This article presents a two-stage multi-modal conversational emotion prediction model based on word-level relationship tagging, capable of perceiving contextual word-level information. At the same time, the designed prosody enhancement module captures prosodic information from the audio modality. In order to fuse spectral domain information, we employ the melspectrogram to perform two-step fusion process. Experiments have demonstrated the effectiveness of each proposed module and the superiority of the overall performance.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The left side is the main framework of our model. (a) multi-modal fusion module, the green part represents the pre-trained",
      "page": 2
    },
    {
      "caption": "Figure 2: We exclude function words, re-",
      "page": 2
    },
    {
      "caption": "Figure 2: A tagging example with KWRT module.",
      "page": 2
    },
    {
      "caption": "Figure 1: b, where {Mrec, Mrel, M} ∈RKxK.",
      "page": 3
    },
    {
      "caption": "Figure 1: a. We employ pre-trained",
      "page": 3
    },
    {
      "caption": "Figure 3: Accuracy confusion matrix on different tasks.",
      "page": 4
    },
    {
      "caption": "Figure 3: a. The results pre-",
      "page": 4
    },
    {
      "caption": "Figure 3: b. Our model continued to exhibit",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0/N\n0/I": "0/N",
          "0/N": "0/H\n0/N",
          "…": "…\n…",
          "0/I": "1/N\n0/H"
        },
        {
          "0/N\n0/I": "",
          "0/N": "…",
          "…": "…",
          "0/I": "…"
        },
        {
          "0/N\n0/I": "",
          "0/N": "0/N",
          "…": "",
          "0/I": "0/H"
        },
        {
          "0/N\n0/I": "",
          "0/N": "",
          "…": "",
          "0/I": "0/N"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Comparison of performance characteristics on EPC",
      "data": [
        {
          "Dataset": "M\nMethod",
          "IEMOCAP": "UAR↑\nM-F1↑",
          "MELD": "UAR↑\nM-F1↑"
        },
        {
          "Dataset": "DEP[5]\nT\nEAMT[7]\nOurs",
          "IEMOCAP": "74.96\n74.54\n77.30\n76.67\n78.53*\n78.06*",
          "MELD": "42.19\n42.67\n45.44\n45.13\n46.97*\n46.23*"
        },
        {
          "Dataset": "DEP[5]\nEAMT[7]\nS\nOurs",
          "IEMOCAP": "61.98\n60.21\n65.01\n65.91\n66.21*\n66.32",
          "MELD": "26.96\n25.13\n28.67\n25.97\n29.12\n27.85*"
        },
        {
          "Dataset": "DEP[5]\nEAMT[7]\nT+S\nOurs",
          "IEMOCAP": "76.31\n75.50\n80.18\n80.01\n81.92*\n81.53*",
          "MELD": "42.39\n42.51\n45.21\n44.36\n46.10\n45.73*"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Comparison of performance characteristics on EPC",
      "data": [
        {
          "Dataset": "M\nMethod",
          "IEMOCAP": "Acc↑\nW-F1↑",
          "MELD": "Acc↑\nW-F1↑"
        },
        {
          "Dataset": "SCFA[26]\nT\nDF-ERC[27]\nOurs",
          "IEMOCAP": "63.82\n62.89\n65.13\n65.46\n64.83\n64.32",
          "MELD": "59.32\n57.76\n65.17\n64.54\n63.82\n63.01"
        },
        {
          "Dataset": "SCFA[26]\nS\nDF-ERC[27]\nOurs",
          "IEMOCAP": "49.24\n48.66\n41.47\n38.62\n47.84\n47.06",
          "MELD": "47.37\n44.18\n43.83\n41.72\n45.97\n42.94"
        },
        {
          "Dataset": "MM-DFN[25]\nT+S\nSCFA[26]\nOurs",
          "IEMOCAP": "-\n65.41\n67.91\n66.42\n66.62\n67.79",
          "MELD": "-\n58.34\n64.86\n63.69\n63.73\n64.36"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Comparison of performance characteristics on EPC",
      "data": [
        {
          "Dataset": "",
          "IEMOCAP": "UAR↑\nM-F1↑",
          "MELD": "UAR↑\nM-F1↑"
        },
        {
          "Dataset": "Ours\nw/o KWRT\nw/o PE\nw/o TMF",
          "IEMOCAP": "/\n/\n-0.93\n-0.89\n-0.75\n-0.72\n-0.88\n-0.84",
          "MELD": "/\n/\n-1.12\n-1.10\n-0.71\n-0.69\n-0.83\n-0.81"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Comparison of performance characteristics on EPC",
      "data": [
        {
          "0.85": "0.01",
          "0.03": "0.78",
          "0.05": "0.02",
          "0.04": "0.04"
        },
        {
          "0.85": "0.02",
          "0.03": "0.02",
          "0.05": "0.83",
          "0.04": "0.09"
        },
        {
          "0.85": "0.03",
          "0.03": "0.04",
          "0.05": "0.08",
          "0.04": "0.80"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: Comparison of performance characteristics on EPC",
      "data": [
        {
          "0.69": "0.02",
          "0.03": "0.65",
          "0.06": "0.02",
          "0.08": "0.15"
        },
        {
          "0.69": "0.04",
          "0.03": "0.04",
          "0.06": "0.68",
          "0.08": "0.13"
        },
        {
          "0.69": "0.06",
          "0.03": "0.12",
          "0.06": "0.21",
          "0.08": "0.66"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Mimicking the thinking process for emotion recognition in conversation with prompts and paraphrasing",
      "authors": [
        "T Zhang",
        "Z Chen",
        "M Zhong",
        "T Qian"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Cauain: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Audio-visual emotion forecasting: Characterizing and predicting future emotion using deep learning",
      "authors": [
        "S Shahriar",
        "Y Kim"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "5",
      "title": "Speech-based emotion recognition and next reaction prediction",
      "authors": [
        "F Noroozi",
        "N Akrami",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "25th Signal Processing and Communications Applications Conference"
    },
    {
      "citation_id": "6",
      "title": "Dimensional emotion prediction based on interactive context in conversation",
      "authors": [
        "X Shi",
        "S Li",
        "J Dang"
      ],
      "year": "2020",
      "venue": "21st Annual Conference of the International Speech Communication Association, Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Saep: A surroundingaware individual emotion prediction model combined with t-lstm and memory attention mechanism",
      "authors": [
        "Y Wang",
        "Y Du",
        "J Hu",
        "X Li",
        "X Chen"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "8",
      "title": "Emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
      "authors": [
        "X Shi",
        "S Li",
        "J Dang"
      ],
      "venue": "24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "9",
      "title": "Aia-net: Adaptive interactive attention network for text-audio emotion recognition",
      "authors": [
        "T Zhang",
        "S Li",
        "B Chen",
        "H Yuan",
        "C Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "10",
      "title": "MMER: Multimodal Multi-task Learning for Speech Emotion Recognition",
      "authors": [
        "S Ghosh",
        "U Tyagi",
        "S Ramaneswaran",
        "H Srivastava",
        "D Manocha"
      ],
      "venue": "24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "11",
      "title": "Leveraging Label Information for Multimodal Emotion Recognition",
      "authors": [
        "P Wang",
        "S Zeng",
        "J Chen",
        "L Fan",
        "M Chen",
        "Y Wu",
        "X He"
      ],
      "venue": "24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "12",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "13",
      "title": "Grid tagging scheme for aspect-oriented fine-grained opinion extraction",
      "authors": [
        "Z Wu",
        "C Ying",
        "F Zhao",
        "Z Fan",
        "X Dai",
        "R Xia"
      ],
      "year": "2020",
      "venue": "Grid tagging scheme for aspect-oriented fine-grained opinion extraction",
      "arxiv": "arXiv:2010.04640"
    },
    {
      "citation_id": "14",
      "title": "Joint multimodal entityrelation extraction based on edge-enhanced graph alignment network and word-pair relation tagging",
      "authors": [
        "L Yuan",
        "Y Cai",
        "J Wang",
        "Q Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI"
    },
    {
      "citation_id": "15",
      "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "authors": [
        "R Speer",
        "J Chin",
        "C Havasi"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI"
    },
    {
      "citation_id": "16",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Dual Memory Fusion for Multimodal Speech Emotion Recognition",
      "authors": [
        "D Prisayad",
        "T Fernando",
        "S Sridharan",
        "S Denman",
        "C Fookes"
      ],
      "venue": "24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "18",
      "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
      "authors": [
        "S Yang",
        "M Tantrawenith",
        "H Zhuang",
        "Z Wu",
        "A Sun",
        "J Wang",
        "N Cheng",
        "H Tang",
        "X Zhao",
        "J Wang",
        "H Meng"
      ],
      "venue": "23rd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Focus-attention-enhanced Crossmodal Transformer with Metric Learning for Multimodal Speech Emotion Recognition",
      "authors": [
        "K Kim",
        "N Cho"
      ],
      "venue": "24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "21",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "22",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale,\" in 9th International Conference on Learning Representations, ICLR",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale,\" in 9th International Conference on Learning Representations, ICLR"
    },
    {
      "citation_id": "23",
      "title": "Efficient multimodal fusion via interactive prompting",
      "authors": [
        "Y Li",
        "R Quan",
        "L Zhu",
        "Y Yang"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "25",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "26",
      "title": "MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Speaker-aware Cross-modal Fusion Architecture for Conversational Emotion Recognition",
      "authors": [
        "H Zhao",
        "B Li",
        "Z Zhang"
      ],
      "venue": "24th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "28",
      "title": "Revisiting disentanglement and fusion on modality and context in conversational multimodal emotion recognition",
      "authors": [
        "B Li",
        "H Fei",
        "L Liao",
        "Y Zhao",
        "C Teng",
        "T Chua",
        "D Ji",
        "F Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    }
  ]
}