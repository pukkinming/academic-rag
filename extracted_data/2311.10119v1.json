{
  "paper_id": "2311.10119v1",
  "title": "Accommodating Missing Modalities In Time-Continuous Multimodal Emotion Recognition Author Version",
  "published": "2023-11-16T09:22:48Z",
  "authors": [
    "Juan Vazquez-Rodriguez",
    "Grégoire Lefebvre",
    "Julien Cumin",
    "James L. Crowley"
  ],
  "keywords": [
    "Affective Computing",
    "Multimodal Emotion Recognition",
    "Machine Learning",
    "Transformers"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Decades of research indicate that emotion recognition is more effective when drawing information from multiple modalities. But what if some modalities are sometimes missing? To address this problem, we propose a novel Transformer-based architecture for recognizing valence and arousal in a timecontinuous manner even with missing input modalities. We use a coupling of cross-attention and self-attention mechanisms to emphasize relationships between modalities during time and enhance the learning process on weak salient inputs. Experimental results on the Ulm-TSST dataset show that our model exhibits an improvement of the concordance correlation coefficient evaluation of 37% when predicting arousal values and 30% when predicting valence values, compared to a latefusion baseline approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Technologies for automatic emotion recognition have been shown valuable for interpersonal communications  [1] , health and wellness concerns  [1]  and stress management  [2] , for example. People express emotions in both verbal and nonverbal manners. Facial expressions, pitch intensity or cardiac rhythms are examples of non-verbal communication.\n\nUsing multiple modalities for emotion recognition is advantageous since modalities may be complementary, and should thus improve the performance of the model when used together  [3] . However, in real-world scenarios, there might be cases where a modality might not be available. If for example, the modalities consist of video, audio, and physiological signals, the camera field of view might be obstructed, the microphone can be too far away, or the physiological sensor might be on a wearable device that is not currently worn. Therefore, we need a model capable of handling missing modalities.\n\nIn this work, we extend a multimodal Transformer  [4]  as an encoder to obtain representations from the different modalities and a Transformer decoder  [5]  to process those representations and make predictions. A Transformer-based approach will continue to work in the case of missing modalities, although the performance often decreases  [6] . We investigated a learning strategy to improve performance by eliminating the most important modalities during part of the training, so that the model is forced to learn from the less informative ones, that nevertheless may carry valuable features. This has two desirable effects. First, the model improves its performance, by training the model to draw information from all modalities rather than focusing on the most important ones. Second, the model becomes less sensitive to missing modalities, as it learns to handle the case where a modality is not present.\n\nA critical aspect of multimodal emotion recognition is modeling the complementarity of information from different modalities. In other words, the model should be capable of weighing the different modalities according to their importance. We then present a novel approach of using the encoder-decoder attention (cross-attention) of the Transformer decoder to weigh the representations generated by the encoder, making this weighting scheme focus on choosing between modalities rather than paying attention to information from different time-steps. In addition, our Transformer decoder is auto-regressive, meaning that it takes into account past predicted values when doing the current inference, which is important when performing timecontinuous predictions.\n\nThe main contributions of this research are: 1. we extend a multimodal Transformer-based architecture to perform timecontinuous value-continuous multimodal emotion recognition, 2. we present a novel approach using cross-attention from the Transformer decoder to weigh the importance of different modalities, 3. and we develop a learning strategy to improve the performance of the model when a modality is missing.\n\n2 Related Work",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Time-Continous Multimodal Emotion Recognition",
      "text": "Several works address the problem of time-continuous multimodal emotion recognition. Traditionally, Long Short Term Memory -Recurrent Neural Networks (LSTM-RNN) have been employed to model the temporal relations of the inputs and to consider past predictions when predicting the current time-step  [7] . Recently, employing Transformer-based approaches  [8] [9] [10]  has gained popularity in addressing this task. Some works rely on LSTM-RNNs to complement the attention mechanisms from the Transformer to model the temporal information better  [8] , while other works use a pure attentionbased approach  [9, 10] .\n\nTo fuse information from different modalities, some authors use late-fusion combining the outputs of single modality models  [11] , while others employ early-fusion by combining the input features before feeding them into the model  [12] . Some approaches that use Transformer-based techniques have presented more elaborate solutions to aggregate multimodal information. Cross-modal attention  [13]  can be used to incorporate information from different modalities  [8, 12] . Different from this, Zhang et al.  [9]  group the query vectors from each modality to form a single query vector and do the same for the key and value vectors. Then, they employ the grouped vectors to perform a modified version of the scaled dot-product attention described in the original Transformer paper  [5] . Chen et al.  [10]  and He et al.  [14]  model temporal information using a standard Transformer approach and they model intermodal information through a multimodal attention mechanism.\n\nA disadvantage of these approaches is that, at some point, the features coming from the different modalities are concatenated. This requires that all the modalities need to be present, thus breaking the approach if a modality is missing. Some authors have worked on addressing this situation, and we review some of these works below.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Handling Missing Modalities",
      "text": "There are three main types of approaches to handle missing modalities  [15] : 1. learning a joint representation from the different modalities, so only one modality could be used at test time, 2. generating the missing modalities from the available ones, and 3. hiding some modalities during training.\n\nFor the first type, an example is the work of Pham et al.  [16] , where a joint representation is learned by encoding the text into a representation (the joint representation) and generating the other modalities from this representation. At test time, only the text input is needed. For the second type, we have the work of Mittal et al.  [17] , where the model generates replacement features using a learned linear transformation that converts features from the available modalities into features of the missing one. For the third type, an example is the work of Neverova et al.  [18] , where a carefully designed network is designed so it can still work even with missing modalities. Then, at train time, some modalities are dropped randomly to make the model robust to missing modalities.\n\nAlthough these approaches make the model robust to missing modalities, a disadvantage of the first type of approach is that it cannot take advantage of using all modalities if they are present at test time. For the second type of approach, a drawback is that there is no guarantee that the generated representation accurately resembles the missing one. And to implement the third type of approach, the architecture should be capable of working with missing modalities. On the contrary, if a Transformer-based approach is used, there is no need to generate the missing modality representations, or do modifications to the architecture so it can work with modalities absent. In this case, the attention mechanisms simply do not attend to the missing modalities, and it is capable of attending to all of them if they are present.\n\nFor the reasons stated in the previous paragraph, many works that use the third type of approach to handle missing modalities use a Transformer-based model. Some examples include the work of Goncalves and Busso  [19] , and the work of Parthasarathy and Sundaram  [20] , where they use a crossmodality Transformer to combine audio and visual modalities, improving the robustness of the model to missing modalities by eliminating a modality during training. A disadvantage of using a cross-modality Transformer is that expanding the approach to use more modalities is not straightforward. To overcome this problem, a Multimodal Transformer  [4]  can be employed, like in the work of Ma et al.  [6] , where robustness to missing modalities is increased using a multitasking approach.\n\nThe Transformer-based models are well suited to model long and short temporal relations of the inputs and to model the cross-modality dependencies. Nevertheless, in the reviewed Transformer-based approaches, the attention layers have to model the temporal and the intermodal dependencies at the same time. We argue that it can be advantageous to attend only to the cross-modal dependencies when aggregating the multimodal information. In addition, the reviewed approaches do not explicitly consider past predictions when making the current prediction, which we believe is beneficial.\n\nOur approach is a Transformer-based approach that uses a Multimodal Transformer as encoder, making it suitable for any number of modalities. We also use the novel idea of using the cross-attention from a Transformer decoder  [5]  to weigh the information from the different modalities. The decoder uses only the information of each modality at the current prediction time-step, relieving it from modeling the temporal information, which is done by the encoder. In addition, we explicitly use past predictions to make the current one by employing an auto-regressive approach. To handle missing modalities, we use the approach of hiding some modalities at train time, but different from the state of the art, we employ a technique to find and then hide the important modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Approach",
      "text": "In this section, we provide a detailed explanation of our approach to perform multimodal time-continuous valuecontinuous emotion recognition. Our objective is to predict values of arousal and valence. We start this section by explaining our encoder that generates multimodal representations. Then we explain our decoder that predicts the values of arousal and valence from those representations. Finally, we describe the loss that we use to train our model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transformer Encoder",
      "text": "e M e M e M ...",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality Encodings",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Input Features",
      "text": "Figure  1 : MultiModal Transformer Encoder (MMTE).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multimodal Tranformer Encoder (Mmte)",
      "text": "We depict our MultiModal Transformer Encoder (MMTE) in Figure  1 . Our MMTE is based on the work by Gabeur et al.  [4] .\n\nThe inputs for our encoder are features extracted from raw data from the different modalities. We discuss the features we use in Section 6.\n\nThe first step in our MMTE architecture is to process each modality individually using a Temporal Convolutional Network (TCN)  [21]  to model local temporal information, similarly to  [10] . Our model learns a different TCN for each modality. We define x m t ∈ R dmodality as the feature corresponding to modality m at time-step t. If we denote [x m 1 , . . . , x m T ] as the sequence with length T of features corresponding to modality m, then during this step we have:\n\nwhere a m t ∈ R dmodel . For all modalities, the TCN output will have a common size d model .\n\nThe next step is to add positional encodings that allow the Transformer to take into account the actual order of the sequence  [5] . If the sequence of positional encodings is P = [p 1 , . . . , p T ], with p t ∈ R dmodel , then the output of this step is\n\nThe elements of P are parameters that are learned during the training of the whole architecture.\n\nThe Transformer also needs to differentiate each modality to process cross-modality information. To do this, we follow the original multimodal Transformer from Gabeur et al., and add modality encodings. Similar to positional encodings, these modality encodings are learned during training. For each modality m, an encoding e m ∈ R dmodel is added to the input. The output of this step is then\n\nWe then concatenate the sequences from all modalities to have a single sequence. If we define the input corresponding to Encoder Outputs",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotion Predictions",
      "text": "Emotion Regression Network (ERN) Fully-Connected (FC)",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Transformer Decoder",
      "text": "Layer (TDL)\n\nFully-Connected (FC)\n\nMulti-Head Attention (MHA)\n\nMulti-Head Attention (MHA)",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Input Features",
      "text": "Figure  2 : Auto-regressive MultiModal Transformer Decoder (AMMTD). Decoder when predicting the emotion value at time t. N x means that there are N stacked TDL layers. modality m at time-step t as f m t = a m t + p t , +e m , and if we have M modalities, the concatenated input sequence is then\n\nWe process this sequence using a Transformer encoder. The output representations r m t of the Transformer are given by [r 1  1 , . . . , r 1 T , . . . , r M 1 , . . . , r\n\nFollowing  [10] , we employ a bidirectional attention mask in the Transformer encoder. When processing a specific time-step, this mask hides the inputs that are farther than mask_length positions in the future and in the past. This allows the model to concentrate on recent information, and not to worry about information too far in time that probably does not influence the current emotional state. Note that we are not hiding complete modalities, therefore this technique is not intended to make the model robust to missing modalities.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Auto-Regressive Multimodal Transformer Decoder (Ammtd)",
      "text": "One of the contributions of this paper is to develop a decoder that predicts emotion from the multimodal representations given by the encoder. To do this, we design an Autoregressive MultiModal Transformer Decoder (AMMTD). This decoder has two important characteristics: first, it takes previous predictions into account to determine the current emotion; second, it aggregates the representations of the different modalities, giving more weight to the more important ones.\n\nA Transformer Decoder Layer (TDL)  [5]  is composed of a Multi-Head Self-Attention module (MHSA), followed by a Multi-Head Cross-Attention module (MHCA), and followed by a fully-connected Feed-Forward Network (FFN). Residual connections are used around each of these three components. The Multi-Head Attention (MHA) mechanism in MHSA and MHCA projects a query vector q from a given position to a key vector k from another position to determine the attention (i.e. the weight) given to a value v associated with the position of k. The final value is the weighted sum of the v vectors from the different positions. We denote the MHA mechanism as\n\nwhere the three parameters Q, K, and V indicate the sequence used as query, key, and value, respectively. More details about MHA can be found in the original Transformer paper  [5] .\n\nwhere d 0 ∈ R dmodel is a randomly initialized vector.\n\nAs for our encoder, we learn positional encodings p ′ t ∈ R dmodel for our decoder and add them to the inputs before feeding them to the TDL stack. Thus, when performing the prediction at time-step t, the input sequence I t = [i 0 , i 1 , . . . , i t-1 ] with I t ∈ R t×dmodel becomes\n\nInside the TDL, the features are first processed by the MHSA module. This module uses self-attention to integrate information from its own inputs. This means that the query, key, and value for the MHSA all come from the input sequence.\n\nTo preserve the auto-regressive property, we make sure that a given input at a certain time-step can only attend inputs from past time-steps. Using Expression 6, the sequence of features\n\n] with H t ∈ R t×dmodel at the output of the MHSA module is\n\nThe sequence of features H t is then processed with the MHCA module, which is used to incorporate information from the input modalities. Specifically, the MHCA module attends to the outputs of the encoder. This means that the query comes from the output sequence of the MHSA, and the key and value come from the output of the encoder. If we are predicting the emotion value at time-step t, the MHCA attends only to the outputs of each modality corresponding to this time step. The output sequence of the MHCA, H ′ t ∈ R t×dmodel , using the output of the encoder from Equation  5 and the output of the MHSA from Equation  9 , is\n\nNote that we force the model to only attend to the encoder outputs at time t instead of attending to all encoder outputs (or other outputs around t) because we want that the MHCA focuses only on finding the best weighting between the different modalities. We want to avoid the MHCA having to weigh which other time-steps in the different modalities might be important. Moreover, this restricts the information flow between modalities, which has been demonstrated to be beneficial  [22] , because it forces the shared representation to condense the most significant information.\n\nThe final step in the TDL is processing each feature of the sequence H ′ t through a fully connected feed-forward network (FFN), applied independently to each position:\n\nIf the TDL stack has more than one layer, the sequence from Equation  11 becomes the input of the next layer. Concretely, the new layer implements Equations 9, 10, and 11 using as input I t = H ′′ t . For the last TLD layer, the sequence in Equation  11 is the newly generated sequence [d 1 , . . . , d t ] that will be used as input for the decoder to predict the emotion value for the next time-step, thus if\n\nOnce the complete output sequence D = [d 1 , . . . , d T ] has been generated, the final step is to process it with the Emotion Regression Network (ERN). As shown in Figure  2 , our ERN is a Fully Connected (FC) layer that independently processes each element of the sequence D to predict the emotion values for each time-step, producing the predicted sequence [ŷ 1 , . . . , ŷT ].",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Loss Function",
      "text": "As suggested in previous works that address the problem of recognizing emotion in a time-continuous manner  [7, 11, 14] , we use the concordance correlation coefficient (CCC)  [23]  as the loss to train our model. Specifically, the loss is\n\nwhere ρ is the Pearson correlation coefficient between the predicted values ŷ and the ground-truth values y. σ denotes the standard deviation and µ denotes the average of either the predicted or the ground-truth values.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Handling Missing Modalities",
      "text": "We now describe another contribution of our paper, which is our approach to deal with missing modalities. In our case, a missing modality means that the modality is completely absent, thus the input sequence defined in Expression 4 is built with the remaining modalities. By construction, our model will not break in the case a modality is missing. The attention mechanisms in our Transformer-based approach can accommodate missing modalities as explained below. In the case of the MMTE, if a modality is not present, the Tranformer encoder will simply attend to the remaining modalities. Similarly, in the AMMTD, the cross-modal attention will be able to attend to the remaining representations without the need to generate a replacement for the missing ones.\n\nEven if our approach is capable to continue working in the case of missing modalities, its performance may be degraded.\n\nTo increase the robustness of the model to missing modalities, we perform an optimized training that we describe below.\n\nFirst, we identify the most important modalities. To do this, we first train our model in a standard way, then test it without one modality at a time. We can then identify in which cases the performance is reduced more, meaning that the missing modalities in those cases should be important. Next, we retrain the model, without the important modalities a part of the time. Specifically, for each batch, we randomly select to eliminate the important modality i with probability ρ i eliminate , and to keep all modalities with probability ρ none = 1-n i=1 ρ i eliminate , where n is the number of important modalities.\n\nOur reasoning behind this training strategy is that by hiding the important modalities, the model is forced to learn from the remaining ones, thus making the model more robust when those important modalities are missing. Moreover, we believe that this training strategy should lead to better results in general, even without missing modalities, as more information will be incorporated from all the modalities, rather than just relying on the important ones.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We test our model on the task of recognizing time-continuous values of arousal and valence. In this section, we describe the dataset, features, and parameters of our model for these experiments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "To evaluate our model, we use the Ulm-Trier Social Stress Test dataset (Ulm-TSST), which was presented for the Muse 2021 Challenge  [24, 25] . This is a multimodal dataset, where participants were recorded in a stressful situation emulating a job interview, following the TSST protocol  [26] . Each participant gave a 5 minutes speech supervised by two interviewers, who did not intervene during that time. Besides audio and video, the following physiological signals are collected: Electrocardiogram (ECG), Respiration (RESP), and heart rate (BPM). A transcription of the speech is also provided.\n\nThe data set was annotated by 3 raters giving continuous values of arousal and valence in the range [-1, 1]. The annotations are done in a time-continuous fashion every 0.5s. To aggregate the valence annotations from the 3 raters, Rater Aligned Annotation Weighting (RAAW)  [27]  is used. For arousal, the annotations corresponding to the lowest inter-rater agreement are discarded, and replaced by the subject's Electrodermal Activity (EDA) signal recorded during the session. The authors of the dataset do this because EDA has been demonstrated to be a good indicator of arousal  [28] . Like it was done for valence, RAAW is used to aggregate the annotations from the two remaining raters and the EDA signal.\n\nThe dataset includes 69 samples, each being a 5-minute presentation given by a subject. In the original dataset, 41 samples are used as train set, 14 as validation set, and 14 as test set. Since annotations are not provided for the test set, we randomly pick 4 samples from the validation set and 6 from the train set to form a new test set with 10 samples. In summary, we have 35 samples in the train set, 10 in the validation set, and 10 in the test set.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Input Features",
      "text": "We use audio, video, and physiological signals as input modalities, with features directly provided in Ulm-TSST. All features are aligned with annotations; that is, they are sampled at a rate of 2 Hz. For audio, we use extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) features. For video, we use Facial Action Units (FAU) intensity. For physiological signals, the features are the concatenation of the values of ECG, RESP, and BPM. We selected these features by running some experiments in the baseline model provided by the authors of the Ulm-TSST dataset and selecting the features that lead to good performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Hyperparameters And Training",
      "text": "We optimize the hyperparameters of our model using the Ray Tune Framework  [29]   During training, we segment each 5-minute sample into smaller samples, as suggested by Christ et al.  [7] . Searching across different options, we found that segments of 125 seconds (250 time-steps) with a hop size of 25 seconds (50 time-steps) work well in our experimental protocol.\n\nWe train our model with a batch size of 64 for a maximum of 100 epochs. We start with a learning rate of 0.0001, and halve it if the metric does not improve for 5 epochs on the validation set, and we early-stop the training if there is not improvement for 15 epochs. We use Adam optimizer with B 1 = 0.9 and B 2 = 0.999. We use a dropout rate of 0.2 throughout all the model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "This section presents and discusses the experimental results. For each experiment, we obtain 30 results by training the model with 30 different initialization seeds, reporting the average of those results. We use the Holm-Bonferroni method to assert statistically significant difference in the comparisons in Section 6.1. For other results, we do a t-test using a threshold of p-value < 0.05 to assert statistical significance, using a one-sided t-test to state that a result is significantly better than another, and a two-sided t-test to check for statistical difference. We use as metric the Root-Mean-Square Error (RMSE) and the Concordance Correlation Coefficient (CCC)  [23]  between ground truths and predicted values.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance With All Modalities Present",
      "text": "We present in Table  1  the performance of our model, along with baseline approaches, when all modalities are present. Approach N o 1 corresponds to the baseline model developed for the Muse 2022 Challenge  [7] , where the Ulm-TSST dataset was presented. We use the provided code 1 and the original hyperparameters to evaluate this model with the same features we employ, using our partition of the Ulm-TSST dataset. This approach is based on Long Short-Term Memory (LSTM) networks and uses late-fusion to aggregate the different modalities. Approach N o 2 corresponds to a model where instead of using our AMMTD to process the representations from the MMTE, it uses directly the ERN. Recall that the ERN is a FC network that performs regression of the emotion values. Similarly, approach N o 3 uses an LSTM to process the outputs of the MMTE. The LSTM has 4 layers, with a hidden dimension of 32. We used a grid search to tune this LSTM. For both approaches N o 2 and 3, the input is the concatenation of all modalities per time-step. The last two entries in Table  1  correspond to the approach presented in this paper. Approach N o 4 is our model trained in a standard way, i.e. with all modalities present during training. Approach N o 5 is our model trained with our optimized training strategy as presented in Sections 4 and 6.2, i.e. hiding some modalities during training.\n\nIn Figure  3 , we present an example of the predictions of our model when using the optimized training strategy. For the same sample, we present the results when predicting arousal, Fig.  3(a) , and valence, Fig.  3(b) . As observed, the real valence values tend to be flat and have less variability than the arousal values, which we noted is a common occurrence in the dataset.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison To The Lstm-Based Baseline Model",
      "text": "If we compare our approach with standard training (approach N o 4 in Table  1 ) with the LSTM baseline (approach N o 1), we can see that in all metrics except for valence RMSE, our model performs better than the LSTM baseline, demonstrating that in general, our Tranformer-based approach is well suited for this task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With Other Predictors",
      "text": "To test our idea of using cross-attention to weigh the input modalities and an auto-regressive approach to incorporate past predictions, we compare our approach with standard training (approach N o 4), with approaches N o 2 and N o 3 in Table  1 , that use the ERN and an LSTM respectively instead of our AMMTD. The results show that our AMMTD module increases the performance in most of the metrics, demonstrating the effectiveness of our ideas of using cross-attention and auto-regression. The performance of our approach is statistically significantly better for all the metrics except for valence RSME, where although our approach outperforms both baselines, the improvement is not statistically significant.\n\nIn general, the baseline models perform well in terms of the RMSE metric when predicting valence, but our model performs better in terms of the CCC metric. We hypothesize that this behavior is produced because the simpler architecture of the baselines is good enough to predict flat sequences of valence values that are close enough to the flat ground-truth. On the other hand, those approaches fail to predict the small changes in the valence values, penalizing the CCC score.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With Our Optimized Training Strategy",
      "text": "The results presented in entries N o 4 and N o 5 in Table  1  show that our optimized training approach, designed to improve the handling of missing modalities, also has the desirable effect of increasing the performance of the model when all modalities are present. For example, arousal RMSE decreases from 0.2948 to 0.2869 and valence CCC increases from 0.1502 to 0.1656. As we expected, the model seems to learn to use more information from the weak modalities, improving the overall performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Accommodating Missing Modalities",
      "text": "We present in Table  2  the results of experiments we conducted when a modality is missing.   2 , we see that there is no significant performance degradation when the audio or physiological modalities are missing. For example, when audio is missing, RMSE goes from 0.2948 to 0.2926, and CCC goes from 0.3578 to 0.3589. These differences are not statistically significant, as indicated by the checkmark ( ). We also see that performance drops significantly when the video modality is missing, with RMSE increasing from 0.2948 to 0.3252 and CCC decreasing from 0.3578 to 0.2713. These results confirm that our model continues to accurately predict arousal when a modality is missing, although performance is reduced in some cases.\n\nFor valence, we see that there is no significant performance degradation when physiological signals are missing, with RMSE going from 0.1796 to 0.1808 and CCC going from 0.1502 to 0.1486. On the contrary, the model performance drops significantly when the audio or video modalities are missing. For example, RMSE increases from 0.1796 to 0.2533 when the audio modality is missing and increases to 0.2170 when the video modality is missing. Much like for arousal, these results show that our model continues to estimate valence when a modality is missing, albeit with a drop in performance.\n\nWith these results, we can identify the most important modalities for our model, by identifying the modalities that when missing, induce a significant drop in performance. When predicting arousal, the most important modality is video, and for valence the most important modalities are audio and video.\n\nModels tend to rely heavily on the important modalities, even though other modalities may carry useful features for recog-nizing emotions. To improve this, we apply our optimized training strategy of hiding important modalities during parts of the training, forcing the model to rely on other modalities. Specifically, we use the strategy described in Section 4, eliminating the video modality with probability ρ video eliminate = 0.25, and maintaining all modalities with probability ρ none = 0.75 when training for arousal prediction. For valence, we use ρ audio eliminate = 0.333, ρ video eliminate = 0.333 and ρ none = 0.334. These probabilities were found empirically by testing several configurations and keeping the best ones for the validation set.\n\nWe see in Table  2  that our optimized training strategy improves all results when any modality is missing. For example, when the physiological signals are missing, CCC improves from 0.3539 to 0.3571 when predicting arousal and from 0.1486 to 0.1637 when predicting valence. Notably, the improvement is statistically significant, as indicated by the double dagger ( ‡), in all cases when the important modalities are missing, except for RMSE when predicting arousal with the video modality missing. In addition, the performance of the model improves even when all modalities are present. For instance, RMSE decreases from 0.2948 to 2869 for arousal and from 0.1796 to 0.1739 for valence. This shows that our optimized training strategy works well, making our model less reliant on the important modalities and using more information from the other ones. This strategy improves the performance of our approach not only when a modality is missing, but also when all modalities are present.\n\nIn this work, we presented a novel Transformer-based architecture with the coupling of self-and cross-attention mechanisms for emotion recognition from multimodal signals. We experimentally showed, using the Ulm-TSST dataset, that our proposal can competitively recognize emotional valence and arousal. In addition, we demonstrated that our optimized learning strategy improves performance. Consequently, our architecture is capable of reaching high performances for emotion recognition even with missing modalities.\n\nFuture works include investigating new ways to pre-train multimodal models with contrastive learning strategies. Metric learning between different signals, sharing the same semantic meaning, could positively influence the whole system to build strong correlations between input sequences produced at different moments in time. Moreover, learning similarities between signals even when some modalities are missing may help to better understand how pretraining on Transformerbased models behave for multimodal emotion recognition.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "We consider two main issues: privacy and harmful applications. First, regarding privacy, knowing the emotional state of a person leads to privacy concerns since it is evident that the emotional state is a private matter. Therefore, privacy should be guaranteed. There might be cases that people involved might agree to share this private information, as in the case of people participating in the dataset that we use. That is why we adhere to the dataset usage agreement. In the general case, to avoid privacy concerns, we think the emotional record of a person should be treated at the same level of privacy that medical records are treated.\n\nThe second issue has to do with potentially harmful applications. Although in this work we do not develop an application that uses the emotional state of a person as input, we are aware that bad actors may use emotion recognition techniques in unethical ways. For example, knowing the emotional state of a person can be used to manipulate their behavior. Direct control of this is out of our hands, and therefore, we think the affective computing community should pressure governmental entities for laws and ways to control these types of applications.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: MultiModal Transformer Encoder (MMTE).",
      "page": 3
    },
    {
      "caption": "Figure 1: Our MMTE is based on the work by Gabeur et al. [4].",
      "page": 3
    },
    {
      "caption": "Figure 2: Auto-regressive MultiModal Transformer Decoder",
      "page": 3
    },
    {
      "caption": "Figure 2: It is com-",
      "page": 4
    },
    {
      "caption": "Figure 3: , we present an example of the predictions of our",
      "page": 6
    },
    {
      "caption": "Figure 3: (a), and valence, Fig. 3(b). As observed, the real valence",
      "page": 6
    },
    {
      "caption": "Figure 3: Example of an output of our model with opti-",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 Orange Innovation, Grenoble, France": "2 Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France"
        },
        {
          "1 Orange Innovation, Grenoble, France": "ABSTRACT"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "Decades of research indicate that emotion recognition is more"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "effective when drawing information from multiple modalities."
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "But what\nif some modalities are sometimes missing? To ad-"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "dress\nthis problem, we propose a novel Transformer-based"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "architecture for\nrecognizing valence and arousal\nin a time-"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "continuous manner even with missing input modalities. We"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "use\na\ncoupling of cross-attention and self-attention mecha-"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "nisms to emphasize relationships between modalities during"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "time and enhance the learning process on weak salient inputs."
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "Experimental results on the Ulm-TSST dataset show that our"
        },
        {
          "1 Orange Innovation, Grenoble, France": "model exhibits an improvement of the concordance correlation"
        },
        {
          "1 Orange Innovation, Grenoble, France": "coefﬁcient evaluation of 37% when predicting arousal values"
        },
        {
          "1 Orange Innovation, Grenoble, France": "and 30% when predicting valence values, compared to a late-"
        },
        {
          "1 Orange Innovation, Grenoble, France": "fusion baseline approach."
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "Keywords Affective Computing, Multimodal Emotion"
        },
        {
          "1 Orange Innovation, Grenoble, France": "Recognition, Machine Learning, Transformers."
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "1\nIntroduction"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "Technologies\nfor\nautomatic\nemotion recognition have been"
        },
        {
          "1 Orange Innovation, Grenoble, France": "shown valuable for interpersonal communications [1], health"
        },
        {
          "1 Orange Innovation, Grenoble, France": "and wellness\nconcerns\n[1]\nand stress management\n[2],\nfor"
        },
        {
          "1 Orange Innovation, Grenoble, France": "example.\nPeople express emotions in both verbal and non-"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "verbal manners. Facial expressions, pitch intensity or cardiac"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "rhythms are examples of non-verbal communication."
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "Using multiple modalities for emotion recognition is advan-"
        },
        {
          "1 Orange Innovation, Grenoble, France": "tageous since modalities may be complementary, and should"
        },
        {
          "1 Orange Innovation, Grenoble, France": "thus\nimprove the performance of\nthe model when used to-"
        },
        {
          "1 Orange Innovation, Grenoble, France": "gether [3]. However,\nin real-world scenarios,\nthere might be"
        },
        {
          "1 Orange Innovation, Grenoble, France": "cases where a modality might not be available. If for example,"
        },
        {
          "1 Orange Innovation, Grenoble, France": "the modalities consist of video, audio, and physiological sig-"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "nals, the camera ﬁeld of view might be obstructed, the micro-"
        },
        {
          "1 Orange Innovation, Grenoble, France": "phone can be too far away, or the physiological sensor might"
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "be on a wearable device that is not currently worn. Therefore,"
        },
        {
          "1 Orange Innovation, Grenoble, France": "we need a model capable of handling missing modalities."
        },
        {
          "1 Orange Innovation, Grenoble, France": ""
        },
        {
          "1 Orange Innovation, Grenoble, France": "In this work, we extend a multimodal Transformer [4] as an"
        },
        {
          "1 Orange Innovation, Grenoble, France": "encoder\nto obtain representations from the different modali-"
        },
        {
          "1 Orange Innovation, Grenoble, France": "ties and a Transformer decoder [5] to process those represen-"
        },
        {
          "1 Orange Innovation, Grenoble, France": "tations and make predictions. A Transformer-based approach"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR VERSION": "if a Transformer-based approach is used,\nthere is no need to"
        },
        {
          "AUTHOR VERSION": "generate the missing modality representations, or do modiﬁca-"
        },
        {
          "AUTHOR VERSION": "tions to the architecture so it can work with modalities absent."
        },
        {
          "AUTHOR VERSION": "In this case, the attention mechanisms simply do not attend to"
        },
        {
          "AUTHOR VERSION": "the missing modalities, and it\nis capable of attending to all of"
        },
        {
          "AUTHOR VERSION": "them if they are present."
        },
        {
          "AUTHOR VERSION": "For the reasons stated in the previous paragraph, many works"
        },
        {
          "AUTHOR VERSION": "that use the third type of approach to handle missing modal-"
        },
        {
          "AUTHOR VERSION": "ities use\na Transformer-based model.\nSome\nexamples\nin-"
        },
        {
          "AUTHOR VERSION": "clude the work of Goncalves and Busso [19], and the work"
        },
        {
          "AUTHOR VERSION": "of Parthasarathy and Sundaram [20], where they use a cross-"
        },
        {
          "AUTHOR VERSION": "modality Transformer to combine audio and visual modalities,"
        },
        {
          "AUTHOR VERSION": "improving the robustness of the model\nto missing modalities"
        },
        {
          "AUTHOR VERSION": "by eliminating a modality during training. A disadvantage of"
        },
        {
          "AUTHOR VERSION": "using a cross-modality Transformer is that expanding the ap-"
        },
        {
          "AUTHOR VERSION": "proach to use more modalities is not straightforward. To over-"
        },
        {
          "AUTHOR VERSION": "come this problem, a Multimodal Transformer [4] can be em-"
        },
        {
          "AUTHOR VERSION": "ployed,\nlike in the work of Ma et al. [6], where robustness to"
        },
        {
          "AUTHOR VERSION": "missing modalities is increased using a multitasking approach."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "The Transformer-based models are well suited to model long"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "and short\ntemporal\nrelations of\nthe inputs and to model\nthe"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "cross-modality dependencies. Nevertheless,\nin the reviewed"
        },
        {
          "AUTHOR VERSION": "Transformer-based approaches,\nthe\nattention layers have to"
        },
        {
          "AUTHOR VERSION": "model\nthe temporal and the intermodal dependencies at\nthe"
        },
        {
          "AUTHOR VERSION": "same time. We argue that\nit can be advantageous to attend"
        },
        {
          "AUTHOR VERSION": "only to the cross-modal dependencies when aggregating the"
        },
        {
          "AUTHOR VERSION": "multimodal information. In addition, the reviewed approaches"
        },
        {
          "AUTHOR VERSION": "do not explicitly consider past predictions when making the"
        },
        {
          "AUTHOR VERSION": "current prediction, which we believe is beneﬁcial."
        },
        {
          "AUTHOR VERSION": "Our approach is\na Transformer-based approach that uses a"
        },
        {
          "AUTHOR VERSION": "Multimodal Transformer as encoder, making it suitable for any"
        },
        {
          "AUTHOR VERSION": "number of modalities. We also use the novel idea of using the"
        },
        {
          "AUTHOR VERSION": "cross-attention from a Transformer decoder [5]\nto weigh the"
        },
        {
          "AUTHOR VERSION": "information from the different modalities. The decoder uses"
        },
        {
          "AUTHOR VERSION": "only the information of each modality at the current prediction"
        },
        {
          "AUTHOR VERSION": "time-step,\nrelieving it\nfrom modeling the temporal\ninforma-"
        },
        {
          "AUTHOR VERSION": "tion, which is done by the encoder.\nIn addition, we explicitly"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "use past predictions to make the current one by employing an"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "auto-regressive approach. To handle missing modalities, we"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "use the approach of hiding some modalities at\ntrain time, but"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "different from the state of the art, we employ a technique to"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "ﬁnd and then hide the important modalities."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "3\nApproach"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "In\nthis\nsection, we\nprovide\na\ndetailed\nexplanation of\nour"
        },
        {
          "AUTHOR VERSION": "approach\nto\nperform multimodal\ntime-continuous\nvalue-"
        },
        {
          "AUTHOR VERSION": "continuous emotion recognition. Our objective is to predict"
        },
        {
          "AUTHOR VERSION": "values of arousal and valence. We start\nthis section by ex-"
        },
        {
          "AUTHOR VERSION": "plaining our encoder\nthat generates multimodal\nrepresenta-"
        },
        {
          "AUTHOR VERSION": "tions.\nThen we explain our decoder\nthat predicts the values"
        },
        {
          "AUTHOR VERSION": "of arousal and valence from those representations. Finally, we"
        },
        {
          "AUTHOR VERSION": "describe the loss that we use to train our model."
        },
        {
          "AUTHOR VERSION": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "...\np′\np′\np′": "1\n2\n...\nT"
        },
        {
          "...\np′\np′\np′": "Encodings"
        },
        {
          "...\np′\np′\np′": "+\n+\n+\nrM\nT"
        },
        {
          "...\np′\np′\np′": "Input\n..."
        },
        {
          "...\np′\np′\np′": "dt−1\nd0\nd1"
        },
        {
          "...\np′\np′\np′": "Features"
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": "Figure 2: Auto-regressive MultiModal Transformer Decoder"
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": "(AMMTD). Decoder when predicting the emotion value at"
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": "time t. N x means that there are N stacked TDL layers."
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": "modality m at time-step t as f m\n= am\nt\nt + pt, +em, and if we"
        },
        {
          "...\np′\np′\np′": "have M modalities, the concatenated input sequence is then"
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": "11\n1 T\n(4)\n[f\n, . . . , f\n, . . . , f M\n, . . . , f M\n]."
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": "We process this sequence using a Transformer encoder. The"
        },
        {
          "...\np′\np′\np′": "of the Transformer are given by\noutput representations rm"
        },
        {
          "...\np′\np′\np′": ""
        },
        {
          "...\np′\np′\np′": "[r1\n, . . . , rM\n1, . . . , r1\nT , . . . , rM"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR VERSION": "Emotion\n..."
        },
        {
          "AUTHOR VERSION": "y1\ny2\nyt"
        },
        {
          "AUTHOR VERSION": "Predictions"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Fully-Connected"
        },
        {
          "AUTHOR VERSION": "Emotion Regression Network (ERN)\n(FC)"
        },
        {
          "AUTHOR VERSION": "Output\n..."
        },
        {
          "AUTHOR VERSION": "d1\nd2\ndt"
        },
        {
          "AUTHOR VERSION": "Features"
        },
        {
          "AUTHOR VERSION": "t−1\nh′′\nh′′\nh′′"
        },
        {
          "AUTHOR VERSION": "Encoder\nNx"
        },
        {
          "AUTHOR VERSION": "H ′′\nOutputs"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Fully-Connected"
        },
        {
          "AUTHOR VERSION": "Feed-Forward Network (FFN)\nr1"
        },
        {
          "AUTHOR VERSION": "(FC)"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "r1\nH ′"
        },
        {
          "AUTHOR VERSION": "Multi-Head\n..."
        },
        {
          "AUTHOR VERSION": "Attention"
        },
        {
          "AUTHOR VERSION": "Multi-Head Cross-Attention (MHCA)"
        },
        {
          "AUTHOR VERSION": "(MHA)\nr1"
        },
        {
          "AUTHOR VERSION": "t"
        },
        {
          "AUTHOR VERSION": "K\nV\nQ\n..."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Ht\nr1"
        },
        {
          "AUTHOR VERSION": "Multi-Head\nT"
        },
        {
          "AUTHOR VERSION": "Attention\n...\nMulti-Head Self-Attention (MHSA)"
        },
        {
          "AUTHOR VERSION": "(MHA)"
        },
        {
          "AUTHOR VERSION": "K\nV\nQ"
        },
        {
          "AUTHOR VERSION": "rM\nTransformer Decoder"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Layer (TDL)\nIt"
        },
        {
          "AUTHOR VERSION": "rM"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "..."
        },
        {
          "AUTHOR VERSION": "it−1\ni0\ni1"
        },
        {
          "AUTHOR VERSION": "rM\nPositional"
        },
        {
          "AUTHOR VERSION": "t\n...\np′\np′\np′"
        },
        {
          "AUTHOR VERSION": "1\n2\n...\nT"
        },
        {
          "AUTHOR VERSION": "Encodings"
        },
        {
          "AUTHOR VERSION": "+\n+\n+\nrM\nT"
        },
        {
          "AUTHOR VERSION": "Input\n..."
        },
        {
          "AUTHOR VERSION": "dt−1\nd0\nd1"
        },
        {
          "AUTHOR VERSION": "Features"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Figure 2: Auto-regressive MultiModal Transformer Decoder"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "(AMMTD). Decoder when predicting the emotion value at"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "time t. N x means that there are N stacked TDL layers."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "modality m at time-step t as f m\n= am\nt\nt + pt, +em, and if we"
        },
        {
          "AUTHOR VERSION": "have M modalities, the concatenated input sequence is then"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "11\n1 T\n(4)\n[f\n, . . . , f\n, . . . , f M\n, . . . , f M\n]."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "We process this sequence using a Transformer encoder. The"
        },
        {
          "AUTHOR VERSION": "of the Transformer are given by\noutput representations rm"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[r1\n, . . . , rM\n1, . . . , r1\nT , . . . , rM\nT ] ="
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "(5)"
        },
        {
          "AUTHOR VERSION": "Transformer Encoder([f 1\n, . . . , f M\n]).\n1 , . . . , f 1\nT , . . . , f M\nT"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Following [10], we\nemploy a bidirectional\nattention mask"
        },
        {
          "AUTHOR VERSION": "in\nthe Transformer\nencoder.\nWhen\nprocessing\na\nspeciﬁc"
        },
        {
          "AUTHOR VERSION": "time-step,\nthis mask hides\nthe\ninputs\nthat\nare\nfarther\nthan"
        },
        {
          "AUTHOR VERSION": "mask_length positions in the future and in the past. This al-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "lows the model to concentrate on recent\ninformation, and not"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "to worry about information too far in time that probably does"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "not inﬂuence the current emotional state. Note that we are not"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "hiding complete modalities, therefore this technique is not in-"
        },
        {
          "AUTHOR VERSION": "tended to make the model robust to missing modalities."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "3.2\nAuto-regressive MultiModal Transformer Decoder"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "(AMMTD)"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "One of\nthe\ncontributions of\nthis paper\nis\nto develop a de-"
        },
        {
          "AUTHOR VERSION": "coder that predicts emotion from the multimodal representa-"
        },
        {
          "AUTHOR VERSION": "tions given by the encoder.\nTo do this, we design an Auto-"
        },
        {
          "AUTHOR VERSION": "regressive MultiModal Transformer Decoder (AMMTD). This"
        },
        {
          "AUTHOR VERSION": "decoder has two important characteristics: ﬁrst,\nit\ntakes pre-"
        },
        {
          "AUTHOR VERSION": "vious predictions into account\nto determine the current emo-"
        },
        {
          "AUTHOR VERSION": "tion; second,\nit aggregates the representations of the different"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "modalities, giving more weight to the more important ones."
        },
        {
          "AUTHOR VERSION": "A Transformer Decoder Layer\n(TDL)\n[5]\nis composed of a"
        },
        {
          "AUTHOR VERSION": "Multi-Head Self-Attention module (MHSA),\nfollowed by a"
        },
        {
          "AUTHOR VERSION": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR VERSION": "the encoder from Equation 5 and the output of the MHSA from"
        },
        {
          "AUTHOR VERSION": "Equation 9, is"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "′ t\n1 t\n1 t\nH\n, . . . , rM\n], [r\n, . . . , rM\n]),\n= MHA([h0, h1, . . . , ht−1], [r"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "(10)"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Note that we force the model\nto only attend to the encoder"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "time t"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "(or other outputs around t) because we want"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "focuses only on ﬁnding the best weighting between the differ-"
        },
        {
          "AUTHOR VERSION": "ent modalities. We want to avoid the MHCA having to weigh"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "which other time-steps in the different modalities might be im-"
        },
        {
          "AUTHOR VERSION": "portant. Moreover, this restricts the information ﬂow between"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "modalities, which has been demonstrated to be beneﬁcial [22],"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "because it\nforces\nthe shared representation to condense the"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "most signiﬁcant information."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "The ﬁnal step in the TDL is processing each feature of\nthe"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "through a fully connected feed-forward network\nsequence H ′"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "(FFN), applied independently to each position:"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "(11)\nH ′′\nt = FFN(H ′\nt)."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "If the TDL stack has more than one layer,\nthe sequence from"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Equation 11 becomes the input of the next\nlayer. Concretely,"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "the new layer\nimplements Equations 9, 10, and 11 using as"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "input It = H ′′\nt ."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "For\nthe last TLD layer,\nthe sequence in Equation 11 is\nthe"
        },
        {
          "AUTHOR VERSION": "newly generated sequence [d1, . . . , dt] that will be used as in-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "put for the decoder to predict\nthe emotion value for the next"
        },
        {
          "AUTHOR VERSION": "time-step,\nthus if H ′′\n0 , h′′\n1 , . . . , h′′\nthen di = h′′\nt = [h′′\nt−1],"
        },
        {
          "AUTHOR VERSION": "i−1"
        },
        {
          "AUTHOR VERSION": "with i ∈ [1, t]."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Once the complete output\nsequence D = [d1, . . . , dT ] has"
        },
        {
          "AUTHOR VERSION": "been generated,\nthe ﬁnal step is to process it with the Emo-"
        },
        {
          "AUTHOR VERSION": "tion Regression Network (ERN). As shown in Figure 2, our"
        },
        {
          "AUTHOR VERSION": "ERN is a Fully Connected (FC) layer that\nindependently pro-"
        },
        {
          "AUTHOR VERSION": "cesses each element of the sequence D to predict the emotion"
        },
        {
          "AUTHOR VERSION": "values for each time-step, producing the predicted sequence"
        },
        {
          "AUTHOR VERSION": "[ˆy1, . . . , ˆyT ]."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "3.3\nLoss Function"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "As suggested in previous works that address the problem of"
        },
        {
          "AUTHOR VERSION": "recognizing emotion in a time-continuous manner [7, 11, 14],"
        },
        {
          "AUTHOR VERSION": "we use the concordance correlation coefﬁcient (CCC) [23] as"
        },
        {
          "AUTHOR VERSION": "the loss to train our model. Speciﬁcally, the loss is"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "L = 1 − CCC"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "2ρσˆyσy\n(12)"
        },
        {
          "AUTHOR VERSION": "CCC ="
        },
        {
          "AUTHOR VERSION": "σ2\ny + (µˆy − µy)2 ,\ny + σ2"
        },
        {
          "AUTHOR VERSION": "where ρ is the Pearson correlation coefﬁcient between the pre-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "dicted values ˆy and the ground-truth values y. σ denotes the"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "standard deviation and µ denotes the average of either the pre-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "dicted or the ground-truth values."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "4\nHandling Missing Modalities"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "We now describe another contribution of our paper, which is"
        },
        {
          "AUTHOR VERSION": "our approach to deal with missing modalities.\nIn our case, a"
        },
        {
          "AUTHOR VERSION": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR VERSION": "Aligned Annotation Weighting (RAAW)\n[27]\nis used.\nFor"
        },
        {
          "AUTHOR VERSION": "arousal, the annotations corresponding to the lowest inter-rater"
        },
        {
          "AUTHOR VERSION": "agreement are discarded, and replaced by the subject’s Electro-"
        },
        {
          "AUTHOR VERSION": "dermal Activity (EDA) signal recorded during the session. The"
        },
        {
          "AUTHOR VERSION": "authors of the dataset do this because EDA has been demon-"
        },
        {
          "AUTHOR VERSION": "strated to be a good indicator of arousal [28]. Like it was done"
        },
        {
          "AUTHOR VERSION": "for valence, RAAW is used to aggregate the annotations from"
        },
        {
          "AUTHOR VERSION": "the two remaining raters and the EDA signal."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "The dataset\nincludes 69 samples, each being a 5-minute pre-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "sentation given by a subject.\nIn the original dataset, 41 sam-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "ples are used as train set, 14 as validation set, and 14 as test"
        },
        {
          "AUTHOR VERSION": "set. Since annotations are not provided for the test set, we ran-"
        },
        {
          "AUTHOR VERSION": "domly pick 4 samples from the validation set and 6 from the"
        },
        {
          "AUTHOR VERSION": "train set\nto form a new test set with 10 samples.\nIn summary,"
        },
        {
          "AUTHOR VERSION": "we have 35 samples in the train set, 10 in the validation set,"
        },
        {
          "AUTHOR VERSION": "and 10 in the test set."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "5.2\nInput Features"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "We use audio, video, and physiological signals as input modal-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "ities, with features directly provided in Ulm-TSST. All\nfea-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "tures are aligned with annotations; that is, they are sampled at"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "a rate of 2 Hz. For audio, we use extended Geneva Minimal-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "istic Acoustic Parameter Set (eGeMAPS) features. For video,"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "we use Facial Action Units (FAU) intensity. For physiologi-"
        },
        {
          "AUTHOR VERSION": "cal signals,\nthe features are the concatenation of the values of"
        },
        {
          "AUTHOR VERSION": "ECG, RESP, and BPM. We selected these features by running"
        },
        {
          "AUTHOR VERSION": "some experiments in the baseline model provided by the au-"
        },
        {
          "AUTHOR VERSION": "thors of the Ulm-TSST dataset and selecting the features that"
        },
        {
          "AUTHOR VERSION": "lead to good performance."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "5.3\nModel Hyperparameters and Training"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "We optimize the hyperparameters of our model using the Ray"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Tune Framework [29] based on the validation set. Our model"
        },
        {
          "AUTHOR VERSION": "is parameterized as follows: we use a TCN with 6 layers and"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "a kernel of size 9, with ReLU activation function. We have a"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "In the Transformer encoder\nmodel dimension of dmodel = 64."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "and the TDL, we use the GELU activation function. The size"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "of the FFN inside the Transformers is dmodel × 4 = 256. We"
        },
        {
          "AUTHOR VERSION": "use a Transformer encoder with 2 attention heads and 2 layers."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Our decoder is composed of a single TDL with one head. The"
        },
        {
          "AUTHOR VERSION": "FC layer in the ERN has a single hidden layer of dmodel/2 ="
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "32, with ReLU activation function. The bidirectional attention"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "mask for\nthe Transformer encoder has a mask_length of 50"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "seconds (100 time-steps)."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "During\ntraining, we\nsegment\neach\n5-minute\nsample\ninto"
        },
        {
          "AUTHOR VERSION": "smaller\nsamples, as\nsuggested by Christ et al.\n[7].\nSearch-"
        },
        {
          "AUTHOR VERSION": "ing across different options, we found that segments of 125"
        },
        {
          "AUTHOR VERSION": "seconds (250 time-steps) with a hop size of 25 seconds (50"
        },
        {
          "AUTHOR VERSION": "time-steps) work well in our experimental protocol."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "We train our model with a batch size of 64 for a maximum of"
        },
        {
          "AUTHOR VERSION": "100 epochs. We start with a learning rate of 0.0001, and halve"
        },
        {
          "AUTHOR VERSION": "it if the metric does not improve for 5 epochs on the validation"
        },
        {
          "AUTHOR VERSION": "set, and we early-stop the training if there is not improvement"
        },
        {
          "AUTHOR VERSION": "for 15 epochs. We use Adam optimizer with B1 = 0.9 and"
        },
        {
          "AUTHOR VERSION": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: the performance of our model, along",
      "data": [
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "RMSE: 0.0934\nRMSE: 0.1317"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "−1\n−1"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "0\n200\n400\n600\n0\n200\n400\n600"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "(a) Arousal Prediction\n(b) Valence Prediction"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "Figure 3:\nExample of\nan output of our model with opti-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "mized training compared with the ground-truth, when predict-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "ing arousal (a) and valence (b) for the same sample."
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "general, our Tranformer-based approach is well suited for this"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "task."
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "6.1.2\nComparison with other predictors"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "To test our\nidea of using cross-attention to weigh the input"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "modalities and an auto-regressive approach to incorporate past"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "predictions, we\ncompare our approach with standard train-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "ing (approach No4), with approaches No2 and No3 in Table"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "1,\nthat use the ERN and an LSTM respectively instead of"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "our AMMTD. The results show that our AMMTD module in-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "creases the performance in most of\nthe metrics, demonstrat-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "ing the effectiveness of our ideas of using cross-attention and"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "auto-regression. The performance of our approach is statisti-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "cally signiﬁcantly better for all the metrics except for valence"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "RSME, where although our approach outperforms both base-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "lines, the improvement is not statistically signiﬁcant."
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "In general,\nthe baseline models perform well\nin terms of\nthe"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "RMSE metric when predicting valence, but our model per-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "forms better\nin terms of\nthe CCC metric. We hypothesize"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "that\nthis behavior\nis produced because the simpler architec-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "ture of the baselines is good enough to predict ﬂat sequences"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "of valence values that are close enough to the ﬂat ground-truth."
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "On the other hand,\nthose approaches fail\nto predict\nthe small"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "changes in the valence values, penalizing the CCC score."
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "6.1.3\nComparison with our optimized training strategy"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "The results presented in entries No4 and No5 in Table 1 show"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "that our optimized training approach, designed to improve the"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "handling of missing modalities, also has the desirable effect"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "of increasing the performance of the model when all modali-"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "ties are present. For example, arousal RMSE decreases from"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "0.2948 to 0.2869 and valence CCC increases from 0.1502 to"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "0.1656. As we expected, the model seems to learn to use more"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "information from the weak modalities,\nimproving the overall"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "performance."
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "6.2\nAccommodating Missing Modalities"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "We present in Table 2 the results of experiments we conducted"
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": "when a modality is missing."
        },
        {
          "−0.5\n−0.5\nCCC: 0.6679\nCCC: 0.2278": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: Summaryof resultswhenmodalitiesare missing, for standardtrainingand ouroptimizedtrainingstrategy. We use",
      "data": [
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "respectively.",
          "respectively.\n(↓) and (↑)": "",
          "indicate that a lower and a higher score is desirable": ""
        },
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "",
          "respectively.\n(↓) and (↑)": "Arousal",
          "indicate that a lower and a higher score is desirable": ""
        },
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "No",
          "respectively.\n(↓) and (↑)": "RMSE↓",
          "indicate that a lower and a higher score is desirable": "RMSE↓"
        },
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "1",
          "respectively.\n(↓) and (↑)": "0.3046 (0.0199)",
          "indicate that a lower and a higher score is desirable": "0.1585 (0.0156)"
        },
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "2",
          "respectively.\n(↓) and (↑)": "0.3238 (0.0227)",
          "indicate that a lower and a higher score is desirable": "0.1850 (0.0167)"
        },
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "3",
          "respectively.\n(↓) and (↑)": "0.3189 (0.0244)",
          "indicate that a lower and a higher score is desirable": "0.1842 (0.0653)"
        },
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "4",
          "respectively.\n(↓) and (↑)": "0.2948†‡ (0.0125)",
          "indicate that a lower and a higher score is desirable": "0.1796 (0.0114)"
        },
        {
          "MMTE+FC, and MMTE+LSTM baselines,": "5",
          "respectively.\n(↓) and (↑)": "0.2869•†‡ (0.0120)",
          "indicate that a lower and a higher score is desirable": "0.1739 (0.0089)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Summaryof resultswhenmodalitiesare missing, for standardtrainingand ouroptimizedtrainingstrategy. We use",
      "data": [
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": ""
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": "is not statistically signiﬁcantly different than the result obtained with all the modalities."
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": ""
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": "Training Mode"
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": "Standard"
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": ""
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": "Optimized"
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": "Standard"
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": ""
        },
        {
          "bold font to indicate that the result is better than its counterpart trained in a different fashion, and if it is statistically signiﬁcantly": "Optimized"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Summaryof resultswhenmodalitiesare missing, for standardtrainingand ouroptimizedtrainingstrategy. We use",
      "data": [
        {
          "Standard\n0.1796\n0.1502\n0.2533": "VALENCE",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": ""
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "Optimized\n0.1739‡\n0.1656‡\n0.2052‡",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "0.1170‡\n0.1809X‡\n0.1676X‡\n0.1746X‡\n0.1637X‡"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "First, we analyze the case where we are predicting arousal with",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "nizing emotions.\nTo improve this, we apply our optimized"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "the model\ntrained in a standard way.\nIn Table 2, we see that",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "training strategy of hiding important modalities during parts"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "there is no signiﬁcant performance degradation when the audio",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "of the training, forcing the model to rely on other modalities."
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "or physiological modalities are missing.\nFor example, when",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "Speciﬁcally, we use the strategy described in Section 4, elim-"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "audio is missing, RMSE goes from 0.2948 to 0.2926, and CCC",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "inating the video modality with probability ρvideo"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "eliminate = 0.25,"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "goes from 0.3578 to 0.3589.\nThese differences are not sta-",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "and maintaining all modalities with probability ρnone = 0.75"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "tistically signiﬁcant, as indicated by the checkmark (X). We",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "when training for arousal prediction.\nFor valence, we use"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "also see that performance drops signiﬁcantly when the video",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "eliminate = 0.333, ρvideo\neliminate = 0.333 and ρnone = 0.334. These"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "modality is missing, with RMSE increasing from 0.2948 to",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "probabilities were found empirically by testing several conﬁg-"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "0.3252 and CCC decreasing from 0.3578 to 0.2713. These re-",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "urations and keeping the best ones for the validation set."
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "sults conﬁrm that our model continues to accurately predict",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": ""
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "We see in Table 2 that our optimized training strategy improves"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "arousal when a modality is missing, although performance is",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": ""
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "all results when any modality is missing. For example, when"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "reduced in some cases.",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": ""
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "the physiological\nsignals are missing, CCC improves\nfrom"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "For valence, we see that\nthere is no signiﬁcant performance",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "0.3539 to 0.3571 when predicting arousal and from 0.1486 to"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "degradation when\nphysiological\nsignals\nare missing, with",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "0.1637 when predicting valence. Notably, the improvement is"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "RMSE going from 0.1796 to 0.1808 and CCC going from",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "statistically signiﬁcant, as indicated by the double dagger (‡),"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "0.1502 to 0.1486. On the contrary,\nthe model performance",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "in all cases when the important modalities are missing, except"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "drops\nsigniﬁcantly when the\naudio or video modalities are",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "for RMSE when predicting arousal with the video modality"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "missing. For example, RMSE increases from 0.1796 to 0.2533",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "missing.\nIn addition,\nthe performance of the model improves"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "when the audio modality is missing and increases to 0.2170",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "even when all modalities are present. For instance, RMSE de-"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "when the video modality is missing. Much like for arousal,",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "creases from 0.2948 to 2869 for arousal and from 0.1796 to"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "these results show that our model continues to estimate valence",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "0.1739 for valence."
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "when a modality is missing, albeit with a drop in performance.",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": ""
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "This shows that our optimized training strategy works well,"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "With these results, we can identify the most important modal-",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "making our model less reliant on the important modalities and"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "ities for our model, by identifying the modalities that when",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "using more information from the other ones.\nThis\nstrategy"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "missing, induce a signiﬁcant drop in performance. When pre-",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "improves the performance of our approach not only when a"
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "dicting arousal, the most important modality is video, and for",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": "modality is missing, but also when all modalities are present."
        },
        {
          "Standard\n0.1796\n0.1502\n0.2533": "valence the most important modalities are audio and video.",
          "0.0738\n0.2170\n0.1564X\n0.1808X\n0.1486X": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "References"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[1] A.\nZenonos, A. Khan,\nG. Kalogridis,\nS. Vatsikas,"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "T. Lewis, and M. Sooriyabandara, “HealthyOfﬁce: Mood"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "recognition at work using smartphones and wearable sen-"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "sors,”\nin 2016 IEEE International Conference on Per-"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "vasive Computing and Communication Workshops (Per-"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "Com Workshops), Mar. 2016, pp. 1–6."
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "[2] E. Nunez, M. Hirokawa, M. Perusquia-Hernandez, and"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": "K. Suzuki,\n“Effect on Social Connectedness and Stress"
        },
        {
          "(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR VERSION": "Levels by Using a Huggable Interface in Remote Com-"
        },
        {
          "AUTHOR VERSION": "munication,”\nin 2019 8th International Conference on"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Affective Computing and Intelligent\nInteraction (ACII),"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Sept. 2019, pp. 1–7."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[3] Yassine Ouzar, Frédéric Bousefsaf, Djamaleddine Djeld-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "jli, and Choubeila Maaoui,\n“Video-Based Multimodal"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Spontaneous Emotion Recognition Using Facial Expres-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "the\nsions and Physiological Signals,”\nin Proceedings of"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Recognition, 2022, pp. 2460–2469."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[4] Valentin Gabeur,\nChen\nSun,\nKarteek Alahari,\nand"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Cordelia Schmid,\n“Multi-modal Transformer for Video"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Retrieval,”\nin Computer Vision – ECCV 2020, Cham,"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "2020, vol. 12349, pp. 214–229, Springer\nInternational"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Publishing."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[5] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "and Illia Polosukhin,\n“Attention is All you Need,” Ad-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "vances in Neural\nInformation Processing Systems, vol."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "30, 2017."
        },
        {
          "AUTHOR VERSION": "[6] Mengmeng Ma,\nJian Ren, Long Zhao, Davide Testug-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "gine,\nand Xi Peng,\n“Are Multimodal Transformers"
        },
        {
          "AUTHOR VERSION": "the\nRobust\nto Missing Modality?,”\nin Proceedings of"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Recognition, 2022, pp. 18177–18186."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[7] Lukas Christ, Shahin Amiriparian, Alice Baird, Panagi-"
        },
        {
          "AUTHOR VERSION": "otis Tzirakis, Alexander Kathan, Niklas Müller, Lukas"
        },
        {
          "AUTHOR VERSION": "Stappen,\nEva-Maria Meßner,\nAndreas König,\nAlan"
        },
        {
          "AUTHOR VERSION": "Cowen, Erik Cambria,\nand Björn W. Schuller,\n“The"
        },
        {
          "AUTHOR VERSION": "MuSe 2022 Multimodal Sentiment Analysis Challenge:"
        },
        {
          "AUTHOR VERSION": "Humor, Emotional Reactions, and Stress,”\nin Proceed-"
        },
        {
          "AUTHOR VERSION": "ings of\nthe 3rd International on Multimodal Sentiment"
        },
        {
          "AUTHOR VERSION": "Analysis Workshop and Challenge, New York, NY, USA,"
        },
        {
          "AUTHOR VERSION": "Oct. 2022, pp. 5–14, Association for Computing Machin-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "ery."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[8]\nJian Huang,\nJianhua Tao, Bin Liu, Zheng Lian,\nand"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Mingyue Niu, “Multimodal Transformer Fusion for Con-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "tinuous Emotion Recognition,”\nin ICASSP 2020 - 2020"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "IEEE International Conference on Acoustics, Speech and"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Signal Processing (ICASSP), May 2020, pp. 3507–3511."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[9] Su Zhang, Ruyi An, Yi Ding, and Cuntai Guan, “Contin-"
        },
        {
          "AUTHOR VERSION": "uous Emotion Recognition using Visual-audio-linguistic"
        },
        {
          "AUTHOR VERSION": "Information: A Technical Report for ABAW3,”\nin 2022"
        },
        {
          "AUTHOR VERSION": "IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "AUTHOR VERSION": "Recognition Workshops\n(CVPRW), New Orleans, LA,"
        },
        {
          "AUTHOR VERSION": "USA, June 2022, pp. 2375–2380, IEEE."
        },
        {
          "AUTHOR VERSION": "[10] Haifeng Chen,\nDongmei\nJiang,\nand Hichem Sahli,"
        },
        {
          "AUTHOR VERSION": "“Transformer Encoder With Multi-Modal Multi-Head"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "IEEE\nAttention for Continuous Affect Recognition,”"
        },
        {
          "AUTHOR VERSION": "Transactions on Multimedia, vol. 23, pp. 4171–4183,"
        },
        {
          "AUTHOR VERSION": "2021."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[11] Yiping Liu, Wei Sun, Xing Zhang, and Yebao Qin, “Im-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "proving Dimensional Emotion Recognition via Feature-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "the 3rd International\nwise Fusion,”\nin Proceedings of"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "on Multimodal Sentiment Analysis Workshop and Chal-"
        },
        {
          "AUTHOR VERSION": "lenge, New York, NY, USA, Oct. 2022, pp. 55–60, Asso-"
        },
        {
          "AUTHOR VERSION": "ciation for Computing Machinery."
        },
        {
          "AUTHOR VERSION": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AUTHOR VERSION": "[21] Shaojie\nBai,\nJ.\nZico Kolter,\nand Vladlen Koltun,"
        },
        {
          "AUTHOR VERSION": "“An\nEmpirical Evaluation\nof Generic Convolutional"
        },
        {
          "AUTHOR VERSION": "and\nRecurrent\nNetworks\nfor\nSequence Modeling,”"
        },
        {
          "AUTHOR VERSION": "arXiv:1803.01271, Apr. 2018."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[22] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Cordelia Schmid, and Chen Sun, “Attention Bottlenecks"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Infor-\nfor Multimodal Fusion,”\nin Advances in Neural"
        },
        {
          "AUTHOR VERSION": "mation Processing Systems. 2021, vol. 34, pp. 14200–"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "14213, Curran Associates, Inc."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[23] L. I. Lin, “A concordance correlation coefﬁcient to eval-"
        },
        {
          "AUTHOR VERSION": "uate reproducibility,” Biometrics, vol. 45, no. 1, pp. 255–"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "268, Mar. 1989."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[24] Lukas Stappen, Alice Baird, Lukas Christ, Lea Schu-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "mann, Benjamin Sertolli, Eva-Maria Meßner, Erik Cam-"
        },
        {
          "AUTHOR VERSION": "bria, Guoying Zhao, and Björn W. Schuller, “The MuSe"
        },
        {
          "AUTHOR VERSION": "2021 Multimodal Sentiment Analysis Challenge: Senti-"
        },
        {
          "AUTHOR VERSION": "ment, Emotion, Physiological-Emotion, and Stress,”\nin"
        },
        {
          "AUTHOR VERSION": "Proceedings of the 2nd on Multimodal Sentiment Analy-"
        },
        {
          "AUTHOR VERSION": "sis Challenge, Virtual Event China, Oct. 2021, pp. 5–14,"
        },
        {
          "AUTHOR VERSION": "ACM."
        },
        {
          "AUTHOR VERSION": "[25] Lukas Stappen, Eva-Maria Meßner, Erik Cambria, Guoy-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "ing Zhao, and Björn W. Schuller,\n“MuSe 2021 Chal-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "lenge: Multimodal Emotion, Sentiment, Physiological-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "the\nEmotion, and Stress Detection,”\nin Proceedings of"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "29th ACM International Conference on Multimedia, New"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "York, NY, USA, Oct. 2021, pp. 5706–5707, Association"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "for Computing Machinery."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[26] C. Kirschbaum, K. M. Pirke,\nand D. H. Hellhammer,"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "“The ’Trier Social Stress Test’–a tool\nfor\ninvestigating"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "psychobiological\nstress\nresponses\nin a\nlaboratory set-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "ting,” Neuropsychobiology, vol. 28, no. 1-2, pp. 76–81,"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "1993."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[27] Lukas Stappen, Lea Schumann, Benjamin Sertolli, Alice"
        },
        {
          "AUTHOR VERSION": "Baird, Benjamin Weigell, Erik Cambria, and Björn W."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Schuller,\n“MuSe-Toolbox: The Multimodal Sentiment"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Analysis Continuous Annotation Fusion and Discrete"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "the\nClass Transformation Toolbox,”\nin Proceedings of"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "2nd on Multimodal Sentiment Analysis Challenge, New"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "York, NY, USA, Oct. 2021, pp. 75–82, Association for"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Computing Machinery."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[28] Alice Baird, Lukas Stappen, Lukas Christ, Lea Schu-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "mann, Eva-Maria Messner, and Björn W. Schuller,\n“A"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Physiologically-Adapted Gold Standard for Arousal dur-"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "ing Stress,”\nin Proceedings of"
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "Sentiment Analysis Challenge, New York, NY, USA, Oct."
        },
        {
          "AUTHOR VERSION": "2021, pp. 69–73, Association for Computing Machinery."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": "[29] Richard Liaw, Eric Liang, Robert Nishihara,\nPhilipp"
        },
        {
          "AUTHOR VERSION": "Moritz, Joseph E. Gonzalez, and Ion Stoica,\n“Tune: A"
        },
        {
          "AUTHOR VERSION": "Research Platform for Distributed Model Selection and"
        },
        {
          "AUTHOR VERSION": "Training,” arXiv:1807.05118 [cs, stat], July 2018."
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        },
        {
          "AUTHOR VERSION": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "HealthyOffice: Mood recognition at work using smartphones and wearable sensors",
      "authors": [
        "A Zenonos",
        "A Khan",
        "G Kalogridis",
        "S Vatsikas",
        "T Lewis",
        "M Sooriyabandara"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Pervasive Computing and Communication Workshops"
    },
    {
      "citation_id": "2",
      "title": "Effect on Social Connectedness and Stress Levels by Using a Huggable Interface in Remote Communication",
      "authors": [
        "E Nunez",
        "M Hirokawa",
        "M Perusquia-Hernandez",
        "K Suzuki"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "3",
      "title": "Video-Based Multimodal Spontaneous Emotion Recognition Using Facial Expressions and Physiological Signals",
      "authors": [
        "Yassine Ouzar",
        "Frédéric Bousefsaf",
        "Djamaleddine Djeldjli",
        "Choubeila Maaoui"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Multi-modal Transformer for Video Retrieval",
      "authors": [
        "Valentin Gabeur",
        "Chen Sun",
        "Karteek Alahari",
        "Cordelia Schmid"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020"
    },
    {
      "citation_id": "5",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Are Multimodal Transformers Robust to Missing Modality?",
      "authors": [
        "Mengmeng Ma",
        "Jian Ren",
        "Long Zhao",
        "Davide Testuggine",
        "Xi Peng"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "The MuSe 2022 Multimodal Sentiment Analysis Challenge: Humor, Emotional Reactions, and Stress",
      "authors": [
        "Lukas Christ",
        "Shahin Amiriparian",
        "Alice Baird",
        "Panagiotis Tzirakis",
        "Alexander Kathan",
        "Niklas Müller",
        "Lukas Stappen",
        "Eva-Maria Meßner",
        "Andreas König",
        "Alan Cowen",
        "Erik Cambria",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "8",
      "title": "Multimodal Transformer Fusion for Continuous Emotion Recognition",
      "authors": [
        "Jian Huang",
        "Jianhua Tao",
        "Bin Liu",
        "Zheng Lian",
        "Mingyue Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "Continuous Emotion Recognition using Visual-audio-linguistic Information",
      "authors": [
        "Su Zhang",
        "Ruyi An",
        "Yi Ding",
        "Cuntai Guan"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "10",
      "title": "Transformer Encoder With Multi-Modal Multi-Head Attention for Continuous Affect Recognition",
      "authors": [
        "Haifeng Chen",
        "Dongmei Jiang",
        "Hichem Sahli"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Improving Dimensional Emotion Recognition via Featurewise Fusion",
      "authors": [
        "Yiping Liu",
        "Wei Sun",
        "Xing Zhang",
        "Yebao Qin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "12",
      "title": "Time-Continuous Audiovisual Fusion with Recurrence vs Attention for In-The-Wild Affect Recognition",
      "authors": [
        "Vincent Karas",
        "Mani Kumar Tellamekala",
        "Adria Mallol-Ragolta",
        "Michel Valstar",
        "Bjorn Schuller"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "13",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "Multimodal Temporal Attention in Sentiment Analysis",
      "authors": [
        "Yu He",
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao",
        "Meng Wang",
        "Yuan Cheng"
      ],
      "year": "2022",
      "venue": "Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge"
    },
    {
      "citation_id": "15",
      "title": "Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "Found in Translation: Learning Robust Joint Representations by Cyclic Translations between Modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "Barnabás Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues",
      "authors": [
        "Trisha Mittal",
        "Uttaran Bhattacharya",
        "Rohan Chandra"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "ModDrop: Adaptive Multi-Modal Gesture Recognition",
      "authors": [
        "Natalia Neverova",
        "Christian Wolf",
        "Graham Taylor",
        "Florian Nebout"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "AuxFormer: Robust Approach to Audiovisual Emotion Recognition",
      "authors": [
        "Lucas Goncalves",
        "Carlos Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "20",
      "title": "Training Strategies to Handle Missing Modalities for Audio-Visual Expression Recognition",
      "authors": [
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "21",
      "title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
      "authors": [
        "J Shaojie Bai",
        "Vladlen Kolter",
        "Koltun"
      ],
      "year": "2018",
      "venue": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling",
      "arxiv": "arXiv:1803.01271"
    },
    {
      "citation_id": "22",
      "title": "Attention Bottlenecks for Multimodal Fusion",
      "authors": [
        "Arsha Nagrani",
        "Shan Yang",
        "Anurag Arnab",
        "Aren Jansen",
        "Cordelia Schmid",
        "Chen Sun"
      ],
      "venue": "Advances in Neural Information Processing Systems. 2021"
    },
    {
      "citation_id": "23",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "L Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "24",
      "title": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment, Emotion, Physiological-Emotion, and Stress",
      "authors": [
        "Lukas Stappen",
        "Alice Baird",
        "Lukas Christ",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Eva-Maria Meßner",
        "Erik Cambria",
        "Guoying Zhao",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge, Virtual Event China"
    },
    {
      "citation_id": "25",
      "title": "MuSe 2021 Challenge: Multimodal Emotion, Sentiment, Physiological-Emotion, and Stress Detection",
      "authors": [
        "Lukas Stappen",
        "Eva-Maria Meßner",
        "Erik Cambria",
        "Guoying Zhao",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "26",
      "title": "The 'Trier Social Stress Test'-a tool for investigating psychobiological stress responses in a laboratory setting",
      "authors": [
        "C Kirschbaum",
        "K Pirke",
        "D Hellhammer"
      ],
      "year": "1993",
      "venue": "Neuropsychobiology"
    },
    {
      "citation_id": "27",
      "title": "MuSe-Toolbox: The Multimodal Sentiment Analysis Continuous Annotation Fusion and Discrete Class Transformation Toolbox",
      "authors": [
        "Lukas Stappen",
        "Lea Schumann",
        "Benjamin Sertolli",
        "Alice Baird",
        "Benjamin Weigell",
        "Erik Cambria",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "28",
      "title": "A Physiologically-Adapted Gold Standard for Arousal during Stress",
      "authors": [
        "Alice Baird",
        "Lukas Stappen",
        "Lukas Christ",
        "Lea Schumann",
        "Eva-Maria Messner",
        "Björn Schuller"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "29",
      "title": "Tune: A Research Platform for Distributed Model Selection and Training",
      "authors": [
        "Richard Liaw",
        "Eric Liang",
        "Robert Nishihara",
        "Philipp Moritz",
        "Joseph Gonzalez",
        "Ion Stoica"
      ],
      "year": "2018",
      "venue": "Tune: A Research Platform for Distributed Model Selection and Training",
      "arxiv": "arXiv:1807.05118"
    }
  ]
}