{
  "paper_id": "2502.12454v2",
  "title": "Zero-Shot Emotion Annotation In Facial Images Using Large Multimodal Models: Benchmarking And Prospects For Multi-Class, Multi-Frame Approaches",
  "published": "2025-02-18T02:36:16Z",
  "authors": [
    "He Zhang",
    "Xinyi Fu"
  ],
  "keywords": [
    "Annotation",
    "large language model",
    "gpt",
    "zero-shot",
    "image augmentation",
    "scalable oversight",
    "image sentiment analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study investigates the feasibility and performance of using large multimodal models (LMMs) to automatically annotate human emotions in everyday scenarios. We conducted experiments on the DailyLife subset of the publicly available FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot labeling of key frames extracted from video segments. Under a seven-class emotion taxonomy (\"Angry,\" \"Disgust,\" \"Fear,\" \"Happy,\" \"Neutral,\" \"Sad,\" \"Surprise\"), the LMM achieved an average precision of approximately 50%. In contrast, when limited to ternary emotion classification (negative/neutral/positive), the average precision increased to approximately 64%. Additionally, we explored a strategy that integrates multiple frames within 1-2 second video clips to enhance labeling performance and reduce costs. The results indicate that this approach can slightly improve annotation accuracy. Overall, our preliminary findings highlight the potential application of zero-shot LMMs in human facial emotion annotation tasks, offering new avenues for reducing labeling costs and broadening the applicability of LMMs in complex multimodal environments. \n CCS Concepts ‚Ä¢ Computing methodologies ‚Üí Computer graphics; Computer vision tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the context of rapid advancements in artificial intelligence, technologies such as computer vision and natural language processing are being applied to a myriad of tasks to promote human wellbeing  [8, 12, 26, 36] . These technologies hold particular significance in providing emerging interaction methods within the field of human-computer interaction  [59] . They rely heavily on machine learning methods, where data annotation serves as a fundamental and indispensable step in model development  [73] .\n\nIn the development of machine learning models, data annotation serves as a foundational and indispensable step  [58, 59] . Accurate annotations are crucial for training machine learning models that can effectively interpret complex data, particularly in tasks involving human emotions and behaviors. However, the annotation process is notoriously labor-intensive and costly  [73] , requiring annotators to spend prolonged periods meticulously labeling data  [69] . This manual effort not only demands significant human resources but also introduces variability and potential biases inherent in human cognition  [13, 15] . The challenge is magnified for emotion annotation tasks, where the subjective and nuanced nature of emotions complicates the labeling process. Addressing these challenges requires annotators to repeatedly review the data and engage in multiple rounds of iteration and discussion  [17, 41, 66] .\n\nTo address these challenges, various annotation methodologies have been proposed, including the utilization of crowdsourcing platforms. Crowdsourcing can accelerate the annotation process by distributing the workload across a large number of annotators, thereby reducing both time and cost  [55] . Despite these advantages, crowdsourcing methods often prove insufficient when dealing with specialized environments or tasks that require nuanced understanding and expert judgment  [56] . In such contexts, the reliance on human labor and expertise remains indispensable, highlighting the persistent need for more efficient and scalable annotation solutions.\n\nRecent advancements in artificial intelligence (AI), particularly in the realm of large multimodal models (LMMs), have opened new avenues for automating annotation tasks  [48, 64] . LMMs, such as Generative Pre-trained Transformer (GPT), possess sophisticated natural language understanding capabilities and operate effectively in zero-shot settings, where they can perform tasks without explicit prior training on specific datasets  [49] . These models have demonstrated potential in various applications, from text generation to semantic understanding, suggesting their utility in assisting or even replacing human annotators  [2, 51, 67] .\n\nFurthermore, the latest iterations of LMMs integrate visual capabilities, enabling them to comprehend and interpret graphical information in conjunction with textual data  [42, 71] . This multimodal proficiency suggests that LMMs could become valuable tools for tasks that encompass both visual and linguistic components  [4, 21, 31] . By leveraging their ability to understand visual inputs and operate in zero-shot settings, LMMs have the potential to streamline the annotation process while maintaining both accuracy and efficiency  [22] . However, aside from capacity and performance considerations, the cost of computing resources is also very important. This matter is especially critical for individual users, students, small teams, and start-ups. Currently, because of the understandable business considerations, these groups rarely or even impossible to participate in the pricing process for computing resources such as API access and hardwares like graphic cards. Although we appreciate the discounts and grants offered to these groups, which show support from businesses and policies, these discounts do not reduce the overall use of computing resources. This means that their workflows and project goals remain limited by budget issues. In particular, some tasks or applications that depend heavily on computing resources may still not be easy to process.\n\nBuilding on these capabilities and considering ways to reduce input and output costs, our study investigates the feasibility and performance of using LMMs for the automatic annotation of human emotions in everyday scenarios. Specifically, we employ the GPT-4o-mini model to conduct rapid, zero-shot labeling of key frames extracted from video segments within the DailyLife subset of the publicly available FERV39k dataset  [57] . Our experiments assess the model's performance across two emotion taxonomies: a sevenclass taxonomy encompassing \"Angry, \" \"Disgust, \" \"Fear, \" \"Happy, \" \"Neutral, \" \"Sad, \" and \"Surprise, \" and a ternary taxonomy categorizing emotions as negative, neutral, or positive.\n\nOur results indicate that the LMM attained an average precision of approximately 50% in the seven-class taxonomy, surpassing a simple baseline. This underscores the model's ability to discern complex emotional states without task-specific training. Notably, when the classification was constrained to a simpler ternary classification, the average precision increased to around 64%, demonstrating the model's enhanced performance in broader emotion categories. Additionally, we investigated strategies of integrating multiple frames within 1-2 second video clips to improve labeling performance and reduce annotation costs. This approach resulted a slight but notable improvement in accuracy.\n\nOur findings contribute to the growing body of research aimed at enhancing data annotation methods through AI. By examining the capabilities and limitations of zero-shot LMMs in emotion annotation tasks, we also discuss the potential for employing LMMs in practical applications to reduce costs and enhance scalability in real-world applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Emotion Annotation",
      "text": "Annotating human emotions has consistently been a challenging task  [37] , not only due to the inherent complexity of emotions  [19]  but also because annotators may have varying evaluation standards (or subjectivity)  [6, 47] . A significant issue in emotion annotation is the annotation method. Although the most reliable annotation standard requires individuals to perform the annotations themselves, real-time self-annotation can lead to distraction and affect the expression of emotions  [14] . On the other hand, retrospective annotation relies on individuals' recollections  [7, 17] , which may lead to bias  [20]  as well as high cost  [23] , and can cause embarrassment  [1] . Another widely used annotation method involves external annotators observing and labeling human emotions  [25] . By leveraging human cognition and understanding, and considering the context, external annotators provide reliable emotional labels based on various cues  [54] . Although these two annotation methods can be combined  [66] , they may not be suitable for largescale data processing. Regardless, emotion annotation remains a labor-intensive task. Therefore, exploring more efficient emotion annotation methods is crucial.\n\nCurrently, many annotation methods involve semi-automated or automated labeling conducted by models  [14, 24, 40, 43, 45, 46, 70] , which greatly improves annotation efficiency. However, such annotations are typically built upon prior preparations, meaning that before the annotation task begins, data with emotion annotations are still required to train the underlying annotation models  [62] . Furthermore, in some specific tasks, these labels cannot be easily shared due to task constraints, but instead require the preparation of pre-trained data that suits specific scenarios  [35] . This implies that the traditional challenges in annotation tasks still persist.\n\nAnother important issue in emotion annotation is the choice of emotion classification scheme. Considering that annotation is a time-consuming and laborious process, researchers often categorize emotions based on task requirements to reduce the difficulty of annotation and improve efficiency. Examples include categorizing emotions into positive and negative  [28] , emotional and neutral states  [3] , classifying specific emotions by their intensities  [66] , and using basic emotions  [33, 65] . In this study, we consider the potential task requirements of these various classification standards and base our research based on the available ground truth emotion labels.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Lmm For Annotation",
      "text": "The emergence and application of large language models (LLMs) and LMMs have introduced unprecedented opportunities in the field of data annotation. An increasing number of researchers and practitioners have recognized the vast potential of LLMs and LMMs for enhancing annotation processes  [32] . As researchers continue to explore and leverage the advancing capabilities of LLMs, particularly in multimodal interactions  [63]  and improvements in processing power  [5] , the range of annotation tasks has expanded significantly. These tasks now encompass various data types, including text  [30] , audio  [16] , images  [11, 39] , and specialized domain-specific data  [50, 68] .\n\nA recent survey has shed light on current trends and leading research in the application of LMMs for annotation tasks  [49] . Within the scope of our study, which focuses on emotion annotation for image data, related work has explored various capabilities of LMMs. For instance, researchers have evaluated the ability of LMMs to predict emotions from captions generated from images-derived captions  [60] , perform image retrieval  [61] , and generate descriptive captions  [44] . Notably, in early 2024, a study compared the performance of LMMs such as GPT-3.5, GPT-4, and Bard against traditional supervised models like Convolutional Neural Networks (CNNs) for emotion recognition in image data  [34] . The findings revealed that deep learning models specifically trained for this task generally achieved higher accuracy than LMMs.\n\nHowever, despite the superior accuracy of traditional supervised models, they also present significant limitations. Nonetheless, LMMs offer the potential to achieve performance that is comparable to traditional models while reducing training and application costs. Therefore, in this study, we further optimized prompt engineering and reorganized annotation strategies to harness the capabilities and advantages of LMMs.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method 3.1 Dataset",
      "text": "We utilized the publicly available FERV39k dataset  [57] , which comprises numerous 1-2 second video clips encompassing seven distinct emotions expressed by individuals across various scenarios (\"Angry,\" \"Disgust,\" \"Fear,\" \"Happy,\" \"Neutral,\" \"Sad,\" \"Surprise\"). This dataset has been manually annotated and extensively used in research, providing a widely recognized benchmark for comparative analyses. Within this dataset, we selected the \"DailyLife\" subset, as this scenario is considered the most representative of real-life conditions, thereby enhancing the potential transferability of our work to a broader range of task scenarios. Specifically, the \"DailyLife\" subset includes 2,339 video clips depicting a variety of daily activities, interactions, and emotional expressions. Each clip is manually annotated with a definitive emotion label based on contextual and visible emotional cues, serving as the ground truth label. Images were then extracted at 25 frames per second, each carrying the associated emotion label.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Selection",
      "text": "In this study, we employed the GPT-4o-mini (\"gpt-4o-mini-2024-07-18\") model, a variant of the GPT-4 architecture optimized for greater efficiency and rapid inference. The selection of GPT-4o-mini was driven by its ability to perform zero-shot tasks while balancing performance  1  and cost 2  considerations. Additionally, GPT-4o-mini integrates vision capabilities  3  , allowing it to accept image inputs and interpret graphical information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotation Process",
      "text": "The annotation methods and processes are illustrated in  Fig 1,   with specific strategies to be detailed in the subsequent sections.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Zero-Shot",
      "text": "Labeling. The annotation process used a zeroshot approach, prompting the GPT-4o-mini model with simple, predefined instructions to label extracted key frames. No additional training or fine-tuning was performed for the specific emotion annotation task. The prompts instructed the model to identify and classify the dominant emotion in each frame based on visual and contextual cues.\n\nTo optimize cost efficiency, five frames from each video segment were annotated: the initial frame, the first quartile (Q1), the middle frame, the third quartile (Q3), and the last frame. This sampling reduced annotation counts while capturing key emotional transitions. Different weighting strategies were then applied to derive comprehensive labels for each segment based on these frames. This approach balances accuracy in emotion recognition with cost constraints, ensuring the methodology remains effective and scalable. Apart from the predefined annotation strategy, we did not impose any additional human intervention on the LMM's labeling process.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Prompt",
      "text": "Engineering. In our study, we implemented prompt engineering to effectively utilize the GPT-4o-mini model for emotion annotation in images. The prompt was meticulously crafted to guide the model's responses by defining clear roles and providing specific instructions  [29] . Initially, a prompt was set to establish the model as a \"professional image emotion analysis assistant,\" explicitly listing the available emotion options derived from the predefined ùê∏ùëÄùëÇùëá ùêºùëÇùëÅ _ùêøùê¥ùêµùê∏ùêøùëÜ. This foundational setup ensures that the model operates within the desired context and understands the classification framework. For each image (or multi-frame integrated image) to be analyzed, we constructed a user message that includes both textual instructions and the image itself, e.g., \"This is an independent image frame, please analyze the emotion. Please analyze the emotion of the following image and select the most matching one from the above options, returning only the emotion name.\"\n\nSubsequently, the user message was structured to include both textual and visual inputs. The textual component began with a customized prompt instructing the model to analyze the emotion conveyed in the image and select the most appropriate emotion from the provided options. This was followed by embedding the image itself, encoded in base64 format, within the message. By integrating the image (linked to local address) in this manner, we facilitated a multimodal interaction, allowing the model to process and interpret visual data alongside textual instructions.\n\nThe model was further directed to return only the name of the identified emotion, ensuring concise and relevant output. After the model generated a response, the content was extracted and stripped of any extraneous whitespace to obtain a clean final emotion label. In addition, we set the temperature parameter to 0 to ensure deterministic and consistent responses from the model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Annotation Strategies",
      "text": "3.4.1 Annotation Strategy A1 (Seven-Class Taxonomy). Strategy A1 involves individually annotating each of the five selected frames within a video segment using the seven-class emotion taxonomy. Specifically, the frames chosen for annotation include the initial frame, the first quartile (Q1) position frame, the middle frame, the third quartile (Q3) position frame, and the final frame of the segment. Each frame is independently labeled with one of the seven emotion categories: \"Angry,\" \"Disgust,\" \"Fear,\" \"Happy,\" \"Neutral,\" \"Sad, \" and \"Surprise. \"\n\nAfter annotation, the accuracy is directly calculated by comparing each frame's predicted emotion label against the ground truth labels provided in the dataset.  five frames, the strategy identifies the absolute majority emotion among the labeled frames. In cases where there is a tie in the distribution of different emotions, the emotion label of the middle frame is selected to represent the video segment's overall emotional state.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Annotation Strategy C1 (Seven-Class Taxonomy).",
      "text": "Strategy C1 is determining the predominant emotion by excluding the \"Neutral\" category. Specifically, if one emotion constitutes an absolute majority among the annotated frames after removing \"Neutral, \" that emotion is assigned to the video segment. However, if all five frames are labeled as \"Neutral, \" the segment is assigned the \"Neutral\" label. In cases where there is an equal distribution of different emotions, the emotion label of the middle frame is selected to represent the overall emotion of the video segment. This approach aims to enhance annotation accuracy by focusing on more distinctly positive or negative emotional states, thereby mitigating the ambiguous property of LMM in classifying the intermediate emotion of \"neutral\".",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "3.4.4",
      "text": "Annotation Strategy D1 (Seven-Class Taxonomy). Strategy D1 employs a multi-frame integration approach by concatenating the five selected frames into a single composite input. Specifically, the initial frame, Q1 position frame, middle frame, Q3 position frame, and the final frames are sequentially joined along the temporal dimension, rather than across the channel, to form a unified image input. This consolidated input is then presented to the GPT-4o-mini model for annotation in a single step.\n\nBy integrating multiple frames, this strategy leverages temporal context, allowing the model to consider the emotional progression within the video segment. This holistic view aims to improve annotation accuracy by providing a broader context for emotion classification, potentially capturing transitional emotional states that individual frame annotations might miss.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3.4.5",
      "text": "Annotation Strategy A2 (Three-Class Taxonomy). Strategy A2 adapts the results from Strategy A1 to the three-class emotion taxonomy. In this strategy, each of the five annotated frames from Strategy A1 is directly mapped to one of three broader categories  [27] : \"Positive,\" \"Neutral,\" or \"Negative.\" Specifically, emotions categorized as \"Angry, \" \"Disgust, \" \"Fear, \" and \"Sad\" are classified as \"Negative,\" while \"Happy\" and \"Surprise\" are classified as \"Positive. \" The \"Neutral\" labels are still \"Neutral\".\n\nEach frame's seven-class label is converted to its corresponding three-class label based on this mapping. The accuracy is then calculated by comparing these three-class labels against the ground truth labels, allowing for an evaluation of the model's performance in a simplified emotion classification scenario. Strategy D2 is similar to the multi-frame ensemble approach of Strategy D1, but uses a three-class classification approach. In this strategy, the five selected frames are concatenated into a single composite input, similar to Strategy D1. This integrated input is then processed by the GPT-4o-mini model to assign a single three-class emotion label (\"Positive,\" \"Neutral,\" or \"Negative\") to the entire video segment.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We assessed our annotation strategies using precision, recall, F1score, support, and accuracy (in Fig.  2 ). Precision measures the proportion of correct predictions for each emotion, while recall evaluates the ability to identify all relevant instances. The F1-score balances precision and recall, making it useful for uneven class distributions. Accuracy reflects the overall correctness of the model. Additionally, we report macro average and weighted average to provide insights into performance across all classes, with macro average treating each class equally and weighted average accounting for class imbalance by weighting metrics based on class support. Support refers to the number of true instances for each emotion category in the dataset, providing context for the other metrics by indicating the distribution of classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Seven-Class Taxonomy",
      "text": "Table  1  presents the performance metrics for four annotation strategies (A1, B1, C1, and D1) under the seven-class taxonomy.\n\nStrategy A1 (Individual Frame Annotation) attained an overall accuracy of 38%. The model exhibited robust precision for the \"Happy\" category (0.84) but encountered significant challenges in accurately classifying \"Disgust\" (precision: 0.04). The recall metric was notably high for \"Sad\" (0.65) and considerably low for \"Disgust\" (0.18), highlighting the model's difficulty in reliably identifying certain emotional states.\n\nStrategy B1 (Majority Voting) yielded an incremental improvement in accuracy, reaching 41%. Precision for \"Happy rose to 0.89, while \"Disgust\" experienced marginal enhancements in both precision (0.07) and recall (0.23). This suggests that aggregating frame-level annotations through majority voting can slightly bolster performance for specific emotions.\n\nStrategy C1 (Majority Voting Excluding \"Neutral\") further augmented the accuracy to 46%. By excluding the \"Neutral\" category from the majority voting process, this approach improved recall for \"Sad\" to 0.76 and maintained high precision for \"Happy\" (0.85). This indicates that focusing on \"Negative\" and \"Positive\" emotions can mitigate some inaccuracies associated with the \"Neutral\" classifications, thereby enhancing overall annotation reliability.\n\nStrategy D1 (Multi-Frame Integration) achieved an accuracy of 46%, paralleling Strategy C1. By amalgamating multiple frames into a single input, this strategy effectively harnessed temporal context, thereby improving the model's capacity to capture the dynamic progression of emotions across video segments. This integration allows the model to consider the emotional transitions and consistencies present within the selected frames, leading to more coherent and accurate segment-level annotations.\n\nAdditionally, when considering the macro average and weighted average metrics, Strategies C1 and D1 not only achieved higher accuracy but also demonstrated improved balanced performance across all classes. The macro average indicates that these strategies perform more consistently across less frequent emotion categories, while the weighted average reflects their enhanced overall performance, accounting for class imbalances in the dataset.\n\nTo further validate these observations, a non-parametric Friedman test was conducted on the merged dataset (comprising the results from Strategies B1, C1, and D1). As shown in Table  2 , the Friedman test revealed a statistically significant difference among these strategies (p<0.05). This overall significance suggests that at least one strategy performs differently compared to the others.\n\nSubsequently, pairwise comparisons were performed using Dunn's post-hoc test with Bonferroni correction (see Table  3 ). The results indicate that there is no statistically significant difference between Strategies B1 and C1. In contrast, Strategy D1 is statistically significantly different from both B1 and C1 (p<0.05). These statistical findings provide robust evidence that the multi-frame integration approach (Strategy D1), which harnesses temporal context, exhibits a distinct performance profile compared to the majority voting methods.\n\nFurthermore, when considering the macro average and weighted average metrics, both Strategies C1 and D1 demonstrated improved balanced performance across all emotion categories. The macro average reflects consistency in performance across less frequent categories, while the weighted average accounts for class imbalances, further reinforcing the overall effectiveness of these strategies.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Three-Class Taxonomy",
      "text": "Table  4  presents the performance metrics for four annotation strategies within the three-class taxonomy framework.\n\nStrategy A2 (Mapped Three-Class Classification) achieved an accuracy of 57%. The \"Positive\" category exhibited strong precision (0.72), whereas the \"Neutral\" category demonstrated moderate performance with a precision of 0.27 and recall of 0.41.\n\nAnnotation Strategy A1 3.4.1  Annotation Strategy B1 3.4.2 Annotation Strategy C1 3.4.3 Annotation Strategy D1  Strategy B2 (Majority Voting) resulted in a substantial accuracy improvement, attaining 65%. Precision for \"Positive\" increased to 0.79, while the \"Negative\" category demonstrated robust performance with a precision of 0.70 and recall of 0.74.\n\nStrategy C2 (Majority Voting Excluding \"Neutral\") also achieved an accuracy of 65%. This strategy maintained high precision for \"Negative\" (0.67) and significantly improved recall for \"Negative\" to 0.87, while the \"Positive\" category maintained consistent performance with a precision of 0.76 and recall of 0.58.\n\nStrategy D2 (Multi-Frame Integration) matched the accuracy of 65%, effectively leveraging both temporal context and simplified emotion categories to ensure efficient and accurate annotation.\n\nOverall, Strategies B2, C2, and D2 consistently outperformed Strategy A2, further highlighting the effectiveness of aggregation and integration methods in enhancing annotation accuracy within zero-shot classification approaches based on LMMs. Furthermore, the macro average and weighted average metrics underscore the balanced performance of Strategies B2, C2, and D2 across all emotion categories. The macro average indicates that these strategies maintain consistent precision and recall across both common and rare classes, while the weighted average reflects their strong overall performance by accounting for the distribution of classes in the dataset.\n\nTo further validate these observations, a non-parametric Friedman test was conducted on the merged dataset (comprising the results of Strategies B2, C2, and D2). As summarized in Table  5 , the Friedman test revealed a statistically significant difference among the three methods (p < 0.05), which supports the notion that at least one strategy produces a performance profile that is distinct from the others. Subsequent pairwise comparisons using Dunn's posthoc test with Bonferroni adjustment (see Table  6 ) demonstrated that Strategy B2 significantly differs from both Strategies C2 and D2 (p < 0.05), while no significant difference was found between Strategies C2 and D2. This statistical evidence reinforces our conclusion that the choice of aggregation strategy plays a critical role in determining annotation accuracy and consistency.\n\nIn summary, the combined performance metrics and the statistical tests confirm that the aggregation and integration methods (especially Strategies C2 and D2) not only yield superior overall accuracy but also provide a more balanced performance across diverse emotion categories, thereby underscoring the importance of temporal context and strategic data integration in zero-shot annotation tasks.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance Of Different Strategies: Insights From Confusion Matrices",
      "text": "The presented confusion matrices compare the performance of various classification strategies (A1, A2, B1, B2, C1, C2, D1, D2) across different tasks involving emotion and sentiment recognition. Each matrix visualizes the true labels versus the predicted labels, providing insights into the model's accuracy, strengths, and areas requiring improvement. Strategies A1, B1, C1, and D1 are evaluated on their ability to classify seven distinct emotional states, including \"Angry, \" \"Happy, \" \"Neutral, \" and \"Sad. \" The diagonal entries reflect correct classifications, while off-diagonal values indicate confusion between emotions. For example, significant misclassifications are observed between \"Neutral\" and \"Happy\" in some strategies, highlighting challenges in distinguishing subtle emotional variations. Strategies A2, B2, C2, and D2 focus on sentiment classification into three categories: \"Negative, \" \"Neutral, \" and \"Positive. \" While these strategies generally achieve high accuracy for the \"Negative\" category, frequent confusion between \"Neutral\" and \"Positive\" suggests a need for improved sensitivity to nuanced sentiment expressions.\n\nAnnotation Strategy A2 3.4.5  Annotation Strategy B2 3.4.6 Annotation Strategy C2 3.4.7 Annotation Strategy D2   Different strategies exhibit similar distribution patterns, indicating that the zero-shot LMM annotation approach introduces a certain level of ambiguity in labeling the aforementioned emotion categories. This ambiguity often leads to confusion between certain categories, while labels for negative emotions are generally more accurate. This observation suggests potential opportunities for emotion recognition tasks in specific contexts, where leveraging the strengths of zero-shot LMM annotation could enhance performance despite its inherent limitations.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Comparative Analysis",
      "text": "Comparing the seven-class and three-class taxonomies, it is evident that simplifying emotion classification enhances overall accuracy. The three-class strategies (B2, C2, D2) achieved an accuracy of 65%, significantly higher than the best seven-class strategies (C1 and D1) at 46%. This improvement is attributed to the reduced complexity in classification, allowing the model to more effectively distinguish between \"Negative, \" \"Neutral, \" and \"Positive\" emotions.\n\nFurthermore, aggregation methods -whether through majority voting (B1, B2, C1, C2) or multi-frame integration (D1, D2)consistently yielded better performance compared to individual frame annotation (A1, A2). These findings highlight the importance of leveraging temporal context and strategic frame selection to enhance the reliability and accuracy of automated emotion annotation.\n\nOverall, the results demonstrate that the GPT-4o-mini model is capable of effectively annotating human emotions, particularly when employing strategies that aggregate information from multiple frames and simplify emotion categories. These approaches offer a balanced trade-off between annotation accuracy and computational efficiency, making them suitable for large-scale, real-world applications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Baseline Comparison (Random Guessing).",
      "text": "To contextualize the performance of our annotation strategies, we compared them against baseline theoretical chance levels derived from random guessing. In the seven-class taxonomy, random guessing would yield an expected accuracy of approximately 14.3%, while in the three-class taxonomy, the expected accuracy is around 33.3%. Our results demonstrate that all proposed strategies significantly surpass these baseline levels. Specifically, in the seven-class taxonomy, the best-performing strategies (C1 and D1) achieved an accuracy of 46%, more than three times the baseline. Similarly, in the threeclass taxonomy, Strategies B2, C2, and D2 reached an accuracy of 65%, nearly doubling the random guessing baseline. This substantial improvement underscores the effectiveness of our aggregation and integration methods in enhancing annotation accuracy within zero-shot classification tasks using large language models. 4.5.2 Baseline Comparison (Pre-trained Models and Zero-shot Methods). To further contextualize the performance of our annotation strategies, we compared our results against baseline models reported in the FERV39k dataset paper  [57] , with a particular focus on the DailyLife subset under the seven-class taxonomy. The baseline models encompass various architectures, including ResNet-18 (R18), ResNet-50 (R50), VGG-13 (VGG13), VGG-16 (VGG16), and their LSTM-enhanced variants. The performance metrics reported are WAR (Weighted Average Recall)  4  and UAR (Unweighted/Macro Average Recall)  5  , which provide a balanced evaluation by accounting for class imbalances and ensuring that each class contributes proportionally to the overall performance.\n\nIn the DailyLife category, baseline models achieved the following WAR/UAR scores as shown in Table  7 .\n\nOur best-performing strategy within the seven-class taxonomy, Strategy D1 (Multi-Frame Integration), achieved a WAR of 46%, which closely rivals top-performing pre-trained baseline methods such as AEN  (   ùõæ The result of zero-shot method with label augmentation Table  7 : Comparison of Weighted Average Recall (WAR) and Unweighted Average Recall (UAR) between baseline architectures (trained from scratch) and the LMM-based zero-shot method (GPT-4o-mini) on the FERV39k DailyLife subset  [57] . methods, aiming to further save computational resources while still improving recognition results.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Cost-Efficiency And Scalability",
      "text": "Besides performing close to or outperforming baseline models in performance, compared to traditional supervised models, GPT-4omini-based annotation strategies have significant advantages in cost-effectiveness and scalability. Strategy D1 reduces operational costs by minimizing the number of API requests and decreasing token lengths through preprocessing. This cost-effective approach ensures that large-scale annotation tasks remain financially feasible. Furthermore, our zero-shot annotation approach leverages the capabilities of LMMs without necessitating task-specific training, allowing for rapid deployment and adaptation to various annotation tasks with minimal additional resources.\n\nWhile some baseline models, such as Two VGG13-LSTM, exhibit marginally higher WAR scores, our annotation strategy D1 achieves comparable performance levels with enhanced cost and operational efficiencies. This underscores the effectiveness of aggregation techniques and temporal context integration in zero-shot annotation tasks using LMMs, presenting a viable alternative to traditional supervised models, especially in scenarios constrained by budget and computational resources.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion With Future Work",
      "text": "This study demonstrates the feasibility of using LMMs for automated emotion annotation in facial images through zero-shot classification. By exploring various annotation strategies, we identified the potential of LMMs to achieve competitive performance, particularly in tasks involving ternary classification of emotions. Strategies that integrate multiple frames or aggregate annotations through majority voting significantly enhance the reliability of emotion recognition, offering a promising alternative to traditional supervised methods.\n\nWhile LMMs exhibit inherent ambiguity in distinguishing closely related emotion categories, particularly within the seven-class taxonomy, they achieve higher accuracy in simpler classification tasks. This highlights their utility in scenarios where efficiency and scalability are prioritized over fine-grained classification precision. Moreover, the cost-effective nature of zero-shot LMM annotation enables large-scale deployment, reducing the reliance on human annotators and minimizing operational costs.\n\nOur findings underscore the importance of leveraging aggregation techniques, temporal context, and task simplification to maximize the potential of LMMs in emotion annotation. Future work should focus on fine-tuning multimodal LMMs for emotion recognition, addressing ambiguities in classification, and expanding their application in real-world multimodal environments, such as driver attention detection, live streaming platform moderation, and health management systems. This research provides a foundation for advancing automated annotation techniques, fostering innovation in human-computer interaction and affective computing domains.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Safe And Responsible Innovation Statement",
      "text": "We hope to take this opportunity to remind researchers employing this method to use it responsibly and thoughtfully, particularly under the concept of superalignment. It is also important to note that LLMs may hallucinate in the label space, potentially introducing inaccurate or spurious outputs that could mislead downstream processes. In this paper, our tests were carried out by calling the API, and under the privacy policy, none of these data will be shared or used for training purposes. In our study, the testing was carried out via API calls; under the privacy policy, this data will neither be shared nor used for training purposes. We also recommend experimenting with on-premise models in the future to eliminate the risk of data leaks. Beyond that, when prototyping responsibly around this capability, it may be wise to target specific tasks and scenarios first rather than rolling it out directly to a wider market.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of Multi-Strategy Annotation Framework for Emotion Recognition in Video (Image/Segment) Data",
      "page": 4
    },
    {
      "caption": "Figure 2: ). Precision measures the",
      "page": 4
    },
    {
      "caption": "Figure 2: Precision Comparison for Seven-Class and Three-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Annotation Strategy A13.4.1": "precision\nrecall\nf1-score",
          "Annotation Strategy B13.4.2": "precision\nrecall\nf1-score",
          "Annotation Strategy C13.4.3": "precision\nrecall\nf1-score",
          "Annotation Strategy D13.4.4": "precision\nrecall\nf1-score"
        },
        {
          "Annotation Strategy A13.4.1": "0.61\n0.25\n0.35\n0.04\n0.18\n0.07\n0.17\n0.12\n0.14\n0.84\n0.39\n0.53\n0.27\n0.41\n0.32\n0.38\n0.65\n0.48\n0.15\n0.14\n0.14\n0.38\n0.35\n0.30\n0.29\n0.49\n0.38\n0.39",
          "Annotation Strategy B13.4.2": "0.69\n0.24\n0.36\n0.07\n0.23\n0.10\n0.17\n0.12\n0.14\n0.89\n0.42\n0.57\n0.27\n0.43\n0.34\n0.38\n0.68\n0.49\n0.17\n0.15\n0.16\n0.40\n0.38\n0.32\n0.31\n0.53\n0.40\n0.40",
          "Annotation Strategy C13.4.3": "0.63\n0.30\n0.40\n0.06\n0.26\n0.09\n0.19\n0.14\n0.16\n0.85\n0.55\n0.66\n0.29\n0.18\n0.23\n0.35\n0.76\n0.48\n0.12\n0.16\n0.14\n0.41\n0.36\n0.34\n0.31\n0.50\n0.41\n0.41",
          "Annotation Strategy D13.4.4": "0.64\n0.37\n0.47\n0.05\n0.10\n0.06\n0.23\n0.22\n0.22\n0.79\n0.62\n0.70\n0.31\n0.28\n0.29\n0.39\n0.71\n0.50\n0.20\n0.13\n0.16\n0.46\n0.37\n0.35\n0.34\n0.50\n0.46\n0.46"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Annotation Strategy A23.4.5": "precision\nrecall\nf1-score",
          "Annotation Strategy B23.4.6": "precision\nrecall\nf1-score",
          "Annotation Strategy C23.4.7": "precision\nrecall\nf1-score",
          "Annotation Strategy D23.4.8": "precision\nrecall\nf1-score"
        },
        {
          "Annotation Strategy A23.4.5": "0.69\n0.70\n0.69\n0.27\n0.41\n0.32\n0.72\n0.39\n0.50\n0.57\n0.56\n0.50\n0.51\n0.61\n0.57\n0.58",
          "Annotation Strategy B23.4.6": "0.70\n0.74\n0.72\n0.28\n0.41\n0.33\n0.79\n0.41\n0.54\n0.59\n0.52\n0.53\n0.64\n0.59\n0.60",
          "Annotation Strategy C23.4.7": "0.67\n0.87\n0.76\n0.32\n0.16\n0.22\n0.74\n0.53\n0.62\n0.65\n0.58\n0.52\n0.53\n0.62\n0.65\n0.62",
          "Annotation Strategy D23.4.8": "0.71\n0.80\n0.75\n0.31\n0.28\n0.29\n0.76\n0.58\n0.66\n0.65\n0.59\n0.55\n0.57\n0.64\n0.65\n0.64"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Natural affect data: Collection and annotation",
      "authors": [
        "Shazia Afzal",
        "Peter Robinson"
      ],
      "year": "2011",
      "venue": "New perspectives on affect and learning technologies",
      "doi": "10.1007/978-1-4419-9625-1_5"
    },
    {
      "citation_id": "2",
      "title": "Open-source large language models outperform crowd workers and approach ChatGPT in textannotation tasks",
      "authors": [
        "Meysam Alizadeh",
        "Ma√´l Kubli",
        "Zeynab Samei",
        "Shirin Dehghani",
        "Diego Bermeo",
        "Maria Korobeynikova",
        "Fabrizio Gilardi"
      ],
      "year": "2023",
      "venue": "Open-source large language models outperform crowd workers and approach ChatGPT in textannotation tasks",
      "arxiv": "arXiv:2307.02179101"
    },
    {
      "citation_id": "3",
      "title": "How to find trouble in communication",
      "authors": [
        "Anton Batliner",
        "Kerstin Fischer",
        "Richard Huber",
        "J√∂rg Spilker",
        "Elmar N√∂th"
      ],
      "year": "2003",
      "venue": "Speech communication",
      "doi": "10.1016/S0167-6393(02)00079-1"
    },
    {
      "citation_id": "4",
      "title": "An Empirical Evaluation of the GPT-4 Multimodal Language Model on Visualization Literacy Tasks",
      "authors": [
        "Alexander Bendeck",
        "John Stasko"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2024.3456155"
    },
    {
      "citation_id": "5",
      "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
      "authors": [
        "Emily Bender",
        "Timnit Gebru",
        "Angelina Mcmillan-Major",
        "Shmargaret Shmitchell"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT '21)",
      "doi": "10.1145/3442188.3445922"
    },
    {
      "citation_id": "6",
      "title": "Exploring Retrospective Annotation in Long-Videos for Emotion Recognition",
      "authors": [
        "Patr√≠cia Bota",
        "Pablo Cesar",
        "Ana Fred",
        "Hugo Pl√°cido Da Silva"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2024.3359706"
    },
    {
      "citation_id": "7",
      "title": "Asserting Real-Time Emotions through Cued-Recall: Is it Valid?",
      "authors": [
        "Anders Bruun",
        "Lai-Chong Law",
        "Matthias Heintz",
        "Poul Svante Eriksen"
      ],
      "year": "2016",
      "venue": "Proceedings of the 9th Nordic Conference on Human-Computer Interaction",
      "doi": "10.1145/2971485.2971516"
    },
    {
      "citation_id": "8",
      "title": "Positive computing: technology for wellbeing and human potential",
      "authors": [
        "A Rafael",
        "Dorian Calvo",
        "Peters"
      ],
      "year": "2014",
      "venue": "Positive computing: technology for wellbeing and human potential"
    },
    {
      "citation_id": "9",
      "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "10",
      "title": "FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs",
      "authors": [
        "Haodong Chen",
        "Haojian Huang",
        "Junhao Dong",
        "Mingzhe Zheng",
        "Dian Shao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia (Melbourne VIC, Australia) (MM '24). Association for Computing Machinery",
      "doi": "10.1145/3664647.3680827"
    },
    {
      "citation_id": "11",
      "title": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Jingdong Sun",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
      "arxiv": "arXiv:2406.11161"
    },
    {
      "citation_id": "12",
      "title": "National high performance computer technology act: SIG-GRAPH and nationl high-tech public policy issues",
      "authors": [
        "Donna Cox"
      ],
      "year": "1989",
      "venue": "ACM SIGGRAPH Computer Graphics"
    },
    {
      "citation_id": "13",
      "title": "Contested collective intelligence: Rationale, technologies, and a human-machine annotation study",
      "authors": [
        "Anna De Liddo",
        "√Ågnes S√°ndor",
        "Simon Shum"
      ],
      "year": "2012",
      "venue": "Computer Supported Cooperative Work"
    },
    {
      "citation_id": "14",
      "title": "Challenges in reallife emotion annotation and machine learning based detection",
      "authors": [
        "Laurence Devillers",
        "Laurence Vidrascu",
        "Lori Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2005.03.007"
    },
    {
      "citation_id": "15",
      "title": "Impact of annotator demographics on sentiment dataset labeling",
      "authors": [
        "Yi Ding",
        "Jacob You",
        "Tonja-Katrin Machulla",
        "Jennifer Jacobs",
        "Pradeep Sen",
        "Tobias H√∂llerer"
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "16",
      "title": "Joint Music and Language Attention Models for Zero-Shot Music Tagging",
      "authors": [
        "Xingjian Du",
        "Zhesong Yu",
        "Jiaju Lin",
        "Bilei Zhu",
        "Qiuqiang Kong"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10447760"
    },
    {
      "citation_id": "17",
      "title": "On the Influence of an Iterative Affect Annotation Approach on Inter-Observer and Self-Observer Reliability",
      "authors": [
        "Sidney Mello"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2457413"
    },
    {
      "citation_id": "18",
      "title": "EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition",
      "authors": [
        "Niki Maria",
        "Ioannis Patras"
      ],
      "year": "2024",
      "venue": "2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)",
      "doi": "10.1109/FG59268.2024.10581982"
    },
    {
      "citation_id": "19",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "Hatice Gunes",
        "Bj√∂rn Schuller",
        "Maja Pantic",
        "Roddy Cowie"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)",
      "doi": "10.1109/FG.2011.5771357"
    },
    {
      "citation_id": "20",
      "title": "A matter of annotation: an empirical study on in situ and self-recall activity annotations from wearable sensors",
      "authors": [
        "Alexander Hoelzemann",
        "Kristof Van Laerhoven"
      ],
      "year": "2024",
      "venue": "Frontiers in Computer Science",
      "doi": "10.3389/fcomp.2024.1379788"
    },
    {
      "citation_id": "21",
      "title": "PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3",
      "authors": [
        "Yushi Hu",
        "Hang Hua",
        "Zhengyuan Yang",
        "Weijia Shi",
        "Noah Smith",
        "Jiebo Luo"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV51070.2023.00277"
    },
    {
      "citation_id": "22",
      "title": "TV-SAM: Increasing Zero-Shot Segmentation Performance on Multimodal Medical Images Using GPT-4 Generated Descriptive Prompts Without Human Annotation",
      "authors": [
        "Zekun Jiang",
        "Dongjie Cheng",
        "Ziyuan Qin",
        "Jun Gao",
        "Qicheng Lao",
        "Abdullaev Bakhrom Ismoilovich",
        "Urazboev Gayrat",
        "Yuldashov Elyorbek",
        "Bekchanov Habibullo",
        "Defu Tang",
        "Linjing Wei",
        "Kang Li",
        "Le Zhang"
      ],
      "year": "2024",
      "venue": "Big Data Mining and Analytics",
      "doi": "10.26599/BDMA.2024.9020058"
    },
    {
      "citation_id": "23",
      "title": "I Didn't Know I Looked Angry\": Characterizing Observed Emotion and Reported Affect at Work",
      "authors": [
        "Harmanpreet Kaur",
        "Daniel Mcduff",
        "Alex Williams",
        "Jaime Teevan",
        "Shamsi Iqbal"
      ],
      "year": "0199",
      "venue": "Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3491102.3517453"
    },
    {
      "citation_id": "24",
      "title": "Exploiting Multi-CNN Features in CNN-RNN Based Dimensional Emotion Recognition on the OMG in-the-Wild Dataset",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.3014171"
    },
    {
      "citation_id": "25",
      "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research in the Wild",
      "authors": [
        "Jean Kossaifi",
        "Robert Walecki",
        "Yannis Panagakis",
        "Jie Shen",
        "Maximilian Schmitt",
        "Fabien Ringeval",
        "Jing Han",
        "Vedhas Pandit",
        "Antoine Toisoul",
        "Bj√∂rn Schuller",
        "Kam Star",
        "Elnar Hajiyev",
        "Maja Pantic"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2019.2944808"
    },
    {
      "citation_id": "26",
      "title": "Sofian Berrouiguet, et al. 2021. Machine learning and natural language processing in mental health: systematic review",
      "authors": [
        "Aziliz Le Glaz",
        "Yannis Haralambous",
        "Deok-Hee Kim-Dufor",
        "Philippe Lenca",
        "Romain Billot",
        "Jonathan Taylor C Ryan",
        "Jordan Marsh",
        "Michel Devylder",
        "Walter"
      ],
      "year": "2021",
      "venue": "Journal of medical Internet research"
    },
    {
      "citation_id": "27",
      "title": "Frame Level Emotion Guided Dynamic Facial Expression Recognition With Emotion Grouping",
      "authors": [
        "Bokyeung Lee",
        "Hyunuk Shin",
        "Bonhwa Ku",
        "Hanseok Ko"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "28",
      "title": "Recognition of negative emotions from the speech signal",
      "authors": [
        "C Lee",
        "S Narayanan",
        "R Pieraccini"
      ],
      "year": "2001",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding",
      "doi": "10.1109/ASRU.2001.1034632"
    },
    {
      "citation_id": "29",
      "title": "How to write effective prompts for large language models",
      "authors": [
        "Zhicheng Lin"
      ],
      "year": "2024",
      "venue": "Nature Human Behaviour"
    },
    {
      "citation_id": "30",
      "title": "EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis",
      "authors": [
        "Zhiwei Liu",
        "Kailai Yang",
        "Qianqian Xie",
        "Tianlin Zhang",
        "Sophia Ananiadou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
      "doi": "10.1145/3637528.3671552"
    },
    {
      "citation_id": "31",
      "title": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
      "authors": [
        "Mayug Maniparambil",
        "Chris Vorster",
        "Derek Molloy",
        "Noel Murphy",
        "Kevin Mcguinness",
        "Noel O'connor"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "doi": "10.1109/ICCVW60793.2023.00034"
    },
    {
      "citation_id": "32",
      "title": "Are large language models good annotators?",
      "authors": [
        "Jay Mohta",
        "Kenan Ak",
        "Yan Xu",
        "Mingwei Shen"
      ],
      "year": "2023",
      "venue": "Proceedings on \"I Can't Believe It's Not Better: Failure Modes in the Age of Foundation Models"
    },
    {
      "citation_id": "33",
      "title": "A Framework for Automatic Human Emotion Classification Using Emotion Profiles",
      "authors": [
        "Emily Mower",
        "J Maja",
        "Shrikanth Matariƒá",
        "Narayanan"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASL.2010.2076804"
    },
    {
      "citation_id": "34",
      "title": "Vision-Enabled Large Language and Deep Learning Models for Image-Based Emotion Recognition",
      "authors": [
        "Mohammad Nadeem",
        "Saquib Shahab",
        "Laeeba Sohail",
        "Faisal Javed",
        "Abdul Anwer",
        "Jilani Khader",
        "Khan Saudagar",
        "Muhammad"
      ],
      "year": "2024",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "35",
      "title": "Pre-trained ensemble model for identification of emotion during COVID-19 based on emergency response support system dataset",
      "authors": [
        "B Nimmi",
        "Janet",
        "Kalai Selvan",
        "Sivakumaran"
      ],
      "year": "2022",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "36",
      "title": "AI and human-robot interaction: A review of recent advances and challenges",
      "authors": [
        "Alexander Obaigbena",
        "Augustine Oluwaseun",
        "Ejike Lottu",
        "David Ugwuanyi",
        "Sonimitiem Boma",
        "Enoch Jacks",
        "Obinna Sodiya",
        "Daraojimba Donald"
      ],
      "year": "2024",
      "venue": "GSC Advanced Research and Reviews"
    },
    {
      "citation_id": "37",
      "title": "Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2929050"
    },
    {
      "citation_id": "38",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark",
        "Gretchen Krueger",
        "Ilya Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning (Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "39",
      "title": "Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development",
      "authors": [
        "Ranjan Sapkota",
        "Achyut Paudel",
        "Manoj Karkee"
      ],
      "year": "2024",
      "venue": "Zero-Shot Automatic Annotation and Instance Segmentation using LLM-Generated Datasets: Eliminating Field Imaging and Manual Annotation for Deep Learning Model Development",
      "arxiv": "arXiv:2411.11285"
    },
    {
      "citation_id": "40",
      "title": "Methods and tools for automatic or semi-automatic recognition of selected emotions using machine learning algorithms",
      "authors": [
        "Piotr Schneider",
        "Dariusz Miko≈Çajewski",
        "Anna Bryniarska",
        "Magdalena Igras-Cybulska",
        "Artur Cybulski",
        "Walery Marcinowicz",
        "Maciej Janiszewski",
        "Aleksandra Kawala-Sterniuk"
      ],
      "year": "2024",
      "venue": "Progress in Applied Electrical Engineering (PAEE)",
      "doi": "10.1109/PAEE63906.2024.10701433"
    },
    {
      "citation_id": "41",
      "title": "Annotation, modelling and analysis of fine-grained emotions on a stance and sentiment detection corpus",
      "authors": [
        "Hendrik Schuff",
        "Jeremy Barnes",
        "Julian Mohme",
        "Sebastian Pad√≥",
        "Roman Klinger"
      ],
      "year": "2017",
      "venue": "Proceedings of the 8th workshop on computational approaches to subjectivity, sentiment and social media analysis"
    },
    {
      "citation_id": "42",
      "title": "Automated comparative analysis of visual and textual representations of logographic writing systems in large language models",
      "authors": [
        "Peng Shao",
        "Ruichen Li",
        "Kai Qian"
      ],
      "year": "2024",
      "venue": "Automated comparative analysis of visual and textual representations of logographic writing systems in large language models"
    },
    {
      "citation_id": "43",
      "title": "Automated emotion recognition based on higher order statistics and deep learning algorithm",
      "authors": [
        "Rahul Sharma",
        "Ram Bilas Pachori",
        "Pradip Sircar"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control",
      "doi": "10.1016/j.bspc.2020.101867"
    },
    {
      "citation_id": "44",
      "title": "Howtocaption: Prompting llms to transform video annotations at scale",
      "authors": [
        "Nina Shvetsova",
        "Anna Kukleva",
        "Xudong Hong",
        "Christian Rupprecht",
        "Bernt Schiele",
        "Hilde Kuehne"
      ],
      "year": "2025",
      "venue": "European Conference on Computer Vision",
      "doi": "10.1007/978-3-031-72992-8_1"
    },
    {
      "citation_id": "45",
      "title": "Dystemo: Distant Supervision Method for Multi-Category Emotion Recognition in Tweets",
      "authors": [
        "Valentina Sintsova",
        "Pearl Pu"
      ],
      "year": "2016",
      "venue": "ACM Trans. Intell. Syst. Technol. 8, 1, Article",
      "doi": "10.1145/2912147"
    },
    {
      "citation_id": "46",
      "title": "Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection",
      "authors": [
        "Mohammad Soleymani",
        "Sadjad Asghari-Esfeden",
        "Yun Fu",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2436926"
    },
    {
      "citation_id": "47",
      "title": "Experiencing Annotation: Emotion, Motivation and Bias in Annotation Tasks",
      "authors": [
        "Teodor Stoev",
        "Kristina Yordanova",
        "Emma Tonkin"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)",
      "doi": "10.1109/PerComWorkshops56833.2023.10150364"
    },
    {
      "citation_id": "48",
      "title": "Large Language Models for Data Annotation and Synthesis: A Survey",
      "authors": [
        "Zhen Tan",
        "Dawei Li",
        "Song Wang",
        "Alimohammad Beigi",
        "Bohan Jiang",
        "Amrita Bhattacharjee",
        "Mansooreh Karami",
        "Jundong Li",
        "Lu Cheng",
        "Huan Liu"
      ],
      "year": "2024",
      "venue": "Large Language Models for Data Annotation and Synthesis: A Survey",
      "arxiv": "arXiv:2402.13446[cs.CL"
    },
    {
      "citation_id": "49",
      "title": "Large Language Models for Data Annotation and Synthesis: A Survey",
      "authors": [
        "Zhen Tan",
        "Dawei Li",
        "Song Wang",
        "Alimohammad Beigi",
        "Bohan Jiang",
        "Amrita Bhattacharjee",
        "Mansooreh Karami",
        "Jundong Li",
        "Lu Cheng",
        "Huan Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/2024.emnlp-main.54"
    },
    {
      "citation_id": "50",
      "title": "PDFChatAnnotator: A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs",
      "authors": [
        "Yi Tang",
        "Chia-Ming Chang",
        "Xi Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 29th International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "51",
      "title": "From humans to machines: can chatgpt-like llms effectively replace human annotators in nlp tasks",
      "authors": [
        "Surendrabikram Thapa",
        "Usman Naseem",
        "Mehwish Nasim"
      ],
      "year": "2023",
      "venue": "Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media"
    },
    {
      "citation_id": "52",
      "title": "Learning Spatiotemporal Features With 3D Convolutional Networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "53",
      "title": "A Closer Look at Spatiotemporal Convolutions for Action Recognition",
      "authors": [
        "Du Tran",
        "Heng Wang",
        "Lorenzo Torresani",
        "Jamie Ray",
        "Yann Lecun",
        "Manohar Paluri"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "54",
      "title": "Dimensional modeling of emotions in text with appraisal theories: Corpus creation, annotation reliability, and prediction",
      "authors": [
        "Enrica Troiano",
        "Laura Oberl√§nder",
        "Roman Klinger"
      ],
      "year": "2023",
      "venue": "Computational Linguistics",
      "doi": "10.1162/coli_a_00461"
    },
    {
      "citation_id": "55",
      "title": "Efficiently scaling up video annotation with crowdsourced marketplaces",
      "authors": [
        "Carl Vondrick",
        "Deva Ramanan",
        "Donald Patterson"
      ],
      "year": "2010",
      "venue": "Computer Vision-ECCV 2010: 11th European Conference on Computer Vision"
    },
    {
      "citation_id": "56",
      "title": "Unlocking the Emotional World of Visual Media: An Overview of the Science, Research, and Impact of Understanding Emotion",
      "authors": [
        "James Wang",
        "Sicheng Zhao",
        "Chenyan Wu",
        "Reginald Adams",
        "Michelle Newman",
        "Tal Shafir",
        "Rachelle Tsachor"
      ],
      "year": "2023",
      "venue": "Proc. IEEE",
      "doi": "10.1109/JPROC.2023.3273517"
    },
    {
      "citation_id": "57",
      "title": "FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Yiwen Huang",
        "Zhongying Liu",
        "Shuyong Gao",
        "Wei Zhang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "58",
      "title": "Machine Learning Driven Developments in Behavioral Annotation: A Recent Historical Review",
      "authors": [
        "Eleanor Watson",
        "Thiago Viana",
        "Shujun Zhang"
      ],
      "year": "2024",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "59",
      "title": "A survey of human-in-the-loop for machine learning",
      "authors": [
        "Xingjiao Wu",
        "Luwei Xiao",
        "Yixuan Sun",
        "Junhang Zhang",
        "Tianlong Ma",
        "Liang He"
      ],
      "year": "2022",
      "venue": "Future Generation Computer Systems"
    },
    {
      "citation_id": "60",
      "title": "Contextual Emotion Estimation from Image Captions",
      "authors": [
        "Vera Yang",
        "Archita Srivastava",
        "Yasaman Etesam",
        "Chuxuan Zhang",
        "Angelica Lim"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "62",
      "title": "LDRE: LLM-based Divergent Reasoning and Ensemble for Zero-Shot Composed Image Retrieval",
      "authors": [
        "Zhenyu Yang",
        "Dizhan Xue",
        "Shengsheng Qian",
        "Weiming Dong",
        "Changsheng Xu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "doi": "10.1145/3626772.3657740"
    },
    {
      "citation_id": "63",
      "title": "Building a large scale dataset for image emotion recognition: The fine print and the benchmark",
      "authors": [
        "Quanzeng You",
        "Jiebo Luo",
        "Jin Hailin",
        "Jianchao Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence",
      "doi": "10.1609/aaai.v30i1.9987"
    },
    {
      "citation_id": "64",
      "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
      "authors": [
        "Duzhen Zhang",
        "Yahan Yu",
        "Jiahua Dong",
        "Chenxing Li",
        "Dan Su",
        "Chenhui Chu",
        "Dong Yu"
      ],
      "year": "2024",
      "venue": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
      "arxiv": "arXiv:2401.13601[cs.CL"
    },
    {
      "citation_id": "65",
      "title": "Augmenting Image Annotation: A Human-LMM Collaborative Framework for Efficient Object Selection and Label Generation",
      "authors": [
        "He Zhang",
        "Xinyi Fu",
        "John Carroll"
      ],
      "year": "2025",
      "venue": "ICLR 2025 Workshop on Bidirectional Human-AI Alignment"
    },
    {
      "citation_id": "66",
      "title": "Decoding Fear: Exploring User Experiences in Virtual Reality Horror Games",
      "authors": [
        "He Zhang",
        "Xinyang Li",
        "Christine Qiu",
        "Xinyi Fu"
      ],
      "year": "2024",
      "venue": "Proceedings of the Eleventh International Symposium of Chinese CHI",
      "doi": "10.1145/3629606.3629646"
    },
    {
      "citation_id": "67",
      "title": "VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games",
      "authors": [
        "He Zhang",
        "Xinyang Li",
        "Yuanxi Sun",
        "Xinyi Fu",
        "Christine Qiu",
        "John Carroll"
      ],
      "year": "2024",
      "venue": "2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)",
      "doi": "10.1109/VR58804.2024.00054"
    },
    {
      "citation_id": "68",
      "title": "Harnessing the power of AI in qualitative research: Exploring, using and redesigning ChatGPT",
      "authors": [
        "He Zhang",
        "Chuhao Wu",
        "Jingyi Xie",
        "Yao Lyu",
        "Jie Cai",
        "John Carroll"
      ],
      "year": "2025",
      "venue": "Computers in Human Behavior: Artificial Humans",
      "doi": "10.1016/j.chbah.2025.100144"
    },
    {
      "citation_id": "69",
      "title": "When Qualitative Research Meets Large Language Model: Exploring the Potential of QualiGPT as a Tool for Qualitative Coding",
      "authors": [
        "He Zhang",
        "Chuhao Wu",
        "Jingyi Xie",
        "Fiona Rubino",
        "Sydney Graver",
        "Chanmin Kim",
        "John Carroll",
        "Jie Cai"
      ],
      "year": "2024",
      "venue": "When Qualitative Research Meets Large Language Model: Exploring the Potential of QualiGPT as a Tool for Qualitative Coding",
      "arxiv": "arXiv:2407.14925[cs.HC"
    },
    {
      "citation_id": "70",
      "title": "Active image labeling and its application to facial action labeling",
      "authors": [
        "Lei Zhang",
        "Yan Tong",
        "Qiang Ji"
      ],
      "year": "2008",
      "venue": "Computer Vision-ECCV 2008: 10th European Conference on Computer Vision"
    },
    {
      "citation_id": "71",
      "title": "Toward Label-Efficient Emotion and Sentiment Analysis",
      "authors": [
        "Sicheng Zhao",
        "Xiaopeng Hong",
        "Jufeng Yang",
        "Yanyan Zhao",
        "Guiguang Ding"
      ],
      "year": "2023",
      "venue": "Proc. IEEE",
      "doi": "10.1109/JPROC.2023.3309299"
    },
    {
      "citation_id": "72",
      "title": "LEVA: Using Large Language Models to Enhance Visual Analytics",
      "authors": [
        "Yuheng Zhao",
        "Yixing Zhang",
        "Yu Zhang",
        "Xinyi Zhao",
        "Junjie Wang",
        "Zekai Shao",
        "Cagatay Turkay",
        "Siming Chen"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Visualization and Computer Graphics",
      "doi": "10.1109/TVCG.2024.3368060"
    },
    {
      "citation_id": "73",
      "title": "General Facial Representation Learning in a Visual-Linguistic Manner",
      "authors": [
        "Yinglin Zheng",
        "Hao Yang",
        "Ting Zhang",
        "Jianmin Bao",
        "Dongdong Chen",
        "Yangyu Huang",
        "Lu Yuan",
        "Dong Chen",
        "Ming Zeng",
        "Fang Wen"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "74",
      "title": "Machine learning on big data: Opportunities and challenges",
      "authors": [
        "Lina Zhou",
        "Shimei Pan",
        "Jianwu Wang",
        "Athanasios Vasilakos"
      ],
      "year": "2017",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2017.01.026"
    }
  ]
}