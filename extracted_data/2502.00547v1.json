{
  "paper_id": "2502.00547v1",
  "title": "Milmer: A Framework For Multiple Instance Learning Based Multimodal Emotion Recognition",
  "published": "2025-02-01T20:32:57Z",
  "authors": [
    "Zaitian Wang",
    "Jian He",
    "Yu Liang",
    "Xiyuan Hu",
    "Tianhao Peng",
    "Kaixin Wang",
    "Jiakai Wang",
    "Chenlong Zhang",
    "Weili Zhang",
    "Shuang Niu",
    "Xiaoyang Xie"
  ],
  "keywords": [
    "Human-computer interaction",
    "Emotion recognition",
    "Multiple instance learning",
    "Physiological signals",
    "Human facial expressions"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions play a crucial role in human behavior and decision-making, making emotion recognition a key area of interest in human-computer interaction (HCI). This study addresses the challenges of emotion recognition by integrating facial expression analysis with electroencephalogram (EEG) signals, introducing a novel multimodal framework-Milmer. The proposed framework employs a transformer-based fusion approach to effectively integrate visual and physiological modalities. It consists of an EEG preprocessing module, a facial feature extraction and balancing module, and a crossmodal fusion module. To enhance visual feature extraction, we fine-tune a pre-trained Swin Transformer on emotion-related datasets. Additionally, a cross-attention mechanism is introduced to balance token representation across modalities, ensuring effective feature integration. A key innovation of this work is the adoption of a multiple instance learning (MIL) approach, which extracts meaningful information from multiple facial expression images over time, capturing critical temporal dynamics often overlooked in previous studies. Extensive experiments conducted on the DEAP dataset demonstrate the superiority of the proposed framework, achieving a classification accuracy of 96.72% in the four-class emotion recognition task. Ablation studies further validate the contributions of each module, highlighting the significance of advanced feature extraction and fusion strategies",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions, as an integral part of human daily life, significantly influence individual behavior, decision making, social interactions, and mental health. In recent years, with the rapid advancement of human-computer interaction (HCI) technologies, emotion recognition has emerged as a highly prominent research area. Extensive studies have explored this topic through various data modalities. The development of deep learning methods, coupled with the utilization of large-scale multimodal datasets such as DEAP  [1]  have revolutionized the field of emotion recognition. These advancements have paved the way for detecting and interpreting human emotions with unprecedented accuracy.\n\nEmotion recognition relies broadly on two primary modalities of human data: visual data such as images or videos, and physiological signals such as electroencephalogram  (EEG) . Research in this field has first focused on vision-based approaches, using facial images and videos, as evident in prior studies  [2, 3, 4, 5] . Facial expressions, as direct and easily captured indicators of human emotions, have been extensively studied. However, facial expressions in datasets commonly used in facial emotion recognition often exhibit exaggerated or highly recognizable features. These expressions, while easily identifiable, are not representative of real-life emotional expressions, where emotions are not always expressed in an exaggerated or obvious way. In fact, people often display little to no facial expression in response to their emotions. This discrepancy poses a significant challenge in accurately recognizing emotions in real-world scenarios and highlights a major weakness in the field of facial emotion recognition.\n\nIn contrast, physiological signals such as EEG offer distinctive insights into human emotions by reflecting intrinsic responses that occur instinctively and are not subject to conscious control, providing a more accurate representation of genuine emotions  [6, 7, 8] . Despite their potential, the adoption of deep learning strategies for physiological signals remains limited, hindered by challenges such as hardware inconsistencies, the differences in data preprocessing, and the high cost of bio-sensing data acquisition.\n\nBeyond single modality approaches, recent studies have highlighted the advantages of multimodal affective computing techniques, which integrate vision and physiological data to achieve improved emotion recognition performance  [9, 10, 11] . Each modality contributes unique strengths that complement the other, and their fusion enhances the quality of the extracted features, resulting in a more comprehensive representation of human emotions. Each modality contributes unique strengths that complement each other, and their fusion enhances the quality of the extracted features, resulting in a more comprehensive representation of human emotions, which significantly improves the accuracy compared to single modality approaches  [12] . Moreover, in multimodal datasets, facial expression data are typically derived from videos of participants watching stimulus videos, where their expressions are more subdued and natural. This makes the facial expressions in multimodal studies closer to real-life scenarios compared to those captured in single modality datasets, where expressions are often more exaggerated.\n\nHowever, despite the proven value of using EEG and facial expressions in multimodal emotion recognition, there remains a substantial gap in research in this area, leaving considerable opportunities for further exploration  [12] . Current research tends to focus more on feature extraction within each modality independently, while neglecting the crucial task of modality fusion. Many approaches rely on simple methods such as concatenation or decision-tree-based fusion techniques, yet EEG and facial expressions represent two fundamentally different modalities. Such simplistic fusion strategies often lead to the loss of valuable information and fail to fully utilize the complementary features extracted from both modalities. This study argues that these limitations are a significant reason why existing models have not fully realized their potential, and that more sophisticated, comprehensive fusion methods should become the standard in future research.\n\nIt is worth noting that, due to the inherently higher complexity of visual information compared to EEG signals, most prior research has placed a stronger emphasis on facial expression analysis. However, this study argues that facial expressions in multimodal datasets tend to be more constrained and neutral, closely resembling real-world conditions where distinguishing subtle emotions becomes particularly challenging. In such scenarios, EEG signals play an irreplaceable role, as they can capture underlying emotional states that are not easily reflected in facial expressions. This highlights the equal importance of both modalities and underscores the critical need for effective multimodal fusion strategies  [13] . Unfortunately, previous studies often overlook this aspect, applying overly simplistic fusion techniques that fail to effectively combine these two fundamentally different types of data.\n\nIn the feature extraction process for the facial expression modality, CNNbased architectures are predominantly used in this field, often capturing localized features rather than global ones. Additionally, research frequently relies on single facial expression images rather than multiple images over time to assess emotions, thereby failing to fully utilize the available information. These two limitations reduce the quality of visual feature representation and, ultimately, impact emotion classification performance.\n\nTo conclude, several limitations have been identified in the current field: (1) Despite its potential, the field remains underexplored, with relatively few studies dedicated to fully integrating facial expression and EEG signals for emotion recognition. (2) Most research efforts focus primarily on feature extraction from individual modalities, with limited investigation into effective multimodal fusion strategies. (3) Current approaches to facial expression feature extraction rely on outdated methods and fail to consider using multiple images over time for emotion recognition, missing important temporal information.\n\nTo address these challenges, this paper proposes a hybrid framework named Milmer for Multiple Instance Learning based Multimodal Emotion Recognition, which includes an EEG preprocessing module, a facial feature extraction and balancing module, and a transformer-based modality fusion module. The key contributions of this paper are as follows:\n\n1. Multimodal emotion recognition using facial expressions and EEG signals remains underexplored. This study fills this gap by providing a comprehensive exploration of facial expression and EEG signal integration, along with open-sourcing the code to facilitate further research in this area; also leverages a pre-trained Swin Transformer, fine-tuned on emotionrelated datasets, to extract high-quality features  [14] . Additionally, a cross-attention mechanism is used to balance the features of facial images, ensuring a more comprehensive and effective fusion of both modalities.\n\nThe rest of this paper is organized as follows. Section 2 introduces related work on the topic. Section 3 outlines the structure of the proposed method. Section 4 details the experimental setup and discusses the results. Finally, Section 5 draws the conclusions and presents future research directions.",
      "page_start": 2,
      "page_end": 5
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg-Based Emotion Recognition",
      "text": "Electroencephalogram (EEG)-based emotion recognition utilizes brain activity signals to classify emotional states, offering an intrinsic and unconscious reflection of emotional responses. Deep learning models such as convolutional neural networks (CNNs)  [15]  and recurrent neural networks (RNNs)  [16]  have been extensively applied for feature extraction and classification. More recently, transformer-based approaches  [17, 6]  have demonstrated strong potential in modeling temporal dynamics and long-range dependencies in EEG signals. Advanced frameworks such as graph neural networks (GNNs)  [18]  have also been proposed to capture the spatial interrelations among EEG channels effectively.\n\nHowever, EEG recording is associated with several challenges. Key obstacles include the presence of internal and external artifacts, such as eye movements and muscle activity, which can interfere with signal accuracy. Furthermore, recording EEG data for extended periods can introduce noise and variability, which complicates the extraction of consistent emotional features  [19] . These factors limit the effectiveness of EEG as a standalone modality for emotion recognition, highlighting the need for complementary approaches.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Facial Emotion Recognition",
      "text": "Facial emotion recognition (FER) involves analyzing human facial expressions from images or videos to classify emotions. Deep learning methods, particularly convolutional neural networks (CNNs), have significantly advanced this field by automating facial feature extraction. For static images, 2D CNNs effectively extract spatial features, achieving high accuracy in emotion classification  [20] . For video data, methods such as Conv3D  [21]  and ConvLSTM  [22]  are employed to capture both spatial and temporal information, improving recognition performance. Recently, transformer architectures  [23, 24]  have gained attention for their ability to encode context-aware features through attention mechanisms, offering superior results in dynamic emotion analysis.\n\nThese advancements have made FER a cornerstone of emotion recognition research. However, relying solely on facial images or videos has inherent limitations, particularly in real-world scenarios where emotions are often subtle, neutral, or influenced by contextual factors that are not captured visually  [25] . The absence of complementary data, such as physiological signals, restricts FER methods from fully understanding the underlying emotional states. These limitations highlight the necessity of exploring multimodal approaches, which integrate diverse data sources to achieve more accurate and robust emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Multimodal emotion recognition, which integrates facial expressions and EEG signals, has gained significant attention due to its ability to provide more accurate emotion classification by combining complementary information. However, research in this area still lacks sufficient exploration of effective fusion techniques, which limits the potential of multimodal models.\n\nSeveral approaches have been proposed to fuse EEG and facial expression data. Early methods often relied on simple fusion strategies such as decision trees or voting mechanisms to combine the outputs of each modality  [26, 10] . While these approaches are straightforward, they fail to effectively utilize the rich, modality-specific features extracted from each source, often resulting in suboptimal performance. Methods like Deep Canonical Correlation Analysis (DeepCCA), have attempted to improve feature correlation between modalities, yet they still fall short of fully exploiting the complex, individual features of each modality  [27] . Low-rank fusion techniques have also been explored, offering a more efficient way to combine modality-specific features  [28] . While these methods are computationally efficient, they are not advanced enough to capture the complex interactions between the two modalities.\n\nConcatenation-based methods, which combine the features of both modalities into a single vector, are also widely used but can lead to information loss  [29, 11, 2] . In contrast, more sophisticated methods, such as cross-attention mechanisms in transformers, allow the model to focus on the most relevant features from each modality, enhancing fusion accuracy and improving overall performance  [5] . Despite these advancements, the field still lacks comprehensive, effective fusion strategies that can fully harness the strengths of both EEG and facial expression data, highlighting the need for further research in this area.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Multiple Instance Learning",
      "text": "Multiple Instance Learning (MIL) is a variation of supervised learning in which a class label is assigned to a bag of instances rather than individual instances. Unlike traditional supervised learning, where each input (such as an image) is labeled with a specific category, MIL is applied in scenarios where the labeling is more ambiguous. In MIL, a \"bag\" contains multiple instances, and only the overall class of the bag is provided, not individual labels for each instance. This approach is particularly useful when data annotations are weak or incomplete, a common occurrence in real-world tasks.\n\nTraditional MIL typically relies on pooling methods like max pooling or average pooling to combine the individual instances in a bag and make a prediction. However, these pooling techniques often fail to capture the importance of specific instances, as they treat all instances equally. In contrast, attention-based MIL (AMIL)  [30] introduces an attention mechanism that assigns different weights to instances, allowing the model to focus more on the most relevant instances while ignoring less informative ones. This approach significantly improves MIL performance by enabling more precise feature extraction and decision-making.\n\nIn the field of multimodal emotion recognition using EEG and facial expressions, the typical approach involves associating a short segment of EEG data (e.g., 3 seconds) with a single facial expression image, which is then used to represent the entire 3-second window. However, this approach is inherently flawed as it may overlook valuable temporal and subtle details present within the sequence of images. A 3-second video, for instance, should be understood as a \"bag\" of instances, with each individual frame or image representing a potential instance. Relying on just one image to represent the entire bag could result in the loss of critical information, especially in capturing emotional variations that occur over time.\n\nBy adopting MIL, we can leverage all instances within the video to create a more balanced and accurately represent the entire segment. MIL allows for",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Transformer Multimodal Encoder",
      "text": "Fully Connected Layer",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "×6",
      "text": "Swin Transformer Encoder\n\nemotional pretrained weights",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Cross Attention",
      "text": "Visual Tokens EEG Tokens",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "×2",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Instances Features Within A Bag",
      "text": "Top-K selection",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Selected Instances",
      "text": "Learnable Query Embeddings ...",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Multiple Instance Learning",
      "text": "...",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Attention Scores",
      "text": "time slice a more comprehensive understanding by treating the entire video as a bag of instances, thus enhancing the emotion recognition process. Despite the potential benefits, very few studies have recognized the value of MIL in this context  [31, 32] . This research aims to incorporate MIL to better capture and represent the emotional content in the facial expression modality.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Eeg Preprocessing Module Facial Feature Extraction And Balancing Module",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Overview",
      "text": "As discussed earlier, many existing methods overlook the importance of effective modality fusion, relying on simple concatenation or decision-treebased approaches that fail to fully exploit the complementary nature of different modalities. Transformer  [33] , an emerging neural network architecture initially developed for machine translation tasks, has recently achieved remarkable success in the field of natural language processing (NLP). However, its potential application to emotion recognition tasks involving EEG and facial modalities remains underexplored.\n\nIn this work, we propose a transformer-based framework to achieve effective cross-modal fusion. As illustrated in Fig.  1 , the proposed architecture consists of four main modules: (1) EEG preprocessing module, (  2 ) facial feature extraction and balancing module, and (3) modality fusion module.\n\nIn the first module, noise and artifacts in the EEG signals are filtered out to ensure clean input data. In the second module, Swin Transformer  [14]  is used to extract features from facial images. A MIL approach is then introduced to better represent the facial expressions by selecting the most representative frames. To further reduce the feature dimension and highlight key information, a cross-attention mechanism  [33, 34]  is applied to the Swin Transformer output. Then, both the EEG and facial features are embedded and fed into a transformer for cross-modal fusion. Finally, the fused features, which encapsulate frequency, temporal, and spatial information, are passed through a fully connected classifier to produce the final emotion classification results. The details of these three modules are explained in the subsequent sections.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Eeg Preprocessing Module",
      "text": "The majority of EEG signals are concentrated within the frequency range of 1 Hz-50 Hz. Therefore, a bandpass filter with a passband of 1 Hz-50 Hz is applied in the proposed model. This filtering step serves two primary purposes: (1) removing low-frequency baseline drift, electrocardiographic (ECG) interference, and other high-frequency noise, and (2) effectively mitigating the most prominent power line interference, which typically occurs at 50 Hz in China.\n\nHowever, EEG signals often overlap with electrooculogram (EOG) and electromyogram (EMG) signals within the same frequency band, making a single bandpass filter insufficient for completely eliminating these artifacts. To address this issue, Independent Component Analysis (ICA) is employed. ICA is a computational technique that decomposes a multivariate signal into a set of statistically independent components. By maximizing the non-Gaussianity of the sources, ICA separates mixed signals into independent sources, which helps to isolate and remove artifacts such as EOG and EMG from EEG signals.\n\nThe ICA process involves estimating a matrix that unmixes the data into independent components, which are then identified and removed based on their spatial and temporal characteristics. Specifically, the Fast ICA algorithm from the MNE library  [35]  is utilized to perform this decomposition. For ICA processing, continuous EEG data from a single subject (without segmenting it into epochs) is used, with all other settings left at their default values in the MNE library. By combining bandpass filtering and ICA, this preprocessing module effectively ensures cleaner EEG signals for subsequent tasks.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Facial Feature Extraction And Balancing Module",
      "text": "This study significantly advances the visual feature extraction process compared to prior research in the field. First, in contrast to traditional CNN-based methods prevalent in the field, we utilize the advanced Swin Transformer fine-tuned on emotion classification dataset as the backbone for feature extraction.\n\nPrevious studies typically align a single facial image with a segment of EEG data (commonly 3 seconds), assuming that a single frame can represent the entire video segment. However, it is difficult to determine which frame best encapsulates the information from a 3-second video. To address this limitation, we introduce MIL into this domain. By extracting multiple frames from the video segment and leveraging MIL, the most representative K frames are selected. This approach ensures a more comprehensive and nuanced representation of the facial modality, capturing richer emotional information.\n\nLastly, this study addresses the critical challenge of balancing the contributions of the modality during the fusion stage. Selecting multiple frames through MIL inevitably introduces far more visual tokens compared to the EEG modality, which could lead to the model disproportionately focusing on facial features while neglecting EEG information. To mitigate this imbalance, we employ a cross-attention-based mechanism, enabling the K frames to interact and learn from each other while reducing their dimensionality before being entered into the fusion module. This step ensures balanced and effective multimodal integration.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Swin Feature Extraction",
      "text": "Compared to traditional CNN-based methods commonly used in this field, the Swin Transformer demonstrates superior capabilities in visual feature extraction. By employing a hierarchical architecture with shifted window attention mechanisms, the Swin Transformer captures both local and global dependencies efficiently. This design allows it to retain the fine-grained details of local features while incorporating contextual information across larger regions, surpassing CNNs that rely on fixed local receptive fields and hierarchical feature aggregation. This enables the Swin Transformer to extract richer and more robust features, providing a solid foundation for subsequent multimodal fusion tasks.\n\nIn this module, we use a 6-layer Swin Transformer as the image encoder, initialized with weights fine-tuned on emotion recognition dataset. The Swin Transformer serves as the backbone for extracting visual features, generating M feature tokens, each with dimension D, from the input facial images. These feature tokens retain hierarchical and contextual information, making them highly suitable for subsequent processing in the fusion stage.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Multiple Instance Learning",
      "text": "In the classical supervised 2-class classification problem, the objective is to develop a model that predicts a target variable y ∈ 0, 1 for a given instance, x ∈ R D . However, in the context of the MIL problem, the model is presented with a bag of instances X = {x 1 , x 2 , . . . , x q }, where the individual instances are independent of each other. The bag is associated with a single label Y belonging to one of the two classes, i.e., y ∈ 0, 1. Furthermore, although each instance within the bag is assumed to have its own label y 1 , y 2 , . . . , y q , these labels are partially indicative of the bag's overall label Y but cannot be equated with it.\n\nIn MIL research, there is often a focus on understanding the relationships between instances within a bag and selecting a method that represents the bag in a more balanced or universal manner. The most common approach for this is pooling, such as average pooling or max pooling. With the advent of attention mechanisms, attention-based learnable pooling methods have emerged, one of the most notable being AMIL  [30] . AMIL presents a simple yet effective approach that significantly enhances accuracy. It proposes an attention-based weighting method that calculates weights a 1 , a 2 , . . . , a q for the instances x 1 , x 2 , . . . , x q , then performs a weighted sum of the instances using these weights to obtain the pooled result for subsequent classification.\n\nThis study is primarily based on the AMIL approach, but with a key difference. While AMIL directly performs the weighted sum and classification after obtaining the weights, our method, which involves subsequent modality fusion with EEG data, avoids pooling into a single dimension at this stage. Therefore, instead of directly pooling, we select the top-K embeddings with the highest a q values that best represent the subject's emotional state, which are then used for fusion. K is a hyperparameter representing the number of selected images. The formula is as follows:\n\nwhere T is the set of indices corresponding to the top K largest a q , defined as:\n\nand the value of a q is computed as:\n\nwhere w ∈ R L×1 and V ∈ R L×M are parameters. This process allows the selected top-K images to effectively represent the entire bag.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Cross Attention",
      "text": "In multi-modal fusion, the number of tokens provided by each modality plays a critical role in determining the quality of the fusion process. If the facial modality provides an excessively large number of tokens compared to the EEG modality, the transformer used for fusion may focus disproportionately on the visual features while neglecting those from the EEG signals. This imbalance can hinder the overall classification performance.\n\nTo address this, the cross-attention mechanism is employed to adjust the dimensionality of the feature vectors output by Swin. Specifically, the cross-attention mechanism compresses the M visual tokens into N learnable query tokens, where N ≪ M. This adjustment enables the visual tokens to match the token count of the EEG modality, achieving a more balanced and harmonious representation for fusion. This process ultimately contributes to improved classification performance by ensuring that both modalities are equally represented in the downstream tasks. As illustrated in Fig.  2 , the cross-attention mechanism shows as follows:\n\nThe learnable query tokens (N, D) and the visual tokens (M, D) extracted from the Swin transformer are first passed through linear layers to generate the query Q, key K, and value V matrices, which are expressed as:\n\nwhere X q represents the learnable query tokens of size (N, D), and X v represents the visual tokens extracted from the Swin transformer of size (M, D), with W Q , W K , and W V as the corresponding learnable weight matrices.\n\nThe attention output is then computed via the scaled dot-product attention mechanism:\n\nwhere A is the reduced feature representation of size (N, D). This dimensionality reduction condenses the most relevant spatial and contextual information into a set of visual tokens, which helps balance the representation of the EEG and facial expression modalities, ensuring more harmonious multimodal fusion in subsequent tasks while reducing computational overhead.\n\nTo summarize, the facial feature extraction and balancing module utilizes the Swin Transformer for global feature representation, employs an AMILbased method to select the most representative Top-K features, and incorporates a cross-attention mechanism to align and compress the visual tokens. This approach ensures a balanced token representation between the visual and EEG modalities, thereby enhancing the quality of multimodal fusion and improving overall classification performance.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Fusion Module",
      "text": "In the proposed framework, the Fusion Module is designed to effectively combine information from EEG signals and facial features while preserving modality-specific characteristics. To achieve this, we use modal type embedding, position embedding, and a classification token (CLS token).\n\nModal Type Embedding: To differentiate between the two modalities, we introduce modal type embeddings. These are learnable vectors unique to each modality, appended to their respective input features. Modal-type embeddings provide the model with explicit information about the origin of each input token, enabling the Transformer to process and fuse modalityspecific features more effectively. For instance, all EEG tokens are associated with a dedicated embedding vector, while facial tokens are assigned a distinct embedding, ensuring clear separation in the shared representation space.\n\nPosition Embedding: Position embeddings are applied to retain the order and structure of the input tokens for both EEG and facial features. EEG position embeddings encode temporal dependencies by representing the timestep information of each token, while facial position embeddings capture the spatial relationships among visual tokens extracted by Swin, preserving global contextual information essential for emotion recognition.\n\nCLS Token for Classification: A CLS token is appended to the input sequence, serving as a global representation of the fused features across modalities. During the Transformer encoding process, the CLS token aggregates information from both EEG and facial feature tokens through attention mechanisms. In the output stage, the CLS token is passed to a fully connected layer for emotion classification. This approach allows the model to condense multimodal information into a compact and informative representation, optimizing the subsequent classification task.\n\nFinally, the fused features are passed through a fully connected layer to produce the classification results. The cross-entropy loss is employed as the objective function to train the label classifier, and it is defined as follows:\n\nwhere n is the number of classes; y ji and ŷji denote the true and predicted labels in a batch, respectively.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Experiments And Result",
      "text": "In this section, we present the experimental results. First, we provide an overview of the DEAP dataset and describe the preprocessing methods applied to the data. Subsequently, we conduct extensive experiments on the DEAP dataset to evaluate and compare the classification performance of various multimodal fusion strategies. Furthermore, we investigate the impact of the token quantities from the two modalities on classification performance during multimodal fusion. Finally, we perform ablation studies to demonstrate the significance of each component in our proposed network architecture.\n\nThanks to the fast convergence of Milmer, the experimental trends and results become apparent within just 100 epochs. Loshchilov et al.  [36]  introduced the cosine learning rate decay strategy, which has been widely validated for its effectiveness. All experiments in this section are conducted using the cosine learning rate decay strategy, with results reported over 100 epochs.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Dataset And Settings",
      "text": "We employ the widely recognized DEAP dataset, which is commonly used in the field of multimodal affective computing, to evaluate the effectiveness of our proposed architecture. In the DEAP dataset, participants' emotions are elicited by watching music videos in a controlled laboratory environment, while their facial videos and bio-sensing signals are recorded synchronously. Following the viewing session, participants rate their emotional responses based on four dimensions: valence, arousal, dominance, and liking. Most studies in this area primarily focus on valence and arousal for emotion evaluation, and our study adheres to this convention.\n\nThe DEAP dataset provides continuous emotional ratings on a scale from 1 to 9, which are typically grouped into two, three, or four categories. For studies that focus on two-or three-class classification, valence and arousal are usually treated as independent dimensions. In the two-class approach, both dimensions are divided into high and low levels, while the three-class approach includes an additional neutral category. In contrast, the four-class classification treats valence and arousal as interrelated dimensions, dividing each of them into high and low levels, which results in a total of four distinct categories.\n\nThese three classification schemes-two-class, three-class, and fourclass-are all commonly employed in the field. To enable comparison with prior research, we conduct experiments using all three classification schemes, referred to as DEAP-2, DEAP-3, and DEAP-4. The four-class scheme, being the most complex, captures the most nuanced emotional distinctions and is better suited for demonstrating differences in experimental results. In contrast, the two-class approach, though widely used, simplifies the task but may lead to overfitting and less persuasive results due to its reduced complexity. Therefore, the main results presented in this paper focus on the DEAP-4 experiment, while DEAP-2 and DEAP-3 are used primarily for comparison with other studies.\n\nIn the DEAP-4 setting, we use 5 as a threshold to classify emotions into four quadrants of the valence-arousal space: High Arousal High Valence (HAHV), High Arousal Low Valence (HALV), Low Arousal High Valence (LAHV), and Low Arousal Low Valence (LALV). The classification rule is defined as follows:\n\nHere, a and v represent the arousal and valence scores, respectively.\n\nFor the DEAP-3 setting, we classify the emotional ratings into three categories: low (ratings from 1 to 3), medium (ratings from 4 to 6), and high (ratings from 7 to 9). In the DEAP-2 setting, the classification is simplified into two categories: low (ratings below 5) and high (ratings above 5).\n\nThe DEAP dataset consists of data from 32 participants, among whom only 22 provided facial image information. Each participant completed 40 trials. Notably, data from Participant 11 was excluded due to the absence of three facial expression videos, a decision made to prevent potential misalignment issues within the dataset.\n\nIn this study, the data is segmented into 3-second time windows, with EEG signals from each segment paired with 10 evenly spaced facial frames extracted over the 3-second duration. Google's Mediapipe library, built upon the efficient BlazeFace framework  [37] , is utilized for face detection. The detected faces are resized to 224×224 pixels using center padding, with black borders added as needed to maintain aspect ratio. For the 32-channel EEG signals, preprocessing includes a bandpass filter within the 1-50 Hz range, followed by artifact removal using Fast ICA to eliminate noise caused by ocular and muscular activities, ensuring cleaner signals for subsequent analysis. Since the DEAP dataset has an EEG sampling rate of 128 Hz, resulting in 384 data points over a 3-second period, we employ a multi-layer perceptron (MLP) to upsample the signal length to 768, ensuring alignment with the image data.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Comparison Experiments",
      "text": "The classification results of our proposed method, along with other studies that adopt the four-class approach, are presented in Table  1 . The results of studies employing the three-class approach are summarized in Table  2 , while the binary classification results for valence and arousal are shown in Table  3 . The table presents various methods categorized by modality, including EEG-only and multimodal approaches, the number of emotion classes classified, and their corresponding accuracy and F1 scores. This comparison provides insights into the effectiveness of different methodologies under the DEAP-4 classification scheme.\n\nTable  1  first presents three highly cited studies that focus exclusively on the EEG modality, without incorporating facial expressions, for emotion recognition. Among these studies, the highest performance achieved is an accuracy of 81.25% and an F1 score of 78.56%.\n\nFollowing this, we compare several multimodal emotion recognition studies, primarily focusing on frameworks that integrate EEG and facial features. Notably, Cimtay et al. and Jung et al. are two highly cited works in this category, achieving accuracies of 53.87% and 54.22%, respectively, in the four-class classification task, which are lower compared to other studies. The underperformance of these models can be attributed to their reliance on conventional CNN-based feature extraction methods for each modality, which may not effectively capture the complex characteristics of EEG and facial expressions. Additionally, Cimtay et al. employ a decision tree mechanism for feature fusion, which struggles with high-dimensional and complex data, leading to suboptimal performance.\n\nA very recent study by Lee et al. presents a multimodal emotion recognition framework that effectively leverages contrastive learning and cross-modal attention mechanisms to enhance inter-modal feature alignment. Their use of contrastive learning stands out as a significant strength, enabling the model to learn richer and more discriminative multimodal representations. However, their approach to visual feature extraction is relatively simplistic, relying solely on a ViT-based encoder without further refinement, which may limit the potential of their model. Consequently, their framework achieves an accuracy of 83.20% and an F1 score of 84.10%.\n\nFinally, our proposed framework, Milmer, achieves the best results with an accuracy of 96.72% and an F1 score of 96.71%, demonstrating the superiority of our approach. These results validate the effectiveness of our feature extraction and fusion strategies in improving emotion recognition performance.\n\nTable  2  presents the experimental results of various multimodal learning studies conducted on the DEAP-3 classification scheme. Wu et al. proposed an innovative approach by leveraging Action Units (AUs) to represent facial expressions and designing a multimodal learning framework based on CNN and LSTM, achieving an accuracy of 71.48%. However, their feature fusion strategy remains relatively simplistic, and the use of AUs alone may not fully capture the richness of facial expression information, potentially limiting the model's overall performance. Two recent studies, Husformer and Muhammad et al., have demonstrated significant advancements, achieving accuracies exceeding 90%, thereby outperforming previous works. Husformer employs a cross-attention-based fusion strategy to effectively integrate multimodal information, while Muhammad et al. utilize DeepCCA for feature fusion. These advanced fusion techniques contribute to their superior performance. However, Husformer does not incorporate facial expression features, which are crucial emotional cues; thus, relying solely on physiological signals may restrict its ability to comprehensively capture the emotional states of subjects. On the other hand, Muhammad et al.'s approach, despite its strong fusion capabilities, faces challenges in feature extraction from individual modalities, which could limit further performance improvements.\n\nFinally, our proposed framework, Milmer, achieves the highest performance on the DEAP-3 dataset, with an accuracy of 96.84% and an F1 score of 96.89%, demonstrating its superiority over existing approaches.\n\nTo facilitate comparisons with the most studies in the field, we conducted experiments on the DEAP-2 dataset, as shown in Table  3 . The overall accuracy of the two-class task is generally higher than that of the previously discussed three and four-class tasks. Among the listed studies, Wu et al. and Jung et al., which have been previously discussed, employ multiple classification strategies and will not be elaborated further here.\n\nHuang et al. adopt a CNN-based feature extraction approach and utilize a weighted summation strategy for modality fusion, achieving arousal and  of 97.91% and 97.84%, respectively. Furthermore, confusion matrices are presented for the experiments conduct under subject-dependent and subject-independent settings, as depicted in Fig.  3 . The results demonstrate that our model performs exceptionally well in the subject-dependent setting, achieving high recognition accuracy across all emotion categories. This highlights the effectiveness of the proposed approach in capturing intra-subject patterns in EEG signals and facial expressions.\n\nIn subject-independent experiments, although the model demonstrates notable improvements compared to previous studies, a significant performance gap persists when compared to the subject-dependent setting. This discrepancy can be attributed to the substantial individual differences in both EEG signals and facial expressions, which pose a considerable challenge to the model's generalization capability. This issue is a common challenge faced in the field of multimodal emotion recognition research.\n\nIn general, these results underscore the strengths of the proposed method in controlled conditions while highlighting the inherent difficulties of crosssubject emotion recognition tasks. Future work could focus on strategies to further enhance the model's robustness to individual variability.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Visual Feature Integration Analysis",
      "text": "Table  4  presents a comparison of three strategies for integrating visual features extracted from multiple facial images before fusing them with EEG features in a cross-modal framework: (1) no integration, (2) MLP, and (3) cross-attention (CA). In the no integration approach, visual features are directly passed to the next stage of multimodal fusion without additional processing. In contrast, the MLP approach linearly maps the visual features to a four-fold higher dimension before reducing them back. The results demonstrate that employing cross-attention significantly improves classification performance, far outperforming both no integration and MLP, thereby highlighting the importance of CA in this framework. Furthermore, the table investigates the impact of varying the number of facial modality tokens on classification performance. In addition to the original length of 147, six additional configurations were evaluated. The results reveal that excessively increasing the dimension of facial features leads to degradation of performance. This aligns with our hypothesis that an overly high-dimensional visual representation may dominate the transformer's attention mechanism, potentially overshadowing the contribution of EEG features. On the other hand, when the number of visual tokens is reduced to within three times the EEG token count, promising classification results are achieved. However, further reducing the number of visual tokens below that of EEG results in decreased performance. Ultimately, the best performance, with an accuracy of 96.72%, is achieved when the visual token output size is set to 64. This suggests that a balanced configuration allows the model to effectively leverage both modalities without introducing significant bias toward one.\n\nThese findings emphasize the importance of careful tuning of token dimensions in cross-modal fusion tasks to ensure that critical information from both modalities is preserved and utilized effectively. To validate the contributions of the three critical components in our framework, the EEG module, the facial feature extraction and balancing module, and the modality fusion module, we conducted ablation studies, as shown in Table  5 . We assessed the classification performance of each modality independently and compared our transformer-based fusion approach with two commonly used fusion methods in the field.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Ablation Study",
      "text": "The results presented in Table  5  indicate that when only the EEG modality is used, the framework achieves an accuracy of 62.33% and an F1 score of 60.93%, while the facial modality alone yields slightly better performance, with an accuracy of 83.59% and an F1 score of 83.67%. The relatively lower performance of the EEG-only model can be attributed to the simplified EEG processing adopted in our study, as our primary focus lies in cross-modal fusion rather than sophisticated EEG signal processing.\n\nWhen both modalities are incorporated, performance improves significantly, with accuracy exceeding 88% across different fusion methods. Specifically, the DeepCCA and Concatenation methods achieve accuracies of 88.13% and 89.88%, respectively, which are notably lower compared to the 96.72% accuracy and 96.71% F1 score achieved by our transformer-based fusion approach.\n\nThese findings highlight the complementary nature of EEG and facial features and emphasize the crucial role of the fusion module in effectively harnessing their synergy. The superior performance of the transformer-based approach underscores its potential in cross-modal fusion tasks, demonstrating its ability to capture complex interactions between modalities more effectively than traditional methods. Furthermore, our facial feature extraction and balancing module is composed of three key submodules: fine-tuned (FT) Swin feature extraction, multiple instance learning (MIL), and cross-attention (CA). To validate the contributions of these components, we conducted ablation studies, as presented in Table  6 .\n\nThe results demonstrate that whether using the conventional single-image representation of facial features or our multi-image approach, the absence of the facial feature extraction and balancing module results in an accuracy of approximately 86%.\n\nIndividually adding the FT, MIL, and CA submodules results in accuracies of 88.42%, 89.83%, and 87.64%, respectively, showing incremental improvements over the baseline. When FT is combined with MIL or CA, the accuracy increases to 89.91% and 90.01%, respectively, slightly exceeding the performance achieved using FT alone. However, the combination of CA and MIL leads to a significant performance boost, achieving an accuracy of 94.46% and an F1 score of 94.58%, substantially outperforming individual module contributions. This improvement can be attributed to the strong correlation between CA and MIL, as the primary role of the CA module is to effectively integrate the multi-instance outputs generated by the MIL module.\n\nUltimately, the best results are obtained when all three submodules-FT, MIL, and CA-are incorporated, achieving an accuracy of 96.72% and an F1 score of 96.71%, demonstrating the effectiveness of our proposed module design. These findings underscore the importance of integrating multiple complementary strategies within the facial feature extraction and balancing module. The synergy among FT, MIL, and CA enables the model to leverage richer and more discriminative facial representations, contributing to the overall robustness and accuracy of the emotion recognition framework.",
      "page_start": 19,
      "page_end": 25
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "This paper proposes a comprehensive method for assessing the credibility of Internet of Things devices. We evaluate multiple dimensions of device network behaviors and utilize a Transformer to capture the temporal variability of network features and to predict the features for the next time step. The predicted behavior information is then compared with the actual collected behavior information to calculate similarities and differences. These metrics are used to ascertain the credibility of the device. Finally, our method's reliability is validated through experimental testing, demonstrating its effectiveness in accurately assessing the trustworthiness of IoT devices.This study proposes a multimodal learning framework based on EEG and facial expressions, integrating feature extraction and deep learning for emotion recognition. For the EEG modality, widely adopted methods in the field were employed, while more advanced feature extraction techniques were introduced for facial expression processing. Additionally, a more sophisticated fusion module was utilized to explore effective strategies for integrating features from the two modalities. Extensive experiments conducted on the DEAP dataset demonstrate that the proposed framework surpasses state-of-the-art methods in terms of accuracy. Furthermore, ablation studies on individual modules validate the effectiveness of our approach.\n\nThis study addresses the challenges of emotion recognition by integrating facial expression analysis with EEG signals, introducing a novel frame-work-Milmer. To overcome the limitations observed in previous research, Milmer proposes several optimization strategies.\n\nFirst, prior studies often employed overly simplistic and coarse multimodal fusion techniques, neglecting the substantial differences between EEG and facial expressions. In contrast, Milmer adopts a transformer-based fusion approach, utilizing cross-attention to balance token distribution during modality fusion, thereby enhancing the integration of these heterogeneous data sources (see Section 3.3 and Section 3.4). Experimental results validate the effectiveness of our fusion and balancing strategies, as demonstrated in Section 4.3 and Section 4.4.\n\nSecond, traditional facial expression feature extraction methods rely on outdated techniques and typically fail to incorporate temporal dynamics by using multiple images over time, thus missing critical sequential information. To address this gap, we propose a multi-instance learning based approach (see Section 3.3), which is further validated through ablation studies, demonstrating its contribution to the overall performance (see  Section 4.4) .\n\nFurthermore, we conducted comprehensive comparisons with numerous well-established studies in the field, covering both unimodal and multimodal approaches across various classification tasks. Extensive experiments on the DEAP dataset confirm the superiority of our proposed framework, achieving an average classification accuracy of 96.72% in the four-class emotion classification task, surpassing existing methods and further validating the effectiveness of our approach (see Section 4.2).\n\nFor future work, integrating additional physiological signals, such as electrocardiogram (ECG) and galvanic skin response (GSR), could further enhance the diversity and robustness of multimodal learning frameworks. Continuous advancements in feature extraction techniques for individual modalities, highlight the need to stay abreast of developments and refine unimodal processing approaches. Moreover, the design of classification tasks and loss functions for emotion recognition presents opportunities for further optimization, offering valuable insights into the burgeoning field of multimodal learning. Finally, the joint recognition of facial expressions and physiological signals remains a challenging yet underexplored area, warranting deeper and more comprehensive research in the future.",
      "page_start": 25,
      "page_end": 26
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overview of our framework, consisting of a EEG preprocessing module, a",
      "page": 8
    },
    {
      "caption": "Figure 1: , the proposed architecture",
      "page": 9
    },
    {
      "caption": "Figure 2: The image feature dimensionality reduction module in this work.",
      "page": 13
    },
    {
      "caption": "Figure 3: Confusion matrices of subject-dependent and subject-independent experiments.",
      "page": 21
    },
    {
      "caption": "Figure 3: The results demonstrate that our model performs exceptionally",
      "page": 21
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "Abstract"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "Emotions play a crucial role in human behavior and decision-making, mak-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "ing\nemotion recognition a key area\nof\ninterest\nin human-computer\ninter-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "action (HCI). This\nstudy addresses\nthe\nchallenges of\nemotion recognition"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "by integrating facial expression analysis with electroencephalogram (EEG)"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "signals,\nintroducing a novel multimodal\nframework-Milmer. The proposed"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "framework employs a transformer-based fusion approach to effectively inte-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "grate visual and physiological modalities.\nIt consists of an EEG preprocess-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "ing module, a facial\nfeature extraction and balancing module, and a cross-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "modal\nfusion module.\nTo enhance visual\nfeature\nextraction, we fine-tune"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "a pre-trained Swin Transformer on emotion-related datasets. Additionally,"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "a cross-attention mechanism is\nintroduced to balance token representation"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "across modalities, ensuring effective feature integration. A key innovation of"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "this work is\nthe adoption of a multiple instance learning (MIL) approach,"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "which extracts meaningful\ninformation from multiple facial expression im-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "ages over\ntime,\ncapturing critical\ntemporal dynamics often overlooked in"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "previous\nstudies.\nExtensive experiments conducted on the DEAP dataset"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "demonstrate\nthe\nsuperiority of\nthe proposed framework, achieving a clas-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "sification accuracy\nof\n96.72% in the\nfour-class\nemotion recognition task."
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "Ablation studies\nfurther validate\nthe\ncontributions of\neach module, high-"
        },
        {
          "cZhongguancun Laboratory, Beijing, 100194, China": "lighting the significance of advanced feature extraction and fusion strategies"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in enhancing emotion recognition performance. Our code are available at": "https://github.com/liangyubuaa/Milmer."
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "Keywords:\nHuman-computer interaction, Emotion recognition, Multiple"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "instance learning, Physiological signals, Human facial expressions"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "1.\nIntroduction"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "Emotions, as an integral part of human daily life, significantly influence"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "individual behavior, decision making, social\ninteractions, and mental health."
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "In recent years, with the rapid advancement of human-computer interaction"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "(HCI) technologies, emotion recognition has emerged as a highly prominent"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "research area.\nExtensive\nstudies have\nexplored this\ntopic through various"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "data modalities. The development of deep learning methods, coupled with"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "the utilization of large-scale multimodal datasets such as DEAP [1] have rev-"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "olutionized the field of emotion recognition. These advancements have paved"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "the way for detecting and interpreting human emotions with unprecedented"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "accuracy."
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "Emotion recognition relies broadly on two primary modalities of human"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "data:\nvisual data such as\nimages or videos, and physiological\nsignals\nsuch"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "as electroencephalogram (EEG). Research in this field has first\nfocused on"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "vision-based approaches, using facial\nimages and videos, as evident in prior"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "studies [2, 3, 4, 5]. Facial expressions, as direct and easily captured indicators"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "of human emotions, have been extensively studied. However,\nfacial expres-"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "sions in datasets commonly used in facial emotion recognition often exhibit"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "exaggerated or highly recognizable features. These expressions, while easily"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "identifiable, are not\nrepresentative of\nreal-life emotional expressions, where"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "emotions are not always\nexpressed in an exaggerated or obvious way.\nIn"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "fact, people often display little to no facial expression in response to their"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "emotions. This discrepancy poses a significant challenge in accurately recog-"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "nizing emotions in real-world scenarios and highlights a major weakness in"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "the field of\nfacial emotion recognition."
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "In contrast, physiological\nsignals\nsuch as EEG offer distinctive insights"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "into human emotions by reflecting intrinsic responses that occur instinctively"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "and are not\nsubject\nto conscious control, providing a more accurate repre-"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "sentation of genuine emotions [6, 7, 8]. Despite their potential, the adoption"
        },
        {
          "in enhancing emotion recognition performance. Our code are available at": "of deep learning strategies for physiological signals remains limited, hindered"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "processing, and the high cost of bio-sensing data acquisition."
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "Beyond single modality approaches,\nrecent studies have highlighted the"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "advantages of multimodal affective\ncomputing techniques, which integrate"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "vision and physiological data to achieve improved emotion recognition per-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "formance [9, 10, 11]. Each modality contributes unique strengths that com-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "plement the other, and their fusion enhances the quality of the extracted fea-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "tures, resulting in a more comprehensive representation of human emotions."
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "Each modality contributes unique\nstrengths\nthat\ncomplement\neach other,"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "and their fusion enhances the quality of the extracted features, resulting in a"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "more comprehensive representation of human emotions, which significantly"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "improves\nthe accuracy compared to single modality approaches[12]. More-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "over,\nin multimodal datasets,\nfacial\nexpression data are\ntypically derived"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "from videos of participants watching stimulus videos, where\ntheir\nexpres-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "sions are more subdued and natural. This makes\nthe facial expressions\nin"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "multimodal studies closer to real-life scenarios compared to those captured"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "in single modality datasets, where expressions are often more exaggerated."
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "However, despite the proven value of using EEG and facial expressions in"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "multimodal emotion recognition, there remains a substantial gap in research"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "in this area,\nleaving considerable opportunities\nfor\nfurther exploration[12]."
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "Current\nresearch tends\nto\nfocus more\non feature\nextraction within each"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "modality independently, while neglecting the\ncrucial\ntask of modality fu-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "sion. Many approaches\nrely on simple methods\nsuch as\nconcatenation or"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "decision-tree-based fusion techniques, yet EEG and facial expressions repre-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "sent two fundamentally different modalities. Such simplistic fusion strategies"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "often lead to the loss of valuable information and fail to fully utilize the com-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "plementary features extracted from both modalities. This study argues that"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "these limitations are a significant reason why existing models have not fully"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "realized their potential, and that more sophisticated, comprehensive fusion"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "methods should become the standard in future research."
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "It\nis worth noting that, due to the inherently higher complexity of vi-"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "sual\ninformation compared to EEG signals, most prior research has placed a"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "stronger emphasis on facial expression analysis. However, this study argues"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "that facial expressions in multimodal datasets tend to be more constrained"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "and neutral,\nclosely resembling real-world conditions where distinguishing"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "subtle emotions becomes particularly challenging.\nIn such scenarios, EEG"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "signals play an irreplaceable role, as they can capture underlying emotional"
        },
        {
          "by challenges such as hardware inconsistencies,\nthe differences in data pre-": "states that are not easily reflected in facial expressions. This highlights the"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "effective multimodal\nfusion strategies[13].\nUnfortunately, previous\nstudies"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "often overlook this aspect, applying overly simplistic fusion techniques that"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "fail to effectively combine these two fundamentally different types of data."
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "In the feature extraction process for the facial expression modality, CNN-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "based architectures are predominantly used in this field, often capturing lo-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "calized features\nrather\nthan global ones. Additionally,\nresearch frequently"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "relies on single\nfacial\nexpression images\nrather\nthan multiple\nimages over"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "time to assess emotions, thereby failing to fully utilize the available informa-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "tion. These two limitations reduce the quality of visual feature representation"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "and, ultimately,\nimpact emotion classification performance."
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "To conclude, several\nlimitations have been identified in the current field:"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "(1) Despite its potential, the field remains underexplored, with relatively few"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "studies dedicated to fully integrating facial expression and EEG signals for"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "emotion recognition.\n(2) Most research efforts focus primarily on feature ex-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "traction from individual modalities, with limited investigation into effective"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "multimodal\nfusion strategies.\n(3) Current approaches\nto facial\nexpression"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "feature extraction rely on outdated methods and fail to consider using mul-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "tiple images over time for emotion recognition, missing important temporal"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "information."
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "To\naddress\nthese\nchallenges,\nthis paper proposes\na hybrid framework"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "named Milmer\nfor Multiple Instance Learning based Multimodal Emotion"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "Recognition, which includes an EEG preprocessing module, a facial\nfeature"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "extraction and balancing module, and a transformer-based modality fusion"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "module. The key contributions of this paper are as follows:"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "1. Multimodal emotion recognition using facial expressions and EEG sig-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "nals\nremains underexplored. This\nstudy fills\nthis gap by providing a"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "comprehensive exploration of facial expression and EEG signal integra-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "tion, along with open-sourcing the code to facilitate further research in"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "this area;"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "2. This paper proposes a transformer-based modality fusion approach,"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "which more effectively integrates the visual and physiological modali-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "ties, overcoming the limitations of previous, simplistic fusion methods;"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "3. This study employs a multi-instance learning approach to extract use-"
        },
        {
          "equal\nimportance of both modalities and underscores\nthe critical need for": "ful\ninformation from multiple\nfacial\nexpression images over\ntime.\nIt"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "also leverages a pre-trained Swin Transformer, fine-tuned on emotion-": "related datasets,"
        },
        {
          "also leverages a pre-trained Swin Transformer, fine-tuned on emotion-": "cross-attention mechanism is used to balance the features of"
        },
        {
          "also leverages a pre-trained Swin Transformer, fine-tuned on emotion-": "ages, ensuring a more comprehensive and effective fusion of both modal-"
        },
        {
          "also leverages a pre-trained Swin Transformer, fine-tuned on emotion-": "ities."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "ities."
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "The rest of this paper is organized as follows. Section 2 introduces related"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "work on the topic. Section 3 outlines the structure of the proposed method."
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "Section 4 details the experimental setup and discusses the results. Finally,"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "Section 5 draws the conclusions and presents future research directions."
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "2. Related Work"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "2.1. EEG-Based Emotion Recognition"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "Electroencephalogram (EEG)-based emotion recognition utilizes brain"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "activity signals\nto classify emotional\nstates,\noffering an intrinsic and un-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "conscious\nreflection of emotional\nresponses. Deep learning models\nsuch as"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "convolutional neural networks\n(CNNs)\n[15] and recurrent neural networks"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "(RNNs)\n[16] have been extensively applied for\nfeature extraction and clas-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "sification. More recently, transformer-based approaches [17, 6] have demon-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "strated strong potential\nin modeling temporal dynamics and long-range de-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "pendencies in EEG signals. Advanced frameworks such as graph neural net-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "works (GNNs)[18] have also been proposed to capture the spatial\ninterrela-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "tions among EEG channels effectively."
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "However, EEG recording is associated with several challenges. Key ob-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "stacles\ninclude the presence of\ninternal and external artifacts,\nsuch as eye"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "movements and muscle activity, which can interfere with signal accuracy."
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "Furthermore, recording EEG data for extended periods can introduce noise"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "and variability, which complicates\nthe\nextraction of\nconsistent\nemotional"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "features[19].\nThese factors\nlimit\nthe effectiveness of EEG as a standalone"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "modality for emotion recognition, highlighting the need for complementary"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "approaches."
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "2.2. Facial Emotion Recognition"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "Facial\nemotion recognition (FER)\ninvolves analyzing human facial\nex-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "pressions\nfrom images or videos\nto classify emotions. Deep learning meth-"
        },
        {
          "ages, ensuring a more comprehensive and effective fusion of both modal-": "ods, particularly convolutional neural networks\n(CNNs), have significantly"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "ages, 2D CNNs effectively extract spatial features, achieving high accuracy in"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "emotion classification [20]. For video data, methods such as Conv3D [21] and"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "ConvLSTM [22] are employed to capture both spatial and temporal\ninfor-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "mation,\nimproving recognition performance. Recently, transformer architec-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "tures [23, 24] have gained attention for their ability to encode context-aware"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "features through attention mechanisms, offering superior results in dynamic"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "emotion analysis."
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "These advancements have made FER a cornerstone of emotion recogni-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "tion research. However, relying solely on facial\nimages or videos has inherent"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "limitations, particularly in real-world scenarios where emotions are often sub-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "tle, neutral, or influenced by contextual\nfactors that are not captured visu-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "ally [25]. The absence of complementary data, such as physiological signals,"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "restricts FER methods\nfrom fully understanding the underlying emotional"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "states. These limitations highlight the necessity of exploring multimodal ap-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "proaches, which integrate diverse data sources to achieve more accurate and"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "robust emotion recognition."
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "2.3. Multimodal Emotion Recognition"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "Multimodal emotion recognition, which integrates facial expressions and"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "EEG signals, has gained significant attention due to its ability to provide"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "more accurate emotion classification by combining complementary informa-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "tion. However, research in this area still\nlacks sufficient exploration of effec-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "tive fusion techniques, which limits the potential of multimodal models."
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "Several approaches have been proposed to fuse EEG and facial expression"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "data. Early methods often relied on simple fusion strategies such as decision"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "trees or voting mechanisms to combine the outputs of each modality [26, 10]."
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "While these approaches are straightforward, they fail to effectively utilize the"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "rich, modality-specific features extracted from each source, often resulting in"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "suboptimal performance. Methods like Deep Canonical Correlation Analysis"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "(DeepCCA), have attempted to improve feature correlation between modali-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "ties, yet they still fall short of fully exploiting the complex, individual features"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "of each modality [27]. Low-rank fusion techniques have also been explored,"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "offering a more efficient way to combine modality-specific features [28]. While"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "these methods are computationally efficient, they are not advanced enough"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "to capture the complex interactions between the two modalities."
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "Concatenation-based methods, which combine the features of both modal-"
        },
        {
          "advanced this field by automating facial\nfeature extraction.\nFor\nstatic im-": "ities into a single vector, are also widely used but can lead to information loss"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "mechanisms in transformers, allow the model to focus on the most relevant"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "features from each modality, enhancing fusion accuracy and improving overall"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "performance [5]. Despite these advancements, the field still lacks comprehen-"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "sive, effective fusion strategies that can fully harness the strengths of both"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "EEG and facial expression data, highlighting the need for further research in"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "this area."
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "2.4. Multiple Instance Learning"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "Multiple Instance Learning (MIL) is a variation of supervised learning in"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "which a class label\nis assigned to a bag of\ninstances rather than individual"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "instances. Unlike traditional supervised learning, where each input (such as"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "an image) is labeled with a specific category, MIL is applied in scenarios where"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "the labeling is more ambiguous.\nIn MIL, a ”bag” contains multiple instances,"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "and only the overall class of\nthe bag is provided, not\nindividual\nlabels\nfor"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "each instance. This approach is particularly useful when data annotations"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "are weak or incomplete, a common occurrence in real-world tasks."
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "Traditional MIL typically relies on pooling methods\nlike max pooling"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "or average pooling to combine the individual\ninstances\nin a bag and make"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "a prediction.\nHowever,\nthese pooling techniques often fail\nto capture\nthe"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "importance of specific instances, as they treat all\ninstances equally.\nIn con-"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "trast, attention-based MIL (AMIL)\n[30]introduces an attention mechanism"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "that assigns different weights to instances, allowing the model to focus more"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "on the most\nrelevant\ninstances while\nignoring less\ninformative ones.\nThis"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "approach significantly improves MIL performance by enabling more precise"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "feature extraction and decision-making."
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "In the field of multimodal emotion recognition using EEG and facial ex-"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "pressions, the typical approach involves associating a short segment of EEG"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "data (e.g., 3 seconds) with a single facial expression image, which is then used"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "to represent\nthe entire 3-second window. However,\nthis approach is\ninher-"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "ently flawed as it may overlook valuable temporal and subtle details present"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "within the sequence of\nimages.\nA 3-second video,\nfor\ninstance,\nshould be"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "understood as a ”bag” of\ninstances, with each individual\nframe or\nimage"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "representing a potential\ninstance.\nRelying on just one image to represent"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "the entire bag could result\nin the loss of critical\ninformation, especially in"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "capturing emotional variations that occur over time."
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "By adopting MIL, we can leverage all instances within the video to create"
        },
        {
          "[29, 11, 2].\nIn contrast, more sophisticated methods, such as cross-attention": "a more balanced and accurately represent the entire segment. MIL allows for"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EEG Preprocessing Module\nFacial Feature Extraction and Balancing Module": ""
        },
        {
          "EEG Preprocessing Module\nFacial Feature Extraction and Balancing Module": "feature extraction and balancing module, and a modality fusion module."
        },
        {
          "EEG Preprocessing Module\nFacial Feature Extraction and Balancing Module": ""
        },
        {
          "EEG Preprocessing Module\nFacial Feature Extraction and Balancing Module": "thus enhancing the emotion recognition process. Despite the"
        },
        {
          "EEG Preprocessing Module\nFacial Feature Extraction and Balancing Module": ""
        },
        {
          "EEG Preprocessing Module\nFacial Feature Extraction and Balancing Module": "[31, 32]. This\nresearch aims\nto incorporate MIL to better capture"
        },
        {
          "EEG Preprocessing Module\nFacial Feature Extraction and Balancing Module": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3. Methods": "3.1. Overview"
        },
        {
          "3. Methods": "As discussed earlier, many existing methods overlook the importance of"
        },
        {
          "3. Methods": "effective modality fusion,\nrelying on simple concatenation or decision-tree-"
        },
        {
          "3. Methods": "based approaches that fail to fully exploit the complementary nature of dif-"
        },
        {
          "3. Methods": "ferent modalities. Transformer[33], an emerging neural network architecture"
        },
        {
          "3. Methods": "initially developed for machine translation tasks, has\nrecently achieved re-"
        },
        {
          "3. Methods": "markable success\nin the field of natural\nlanguage processing (NLP). How-"
        },
        {
          "3. Methods": "ever,\nits potential application to emotion recognition tasks\ninvolving EEG"
        },
        {
          "3. Methods": "and facial modalities remains underexplored."
        },
        {
          "3. Methods": "In this work, we propose a transformer-based framework to achieve effec-"
        },
        {
          "3. Methods": "tive cross-modal\nfusion. As illustrated in Fig. 1,\nthe proposed architecture"
        },
        {
          "3. Methods": "consists of\nfour main modules:\n(1) EEG preprocessing module,\n(2)\nfacial"
        },
        {
          "3. Methods": "feature extraction and balancing module, and (3) modality fusion module."
        },
        {
          "3. Methods": "In the first module, noise and artifacts\nin the EEG signals are filtered"
        },
        {
          "3. Methods": "out to ensure clean input data.\nIn the second module, Swin Transformer[14]"
        },
        {
          "3. Methods": "is used to extract\nfeatures\nfrom facial\nimages.\nA MIL approach is\nthen"
        },
        {
          "3. Methods": "introduced to better\nrepresent\nthe facial expressions by selecting the most"
        },
        {
          "3. Methods": "representative frames. To further reduce the feature dimension and highlight"
        },
        {
          "3. Methods": "key information, a cross-attention mechanism[33, 34]\nis applied to the Swin"
        },
        {
          "3. Methods": "Transformer output. Then, both the EEG and facial\nfeatures are embedded"
        },
        {
          "3. Methods": "and fed into a transformer for cross-modal fusion. Finally, the fused features,"
        },
        {
          "3. Methods": "which encapsulate frequency, temporal, and spatial\ninformation, are passed"
        },
        {
          "3. Methods": "through a fully connected classifier to produce the final emotion classification"
        },
        {
          "3. Methods": "results. The details of these three modules are explained in the subsequent"
        },
        {
          "3. Methods": "sections."
        },
        {
          "3. Methods": "3.2. EEG Preprocessing Module"
        },
        {
          "3. Methods": "The majority of EEG signals are concentrated within the frequency range"
        },
        {
          "3. Methods": "of 1 Hz–50 Hz. Therefore, a bandpass filter with a passband of 1 Hz–50 Hz"
        },
        {
          "3. Methods": "is applied in the proposed model. This filtering step serves two primary pur-"
        },
        {
          "3. Methods": "poses:\n(1) removing low-frequency baseline drift, electrocardiographic (ECG)"
        },
        {
          "3. Methods": "interference, and other high-frequency noise, and (2) effectively mitigating"
        },
        {
          "3. Methods": "the most prominent power line interference, which typically occurs at 50 Hz"
        },
        {
          "3. Methods": "in China."
        },
        {
          "3. Methods": "However, EEG signals often overlap with electrooculogram (EOG) and"
        },
        {
          "3. Methods": "electromyogram (EMG) signals within the same frequency band, making a"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "To address this issue, Independent Component Analysis (ICA) is employed."
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "ICA is\na\ncomputational\ntechnique\nthat decomposes\na multivariate\nsignal"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "into a set of statistically independent components. By maximizing the non-"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "Gaussianity of\nthe\nsources,\nICA separates mixed signals\ninto independent"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "sources, which helps to isolate and remove artifacts such as EOG and EMG"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "from EEG signals."
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "The ICA process involves estimating a matrix that unmixes the data into"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "independent components, which are then identified and removed based on"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "their\nspatial and temporal characteristics.\nSpecifically,\nthe Fast\nICA algo-"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "rithm from the MNE library[35]\nis utilized to perform this decomposition."
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "For\nICA processing,\ncontinuous EEG data from a single subject\n(without"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "segmenting it into epochs) is used, with all other settings left at their default"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "values in the MNE library. By combining bandpass filtering and ICA, this"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "preprocessing module effectively ensures cleaner EEG signals for subsequent"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "tasks."
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "3.3. Facial Feature Extraction and Balancing Module"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "This\nstudy significantly advances\nthe visual\nfeature\nextraction process"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "compared to prior\nresearch in the field.\nFirst,\nin contrast\nto traditional"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "CNN-based methods prevalent\nin the field, we utilize\nthe advanced Swin"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "Transformer fine-tuned on emotion classification dataset as the backbone for"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "feature extraction."
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "Previous\nstudies\ntypically align a single facial\nimage with a segment of"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "EEG data (commonly 3 seconds), assuming that a single frame can represent"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "the entire video segment. However,\nit is difficult to determine which frame"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "best encapsulates\nthe information from a 3-second video.\nTo address\nthis"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "limitation, we introduce MIL into this domain. By extracting multiple frames"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "from the video segment and leveraging MIL, the most representative K frames"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "are\nselected.\nThis approach ensures a more\ncomprehensive and nuanced"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "representation of the facial modality, capturing richer emotional information."
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "Lastly, this study addresses the critical challenge of balancing the contri-"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "butions of\nthe modality during the fusion stage.\nSelecting multiple frames"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "through MIL inevitably introduces far more visual tokens compared to the"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "EEG modality, which could lead to the model disproportionately focusing"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "on facial features while neglecting EEG information. To mitigate this imbal-"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "ance, we employ a cross-attention-based mechanism, enabling the K frames"
        },
        {
          "single bandpass filter insufficient for completely eliminating these artifacts.": "to interact and learn from each other while\nreducing their dimensionality"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "before being entered into the fusion module. This step ensures balanced and": "effective multimodal\nintegration."
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "3.3.1. Swin Feature Extraction"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "Compared to traditional CNN-based methods commonly used in this field,"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "the Swin Transformer demonstrates\nsuperior\ncapabilities\nin visual\nfeature"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "extraction.\nBy employing a hierarchical architecture with shifted window"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "attention mechanisms, the Swin Transformer captures both local and global"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "dependencies efficiently. This design allows it to retain the fine-grained de-"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "tails of local features while incorporating contextual information across larger"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "regions,\nsurpassing CNNs that\nrely on fixed local\nreceptive fields and hier-"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "archical\nfeature aggregation. This enables the Swin Transformer to extract"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "richer and more robust features, providing a solid foundation for subsequent"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "multimodal\nfusion tasks."
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "In this module, we use a 6-layer Swin Transformer as the image encoder,"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "initialized with weights fine-tuned on emotion recognition dataset. The Swin"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "Transformer serves as the backbone for extracting visual features, generating"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "M feature tokens, each with dimension D, from the input facial images. These"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "feature tokens retain hierarchical and contextual\ninformation, making them"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "highly suitable for subsequent processing in the fusion stage."
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "3.3.2. Multiple Instance Learning"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "In the classical supervised 2-class classification problem, the objective is"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "to develop a model that predicts a target variable y ∈ 0, 1 for a given instance,"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "x ∈ RD. However,\nin the context of the MIL problem, the model is presented"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "with a bag of\ninstances\ninstances X = {x1, x2, . . . , xq}, where the individual"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "are independent of each other. The bag is associated with a single label Y"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "belonging to one of\nthe two classes,\ni.e., y ∈ 0, 1.\nFurthermore, although"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "each instance within the bag is assumed to have its own label y1, y2, . . . , yq,"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "these labels are partially indicative of the bag’s overall\nlabel Y\nbut cannot"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "be equated with it."
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "In MIL research, there is often a focus on understanding the relationships"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "between instances within a bag and selecting a method that represents the"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "bag in a more balanced or universal manner. The most common approach"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "for this is pooling, such as average pooling or max pooling. With the advent"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "of attention mechanisms,\nattention-based learnable pooling methods have"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "emerged, one of the most notable being AMIL[30]. AMIL presents a simple"
        },
        {
          "before being entered into the fusion module. This step ensures balanced and": "yet effective approach that\nsignificantly enhances accuracy.\nIt proposes an"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "then performs a weighted sum of the instances\nthe instances x1, x2, . . . , xq,"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "using these weights to obtain the pooled result for subsequent classification."
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "This\nstudy is primarily based on the AMIL approach, but with a key"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "difference. While AMIL directly performs the weighted sum and classification"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "after obtaining the weights, our method, which involves subsequent modality"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "fusion with EEG data, avoids pooling into a single dimension at this stage."
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "Therefore,\ninstead of directly pooling, we select the top-K embeddings with"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "the highest aq values that best represent the subject’s emotional state, which"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "are then used for fusion. K is a hyperparameter representing the number of"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "selected images. The formula is as follows:"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "| q ∈ T } ,\n(1)\nz = {xq"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "where T\nis the set of\nindices corresponding to the top K largest aq, defined"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "as:"
        },
        {
          "for\nattention-based weighting method that calculates weights a1, a2, . . . , aq": "(2)\nT = {q | aq\nis among the top K largest values of a1, a2, . . . , aq} ."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "attention mechanism is used to reduce\nthe number of\nfeature vectors output by Swin"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "Transformer to the predefined number of\nlearnable query tokens. Here, M represents the"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "number of\nfeature vectors output by Swin Transformer, D is the vector dimension, and N"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "is the number of\nlearnable query tokens."
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "harmonious\nrepresentation for\nfusion.\nThis process ultimately contributes"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "to improved classification performance by ensuring that both modalities are"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "equally represented in the downstream tasks. As\nillustrated in Fig. 2,\nthe"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "cross-attention mechanism shows as follows:"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "The learnable query tokens (N, D) and the visual tokens (M, D) extracted"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "from the Swin transformer are first passed through linear layers to generate"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "the query Q, key K, and value V matrices, which are expressed as:"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "Q = WQXq,\nK = WKXv,\nV = WV Xv"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "where Xq represents the learnable query tokens of size (N, D), and Xv repre-"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "sents the visual tokens extracted from the Swin transformer of size (M, D),"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "as the corresponding learnable weight matrices.\nwith WQ, WK, and WV"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "The attention output is then computed via the scaled dot-product atten-"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "tion mechanism:"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "(cid:18) QKT"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "√\nV\nA = softmax"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "dk"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "where A is the reduced feature representation of size (N, D). This dimension-"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "ality reduction condenses the most relevant spatial and contextual\ninforma-"
        },
        {
          "Figure 2:\nThe\nimage\nfeature dimensionality reduction module\nin this work.\nA cross-": "tion into a set of visual tokens, which helps balance the representation of the"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "fusion in subsequent tasks while reducing computational overhead."
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "To summarize, the facial feature extraction and balancing module utilizes"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "the Swin Transformer for global\nfeature representation, employs an AMIL-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "based method to select the most representative Top-K features, and incorpo-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "rates a cross-attention mechanism to align and compress the visual tokens."
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "This approach ensures a balanced token representation between the visual"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "and EEG modalities, thereby enhancing the quality of multimodal fusion and"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "improving overall classification performance."
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "3.4. Fusion Module"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "In the proposed framework, the Fusion Module is designed to effectively"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "combine information from EEG signals and facial\nfeatures while preserving"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "modality-specific characteristics. To achieve this, we use modal type embed-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "ding, position embedding, and a classification token (CLS token)."
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "Modal Type Embedding: To differentiate between the\ntwo modalities,"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "we introduce modal\ntype embeddings. These are learnable vectors unique"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "to each modality, appended to their\nrespective input\nfeatures. Modal-type"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "embeddings provide\nthe model with explicit\ninformation about\nthe origin"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "of each input token, enabling the Transformer to process and fuse modality-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "specific features more effectively. For instance, all EEG tokens are associated"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "with a dedicated embedding vector, while facial tokens are assigned a distinct"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "embedding, ensuring clear separation in the shared representation space."
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "Position Embedding: Position embeddings are applied to retain the order"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "and structure of\nthe input\ntokens\nfor both EEG and facial\nfeatures. EEG"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "position embeddings encode temporal dependencies by representing the time-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "step information of\neach token, while\nfacial position embeddings\ncapture"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "the spatial relationships among visual tokens extracted by Swin, preserving"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "global contextual\ninformation essential\nfor emotion recognition."
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "CLS Token for Classification: A CLS token is appended to the input se-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "quence, serving as a global representation of the fused features across modali-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "ties. During the Transformer encoding process, the CLS token aggregates in-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "formation from both EEG and facial feature tokens through attention mech-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "anisms.\nIn the output stage,\nthe CLS token is passed to a fully connected"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "layer for emotion classification. This approach allows the model to condense"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "multimodal\ninformation into a compact and informative representation, op-"
        },
        {
          "EEG and facial expression modalities, ensuring more harmonious multimodal": "timizing the subsequent classification task."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Finally, the fused features are passed through a fully connected layer to": "produce the classification results. The cross-entropy loss is employed as the"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "objective function to train the label classifier, and it is defined as follows:"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "batchsize"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "1"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "(cid:88) j\nn(cid:88) i\n(4)\nLoss = −\nyjilog( ˆyji),"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "batchsize"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "=1\n=1"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "where n is the number of classes; yji and\nˆyji denote the true and predicted"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "labels in a batch, respectively."
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "4. Experiments and Result"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "In this\nsection, we present\nthe experimental\nresults.\nFirst, we provide"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "an overview of\nthe DEAP dataset and describe the preprocessing methods"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "applied to the data.\nSubsequently, we\nconduct\nextensive\nexperiments on"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "the DEAP dataset\nto evaluate and compare the classification performance"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "of various multimodal\nfusion strategies.\nFurthermore, we\ninvestigate\nthe"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "impact of the token quantities from the two modalities on classification per-"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "formance during multimodal\nfusion.\nFinally, we perform ablation studies"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "to demonstrate the significance of each component in our proposed network"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "architecture."
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "Thanks to the fast convergence of Milmer,\nthe experimental trends and"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "results become apparent within just 100 epochs. Loshchilov et al.[36]\nintro-"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "duced the cosine learning rate decay strategy, which has been widely val-"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "idated for\nits\neffectiveness.\nAll\nexperiments\nin this\nsection are\nconducted"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "using the cosine learning rate decay strategy, with results reported over 100"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "epochs."
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "4.1. Dataset and Settings"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "We employ the widely recognized DEAP dataset, which is commonly used"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "in the field of multimodal affective computing, to evaluate the effectiveness"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "of our proposed architecture.\nIn the DEAP dataset, participants’ emotions"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "are elicited by watching music videos in a controlled laboratory environment,"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "while their facial videos and bio-sensing signals are recorded synchronously."
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "Following the viewing session, participants\nrate\ntheir\nemotional\nresponses"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "based on four dimensions:\nvalence, arousal, dominance, and liking. Most"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "studies in this area primarily focus on valence and arousal\nfor emotion eval-"
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "uation, and our study adheres to this convention."
        },
        {
          "Finally, the fused features are passed through a fully connected layer to": "15"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "1 to 9, which are typically grouped into two,\nthree, or four categories. For"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "studies\nthat\nfocus on two- or\nthree-class classification, valence and arousal"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "are usually treated as\nindependent dimensions.\nIn the two-class approach,"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "both dimensions are divided into high and low levels, while the three-class"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "approach includes an additional neutral category.\nIn contrast, the four-class"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "classification treats valence and arousal as interrelated dimensions, dividing"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "each of them into high and low levels, which results in a total of four distinct"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "categories."
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "These\nthree\nclassification\nschemes—two-class,\nthree-class,\nand\nfour-"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "class—are all commonly employed in the field. To enable comparison with"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "prior research, we conduct experiments using all three classification schemes,"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "referred to as DEAP-2, DEAP-3, and DEAP-4. The four-class scheme, being"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "the most\ncomplex,\ncaptures\nthe most nuanced emotional distinctions and"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "is better\nsuited for demonstrating differences\nin experimental\nresults.\nIn"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "contrast,\nthe\ntwo-class approach,\nthough widely used,\nsimplifies\nthe\ntask"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "but may lead to overfitting and less persuasive\nresults due\nto its\nreduced"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "complexity.\nTherefore,\nthe main results presented in this paper\nfocus on"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "the DEAP-4 experiment, while DEAP-2 and DEAP-3 are used primarily for"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "comparison with other studies."
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "In the DEAP-4 setting, we use 5 as a threshold to classify emotions into"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "four quadrants\nof\nthe valence-arousal\nspace:\nHigh Arousal High Valence"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "(HAHV), High Arousal Low Valence\n(HALV), Low Arousal High Valence"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "(LAHV), and Low Arousal Low Valence (LALV). The classification rule is"
        },
        {
          "The DEAP dataset provides continuous emotional ratings on a scale from": "defined as follows:"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 1: The results of",
      "data": [
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "of three facial expression videos, a decision made to prevent potential mis-"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "alignment issues within the dataset."
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "In this\nstudy,\nthe data is\nsegmented into 3-second time windows, with"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "EEG signals from each segment paired with 10 evenly spaced facial\nframes"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "extracted over the 3-second duration. Google’s Mediapipe library, built upon"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "the efficient BlazeFace framework[37],\nis utilized for face detection. The de-"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "tected faces are resized to 224×224 pixels using center padding, with black"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "borders added as needed to maintain aspect ratio. For the 32-channel EEG"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "signals, preprocessing includes a bandpass filter within the 1–50 Hz range,"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "followed by artifact removal using Fast ICA to eliminate noise caused by oc-"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "ular and muscular activities, ensuring cleaner signals for subsequent analysis."
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "Since the DEAP dataset has an EEG sampling rate of 128 Hz, resulting in"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "384 data points over a 3-second period, we employ a multi-layer perceptron"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "(MLP)\nto upsample the signal\nlength to 768, ensuring alignment with the"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "image data."
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "4.2. Comparison Experiments"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "The classification results of our proposed method, along with other studies"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "that adopt the four-class approach, are presented in Table 1. The results of"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "studies employing the three-class approach are summarized in Table 2, while"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "the binary classification results for valence and arousal are shown in Table 3."
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "Table 1: Comparison of emotion recognition performance across different studies on the"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "DEAP dataset(DEAP-4 classification). This table presents various methods categorized by"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "modality,\nincluding EEG-only and multimodal approaches, the number of emotion classes"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "classified, and their corresponding accuracy and F1 scores. The best results are highlighted"
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "in bold, and differences from the best result are marked with downward arrows."
        },
        {
          "trials. Notably, data from Participant 11 was excluded due to the absence": "Method\nModality\nClass\nAccuracy(%)\nF1(%)"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 1: The results of",
      "data": [
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "DEAP dataset(DEAP-4 classification). This table presents various methods categorized by"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "modality,"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "classified, and their corresponding accuracy and F1 scores. The best results are highlighted"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "in bold, and differences from the best result are marked with downward arrows."
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Method"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Gupta wt al.(2018)[38]"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Kwon et al.(2018)[39]"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Marjit et al.(2021)[40]"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Cimtay et al.(2020)[41]"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Jung et al.(2019)[29]"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Lee et al.(2024)[42]"
        },
        {
          "Table 1: Comparison of emotion recognition performance across different studies on the": "Milmer (ours)"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 1: first presents three highly cited studies that focus exclusively",
      "data": [
        {
          "The table presents various methods categorized by modality,\nincluding": "EEG-only and multimodal approaches, the number of emotion classes clas-"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "sified,\nand their\ncorresponding accuracy and F1 scores.\nThis\ncomparison"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "provides insights into the effectiveness of different methodologies under the"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "DEAP-4 classification scheme."
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "Table 1 first presents\nthree highly cited studies\nthat\nfocus\nexclusively"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "on the EEG modality, without incorporating facial expressions,\nfor emotion"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "recognition. Among these studies,\nthe highest performance achieved is an"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "accuracy of 81.25% and an F1 score of 78.56%."
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "Following this, we compare several multimodal emotion recognition stud-"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "ies, primarily focusing on frameworks that integrate EEG and facial features."
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "Notably, Cimtay et al.\nand Jung et al.\nare two highly cited works\nin this"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "category,\nachieving accuracies of 53.87% and 54.22%,\nrespectively,\nin the"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "four-class classification task, which are lower compared to other studies. The"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "underperformance of these models can be attributed to their reliance on con-"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "ventional CNN-based feature extraction methods\nfor each modality, which"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "may not effectively capture the complex characteristics of EEG and facial"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "expressions. Additionally, Cimtay et al.\nemploy a decision tree mechanism"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "for feature fusion, which struggles with high-dimensional and complex data,"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "leading to suboptimal performance."
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "A very recent study by Lee et al. presents a multimodal emotion recogni-"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "tion framework that effectively leverages contrastive learning and cross-modal"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "attention mechanisms to enhance inter-modal feature alignment. Their use of"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "contrastive learning stands out as a significant strength, enabling the model"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "to learn richer and more discriminative multimodal\nrepresentations. How-"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "ever,\ntheir approach to visual\nfeature extraction is\nrelatively simplistic,\nre-"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "lying solely on a ViT-based encoder without further refinement, which may"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "limit\nthe potential of\ntheir model. Consequently,\ntheir\nframework achieves"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "an accuracy of 83.20% and an F1 score of 84.10%."
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "Finally, our proposed framework, Milmer, achieves the best results with"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "an accuracy of 96.72% and an F1 score of 96.71%, demonstrating the su-"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "periority of our approach.\nThese\nresults validate\nthe\neffectiveness of our"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "feature\nextraction and fusion strategies\nin improving emotion recognition"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "performance."
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "Table 2 presents the experimental results of various multimodal\nlearning"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "studies conducted on the DEAP-3 classification scheme. Wu et al. proposed"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "an innovative approach by leveraging Action Units (AUs) to represent facial"
        },
        {
          "The table presents various methods categorized by modality,\nincluding": "expressions and designing a multimodal\nlearning framework based on CNN"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 2: Comparison of emotion recognition performance across different studies on the",
      "data": [
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": "(DEAP-3 classification). The best"
        },
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": ""
        },
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": "Modality"
        },
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": "EEG+Face"
        },
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": "EEG+Face"
        },
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": "EEG+EMG+EOG+GSR"
        },
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": "EEG+Face"
        },
        {
          "Table 2: Comparison of emotion recognition performance across different studies on the": "Multimodal"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 3: Comparison of binary classification performance across different studies on the",
      "data": [
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "(DEAP-2\nclassification).\nThis\ntable presents methods\ncategorized by"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "the number of emotion classes classified, and their corresponding accuracy for"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": ""
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": ""
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "Modality\nClass\nArousal(%)\nValence(%)"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 3: Comparison of binary classification performance across different studies on the",
      "data": [
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "classification).\nThis"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "the number of emotion classes classified, and their corresponding accuracy for"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "arousal and valence classification. The best results are highlighted in bold, with perfor-"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": ""
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "Modality"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "EEG+Face"
        },
        {
          "Table 3: Comparison of binary classification performance across different studies on the": "Multimodal"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 4: presents a comparison of three strategies for integrating visual",
      "data": [
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u0014\u0000\u0011\u0000\u001c\u0000\u0015\u0000\b",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u0019\u0000\u0011\u0000\u0013\u0000\u001b\u0000\b"
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u001c\u0000\u0018\u0000\u0011\u0000\u0015\u0000\u0014\u0000\b",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u001a\u0000\u0018\u0000\u0011\u0000\u0019\u0000\u001b\u0000\b"
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u0014\u0000\u0011\u0000\u0018\u0000\u0016\u0000\b",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u0014\u0000\u0013\u0000\u0011\u0000\u0014\u0000\u0017\u0000\b"
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u0014\u0000\u0011\u0000\u0016\u0000\u0017\u0000\b",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000\u001b\u0000\u0011\u0000\u0014\u0000\u0014\u0000\b"
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000+\u0000$\u0000/\u00009",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "\u0000+\u0000$\u0000/\u00009"
        },
        {
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000'\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": "",
          "\u00006\u0000X\u0000E\u0000M\u0000H\u0000F\u0000W\u0000\u0010\u0000,\u0000Q\u0000G\u0000H\u0000S\u0000H\u0000Q\u0000G\u0000H\u0000Q\u0000W": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multiple images before fusing them with EEG features in a cross-modal": "no integration (None), (2) two fully connected layer (MLP), and (3) cross-attention (CA).",
          "framework:": ""
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "The table also examines\nthe effect of varying the output\nsize of",
          "framework:": "the integrated features"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "classification performance.\nThe best",
          "framework:": "results are highlighted in bold, and"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "differences from the best result are marked with downward arrows.",
          "framework:": ""
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "Fusion Methods\nOutput Size\nAccuracy(%)",
          "framework:": "F1(%)"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "None\n147\n89.91↓6.81",
          "framework:": "89.35↓7.36"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "MLP\n147\n88.83↓7.89",
          "framework:": "89.16↓7.55"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "CA\n196\n94.25↓2.47",
          "framework:": "94.27↓2.44"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "CA\n147\n95.51↓1.21",
          "framework:": "95.87↓0.84"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "CA\n128\n95.99↓0.73",
          "framework:": "95.92↓0.79"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "CA\n96\n96.23↓0.49",
          "framework:": "96.42↓0.29"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "CA (Milmer)\n64\n96.72",
          "framework:": "96.71"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "CA\n32\n95.05↓1.67",
          "framework:": "95.25↓1.46"
        },
        {
          "multiple images before fusing them with EEG features in a cross-modal": "CA\n16\n93.84↓2.88",
          "framework:": "93.99↓2.72"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data, and Fusion",
      "data": [
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "Methods in Model Performance. The best results are highlighted in bold, and differences",
          "and Fusion": ""
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "from the best result are marked with downward arrows.",
          "and Fusion": ""
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "Facial Data\nEEG Data\nFusion Methods\nAccuracy(%)",
          "and Fusion": "F1(%)"
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "×\n✓",
          "and Fusion": ""
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "-\n62.33↓34.39",
          "and Fusion": "60.93↓36.32"
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "✓\n×",
          "and Fusion": ""
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "-\n83.59↓13.13",
          "and Fusion": "83.67↓13.04"
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "✓\n✓",
          "and Fusion": ""
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "DeepCCA\n88.13↓8.59",
          "and Fusion": "88.74↓7.97"
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "✓\n✓",
          "and Fusion": ""
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "Concatenation\n89.88↓6.84",
          "and Fusion": "90.28↓6.43"
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "✓\n✓",
          "and Fusion": ""
        },
        {
          "Table 5: Ablation Study on the Contribution of EEG Data, Facial Data,": "Transformer (Milmer)\n96.72",
          "and Fusion": "96.71"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 6: Ablation study on the effectiveness of the facial feature extraction and balancing",
      "data": [
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "these\nsubmodules"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "classification performance. The best results are highlighted in bold, and differences from"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "the best result are marked with downward arrows."
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "Methods\nFT"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "×"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "×"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "✓"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "×"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "×"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "✓"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "✓"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "×"
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": ""
        },
        {
          "multiple instance learning (MIL), and cross-attention (CA). The table compares different": "✓\nMilmer (ours)"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "94.46% and an F1 score of 94.58%,\nsubstantially outperforming individual"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "module\ncontributions.\nThis\nimprovement\ncan be attributed to the\nstrong"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "correlation between CA and MIL, as\nthe primary role of\nthe CA module"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "is to effectively integrate the multi-instance outputs generated by the MIL"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "module."
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "Ultimately, the best results are obtained when all three submodules—FT,"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "MIL, and CA—are incorporated, achieving an accuracy of 96.72% and an F1"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "score of 96.71%, demonstrating the\neffectiveness of our proposed module"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "design.\nThese findings underscore\nthe\nimportance of\nintegrating multiple"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "complementary strategies within the facial\nfeature extraction and balancing"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "module. The synergy among FT, MIL, and CA enables the model to leverage"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "richer and more discriminative\nfacial\nrepresentations,\ncontributing to the"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "overall robustness and accuracy of the emotion recognition framework."
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "5. Conclusion and Future work"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "This paper proposes a comprehensive method for assessing the credibility"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "of Internet of Things devices. We evaluate multiple dimensions of device net-"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "work behaviors and utilize a Transformer to capture the temporal variability"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "of network features and to predict the features for the next time step. The"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "predicted behavior\ninformation is\nthen compared with the actual collected"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "behavior information to calculate similarities and differences. These metrics"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "are used to ascertain the\ncredibility of\nthe device.\nFinally, our method’s"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "reliability is validated through experimental testing, demonstrating its effec-"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "tiveness in accurately assessing the trustworthiness of IoT devices.This study"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "proposes a multimodal\nlearning framework based on EEG and facial expres-"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "sions,\nintegrating feature extraction and deep learning for emotion recogni-"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "tion. For the EEG modality, widely adopted methods in the field were em-"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "ployed, while more advanced feature extraction techniques were introduced"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "for\nfacial expression processing.\nAdditionally, a more sophisticated fusion"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "module was utilized to explore\neffective\nstrategies\nfor\nintegrating features"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "from the two modalities.\nExtensive experiments conducted on the DEAP"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "dataset demonstrate that the proposed framework surpasses state-of-the-art"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "methods in terms of accuracy. Furthermore, ablation studies on individual"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "modules validate the effectiveness of our approach."
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "This study addresses the challenges of emotion recognition by integrat-"
        },
        {
          "and MIL leads to a significant performance boost, achieving an accuracy of": "ing facial expression analysis with EEG signals,\nintroducing a novel\nframe-"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "Milmer proposes several optimization strategies."
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "First, prior\nstudies often employed overly simplistic and coarse multi-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "modal fusion techniques, neglecting the substantial differences between EEG"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "and facial expressions.\nIn contrast, Milmer adopts a transformer-based fu-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "sion approach, utilizing cross-attention to balance token distribution during"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "modality fusion,\nthereby enhancing the integration of\nthese heterogeneous"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "data sources (see Section 3.3 and Section 3.4). Experimental results validate"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "the effectiveness of our fusion and balancing strategies, as demonstrated in"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "Section 4.3 and Section 4.4."
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "Second,\ntraditional\nfacial expression feature extraction methods rely on"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "outdated techniques and typically fail to incorporate temporal dynamics by"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "using multiple images over time, thus missing critical sequential\ninformation."
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "To address this gap, we propose a multi-instance learning based approach (see"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "Section 3.3), which is further validated through ablation studies, demonstrat-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "ing its contribution to the overall performance (see Section 4.4)."
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "Furthermore, we conducted comprehensive comparisons with numerous"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "well-established studies in the field, covering both unimodal and multimodal"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "approaches across various classification tasks. Extensive experiments on the"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "DEAP dataset confirm the superiority of our proposed framework, achiev-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "ing an average\nclassification accuracy of 96.72% in the\nfour-class\nemotion"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "classification task,\nsurpassing existing methods and further validating the"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "effectiveness of our approach (see Section 4.2)."
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "For future work,\nintegrating additional physiological signals, such as elec-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "trocardiogram (ECG) and galvanic skin response (GSR), could further en-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "hance the diversity and robustness of multimodal\nlearning frameworks. Con-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "tinuous advancements in feature extraction techniques for individual modal-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "ities, highlight the need to stay abreast of developments and refine unimodal"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "processing approaches. Moreover, the design of classification tasks and loss"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "functions for emotion recognition presents opportunities for further optimiza-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "tion, offering valuable insights into the burgeoning field of multimodal\nlearn-"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "ing.\nFinally,\nthe\njoint\nrecognition of\nfacial\nexpressions and physiological"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "signals remains a challenging yet underexplored area, warranting deeper and"
        },
        {
          "work—Milmer. To overcome the limitations observed in previous research,": "more comprehensive research in the future."
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,"
        },
        {
          "References": "T. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion anal-"
        },
        {
          "References": "ysis; using physiological\nsignals,” IEEE transactions on affective com-"
        },
        {
          "References": "puting, vol. 3, no. 1, pp. 18–31, 2011."
        },
        {
          "References": "[2] Y. Huang, J. Yang, S. Liu, and J. Pan, “Combining facial expressions"
        },
        {
          "References": "and electroencephalography to enhance\nemotion recognition,” Future"
        },
        {
          "References": "Internet, vol. 11, no. 5, p. 105, 2019."
        },
        {
          "References": "[3] Y. Khaireddin and Z. Chen, “Facial emotion recognition: State of the"
        },
        {
          "References": "art performance on fer2013,” arXiv preprint arXiv:2105.03588, 2021."
        },
        {
          "References": "[4] W. Nie, M. Ren,\nJ. Nie,\nand\nS. Zhao,\n“C-gcn:\nCorrelation\nbased"
        },
        {
          "References": "graph convolutional network for audio-video emotion recognition,” IEEE"
        },
        {
          "References": "Transactions on Multimedia, vol. 23, pp. 3793–3804, 2020."
        },
        {
          "References": "[5] R. Wang, W. Jo, D. Zhao, W. Wang, A. Gupte, B. Yang, G. Chen, and"
        },
        {
          "References": "B.-C. Min,\n“Husformer:\nA multi-modal\ntransformer\nfor multi-modal"
        },
        {
          "References": "human state recognition,” IEEE Transactions on Cognitive and Devel-"
        },
        {
          "References": "opmental Systems, 2024."
        },
        {
          "References": "[6] Y. Liang, C. Zhang, S. An, Z. Wang, K. Shi, T. Peng, Y. Ma, X. Xie,"
        },
        {
          "References": "J. He, and K. Zheng, “Fetcheeg:\na hybrid approach combining feature"
        },
        {
          "References": "extraction and temporal-channel\njoint attention for eeg-based emotion"
        },
        {
          "References": "classification,” Journal of Neural Engineering, vol. 21, no. 3, p. 036011,"
        },
        {
          "References": "2024."
        },
        {
          "References": "[7] L. Shu, J. Xie, M. Yang, Z. Li, Z. Li, D. Liao, X. Xu, and X. Yang,"
        },
        {
          "References": "“A review of emotion recognition using physiological signals,” Sensors,"
        },
        {
          "References": "vol. 18, no. 7, p. 2074, 2018."
        },
        {
          "References": "[8] C. Zhang, J. He, Y. Liang, Z. Wang, and X. Xie, “A fusion framework for"
        },
        {
          "References": "confusion analysis in learning based on eeg signals,” Applied Sciences,"
        },
        {
          "References": "vol. 13, no. 23, p. 12832, 2023."
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "ing multi-modal data and machine learning techniques: A tutorial and"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "review,” Information Fusion, vol. 59, pp. 103–126, 2020."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[10] Y. Huang, J. Yang, P. Liao, and J. Pan, “Fusion of\nfacial expressions"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "and eeg for multimodal\nemotion recognition,” Computational\nintelli-"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "gence and neuroscience, vol. 2017, no. 1, p. 2107451, 2017."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[11]\nI. Hosseini, M. Z. Hossain, Y. Zhang, and S. Rahman, “Deep learning"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "model for simultaneous recognition of quantitative and qualitative emo-"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "tion using visual and bio-sensing data,” Computer Vision and Image"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "Understanding, vol. 248, p. 104121, 2024."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[12] B. Pan, K. Hirota, Z. Jia, and Y. Dai, “A review of multimodal emotion"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "recognition from datasets, preprocessing, features, and fusion methods,”"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "Neurocomputing, p. 126866, 2023."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[13] W. Kim, B. Son, and I. Kim, “Vilt: Vision-and-language transformer"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "without convolution or region supervision,” in International conference"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "on machine learning, pp. 5583–5594, PMLR, 2021."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[14] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "“Swin transformer: Hierarchical vision transformer using shifted win-"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "dows,”\nin Proceedings of\nthe\nIEEE/CVF international\nconference on"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "computer vision, pp. 10012–10022, 2021."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[15] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstet-"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "ter, K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard,\nand"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "T. Ball,\n“Deep learning with convolutional neural networks\nfor\neeg"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "decoding and visualization,” Human brain mapping,\nvol. 38, no. 11,"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "pp. 5391–5420, 2017."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[16] W.-L. Zheng, J.-Y. Zhu, and B.-L. Lu, “Identifying stable patterns over"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "time for emotion recognition from eeg,” IEEE transactions on affective"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "computing, vol. 10, no. 3, pp. 417–429, 2017."
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "[17] Z. Wang, Y. Wang, C. Hu, Z. Yin, and Y. Song, “Transformers for eeg-"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "based emotion recognition: A hierarchical spatial\ninformation learning"
        },
        {
          "[9] J. Zhang, Z. Yin, P. Chen, and S. Nichele, “Emotion recognition us-": "model,” IEEE Sensors Journal, vol. 22, no. 5, pp. 4359–4368, 2022."
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "ing regularized graph neural networks,” IEEE Transactions on Affective"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "Computing, vol. 13, no. 3, pp. 1290–1301, 2020."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[19] M. Jafari, A. Shoeibi, M. Khodatars, S. Bagherzadeh, A. Shalbaf, D. L."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "Garc´ıa, J. M. Gorriz, and U. R. Acharya, “Emotion recognition in eeg"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "signals using deep learning methods: A review,” Computers in Biology"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "and Medicine, p. 107450, 2023."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[20] R. Breuer and R. Kimmel, “A deep learning perspective on the origin"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "of\nfacial expressions,” arXiv preprint arXiv:1705.01842, 2017."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[21] Y. Fan, X. Lu, D. Li, and Y. Liu, “Video-based emotion recognition"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "using cnn-rnn and c3d hybrid networks,” in Proceedings of the 18th ACM"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "international conference on multimodal\ninteraction, pp. 445–450, 2016."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[22] J. Huang, Y. Li, J. Tao, Z. Lian, and J. Yi, “End-to-end continuous"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "emotion recognition from video using 3d convlstm networks,” in 2018"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "IEEE International Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "cessing (ICASSP), pp. 6837–6841, IEEE, 2018."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[23] Z. Zhao and Q. Liu, “Former-dfer: Dynamic facial expression recognition"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "transformer,” in Proceedings of\nthe 29th ACM International Conference"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "on Multimedia, pp. 1553–1561, 2021."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[24] Q. Huang, C. Huang, X. Wang, and F. Jiang, “Facial expression recog-"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "nition with grid-wise attention and visual\ntransformer,”\nInformation"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "Sciences, vol. 580, pp. 35–54, 2021."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[25] F. Z. Canal, T. R. M¨uller, J. C. Matias, G. G. Scotton, A. R. de Sa Ju-"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "nior, E. Pozzebon, and A. C. Sobieranski, “A survey on facial emotion"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "recognition techniques: A state-of-the-art\nliterature review,” Informa-"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "tion Sciences, vol. 582, pp. 593–617, 2022."
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "[26] Y. Tan, Z. Sun, F. Duan, J. Sol´e-Casals, and C. F. Caiafa, “A multi-"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "modal emotion recognition method based on facial expressions and elec-"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "troencephalography,” Biomedical Signal Processing and Control, vol. 70,"
        },
        {
          "[18] P. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition us-": "p. 103029, 2021."
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[28] Z. Liu, Y. Shen, V. B. Lakshminarasimhan, P. P. Liang, A. Zadeh, and"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[29] T.-P. Jung, T. J. Sejnowski,"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[30] M."
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[31] T. Rao, M. Xu, H. Liu, J. Wang, and I. Burnett, “Multi-scale blocks"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[32] L. Romeo, A. Cavallo, L. Pepa, N. Bianchi-Berthouze, and M. Pontil,"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[33] A. Vaswani, “Attention is all you need,” Advances in Neural Information"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[34] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[35] A. Gramfort, M. Luessi, E. Larson, D. A. Engemann, D. Strohmeier,"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": "[36]"
        },
        {
          "[27] F. Muhammad, M. Hussain, and H. Aboalsamh, “A bimodal emotion": ""
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "M. Grundmann, “Blazeface:\nSub-millisecond neural\nface detection on"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "mobile gpus,” arXiv preprint arXiv:1907.05047, 2019."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[38] V. Gupta, M. D. Chopda, and R. B. Pachori, “Cross-subject emotion"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "recognition using flexible analytic wavelet transform from eeg signals,”"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "IEEE Sensors Journal, vol. 19, no. 6, pp. 2266–2274, 2018."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[39] Y.-H. Kwon, S.-B. Shin, and S.-D. Kim, “Electroencephalography based"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "fusion two-dimensional\n(2d)-convolution neural networks\n(cnn) model"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "for emotion recognition system,” Sensors, vol. 18, no. 5, p. 1383, 2018."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[40] S. Marjit, U. Talukdar, and S. M. Hazarika, “Eeg-based emotion recogni-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "tion using genetic algorithm optimized multi-layer perceptron,” in 2021"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "International Symposium of Asian Control Association on Intelligent"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "Robotics and Industrial Automation (IRIA), pp. 304–309, IEEE, 2021."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[41] Y. Cimtay, E. Ekmekcioglu, and S. Caglar-Ozhan, “Cross-subject multi-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "modal emotion recognition based on hybrid fusion,” IEEE Access, vol. 8,"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "pp. 168865–168878, 2020."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[42] J.-H. Lee, J.-Y. Kim, and H.-G. Kim, “Emotion recognition using eeg"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "signals and audiovisual features with contrastive learning,” Bioengineer-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "ing, vol. 11, no. 10, p. 997, 2024."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[43] Y.-C. Wu, L.-W. Chiu, C.-C. Lai, B.-F. Wu, and S. S. Lin, “Recogniz-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "ing,\nfast and slow: Complex emotion recognition with facial expression"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "detection and remote physiological measurement,” IEEE Transactions"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "on Affective Computing, vol. 14, no. 4, pp. 3177–3190, 2023."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[44] E. S. Salama, R. A. El-Khoribi, M. E. Shoman, and M. A. W. Shal-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "aby, “Eeg-based emotion recognition using 3d convolutional neural net-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "works,” International Journal of Advanced Computer Science and Ap-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "plications, vol. 9, no. 8, 2018."
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "[45] X. Wang, C.-Z. Li, Z. Sun, and Y. Xu, “Design and analysis of a closed-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "loop emotion regulation system based on multimodal affective comput-"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "ing and emotional markov chain,” IEEE Transactions on Systems, Man,"
        },
        {
          "[37] V. Bazarevsky,\nY. Kartynnik,\nA. Vakunov,\nK. Raveendran,\nand": "and Cybernetics: Systems, 2025."
        }
      ],
      "page": 31
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "tion method based on the bidirectional\nlstm and attention mechanism,”"
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "Computational and Mathematical Methods in Medicine, vol. 2021, no. 1,"
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "p. 9967592, 2021."
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "[47] Y. Wu and J. Li, “Multi-modal emotion identification fusing facial ex-"
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "pression and eeg,” Multimedia Tools and Applications, vol. 82, no. 7,"
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "pp. 10901–10919, 2023."
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "[48] S. Wang, J. Qu, Y. Zhang, and Y. Zhang, “Multimodal emotion recog-"
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "nition from eeg signals and facial expressions,” IEEE Access, vol. 11,"
        },
        {
          "[46] Y. Zhao and D. Chen, “Expression eeg multimodal\nemotion recogni-": "pp. 33061–33068, 2023."
        }
      ],
      "page": 32
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "2",
      "title": "Combining facial expressions and electroencephalography to enhance emotion recognition",
      "authors": [
        "Y Huang",
        "J Yang",
        "S Liu",
        "J Pan"
      ],
      "year": "2019",
      "venue": "Future Internet"
    },
    {
      "citation_id": "3",
      "title": "Facial emotion recognition: State of the art performance on fer2013",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition: State of the art performance on fer2013",
      "arxiv": "arXiv:2105.03588"
    },
    {
      "citation_id": "4",
      "title": "C-gcn: Correlation based graph convolutional network for audio-video emotion recognition",
      "authors": [
        "W Nie",
        "M Ren",
        "J Nie",
        "S Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "5",
      "title": "Husformer: A multi-modal transformer for multi-modal human state recognition",
      "authors": [
        "R Wang",
        "W Jo",
        "D Zhao",
        "W Wang",
        "A Gupte",
        "B Yang",
        "G Chen",
        "B.-C Min"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "6",
      "title": "Fetcheeg: a hybrid approach combining feature extraction and temporal-channel joint attention for eeg-based emotion classification",
      "authors": [
        "Y Liang",
        "C Zhang",
        "S An",
        "Z Wang",
        "K Shi",
        "T Peng",
        "Y Ma",
        "X Xie",
        "J He",
        "K Zheng"
      ],
      "year": "2024",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "7",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "8",
      "title": "A fusion framework for confusion analysis in learning based on eeg signals",
      "authors": [
        "C Zhang",
        "J He",
        "Y Liang",
        "Z Wang",
        "X Xie"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "9",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "10",
      "title": "Fusion of facial expressions and eeg for multimodal emotion recognition",
      "authors": [
        "Y Huang",
        "J Yang",
        "P Liao",
        "J Pan"
      ],
      "year": "2017",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "11",
      "title": "Deep learning model for simultaneous recognition of quantitative and qualitative emotion using visual and bio-sensing data",
      "authors": [
        "I Hosseini",
        "M Hossain",
        "Y Zhang",
        "S Rahman"
      ],
      "year": "2024",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "12",
      "title": "A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods",
      "authors": [
        "B Pan",
        "K Hirota",
        "Z Jia",
        "Y Dai"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "authors": [
        "W Kim",
        "B Son",
        "I Kim"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "14",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "15",
      "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "16",
      "title": "Identifying stable patterns over time for emotion recognition from eeg",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Transformers for eegbased emotion recognition: A hierarchical spatial information learning model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "18",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "M Jafari",
        "A Shoeibi",
        "M Khodatars",
        "S Bagherzadeh",
        "A Shalbaf",
        "D García",
        "J Gorriz",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "20",
      "title": "A deep learning perspective on the origin of facial expressions",
      "authors": [
        "R Breuer",
        "R Kimmel"
      ],
      "year": "2017",
      "venue": "A deep learning perspective on the origin of facial expressions",
      "arxiv": "arXiv:1705.01842"
    },
    {
      "citation_id": "21",
      "title": "Video-based emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "22",
      "title": "End-to-end continuous emotion recognition from video using 3d convlstm networks",
      "authors": [
        "J Huang",
        "Y Li",
        "J Tao",
        "Z Lian",
        "J Yi"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "23",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Z Zhao",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "Facial expression recognition with grid-wise attention and visual transformer",
      "authors": [
        "Q Huang",
        "C Huang",
        "X Wang",
        "F Jiang"
      ],
      "year": "2021",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "25",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "26",
      "title": "A multimodal emotion recognition method based on facial expressions and electroencephalography",
      "authors": [
        "Y Tan",
        "Z Sun",
        "F Duan",
        "J Solé-Casals",
        "C Caiafa"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "27",
      "title": "A bimodal emotion recognition approach through the fusion of electroencephalography and facial sequences",
      "authors": [
        "F Muhammad",
        "M Hussain",
        "H Aboalsamh"
      ],
      "year": "2023",
      "venue": "Diagnostics"
    },
    {
      "citation_id": "28",
      "title": "Efficient low-rank multimodal fusion with modalityspecific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modalityspecific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "29",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Attention-based deep multiple instance learning",
      "authors": [
        "M Ilse",
        "J Tomczak",
        "M Welling"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "31",
      "title": "Multi-scale blocks based image emotion classification using multiple instance learning",
      "authors": [
        "T Rao",
        "M Xu",
        "H Liu",
        "J Wang",
        "I Burnett"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "32",
      "title": "Multiple instance learning for emotion recognition using physiological signals",
      "authors": [
        "L Romeo",
        "A Cavallo",
        "L Pepa",
        "N Bianchi-Berthouze",
        "M Pontil"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "33",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "35",
      "title": "Meg and eeg data analysis with mne-python",
      "authors": [
        "A Gramfort",
        "M Luessi",
        "E Larson",
        "D Engemann",
        "D Strohmeier",
        "C Brodbeck",
        "R Goj",
        "M Jas",
        "T Brooks",
        "L Parkkonen"
      ],
      "year": "2013",
      "venue": "Frontiers in Neuroinformatics"
    },
    {
      "citation_id": "36",
      "title": "Sgdr: Stochastic gradient descent with warm restarts",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": "2016",
      "venue": "Sgdr: Stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "37",
      "title": "Blazeface: Sub-millisecond neural face detection on mobile gpus",
      "authors": [
        "V Bazarevsky",
        "Y Kartynnik",
        "A Vakunov",
        "K Raveendran",
        "M Grundmann"
      ],
      "year": "2019",
      "venue": "Blazeface: Sub-millisecond neural face detection on mobile gpus",
      "arxiv": "arXiv:1907.05047"
    },
    {
      "citation_id": "38",
      "title": "Cross-subject emotion recognition using flexible analytic wavelet transform from eeg signals",
      "authors": [
        "V Gupta",
        "M Chopda",
        "R Pachori"
      ],
      "year": "2018",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "39",
      "title": "Electroencephalography based fusion two-dimensional (2d)-convolution neural networks (cnn) model for emotion recognition system",
      "authors": [
        "Y.-H Kwon",
        "S.-B Shin",
        "S.-D Kim"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "40",
      "title": "Eeg-based emotion recognition using genetic algorithm optimized multi-layer perceptron",
      "authors": [
        "S Marjit",
        "U Talukdar",
        "S Hazarika"
      ],
      "year": "2021",
      "venue": "2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA)"
    },
    {
      "citation_id": "41",
      "title": "Cross-subject multimodal emotion recognition based on hybrid fusion",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu",
        "S Caglar-Ozhan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "42",
      "title": "Emotion recognition using eeg signals and audiovisual features with contrastive learning",
      "authors": [
        "J.-H Lee",
        "J.-Y Kim",
        "H.-G Kim"
      ],
      "year": "2024",
      "venue": "Bioengineering"
    },
    {
      "citation_id": "43",
      "title": "Recognizing, fast and slow: Complex emotion recognition with facial expression detection and remote physiological measurement",
      "authors": [
        "Y.-C Wu",
        "L.-W Chiu",
        "C.-C Lai",
        "B.-F Wu",
        "S Lin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Eeg-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "E Salama",
        "R El-Khoribi",
        "M Shoman",
        "M Shalaby"
      ],
      "year": "2018",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "45",
      "title": "Design and analysis of a closedloop emotion regulation system based on multimodal affective computing and emotional markov chain",
      "authors": [
        "X Wang",
        "C.-Z Li",
        "Z Sun",
        "Y Xu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "46",
      "title": "Expression eeg multimodal emotion recognition method based on the bidirectional lstm and attention mechanism",
      "authors": [
        "Y Zhao",
        "D Chen"
      ],
      "year": "2021",
      "venue": "Computational and Mathematical Methods in Medicine"
    },
    {
      "citation_id": "47",
      "title": "Multi-modal emotion identification fusing facial expression and eeg",
      "authors": [
        "Y Wu",
        "J Li"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "48",
      "title": "Multimodal emotion recognition from eeg signals and facial expressions",
      "authors": [
        "S Wang",
        "J Qu",
        "Y Zhang",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    }
  ]
}