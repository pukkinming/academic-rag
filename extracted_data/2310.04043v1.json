{
  "paper_id": "2310.04043v1",
  "title": "In The Blink Of An Eye: Event-Based Emotion Recognition",
  "published": "2023-10-06T06:33:20Z",
  "authors": [
    "Haiwei Zhang",
    "Jiqing Zhang",
    "Bo Dong",
    "Pieter Peers",
    "Wenwei Wu",
    "Xiaopeng Wei",
    "Felix Heide",
    "Xin Yang"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Computing methodologies ‚Üí Computer vision",
    "Supervised learning by classification",
    "Spiking neural networks Event-based cameras, eye-based emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We introduce a wearable single-eye emotion recognition device and a real-time approach to recognizing emotions from partial observations of an emotion that is robust to changes in lighting conditions. At the heart of our method is a bio-inspired event-based",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Real-time emotion recognition in uncontrolled environments is a challenging problem that forms the cornerstone of many in-the-wild human-centered interactive computer graphics experiences such as interactive storytelling that adapts to the users emotions, and emotion-aware virtual avatars. Predicting emotions from regular RGB video streams is a challenging and ambiguous endeavor; informative spatial and temporal emotive cues can be adversely affected by head pose and partial occlusions. To help classify emotions in RGB video frames, existing facial emotion recognition models build on complex CNN-based models such as ResNet 50  [Deng et al. 2020b] , Transformer  [Zhao and Liu 2021] , and Inception-based methods  [Hickson et al. 2019] . Robustly handling varying lighting conditions and rapid user movements further complicates emotion recognition, and existing methods rely on cumbersome large network enhancement modules  [Zhao and Liu 2021]  or impose active IR lighting  [Wu et al. 2020 ]. Despite all these innovations, emotion recognition from RGB video streams remains difficult and fragile.\n\nIn this paper, we introduce a novel wearable emotion recognition prototype in which a bio-inspired event-based camera (DAVIS346) is affixed in front of a user's right eye. An event-based camera can provide more robust temporal cues for emotion recognition under adverse lighting conditions as it offers a higher dynamic range (up to 140 dB vs. 80 dB) and a higher temporal resolution (in the order of ùúás vs. 10s of ùëöùë†) than a conventional camera. Even though this setup provides a stable fixed perspective of a right eye and it can robustly handle various lighting conditions, estimating emotion from a single eye still poses unique challenges.\n\nA key issue is that event-based cameras do not capture texture information effectively (see Figure  1 ). These spatial features are not only essential for emotion recognition but also important for inferring more informative temporal features. For example, while pupil motion and blinking are dominant temporal cues, they are less informative for emotion classification. In contrast, the subtle movements related to the facial units, such as raising the outer brow and squinting, are stronger cues for eye-based emotion recognition.\n\nTo address these challenges, we devise a lightweight SEEN, which combines the best from both events and intensity frames to guide emotion recognition from asynchronous events with spatial texture cues from corresponding intensity frames. In particular, SEEN consists of a spatial feature extractor and a temporal feature extractor that partially share the same convolutional architecture. During training, the shared convolutional parts are only learned in the spatial feature extractor, and the updated weights are copied to the temporal feature extractor. Consequently, spatial attention can be effectively conveyed to the temporal decoding process. As such, the temporal feature extractor learns to associate spatial and temporal features, resulting in a consistent emotion classification.\n\nTo train our lightweight Spiking Eye Emotion Network (SEEN) and to stimulate research in event-based single-eye emotion recognition, we introduce a new Single-eye Event-based Emotion (SEE) dataset. We validate our approach on the SEE dataset and demonstrate state-of-the-art emotion recognition under different challenging lighting conditions, outperforming the runner-up method by a significant margin, 4.8% and 4.6% in WAR and UAR, respectively. The prototype system with an NVIDIA Jetson TX2 operates at 30 FPS in real-world testing scenarios.\n\nSpecifically, our work makes the following contributions:\n\n‚Ä¢ a novel real-time emotion recognition method based on event camera measurements and a spiking neural network suited for in-the-wild deployment; ‚Ä¢ a weight-copy training scheme to enforce learned weights awareness of both spatial and temporal cues; and ‚Ä¢ the first publicly available single-eye emotion dataset containing both intensity frames and corresponding raw events, captured under four different lighting conditions.\n\nLimitations. SEEN partially relies on spatial features extracted from intensity frames, which can be adversely affected by extremely degraded lighting conditions, resulting in a significant performance drop. While our method robustly handles most lighting conditions effectively, as evidenced by our experimental results, further improving robustness by solely leveraging events forms an exciting avenue for future research in eye-based emotion recognition.\n\nThe code and dataset are available on github.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We focus our discussion on related work in emotion recognition on measuring emotions (wearable emotion sensing systems) and recognition (facial emotion recognition).\n\nWearable Emotion Sensing Systems. Emotions impact the human body in subtle ways. However, not all of these signals are equally robust indicators of emotional state, and not all are easily measured. Various bio-signals have been investigated for convenient measurement of indicators of emotional state. Long-term heart rate variability (HRV) has been shown to strongly correlate with emotional patterns  [Appelhans and Luecken 2006; Costa et al. 2019 ]. Similarly, brain activity recorded by electroencephalogram (EEG) sensors also correlates to different emotions  [Li et al. 2018; Liu et al. 2020] . Inspired by human perception of emotions, Electromyogram (EMG) measurements of facial muscle contractions  [Lucero and Munhall 1999]  map to emotions, making wearable emotional detection devices possible  [Gruebler and Suzuki 2014] . A disadvantage of these methods is that they require the sensors to make direct skin contact, dramatically restricting freedom of activity. Furthermore, due to the displacement of sensors and muscular cross-talk during movement, the results can be of low reliability. An alternative to contact-based measurement is pupillometry, i.e., the measurement of pupil size and reactivity, as a potential indicator of emotion  [Math√É≈•t 2018; Nie et al. 2020 ]. However, pupilometry requires expensive equipment, and the reliability is significantly impacted by ambient lighting  [Couret et al. 2019 ]. Similar to pupilometry, our method also focuses on the eye as an indicator of emotional state. However, in contrast to prior work, we employ an event-based camera that does not require direct skin contact and which can operate in challenging lighting conditions.\n\nFacial Emotion Recognition. Facial emotion recognition has received significant attention in computer graphics and computer vision, with applications ranging from driving facial expressions  [Hickson et al. 2019; Ji et al. 2022]  to facial reenactment for efficient social interactions  [Burgos-Artizzu et al. 2015; Li et al. 2015] . A significant portion of prior work in facial emotion recognition requires observations of the entire face, and several methods have been introduced for effective facial feature learning  [Ruan et al. 2021; Xue et al. 2021] , dealing with uncertainties in facial expression data  [Zhang et al. 2021a] , handing partial occlusions  [Georgescu and Ionescu 2019; Houshmand and Khan 2020] , and exploiting temporal cues  [Deng et al. 2020b; Sanchez et al. 2021] . Combinations with other modalities such as contextual information  [Lee et al. 2019]  and depth  [Lee et al. 2020 ] have also been explored to further improve facial recognition accuracy.\n\nHowever, observing the entire face is not feasible in many practical situations. Alternatively, several methods focus on the eye area only for emotion recognition.  Hickson et al. [2019]  infer emotional expressions based on images of both eyes captured with an infrared gaze-tracking camera inside a virtual reality headset.  Wu et al. [2020]  rely on infrared single-eye observations to reduce camera synchronization and data bandwidth issues when monitoring both eyes. Both systems require a personalized initialization procedure; Hickson et al. require a personalized neutral image, and Wu et al. require a reference feature vector of each emotion. The need for a personalized setup makes these systems intrusive and non-transparent to the user and could raise privacy concerns. Furthermore, neither system leverages temporal cues, which are essential for robust emotion recognition  [Sanchez et al. 2021 ]. Our approach does not require personalization, and it leverages temporal and spatial cues to improve emotion recognition accuracy.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Background",
      "text": "Before detailing our method, we first review work related to the two key components of our event-based emotion recognition method: event-based cameras and spiking neural networks.\n\nEvent-based Cameras. An event-based camera differs from a conventional camera in that it does not measure pixel intensities, but instead, an event-based camera records asynchronous (log-encoded) per-pixel brightness changes  [Gallego et al. 2022; Gehrig et al. 2021] . Event-based cameras offer a significantly higher dynamic range (up to 140 dB) and a higher temporal resolution (in the order of ùúás) than conventional cameras. Each event ùëí is characterized by three pieces of information: the pixel location, (ùë•, ùë¶); the event triggering time, ùë°; and a polarity, ùëù ‚àà {-1, 1} which reflects the direction of the brightness change. Formally, a set of ùëÅ events can be defined as:\n\nUnder static lighting, a stationary event-based camera only records scene motion, and events are typically triggered by moving edges (e.g., object contours, and texture boundaries). Since the events predominately stem from the motion of edges, the measured events are inherently sparse and devoid of texture information. Furthermore, since the captured events are triggered asynchronously, events are incompatible with CNN-based architectures. Instead, events are aggregated into a frame or grid-based representation  [Gehrig et al. 2019; Lagorce et al. 2017; Maqueda et al. 2018; Wang et al. 2022 ] before neural processing. In our implementation, we adopt the aggregation algorithm of  Zhang et al. [2021b] , which currently offers the highest performance for single object tracking under normal and degraded conditions. We refer to the Supplementary Material for additional details.\n\nSpiking Neural Network (SNN). Spiking neural networks (SNNs) closely mimic biological information processes. An SNN incorporates the concept of time and only exchanges information (i.e., spike) when a membrane potential exceeds some potential threshold. Mathematically an SNN neuron simulates the properties of a cell in a nervous system with varying degrees of detail, which models three states of a biological neuron: rest, depolarization, and hyperpolarization  [Ding et al. 2022] . When a neuron is at rest, its membrane potential remains constant; typically set to 0. When not at rest, the change in the membrane potential can either decrease or increase. An increase in membrane potential is called depolarization. In contrast, hyperpolarization describes a reduction in membrane potential. When a membrane potential is higher than a potential threshold, an action potential, i.e., spike, is triggered, which for an SNN is a binary value. We refer the interested reader to  Ding et al. [2022]  for an in-depth discussion of these concepts.\n\nIn this paper, we use the leaky integrate-and-fire (LIF) spiking neuron model  [Gerstner and Kistler 2002] , one of the most widely used spiking models. When a LIF neuron receives spikes from other neurons, the spikes are scaled accordingly based on learned synaptic weights. Depolarization is achieved by summing over all the scaled spikes. A decay function over time is used to drive the potential membrane to hyperpolarization. We refer to the Supplemental Material for a detailed formal definition of LIF.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Spiking Eye Emotion Network (Seen)",
      "text": "Existing facial emotion recognition methods typically only identify the \"peak\" states of emotions  [Hickson et al. 2019 ] or a single emotion state over a whole sequence  [Zhao and Liu 2021] , making these methods unsuitable for applications that also require a robust estimate of the in-between states. We introduce a lightweight Spiking Eye Emotion Network (SEEN) that is able to effectively recognize emotions from various states of emotions.\n\nInstead of only memorizing the peak phase of an individual's facial emotion, SEEN is designed to leverage temporal cues to distinguish different phases of emotions using sparse events input captured with an event-based camera (DAVIS346 camera). Compared to a conventional camera, an event-based camera has a number of advantages: it is more sensitive to motion, less sensitive to ambient lighting, and it offers a high dynamic range. Hence, an event-based camera is capable of providing stable temporal information under different lighting conditions. While this makes event-based cameras, in theory, an attractive input modality for motion-based measurements, in practice, a major drawback of existing eventbased cameras is that the recorded events are noisy and lack texture information. We address this drawback with a hybrid system that leverages both spatial cues together with conventional intensity frames to guide temporal feature extraction during training and inference. Most commercial event-based cameras are capable of simultaneously capturing both intensity frames and events through spatially-multiplexed sensing.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Seen Architecture",
      "text": "As illustrated in Figure  2 (a), at its core, the architecture of SEEN consists of a spatial feature extractor, ùëÜ (described in detail in subsection 4.2), and a temporal feature extractor, ùëá (detailed in subsection 4.3). Given two intensity frames, ùêº 1 and ùêº ùëõ , SEEN interpolates the asynchronous captured events between both intensity frames in ùëõ synchronous event frames. Next, the spatial feature extractor ùëÜ distills spatial cues from the intensity frames ùêº 1 and ùêº ùëõ , and the temporal feature extractor ùëá processes each of the ùëõ event frames sequentially in time order. Finally, the temporal features and the spatial cues are then combined to predict ùëõ emotion scores. The final predicted emotion is based on the average of the ùëõ scores. The core component of the temporal feature extractor ùëá is the SNN layers that make decisions based on membrane potentials to remember temporal information from previous event frames. Unlike RNNs  [Kag and Saligrama 2021; Nah et al. 2019] , SNNs can effectively learn temporal dependencies of arbitrary length without any special treatment.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Spatial Feature Extractor ùëÜ",
      "text": "To make spatial feature extraction independent from the intensity sequence length, we only use the first and last frames of a sequence as the input to the spatial feature extractor, thereby fixing the input size regardless of the sequence length, i.e., two frames. The spatial feature extractor ùëÜ (Figure  2(b) ) leverages a multiscale selfattention perception module, Œ©, to obtain discriminative features from different-sized neighborhoods. The extracted spatial features are then transferred into the spiking format, ùêΩ ùë† , via a spiking layer, which is subsequently combined with temporal features to enhance feature discrimination. Formally, the spatial feature extractor can be defined as:\n\nwhere [‚Ä¢] and ‚ü®‚Ä¢‚ü© indicate channel-wise concatenation and a vector, respectively; ùê∂ ùëñ and ùúé denote an ùëñ√óùëñ convolution layer and a softmax function, respectively; A denotes an adaptive pooling layer; BR is a fused batch normalization layer with a ReLU activation function; Œ¶ ùë° is a spiking layer that keeps membrane potential from the previous time step, ùë° -1. The initial membrane potential, i.e., ùë° = 0, is set to 0 (see Equation  13 ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Temporal Feature Extractor ùëá",
      "text": "The basis building blocks of the temporal feature extractor ùëá are SNN layers. An SNN neuron outputs signals based on a membrane potential accumulation, decay, and reset mechanisms to capture the temporal trends in an input sequence  [Ding et al. 2022 ]. When the membrane potential exceeds a threshold, an action potential (i.e., spike) is triggered and the membrane potential is reset. The trigger process itself is non-differentiable, prohibiting training via conventional stochastic gradient descent optimization methods. Instead, we adopt spatio-temporal backpropagation (STBP) along with a CNN-SNN layer  [Wu et al. 2018]  to circumvent this issue. This CNN-SNN layer employs a CNN-based layer for the aggregation process and a LIF-based SNN neuron  [Gerstner and Kistler 2002]  for managing the potential decay and reset processes. This modification takes advantage of CNN-based layers that enable learning of diverse accumulation strategies, resulting in more effective SNN neurons in the temporal domain.\n\nIntensity Attention-Guided Temporal Features. Purely relying on events does not yield a robust solution due to the lack of reliable texture information in the event domain. We, therefore, leverage spatial features from ùëÜ to inject rich texture cues. Figure  2 (c) illustrates the architecture of the temporal extractor ùëá .\n\nThe feature extractor ùëá takes ùëõ event frames, ùê∏ 1 to ùê∏ ùëõ , as input and processes each frame sequentially in time order. Formally, given the spatial feature ùêΩ ùë† , the temporal feature extraction of ùê∏ ùë° is defined < l a t e x i t s h a 1 _ b a s e 6 4 = \" c i l w J e J r x V m A r L r j u 9 y C m m l Y a\n\n+ 5 q 0 5 J 5 s 5 h D 9 w P n 8 A G 9 u N s g = = < / l a t e x i t > E n < l a t e x i t s h a 1 _ b a s e 6 4 = \" A N 7 D 9 T q i\n\ny t m I 6 J J t S 6 m E o u h G D 1 5 X X S v q w F V 7 X 6 f b 3 a q O R x F N E Z q q A L F K B r 1 E B 3 q I l a i K I E P a N X 9 O a l 3 o v 3 7 n 0 s W w t e P n O K / s D 7 / A E K y J G Z < / l a t e x i t > ! s1\n\n< l a t e x i t s h a 1 _ b a s e 6 4 = \" T 0 P 7 b g J J z F b X 3 C 0 t D o 6 0 j p t w o 6 g = \" > A A A B 8 3 i c b V D L S g N B E J z 1 G e M r 6 t H L k C B 4 C r s h q M e A F 4 8 R z A O S N c x O e p M h 8 1 h m Z o W w 5 D e 8 e F D E q z / j z b 9 x k u x B E w s a i q p u u r u i h D N j f f / b 2 9 j c 2 t 7 Z L e w V 9 w 8 O j 4 5 L J 6     by:\n\nwhere M is an operator for obtaining membrane potentials from an SNN layer, and Œ® represents a fully connected layer; Œ¶ ùë° (‚Ä¢) indicates an SNN layer, which records the previous spiking status, ùëÉ ùë° -1 , and potential value, ùëâ ùë° -1 . When receiving membrane potentials ùëã ùë° , this SNN layer outputs updated spikes, ùëÉ ùë° , and updates the recorded membrane potential ùëâ ùë° as follows:\n\nwhere Œò is the membrane potential threshold set to 0.3 in all our experiments. The parameter ùõº is a decay factor used for achieving hyperpolarization. The potential value ùëâ ùë° is updated such that, for a spike at timestamp ùë° -1, the membrane potential should be reset to 0 by scaling 1 -ùëÉ ùë° -1 , and ùëã ùë° is the corresponding item here. Finally, the emotion is the average of ùëÇ ùë° , ùë° ‚àà [1, ùëõ]:\n\nwhere ùúé is a Softmax activation function.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Weight-Copy Scheme",
      "text": "Intuitively, we want temporal information extraction to focus on informative spatial positions, such as facial action units  [Ekman and Friesen 1978] . However, events lack sufficient texture information, which impedes the temporal feature extractor from considering spatial information. To alleviate this problem, we propose a weightcopy scheme that copies the weights from the spatial feature extractor to the temporal feature extractor. Thus, during training, only the fully connected layers in ùëá are trained. The weight-copy scheme requires that all convolutional blocks before the spiking-addition operator, i.e., Equation 9, are of the same architecture in ùëÜ and ùëá ; see Equation 3 and Equation 11. Note that the supervised loss conveys the impact from both the spatial and temporal domains enabled by the spiking-addition. Since the weights are fixed before the spikingaddition in the temporal feature extractor ùëá , the training of the spatial features must also account for temporal cues. Therefore, the weight updating implicitly bridges the domain gap between intensity and event frames.\n\nWeight copying is also applied to the self-attention weights, i.e., the self-attention weights in Equation 11 are replaced by the weights from Equation  5 ; see Figure  2(a) . As we will show in our experimental results, this design is more effective than inferring the self-attention weights based on input events (row E in Table  2  except E4-S0) and it yields a more efficient inference.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Loss Function",
      "text": "Because emotion recognition is a classification task, we use a regular cross-entropy loss for supervised training of SEEN:\n\nwhere ùë¶ ùëñ and ≈∑ùëñ are the predicted ùëñ-th emotion's probability and corresponding ground truth probability, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "To the best of our knowledge, there does not exist an event-based dataset for single-eye emotion recognition. The two most related are  the active infrared lighting/camera datasets Eyemotion  [Hickson et al. 2019 ] (both eyes) and EMO  [Wu et al. 2020 ] (single eye).\n\nTo address this lack of training data for event-based emotion recognition, we collect a new Single-eye Event-based Emotion (SEE) dataset; see Figure  3 . SEE contains data from 111 volunteers captured with a DAVIS346 event-based camera placed in front of the right eye and mounted on a helmet; see Figure  1 . The DAVIS346 camera is equipped with a dynamic version sensor (DVS) and an active pixel sensor (APS), providing both raw events and conventional frames simultaneously. Unlike Eyemotion and EMO, our approach does not require any active lighting source, thereby simplifying installation, testing, and maintenance of the hardware setup. A summary of the technical differences between SEE and the existing emotion datasets is provided in Supplementary Materials.\n\nSEE contains videos of 7 emotions (see Figure  3 (c) for an example) under four different lighting conditions: normal, overexposure, low-light, and high dynamic range (HDR) (Figure  3 (a)). The average video length ranges from 18 to 131 frames, with a mean frame number of 53.5 and a standard deviation of 15.2 frames, reflecting the differences in the duration of emotions between subjects. In total, SEE contains 2, 405/128, 712 sequences/frames with corresponding raw events for a total length of 71.5 minutes (Figure  3(b) ), which we split in 1, 638 and 767 sequences for training and testing, respectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Assessment",
      "text": "The main goal of SEEN is to recognize an emotion for any phase of the emotion. Consequently, when evaluating a test sequence, we choose a uniformly distributed random starting point and corresponding testing length. A start point is selected such that the rest sequence is longer than the corresponding testing length. The testing length is defined as the total accumulation time of all included event frames, ùë•, and a skip time, ùë¶, between two adjacent event frames, denoted as Eùë•-Sùë¶. The skip time defines a window in the time domain where all events are ignored; see \"skip\" in Figure  2 . Note that the skip time is not associated with event-based cameras but an experimental setting. Without loss of generality, the accumulation time and skip time are expressed as a multiple of 1/30 s. Thus, Eùë•-Sùë¶ indicates a testing length equal to (ùë• + (ùë• -1) √óùë¶)/30 s. To reduce the impact of the randomness, we evaluate all competing methods 20 times for different randomly selected start points for each testing sequence; we use the same random starting points for single-frame competing methods, where only the random start frame is used. To evaluate the proposed approach and compare it to competing methods, we adopt two widely used metrics: Unweighted Average Recall (UAR) and Weighted Average Recall (WAR)  [Schuller et al. 2011] . UAR reflects the average accuracy of different emotion classes without considering instances per class, while WAR indicates the accuracy of overall emotions; we refer to the Supplementary Materials for formal definitions of both metrics.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Setup",
      "text": "SEEN is implemented in PyTorch  [Paszke et al. 2019 ] and trained with stochastic gradient descent (SGD) with a momentum of 0.9 and a weight decay of 1e-3. We train SEEN for 180 epochs with a batch size of 32 on an NVIDIA TITAN V GPU. We use the StepLR scheduler to moderate the learning rate. Specifically, the initial learning rate is set to 0.015, the step size is set to 1, and the decay rate is set to 0.94. For the SNN settings, we use a spiking threshold of 0.3 and a decay factor of 0.2 for all SNN neurons.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Qualitative And Quantitative Evaluation",
      "text": "We compare the effectiveness of SEEN to existing emotion recognition methods relying on conventional intensity images only, including whole-face, single-eye, and double-eye based methods. Of these prior methods, Eyemotion  [Hickson et al. 2019]  and EMO  [Wu et al. 2020 ] are single-frame methods for predicting an emotion, while all other methods require the full video sequence. As shown in Table  1 , SEEN for E4-S3 offers the best performance, outperforming the runner-up method, Eyemotion, by significant margins, 4.8% and 4.6% higher in WAR and UAR, respectively. Under normal, overexposure, and HDR lighting conditions, our approach with the same setting also outperforms Eyemotion by at least 4% in accuracy. However, Eyemotion offers slightly better performance under low-light conditions than SEEN with E4-S3. We posit that Eymotion benefits from the Imagenet  [Deng et al. 2009 ] pre-training process; without this pre-training step, Eyemotion's accuracy is 1% less than the one offered by SEEN with E4-S3 setting. Moreover, we note that Eyemotion requires a personalization preprocessing step, which requires subtracting a mean neutral image for each person. Personalization dramatically increases the accuracy of neutral emotion estimation regardless of whether Eyemotion is pre-trained on ImageNet or not.\n\nWe compare SEEN with three different sequence lengths: 4/30 s, i.e., E4-S0; 7/30 s, i.e., E4-S1 group; 13/30 s, i.e., E4-S3 group. The experimental results show that the accuracy of SEEN improves with longer sequence length under all lighting conditions, especially under HDR conditions. Note, all other prior video-based approaches require the full video sequences; consequently, their delay time is the length of an input sequence. In contrast, our method can flexibly adjust the delay time by changing input settings. Figures  4  and 5  qualitatively demonstrate the benefits of our method compared to prior eye-based emotion recognition methods. In Table  1 , the complexity and processing speed of each competing approach are also provided. As the temporal feature extractor processes event frames iteratively, the complexity and processing time increase with the number of event frames. Nevertheless, with the E4-S3 setting, our method offers the second fastest processing speed, but it is more than 20% more accurate than the fastest method, EMO.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "To gain better insight into the abilities of SEEN, we perform a series of ablation studies that investigate a) the impacts of input, b) the influence of each component of SEEN, and c) the impact of outputs.  Impacts of Input. SEEN leverages the first and last intensity frames. Experiments (A), (B) and (C) gauge the impact of the intensity frames: experiment (A) only uses the first intensity frame, experiment (B) replaces the last intensity frame with the second frame, and experiment (C) uses all the intensity frames corresponding to the included event frames. The results of (A) and (B) demonstrate spatial differences are critical for ùëá to extract descriptive temporal cues. Compared to experiments (A) and (B), the results of experiment (C) show that using more intensity frames slightly increases performance. However, compared to our method, the setup dramatically increases data bandwidth.\n\nInfluence of SEEN components. We investigate the effectiveness of the different components that comprise SEEN: 1) the effectiveness of the weight-copy scheme (experiments (D) and (E)) and 2) the benefits of SNNs (experiments (F) to (I)). These two experiment groups show that SEEN with all components offers the best performance, except experiment E under the E4-S0 setting. Experiments (F) to (I) show that replacing the CNN-SNN with a 3-layer CNN, LSTM, Transformer, or 3D CNN significantly degrades performance. A CNN fails to extract useful temporal cues, so the performance degradation further justifies the inclusion of temporal cues. Although LSTM, Transformer, and 3D CNN can extract temporal cues, they are less effective than SNNs. Notably, an SNN neuron's spiking mechanism acts as temporal memory and a natural noise filter, which is beneficial for robust emotion recognition.\n\nImpact of outputs. SEEN estimates emotions based on the average of ùëõ membrane potentials; see Equation 8 and Equation 14. To better understand the impact of this design decision, we conduct three ablation experiments: instead of using the average of ùëõ membrane potentials, we define the prediction score based on the potential generated by the last event frame only (experiment (J)); similar to the previous but using output spikes instead of potential (experiment (K)); and finally using the average of ùëõ output spikes instead of the ùëõ membrane potentials for emotion classification, i.e., remove the M operator in Equation  8 (experiment (L)). These results show that membrane potentials are more effective signals than spikes. We posit that the higher precision of membrane potentials (float vs. binary for spikes) offers more discriminative features for emotion classification. When a membrane potential triggers a spike, the potential is reset to 0. However, it becomes a problem if we leverage the potential as an output signal since the rest operation breaks the temporal cues. To address the problem, we design to use the average of the output potentials as the output signal. Experiment (J) validates the effectiveness of this design.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce a novel wearable single-eye-based emotion recognition prototype that can effectively estimate emotions under challenging lighting conditions. To this end, we investigate event-based camera inputs for emotion recognition. Due to the high dynamic range and temporal resolution of event-based cameras, the captured events can robustly encode temporal information under different lighting conditions. However, the captured events are asynchronous, noisy, and lack texture cues. We introduce SEEN, a novel learning-based solution to extract informative temporal cues for emotion recognition. SEEN introduces two novel design components: a weight-copy scheme and a CNN-SNN-based temporal feature extractor. The former injects spatial attention into temporal feature extraction during the training and inference phases. The latter exploits both spatial awareness and the spiking mechanism of SNNs to provide discriminative features for emotion classification effectively. Our extensive experimental results show that SEEN can effectively estimate an emotion from any phase of the emotion. To the best of our knowledge, SEEN is the first attempt at leveraging event-based cameras and SNNs for emotion recognition tasks. and normal lighting conditions. The frames marked with red boxes are the inputs for EMO  [Wu et al. 2020]  and Eyemotion  [Hickson et al. 2019] , which is also the first input frame of our approach. Our approach offers the most accurate emotion predictions under all test settings.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Demonstration of a wearable single-eye emotion recognition prototype system consisting with a bio-inspired event-",
      "page": 1
    },
    {
      "caption": "Figure 1: ). These spatial features are",
      "page": 2
    },
    {
      "caption": "Figure 2: (a), at its core, the architecture of SEEN",
      "page": 4
    },
    {
      "caption": "Figure 2: (b)) leverages a multiscale self-",
      "page": 4
    },
    {
      "caption": "Figure 2: (c) illus-",
      "page": 4
    },
    {
      "caption": "Figure 2: Our Spiking Eye Emotion Network (a) leverages a CNN-SNN-based temporal feature extractor, ùëá, (c) to process ùëõ",
      "page": 5
    },
    {
      "caption": "Figure 2: (a). As we will show in our",
      "page": 5
    },
    {
      "caption": "Figure 3: The newly collected Single-eye Event-based Emo-",
      "page": 6
    },
    {
      "caption": "Figure 3: SEE contains data from 111 volunteers cap-",
      "page": 6
    },
    {
      "caption": "Figure 1: The DAVIS346",
      "page": 6
    },
    {
      "caption": "Figure 3: (c) for an exam-",
      "page": 6
    },
    {
      "caption": "Figure 3: (a)). The aver-",
      "page": 6
    },
    {
      "caption": "Figure 2: Note that the skip time is not associated with event-based cameras",
      "page": 6
    },
    {
      "caption": "Figure 4: We show four examples across four different emotions, Fear, Anger, Disgust, and Happiness, under overexposure",
      "page": 10
    },
    {
      "caption": "Figure 5: We show four additional examples across another four different emotions, Surprise, Sadness, Anger, and Surprise,",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "Acc. of Emotion Class (%)": "Ha\nSa\nAn\nDi\nSu\nFe\nNe",
          "Acc. under Light Conditions (%)": "Nor\nOver\nLow\nHDR",
          "Metrics (%)": "WAR ‚Üë\nUAR ‚Üë"
        },
        {
          "Methods": "Resnet18 + LSTM [2016; 1997]\nResnet50 + GRU [2020a; 2016]\n3D Resnet18 [2018]\nR(2+1)D [2018]\nFormer DFER [2021]\nFormer DFER w/o pre-train",
          "Acc. of Emotion Class (%)": "57.8\n86.0\n64.9\n46.5\n9.2\n81.6\n59.8\n27.9\n38.0\n49.7\n44.5\n6.9\n70.0\n5.6\n54.8\n45.4\n67.7\n23.8\n37.2\n42.8\n81.6\n63.6\n45.5\n65.7\n27.8\n33.3\n37.9\n86.6\n81.5\n75.2\n85.8\n59.4\n39.3\n50.8\n78.6\n44.1\n65.2\n46.0\n66.5\n28.0\n50.3\n36.1",
          "Acc. under Light Conditions (%)": "57.9\n60.4\n53.9\n52.5\n43.0\n35.7\n28.9\n32.8\n51.9\n51.4\n44.8\n47.8\n54.3\n50.3\n44.4\n49.3\n70.1\n65.4\n66.2\n61.1\n47.0\n51.9\n45.6\n47.2",
          "Metrics (%)": "56.3\n58.0\n35.2\n34.7\n49.1\n50.5\n49.7\n51.5\n65.8\n67.2\n48.0\n48.0"
        },
        {
          "Methods": "Eyemotion [2019]\nEyemotion w/o pre-train\nEMO [2020]\nEMO w/o pre-train",
          "Acc. of Emotion Class (%)": "74.3\n85.5\n79.5\n74.3\n69.1\n79.2\n94.5\n96.4\n79.6\n85.7\n81.2\n71.2\n54.7\n71.6\n75.0\n75.1\n70.2\n48.1\n37.5\n54.1\n82.8\n62.0\n73.2\n60.1\n38.7\n25.7\n48.0\n65.3",
          "Acc. under Light Conditions (%)": "81.5\n79.0\n81.8\n72.5\n77.8\n75.9\n79.8\n69.7\n61.8\n62.8\n60.1\n69.6\n46.1\n60.2\n55.5\n58.9",
          "Metrics (%)": "78.8\n79.5\n75.9\n77.2\n63.1\n63.3\n53.2\n53.3"
        },
        {
          "Methods": "Ours(E4-S0)",
          "Acc. of Emotion Class (%)": "76.0\n85.0\n85.8\n74.8\n66.8\n79.9\n85.3",
          "Acc. under Light Conditions (%)": "78.0\n80.0\n78.1\n78.3",
          "Metrics (%)": "78.6\n79.1"
        },
        {
          "Methods": "Ours(E4-S1)\nOurs(E7-S0)",
          "Acc. of Emotion Class (%)": "76.9\n89.2\n88.9\n76.3\n69.0\n82.3\n86.6\n76.7\n86.8\n87.6\n74.2\n66.2\n82.4\n86.7",
          "Acc. under Light Conditions (%)": "78.5\n83.4\n80.5\n81.0\n78.1\n80.9\n77.3\n82.1",
          "Metrics (%)": "80.9\n81.3\n79.6\n80.1"
        },
        {
          "Methods": "Ours(E4-S3)\nOurs(E7-S1)\nOurs(E13-S0)",
          "Acc. of Emotion Class (%)": "85.0\n92.2\n72.1\n87.7\n89.9\n76.7\n85.2\n90.9\n79.0\n91.1\n77.2\n71.7\n85.0\n84.4\n79.2\n77.9\n88.7\n90.2\n69.7\n87.6\n84.6",
          "Acc. under Light Conditions (%)": "83.3\n84.8\n85.6\n80.8\n86.7\n82.4\n79.8\n80.3\n81.1\n86.5\n79.4\n81.8",
          "Metrics (%)": "83.6\n84.1\n82.4\n82.7\n82.3\n82.5"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "E4-S0": "",
          "E4-S1": "WAR UAR WAR UAR WAR UAR",
          "E4-S3": ""
        },
        {
          "E4-S0": "77.1\n77.6\n76.4\n76.9\n78.0\n78.4",
          "E4-S1": "79.9\n80.2\n80.1\n80.6\n79.9\n80.2",
          "E4-S3": "81.3\n81.8\n81.8\n82.2\n82.9\n83.3"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùê∑ No weight copy\nùê∏ No Att. weight copy": "ùêπ\nSNN ‚Üí CNN\nùê∫ SNN ‚Üí LSTM\nùêª SNN ‚Üí Transformer\nùêº\nSNN ‚Üí 3D CNN",
          "77.5\n78.0\n78.7\n79.2": "50.2\n50.2\n52.9\n53.0\n69.2\n69.8\n54.3\n54.3",
          "79.6\n80.0\n80.7\n81.1": "53.2\n53.2\n55.3\n55.2\n73.6\n74.2\n57.7\n57.7",
          "82.1\n82.6\n83.0\n83.2": "55.7\n55.6\n55.8\n55.7\n77.1\n77.3\n59.9\n59.9"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Methods Acc. of Emotion Class (%) Acc. under Light Conditions (%) Metrics (%)",
      "venue": "Methods Acc. of Emotion Class (%) Acc. under Light Conditions (%) Metrics (%)"
    },
    {
      "citation_id": "2",
      "title": "Ha Sa An Di Su Fe Ne Nor Over Low HDR WAR ‚Üë UAR ‚Üë FLOPS (G)",
      "venue": "Ha Sa An Di Su Fe Ne Nor Over Low HDR WAR ‚Üë UAR ‚Üë FLOPS (G)"
    },
    {
      "citation_id": "3",
      "title": "Resnet18 + LSTM",
      "year": "1997",
      "venue": "Resnet18 + LSTM"
    },
    {
      "citation_id": "4",
      "title": "Heart Rate Variability as an Index of Regulated Emotional Responding",
      "authors": [
        "M Bradley",
        "Linda Appelhans",
        "Luecken"
      ],
      "year": "2006",
      "venue": "Review of General Psychology",
      "doi": "10.1037/1089-2680.10.3.229"
    },
    {
      "citation_id": "5",
      "title": "Real-Time Expression-Sensitive HMD Face Reconstruction",
      "authors": [
        "P Xavier",
        "Julien Burgos-Artizzu",
        "Olivier Fleureau",
        "Thierry Dumas",
        "Fran√ßois Tapie",
        "Nicolas Leclerc",
        "Mollet"
      ],
      "year": "2015",
      "venue": "SIGGRAPH Asia 2015 Technical Briefs (Kobe, Japan) (SA '15)",
      "doi": "10.1145/2820903.2820910"
    },
    {
      "citation_id": "6",
      "title": "BoostMeUp: Improving Cognitive Performance in the Moment by Unobtrusively Regulating Emotions with a Smartwatch",
      "authors": [
        "Jean Costa",
        "Fran√ßois Guimbreti√®re",
        "Malte Jung",
        "Tanzeem Choudhury"
      ],
      "year": "2019",
      "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
      "doi": "10.1145/3328911"
    },
    {
      "citation_id": "7",
      "title": "The effect of ambient-light conditions on quantitative pupillometry: a history of rubber cup",
      "authors": [
        "David Couret",
        "Pierre Simeone",
        "S√©bastien Freppel",
        "Lionel Velly"
      ],
      "year": "2019",
      "venue": "Neurocritical Care"
    },
    {
      "citation_id": "8",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020",
      "doi": "10.1109/FG47880.2020.00131"
    },
    {
      "citation_id": "9",
      "title": "Mimamo net: Integrating micro-and macro-motion for video emotion recognition",
      "authors": [
        "Didan Deng",
        "Zhaokang Chen",
        "Yuqian Zhou",
        "Bertram Shi"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "10",
      "title": "ImageNet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2009.5206848"
    },
    {
      "citation_id": "11",
      "title": "Biologically Inspired Dynamic Thresholds for Spiking Neural Networks",
      "authors": [
        "Jianchuan Ding",
        "Bo Dong",
        "Felix Heide",
        "Yufei Ding",
        "Yunduo Zhou",
        "Baocai Yin",
        "Xin Yang"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems",
      "doi": "10.48550/arXiv.2206.04426"
    },
    {
      "citation_id": "12",
      "title": "Facial action coding systems",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding systems"
    },
    {
      "citation_id": "13",
      "title": "Event-Based Vision: A Survey",
      "authors": [
        "Guillermo Gallego",
        "Tobi Delbr√£ƒ≥ck",
        "Garrick Orchard",
        "Chiara Bartolozzi",
        "Brian Taba",
        "Andrea Censi",
        "Stefan Leutenegger",
        "Andrew Davison",
        "J√£≈±rg Conradt",
        "Kostas Daniilidis",
        "Davide Scaramuzza"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2020.3008413"
    },
    {
      "citation_id": "14",
      "title": "End-to-end learning of representations for asynchronous event-based data",
      "authors": [
        "Daniel Gehrig",
        "Antonio Loquercio",
        "Konstantinos Derpanis",
        "Davide Scaramuzza"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
      "doi": "10.1109/ICCV.2019.00573"
    },
    {
      "citation_id": "15",
      "title": "Combining Events and Frames Using Recurrent Asynchronous Multimodal Networks for Monocular Depth Prediction",
      "authors": [
        "Daniel Gehrig",
        "Michelle R√ºegg",
        "Mathias Gehrig",
        "Javier Hidalgo-Carri√≥",
        "Davide Scaramuzza"
      ],
      "year": "2021",
      "venue": "IEEE Robotics and Automation Letters",
      "doi": "10.1109/LRA.2021.3060707"
    },
    {
      "citation_id": "16",
      "title": "Recognizing facial expressions of occluded faces using convolutional neural networks",
      "authors": [
        "Mariana-Iuliana Georgescu",
        "Radu Tudor"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing",
      "doi": "10.1007/978-3-030-36808-1_70"
    },
    {
      "citation_id": "17",
      "title": "Spiking Neuron Models: Single Neurons, Populations, Plasticity",
      "authors": [
        "Wulfram Gerstner",
        "Werner Kistler"
      ],
      "year": "2002",
      "venue": "Spiking Neuron Models: Single Neurons, Populations, Plasticity"
    },
    {
      "citation_id": "18",
      "title": "Design of a Wearable Device for Reading Positive Expressions from Facial EMG Signals",
      "authors": [
        "Anna Gruebler",
        "Kenji Suzuki"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2313557"
    },
    {
      "citation_id": "19",
      "title": "Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet",
      "authors": [
        "Kensho Hara",
        "Hirokatsu Kataoka",
        "Yutaka Satoh"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00685"
    },
    {
      "citation_id": "20",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "21",
      "title": "Eyemotion: Classifying facial expressions in VR using eye-tracking cameras",
      "authors": [
        "Steven Hickson",
        "Nick Dufour",
        "Avneesh Sud",
        "Vivek Kwatra",
        "Irfan Essa"
      ],
      "year": "2019",
      "venue": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV.2019.00178"
    },
    {
      "citation_id": "22",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "J√ºrgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation",
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "citation_id": "23",
      "title": "Facial expression recognition under partial occlusion from virtual reality headsets based on transfer learning",
      "authors": [
        "Bita Houshmand",
        "Naimul Mefraz Khan"
      ],
      "year": "2020",
      "venue": "2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM)",
      "doi": "10.1109/BigMM50055.2020.00020"
    },
    {
      "citation_id": "24",
      "title": "EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model",
      "authors": [
        "Xinya Ji",
        "Hang Zhou",
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Wayne Wu",
        "Feng Xu",
        "Xun Cao"
      ],
      "year": "2022",
      "venue": "ACM SIGGRAPH 2022 Conference Proceedings (SIGGRAPH '22)",
      "doi": "10.1145/3528233.3530745"
    },
    {
      "citation_id": "25",
      "title": "Time adaptive recurrent neural network",
      "authors": [
        "Anil Kag",
        "Venkatesh Saligrama"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR46437.2021.01490"
    },
    {
      "citation_id": "26",
      "title": "Hots: a hierarchy of event-based time-surfaces for pattern recognition",
      "authors": [
        "Xavier Lagorce",
        "Garrick Orchard",
        "Francesco Galluppi",
        "Bertram Shi",
        "Ryad Benosman"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence",
      "doi": "10.1109/TPAMI.2016.2574707"
    },
    {
      "citation_id": "27",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "1024",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision",
      "doi": "10.1109/ICCV.2019.01024"
    },
    {
      "citation_id": "28",
      "title": "Multi-modal recurrent attention networks for facial expression recognition",
      "authors": [
        "Jiyoung Lee",
        "Sunok Kim",
        "Seungryong Kim",
        "Kwanghoon Sohn"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing",
      "doi": "10.1109/TIP.2020.2996086"
    },
    {
      "citation_id": "29",
      "title": "Facial Performance Sensing Head-Mounted Display",
      "authors": [
        "Hao Li",
        "Laura Trutoiu",
        "Kyle Olszewski",
        "Lingyu Wei",
        "Tristan Trutna",
        "Pei-Lun Hsieh",
        "Aaron Nicholls",
        "Chongyang Ma"
      ],
      "year": "2015",
      "venue": "ACM Trans. Graph",
      "doi": "10.1145/2766939"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition from multichannel EEG signals using K-nearest neighbor classification",
      "authors": [
        "Mi Li",
        "Hongpei Xu",
        "Xingwang Liu",
        "Shengfu Lu"
      ],
      "year": "2018",
      "venue": "Technology and Health Care",
      "doi": "10.3233/THC-174836"
    },
    {
      "citation_id": "31",
      "title": "EEG-Based Emotion Classification Using a Deep Neural Network and Sparse Autoencoder",
      "authors": [
        "Junxiu Liu",
        "Guopei Wu",
        "Yuling Luo",
        "Senhui Qiu",
        "Su Yang",
        "Wei Li",
        "Yifei Bi"
      ],
      "year": "2020",
      "venue": "Frontiers in Systems Neuroscience",
      "doi": "10.3389/fnsys.2020.00043"
    },
    {
      "citation_id": "32",
      "title": "A model of facial biomechanics for speech production",
      "authors": [
        "Jorge Lucero",
        "Kevin Munhall"
      ],
      "year": "1999",
      "venue": "The Journal of the Acoustical Society of America",
      "doi": "10.1121/1.428108"
    },
    {
      "citation_id": "33",
      "title": "Event-based vision meets deep learning on steering prediction for self-driving cars",
      "authors": [
        "Ana Maqueda",
        "Antonio Loquercio",
        "Guillermo Gallego",
        "Narciso Garc√≠a",
        "Davide Scaramuzza"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00568"
    },
    {
      "citation_id": "34",
      "title": "Pupillometry: Psychology, Physiology, and Function",
      "authors": [
        "Sebastiaan Math√£≈•t"
      ],
      "year": "2018",
      "venue": "Journal of Cognition",
      "doi": "10.5334/joc.18"
    },
    {
      "citation_id": "35",
      "title": "Recurrent neural networks with intra-frame iterations for video deblurring",
      "authors": [
        "Seungjun Nah",
        "Sanghyun Son",
        "Kyoung Mu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2019.00829"
    },
    {
      "citation_id": "36",
      "title": "SPIDERS: Low-Cost Wireless Glasses for Continuous In-Situ Bio-Signal Acquisition and Emotion Recognition",
      "authors": [
        "Jingping Nie",
        "Yigong Hu",
        "Yuanyuting Wang",
        "Stephen Xia",
        "Xiaofan Jiang"
      ],
      "year": "2020",
      "venue": "2020 IEEE/ACM Fifth International Conference on Internet-of-Things Design and Implementation (IoTDI)",
      "doi": "10.1109/IoTDI49375.2020.00011"
    },
    {
      "citation_id": "37",
      "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga",
        "Alban Desmaison",
        "Andreas Kopf",
        "Edward Yang",
        "Zachary Devito",
        "Martin Raison",
        "Alykhan Tejani",
        "Sasank Chilamkurthy",
        "Benoit Steiner",
        "Lu Fang",
        "Junjie Bai",
        "Soumith Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "Feature decomposition and reconstruction learning for effective facial expression recognition",
      "authors": [
        "Delian Ruan",
        "Yan Yan",
        "Shenqi Lai",
        "Zhenhua Chai",
        "Chunhua Shen",
        "Hanzi Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR46437.2021.00757"
    },
    {
      "citation_id": "39",
      "title": "Affective Processes: stochastic modelling of temporal context for emotion and facial expression recognition",
      "authors": [
        "Enrique Sanchez",
        "Mani Kumar Tellamekala",
        "Michel Valstar",
        "Georgios Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR46437.2021.00896"
    },
    {
      "citation_id": "40",
      "title": "Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wo?llmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2010.8"
    },
    {
      "citation_id": "41",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "Du Tran",
        "Heng Wang",
        "Lorenzo Torresani",
        "Jamie Ray",
        "Yann Lecun",
        "Manohar Paluri"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00675"
    },
    {
      "citation_id": "42",
      "title": "Event-Stream Representation for Human Gaits Identification Using Deep Neural Networks",
      "authors": [
        "Yanxiang Wang",
        "Xian Zhang",
        "Yiran Shen",
        "Bowen Du",
        "Guangrong Zhao",
        "Lizhen Cui Cui Lizhen",
        "Hongkai Wen"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2021.3054886"
    },
    {
      "citation_id": "43",
      "title": "EMO: Real-time emotion recognition from singleeye images for resource-constrained eyewear devices",
      "authors": [
        "Hao Wu",
        "Jinghao Feng",
        "Xuejin Tian",
        "Edward Sun",
        "Yunxin Liu",
        "Bo Dong",
        "Fengyuan Xu",
        "Sheng Zhong"
      ],
      "year": "2020",
      "venue": "Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services",
      "doi": "10.1145/3386901.3388917"
    },
    {
      "citation_id": "44",
      "title": "Spatio-temporal backpropagation for training high-performance spiking neural networks",
      "authors": [
        "Yujie Wu",
        "Lei Deng",
        "Guoqi Li",
        "Jun Zhu",
        "Luping Shi"
      ],
      "year": "2018",
      "venue": "Frontiers in neuroscience",
      "doi": "10.3389/fnins.2018.00331"
    },
    {
      "citation_id": "45",
      "title": "Transfer: Learning relationaware facial expression representations with transformers",
      "authors": [
        "Fanglei Xue",
        "Qiangchang Wang",
        "Guodong Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
      "doi": "10.1109/ICCV48922.2021.00358"
    },
    {
      "citation_id": "46",
      "title": "2021b. Object tracking by jointly exploiting frame and event domain",
      "authors": [
        "Jiqing Zhang",
        "Xin Yang",
        "Yingkai Fu",
        "Xiaopeng Wei",
        "Baocai Yin",
        "Bo Dong"
      ],
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
      "doi": "10.1109/ICCV48922.2021.01280"
    },
    {
      "citation_id": "47",
      "title": "2021a. Relative Uncertainty Learning for Facial Expression Recognition",
      "authors": [
        "Yuhang Zhang",
        "Chengrui Wang",
        "Weihong Deng"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "48",
      "title": "Former-DFER: Dynamic Facial Expression Recognition Transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia",
      "doi": "10.1145/3474085.3475292"
    }
  ]
}