{
  "paper_id": "2105.08957v1",
  "title": "Semour: A Scripted Emotional Speech Repository For Urdu",
  "published": "2021-05-19T07:15:03Z",
  "authors": [
    "Nimra Zaheer",
    "Obaid Ullah Ahmad",
    "Ammar Ahmed",
    "Muhammad Shehryar Khan",
    "Mudassir Shabbir"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Computing methodologies â†’ Speech recognition",
    "Language resources",
    "Neural networks",
    "Supervised learning by classification Emotional speech, speech dataset, digital recording, speech emotion recognition, Urdu language, human annotation, machine learning, deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufficient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the first scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition System. Our gender-balanced dataset contains 15, 040 unique instances recorded by eight professional actors eliciting a syntactically complex script. The dataset is phonetically balanced, and reliably exhibits a varied set of emotions as marked by the high agreement scores among human raters in experiments. We also provide various baseline speech emotion prediction scores on the database, which could be used for various applications like personalized robot assistants, diagnosis of psychological disorders, and getting feedback from a low-tech-enabled population, etc. On a random test sample, our model correctly predicts an emotion with a state-of-the-art 92% accuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sound waves are an information-rich medium containing multitudes of properties like pitch (relative positions of frequencies on a scale), texture (how different elements are combined), loudness (intensity of sound pressure in decibels), and duration (length of time a tone is sounded), etc. People instinctively use this information to perceive underlying emotions, e.g., sadness, fear, happiness, aggressiveness, appreciation, sarcasm, etc., even when the words carried by the sound may not be associated with these emotions. Due to the availability of vast amounts of data, and the processing power, it has become possible to detect and measure the intensity of these and other emotions in a sound bite. Speech emotion recognition systems can be used in health sciences, for example in aiding diagnosis of psychological disorders and speech-based assistance in communities and homes and elder care  [8, 26, 46] . Similarly, in developing countries where it is difficult to get feedback from the low-tech-enabled population using internet-based forms, it is possible to get a general overview of their opinion by getting feedback through speech recording and recognizing the emotions in this feedback  [20] . Another potential use of such systems is in measuring any discrimination (based on gender, religion, or race) in public conversations  [50] .\n\nA prerequisite for building such systems is the availability of large well-annotated datasets of sounds emotions. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. Urdu is the 11 ð‘¡â„Ž most widely spoken language in the world, with 171 million total speakers  [15] . Moreover, it becomes the third most widely spoken language when grouped with its close variant, Hindi. Many of the applications mentioned above are quite relevant for the native speakers of the Urdu language in South Asia. Thus, a lot of people will benefit from a speech emotion recognition system for the Urdu language. In this paper, we address this research problem and present the first comprehensive emotions dataset for spoken Urdu. We make sure that our dataset approximates the common tongue in terms of the distribution of phonemes so that models trained on our dataset will be easily generalizable. Further, we collect a large dataset of more than 15, 000 utterances so that state-of-the-art data-hungry machine learning tools can be applied without causing overfitting. The utterances are recorded in a sound-proof radio studio by professional actors. Thus, the audio files that we share with the research community are of premium quality. We have also developed a basic machine learning model for speech emotion recognition and report an excellent accuracy of emotion prediction Figure  1 . In summary, the paper has the following contributions:\n\nâ€¢ We study the speech emotion recognition in Urdu language and build the first comprehensive dataset that contains more than 15, 000 high-quality sound instances tagged with eight different emotions. â€¢ We report the results of human accuracy of detecting emotion in this dataset, along with other statistics that were collected during an experiment of 16 human subjects annotating about 5, 000 utterances with an emotion. â€¢ We train a basic machine learning model to recognize emotion in spoken Urdu language. We report an excellent crossvalidation accuracy of 92% which compares favorably with the state-of-the-art.\n\nIn the following section, we provide a brief literature overview of existing acoustic emotional repositories for different languages and Speech Emotion Recognition (SER) systems. In Section 3 , we provide details about the design and recording of our dataset. The results of the human annotation of SEMOUR are discussed in Section 4. In Section 5, we provide details of a machine learning framework to predict emotions using this dataset for training followed by a detailed discussion in Section 6. Finally, we conclude our work in Section 7.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Prominent examples of speech databases like Emo-DB  [5] , IEMO-CAP  [6] , are well studied for speech emotion recognition problem. The English language encompasses multiple datasets with each one having its unique properties. IEMOCAP  [6]  is the benchmark for speech emotion recognition systems, it consists of emotional dialogues between two actors. The dyadic nature of the database allows systems to learn in a way that it performs better when used in real life. IEMOCAP contains 12 hours of audio-video recordings performed by 10 actors and it has 10039 utterances rated by three annotators. The database consists of 5 emotions. Another wellstudied database is Emo-DB  [5]  which is in the German language and it contains seven emotions. Each emotion contains almost the same number of instances making it class-balanced which means it might be easier for learning algorithms to learn the emotions distribution. There are ten sentences in seven different emotions uttered by ten different speakers with a total of 535 instances. Emovo is an emotional corpus in the Italian language containing seven emotions performed by six actors  [12] . The content of the corpus is semantically constant to allow the tone of the delivery to play a greater role in predicting the emotion of an instance. The same methodology is also used in  [42]  while designing a database for the English language. Other dataset for English language include MSP-IPROV  [7] , RAVDESS  [31] , SAVEE  [23]  and VESUS  [42] . Speech corpus, EMO-DB  [5] , VAM  [19] , FAU Aibo  [4]  for German language and EMOVO  [12]  for Italian language are also available. Furthermore, CASIA  [53] , CASS  [28]  and CHEAVD  [30]  exists for Mandarin, Keio ESD  [34]  for Japanese, RECOLA  [40]  for French, TURES  [36]  and BAUM-1  [52]  for Turkish, REGIM-TES  [33]  for Arabic and SheMo  [35]  for Persian language. There are a lot of other databases in various languages along with their uses whose details can be found in  [49] .\n\nSpeech Emotion Recognition (SER) systems. Speech emotion recognition systems that are built around deep neural networks are dominant in literature. They are known to produce better results than classical machine learning algorithms  [17] . They are also flexible to various input features that can be extracted from speech corpus. Neural network architectures for speech emotions are built around three-block structures. First is the convolutional neural network (CNN) block to extract local features, then it is followed by a recurrent neural network (RNN) to capture context from all local features or to give attention  [2]  to only specific features. In the end, the weighted average is taken of the output using a fully connected neural network layer  [11, 54] . It has been shown that the CNN module combined with long short-term memory (LSTM) neural networks works better than standalone CNN or LSTM based models for SER tasks in cross-corpus setting  [37] . Other deep learning-based models like zero-shot learning, which learns using only a few labels  [51]  and Generative Adversarial Networks (GANs) to generate synthetic samples for robust learning have also been studied  [10] .\n\nVarious features are used for solving speech emotion recognition problem. They vary from using Mel-frequency Cepstral Coefficients (MFCC)  [43]  to using Mel-frequency spectrogram  [48] . Other most commonly used ones are ComParE  [45]  and extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)  [16] . To improve the accuracy of SER systems, additional data has been incorporated with the speech corpus. Movie script database has been used to generate a personalized profile for each speaker while classifying emotion for individual speakers  [29] . Fusion techniques, that fuse words, and speech features to identify emotion have been studied in  [47] .\n\nResources for the Urdu Language. The only previously available emotionally rich dataset for URDU language  [27]  consists of spontaneous speech extracted from talk shows. It contains only 400 audio clips with 4 emotion classes and its instances lack proper sentencelevel segmentation. It includes instances that have no vocals. These instances are validated by only 2 annotators each, and no further details of validation are provided. The size of the dataset and quality of vocals in audio clips is not suited for the learning algorithm to generalize various emotions over a complex vocabulary.\n\nApart from the above mentioned emotional dataset for the Urdu language, there are several large corpora of spoken Urdu developed in previous works with some applications in Speech automation. Unfortunately, none of these datasets are labeled for emotions. Also, the instances in these datasets lack the emotional aspect of the speech and monotonically fall within the natural class of emotion. Two of the most famous datasets for spoken Urdu are designed for models that would benefit from neutral or emotion-free speech  [1, 39] . The applications of such datasets include text-to-speech systems and automatic speech recognition systems to detect and recognize speaker attributes, for example, gender, age, and dialect. They are not suitable for a generalized speech emotion recognition system.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Semour Script Design",
      "text": "Dataset construction for speech instances is a tiresome endeavor with a lot of inherent challenges. In this section, we provide script design details and the obstacles faced during the process.\n\nUrdu is an Indo-Aryan language with 171 million speakers all over the world according to the Ethnologue 23 ð‘Ÿð‘‘ edition  [15] . It is the national language of Pakistan and an official language in more than five states of India. The language is known for its exceptionally rich inventory of 67 phonemes as compared to 36 phonemes of English and 35 phonemes of Chinese  [25] . It follows a Perso-Arabic written style named Nastaliq  [22] . There are four dialects namely Urdu, Dakhini, Hyderabadi Urdu and Rekhta  [18] . There exist six diverse types of accents namely Urdu, Punjabi, Pashto, Saraiki, Balochi, and Sindhi for Urdu dialect  [38] . A language spoken by such a large community poses diverse challenges while procuring a speech dataset. Diversity in dialect is one of the major concerns and selecting speakers to cover such diversity is a laborious task. Moreover, coverage of all phonemes plays a key role in designing a rich acoustic dataset.\n\nAs mentioned above, our focus while building this dataset is to mimic routine conversations among native speakers. Therefore, the goal of a phonetically balanced repository is to ensure that frequencies of all instances of sounds closely approximate their densities in the set of spoken words. To achieve this goal, we use two sources. Firstly, we considered the set of top 5000 most frequently used Urdu words that appear in the list collected here  [22] .\n\nSecondly, we use a complete Urdu lexicon of about 46, 000 words that were collected in  [21] . A uniformly random sample of words from any one of these sources will have, in expectation, the property of being phonetically balanced. However, we also wanted our dataset to have phrases and sentences with sufficient lexical complexity. Our script consists of 235 instances composed of 43 common words , 66 two-word phrases, and 126 simple-structured Urdu sentences as shown in Table  1 . Frequently used vocabulary, and a small subset of emotionally enabled sentences against each emotion has also been incorporated in the preparation of the script.\n\nWe used methods in  [21] , to compute phoneme frequency for our script and compared it against the two sources as shown in Figure  2 . It can be seen that our script covers all phonemes as frequently as they appear in the two standard datasets almost always. Also note that model  [21]  is trained for 62 phonemes, hence Figure  2  shows plot for phoneme normalized frequencies against 62 phonemes.\n\nWe used the services of a local radio studio to conduct soundproof recording sessions. After the recording, basic noise reduction, voice normalization, and amplification techniques are applied to the audio clips. Each recording is manually segmented based on script sequence, and emotion instance. The final dataset consists of more than 15, 000 utterances with a cumulative duration of 6 hr, 55 min, 29 sec. We have uploaded high and low definition versions of our dataset 1  and are making it publicly available for the research community. Each audio clip in the low definition version of the dataset has 2 channels (stereo) with sample rate, sample size, and bit rate of 44.100 kHz, 16 bit, and 1411 kbps, respectively. Each audio clip in high definition has 2 channels (stereo) with sample rate, sample size, and bit rate of 48.000 kHz, 24 bit, and 2304 kbps, respectively. Table  1  shows the details of utterance and script level statistics.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Human Evaluation Of Semour",
      "text": "The first experiment that we performed on SEMOUR, was the human evaluation of the dataset. We selected one-third of the audio clips uniformly at random from our dataset and presented them to the evaluators one at a time with a list of questions. We used sixteen evaluators to annotate about 5, 000 clips with each clip receiving at least two evaluations. All of the annotators have proficient Urdu speaking and Listening skills. All but two have Urdu as their first language; the other two have Urdu as a second language. All the evaluators have a minimum of secondary school education aging from 17 to 32 years old.\n\nAfter listening to each clip, evaluators were asked to input two evaluations, namely discrete categorical (a choice among neutral, happiness, surprise, sadness, boredom, fearful, anger, and disgust) and continuous attribute (a numeric value for each of valence, dominance, and activation). Valence encapsulates degrees of pleasantness elicited in an audio clip varying from negative to positive, activation depicts the levels of intensity such as calmness or excitement and dominance portrays the speaker's control over the utterance i.e. weak or strong  [41] . Furthermore, an actor's performance was also rated in terms of genuineness, i.e., fake or natural. The evaluators were given a short training on the use of the survey application and the meaning of each term used in the questions.\n\nWe performed extensive analyses on the feedback received from evaluators. As shown in Table  3 , we see an average accuracy of 78%, i.e., on average, an evaluator correctly identified the emotion in a clip with 78% accuracy. With majority voting, we observe an average accuracy of 79% as shown in Table  4 . This is a very high accuracy compared to a random classification which would result in an accuracy of 12.5%. One can conclude, that most of the audio files are labeled with correct emotion. Secondly, this also shows that humans can correctly perceive labeled emotions in most of these clips. However, we observe some variance in the accurate recognition of different emotions. For example, two emotions, Disgust and Fearful, were identified with the lowest accuracy as compared to other emotions. It turns out that some instances of Disgust were confused with Anger and Neutral emotions. We suspect that this emotion is very hard to differentiate from other emotions in general, and probably, not as commonly used as other emotions. Similarly, Fear was incorrectly classified as Sadness in some instances because shivers while crying were perceived as hiccups. Furthermore, these ratings also give us a yardstick to measure the performance of individual actors. For each actor, accuracy, precision, recall, f1-score along with Cohen's kappa score for measuring rater's agreement are presented in Table  2 . It can be observed that actor number 8 performed well and actor number 3 has the highest rater's agreement.\n\nMoreover, all scores are greater than 0.4, (i.e., fair agreement), we conclude that our actors performed well and the general audience was able to distinguish among the emotions uttered. Additionally, while measuring genuineness of acting, there were 115 and 267 votes for fake category for males and females, respectively, implying that male actors may have performed relatively better as shown in Figure  3a .\n\nWe have also performed experiments based on the syntactical complexity of our script, as naturally perceived single words and phrases encapsulate fewer emotions than sentences. Intuitively, it should be harder to utter emotion in a word or phrase. Our results confirm this intuition in terms of accuracy, precision, recall, and f1-measure as shown in Figure  3b . Along with discrete labels, raters were asked to rank activation and dominance for each utterance. With majority voting, our repository conforms with generally perceived notions of these values in the discrete emotions, e.g., sadness has low activation and weak dominance as compared to Anger. The results are as shown in Figure  3c .\n\nLastly, we applied t-Distributed Stochastic Neighbor Embedding (t-SNE), a dimensionality reduction technique to our ratings for detailed analysis of emotions. Figure  4a  shows the t-SNE relation between ground truth and the independent variables annotator had control on, i.e., Emotion-Tag, Valence, Activation, Dominance, and Genuineness (TVADG). The distribution for various emotions in the figure shows the relative impacts of all of the annotated properties, and an overlap between two color classes indicates a potentially incorrect annotation. For example, it seems that point clouds for Happiness and Surprise emotions are close to each other, implying that they may have been misclassified for each other. Similarly, Sadness and Disgust classes have some overlap too. On the other hand, classes of Anger, Neutral, Boredom, and Disgust emotion seem quite distinguishable from each other. Figure  4b  is the t-SNE relation between ground truth and the independent variables annotator had excluding the emotion-tag property, i.e., VADG. Since the properties have a very small distribution, the clusters of different classes are subsumed into one another. Although, boredom class remains the most distinguishable one.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition Using Machine Learning",
      "text": "The ultimate goal of building a large balanced dataset is to be able to train machine learning models that can predict emotions in an Figure  3 : Results for experimentation on human annotation : (a) performance analysis against evaluation metrics for male and female actors. Male actors perform better than female actors when their clips were annotated. The values against males for accuracy, recall, precision, F1 score are 0.83, 0.83, 0.83 and 0.83 respectively. The values against females for accuracy, recall, precision, F1 score are 0.79, 0.81, 0.79 and 0.79 respectively. (b) performance analysis against evaluation metrics for the script's lexical complexity. Sentences cover rich emotion than phrases and so on. The values against words for accuracy, recall, precision, F1 score are 0.70, 0.71, 0.70 and 0.70 respectively. The values against phrases for accuracy, recall, precision, F1 score are 0.81, 0.82, 0.81 and 0.81 respectively.The values against sentences for accuracy, recall, precision, F1 score are 0.82, 0.83, 0.82 and 0.82 respectively. Performance scale for (a) and (b) ranges from 0 to 1. (c) Continuous attributes categorization for each emotion. The left and right y-axis depict activation (low, natural, calm, and excited) and dominance (weak, neutral, and strong) respectively. Surprise has excited activation and neutral dominance.\n\nunseen sound clip. In this section, we discuss the performance, and evaluation of the first machine learning model to use SEMOUR as a dataset. We aim to solve a multi-label classification problem that predicts emotion based on features extracted from an acoustic clip. Formally, Let ð‘ˆ = {ð‘¢ 1 , ð‘¢ 2 , . . . , ð‘¢ ð‘› } âˆˆ R ð‘›Ã—ð‘‘ be a list input vector where ð‘› is the number of utterances in a training dataset, and ð‘‘ is the number of features extracted from each utterance. Let output vector be ð‘‚ = {ð‘œ 1 , ð‘œ 2 , . . . , ð‘œ ð‘› : 1 â‰¤ ð‘œ ð‘– â‰¤ ð‘š} where each ð‘œ ð‘– is the class label associated with the training input vector ð‘¢ ð‘– , and ð‘š is the number of emotions. Our goal is to learn a function F : ð‘ˆ â†’ ð‘‚ so that, (1). function F correctly maps emotion label ð‘œ ð‘– for a feature vector ð‘¢ ð‘– for maximum number of instances in ð‘ˆ , (training accuracy), (2) F outputs a correct emotion label for a feature vector ð‘¢ ð‘— âˆ‰ ð‘ˆ that corresponds to an unseen audio clip, assuming unseen clip is drawn from the distribution of ð‘ˆ , (test accuracy).\n\nThe features used for training were Mel-frequency Cepstral Coefficients (MFCCs), chromagram, and Mel-spectrogram. Collectively, for each sample, an array of 40 coefficients of MFCC, 12 pitch classes values, and 128 mean values of Mel-spectrogram form a feature vector of size, ð‘‘ = 180. This feature vector is used for the task of   classification. Visualization of features for neutral emotion audio sample are shown in Figure  5 .\n\nA five-layered neural architecture was trained for classification, as shown in Figure  5 . Four hidden layers with 1024, 512, 64, and 48 neurons were used respectively along with 8-dimensional fully connected output layer. The parameters for epochs, L1-regularization, learning rate, batch size were set to 30,0.001, 0.001, and 50, respectively, along with Scaled Exponential Linear Units (SELU) and softmax as activation functions for hidden and output layers respectively. The above mentioned three features were extracted using the Librosa speech library and were used as an input to our proposed architecture  [32] . Three different variations of the experiment namely random splits, leave one speaker out, and gender based setting were selected for detailed analysis.\n\nOur proposed architecture outperforms existing machine learning techniques as shown in Table  5 . The comparison features of SEMOUR and the accuracy of our model are compared with the existing dataset in Table  6 . In the following subsections, we provide comprehensive details regarding experimentation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Stochastic Division Of Train And Test Sets",
      "text": "The first experiment, we designed, was based on a stochastic split experiment, where we test our accuracy on the complete dataset. For this experiment, we used the architecture as explained in Figure  5 . We trained our model for 100 epochs on a randomly selected 90% dataset  (13, 536 instances) . Once trained, we test it on the remaining 1504 instances and obtained the highest accuracy of 92% and an average accuracy of 90%. To validate the experiment, 10-folds cross-validation technique was used. The results are visualized in Figure  6c . Accuracy for individual emotion class was also analyzed for the variance. We observed that our model performed exceptionally well on the Boredom and Neutral emotions with an accuracy of 99%, and 98%, respectively. The worse performing emotions were Fearful and Happiness which were identified with an accuracy of 86%, and 85%, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Gender-Based Analysis Among Actors",
      "text": "The second experiment, we conducted, was a gender-based experiment where we had a binary combinatorial testing technique. We have an equal distribution of male and female actors in our dataset, therefore, in a random setting, one would expect a balancing classification accuracy. All four binary combinations were evaluated, i.e., binary choices are male and female actors, and training and testing samples.\n\nAs shown in Table  7  the experiments on the same gender yielded excellent results whereas the cross-gender study resulted in significantly lower accuracy. We believe this is since each speaker primarily has an independent distribution that is not concerning a specific gender, rather unique to each individual. To cement this we conducted a leave-one-out experiment on the same gender and the accuracy dropped from 96% to 50% for the males and 92% to 45% for the females. The details of the leave-one-out experiment are discussed in the next subsection.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Speaker Independent Cross-Validation Experiment",
      "text": "In this experimentation setting, the model was trained on 7 actors and tested on the remaining actor with overall testing accuracy, recall, precision, and f1-score of 39%, 39%, 36%, and 35%, respectively, averaged over all actors as shown in Table  7 . In the individual actor analysis, predictions on actor 3 have, relatively, better results. As mentioned earlier, this below-par accuracy is due to a significantly different distribution of features for individual actors, as seen in Figure  6a . The model fails on an unseen actor because of diversity in style to utter emotions. Training accuracy was observed to be 100%, even with extensive experimentation with reduction of layers, the addition of regularization, low learning rate, the testing accuracy did not improve, which shows that a simple deep neural network is not suitable for this variation of the experiment. We propose that complex models like LSTMs and transformers should be tested for better representation of heterogeneous distributions among speakers. Moreover, our model can only identify anger and surprise emotions for all actors and perform well while predicting anger emotion as compared to others as shown in Figure  6b . Boredom, happiness, and sadness emotions cannot be discriminated against for all actors. Disgust has the lowest overall accuracy for all speakers. We conclude that there exists heterogeneity among speakers and emotions collectively which is only natural considering diversity in emotion utterance for each individual, i.e., elicitation of similar emotions can vary speaker-wise.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Discussion",
      "text": "In this section, we elaborate on the design and construction of our dataset and the results of the human annotation process. We also discuss the limitations of our current work in terms of dialect representation, machine-based model generalization, and speech spontaneity.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dataset Construction Process",
      "text": "This section summarizes the pursuit of gathering a high-quality database followed by the authors for the production or extension of any dataset. The first and foremost step is to select a language and study relevant published work to identify possible gaps in the  resources available for that language. In light of our problem statement, a phonetically balanced script was designed enriched with words, phrases, and sentences to be elicited in diverse emotions. Rich vocabulary sentences in the neutral speech were available  [39]  but not useful in our scenario as the vocabulary employed in these is not used in daily routine, hence a new script had to be designed to target the spoken Urdu language composed from frequently used words. The next step was to recruit actors by advertisement following a strict screening process based upon their language proficiency and performance experience. The script was distributed among the speakers for practice before recordings. A soundproof recording room was booked for actors to record their sessions. Actors with a performance lacking in the authenticity of emotions were asked to rerecord for better elicitation. Hence, recordings against each actor's emotion were procured and clipped according to the strict order and correct pronunciation of the script's instances.\n\nOnce the sound clips were ready, one-third of the repository was tagged by annotators to obtain human accuracy. A user-friendly application to aid annotation was designed to achieve this goal. Annotators were asked to tag discrete categorical and continuous Figure  6 : Summarized results for SER experiments with performance ranging from 0 to 1. (a) Performance analysis across actors against evaluation metrics. The mean value for accuracy, recall, precision, F1 score is 0.39, 0.39, 0.36 and 0.36 respectively. The standard deviation for accuracy, recall, precision, F1 score is 0.09, 0.09, 0.1 and 0.1 respectively. (b) Performance analysis across all emotions against each actor. The mean value for anger, boredom, disgust, fearful, happiness, neutral, sadness and surprise is 0.58, 0.52, 0.27, 0.27, 0.28, 0.24, 0.24, 0.23 and 0.50 respectively. The standard deviation for anger, boredom, disgust, fearful, happiness, neutral, sadness and surprise is 0.09, 0.41, 0.18, 0.31, 0.21, 0.27, 0.22 and 0.22 respectively. The legend for (a) and (b) is shown on the right side. (c) Average accuracy against all emotions for stochastic testing. The graph shows that average accuracy is not only very high, it is also stable and consistent among all emotions. The mean value for anger, boredom, disgust, fearful, happiness, neutral, sadness and surprise is 0.89, 0.92, 0.89, 0.80, 0.84, 0.91, 0.89, 0.84 and 0.89 respectively. The standard deviation for anger, boredom, disgust, fearful, happiness, neutral, sadness and surprise is 0.03, 0.04, 0.04, 0.08, 0.06, 0.02, 0.06 and 0.05 respectively. attributes along with the authenticity of sound clips. Extensive experimentation to address accuracy and performance measures were performed along with comparative analysis for providing a fine benchmark for further explorations on the repository. Figure  1  elaborates the aforementioned steps for procuring this dataset.\n\nThe authors would like to highlight that each step of acquisition posed diverse challenges. Instances of the script were modified continuously until a reasonable balance in phonemes and their frequencies were achieved as compared to the existing language corpus and most frequently used word list. Moreover, various re-takes were performed to ensure correct pronunciations and authentic emotion elicitation by actors. Post-processing of audio clips after successful recording sessions was indeed a tedious task. Gaps of silences were removed then clips were pruned and renamed according to instances of the script. During this process, a list of mistakes was maintained for the actors to elicit the mispronounced instances again. Experimentation on the dataset was only made possible once all the instances were correctly saved.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Interpreting Human Subjects Study",
      "text": "As discussed in section 4 we used human subjects to evaluate onethird of the instances in SEMOUR. The goal of this study was to verify whether the expressions in these instances represent the corresponding emotion labels. Among the randomly chosen subset, 78% of the instances were correctly classified by a human evaluator on average. We concluded that these 78% instances contain enough information to detect a correlation between the acted expression and the corresponding emotion label. For the remaining 22% instances, there are two possibilities. Either those instances were not uttered or recorded with the correct emotional expression, or, the instances contain the correct acted expression but it was missed by the human evaluator and they classified it incorrectly. We designed an experiment to investigate the cause of these misclassifications by human evaluators as follows. We trained a simple Neural Network on the instances that were correctly classified by the evaluators. So, the model learned to distinguish the correct expression based on the labels of instances on which human evaluators agreed with dataset labels. We tested this Neural Network model on the remaining instances that were either acted incorrectly by actors or misclassified which human evaluators. The Neural Network achieved a test accuracy of 92% on these instances. This indicated a significant correlation between the emotion-labels and the features of respective utterances. We conclude that these correlations may have been missed by the evaluators.\n\nThere is also a concern whether the uttered instances contain exaggerated expressions of emotions which may lead to a system that won't generalize to spontaneous speech. We asked our human subjects to tag whether a given acted expression was perceived as natural or exaggerated. Of the 9977 sample instances for which we received a vote, > 84% were reported to have a natural expression while the remaining < 16% were tagged with a fake or exaggerated expression. We conclude that most of the acted expressions in SEMOUR successfully mimic the corresponding natural expressions in spontaneous speech.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Limitations",
      "text": "Semour is the first dataset of scripted emotional speech dataset for the Urdu Language recorded in eight complex emotions. Naturally, there are several limitations in this work that offer interesting avenues for improvement. Below, we discuss these limitations and provide future directions for this work.\n\nStudies on speech recognition systems have noted several disadvantages to using acted speech as compared to data from spontaneous conversations in natural settings  [3, 14] . As the ultimate goal of a recognition system is to classify uncontrolled speech, a model trained on natural datasets is expected to generalize better. Similarly, the spontaneous speech also provides a context that is lacking in the acted counterpart  [9] . Despite these benefits, most of the works on speech emotion recognition, including SEMOUR, are based on datasets that use acted speech expressions as mentioned in  [49] . The general drawback in building any potential datasets of spontaneous speech is that the audio samples are unlabelled by nature and depend on either the subjective opinion of the people tagging the dataset or some machine-assisted predictions to establish the ground truth. Secondly, the collection and cleaning process of spontaneous data requires considerably more effort  [13] . For instance, the voice quality in the natural datasets has a high variance that results in a significant loss of accuracy, as studied in  [44] . Therefore, one needs more advanced methods for noise removal. As one expects, natural datasets are also highly unbalanced for emotion classes. An overwhelming majority of the instances need to be discarded because they contain redundant information that is unproductive for model training. The construction of a natural speech dataset while overcoming these challenges is an open problem. Meanwhile, there is ample evidence to suggest that acted speech expressions provide a good approximation to spontaneous speech  [24] . Also, acted speech expressions are ideal to train on for certain applications. One such potential application is to understand the extent of any inherent emotional bias towards a community in movies. Since unobserved target utterances are acted expressions, it is a good idea to have a model train on acted instances.\n\nUrdu has four dialects and several accents that depend on the demographics of the speaker  [18] . Due to a lack of resources and accessibility, utterances in SEMOUR are limited to a single (Urdu) dialect spoken by the native people of Lahore. An extension covering rich dialect representation is required in this repository. Our current work also lacks the study of the impact of demographic effects on actors' emotion elicitation as well as taggers' human annotation.\n\nLastly, We report a modest 39% accuracy on the leave-one-out experiment. We strongly believe this is due to the limitations of the simple NN that we use. Our main contribution is a dataset, and we have not invested efforts into a sophisticated model. The goal of the experiments is to provide a baseline for future works. Improving the accuracy of classification on unknown data using a deep model is an interesting future direction. Training a model to predict emotions for a previously unknown speaker is, indeed, a challenging problem. For example, a CNN-BLSTM based model on the IEMOCAP dataset reports 52% accuracy for 6 emotions  [54] . This also highlights a problem regarding diversity in speaker profiles. Along with utilizing a deep model, learning better speaker embeddings might improve speaker-independent accuracy.\n\nFor future direction, we plan to diversify SEMOUR to cover all the major dialects as well as accents of the Urdu Language. With this goal in mind, we invite the research community to expand the dataset further to help create a universal speech emotion recognition system for Urdu. The dataset and other relevant metadata are available at acoustics-lab.itu.edu.pk through a request form. We allow the research community to download and independently extend and improve the dataset for non-commercial research purposes. We have also devised a system where the researchers can help improve the current version of the SEMOUR dataset by supplementing the recorded samples in different dialects and accents. The researchers can get the dataset and the script from acoustics-lab.itu.edu.pk and have their uttered collection appended in the current version after quality checks. We also intend to expand the current version on our own to develop a more generalized speech emotion recognition system.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we provide a first of its kind, gender, and phonetically balanced, scripted emotional speech dataset for the Urdu Language recorded by eight actors in eight complex emotions with 15, 040 unique instances. A uniformly selected one-third of the instances in SEMOUR are manually tagged and validated by resulting in human accuracy of 78% and high inter-evaluator correlation scores. We also provide a 5-layered neural network for speech emotion recognition task with variations of experiments on SEMOUR in comparison with classical machine learning techniques. Our model performs with an average accuracy of 90% for a 10-fold random split experiment.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This figure elaborates steps involved in the design of SEMOUR: scripted emotional speech repository for Urdu along",
      "page": 1
    },
    {
      "caption": "Figure 1: In summary,",
      "page": 2
    },
    {
      "caption": "Figure 2: It can be seen that our script covers all phonemes as frequently as",
      "page": 3
    },
    {
      "caption": "Figure 2: Phoneme comparison of our designed script with Urdu lexicon of 46, 000 words, and 5000 most frequently used words",
      "page": 4
    },
    {
      "caption": "Figure 3: b. Along with discrete labels, raters",
      "page": 4
    },
    {
      "caption": "Figure 4: a shows the t-SNE relation",
      "page": 4
    },
    {
      "caption": "Figure 3: Results for experimentation on human annotation : (a) performance analysis against evaluation metrics for male",
      "page": 5
    },
    {
      "caption": "Figure 4: t-SNE plots against human annotation. (a) Ground truth plotted against Tag, Valence, Activation, Dominance, Gen-",
      "page": 6
    },
    {
      "caption": "Figure 5: A five-layered neural architecture was trained for classification,",
      "page": 6
    },
    {
      "caption": "Figure 5: Four hidden layers with 1024, 512, 64, and 48",
      "page": 6
    },
    {
      "caption": "Figure 5: We trained our model for 100 epochs on a randomly selected 90%",
      "page": 7
    },
    {
      "caption": "Figure 6: c. Accuracy for individual emotion class was also analyzed",
      "page": 7
    },
    {
      "caption": "Figure 6: a. The model fails on an unseen actor because of diversity in",
      "page": 7
    },
    {
      "caption": "Figure 6: b. Boredom, happiness,",
      "page": 7
    },
    {
      "caption": "Figure 5: Proposed 5-layered neural network for Speech Emotion Recognition (SER). Three core features namely MFCCs, chro-",
      "page": 8
    },
    {
      "caption": "Figure 6: Summarized results for SER experiments with performance ranging from 0 to 1. (a) Performance analysis across",
      "page": 9
    },
    {
      "caption": "Figure 1: elaborates the aforementioned steps for procuring this dataset.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "Mal\nFem",
          "Column_5": "e\nale"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "Wo\nPhr\nSen",
          "Column_5": "rd\nas\nte",
          "Column_6": "s\nes\nnc",
          "Column_7": "e",
          "Column_8": "s"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Av",
          "Column_2": "erage",
          "Column_3": "Ac",
          "Column_4": "cura",
          "Column_5": "ccy",
          "Column_6": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A medium vocabulary Urdu isolated words balanced corpus for automatic speech recognition",
      "authors": [
        "Hazrat Ali",
        "Nasir Ahmad",
        "Khawaja Yahya",
        "Omar Farooq"
      ],
      "year": "2012",
      "venue": "2012 international conference on electronics computer technology"
    },
    {
      "citation_id": "2",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015. Computational and Biological Learning Society"
    },
    {
      "citation_id": "3",
      "title": "Desperately seeking emotions or: Actors, wizards, and human beings",
      "authors": [
        "Anton Batliner",
        "Kerstin Fischer",
        "Richard Huber",
        "JÃ¶rg Spilker",
        "Elmar NÃ¶th"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion. International Speech Communication Association"
    },
    {
      "citation_id": "4",
      "title": "Releasing a thoroughly annotated and processed spontaneous emotional database: the FAU Aibo Emotion Corpus",
      "authors": [
        "Anton Batliner",
        "Stefan Steidl",
        "Elmar NÃ¶th"
      ],
      "year": "2008",
      "venue": "Proc. of a Satellite Workshop of LREC"
    },
    {
      "citation_id": "5",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Ninth European Conference on Speech Communication and Technology. International Speech Communication Association"
    },
    {
      "citation_id": "6",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "MSP-IMPROV: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "A Framework for Recognizing and Regulating Emotions in the Elderly",
      "authors": [
        "Carlos JosÃ©",
        "Antonio Castillo",
        "Ãlvaro FernÃ¡ndez-Caballero",
        "Miguel Castro-GonzÃ¡lez",
        "MarÃ­a Salichs",
        "LÃ³pez"
      ],
      "year": "2014",
      "venue": "Ambient Assisted Living and Daily Activities"
    },
    {
      "citation_id": "9",
      "title": "Where did the anger go? The role of context in interpreting emotion in speech",
      "authors": [
        "Richard T Cauldwell"
      ],
      "year": "2000",
      "venue": "ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion. International Speech Communication Association"
    },
    {
      "citation_id": "10",
      "title": "Data Augmentation Using GANs for Speech Emotion Recognition",
      "authors": [
        "Aggelina Chatziagapi",
        "Georgios Paraskevopoulos",
        "Dimitris Sgouropoulos"
      ],
      "year": "2019",
      "venue": "Georgios Pantazopoulos, Malvina Nikandrou, Theodoros Giannakopoulos, Athanasios Katsamanis, Alexandros Potamianos, and Shrikanth Narayanan"
    },
    {
      "citation_id": "11",
      "title": "3-D convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "12",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "13",
      "title": "Emotional speech: Towards a new generation of databases",
      "authors": [
        "Ellen Douglas-Cowie",
        "Nick Campbell",
        "Roddy Cowie",
        "Peter Roach"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "14",
      "title": "Multimodal databases of everyday emotion: Facing up to complexity",
      "authors": [
        "Ellen Douglas-Cowie",
        "Laurence Devillers",
        "Jean-Claude Martin",
        "Roddy Cowie",
        "Suzie Savvidou",
        "Sarkis Abrilian",
        "Cate Cox"
      ],
      "year": "2005",
      "venue": "Ninth European conference on speech communication and technology. International Speech Communication Association"
    },
    {
      "citation_id": "15",
      "title": "Ethnologue: Languages of the world",
      "authors": [
        "Gary David M Eberhard",
        "Charles Simons",
        "Fennig"
      ],
      "year": "2020",
      "venue": "Ethnologue: Languages of the world"
    },
    {
      "citation_id": "16",
      "title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "BjÃ¶rn Schuller",
        "Johan Sundberg",
        "Elisabeth AndrÃ©",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "18",
      "title": "Twitter and Urdu",
      "authors": [
        "S Ghulam",
        "T Soomro"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET)"
    },
    {
      "citation_id": "19",
      "title": "",
      "authors": [
        "Ieee",
        "Pakistan Sukkur"
      ],
      "venue": "",
      "doi": "10.1109/ICOMET.2018.8346370"
    },
    {
      "citation_id": "20",
      "title": "The Vera am Mittag German audio-visual emotional speech database",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "2008 IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "21",
      "title": "Ordinal Learning for Emotion Recognition in Customer Service Calls",
      "authors": [
        "Wenjing Han",
        "Tao Jiang",
        "Yan Li",
        "BjÃ¶rn Schuller",
        "Huabin Ruan"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "PronouncUR: An Urdu Pronunciation Lexicon Generator",
      "authors": [
        "Agha Ali",
        "Raza Haris",
        "Bin Zia",
        "Awais Athar"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "23",
      "title": "Corpus based Urdu lexicon development",
      "authors": [
        "Madiha Ijaz",
        "Sarmad Hussain"
      ],
      "year": "2007",
      "venue": "the Proceedings of Conference on Language Technology (CLT07)"
    },
    {
      "citation_id": "24",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "25",
      "title": "Effect of acting experience on emotion expression and recognition in voice: Non-actors provide better stimuli than expected",
      "authors": [
        "Rebecca JÃ¼rgens",
        "Annika Grass",
        "Matthis Drolet",
        "Julia Fischer"
      ],
      "year": "2015",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "26",
      "title": "Speech assessment methods phonetic alphabet (SAMPA): Analysis of Urdu",
      "authors": [
        "Hasan Kabir",
        "Abdul Mannan"
      ],
      "year": "2002",
      "venue": "Speech assessment methods phonetic alphabet (SAMPA): Analysis of Urdu"
    },
    {
      "citation_id": "27",
      "title": "The effect of emotional speech on a smart-home application",
      "authors": [
        "Theodoros Kostoulas",
        "Iosif Mporas",
        "Todor Ganchev",
        "Nikos Fakotakis"
      ],
      "year": "2008",
      "venue": "International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems"
    },
    {
      "citation_id": "28",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "Siddique Latif",
        "Adnan Qayyum",
        "Muhammad Usman",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "International Conference on Frontiers of Information Technology (FIT)",
      "doi": "10.1109/FIT.2018.00023"
    },
    {
      "citation_id": "29",
      "title": "CASS: A phonetically transcribed corpus of Mandarin spontaneous speech",
      "authors": [
        "Aijun Li",
        "Fang Zheng",
        "William Byrne",
        "Pascale Fung",
        "Terri Kamm",
        "Yi Liu",
        "Zhanjiang Song",
        "Umar Ruhi",
        "Veera Venkataramani",
        "Xiaoxia Chen"
      ],
      "year": "2000",
      "venue": "Sixth International Conference on Spoken Language Processing. International Speech Communication Association"
    },
    {
      "citation_id": "30",
      "title": "Attentive to Individual: A Multimodal Emotion Recognition Network with Personalized Attention Profile",
      "authors": [
        "Jeng-Lin Li",
        "Chi-Chun Lee"
      ],
      "year": "2019",
      "venue": "INTERSPEECH. International Speech Communication Association"
    },
    {
      "citation_id": "31",
      "title": "CHEAVD: a Chinese natural emotional audio-visual database",
      "authors": [
        "Ya Li",
        "Jianhua Tao",
        "Linlin Chao",
        "Wei Bao",
        "Yazhu Liu"
      ],
      "year": "2017",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "32",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "33",
      "title": "Eric Battenberg, and Oriol Nieto. 2015. librosa: Audio and music signal analysis in python",
      "authors": [
        "Brian Mcfee",
        "Colin Raffel",
        "Dawen Liang",
        "P Daniel",
        "Matt Ellis",
        "Mcvicar"
      ],
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "34",
      "title": "Building and analysing emotion corpus of the Arabic speech. In 2017 1st International Workshop on Arabic Script Analysis and Recognition (ASAR)",
      "authors": [
        "Mohamed Meddeb",
        "Hichem Karray",
        "Adel Alimi"
      ],
      "year": "2017",
      "venue": "Building and analysing emotion corpus of the Arabic speech. In 2017 1st International Workshop on Arabic Script Analysis and Recognition (ASAR)",
      "doi": "10.1109/ASAR.2017.8067775"
    },
    {
      "citation_id": "35",
      "title": "Emotional speech synthesis using subspace constraints in prosody",
      "authors": [
        "Shinya Mori",
        "Tsuyoshi Moriyama",
        "Shinji Ozawa"
      ],
      "year": "2006",
      "venue": "2006 IEEE International Conference on Multimedia and Expo",
      "doi": "10.1109/ICME.2006.262725"
    },
    {
      "citation_id": "36",
      "title": "ShEMO: a large-scale validated database for Persian speech emotion detection",
      "authors": [
        "Mohamad Omid",
        "Paria Jamshid Nezami",
        "Mansoureh Lou",
        "Karami"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "37",
      "title": "Recognizing emotion from Turkish speech using acoustic features",
      "authors": [
        "Caglar Oflazoglu",
        "Serdar Yildirim"
      ],
      "year": "2013",
      "venue": "EURASIP Journal on Audio, Speech, and Music Processing"
    },
    {
      "citation_id": "38",
      "title": "Analysis of Deep Learning Architectures for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Jack Parry",
        "Dimitri Palaz",
        "Georgia Clarke",
        "Pauline Lecomte",
        "Rebecca Mead",
        "Michael Berger",
        "Gregor Hofer"
      ],
      "year": "2019",
      "venue": "INTERSPEECH. International Speech Communication Association"
    },
    {
      "citation_id": "39",
      "title": "Urdu speech recognition system for district names of Pakistan: Development, challenges and solutions",
      "authors": [
        "M Qasim",
        "S Nawaz",
        "S Hussain",
        "T Habib"
      ],
      "year": "2016",
      "venue": "2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques"
    },
    {
      "citation_id": "40",
      "title": "",
      "authors": [
        "Ieee",
        "Indonesia Bali"
      ],
      "venue": "",
      "doi": "10.1109/ICSDA.2016.7918979"
    },
    {
      "citation_id": "41",
      "title": "Design and development of phonetically rich Urdu speech corpus",
      "authors": [
        "Agha Ali Raza",
        "Sarmad Hussain",
        "Huda Sarfraz",
        "Inam Ullah",
        "Zahid Sarfraz"
      ],
      "year": "2009",
      "venue": "2009 oriental COCOSDA international conference on speech database and assessments"
    },
    {
      "citation_id": "42",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)",
      "doi": "10.1109/FG.2013.6553805"
    },
    {
      "citation_id": "43",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "A James",
        "Albert Russell",
        "Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of Research in Personality",
      "doi": "10.1016/0092-6566(77)90037-X"
    },
    {
      "citation_id": "44",
      "title": "VESUS: A Crowd-Annotated Database to Study Emotion Production and Perception in Spoken English",
      "authors": [
        "Jacob Sager",
        "Ravi Shankar",
        "Jacob Reinhold",
        "Archana Venkataraman"
      ],
      "year": "2019",
      "venue": "INTERSPEECH. International Speech Communication Association"
    },
    {
      "citation_id": "45",
      "title": "Design, analysis and experimental evaluation of block based transformation in MFCC computation for speaker recognition",
      "authors": [
        "Md Sahidullah",
        "Goutam Saha"
      ],
      "year": "2012",
      "venue": "Speech communication"
    },
    {
      "citation_id": "46",
      "title": "Vocal communication of emotion: A review of research paradigms",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "47",
      "title": "The interspeech 2016 computational paralinguistics challenge: Deception, sincerity & native language",
      "authors": [
        "BjÃ¶rn Schuller",
        "Stefan Steidl",
        "Anton Batliner",
        "Julia Hirschberg",
        "Alice Judee K Burgoon",
        "Aaron Baird",
        "Yue Elkins",
        "Eduardo Zhang",
        "Keelan Coutinho",
        "Evanini"
      ],
      "year": "2001",
      "venue": "17TH ANNUAL CONFERENCE OF THE INTERNA-TIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2016), VOLS 1-5. International Speech Communication Association"
    },
    {
      "citation_id": "48",
      "title": "Human, All Too Human\": NOAA Weather Radio and the Emotional Impact of Synthetic Voices",
      "authors": [
        "Kristen Scott",
        "Simone Ashby",
        "Julian Hanna"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3313831.3376338"
    },
    {
      "citation_id": "49",
      "title": "Fusion Techniques for Utterance-Level Emotion Recognition Combining Speech and Transcripts",
      "authors": [
        "Jilt Sebastian",
        "Piero Pierucci"
      ],
      "year": "2019",
      "venue": "INTERSPEECH. International Speech Communication Association"
    },
    {
      "citation_id": "50",
      "title": "A scale for the measurement of the psychological magnitude pitch",
      "authors": [
        "Stanley Stevens",
        "John Volkmann",
        "Edwin Newman"
      ],
      "year": "1937",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "51",
      "title": "Databases, features and classifiers for speech emotion recognition: a review",
      "authors": [
        "Monorama Swain",
        "Aurobinda Routray",
        "Prithviraj Kabisatpathy"
      ],
      "year": "2018",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "52",
      "title": "Threats, Abuses, Flirting, and Blackmail: Gender Inequity in Social Media Voice Forums",
      "authors": [
        "Aditya Vashistha",
        "Abhinav Garg",
        "Richard Anderson",
        "Agha Ali"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3290605.3300302"
    },
    {
      "citation_id": "53",
      "title": "Autonomous Emotion Learning in Speech: A View of Zero-Shot Speech Emotion Recognition",
      "authors": [
        "Xinzhou Xu",
        "Jun Deng",
        "Nicholas Cummins",
        "Zixing Zhang",
        "Li Zhao",
        "BjÃ¶rn Schuller"
      ],
      "year": "2019",
      "venue": "INTERSPEECH. International Speech Communication Association"
    },
    {
      "citation_id": "54",
      "title": "BAUM-1: A spontaneous audio-visual face database of affective and mental states",
      "authors": [
        "Sara Zhalehpour",
        "Onur Onder",
        "Zahid Akhtar",
        "Cigdem Erdem"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "55",
      "title": "Design of speech corpus for mandarin text to speech",
      "authors": [
        "Jianhua Tao",
        "Fangzhou Liu",
        "Meng Zhang",
        "Huibin Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge 2008 workshop. International Speech Communication Association"
    },
    {
      "citation_id": "56",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    }
  ]
}