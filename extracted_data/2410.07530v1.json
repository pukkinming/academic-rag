{
  "paper_id": "2410.07530v1",
  "title": "Audio Explanation Synthesis With Generative Foundation Models",
  "published": "2024-10-10T01:55:58Z",
  "authors": [
    "Alican Akman",
    "Qiyang Sun",
    "Björn W. Schuller"
  ],
  "keywords": [
    "audio explainability",
    "computer audition",
    "audio transformers",
    "explainable artificial intelligence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The increasing success of audio foundation models across various tasks has led to a growing need for improved interpretability to understand their intricate decision-making processes better. Existing methods primarily focus on explaining these models by attributing importance to elements within the input space based on their influence on the final decision. In this paper, we introduce a novel audio explanation method that capitalises on the generative capacity of audio foundation models. Our method leverages the intrinsic representational power of the embedding space within these models by integrating established feature attribution techniques to identify significant features in this space. The method then generates listenable audio explanations by prioritising the most important features. Through rigorous benchmarking against standard datasets, including keyword spotting and speech emotion recognition, our model demonstrates its efficacy in producing audio explanations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Generating explanations for large artificial intelligence (AI) models has been gaining importance as they are used in various domains such as audio processing and computer vision. Most existing explainable artificial intelligence (XAI) methods try to extract important features in the input space towards the model's final decision, that can be categorised into perturbationbased  [1] -  [4]  and backpropagation-based  [5] -  [9]  techniques  [10] . These methods aim to identify relevant input features, such as pixels for computer vision tasks and tokens for natural language processing tasks. On the other hand, providing audio explanations is a useful method due to its intuitiveness on audiobased tasks and higher expressiveness over other modalities in specific scenarios, such as where understanding visual explanations needs expertise  [11] . Aiming to generate listenable and interpretable audio explanations,  [12] ,  [13]  exploit nonnegative matrix factorisation (NMF)  [14]  to decompose audio into meaningful components.\n\nFoundation models are extensively used in audio processing to achieve state-of-the-art performance on various tasks such as automatic speech recognition, keyword spotting, and speaker recognition  [15] -  [21] . In addition to that, these models offer a generalised and meaningful embedding space due to their broad range of training data; some foundation models such as EnCodec  [22]  enable generation from this space. Although certain studies target to explain transformer-based foundation models by leveraging their attention mechanism and presenting attention weights as explanations  [23] -  [25] , they do not consider computing feature importance in the meaningful embedding space to understand model behaviour. Testing with Concept Activation Vectors (TCAV)  [26]  and Network Dissection  [27]  focus on explaining a model's behaviour with provided concepts by exploiting the model internal representation. However, these methods require user-defined concepts without considering unleashing the learnt concepts which are already embedded in the latent space of a foundation model.\n\nTo address these issues, we propose a method which combines prominent feature attribution methods with foundation models to explain model behaviour in audio processing tasks. We first exploit an audio foundation model as an encoder, and train an additional model on this backbone depending on the type of the downstream task. To understand the behaviour of the final model on the task, we analyse important features for a decision in the latent space using a feature attribution method. In the final step, we use the generative part of the foundation model to construct the relevant audio in the input space. We verify that our method can generate high-fidelity explanations through experiments that simulate removing relevant features and assess the original model's performance on these essential features. The main contributions of this study are as follows:\n\n• We propose a novel audio explanation method that integrates common feature attribution methods into the latent space of foundation models. Our approach takes advantage of this meaningful space for creating understandable explanations without mapping feature relevance to the input space where individual features are difficult to interpret like audio frequencies. • Our method leverages the generative capacity of foundation models from latent space to produce meaningful audio explanations in the input space. In this way, our method generates audio explanations in a listenable format that are interpretable to the end-user.\n\n• We evaluate our model on keyword spotting and speech emotion recognition tasks. We show that while our method provides high-fidelity explanations, it captures meaningful high-level audio components for the investigated tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Adapting existing feature attribution methods to understand audio model predictions is a common practice.  [28]  explores the interpretability of deep audio models through the utilisation of layer-wise relevance propagation (LRP)  [5] ,  [29] , a technique that computes relevance scores for each neuron in a deep neural network by recursively propagating relevance scores from the output. They examine the correlation between feature relevance scores and fundamental concepts like phonemes and distinct frequency ranges in classification tasks related to spoken digits and speaker gender. In  [30] , the authors use DFT-LRP  [31] , a recently introduced variant of LRP integrating Fourier transformation, to explain audio event detection models with different architectures. They evaluate the importance of individual time-frequency components regarding the predicted classes of the models. However, there is still room for enhancement in interpreting the feature importance maps provided by these methods.\n\nDecomposing an audio input into meaningful components offers a practical approach for identifying the audio elements pertinent to XAI. CoughLIME  [12]  extends the LIME method to explain audio processing models tailored specifically for cough data. A critical aspect of CoughLIME, distinguishing it from applying standard LIME to audio spectrograms, involves decomposing the input audio into interpretable components using NMF. The authors in  [13]  introduce an interpreter network built from scratch, incorporating NMF as an audio decomposition technique. Their network is trained to develop surrogate models that replicate the output of the original classifier and generate temporal activations of pre-learnt NMF components. However, these methods primarily focus on computing relevance in the input space without delving into the intermediate representations within a deep model.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "This section elaborates on the design of our method to generate meaningful audio explanations. We begin by introducing the general structure of audio foundation models and their usage for downstream tasks. Following this, we provide a detailed description of our explainer system, which involves assigning importance in the embedding space of a foundation model. Lastly, we outline the steps for producing meaningful audio explanations using our approach. We present an overview of our system in Figure  1 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Audio Foundation Models",
      "text": "Audio foundation models are typically pre-trained on large datasets of audio samples to learn patterns from the audio signals, which can then be fine-tuned on specific tasks. They exploit self-supervised learning strategies to discover general representations from large-scale data without requiring expensive labels. To learn these representations which reflect meaningful patterns in audio signals, they build a high-level embedding space using deep learning frameworks such as autoencoders. This framework uses an encoder-decoder pair to project the audio input into the embedding space and then reconstruct it. In this paper, we focus on audio foundation models using autoencoder architecture to be able to generate listenable explanations using its decoder component. The standard autoencoder architecture can be formulated as follows:\n\nwhere X ∈ [-1, 1] D×Fs represents an audio signal input of duration D with sample rate F s , Z represents the latent vector with T denoting the number of audio frames after downsampling in the encoder, L the feature dimension of the encoder, and X represents the reconstructed audio signal.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. System Design",
      "text": "We aim to understand the important audio features for a model decision by leveraging the high-level embedding space of foundation models. For this purpose, we target explaining foundation model-based audio models trained on specific audio tasks such as audio classification. Thus, our system starts with finetuning a foundation model with an autoencoder style framework on a desired task. To maintain the learnt representation space during finetuning, we freeze the weights of the encoder part and only update the additional task-specific model part such as a classification head. Then, our framework uses feature attribution methods to determine the most relevant features in the latent space for a model decision. Without backpropagating feature attribution computation to the audio input space, our method learns the important high-level components in the latent space, which is not restricted with the dimensions of the input space. We formulate our feature attribution method in the latent space as follows: where Θ represents our explainer module that computes feature attribution, att, in the latent space. Note that the classifier module is designed to process the latent representation of an audio input extracted by the fixed encoder module.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Explanation Generation",
      "text": "To generate listenable audio explanations, our method first extracts the relevant latent vector based on the computed feature attributions in Equation  2 . While keeping the latent dimensions with high importance, it replaces less important dimensions with a base latent vector which is obtained by encoding a noise audio with appropriate length. It then uses the decoder part of the foundation model of interest to transform the relevant latent vector into audio explanations. The explanation generation can be written as:\n\nwhere Z Θ represents the relevant latent vector for a specific prediction and X Θ represents the audio explanation in the input space. Although our method generates an audio explanation in the input space, it goes beyond only selecting features in this space by the integration of meaningful latent space.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experiments",
      "text": "We evaluated our method on two datasets, namely, Speech Commands  [32] , and the Toronto Emotional Speech Set (TESS)  [33] , to assess its performance across keyword spotting and speech emotion recognition tasks. In this section, we provide implementation details of our method, followed by a comprehensive quantitative and qualitative evaluation. The implementation code and sample audio explanations is available on our project page 1  .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Implementation Details",
      "text": "We choose the EnCodec neural audio codec  [22]  as our foundation model to learn the audio representations from the raw waveform, which is trained across diverse domains including general audio, speech, and music. EnCodec comprises two key components: an encoder that extracts features based on a convolutional neural network (CNN), and a decoder module that reconstructs the same audio. While our method leverages the encoder module to extract meaningful audio representations and assign importance based on the classification model, the decoder part allows it to map these features to the input space.\n\nIn our experiments, we use the EnCodec version for 24 kHz audio at 1.5 kbps bandwidth. We also eliminate the quantisation part of the model to increase the accuracy on the classification model with higher dimensional representation.\n\nAs our classification model, we train a transformer-based classifier on top of the embeddings extracted by EnCodec. Note that we only train a base model without extra tuning. In the classifier architecture for keyword spotting on Speech Commands, we use 3 layers of transformer with an 8 head multi-head attention module. We employ a dropout probability of 0.1 and the dimension of the feed-forward network model is 512 for each transformer layer. For speech emotion recognition on TESS, we use a gated recurrent unit (GRU) based recurrent neural network with 2 layers and 128 hidden dimensions and employ a dropout of 0.2. Our classifiers achieve an accuracy of 85.4% on the test set for Speech Commands and 96.4% on the arranged test set for TESS. To arrange the TESS test set, we select random emotions from each spoken word using 0.2 split ratio and share the test indices on our project page for reproducibility. To compute the feature attribution in the latent space, we use Integrated Gradients (IG)  [6]  which calculate the integral of the gradients of the model's output along the straight line path from the baseline to the input.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Quantitative Evaluation",
      "text": "We conduct fidelity experiments to measure how well the prediction of the underlying model and the generated explanation agree. Since our method investigates feature importance beyond the space of the original input features by integrating Encodec latent space, it is not possible to select important features in this space. Thus, our strategy involves selecting the latent dimensions with the highest relevance with respect to the IG algorithm by a ratio of α. We set the remaining dimensions to a base value using a base latent vector which is obtained by encoding a noise audio with appropriate length. We compute the fidelity score as the fraction of samples where the predicted class for provided explanations in the latent space aligns with the classifier's prediction. We compare our methodology with three approaches:  (1)  We propose a baseline that randomly selects latent dimensions to obtain explanation embedding; (2) We select the most important features in the input audio space by a ratio of α using the IG method and generate the explanation embedding with the Encodec encoder -here, eliminating the quantisation part of Encodec enables us to backpropagate the IG gradient calculation through the encoder safely; (3) We also implement the random feature selection strategy in the input audio space. The results in Table  I  show that our method can generate explanations with higher fidelity compared to the two baselines (1) and (3) for both datasets. It also outperforms the standard IG method (2) which validates our approach on providing explanations leveraging a high-level embedding space.\n\nIn addition, we evaluate the fidelity of our method by analysing accuracy drop upon important feature removal to demonstrate that the generated explanations are relevant to the model's final decision. We measure the accuracy drop for different β values which represent the ratio of most important features to be removed. Then, we compare our method with the other methods by following the same steps to produce the explanation embeddings. As shown in Table  II , feature removal based on our method leads to highest accuracy drop outperforming the other methods.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Qualitative Evaluation",
      "text": "To evaluate the quality of generated explanations, we observe our model's behaviour on audios from the same spoken words and separate emotion classes on the TESS dataset. By following similar procedures with quantitative evaluation experiments, we select an audio sample and generate the audio explanation using our method by setting α = 0.2 in the first experiment. In the second experiment setting, we removed the explanation from the original audio sample in the embedding space with the same ratio. We then generate the irrelevant audio part by using our framework. Ideally, we expect that while the generated audio in the first experiment can represent the emotion in the original audio sample, the irrelevant audio in the second experiment represents a neutral state of the same word spoken. In Figure  2 , we present an example of audio from the class \"Happy\" to conduct these experiments. We also present the audio of the same word from the class \"Neutral\" to enable comparison for explanation-removed audio. We observe that while the generated explanation can represent the emotion in the original audio, explanation-removed audio looks more similar to the neutral version of the spoken word. We use spectrogram representation to increase the interpretability of the visuals.\n\nWe also investigate the classifier model behaviour for the TESS dataset after explanation removal by a ratio of β = 0.1 using our method. In Figure  3 , we present the confusion matrix of the classifier for each class in TESS dataset. The results show that majority of the audios are classified as \"Neutral\" when explanation is removed by a small ratio which shows our explanation generation method focuses on emotions.\n\nV. CONCLUSION In this paper, we presented a novel audio explanation method which targets audio-processing foundation models. Unlike existing feature attribution methods which assign importance in the input space, our method integrates the latent space of a generative foundation model to generate meaningful and listenable explanations. The experiments demonstrated that our method delivers high-fidelity explanations, effectively capturing meaningful audio components pertinent to the specific task. Our work enlightens the way to promising research in interpreting state-of-the-art audio models as well as encompassing their application for model debugging and justification. An extension of our work could involve using rapidly growing audio generative AI models to produce higher quality audio explanations.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of our method: The top row depicts the role of a foundation",
      "page": 2
    },
    {
      "caption": "Figure 1: A. Audio Foundation Models",
      "page": 2
    },
    {
      "caption": "Figure 2: Sample spectrogram visualisations for the qualitative audio experiments:",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrix for the classifier for TESS dataset after explanation",
      "page": 4
    },
    {
      "caption": "Figure 2: , we present an example of audio from the class",
      "page": 4
    },
    {
      "caption": "Figure 3: , we present the confusion matrix",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "why should I trust you?\": Explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "2",
      "title": "RISE: randomized input sampling for explanation of black-box models",
      "authors": [
        "V Petsiuk",
        "A Das",
        "K Saenko"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "3",
      "title": "Visualizing and understanding convolutional networks",
      "authors": [
        "M Zeiler",
        "R Fergus"
      ],
      "year": "2013",
      "venue": "CoRR"
    },
    {
      "citation_id": "4",
      "title": "Understanding deep networks via extremal perturbations and smooth masks",
      "authors": [
        "R Fong",
        "M Patrick",
        "A Vedaldi"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "5",
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "authors": [
        "S Bach",
        "A Binder",
        "G Montavon",
        "F Klauschen",
        "K.-R Müller",
        "W Samek"
      ],
      "year": "2015",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0130140"
    },
    {
      "citation_id": "6",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "M Sundararajan",
        "A Taly",
        "Q Yan"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "7",
      "title": "Learning important features through propagating activation differences",
      "authors": [
        "A Shrikumar",
        "P Greenside",
        "A Kundaje"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "8",
      "title": "Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "A Das",
        "R Vedantam",
        "M Cogswell",
        "D Parikh",
        "D Batra"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "9",
      "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
      "authors": [
        "G Montavon",
        "S Bach",
        "A Binder",
        "W Samek",
        "K Müller"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "10",
      "title": "A unified view of gradient-based attribution methods for deep neural networks",
      "authors": [
        "M Ancona",
        "E Ceolini",
        "A Öztireli",
        "M Gross"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "11",
      "title": "Towards sonification in multimodal and userfriendlyexplainable artificial intelligence",
      "authors": [
        "B Schuller",
        "T Virtanen",
        "M Riveiro",
        "G Rizos",
        "J Han",
        "A Mesaros",
        "K Drossos"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 International Conference on Multimodal Interaction, ser. ICMI '21",
      "doi": "10.1145/3462244.3479879"
    },
    {
      "citation_id": "12",
      "title": "Coughlime: Sonified explanations for the predictions of covid-19 cough classifiers",
      "authors": [
        "A Wullenweber",
        "A Akman",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "13",
      "title": "Listen to interpret: Post-hoc interpretability for audio networks with nmf",
      "authors": [
        "J Parekh",
        "S Parekh",
        "P Mozharovskyi",
        "F Buc",
        "G Richard"
      ],
      "year": "2022",
      "venue": "Listen to interpret: Post-hoc interpretability for audio networks with nmf"
    },
    {
      "citation_id": "14",
      "title": "Algorithms for non-negative matrix factorization",
      "authors": [
        "D Lee",
        "H Seung"
      ],
      "year": "2000",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "AST: audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "Y Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "17",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "F Wei"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "18",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "19",
      "title": "Audio albert: A lite bert for self-supervised learning of audio representation",
      "authors": [
        "P.-H Chi",
        "P.-H Chung",
        "T.-H Wu",
        "C.-C Hsieh",
        "Y.-H Chen",
        "S.-W Li",
        "H Yi Lee"
      ],
      "year": "2021",
      "venue": "Audio albert: A lite bert for self-supervised learning of audio representation"
    },
    {
      "citation_id": "20",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "M Ravanelli",
        "J Zhong",
        "S Pascual",
        "P Swietojanski",
        "J Monteiro",
        "J Trmal",
        "Y Bengio"
      ],
      "year": "2020",
      "venue": "Multi-task self-supervised learning for robust speech recognition"
    },
    {
      "citation_id": "21",
      "title": "Data augmenting contrastive learning of speech representations in the time domain",
      "authors": [
        "E Kharitonov",
        "M Rivière",
        "G Synnaeve",
        "L Wolf",
        "P.-E Mazaré",
        "M Douze",
        "E Dupoux"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "22",
      "title": "High fidelity neural audio compression",
      "authors": [
        "A Défossez",
        "J Copet",
        "G Synnaeve",
        "Y Adi"
      ],
      "year": "2022",
      "venue": "High fidelity neural audio compression",
      "arxiv": "arXiv:2210.13438"
    },
    {
      "citation_id": "23",
      "title": "A multiscale visualization of attention in the transformer model",
      "authors": [
        "J Vig"
      ],
      "year": "1906",
      "venue": "CoRR"
    },
    {
      "citation_id": "24",
      "title": "Attentionviz: A global view of transformer attention",
      "authors": [
        "C Yeh",
        "Y Chen",
        "A Wu",
        "C Chen",
        "F Viégas",
        "M Wattenberg"
      ],
      "year": "2023",
      "venue": "Attentionviz: A global view of transformer attention"
    },
    {
      "citation_id": "25",
      "title": "Quantifying attention flow in transformers",
      "authors": [
        "S Abnar",
        "W Zuidema"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "authors": [
        "B Kim",
        "M Wattenberg",
        "J Gilmer",
        "C Cai",
        "J Wexler",
        "F Viegas",
        "R Sayres"
      ],
      "year": "2018",
      "venue": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)"
    },
    {
      "citation_id": "27",
      "title": "Network dissection: Quantifying interpretability of deep visual representations",
      "authors": [
        "D Bau",
        "B Zhou",
        "A Khosla",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2017",
      "venue": "Network dissection: Quantifying interpretability of deep visual representations"
    },
    {
      "citation_id": "28",
      "title": "Interpreting and explaining deep neural networks for classification of audio signals",
      "authors": [
        "S Becker",
        "M Ackermann",
        "S Lapuschkin",
        "K.-R Müller",
        "W Samek"
      ],
      "year": "2019",
      "venue": "Interpreting and explaining deep neural networks for classification of audio signals"
    },
    {
      "citation_id": "29",
      "title": "Layer-wise relevance propagation for neural networks with local renormalization layers",
      "authors": [
        "A Binder",
        "G Montavon",
        "S Bach",
        "K Müller",
        "W Samek"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "30",
      "title": "Xai-based comparison of input representations for audio event classification",
      "authors": [
        "A Frommholz",
        "F Seipel",
        "S Lapuschkin",
        "W Samek",
        "J Vielhaben"
      ],
      "year": "2023",
      "venue": "Xai-based comparison of input representations for audio event classification"
    },
    {
      "citation_id": "31",
      "title": "Explainable ai for time series via virtual inspection layers",
      "authors": [
        "J Vielhaben",
        "S Lapuschkin",
        "G Montavon",
        "W Samek"
      ],
      "year": "2023",
      "venue": "Explainable ai for time series via virtual inspection layers"
    },
    {
      "citation_id": "32",
      "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition",
      "authors": [
        "P Warden"
      ],
      "year": "2018",
      "venue": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition"
    },
    {
      "citation_id": "33",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "M Pichora-Fuller",
        "K Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)",
      "doi": "10.5683/SP2/E8H2MF"
    }
  ]
}