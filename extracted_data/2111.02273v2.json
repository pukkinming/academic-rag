{
  "paper_id": "2111.02273v2",
  "title": "Computer Vision And Image Understanding",
  "published": "2021-11-03T15:08:55Z",
  "authors": [
    "Willams Costa",
    "David Macêdo",
    "Cleber Zanchettin",
    "Lucas S. Figueiredo",
    "Veronica Teichrieb"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Expressing and identifying emotions through facial and physical expressions is a significant part of social interaction. The computer task for identifying emotions and allowing a more natural interaction between humans and machines is emotion recognition. The common approaches for emotion recognition focus on analyzing facial expressions and requires the automatic localization of the face in the evaluated scene. Although these approaches can classify emotion in controlled scenarios, such techniques are limited when dealing with unconstrained daily interactions. We propose a new direction for emotion recognition based on adaptive multi-cues using face and information from context and body poses. We evaluate the proposed approach in the CAER-S dataset, considering different components in a pipeline that is 21.49% better than the current state-of-the-art method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Communication plays a crucial role in our social experiences. By conveying our thoughts and feelings, we can create social links. The human communication comprises verbal communication, in which we express ourselves using speech, and nonverbal communication, which refers to how our body expresses our feelings using facial expressions, gaze, gestures, and body language. Surprisingly, the nonverbal communication constitutes up to half of what we are communicating  (Patel, 2014) . Moreover, humans can interpret it even in an unconscious way.\n\nRecent approaches for improving human-computer interaction (HCI) are shifting the focus from computer-centered domain to user-centered applications  (Rouast et al., 2018) . In this sense, understanding emotional responses from users can make artificial systems change the context of what they are communicating and trigger different actions  (Valli, 2008) , enabling different HCI applications such as education  (Yadegaridehkordi et al., 2019) , entertainment  (Setiono et al., 2021) , virtual reality  (Marín-Morales et al., 2018) , mental health monitoring  (Zucco et al., 2017)  and general healthcare  (Yannakakis, 2018) .\n\nCurrent research on emotion recognition is focused on facial expressions, motivated by the many discriminative features present on the human face. However, research on behavioral psychology indicates that one modality of nonverbal communication is not necessarily superior to the other on perceiving feelings and intents  (Kleinsmith and Bianchi-Berthouze, 2013) .\n\nStill, estimating the correct user's feelings using techniques based only on facial expressions is a challenging task. For example, such methods do not perform well on unconstrained, daily situations (e.g., social interactions), mainly due to the need for a specific alignment of the user's face with the camera capturing the action  (Lucey et al., 2010) . Some methods boost the recognition performance by leveraging context information, such as gesture and context background, to allow more unconstrained scenarios  (Lee et al., 2019; Kosti et al., 2017 Kosti et al., , 2019;; Randhavane et al., 2019a,b) . However, investigating other nonverbal cues, such as body language and the combination of different approaches, is yet an open topic in HCI research.\n\nRecent works extract features from gait estimation, such as arm swinging, long strides, foot landing force, and posture to identify sentiment on videos. However, they also do not work well in unconstrained situations, obliging the user to face directly the camera  (Randhavane et al., 2019a,b; Bhattacharya et al., 2020a; Crenn et al., 2016) . We propose a face encoding stream, which can extract facial features and contribute as one of many input cues. Another relevant information is context since it can directly affect how one feels towards the environment.\n\nAs we show in (c), we also propose a context encoding stream powered by self-calibrated convolutions that can extract meaningful information from context, which is another input for our method. Finally, as we show in (d), another important form of non-verbal communication is body language and body pose, and we also consider it by proposing a body encoding stream that extracts information regarding body pose.\n\nIn this work, we investigate the use of multiple cues that correspond to nonverbal communication, namely facial expressions, body language, and context, to recognize emotion in unconstrained scenarios. Fig.  1  presents an overview of the proposed solution, named Multi-Cue Adaptive Emotion Recognition Network (MCAER-Net).\n\nWe considered the work of  Lee et al. (2019)  as a baseline and evolved a new pipeline for emotion recognition. Our approach comprises image preprocessing and face selection strategies, coupled with the usage of self-calibrated convolutions and detection of body keypoints. We display an overview of our architecture in Fig.  2 . Based on the excellent results, our main contributions are:\n\n• A body encoding stream that extracts features from body language for emotion recognition, lowering ambiguity and opening a path for in-the-wild emotion recognition\n\n• A context encoding stream powered by an adaptation of the self-calibrated convolutions  (Liu et al., 2020) . The encoder-like method allows the extraction of more representative features from context information\n\n• An approach for selecting individual faces in crowded scenes, lowering the impact of the lack of per-person annotations in the evaluated dataset   (Lhommet and Marsella, 2014; Darwin, 1872) . Darwin exemplifies how emotions can act directly on our bodily behavior. Joy, for instance, has a strong tendency to purposeless movements since this feeling causes our blood circulation to accelerate, which stimulates the brain, leading to reactions on the body. On the other hand, pride makes a person exhibit its sense of superiority by holding its head and body erect, making them appear as large as possible, as if, metaphorically, they were puffed up with pride.\n\nThe psychology literature tackled the problem of posed emotions against expressed emotions. A few pieces of evidence suggest that these posed expressions are, at least, an approximation to what is felt. However, we may diminish the impact of these problems by using different actors in the experiments that are not aware that body movements are a part of the task, leading them to focus attention on the sentences of the scene  (Wallbott and Scherer, 1986; Zuckerman et al., 1976; Wallbott, 1998) .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition",
      "text": "Most research works focus on recognizing emotion through facial expressions, and they are only able to extract features from facial crops of the target person, showing the limited ability for emotion recognition in the wild  (Su et al., 2021; Wang et al., 2020a,b; Li et al., 2018) . Considering that, to explore these limitations, some works use different visual cues from different input sources  (Chen et al., 2016; Kosti et al., 2017) . However, there is a lack of solutions focusing on salient parts of the scene that would help the models generalize context information  (Lee et al., 2019) .  Lee et al. (2019)  proposed an approach to deal with this limitation using an attention module that adapts itself and allows a context encoding stream to search for meaningful and salient information on the scene to help recognize and classify emotions. However, although the context encoding stream enables selecting other parts of the scene and even parts of the body, the approach is not trained with specific knowledge on body pose estimation.\n\nBased on the concept that body expression can help the perception of emotions,  Randhavane et al. (2019a)  proposes the analysis of gait to classify emotions. Given an RGB video of an individual walking, the authors use 3D pose estimation techniques to create a set of 3D poses, which are spatiotemporally investigated. Although this technique uses information from body language, it requires the user to be walking to extract features such as the swing of the arms and posture, therefore imposing constraints to the user and limiting the range of applicability. A more recent approach  (Bhattacharya et al., 2020b)  improves the previously mentioned technique by 14% on the accuracy metric but is also limited to the users walking.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Mcaer-Net",
      "text": "Given an image I, we aim to infer an emotion y among a set of K emotion labels by using a convolutional neural network Fig.  2 . The architecture of the proposed multi-cue emotion recognition approach. Given an input image on an unconstrained scenario, we use an off-theshelf face detector algorithm  King (2009)  to get the localization of the face on the image. First, we crop the face and use it as input for the face encoding stream, responsible for extracting features from the face. Next, we fill the cropped region with a black rectangle and use this new image as input for the context encoding stream. Since the facial crop is occluded, this stream is \"forced\" to search for features from other image regions during training (i.e., the background context). Finally, we apply a segmentation technique  He et al. (2017)  to remove background noise and persons that are not acting directly on the scene. We use this segmentation mask as input for an off-the-shelf human keypoint extractor  Xiao et al. (2018) . The features extracted from these three streams are fused in an adaptive way allowing the emotion classification.\n\nmodel. The proposed network architecture extracts features of three streams: face encoding stream, context encoding stream and body encoding stream. By combining these features in an adaptive fusion network, the proposed method can infer emotion from multiple non-verbal cues. In Fig.  2 , we present the proposed architecture, and each module is detailed below.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing Pipeline",
      "text": ". Before our training procedure, we perform a preprocessing step to allow some variability between epochs and enable training while keeping important features available. For the input of the face encoding stream, we resize the facial crops to a fixed size of 96×96, following the baseline. For the context encoding stream, we pad each image according to the shape of the larger image on the dataset, which is 400 × 712. We also resize the images by a factor of three to maintain the aspect ratio and use a random crop with a padding of 5 pixels on all sides to augment our training dataset. For the images used in the body encoding stream, we follow the pipeline proposed by  Xiao et al. (2018)  and resize the image to 256 × 256.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Cue Streams",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Face Encoding Stream",
      "text": "We use an off-the-shelf face detector  (King, 2009)  to detect the faces present on the image. In the CAER-S dataset  (Lee et al., 2019) , the images are extracted from TV shows and contain scenes of interaction between actors; however, there is only a single annotation of emotion on the scene with no remarks of which actor is displaying such emotion. Therefore, if more than one face is present, we use a face selector algorithm that selects the principal actor's face based on the estimated distance from the camera (since they are usually in the foreground of the scene) and their position on the image (since they are usually centered on the scene), considering that the given annotation is related to the leading actor. We crop the bounding-box region and use it as input to the face encoding stream, as shown in Fig.  2 . This module consists of five convolutional layers with 3x3 kernels with sizes 32, 64, 128, 256, and 256, followed by batch normalization (BN) and rectified linear unit (ReLU) activation, and four max-pooling layers with a kernel size of 2. We spatially average the final feature layer using an averagepooling layer. In Fig.  3 , we show a few examples of the face selector algorithm compared against the original approach.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Context Encoding Stream",
      "text": "Extracting emotional information from context is a problematic task due to the high variability of context information. Moreover, many essential details may be hidden in the scene, motivating a robust encoding stream for context. Therefore, proposing approaches to enrich the representations extracted on the context encoding stream could improve the results by allowing more representative features to classify emotion. This module consists of four convolutional layers with 3 × 3 kernels with sizes 32, 64, 128, and 256, followed by BN layers, ReLU activations, and four max-pooling layers with a kernel size of 2 × 2. Finally, we add an adaptive self-calibrated convolution with kernel size 3 × 3 and a ReLU activation layer.\n\nAdaptive self-calibrated convolutions. We propose the usage of self-calibrated convolutions  Liu et al. (2020)  to allow output features enriched. Those modules provide internal communications of the convolutional layer. It can generate more discriminative representations and improve the overall quality of the extracted features. We replace the last convolutional layer of the convolutional block with a self-calibrated convolution. The adaptive self-calibrated convolution module receives an input with channels size C and outputs a features map with channels size C ; a restriction when using the original self-calibrated convolutions that we overcame. By altering the last convolution on this block, we allowed encoder networks that vary the output channel size on each convolution based on self-calibrated convolutions.\n\nAttention inference module. An attention inference module is learned in a non-supervised way, enabling the context encoding stream to focus on the salient contexts. We use the output features of the context encoding stream as input, lowering the   (King, 2009) , as proposed by  Lee et al. (2019) , and in green, the chosen face using our face selector algorithm. In (a) the only face in the scene is selected. In (b), the face selector algorithm correctly chooses the face of the principal actor in the TV show scene. The selected face is classified as anger. In (c), a sample where the algorithm fails to choose the face of the actor. In (d) we show an example of a crowded situation, the face selector algorithm success on selected the principal actress in the scene.\n\nnumber of channels from 236 to 1 and applying a softmax operation to make the sum of attention for each pixel of the input features to be 1. Afterward, the attention is applied to the output feature to make an attention-boosted feature. The intuition beyond the usage of this module is that context may have too much background information, and the attention module helps the context encoding stream to select which features are more important, thus boosting its participation in the overall process.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Body Encoding Stream",
      "text": "Following our proposal to investigate the human body pose as a nonverbal communication input, we used an approach proposed by  Xiao et al. (2018)  known as SimpleHRNet. The technique is based on the ResNet  (He et al., 2016a)  and proposes to add a few deconvolutional layers on the end of the backbone network to estimate heatmaps from feature maps.\n\nWe used the features learned up to the last convolutional layer of the network. However, as described previously, we have more than one interlocutor interacting on a scene in some datasets, and this is a real-world prerequisite. Therefore, we propose to use segmentation techniques to separate the person of interest in the scene and extract only its body pose. Fig.  4  presents an example of how to apply segmentation masks for cluttered backgrounds. We used Mask R-CNN  (He et al., 2017)  to create masks of the people present in the scene and calculate the overlap between the selected face from the face selector algorithm and each of the masks.\n\nThis approach prevents that two different persons from being considered on two modules. This way, a refined version of the dataset was preprocessed and saved to eliminate the need to predict each epoch's masks. The body encoding stream receives the full, uncropped image, with the removed background, as we show in Fig.  2  and Fig.  4 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Adaptive Fusion Networks",
      "text": ". To combine face, context, and body pose, we fuse the features extracted from the three modules using an adaptive fusion network with an attention model since the direct concatenation of such different features often fails to provide adequate performance  (Lee et al., 2019) . According to the contents, the attention weights are determined to self-adapt to yield an optimal solution.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "We implemented MCAER-Net using the PyTorch library  (Paszke et al., 2017) . Next, we trained the face encoding module and context encoding module from scratch with an initial learning rate initialized as 4 × 10 -3 and dropped by a factor of 0.4 every 40 epochs using the RMSProp optimizer. Finally, we trained the model using the cross-entropy loss function on a batch size of 32.\n\nWe use the CAER-S dataset  (Lee et al., 2019)  for experimentation. The dataset is focused on context-aware emotion recognition and is sufficient for the task of multi-cue emotion recognition. The dataset is based on video clips from 79 television shows, and each frame is categorized as one out of seven emotional states, namely: \"angry\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\" and \"neutral\". Fig.  5  present few examples of the dataset. Each collected clip was annotated manually by three different annotators blindly and independently. The authors used about 70 thousand static images from this dataset to create a subset for the specific task of recognizing emotion on static images. Following the literature, we call this subset CAER-S, which is randomly split into training (70%), validation (10%), and testing (20%) sets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "The quantitative results of each proposed experiment are displayed in Table  1  and discussed below. All experiments are evaluated using the accuracy metric following literature standards  (Lee et al., 2019; Dhall et al., 2016) .\n\nOur reproduction of the baseline proposed by  Lee et al. (2019)  yielded an accuracy of 81.5% in the CAER-S test set. This implementation is our base of comparison against our proposed improvements. Our first proposed experiment is based on the usage of self-calibrated convolutions  (Liu et al., 2020)  for better extraction of features in the context encoding stream. However, swapping the last convolutional layer of both face encoding stream and context encoding stream prejudiced the overall accuracy. This is due to the size of the face crops, which in a few examples could be relatively small because of the placement of the leading actor on the scene. Thus we proceeded to exchange only the last convolutional layer of the context encoding stream, and, as we show in Table  1 , this experiment led us to a higher accuracy score. We also evaluated exchanging all convolutional layers with the proposed adaptive self-calibrated convolutions. However, we verified a negative impact on the overall accuracy.\n\nAnother experiment involved the usage of the face selector algorithm. It is not uncommon to have different interlocutors portraying different emotions in a scene with multiple subjects. Therefore, we investigate how a wrongly selected face could impact the overall accuracy. We noticed a slight increase in the accuracy score using this approach. However, it does not significantly contribute to the final accuracy, even selecting the correct face in crowded scenarios. This experiment may point out the robustness of the context encoding stream that can leverage emotion from context even with an incorrect face selected.\n\nFollowing the proposed improvements, we evaluated the contribution of the new preprocessing pipeline, introduced in Section 3.1, to the overall result, which again leads to an increase in inaccuracy. Finally, we experiment on the usage of body language as an input to the model. The features learned up to the last convolutional layer are combined on the adaptive fusion module, which learns weights for three inputs. A first experiment focused solely on the usage of the body pose estimation technique as an extra input cue to the model, and, as shown in Table . 1, yielded a low accuracy score compared to the previously implemented improvements. However, during further investigation, we noticed that more than one actor is present on the scene in many cases, and the body posture of these actors was also considered when leveraging the body pose. Therefore, as previously explained in Section. 3.2.3, we use Mask R-CNN  (He et al., 2017)  to segment the principal actor's body and isolate it from context leading to our best result of 89.3%. Although the accuracy increment may be considered small compared with the approach using only context and face, the pose encoding stream may be decisive in complex cases where the context does not contain useful information. However, further investigation is needed, specially hyperparameter-wise, to understand if any other constraints contribute negatively to this result.\n\nWe compare our results with different baseline approaches in Table  2 . The proposed method was 21.49% better when compared to the highest-scoring approach -CAER-Net-S  Lee et al. (2019) . We also performing consistently against traditional deep neural networks approaches for image tasks, such Fig.  6 . Visualization of the attention module from the context encoding stream. Since this module is responsible for extracting context information, the main person's facial region on the scene is occluded with a black rectangle. On the top row, we see the image used as input for the module, and on the bottom row, the output from Grad-CAM  (Selvaraju et al., 2017)  with respect to the last convolutional layer of the attention inference module. The red and yellow regions are the most relevant regions that lead to the given prediction. (a) An example of the testing set correctly classified as \"neutral.\" In this example, not much information is extracted from the background, mostly from the main actor's body. (b) An example from the \"surprise\" subset. In this example, a major part of the main actor's face is considered relevant. (c) In this example from the \"sad\" subset, the visualization points to the main actor's hands, as if a personal object was torn apart. Finally, in (d) we have an example from the \"disgust\" subset, pointing to the way that the actor is lying on the couch.\n\nas AlexNet  (Krizhevsky et al., 2012) , VGG-Net  (Simonyan and Zisserman, 2014) , and ResNet  (He et al., 2016c) .\n\nAfter quantitative evaluation, we proceeded with a visual investigation based on the Grad-CAM technique  (Selvaraju et al., 2017) . The Grad-CAM allows visualizations of which regions of the input image are more relevant for predictions by using class-specific gradient information to localize these crucial regions. In Fig.  6 , we show a few examples of how the context encoding stream acts towards the correct prediction of the emotion. Next, we investigated the specific cases in which the context is not rich in information, such as images containing only white walls in the background. For these scenarios, the technique relies only on the facial features for emotion recognition. It could benefit from more features from the person's posture and body language, as shown in Fig.  7 . This investigation supports our assumption that relying only on context and face information is not sufficient in some cases, especially on in-thewild scenarios, in which occlusion of the face is expected, and context may not be representative at all.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we presented an approach for emotion recognition using multiple cues that represents important non-verbal communication as input. Our proposed model learns to weigh each input adaptively, attributing higher weights to inputs with more descriptive features in each scenario.\n\nOur proposal reaches an accuracy score of 89.30% on the CAER-S dataset, increasing 21.49% regarding the current stateof-the-art in the same dataset. Our contributions were supported by research on behavioral psychology and the improvement regarding extraction of features from context and body, among other improvements, open paths towards in-the-wild applications. Visualization of an example from the \"fear\" subset. We see a weak activation map on the image, meaning that the context encoding stream could not extract any information in this case. The proposed technique correctly classified this example, mainly due to the close-up on the face. However, this example would benefit from a body encoding stream.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our solution.",
      "page": 2
    },
    {
      "caption": "Figure 1: presents an overview of the pro-",
      "page": 2
    },
    {
      "caption": "Figure 2: Based on the excellent results, our main",
      "page": 2
    },
    {
      "caption": "Figure 2: The architecture of the proposed multi-cue emotion recognition approach. Given an input image on an unconstrained scenario, we use an oﬀ-the-",
      "page": 3
    },
    {
      "caption": "Figure 2: , we present the",
      "page": 3
    },
    {
      "caption": "Figure 2: This module consists of ﬁve convolutional layers with",
      "page": 3
    },
    {
      "caption": "Figure 3: , we show a few examples of the face",
      "page": 3
    },
    {
      "caption": "Figure 3: Comparison of diﬀerent approaches for face crop. In red, the se-",
      "page": 4
    },
    {
      "caption": "Figure 4: presents an example of how to apply segmentation masks for",
      "page": 4
    },
    {
      "caption": "Figure 2: and Fig. 4.",
      "page": 4
    },
    {
      "caption": "Figure 4: The proposed approach to deal with the cluttered background prob-",
      "page": 4
    },
    {
      "caption": "Figure 5: Examples from the CAER-S dataset. From left to right, we present",
      "page": 4
    },
    {
      "caption": "Figure 5: present few examples",
      "page": 5
    },
    {
      "caption": "Figure 6: Visualization of the attention module from the context encoding stream. Since this module is responsible for extracting context information, the",
      "page": 6
    },
    {
      "caption": "Figure 6: , we show a few examples of how the con-",
      "page": 6
    },
    {
      "caption": "Figure 7: This investigation",
      "page": 6
    },
    {
      "caption": "Figure 7: Visualization of an example from the ”fear” subset. We see a weak",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: and discussed below. All experiments are late it from context leading to our best result of 89.3%. Al-",
      "data": [
        {
          "Self-calibratedconvolutiononthe Faceselector Preprocessing Bodyencoding Bodyencodingstream\nlastlayerofcontextencodingstream algorithm pipeline stream withsegmentedbody": "(cid:88) x x x x\n(cid:88) (cid:88) (cid:88) x x\n(cid:88) (cid:88) (cid:88) (cid:88) x\n(cid:88) (cid:88) (cid:88) x (cid:88)"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Emotion in context: Deep semantic feature fusion for video emotion recognition",
      "authors": [
        "C Chen",
        "Z Wu",
        "Y Jiang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM international conference on Multimedia"
    },
    {
      "citation_id": "4",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "A Crenn",
        "R Khan",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2016",
      "venue": "2016 International Conference on 3D Imaging (IC3D)"
    },
    {
      "citation_id": "5",
      "title": "the expression of the emotions in man and animals",
      "authors": [
        "C Darwin"
      ],
      "year": "1872",
      "venue": "the expression of the emotions in man and animals"
    },
    {
      "citation_id": "6",
      "title": "Emotiw 2016: Video and group-level emotion recognition challenges",
      "authors": [
        "A Dhall",
        "R Goecke",
        "J Joshi",
        "J Hoey",
        "T Gedeon"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "7",
      "title": "Mask r-cnn",
      "authors": [
        "K He",
        "G Gkioxari",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "8",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Identity mappings in deep residual networks",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "12",
      "title": "Affective body expression perception and recognition: A survey",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2012.16"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "15",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "16",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "17",
      "title": "Expressing emotion through posture. The Oxford handbook of affective computing 273",
      "authors": [
        "M Lhommet",
        "S Marsella"
      ],
      "year": "2014",
      "venue": "Expressing emotion through posture. The Oxford handbook of affective computing 273"
    },
    {
      "citation_id": "18",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "19",
      "title": "Improving convolutional networks with self-calibrated convolutions",
      "authors": [
        "J Liu",
        "Q Hou",
        "M Cheng",
        "C Wang",
        "J Feng"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition -Workshops",
      "doi": "10.1109/CVPRW.2010.5543262"
    },
    {
      "citation_id": "21",
      "title": "Affective computing in virtual reality: emotion recognition from brain and heartbeat dynamics using wearable sensors",
      "authors": [
        "J Marín-Morales",
        "J Higuera-Trujillo",
        "A Greco",
        "J Guixeres",
        "C Llinares",
        "E Scilingo",
        "M Alcañiz",
        "G Valenza"
      ],
      "year": "2018",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "22",
      "title": "",
      "authors": [
        "A Paszke",
        "S Gross",
        "S Chintala",
        "G Chanan",
        "E Yang",
        "Z Devito",
        "Z Lin",
        "A Desmaison",
        "L Antiga",
        "A Lerer"
      ],
      "year": "2017",
      "venue": ""
    },
    {
      "citation_id": "23",
      "title": "Body language: An effective communication tool",
      "authors": [
        "D Patel"
      ],
      "year": "2014",
      "venue": "IUP Journal of English Studies"
    },
    {
      "citation_id": "24",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "25",
      "title": "The liar's walk: Detecting deception with gait and gesture",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "The liar's walk: Detecting deception with gait and gesture",
      "arxiv": "arXiv:1912.06874"
    },
    {
      "citation_id": "26",
      "title": "Deep learning for human affect recognition: Insights and new developments",
      "authors": [
        "P Rouast",
        "M Adam",
        "R Chiong"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2890471"
    },
    {
      "citation_id": "27",
      "title": "Grad-cam: Visual explanations from deep networks via gradientbased localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "Grad-cam: Visual explanations from deep networks via gradientbased localization"
    },
    {
      "citation_id": "28",
      "title": "Enhancing player experience in game with affective computing",
      "authors": [
        "D Setiono",
        "D Saputra",
        "K Putra",
        "J Moniaga",
        "A Chowanda"
      ],
      "year": "2020",
      "venue": "th International Conference on Computer Science and Computational Intelligence",
      "doi": "10.1016/j.procs.2021.01.066"
    },
    {
      "citation_id": "29",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "30",
      "title": "Facial expression recognition with confidence guided refined horizontal pyramid network",
      "authors": [
        "W Su",
        "H Zhang",
        "Y Su",
        "J Yu"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "31",
      "title": "Alessandro Valli -Notes on Natural Interaction",
      "authors": [
        "A Valli",
        "H Wallbott"
      ],
      "year": "1998",
      "venue": "European Journal of Social Psychology",
      "doi": "10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W"
    },
    {
      "citation_id": "32",
      "title": "Cues and channels in emotion recognition",
      "authors": [
        "H Wallbott",
        "K Scherer"
      ],
      "year": "1986",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "33",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "D Meng",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "35",
      "title": "Simple baselines for human pose estimation and tracking",
      "authors": [
        "B Xiao",
        "H Wu",
        "Y Wei"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "36",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Ayub",
        "H Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Computers & Education",
      "doi": "10.1016/j.compedu.2019.103649"
    },
    {
      "citation_id": "37",
      "title": "Sentiment analysis and affective computing for depression monitoring",
      "authors": [
        "G Yannakakis",
        "C Zucco",
        "B Calabrese",
        "M Cannataro"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)",
      "doi": "10.1109/BIBM.2017.8217966"
    },
    {
      "citation_id": "38",
      "title": "Encoding and decoding of spontaneous and posed facial expressions",
      "authors": [
        "M Zuckerman",
        "J Hall",
        "R Defrank",
        "R Rosenthal"
      ],
      "year": "1976",
      "venue": "Journal of Personality and Social Psychology"
    }
  ]
}