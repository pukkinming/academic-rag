{
  "paper_id": "2211.02923v1",
  "title": "Cross-Subject Emotion Recognition With Sparsely-Labeled Peripheral Physiological Data Using Shap-Explained Tree Ensembles",
  "published": "2022-11-05T14:57:07Z",
  "authors": [
    "Feng Zhou",
    "Tao Chen",
    "Baiying Lei"
  ],
  "keywords": [
    "Emotion Recognition",
    "Peripheral Physiological Data",
    "Tree Ensembles",
    "Explainability",
    "SHAP"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "There are still many challenges of emotion recognition using physiological data despite the substantial progress made recently. In this paper, we attempted to address two major challenges. First, in order to deal with the sparsely-labeled physiological data, we first decomposed the raw physiological data using signal spectrum analysis, based on which we extracted both complexity and energy features. Such a procedure helped reduce noise and improve feature extraction effectiveness. Second, in order to improve the explainability of the machine learning models in emotion recognition with physiological data, we proposed Light Gradient Boosting Machine (LightGBM) and SHapley Additive exPlanations (SHAP) for emotion prediction and model explanation, respectively. The LightGBM model outperformed the eXtreme Gradient Boosting (XGBoost) model on the public Database for Emotion Analysis using Physiological signals (DEAP) with f1-scores of 0.814, 0.823, and 0.860 for binary classification of valence, arousal, and liking, respectively, with cross-subject validation using eight peripheral physiological signals. Furthermore, the SHAP model was able to identify the most important features in emotion recognition, and revealed the relationships between the predictor variables and the response variables in terms of their main effects and interaction effects. Therefore, the results of the proposed model not only had good performance using peripheral physiological data, but also gave more insights into the underlying mechanisms in recognizing emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A FFECT is one essential element in the human-computer interaction process  [1]  and can be included as a design parameter to optimize task performance and user experience in various areas  [2] . For example, the Yerkes-Dodson law identified an inverted-U shape relationship between task performance and arousal  [3] , participants' positive valence was found to improve their takeover performance in automated driving  [4] , and recognizing and understanding emotions were able to help improve human-robot interaction  [5] .\n\nIn order to support affective computing, one basic question is how to recognize emotions during the interaction process  [6] . Researchers explored different types of data to recognize emotions, including self-reported data, behavioral measures (e.g., facial expressions  [7]  and conversational data  [8] ), and physiological measures (e.g., facial electromyography (EMG), skin conductance response (SCR), electrocardiogram (ECG), electroencephalography (EEG))  [6] ,  [9] ,  [10] . Although selfreported methods are easy to operate, they suffer from recall and selective reporting biases  [11]  and interference with the interaction process  [10] . The progress on emotion recognition using facial expressions and speeches is promising due to the recent advancement in deep learning (e.g.,  [7] ,  [12] ). Nevertheless, such affective displays are subject to social display rules and the expressions might not be consistent with their real emotions  [1] ,  [13] . By contrast, physiological measures are less vulnerable to voluntary control and allow the system to evaluate users' emotions in real time continuously  [6] . The continuous fashion in recognizing emotion is consistent with human perception on emotions in social interaction  [14] , and the real-time characterization is preferable for humancomputer interaction systems that aim to fulfill users' emotional needs by addressing their emotions in real time  [1] ,  [15] ,  [16] . Therefore, many studies have successfully built machine learning models based on physiological measures (e.g.,  [6] ,  [17] -  [25] ).\n\nCorrespondingly, an increasing number of datasets (e.g.,  [26] -  [28] ) have been created to train models to recognize emotions automatically using physiological data. However, one of the biggest challenges in training models is to label the physiological data. Unlike other databases that have one label for each sample (e.g., facial expression images in AffectNet  [29] ), physiological data were continuously recorded when the participant was involved in an emotional episode. Thus, it is extremely time-consuming to label the data frame by frame, which is impossible  [22] . Many datasets only had an overall label for a continuous segment of physiological data recorded in the whole emotional event, such as the Database for Emotion Analysis using Physiological signals (DEAP)  [26] , which was only sparsely labeled using self-reports or expert observation. Here sparse labelling  [22]  was used to indicate that there was only one or a few static labels annotating a segment of physiological data, which was supposed to correspond to dynamic changes in emotional states (i.e., continuous labels). Self-reported labelling was often conducted once (due to possible interference in the interaction process) after the participants finished the interaction with the stimuli, such as video or image viewing  [6] ,  [9] ,  [26]  and game playing  [30] . Labeling by expert observation can potentially increase the number of the labels in a segment of physiological data, and multiple experts should be involved in order to increase reliability of the labels (e.g.,  [31] ). However, both of these methods only label the most salient one within the whole emotional episode rather than the dynamic changes of emotions of the participants. Such labels tend to be noisy when the most salient period only accounts for a small period of the whole labeled episode  [22] .\n\nAnother challenge for many machine learning models in emotion recognition is that they are black-box models (especially for deep learning models) and the knowledge learned by them is hidden. For one thing, without revealing the knowledge learned, the acceptance and trust in such models can be jeopardized  [32] , especially in high risk areas such as medicine  [33] ,  [34]  and transportation  [10] ,  [35] . For another, when such knowledge is revealed, it can help distinguish between different emotions. For instance, Weitz et al.  [36]  used layerwise relevance propagation and local interpretable modelagnostic explanations (LIME) to understand the decision made by a deep neural network, so that they were able to distinguish facial expressions caused by pain and disgust in order for the nurses to give better care for their patients.\n\nGiven the above challenges, the objective of this paper is to recognize emotions from sparsely-labeled peripheral physiological data with explanations. First, we applied singular spectrum analysis (SSA) to decompose the physiological data into individual components, and extracted three types of features from selected components, including sample entropy, fuzzy entropy, and energy. SSA has been widely applied in time series analysis and it can decompose the raw signal into a small number of independent and interpretable components (e.g., periodic components, slow-varying components, and noise components)  [37] , and it has been proved to improve the performance of classification models built on physiological data by removing random noise and data reconstruction at the same time. Entropy measures have also been widely used for feature extraction of physiological measures for classification purposes with good performance. For example, Lu et al.  [38]  extracted entropy features from SSA components of various physiological measures, including EEG, EMG, and ECG, across multiple classification scenarios and the performance was much better than extracting features directly from the raw physiological signals. Particularly, sample entropy was used to predict arrhythmic risk using ECG signals  [39]  and to recognize emotions using EEG  [40] , fuzzy entropy was used to detect epileptic seizure with EEG signals  [41] , sample entropy and fuzzy entropy were used to predict eye states using EEG signals, physical actions using EMG signals, and heart states using ECG signals  [38] . The energy feature was similar to the power spectral density, which was also used in emotion recognition  [42] . Second, in order to build explainable machine learning models, we applied tree-based ensemble models, including eXtreme Gradient Boosting (XGBoost)  [43]  and Light Gradient Boosting Machine (LightGBM)  [44]  and SHapley Additive exPlanations (SHAP)  [33] ,  [45] . The entropy and energy features extracted from SSA components were used to train the XGBoost and LightGBM models. The LightGBM model was found to perform better than the XGBoost model, and we used SHAP to explain the LightGBM model by identifying the most significant features and their main and interaction effects. Such ensemble methods have proved to perform better than other traditional machine learning models, such as support vector machines (SVMs), due to the fact that ensemble learning overcomes the shortcomings of single classifiers by combining dissimilar classifiers  [46] -  [49] . SHAP is based on the game theory to explain the output of the LightGBM model by examining feature contributions in a coalition using the classic Shapley values  [50]  and it has three desirable mathematical properties, i.e., local accuracy, missingness, and consistency, which provide consistent and locally accurate explanations  [33] ,  [51] . However, the computational requirements make it difficult to apply to many machine learning models with a large number of samples, especially for deep learning models. Lundberg et al.  [45]  proposed an algorithm to explain tree-based models efficiently using SHAP. Hence, by making use of SHAP and LightGBM, we were able to understand the patterns captured by the LightGBM model in recognizing emotions, by helping select and explain the most important SSA components of their main effects and interaction effects. In addition, both the machine learning models were validated using a leave-one-subject-out (LOSO) scheme (i.e., user-independent), thus the hidden knowledge revealed tended to be more generalizable.\n\nIn summary, the contributions of this paper are described as follows:\n\n• We decomposed the peripheral physiological data into individual SSA components and extracted both entropy and energy features to deal with the sparse label issue. Such a method explores a wide range of feature space to improve emotion representation across a wide range of signals while removing random noise at the same time. • We built explainable machine learning models using treebased ensemble machine learning models (i.e., XGBoost and LightGBM) for emotion recognition, and used SHAP to identify the most important SSA components to improve our results with LOSO cross-validation. Such a feature selection method provides useful information and implications on the parameter selection in SSA decomposition.\n\n• We identified the main effects and interaction effects that showed the hidden relationships between the emotion representations and the most important features identified. Such insights were used to improve the performance of the proposed models and have the potential to pave the way for future applications with emotion recognition in different domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotion Recognition Using Physiological Data",
      "text": "Emotions can be recognized from physiological data collected from the central nervous system and the peripheral nervous system. The most frequently used signal from the central nervous system is EEG (e.g.,  [21] ,  [26] ,  [52] ) while there are a large number of signals that can be collected from the peripheral nervous system, such as SCR, EMG, electrooculargram (EOG), and ECG (e.g.,  [6] ,  [22] ,  [53] ). In order to recognize emotions, various machine learning models were applied. Picard et al.  [53]  used SCR, EMG, respiration data, and blood volume pulse of a single participant to predict eight emotions with Fisher projection with 81% accuracy. Liu et al.  [54]  used ECG, photoplethysmogram (PPG), phonocardiogram, SCR, and facial EMG to recognize three emotions using SVMs, which obtained 82.9% accuracy in recognizing emotions from autistic children. Many researchers made use of the public DEAP dataset for binary classification. For example, Yin et al.  [52]  applied an ensemble classifier using stacked autoencoder and extracted 425 features from EEG, EOG, GSR, PPG, EMG, respiration, and skin temperature and obtained f1-scores of 77.98% for arousal recognition and of 79.50% for valence recognition. Lin et al.  [55]  proposed a convolutional neural network (CNN) by transforming EEG data into image data with time and frequency information, and other hand-crafted features from peripheral physiological data with 87.30% accuracy (f1-score: 72.84%) for arousal recognition and 85.5% accuracy (f1-score: 80.06%) for valence recognition. Ma et al.  [21]  proposed a deep learning model named multimodal residual long-short-term memory (LSTM) with an accuracy of 92.87% and 92.30% for binary arousal and valence classification. Despite the good performance of the deep learning models, these models were user-dependent, i.e., models trained using data from one specific participant, and thus were less likely to generalize across participants due to potential individual differences.\n\nOther researchers built user-independent models by mixing data from multiple participants with k-fold cross validation. For example, Nasoz et al.  [56]  used SCR, skin temperature, and heart rate to recognize six emotions from 29 participants and obtained the best accuracy of 83% using feedforward neural networks trained by Marquardt back propagation. Frantzidis  [13]  applied a decision tree model with a Mahalanobis distance classifier to recognize four emotions with EEG and SCR data from 28 participants with 78% accuracy. Zhou et al.  [6]  proposed machine learning models using rough set to recognize seven discrete emotions elicited by visual stimuli from 42 participants. They proposed genderspecific, culture-specific, and general models and found that models built from female participants (i.e., gender-specific) had the best performance of 78.0% while general participantindependent models only had 63.5% in terms of f1-scores. Wen et al.  [20]  used a multi-variant correlation method of various physiological signals, including blood oxygen saturation, SCR, and heart rate to identify useful features, based on which a random forest model was proposed to recognize four emotions with 74% accuracy. Ringeval et al.  [57]  proposed an LSTM regression model by combining audio, visual (facial expressions) and physiological (ECG, SCR) data from the Remote Collaborative and Affective Interactions (RECOLA) dataset  [58] , and the concordance correlation coefficients were 0.804 for arousal and 0.528 for valence. Li et al.  [59]  included group-based individual response specificity in building emotion recognition models with ECG, SCR, and PPG and their performance was better (f1-score: 0.75) than a general model (f1-score: 0.67) without such information. Hassan et al.  [60]  proposed a deep belief network and an SVM model and achieved 89.53% accuracy on the DEAP dataset for recognizing 5 discrete emotions using EMG, PPG and SCR data.\n\nThe user-independent models with k-fold cross-validation mentioned above still assume that the samples are independent and identically distributed. However, the samples from one participant are actually correlated biologically and temporally  [61] . Another type of validation strategy that tends to have better generalization is LOSO cross-validation. In this process, one subject is randomly selected for testing while the rest ones are used for training the model and this procedure is repeated until all the subjects are used for testing  [62] . This validation strategy is especially useful for the model to predict emotions of new individuals  [63] . Romeo et al.  [22]  proposed a multiple instance learning strategy with SVMs to recognize emotions using physiological measures from the DEAP dataset. The multiple instance learning treated the data as negative if all the instances were negative and positive if at least one of them was positive. They stated that using the LOSO cross-validation, the results were not satisfactory on the DEAP dataset and thus were not reported. On their own dataset, they had the best binary classification accuracy of 69.1% for arousal and 68.0% for valence. Kandemir et al.  [64]  proposed a multi-task multi-view learning model with SVMs. They reported their LOSO cross-validation performance on the DEAP dataset using both EEG and peripheral physiological measures was 60% (accuracy) and 56% (f1-score) for valence, 58% (accuracy) and 53% (f1-score) for arousal, and 67%(accuracy) and 51% (f1-score) for liking. Zhong et al.  [65]  proposed a temporal information preserving framework to identify the relations between physiological data and emotions over time and by combining both physiological and facial expression data, they obtained the best accuracy for valence and arousal as 70.53% and 73.53% on the MAHNOB-HCI dataset using LOSO crossvalidation  [27] . Gupta et al.  [66]  proposed a flexible analytic wavelet transform to explore channel specific EEG signals in the forms of different sub-band forms for cross-subject emotion recognition with random forest and support vector machines. Another way to help improve the performance of LOSO is to minimize individual differences. Chen et al.  [25]  proposed a personal-Zscore feature processing method, which was found to improve the representation ability of emotion across different subjects, which improved emotion recognition on the SEED dataset. Li et al.  [24]  proposed a transfer learning method that reduced the EEG differences between the target (i.e., the new person's data in the testing set) and each source (i.e., the existing participants' data in the training set) that improved three-category classification accuracy on SEED dataset by 12.72%. Lan et al.  [23]  proposed a domain adaptation technique to reduce cross-subject variance and they improved the accuracy from the baseline by 7.25%-13.40% on the DEAP and SEED datasets.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Explainable Machine Learning Models",
      "text": "We have witnessed great successes of machine learning models in various areas as they can capture the hidden domain knowledge and make predictions for unseen data.\n\nNevertheless, these models can be less acceptable and trusted during the deployment stage without showing the captured domain knowledge, i.e., explainability  [32] . Simpler machine learning models, such as linear regression models and decision trees are explainable. However, the majority of the other machine learning models (e.g., SVMs, LSTM, deep belief networks, CNNs) reviewed above in emotion recognition are black-box models and thus are not explainable. Although the importance of explainability in decision making with high risks is greater (e.g., medicine  [33] ,  [34] , transportation  [10] , and misinformation  [67] ), it is still useful in different affective computing applications (e.g.,  [36] ). In addition, the choice between two types of models, i.e., simple, explainable models and complicated, black-box models tends to be easily made in emotion recognition as simpler models are not able to have good performance. Therefore, the only way to understand the captured domain knowledge is to apply post-hoc explainability to black-box models  [68] . Very few studies attempted to explain models in emotion recognition in the literature. LIME was used to explain a CNN model to distinguish between pain and disgust facial expressions that helped nursing staff identify pain episodes of patients by monitoring their facial expressions  [36] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methods",
      "text": "The proposed method has five major steps (see Fig.  1 ), including 1) data cleaning, 2) feature extraction, 3) emotion recognition, 4) model explanation, and 5) feature selection as described below. Note the third step and the fourth step were conducted twice before and after the fifth step in order to select the optimal features for best performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Dataset And Data Preprocessing",
      "text": "In this paper, the DEAP dataset  [26]  was used to demonstrate the proposed method with eight peripheral physiological signals, including horizontal EOG (i.e., hEOG = hEOG 1 -hEOG 2), vertical EOG (i.e., vEOG = vEOG 1 -vEOG 2), zygomaticus major EMG (i.e., zEMG = zEMG 1 -zEMG 2), trapezius EMG (i.e., tEMG = tEMG 1 -tEMG 2), SCR, PPG, respiration (Resp), and skin temperature (Temp). The data were downsampled from 512 Hz to 128 Hz, which was adequate and efficient for processing the peripheral physiological signals and this has been widely used in previous studies  [22] ,  [69] . The locations of each electrode during the data collection process are shown in Table  I .\n\nA number of 32 participants watched 40 music videos with 60 seconds long and rated on valence, arousal, and liking. Between each trial, there were five seconds of fixations on screen before the trial and three seconds fixations on screen after the playback of each music video. In this study, we included data of the first three seconds with fixations on the screen as the baseline and of the rest 60 seconds of video watching as the usable signals. There was a short break after watching 20 videos and the estimated length of the experiment was about 90 minutes. Thus, there were 40 × 32 = 1280 samples in total. The labels used in this paper included valence, arousal, and liking. Following previous studies, they were binarized by thresholding at 5 from the 9-point Likert scale (e.g.,  [22] ,  [64] ).\n\nFirst, the samples were smoothed with a moving average filter with a span of 64 data points (i.e., 0.5 second since the signals had a sampling rate of 128 Hz) for SCR and Temp and of 5 data points for other signals. Second, the signals were baseline corrected, i.e., the 60 seconds of the usable data were subtracted from the mean of the baseline. Third, each sample was normalized by subtracting the mean and divided by the standard deviation. Last, each sample was detrended quadratically with Matlab 2019b (Mathworks, Natick, MA), except Temp and SCR. Ledalab was used to decompose SCR into a phasic component and a tonic component  [70] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Signal Decomposition With Ssa",
      "text": "The decomposition of the physiological signals using SSA was as follows  [37] . Each sample was represented as a time series signal\n\nThis process was named as embedding and its parameter is the window length L and 2 ≤ L ≤ N . In this research, L was set at 12 based on previous studies  [38] . Then, singular value decomposition was applied to the matrix, S = Y Y T . Assuming λ 1 ≥ λ 2 ≥ ... ≥ λ L were the eigenvalues of S and U 1 , U 2 , ..., U L was the orthonormal basis of the eigenvectors of S. By setting\n\nwhere\n\nIn the reconstruction stage, it had two steps, including eigentriple grouping and diagonal averaging. time series X N = [x 1 , x 2 , ..., x N ] were reconstructed as a sum as follows:\n\nIn this research, we used the pymssa package in Python (https://github.com/kieferk/pymssa) for SSA implementation and only kept a selected number of components to remove the noise, which proved to be effective for feature extraction  [38] . Assuming X N = S N + R N , we only selected I k = {1, ..., s}, s < d in order to remove the noise component R N . We used the formula of singular value hard thresholding to remove the noise components, which proved to be able to recover matrices with bounded nuclear forms with the best possible asymptotic mean squared error  [19] . For example, Fig.  2  (a) shows the 12 principal components of a PPG signal and the formula of singular value hard thresholding kept the first four components for reconstruction and Fig.  2  (b) shows the four reconstructed components (the top four panels) and the comparison between the original signal and the reconstructed one (the fifth panel) with four reconstructed components. Similarly, the first two, two, one, three, two, two, and one component(s) were kept for hEOG, vEOG, zEMG, tEMG, SCR, Resp, and Temp, respectively, for reconstruction and feature extraction. Note the SCR signal used for SSA decomposition was the phasic component of the SCR data and the tonic component was used as the second component for feature extraction. Thus, a total number of 17 SSA components were extracted and we calculated their sample entropy, fuzzy entropy, and energy, which resulted in a total number of 51 features.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Feature Extraction",
      "text": "Three features were extracted for each component derived in the previous step, including sample entropy, fuzzy entropy, and energy. Sample entropy and fuzzy entropy are complexity measures. Sample entropy is based on approximate entropy, which is used to measure the degree of regularity and classify complex systems of both deterministic chaotic and stochastic processes  [71] . However, it is heavily dependent on the data length (at least 1000 data points) and can lead to inconsistency results for short and noisy data  [72] . Sample entropy is an improved version of approximate entropy by removing data length dependency and was widely used as features extracted from physiological signals for classification purposes  [38] ,  [40] ,  [72] . Fuzzy entropy makes fuzzy measurement of signals by introducing a fuzzy membership function to avoid abrupt changes, which can be especially useful for characterizing EMG signals  [73] .\n\nGiven an SSA component,\n\nwhere A was the number of template vector pairs that satisfied d[Y k m+1 (i), Y k m+1 (j)] < r and B was the number of template vector pairs that satisfied d[Y k m (i), Y k m (j)] < r. In calculating fuzzy entropy, Y k m (i) was defined as [y k i , y k i+1 , ..., y k i+m-1 ] -y 0 i , where y 0 i was the baseline defined as\n\nThen we used the following to calculate fuzzy entropy  [73]  F uzzyEn = lnΦ m (n, r) -lnΦ m+1 (n, r),\n\nwhere\n\nAccording to  [38] , we used embedding dimension, m = 2 and tolerance, r = 0.15 in calculating entropy (n = 2 for fuzzy entropy) measures for each SSA component derived from the physiological data. In addition, energy was also calculated for the k-th SSA component of a physiological signal using where N = 7680 was the number of the data points in the SSA component.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Lightgbm-Based Classification Model",
      "text": "LightGBM  [44]  is an ensemble machine learning model based on the technique of gradient boosting decision trees (GBDTs), which is one of the best performing models in machine learning  [74] . LightGBM combines the prediction results of multiple decision trees by iteratively adding them together as follows:\n\nwhere ŷs i is the predicted result for the i-th instance at s-th iteration and f s is the learned model for the s-th decision tree. This process aimed to minimize a loss function with a regularization term, i.e.,\n\nwhere the loss function was binary logloss in this paper and the regularization term controlled the complexity of the model to reduce overfitting. During the training process, GBDTs need to recursively split the data by scanning information gain of each data instance, which is slow when both the sample size and number of the features are big. LightGBM uses a leaf-wise strategy to split the data that has the most information gain using gradient-based one-side sampling (GOSS). The GOSS strategy keeps the samples that contribute the most to information gain with larger gradients and randomly drop those with small gradients. This notion both improves the accuracy and efficiency. LightGBM also uses exclusive feature bundling by combining sparse features that are almost exclusive to each other to further reduce computational resources. These two features made the LightGBM successful and had better performance than XGBoost  [43]  across a range of publicly available datasets  [44] . In this paper, we used the following parameters involved in the training and prediction process of LightGBM: objective: binary, boosting type: goss, metric: binary logloss, early stopping rounds: 30, and num boost round: 500 and used random search for 300 iterations to pick the best combinations among the following parameters, including learning rate between 0.01 and 0.5, sub feature between 0 and 1, num leaves between 5 and 20, min data between 10 and 100, max depth between 5 and 20. Likewise, we used the following parameters involved in XGBoost as a comparison, including objective: binary:logistic, metric: logloss, early stopping rounds: 30, and num boost round: 500 and used random search for 300 iterations to pick the best combinations among the following parameters, including learning rate between 0.01 and 0.5, subsample between 0 and 1, colsample bytree between 0 and 1, max depth between 5 and 20, and min child weight between 1 and 10.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Model Explanation Using Shap",
      "text": "In order to improve the explainablility of the LightGBM model, we used SHAP  [33] ,  [45] ,  [51] , which was built on Shapley value  [50]  from game theory. The Shapley value of a pair of features and its value is computed using:\n\nwhere n is the total number of features, T is a subset of F (i.e., the coalition of all the features), v(T ) is the contribution of the coalition T in predicting emotion, and the above equation sums all the subsets T that do not include the feature i. Due to the computational load to calculate all the contributions of T , when the number of the features is large, SHAP explanation was proposed for tree-based models to reduce its computation from O(T rL2 n ) to O(T rLD 2 ), where T r is the number of the trees, L is the largest number of the leaves of the trees, n is the total number of the features in the model, and D is the depth of the trees. Due to the fact that SHAP has desirable properties, including local accuracy, missingness, and consistency, which offer a solid theory in explaining machine learning models compared to feature importance measures in other methods  [51] . SHAP is also able to identify the most salient interaction effect between two features, defined as the additional combined feature effects minus individual main feature effects:\n\nwhere\n\nIn this research, SHAP was used to not only identify the most important features in emotion recognition in LightGBM as a feature selection process, but also explain the main effects and interaction effects of the important features.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Iv. Results",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Prediction Performance",
      "text": "We used LOSO cross-validation to show the performance of the proposed method for the three binary classification models, i.e., valence, arousal, and liking. The total sample size was 1280, the training sample size was 1240, and the test sample size was 40 in each round. This process was repeated for 32 times for 32 participants. We used f1-score and accuracy to measure the performance of the models  [6] . First, we included all the features in the LightGBM model and SHAP was used to identify the importance of each feature. Second, we added one feature at a time from the most important feature to the least one to find the optimal performance. Table  II  shows the performance of the proposed model with all the features and an optimal set of features for XGBoost and LightGBM. Using the Wilcoxon signed-rank test, we found that LightGBM performed significantly better than XGBoost for valence, arousal, and liking (all p < 0.001) both by comparing the results (both accuracy and f1-score) of 51 models (adding one feature at a time, see Fig.  3 ) between LightGBM (best) and XGBoost (best) and between LightGBM (all) and XGBoost (all). The model predicted Liking the best with an average accuracy of 0.691 and an average f1-score of 0.860, followed by arousal and valence with average f1-scores of 0.823 and 0.814, and average accuracy scores of 0.651 and 0.616. As a comparison, we also included the results from another study  [64]  that used LOSO cross-validation, showing the potential of the proposed model.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Feature Selection And Importance",
      "text": "Since LightGBM performed better than XGBoost, the results below were only produced from LightGBM. We first identified the overall ranking of feature importance by including all the features in the LightGBM model. This importance was measured at predicting all the test samples of the 32 rounds in the LOSO process rather than the training samples in order to help improve their generalizability. Then, we obtained the performance of 51 models (corresponding to the total number of the extracted features) by adding one feature at a time from the most important one to the least important one as shown in Fig.  3 . The best f1-scores were 0.814 (#feature = 36), 0.823 (#feature = 48), and 0.860 (#feature = 22) and the best accuracy was 0.616 (#feature = 6), 0.651 (#feature = 12), and 0.691 (#feature = 14), respectively, for valence, arousal, and liking corresponding to the results in Table  II . Therefore, by only selecting a partial number of the features, the best performance was obtained. The variability of accuracy tended to be larger than that of f1-scores for all valence, arousal, and liking.\n\nBy using the features selected with the optimal f1-scores, we used SHAP to identify the top 20 features for predicting valence, arousal, and liking in Fig.  4  ranked by their global influence of each feature in terms of their mean absolute SHAP value (in log loss) using 1/N s Ns i=1 |ϕ j i |, where N s is the number of the samples. Each dot (i.e., SHAP value ϕ j i ) is plotted horizontally with color coding. Those in blue are smaller values and those in red are larger values. The horizontal location indicates the contributions (i.e., effects of the variable) to the prediction and the vertical location indicates the global importance of the variable on the prediction. For example, vEOG1 FE, vEOG2 SE, and PPG1 FE were the most important features in predicting valence. The density of the violin figures shows how common the samples are in the dataset. Note in the feature names, the number before \" \" is the corresponding SSA component and the letters after \" \" indicate the feature type, where SE = sample entropy, FE = fuzzy entropy, and En = Energy. For example, \"vEOG1 FE\" indicates the fuzzy entropy of the first component of vertical EOG using SSA decomposition.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Main And Interaction Effects",
      "text": "To better understand how individual features influence the model prediction, we further examined the main and interaction effects. Due to space limitations, we only showed one main and interaction effect each for valence, arousal, and liking. Note we only included data from the 2.5th percentile to 97.5th percentile in order to examine the major trend. Fig.  5a  shows the main effect of vEOG1 FE. The overall trend is that the larger the value of vEOG1 FE, the less likely the positive valence. This is consistent with the results in Fig.  4a . Note a larger entropy value in this paper indicates the signal is less regular, and vice versa. At the same time, vEOG1 FE interacts with PPG1 FE. In order to better observe the interaction effect, we show it in Fig.  5b  that larger values of PPG1 FE leads to more contributions to predicting positive valence than smaller values of PPG1 FE when vEOG1 FE is smaller than about 0.018, and this effect is reversed when vEOG1 FE is larger than about 0.018. This interaction effect was also calculated as 80.2 as shown in Fig.  8a  using Eq. 9. Fig.  6a  shows the main effect of vEOG2 SE on predicting arousal, showing a positive correlation between vEOG2 SE and contributions to high arousal in terms of its SHAP values. This effect was interacted by another feature named Resp1 En and its interaction effect is shown in Fig.  6b  where larger values of Resp1 En leads to more contributions to predicting low arousal than smaller values of Resp1 En, when vEOG2 SE is smaller than about 0.28 and this effect is reversed when vEOG2 SE is larger than about 0.28. This interaction effect was also calculated as 18.0 as shown in Fig.  8b  using Eq. 9. Fig.  7a  shows the main effect of PPG1 FE on predicting liking, showing a negative correlation between PPG1 FE and the contributions to liking in terms of its SHAP values. This effect was interacted by another feature named Temp1 En and its interaction effect is shown in Fig.  7b  where smaller values of Temp1 En leads to more contributions to predicting liking than larger values of Temp1 En, when PPG1 FE is smaller than about 4000 and this effect is reversed when PPG1 FE is larger than about 4000. This interaction effect was also calculated as 69.7 as shown in Fig.  8c  using Eq. 9.\n\nIn order to show the interaction effects between different features of the LightGBM (best) model as shown in Table  II , we calculated the interaction matrices for valence, arousal, and liking in Fig.  8  ordered by features of total interaction effects with all other features selected in the model. Those in the diagonal entries indicate the main effects of the specific features, while those in the off-diagonal entries indicate the interaction effects between two features. Note only features of the total interaction effects in the top ten list were shown in the matrix due to space limit. As an example, the top three features are vEOG1 FE, PPG1 FE, and GSR2 FE in Fig.  8a , and the feature vEOG1 FE has the largest total interaction effects by summing all other interaction effects with others.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "V. Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Model Performance",
      "text": "In this paper, we used a LOSO cross-validation process in order to improve the generalizability of the explanations of the LightGBM models. Compared to previous studies  [22] ,  [64]  with LOSO cross-validation, our proposed method had better performance. Despite the fact that there were better results reported using the same dataset, they did not adopt a LOSO cross-validation process or also used EEG data (see Table  II ). The application of SSA was effective to improve the performance of our models, which was reported to eliminate noise in the raw data better than digital filtering methods  [76] , and decomposed the raw data into independent additive components  [37] . By making use of the singular value hard thresholding formula  [19] , we were able to remove the noise effectively. The SSA method was also shown to improve the classification performance of machine learning models constructed from physiological signals  [40] ,  [54] . In addition, there were only two parameters involved in SSA decomposition, including the window length and the number of the components for reconstruction, which made it easier for practical uses. Another important reason that improves the performance of our models might be the extracted features. Rather than using a statistic features (e.g., mean, standard deviation), we made use of complexity and energy measures. For one thing, entropy measures the randomness of a time series signal, which is an essential limitation of moment statistics  [77] . For example, a periodic signal (e.g., RC 2 in Fig.  2b ) could have exactly the same mean value and standard deviation if it was randomized while the entropy measures are able to tell the differences between the two. Similarly, there were only a small number of parameters in calculating entropy measures, including the embedding dimension and the tolerance for sample entropy, and a third parameter, i.e., the step of the fuzzy function. We selected these parameters based on previous studies  [38] ,  [40] ,  [73] , which proved to have good performance in this paper. Moreover, the inclusion of energy feature was also helpful, as evidenced in Fig.  4 , where there are 6, 4, and 7 energy features (those ended with \" En\") included in the top 20 features for valence, arousal, and liking, respectively. Furthermore, LightGBM excelled in emotion recognition using the extracted features in terms of effectiveness and efficiency. The GOSS strategy and the feature bundle approach played a significant role in improving the performance compared to another successful tree-ensemble method, i.e., XGBoost. Finally, the feature selection method enabled by SHAP boosted the performance of emotion recog-nition as evidenced in Fig.  3 . Hence, the good performance was the result by leveraging different aspects of the proposed method.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Explainability For Emotion Recognition",
      "text": "Our research is among the few examples that attempted to explain the hidden knowledge captured by the machine learning models in emotion recognition. First, SHAP was able to identify the most important features. As shown in Fig.  4 , the top three most important features are vEOG1 FE, Resp1 FE and tEMG2 SE for valence prediction, vEOG2 SE, tEMG3 SE, and GSR1 En for arousal prediction, and PPG1 FE, hEGO2 FE, and vEOG2 SE for liking prediction. Also as shown in Fig.  3 , a small number of top features already have good performance in terms of f1-scores. Such insights are extremely helpful in selecting features and improving model performance. In addition, the numbers of SSA components decomposed from each raw signal were small. There were two, two, one, three, two, two, one, and four components extracted from hEOG, vEOG, zEMG, tEMG, SCR, Resp, and PPG. Such SSA components were either varying trends or oscillatory signals (see Fig.  2 ). This helped interpret individual features to a large extent. For example, vEOG1 FE and vEOG2 FE were the fuzzy entropy of the major varying trend and the periodic component of vEOG and the trend component was most important in predicting valence in the model. Even though there were four oscillatory components extracted from PPG signals, they might be related to cardiac changes in the blood volume related to heart beats, superimposed by other components due to respiration, thermoregulation, and activities in the sympathetic nervous system  [78] . Further investigation is needed to identify their mapping relations. Second, we are able to identify the hidden relationships between different features and emotions (i.e., valence, arousal, and liking). For example, vEOG1 FE was negatively correlated with its contributions (i.e., SHAP values) to positive valence prediction in Fig.  5a , vEOG2 SE was positively correlated with its contributions to high arousal prediction in Fig.  6a , and PPG1 FE was negatively correlated with its contributions to liking prediction in Fig.  7a . By examining such relationships, we are able to understand the unique patterns of physiological measures associated with different emotions and such insights are useful for different practical applications as evidenced by  [36] . Thus, the most important features derived from physiological data could recognize emotion reliably and consistently, based on which appropriate measures might be provided in various areas for effective emotion intervention, such as medical care  [79] . Third, what is less well known is the interaction between different features that SHAP identified as shown in Figs. 5b, 6b, and 7b. Such interaction effects were visualized continuously rather than among a small number of levels in classical statistical analysis. We further calculated the interaction matrices in Fig.  8  and we quantitatively calculated the interaction between each features, which offered a big picture in understanding how individual features influence emotion classification.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Limitations And Future Work",
      "text": "Our study also suffers from limitations, which can be left for future work. First, the SSA method helped extract useful features to improve model performance. However, one feature was decomposed into several and the relationships between these decomposed components and emotions can be less clear compared to the original raw signal. For example, we found that PPG2 SE, PPG3 SE, and PPG4 SE were found to be in the top 20 features for valence prediction. As mentioned previously, more investigation is needed to understand what these periodic components mean physiologically as shown in Fig.  4a . Furthermore, it might be less clear about the overall relationship between PPG and valence as a whole without examining all the four components of PPG. One possible solution is to further reduce the number of the features in the model. As shown in Fig.  3 , the performance of models can be quickly stabilized with only a small number of features in terms of f1-scores. Second, the features were extracted from 60-second time series samples, which enabled the LightGMB model to perform the classification purposes. However, we were not able to tell the dynamics of the physiological data. Future research can be conducted by using a sliding time window, which can potentially improve the performance  [40] . Third, we selected the values of the parameters involved in SSA and entropy measures based on previous studies  [38] ,  [40] ,  [73] . In the future, it is possible to conduct a sensitivity analysis with regard to different parameters involved, including the window length and number of components in SSA and the embedding dimension in calculating entropy measures, to help understand how they influence the model performance. Fourth, we used the DEAP dataset to demonstrate the proposed method. Specifically, we only used the 12 peripheral physiological data for emotion recognition with LOSO cross validation. In the future, EEG signals can also be included to check the performance of the proposed method. The dataset was recorded in two separate locations, including Twente and Geneva. It potentially can help improve the capability of generalizing the results, but also had differences in data quality due to different hardware. Our proposed method can also be applied to other datasets to examine its generalizability.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we proposed a method to recognize emotions using peripheral physiological data by taking both performance and explainability into consideration. First, in order to deal with the spare labeling issue in the physiological data, we used both complexity and energy measures to extract features from decomposed components enabled by SSA. Such an approach proved to be effective in this paper as evidenced by the better performance of LightGBM when it was compared with XGBoost and previous research. Second, in order to improve the explainability of the LightGBM model, SHAP was used to identify the most important features and the relationships (i.e., main and interaction effects) between individual features and emotions. Therefore, our proposed method has good implications on not only improving the performance of emotion recognition, but also on deploying the model for affective human-machine systems.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The major steps involved in the proposed method, including 1) data cleaning, 2) feature extraction, 3) emotion",
      "page": 5
    },
    {
      "caption": "Figure 2: (a) shows the 12 principal components of a PPG",
      "page": 5
    },
    {
      "caption": "Figure 2: (b) shows the four reconstructed components (the top four",
      "page": 5
    },
    {
      "caption": "Figure 2: An example of decomposed PPG signals using",
      "page": 6
    },
    {
      "caption": "Figure 3: The best f1-scores were 0.814 (#feature =",
      "page": 8
    },
    {
      "caption": "Figure 4: ranked by their global",
      "page": 8
    },
    {
      "caption": "Figure 5: b that larger values of PPG1 FE leads to",
      "page": 8
    },
    {
      "caption": "Figure 8: a using Eq. 9. Fig. 6a shows the main",
      "page": 8
    },
    {
      "caption": "Figure 6: b where larger values of Resp1 En leads",
      "page": 8
    },
    {
      "caption": "Figure 8: b using Eq. 9. Fig. 7a shows the main",
      "page": 8
    },
    {
      "caption": "Figure 7: b where smaller values of Temp1 En leads to",
      "page": 8
    },
    {
      "caption": "Figure 8: c using Eq. 9.",
      "page": 8
    },
    {
      "caption": "Figure 8: ordered by features of total interaction",
      "page": 8
    },
    {
      "caption": "Figure 3: Feature selection by comparing the performance of the model that added one feature at a time from the most important",
      "page": 9
    },
    {
      "caption": "Figure 4: The top 20 features ranked for their global inﬂuence in terms of SHAP values (from top to bottom in descending",
      "page": 9
    },
    {
      "caption": "Figure 2: b) could have exactly the same mean value and standard",
      "page": 9
    },
    {
      "caption": "Figure 3: Hence, the good performance",
      "page": 9
    },
    {
      "caption": "Figure 3: , a small number of top",
      "page": 9
    },
    {
      "caption": "Figure 5: (a) Main effect of vEOG1 FE on predicting valence, showing a negative correlation between vEOG1 FE and",
      "page": 10
    },
    {
      "caption": "Figure 6: and Fig. 7.",
      "page": 10
    },
    {
      "caption": "Figure 6: (a) Main effect of vEOG2 SE on predicting arousal, showing a positive correlation between vEOG2 SE and",
      "page": 10
    },
    {
      "caption": "Figure 2: ). This helped",
      "page": 10
    },
    {
      "caption": "Figure 5: a, vEOG2 SE was positively",
      "page": 10
    },
    {
      "caption": "Figure 6: a, and PPG1 FE was negatively correlated with its con-",
      "page": 10
    },
    {
      "caption": "Figure 7: a. By examining such",
      "page": 10
    },
    {
      "caption": "Figure 7: (a) Main effect of PPG1 FE on predicting liking, showing a negative correlation between PPG1 FE and the",
      "page": 11
    },
    {
      "caption": "Figure 8: Interaction matrices for the top 10 features that had the strongest total interaction with other features: (a) Valence",
      "page": 11
    },
    {
      "caption": "Figure 8: and we quantitatively calculated",
      "page": 11
    },
    {
      "caption": "Figure 4: a. Furthermore, it might be less clear about the overall",
      "page": 11
    },
    {
      "caption": "Figure 3: , the performance of models can",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2. Feature extraction\n1. Data cleaning\nPeripheral \nPreprocessed \nSingular  spectrum \nEntropy and energy\nphysiological  data in   \nsignals \nanalysis\nfeatures\nDEAP dataset\n5. Feature selection\nResults\nXGBoost\nTraining": "3. Emotion  recognition\nand testing \n4. Model explanation\nsamples\nFeature selection by \nSHAP explanation\nLightGBM\nSHAP"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Prospect-theoretic modeling of customer affective-cognitive decisions under uncertainty for user experience design",
      "authors": [
        "F Zhou",
        "Y Ji",
        "R Jiao"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Human-Machine Systems"
    },
    {
      "citation_id": "3",
      "title": "The relation of strength of stimulus to rapidity of habit-formation",
      "authors": [
        "R Yerkes",
        "J Dodson"
      ],
      "year": "1908",
      "venue": "Punishment: Issues and experiments"
    },
    {
      "citation_id": "4",
      "title": "Examining the effects of emotional valence and arousal on takeover performance in conditionally automated driving",
      "authors": [
        "N Du",
        "F Zhou",
        "E Pulver",
        "D Tilbury",
        "L Robert",
        "A Pradhan",
        "X Yang"
      ],
      "year": "2020",
      "venue": "Transportation research part C: emerging technologies"
    },
    {
      "citation_id": "5",
      "title": "Dynamic emotion understanding in human-robot interaction based on two-layer fuzzy svr-ts model",
      "authors": [
        "L Chen",
        "M Wu",
        "M Zhou",
        "Z Liu",
        "J She",
        "K Hirota"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "6",
      "title": "Affect prediction from physiological measures via visual stimuli",
      "authors": [
        "F Zhou",
        "X Qu",
        "M Helander",
        "J Jiao"
      ],
      "year": "2011",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "7",
      "title": "Fine-grained facial expression analysis using dimensional emotion model",
      "authors": [
        "F Zhou",
        "S Kong",
        "C Fowlkes",
        "T Chen",
        "B Lei"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "8",
      "title": "Design and analysis of a human-machine interaction system for researching human's dynamic emotion",
      "authors": [
        "X Sun",
        "Z Pei",
        "C Zhang",
        "G Li",
        "J Tao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "9",
      "title": "Emotion prediction from physiological signals: A comparison study between visual and auditory elicitors",
      "authors": [
        "F Zhou",
        "X Qu",
        "J Jiao",
        "M Helander"
      ],
      "year": "2014",
      "venue": "Interacting with computers"
    },
    {
      "citation_id": "10",
      "title": "Driver fatigue transition prediction in highly automated driving using physiological features",
      "authors": [
        "F Zhou",
        "A Alsaid",
        "M Blommer",
        "R Curry",
        "R Swaminathan",
        "D Kochhar",
        "W Talamonti",
        "L Tijerina",
        "B Lei"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "11",
      "title": "Ecological momentary assessment (ema) in behavorial medicine",
      "authors": [
        "A Stone",
        "S Shiffman"
      ],
      "year": "1994",
      "venue": "Annals of Behavioral Medicine"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "13",
      "title": "On the classification of emotional biosignals evoked while viewing affective pictures: an integrated data-mining-based approach for healthcare applications",
      "authors": [
        "C Frantzidis",
        "C Bratsas",
        "M Klados",
        "E Konstantinidis",
        "C Lithari",
        "A Vivas",
        "C Papadelis",
        "E Kaldoudi",
        "C Pappas",
        "P Bamidis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Information Technology in Biomedicine"
    },
    {
      "citation_id": "14",
      "title": "Categorical imperative not: facial affect is perceived continuously",
      "authors": [
        "D Schiano",
        "S Ehrlich",
        "K Sheridan"
      ],
      "year": "2004",
      "venue": "Proceedings of the SIGCHI conference on Human factors in computing systems"
    },
    {
      "citation_id": "15",
      "title": "Affective and cognitive design for mass personalization: status and prospect",
      "authors": [
        "F Zhou",
        "Y Ji",
        "R Jiao"
      ],
      "year": "2013",
      "venue": "Journal of Intelligent Manufacturing"
    },
    {
      "citation_id": "16",
      "title": "Emotional design",
      "year": "2020",
      "venue": "Emotional design",
      "arxiv": "arXiv:2010.03046"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "J Kim",
        "E André"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "18",
      "title": "Transsituational individual-specific biopsychological classification of emotions",
      "authors": [
        "S Walter",
        "J Kim",
        "D Hrabal",
        "S Crawcour",
        "H Kessler",
        "H Traue"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems"
    },
    {
      "citation_id": "19",
      "title": "The optimal hard threshold for singular values is 4/ √ 3",
      "authors": [
        "M Gavish",
        "D Donoho"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition based on multi-variant correlation of physiological signals",
      "authors": [
        "W Wen",
        "G Liu",
        "N Cheng",
        "J Wei",
        "P Shangguan",
        "W Huang"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Multiple instance learning for emotion recognition using physiological signals",
      "authors": [
        "L Romeo",
        "A Cavallo",
        "L Pepa",
        "N Berthouze",
        "M Pontil"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Domain adaptation techniques for eeg-based emotion recognition: A comparative study on two public datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Müller-Putz"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "24",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "25",
      "title": "Personal-zscore: Eliminating individual difference for eeg-based cross-subject emotion recognition",
      "authors": [
        "H Chen",
        "S Sun",
        "J Li",
        "R Yu",
        "N Li",
        "X Li",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "27",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "28",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Entertainment modeling through physiology in physical play",
      "authors": [
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2008",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "31",
      "title": "Automatic detection of driver fatigue using driving operation information for transportation safety",
      "authors": [
        "Z Li",
        "L Chen",
        "J Peng",
        "Y Wu"
      ],
      "year": "2017",
      "venue": "Sensors"
    },
    {
      "citation_id": "32",
      "title": "Towards a rigorous science of interpretable machine learning",
      "authors": [
        "F Doshi-Velez",
        "B Kim"
      ],
      "year": "2017",
      "venue": "Towards a rigorous science of interpretable machine learning",
      "arxiv": "arXiv:1702.08608"
    },
    {
      "citation_id": "33",
      "title": "Explainable machine-learning predictions for the prevention of hypoxaemia during surgery",
      "authors": [
        "S Lundberg",
        "B Nair",
        "M Vavilala",
        "M Horibe",
        "M Eisses",
        "T Adams",
        "D Liston",
        ".-W Low",
        "S.-F Newman",
        "J Kim"
      ],
      "year": "2018",
      "venue": "Nature biomedical engineering"
    },
    {
      "citation_id": "34",
      "title": "An interpretable mortality prediction model for covid-19 patients",
      "authors": [
        "L Yan",
        "H.-T Zhang",
        "J Goncalves",
        "Y Xiao",
        "M Wang",
        "Y Guo",
        "C Sun",
        "X Tang",
        "L Jing",
        "M Zhang"
      ],
      "year": "2020",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "35",
      "title": "Modeling dispositional and initial learned trust in automated vehicles with predictability and explainability",
      "authors": [
        "J Ayoub",
        "X Yang",
        "F Zhou"
      ],
      "year": "2021",
      "venue": "Transportation Research Part F: Traffic Psychology and Behaviour"
    },
    {
      "citation_id": "36",
      "title": "Deep-learned faces of pain and emotions: Elucidating the differences of facial expressions with the help of explainable ai methods",
      "authors": [
        "K Weitz",
        "T Hassan",
        "U Schmid",
        "J.-U Garbas"
      ],
      "year": "2019",
      "venue": "tm-Technisches Messen"
    },
    {
      "citation_id": "37",
      "title": "Singular spectrum analysis: a new tool in time series analysis",
      "authors": [
        "J Elsner",
        "A Tsonis"
      ],
      "year": "2013",
      "venue": "Singular spectrum analysis: a new tool in time series analysis"
    },
    {
      "citation_id": "38",
      "title": "Entropy-based pattern learning based on singular spectrum analysis components for assessment of physiological signals",
      "authors": [
        "Y Lu",
        "M Wang",
        "W Wu",
        "Q Zhang",
        "Y Han",
        "T Kausar",
        "S Chen",
        "M Liu",
        "B Wang"
      ],
      "year": "2020",
      "venue": "Complexity"
    },
    {
      "citation_id": "39",
      "title": "Classification of ecg heartbeats using nonlinear decomposition methods and support vector machine",
      "authors": [
        "K Rajesh",
        "R Dhuli"
      ],
      "year": "2017",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "40",
      "title": "Dynamic entropy-based pattern learning to identify emotions from eeg signals across individuals",
      "authors": [
        "Y Lu",
        "M Wang",
        "W Wu",
        "Y Han",
        "Q Zhang",
        "S Chen"
      ],
      "year": "2020",
      "venue": "Measurement"
    },
    {
      "citation_id": "41",
      "title": "Epileptic seizure detection using dwt based fuzzy approximate entropy and support vector machine",
      "authors": [
        "Y Kumar",
        "M Dewal",
        "R Anand"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "42",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective computing"
    },
    {
      "citation_id": "43",
      "title": "Xgboost: A scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "44",
      "title": "Lightgbm: A highly efficient gradient boosting decision tree",
      "authors": [
        "G Ke",
        "Q Meng",
        "T Finley",
        "T Wang",
        "W Chen",
        "W Ma",
        "Q Ye",
        "T.-Y Liu"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "45",
      "title": "From local explanations to global understanding with explainable ai for trees",
      "authors": [
        "S Lundberg",
        "G Erion",
        "H Chen",
        "A Degrave",
        "J Prutkin",
        "B Nair",
        "R Katz",
        "J Himmelfarb",
        "N Bansal",
        "S.-I Lee"
      ],
      "year": "2020",
      "venue": "Nature machine intelligence"
    },
    {
      "citation_id": "46",
      "title": "Ensemble machine learning-based affective computing for emotion recognition using dual-decomposed eeg signals",
      "authors": [
        "K Kamble",
        "J Sengupta"
      ],
      "year": "2021",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "47",
      "title": "Predicting driver fatigue in monotonous automated driving with explanation using gpboost and shap",
      "authors": [
        "F Zhou",
        "A Alsaid",
        "M Blommer",
        "R Curry",
        "R Swaminathan",
        "D Kochhar",
        "W Talamonti",
        "L Tijerina"
      ],
      "year": "2022",
      "venue": "International Journal of Human-Computer Interaction"
    },
    {
      "citation_id": "48",
      "title": "Using eye-tracking data to predict situation awareness in real time during takeover transitions in conditionally automated driving",
      "authors": [
        "F Zhou",
        "X Yang",
        "J Winter"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Intelligent Transportation Systems"
    },
    {
      "citation_id": "49",
      "title": "Modeling dispositional and initial learned trust in automated vehicles with predictability and explainability",
      "authors": [
        "J Ayoub",
        "X Yang",
        "F Zhou"
      ],
      "year": "2021",
      "venue": "Transportation research part F: traffic psychology and behaviour"
    },
    {
      "citation_id": "50",
      "title": "Contributions to the theory of games",
      "authors": [
        "L Shapley",
        "H Kuhn",
        "A Tucker"
      ],
      "year": "1953",
      "venue": "Annals of mathematics studies"
    },
    {
      "citation_id": "51",
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "52",
      "title": "Recognition of emotions using multimodal physiological signals and an ensemble deep learning model",
      "authors": [
        "Z Yin",
        "M Zhao",
        "Y Wang",
        "J Yang",
        "J Zhang"
      ],
      "year": "2017",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "53",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2001",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "54",
      "title": "Physiology-based affect recognition for computer-assisted intervention of children with autism spectrum disorder",
      "authors": [
        "C Liu",
        "K Conn",
        "N Sarkar",
        "W Stone"
      ],
      "year": "2008",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "55",
      "title": "Deep convolutional neural network for emotion recognition using eeg and peripheral physiological signal",
      "authors": [
        "W Lin",
        "C Li",
        "S Sun"
      ],
      "year": "2017",
      "venue": "International Conference on Image and Graphics"
    },
    {
      "citation_id": "56",
      "title": "Emotion recognition from physiological signals using wireless sensors for presence technologies",
      "authors": [
        "F Nasoz",
        "K Alvarez",
        "C Lisetti",
        "N Finkelstein"
      ],
      "year": "2004",
      "venue": "Cognition, Technology & Work"
    },
    {
      "citation_id": "57",
      "title": "Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data",
      "authors": [
        "F Ringeval",
        "F Eyben",
        "E Kroupi",
        "A Yuce",
        "J.-P Thiran",
        "T Ebrahimi",
        "D Lalanne",
        "B Schuller"
      ],
      "year": "2015",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "58",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "59",
      "title": "Analysis of physiological for emotion recognition with the irs model",
      "authors": [
        "C Li",
        "C Xu",
        "Z Feng"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "60",
      "title": "Human emotion recognition using deep belief network architecture",
      "authors": [
        "M Hassan",
        "M Alam",
        "M Uddin",
        "S Huda",
        "A Almogren",
        "G Fortino"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "61",
      "title": "Subject cross validation in human activity recognition",
      "authors": [
        "A Dehghani",
        "T Glatard",
        "E Shihab"
      ],
      "year": "2019",
      "venue": "Subject cross validation in human activity recognition",
      "arxiv": "arXiv:1904.02666"
    },
    {
      "citation_id": "62",
      "title": "Cross-validation approaches for replicability in psychology",
      "authors": [
        "A Koul",
        "C Becchio",
        "A Cavallo"
      ],
      "year": "2018",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "63",
      "title": "The need to approximate the use-case in clinical machine learning",
      "authors": [
        "S Saeb",
        "L Lonini",
        "A Jayaraman",
        "D Mohr",
        "K Kording"
      ],
      "year": "2017",
      "venue": "Gigascience"
    },
    {
      "citation_id": "64",
      "title": "Multitask and multi-view learning of user state",
      "authors": [
        "M Kandemir",
        "A Vetek",
        "M Goenen",
        "A Klami",
        "S Kaski"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "65",
      "title": "Emotion recognition with facial expressions and physiological signals",
      "authors": [
        "B Zhong",
        "Z Qin",
        "S Yang",
        "J Chen",
        "N Mudrick",
        "M Taub",
        "R Azevedo",
        "E Lobaton"
      ],
      "year": "2017",
      "venue": "2017 IEEE Symposium Series on Computational Intelligence (SSCI)"
    },
    {
      "citation_id": "66",
      "title": "Cross-subject emotion recognition using flexible analytic wavelet transform from eeg signals",
      "authors": [
        "V Gupta",
        "M Chopda",
        "R Pachori"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "67",
      "title": "Combat covid-19 infodemic using explainable natural language processing models",
      "authors": [
        "J Ayoub",
        "X Yang",
        "F Zhou"
      ],
      "year": "2021",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "68",
      "title": "Definitions, methods, and applications in interpretable machine learning",
      "authors": [
        "W Murdoch",
        "C Singh",
        "K Kumbier",
        "R Abbasi-Asl",
        "B Yu"
      ],
      "year": "2019",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "69",
      "title": "A mutual information based adaptive windowing of informative eeg for emotion recognition",
      "authors": [
        "L Piho",
        "T Tjahjadi"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "70",
      "title": "Decomposition of skin conductance data by means of nonnegative deconvolution",
      "authors": [
        "M Benedek",
        "C Kaernbach"
      ],
      "year": "2010",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "71",
      "title": "Approximate entropy as a measure of system complexity",
      "authors": [
        "S Pincus"
      ],
      "year": "1991",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "72",
      "title": "Physiological time-series analysis using approximate entropy and sample entropy",
      "authors": [
        "J Richman",
        "J Moorman"
      ],
      "year": "2000",
      "venue": "American Journal of Physiology-Heart and Circulatory Physiology"
    },
    {
      "citation_id": "73",
      "title": "Characterization of surface emg signal based on fuzzy entropy",
      "authors": [
        "W Chen",
        "Z Wang",
        "H Xie",
        "W Yu"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on neural systems and rehabilitation engineering"
    },
    {
      "citation_id": "74",
      "title": "Greedy function approximation: a gradient boosting machine",
      "authors": [
        "J Friedman"
      ],
      "year": "2001",
      "venue": "Annals of statistics"
    },
    {
      "citation_id": "75",
      "title": "Multimodal emotion recognition using deep neural networks",
      "authors": [
        "H Tang",
        "W Liu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "76",
      "title": "Application of singular spectrum analysis to the smoothing of raw kinematic signals",
      "authors": [
        "F Alonso",
        "J Del",
        "P Castillo",
        "Pintado"
      ],
      "year": "2005",
      "venue": "Journal of biomechanics"
    },
    {
      "citation_id": "77",
      "title": "Approximate entropy and sample entropy: A comprehensive tutorial",
      "authors": [
        "A Delgado-Bonal",
        "A Marshak"
      ],
      "year": "2019",
      "venue": "Entropy"
    },
    {
      "citation_id": "78",
      "title": "Chapter 2 -biomedical signals",
      "authors": [
        "A Subasi"
      ],
      "year": "2019",
      "venue": "Practical Guide for Biomedical Signals Analysis Using Machine Learning Techniques"
    },
    {
      "citation_id": "79",
      "title": "Intelligent pervasive healthcare systems",
      "authors": [
        "C Doukas",
        "I Maglogiannis"
      ],
      "year": "2008",
      "venue": "Advanced Computational Intelligence Paradigms in Healthcare-3"
    },
    {
      "citation_id": "80",
      "title": "His main research interests include human factors, human-computer interaction, engineering design, and human-centered design",
      "authors": [
        "Dr"
      ],
      "year": "2013",
      "venue": "Singapore, in 2011 and Ph.D. degree in Mechanical Engineering from Gatech in 2014. He was a Research Scientist at Medi-aScience, Austin TX"
    }
  ]
}