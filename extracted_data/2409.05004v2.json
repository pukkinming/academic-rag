{
  "paper_id": "2409.05004v2",
  "title": "Disentangling The Prosody And Semantic Information With Pre-Trained Model For In-Context Learning Based Zero-Shot Voice Conversion",
  "published": "2024-09-08T07:24:03Z",
  "authors": [
    "Zhengyang Chen",
    "Shuai Wang",
    "Mingyang Zhang",
    "Xuechen Liu",
    "Junichi Yamagishi",
    "Yanmin Qian"
  ],
  "keywords": [
    "Voice conversion",
    "in-context learning",
    "prosody preservation",
    "LibriTTS",
    "Emotion Speech Database"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Human speech contains various aspects of information, such as content, speaker timbre, and speaking style. Voice conversion (VC) allows the alteration of one's voice to mimic another's characteristics without changing the spoken content and speaking style  [1, 2] . This technology finds diverse applications across various sectors, such as dubbing for movies, timbre transformation in online live streaming, and identity anonymization.\n\nTo achieve effective voice conversion, it is crucial to separate the content information from the source speech. Pre-trained automatic speech recognition (ASR) models  [3]  and self-supervised pretrained models  [4, 5, 6, 7]  are typically utilized to extract linguistic content. To minimize the non-content information, outputs from self-supervised pre-trained models are often tokenized to generate semantic tokens  [8, 9, 10, 11, 12, 13] . However, tokenization methods are usually tied to the training objectives of the self-supervised † : Yanmin Qian is corresponding author models, resulting in differing token formats across different pretrained models. This necessitates the design of distinct modules to accommodate various semantic token formats. Interestingly, despite the differing training objectives of these models, we discovered that the k-means method can serve as a universal tokenization strategy. This approach standardizes the semantic token format across different pre-trained models.\n\nVoice conversion technology has significantly evolved over the past few decades, advancing from traditional methods to cuttingedge zero-shot voice conversion. Zero-shot voice conversion represents a paradigm shift, allowing models to convert a source voice to a target voice they have never encountered during training. By leveraging advancements in speaker embedding techniques, zeroshot voice conversion systems can generalize to new, unseen speakers without requiring fine-tuning with additional data  [14] .\n\nHowever, utilizing speaker embeddings to represent timbre information in zero-shot voice conversion demands a robust pretrained speaker encoder  [15] . Additionally, specific modules must be designed to incorporate the information from speaker embeddings into the VC system. Both text-to-speech (TTS) and voice conversion training typically require high-fidelity audio data  [16, 17] , which limits the scalability of the system to large-scale web data. Recently, researchers have proposed leveraging in-context learning (ICL) capability  [18, 19, 20, 21]  to enable TTS systems to synthesize speech for unseen speakers through a target speech prompting strategy. This approach eliminates the need for a pre-trained speaker encoder and has no stringent audio quality requirements.\n\nIn this paper, we equip the voice conversion system with the ICL capability to propose ICL-VC. Initially, we disentangle the content information from speech by extracting semantic tokens using a selfsupervised pre-trained model. Next, we train the model through a mel-spectrogram mask and reconstruction strategy based on flowmatching generative model to equip the model with the ICL capability. During inference, we perform voice conversion by prompting the system with reference speaker speech. This strategy allows the system to achieve zero-shot voice conversion with high speaker similarity. However, leveraging ICL requires the VC system to extract both timbre and prosody information from the reference speech, which can compromise the preservation of the source speech's prosody. To address this issue, a straightforward solution is to incorporate prosody information, such as pitch and energy, into the VC system. However, we found that even normalized pitch and energy can lead to timbre information leakage and limit the system's ability to han- dle prosody-unmatched scenarios. To enhance the prosody preservation capability of the ICL-VC system, we propose disentangling prosody information using the Emotion2Vec  [22]  pre-trained emotion recognition model, which significantly improves system performance. Besides, we also provide some samples at https:// czy97.github.io/ICL-VC/.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "In-Context Learning Based Voice Conversion",
      "text": "An overview of our system is shown in Figure  1 . For the voice conversion task, it is crucial to preserve the content of the source speech. Given an audio segment x = [x1, x2, . . . , xT ] ∈ R T , we first disentangle the content information by extracting the semantic embedding sequence S = [s1, s2, . . . , s T ′ ] ∈ R T ′ ×d , as described in Section 2.2. Additionally, we extract the mel-spectrogram M = [m1, m2, . . . , m T ′ ] ∈ R T ′ ×f from the audio segment x.\n\nDuring training, we employ a mask and reconstruction paradigm. As illustrated in Figure  1 , we mask part of the mel-spectrogram M with random noise to obtain M . The semantic embedding sequence S is then concatenated with the masked mel-spectrogram M to form the model input (S; M ) ∈ R T ′ ×(d+f ) . A generative model G is used to reconstruct the original mel-spectrogram: M ≈ M = G(S; M ). In this learning paradigm, the generative model must learn to extract content information from the semantic embedding and contextual information from the unmasked parts of the mel-spectrogram to reconstruct the masked sections, demonstrating in-context learning (ICL) capabilities.\n\nIn the inference stage, we denote the semantic embeddings extracted from the reference and source speech as Sr ∈ R T ′ r ×d and Ss ∈ R T ′ s ×d , respectively, and concatenate them along the time dimension to form Sinfer ∈ R (T ′ r +T ′ s )×d . The mel-spectrogram extracted from the reference speech is denoted as Mr ∈ R T ′ r ×f , and we mask all parts of the mel-spectrogram extracted from the source speech with random noise to obtain Ms ∈ R T ′ s ×f . The model G then generates the new mel-spectrogram Ms for the corresponding source speech input: [ Mr, Ms] = G(S inf er ; [Mr, Ms]). When generating the mel-spectrogram Ms, the model conditions on the semantic embedding Ss from the source speech and extracts speaker timbre information from the reference mel-spectrogram Mr using its ICL capability. This approach ensures that the generated melspectrogram contains the content of the source speech but the timbre of the reference speech. Thus, voice conversion is achieved, and we name this strategy in-context learning based voice conversion (ICL-VC).\n\nIn some application scenarios, such as voice conversion for video, it is crucial to preserve the prosody information to match the human lip movements and facial expressions. However, using the ICL capability described above, the system infers the prosody information for the generated mel-spectrogram from the reference speech's mel-spectrogram. This may result in generated speech with prosody different from the source speech, which is also verified by our experiment results in Table  2 . To maintain the prosody of the source speech, we propose adding the prosody embedding P = [p1, p2, . . . , p T ′ ] ∈ R T ′ ×h , as introduced in Section 2.3, to the system's input. This enables the system to directly extract prosody information from the source speech's prosody embedding during inference to generate the mel-spectrogram.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Semantic Information Extraction",
      "text": "To extract the semantic embedding, we utilize self-supervised learning-based pre-trained models such as HuBERT  [23]  and Wav2Vec  [24] . To remove as much non-content information as possible, we tokenize the output from the pre-trained model to extract semantic tokens and then use a learnable embedding module to map the tokens to embeddings. It should be noted that prior research  [8, 9, 10, 11, 12]  has employed different pre-trained models for various application scenarios. HuBERT is frequently used due to its straightforward k-means tokenization method. Additionally, researchers prefer Wav2Vec for multilingual scenarios  [12]  because of the availability of the Wav2Vec multilingual pre-trained model 12 . However, the original tokenization strategy in Wav2Vec generates two semantic token sequences  [24, 12] , which is more complex and differs from the single semantic token sequence produced by Hu-BERT. This difference in semantic tokens from various pre-trained models complicates the adaptation of a single system to different semantic tokens without modifying the model architecture. The reason researchers choose different tokenization methods for different pre-trained models is to match the pre-trained models' training objectives  [23, 24] . Interestingly, we found that k-means is a versatile tokenization method applicable to different self-supervised pre-trained models regardless of their training objectives. In our experiments, we propose leveraging the k-means method for semantic tokenization across various pre-trained models.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Prosody Information Extraction",
      "text": "As introduced in Section 2.1, for certain application scenarios, we need to provide the system with additional prosody embedding to maintain the source speech's prosody. Prosody in speech is primarily reflected in the pitch, energy, and duration of each phoneme's articulation  [25] . In our ICL-VC framework, we can directly ensure that the duration of the generated speech matches the source speech. Therefore, the aspects of prosody that need to be considered are mainly pitch and energy. Previous work  [26, 25, 27, 28]  directly leveraged normalized pitch (F0) or energy information to guide speech synthesis. However, in our experiments, we found that even when F0 and energy values are normalized using mean and standard deviation, they may still cause speaker information leakage and do not generalize well to prosody-unmatched voice conversion tasks.\n\nTo mitigate this problem, we propose extracting prosody embeddings from the pre-trained Emotion2Vec  [22]  model. The Emo-tion2Vec model is trained to capture emotion information in speech, which we believe is primarily reflected in speech prosody. Thus, we hypothesize that the representations extracted from the Emotion2Vec model can serve as prosody embeddings in our system. Additionally, to avoid speaker timbre information leakage from the prosody embedding, we first perturb the pitch  [29]  of the speech before feeding it into the Emotion2Vec model using the sox 3 toolkit to raise or lower the pitch by 200 or 400 cents. Then, we use the final output from the Emotion2Vec model as the prosody embedding.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Flow-Matching Based Generative Model",
      "text": "To balance the quality and speed of mel-spectrogram generation, we employ the flow-matching-based  [30]  generation method in our study. This method aims to fit an unknown distribution q(x1), where x1 ∈ R d , by constructing a continuous flow ϕt : R d → R d , t ∈ [0, 1] that transforms a simple known distribution p0, such as a Gaussian distribution, into the target distribution p1 ≈ q. The flow is governed by an ordinary differential equation (ODE):\n\nTo construct the flow model, we can approximate the vector field ut ∈ R d within the ODE equation. However, a closed-form formulation for ut does not exist. Previous work  [30]  has shown that approximating a conditional vector field ut(x|x1) is equivalent to approximating the original ut, leading to the development of a conditional flow matching (CFM) training objective:\n\nwhere pt(x|x1) denotes the conditional probability density function at time t, and vt(x, θ) is a time-dependent neural network used to 3 https://linux.die.net/man/1/sox approximate ut(x|x1). In the training process, t is randomly sampled from [0, 1] and we encode it as a sinusoidal positional embedding, which is concatenated with the model input. Additionally, we adopt the optimal transport (OT) path introduced in  [30]  to define the flow, where pt(x|x1\n\nHere, σmin is a small scalar marginally above zero and is set to 10 -5 in our experiment.\n\nTo implement the flow-matching-based generative model in our system, we extend vt to condition on the mel-spectrogram generated at each timestamp Mt, semantic embedding S, and optionally the prosody embedding P , resulting in vt(Mt, S, P ; θ). During training, we only focuses on optimizing the masked part:\n\nwhere m is a mask function that will mask the unmasked part of input mel-spectrogram M .\n\nIn the inference stage, we integrate the following ODE equation 4 from t = 0 to t = 1 to obtain the predicted mel-spectrograms Mr and Ms, with only Ms retained for the voice conversion task:\n\nTo achieve a balance between generative fidelity and time consumption, we configured the ODE step to 32 in our experiment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "In our experiment, we utilized two datasets: the LibriTTS  [16]  dataset and the Emotional Speech Database (ESD)  [31] . The trainclean-360 subset of the LibriTTS dataset was used for system training, while the test-clean subset was employed to evaluate the basic zero-shot voice conversion ability of our ICL-VC system. The ESD dataset, which contains speech with rich prosody variations, was exclusively used to assess the source speech prosody-preserving ability. Additionally, we resampled the ESD dataset to 24 kHz to match the sample rate of the LibriTTS dataset.\n\nFor evaluating the basic voice conversion ability, we randomly selected 20 speakers from the test-clean subset of the LibriTTS dataset. Ten speakers were designated as reference speakers and another ten as source speakers. For each reference speaker, we randomly selected one utterance, and for each source speaker, we randomly selected two utterances. We then converted the timbre of each source speaker's utterance to match all the reference speakers, resulting in a total of 200 generated utterances for testing.\n\nTo evaluate the prosody-preserving ability of the VC systems, we randomly selected 16 utterances from the English portion of the ESD dataset as the source speech. The ten reference utterances selected in the previous evaluation were also used in this assessment.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model Configuration",
      "text": "We implemented our generative model as an encoder-only Transformer model  [32] . The Transformer consists of 8 layers, with an input dimension set to 768. A linear layer is inserted before the Transformer to adjust the input feature dimension to match the Transformer's input dimension. Additionally, another linear layer maps the Transformer's output to the dimension of the melspectrogram. We extract the mel-spectrogram using the Vocos  [33]  toolkit and employ its pre-trained mel-based  4  vocoder to convert the mel-spectrogram into raw audio in the inference stage.\n\nTo extract prosody information, we use the Pyworld 5  toolkit to extract the pitch (F0) from the audio. We normalize the pitch and energy values for each utterance to mitigate speaker information leakage. The pitch and energy values are then tokenized into 256 bins, producing pitch and energy tokens. A learnable embedding module maps these tokens into embeddings. When using the Emo-tion2Vec  [22]  model to extract prosody information, as introduced in Section 2.3, we first perturb the pitch of the input speech and use the final output of the Emotion2Vec model as the prosody embedding.\n\nIn our experiment, we utilize three pre-trained models to extract semantic tokens following the fairseq 6  recipe: the HuBERT base model trained on LibriSpeech-960, the Wav2Vec base model trained on LibriSpeech-960, and the Wav2Vec-XLSR model trained on 56k hours of multilingual data. For each pre-trained model, we train a kmeans tokenizer with 500 clusters separately. Previous research  [34, 35]  has demonstrated that different layers of self-supervised pretrained models contain varying types of information. Based on this analysis, we select outputs from the 9th layer of both the HuBERT base and Wav2Vec base models, and the 14th layer of the Wav2Vec-XLSR model, to train the k-means tokenizer. This layer selection aims to retain the most semantic information while filtering out nonsemantic information.\n\nDuring the training process of our ICL-VC system, we randomly select an unmasked region of each input utterance, with a duration ranging from 2 to 3 seconds. The left part of this region is then masked with random noise. The system is trained to reconstruct the masked portion by conditioning on the input semantic embedding (or with addition prosody embedding) and the unmasked region.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Baseline System",
      "text": "For system comparison, we introduce two baseline systems in our experiment. The first one is the YourTTS system  [36] , where we use the ResNet34-based pre-trained speaker encoder provided by Wespeaker toolkit  [37] . The other one is RefXVC  [38] , a voice conversion method leveraging self-supervised learning features and enhanced reference-based speaker embedding to improve speaker similarity. All the systems in our experiment are trained on the same training set for fair comparison.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation Metric",
      "text": "In our experiments, we conduct both objective and subjective evaluations. For the objective evaluation, we compute the speaker embedding cosine similarity (SECS) between the converted speech and the reference speech. We use the ECAPA-TDNN  [39]  pre-trained model from the Wespeaker toolkit  [37]  to extract the speaker embeddings. Additionally, following  [40] , we utilize a ASR pre-trained model provided by the NEMO toolkit  7  to evaluate the character error rate (CER) of the ground-truth and converted speech, which reflects the intelligibility of the speech. Furthermore, we calculate the Pearson correlation between the pitch and energy sequences  [41]  extracted from the source speech and the converted speech to determine if the converted speech maintains the prosody of the source speech. For the subjective evaluation, we ask 15 human raters to provide a mean opinion score (MOS) ranging from 1 to 5 regarding the naturalness of the speech and the prosody similarity between the converted and source speech. These 15 human raters simultaneously participated in scoring both naturalness MOS and prosody MOS. When conducting a prosody similarity subjective evaluation, the instruction we provided is: \"Please assess the similarity in speaking prosody between the provided speech and the reference speech. Give a rating from 1 to 5, with 5 indicating completely consistent prosody and 1 indicating completely different prosody.\" On average, each audio sample receives 3 MOS scores for naturalness evaluation and 4 MOS scores for prosody similarity evaluation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Zero-Shot Voice Conversion Evaluation",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Between Different Systems",
      "text": "In this section, we evaluate our proposed ICL voice conversion strategy on the LibriTTS test set, with the results summarized in Table  1 .\n\nOur findings indicate that all ICL based VC systems utilizing different semantic codecs demonstrate superior performance in replicating the target speaker's timbre and generating a more natural voice. As introduced in Section 2.2, unlike other works  [12]  that use the original tokenization strategy in Wav2vec, we employ a simple kmeans tokenization strategy for Wav2vec. The results reveal that the k-means tokenization strategy is effective for the Wav2Vec framework, despite Wav2Vec not being trained with labels clustered by the k-means method. This phenomenon suggests that the k-means tokenization method could be a general strategy for semantic tokenization.\n\nInterestingly, the ICL-VC-Wav2Vec-XLSR system exhibits better cosine speaker similarity compared to ICL-VC-HuBERT and ICL-VC-Wav2Vec. This may be attributed to the Wav2Vec-XLSR model being trained on a larger amount of speech data and exposed to more speakers, thus enhancing its zero-shot voice conversion capability. However, the ICL-VC-Wav2Vec-XLSR system shows a higher Character Error Rate (CER) than the other two systems. This discrepancy is likely because the HuBERT and Wav2Vec models are directly trained on the LibriSpeech dataset, which has a high content correlation with the LibriTTS test set. In this section, we evaluate the impact of reference duration on speaker embedding cosine similarity, as illustrated in Figure  2 . For both YourTTS and our method, the SECS variation trend indicates that longer reference speech allows the system to better model the speaker's timbre information. Notably, our ICL-VC system achieves high cosine speaker similarity even with a very short (3-second) reference utterance.\n\nFig.  2 . The relationship between the speaker embedding cosine similarity (SECS) and reference speech duration.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Prosody Preserving Ability Evaluation",
      "text": "In addition to mimicking the target speaker's timbre, the voice conversion task always requires preserving the source speech's content and prosody  [1] . To evaluate the system's ability to maintain prosody, we directly evaluate the system trained on LibriTTS on the ESD dataset, which features speech with significant prosody variations. Beyond the evaluation metrics listed in Table  1 , we incorporate pitch and energy correlation metrics introduced in section 3.4 to assess the prosody similarity between the converted and source speech. Additionally, we include prosody MOS scores to capture subjective human judgments of prosody similarity. The corresponding results are shown in Table  2 . It is noteworthy that all SECS values in Table  2  are lower than those in Table  1 , due to the prosody mismatch between LibriTTS and ESD speech. Nevertheless, these SECS values remain in the high range, indicating that all systems are capable of performing voice conversion on the ESD dataset. Despite the superior timbre replication capability of the ICLbased VC system, the results in Table  2  indicate that it struggles to maintain prosody information. The ICL-VC system exhibits lower prosody MOS scores compared to the two baseline systems, and lower pitch and energy correlation than the YourTTS system. This shortcoming arises because, during inference, our ICL-VC system relies on the reference speech to extract both timbre and prosody information, while the source speech provides only semantic information. To enhance our ICL-VC system's ability to retain prosody, we employ two strategies introduced in Section 2.3. First, we integrate normalized pitch and energy information into the input of our ICL-VC system. This approach significantly improves prosody correlation and MOS scores, although it leads to a notable decline in speaker similarity and naturalness MOS scores. While normalized pitch reduces timbre leakage from the source speech to the converted speech, the ESD dataset's prosody variations differ greatly from those in the LibriTTS dataset. This discrepancy can cause mismatches during inference, impairing the model's ability to generate high-quality speech and resulting in lower SECS and naturalness MOS scores. Interestingly, our proposed strategy using prosody information extracted from the pre-trained Emotion2Vec model helps mitigate this issue, yielding higher SECS and naturalness MOS scores.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced the in-context learning (ICL) method to the voice conversion task. Our experimental results demonstrate that the ICL method enables the system to perform voice conversion without relying on an external speaker encoder, resulting in higher speaker similarity. However, we observed that simply applying the ICL method does not ensure the preservation of the source speech's prosody. To address this issue, we proposed extracting prosody embeddings from a pre-trained emotion recognition model, achieving impressive prosody-preserving performance on the ESD dataset. Additionally, we found that the k-means tokenization method is a versatile technique that can be applied to various self-supervised pre-trained models, regardless of their different training objectives. Despite these advancements, our method has been validated only on small datasets so far. The ICL method, combined with the information decoupling approach based on self-supervised learning pre-trained models, has the potential to train the system on larger, noisier, stylistically diverse, and even multilingual datasets. This will be the focus of our future work.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: System Overview.",
      "page": 2
    },
    {
      "caption": "Figure 1: For the voice",
      "page": 2
    },
    {
      "caption": "Figure 1: , we mask part of the mel-spectrogram M with",
      "page": 2
    },
    {
      "caption": "Figure 2: For both YourTTS and our method, the SECS variation trend",
      "page": 5
    },
    {
      "caption": "Figure 2: The relationship between the speaker embedding cosine sim-",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "4National Institute of Informatics, Tokyo, Japan"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "models,\nresulting in differing token formats across different pre-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "trained models. This necessitates the design of distinct modules to"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "accommodate various semantic token formats. Interestingly, despite"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "the differing training objectives of these models, we discovered that"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "the k-means method can serve as a universal\ntokenization strategy."
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "This approach standardizes the semantic token format across differ-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "ent pre-trained models."
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "Voice conversion technology has significantly evolved over the"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "past\nfew decades, advancing from traditional methods\nto cutting-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "edge zero-shot voice conversion.\nZero-shot voice conversion rep-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "resents a paradigm shift, allowing models to convert a source voice"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "to a target voice they have never encountered during training. By"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "leveraging advancements\nin speaker embedding techniques,\nzero-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "shot voice conversion systems can generalize to new, unseen speak-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "ers without requiring fine-tuning with additional data [14]."
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "However,\nutilizing\nspeaker\nembeddings\nto\nrepresent\ntimbre"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "information in zero-shot voice conversion demands a robust pre-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "trained speaker encoder\n[15]. Additionally, specific modules must"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "be designed to incorporate the information from speaker embeddings"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "into the VC system. Both text-to-speech (TTS) and voice conver-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "sion\ntraining\ntypically\nrequire\nhigh-fidelity\naudio\ndata\n[16,\n17],"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "which limits the scalability of\nthe system to large-scale web data."
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "Recently,\nresearchers have proposed leveraging in-context\nlearning"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "(ICL) capability\n[18, 19, 20, 21] to enable TTS systems to synthe-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "size speech for unseen speakers through a target speech prompting"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "strategy. This approach eliminates the need for a pre-trained speaker"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "encoder and has no stringent audio quality requirements."
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": ""
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "In this paper, we equip the voice conversion system with the ICL"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "capability to propose ICL-VC. Initially, we disentangle the content"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "information from speech by extracting semantic tokens using a self-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "supervised pre-trained model. Next, we train the model\nthrough a"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "mel-spectrogram mask and reconstruction strategy based on flow-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "matching generative model to equip the model with the ICL capabil-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "ity. During inference, we perform voice conversion by prompting the"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "system with reference speaker speech. This strategy allows the sys-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "tem to achieve zero-shot voice conversion with high speaker similar-"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "ity. However, leveraging ICL requires the VC system to extract both"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "timbre and prosody information from the reference speech, which"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "can compromise the preservation of\nthe source speech’s prosody."
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "To address\nthis\nissue, a straightforward solution is\nto incorporate"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "prosody information, such as pitch and energy,\ninto the VC system."
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "However, we found that even normalized pitch and energy can lead"
        },
        {
          "3Chinese University of Hong Kong, Shenzhen, China": "to timbre information leakage and limit\nthe system’s ability to han-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. System Overview.": "dle prosody-unmatched scenarios. To enhance the prosody preser-"
        },
        {
          "Fig. 1. System Overview.": "vation capability of the ICL-VC system, we propose disentangling"
        },
        {
          "Fig. 1. System Overview.": "prosody information using the Emotion2Vec [22] pre-trained emo-"
        },
        {
          "Fig. 1. System Overview.": "tion recognition model, which significantly improves\nsystem per-"
        },
        {
          "Fig. 1. System Overview.": "formance. Besides, we also provide some samples at https://"
        },
        {
          "Fig. 1. System Overview.": "czy97.github.io/ICL-VC/."
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "2. METHOD"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "2.1.\nIn-context Learning based Voice Conversion"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "An overview of our\nsystem is\nshown in Figure 1.\nFor\nthe voice"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "conversion task,\nit\nis crucial\nto preserve the content of\nthe source"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "speech. Given an audio segment x = [x1, x2, . . . , xT ] ∈ RT , we"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "first disentangle the content\ninformation by extracting the seman-"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "tic embedding sequence S = [s1, s2, . . . , sT ′ ] ∈ RT ′×d, as de-"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "scribed in Section 2.2. Additionally, we extract the mel-spectrogram"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "from the audio segment x.\nM = [m1, m2, . . . , mT ′ ] ∈ RT ′×f"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "During training, we employ a mask and reconstruction paradigm. As"
        },
        {
          "Fig. 1. System Overview.": "illustrated in Figure 1, we mask part of the mel-spectrogram M with"
        },
        {
          "Fig. 1. System Overview.": "˜"
        },
        {
          "Fig. 1. System Overview.": "random noise to obtain\nM . The semantic embedding sequence S is"
        },
        {
          "Fig. 1. System Overview.": "then concatenated with the masked mel-spectrogram ˜M to form the"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "model\ninput (S;\nM ) ∈ RT ′×(d+f ). A generative model G is used"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "to reconstruct the original mel-spectrogram: M ≈ ˆM = G(S;\nM )."
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "In this learning paradigm, the generative model must learn to extract"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "content information from the semantic embedding and contextual in-"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "formation from the unmasked parts of the mel-spectrogram to recon-"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "struct the masked sections, demonstrating in-context learning (ICL)"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "capabilities."
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "In the inference stage, we denote the semantic embeddings ex-"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "r ×d and\ntracted from the reference and source speech as Sr ∈ RT ′"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "s×d, respectively, and concatenate them along the time di-\nSs ∈ RT ′"
        },
        {
          "Fig. 1. System Overview.": "r +T ′"
        },
        {
          "Fig. 1. System Overview.": "s)×d.\nThe mel-spectrogram ex-\nmension to form Sinfer ∈ R(T ′"
        },
        {
          "Fig. 1. System Overview.": "r ×f , and\ntracted from the reference speech is denoted as Mr ∈ RT ′"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "we mask all parts of the mel-spectrogram extracted from the source"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "speech with random noise to obtain\ns×f . The model G\nMs ∈ RT ′"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "then generates the new mel-spectrogram ˆMs for the corresponding"
        },
        {
          "Fig. 1. System Overview.": "source speech input:\n[ ˆMr,\nMs]). When\nMs] = G(Sinf er; [Mr,"
        },
        {
          "Fig. 1. System Overview.": ""
        },
        {
          "Fig. 1. System Overview.": "the model conditions on the\ngenerating the mel-spectrogram ˆMs,"
        },
        {
          "Fig. 1. System Overview.": "semantic embedding Ss from the source speech and extracts speaker"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "reason researchers choose different tokenization methods for differ-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "pled from [0, 1] and we encode it as a sinusoidal positional embed-"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "ent pre-trained models is to match the pre-trained models’ training",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ding, which is concatenated with the model input. Additionally, we"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "objectives [23, 24].\nInterestingly, we found that k-means is a ver-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "adopt\nthe optimal\ntransport\n(OT) path introduced in [30]\nto define"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "satile\ntokenization method applicable\nto different\nself-supervised",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "the flow, where pt(x|x1) = N (x|tx1, (1 − (1 − σmin)t)2I) and"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "pre-trained models regardless of their training objectives.\nIn our ex-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ut(x|x1) = (x1 − (1 − σmin)x)/(1 − (1 − σmin)t). Here, σmin"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "periments, we propose leveraging the k-means method for semantic",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "is a small scalar marginally above zero and is set\nto 10−5\nin our"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "tokenization across various pre-trained models.",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "experiment."
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "To implement the flow-matching-based generative model in our"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "system, we extend vt to condition on the mel-spectrogram generated"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "2.3. Prosody Information Extraction",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "at each timestamp Mt, semantic embedding S, and optionally the"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "As introduced in Section 2.1,\nfor certain application scenarios, we",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "prosody embedding P ,\nresulting in vt(Mt, S, P ; θ). During train-"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "need to provide the system with additional prosody embedding to",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ing, we only focuses on optimizing the masked part:"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "maintain the source speech’s prosody. Prosody in speech is primar-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "2"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "ily reflected in the pitch, energy, and duration of each phoneme’s",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\nm(Ωt)\nL(θ) = Et,q(M ),pt(Mt|M )"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "articulation [25].\nIn our\nICL-VC framework, we can directly en-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "(3)"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "sure that\nthe duration of\nthe generated speech matches the source",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "Ωt = vt(Mt, S, P ; θ) − ut(Mt|M )"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "speech.\nTherefore,\nthe aspects of prosody that need to be consid-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "where m is a mask function that will mask the unmasked part of"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "ered are mainly pitch and energy.\nPrevious work [26, 25, 27, 28]",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "input mel-spectrogram ˜M ."
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "directly leveraged normalized pitch (F0) or energy information to",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "In the inference stage, we integrate the following ODE equation"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "guide speech synthesis. However, in our experiments, we found that",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ˆ"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "4 from t = 0 to t = 1 to obtain the predicted mel-spectrograms\nMr"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "even when F0 and energy values are normalized using mean and",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ˆ\nˆ"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "and\nMs, with only\nMs retained for the voice conversion task:"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "standard deviation, they may still cause speaker information leakage",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "and do not generalize well\nto prosody-unmatched voice conversion",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "tasks.",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "d d\nϕt(Mt) = vt(Mt, Sinfer, Pinfer; θ)"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "t\n(4)"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "To mitigate this problem, we propose extracting prosody em-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "˜"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ϕ0 = [Mr,\nMs]"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "beddings from the pre-trained Emotion2Vec [22] model. The Emo-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "tion2Vec model is trained to capture emotion information in speech,",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "To achieve a balance between generative fidelity and time consump-"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "which we believe is primarily reflected in speech prosody. Thus, we",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "tion, we configured the ODE step to 32 in our experiment."
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "hypothesize that the representations extracted from the Emotion2Vec",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "model can serve as prosody embeddings in our system. Additionally,",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "3. EXPERIMENTAL SETUP"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "to avoid speaker timbre information leakage from the prosody em-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "bedding, we first perturb the pitch [29] of the speech before feeding",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "3.1. Dataset"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "it into the Emotion2Vec model using the sox3 toolkit to raise or lower",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "the pitch by 200 or 400 cents. Then, we use the final output from the",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "In our\nexperiment, we utilized two datasets:\nthe LibriTTS [16]"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "Emotion2Vec model as the prosody embedding.",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "dataset and the Emotional Speech Database (ESD) [31]. The train-"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "clean-360\nsubset\nof\nthe LibriTTS dataset was\nused\nfor\nsystem"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "2.4. Flow-Matching based Generative Model",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "training, while the test-clean subset was employed to evaluate the"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "basic zero-shot voice conversion ability of our ICL-VC system. The"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "To balance the quality and speed of mel-spectrogram generation,",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ESD dataset, which contains speech with rich prosody variations,"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "we employ the flow-matching-based [30] generation method in our",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "was exclusively used to assess the source speech prosody-preserving"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "study. This method aims to fit an unknown distribution q(x1), where",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "ability. Additionally, we resampled the ESD dataset\nto 24 kHz to"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": ": Rd → Rd, t ∈\nx1 ∈ Rd, by constructing a continuous flow ϕt",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "match the sample rate of the LibriTTS dataset."
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "[0, 1] that transforms a simple known distribution p0, such as a Gaus-",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "For evaluating the basic voice conversion ability, we randomly"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "sian distribution,\nThe flow is\ninto the target distribution p1 ≈ q.",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "selected 20 speakers\nfrom the\ntest-clean subset of\nthe LibriTTS"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "governed by an ordinary differential equation (ODE):",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": ""
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "dataset.\nTen speakers were designated as\nreference speakers and"
        },
        {
          "semantic\ntokens without modifying the model\narchitecture.\nThe": "",
          "In the training process, t is randomly sam-\napproximate ut(x|x1).": "another\nten as\nsource\nspeakers.\nFor\neach reference\nspeaker, we"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "In our experiments, we conduct both objective and subjective evalu-"
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "ations. For the objective evaluation, we compute the speaker embed-"
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "ding cosine similarity (SECS) between the converted speech and the"
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "reference speech. We use the ECAPA-TDNN [39] pre-trained model"
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "from the Wespeaker toolkit [37] to extract\nthe speaker embeddings."
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "Additionally,\nfollowing [40], we utilize a ASR pre-trained model"
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "provided by the NEMO toolkit7\nto evaluate the character error rate"
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "(CER) of the ground-truth and converted speech, which reflects the"
        },
        {
          "3.4. Evaluation Metric": ""
        },
        {
          "3.4. Evaluation Metric": "intelligibility of the speech. Furthermore, we calculate the Pearson"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "layer maps the Transformer’s output\nto the dimension of\nthe mel-",
          "correlation between the pitch and energy sequences [41] extracted": "from the source speech and the converted speech to determine if the"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "spectrogram. We extract\nthe mel-spectrogram using the Vocos [33]",
          "correlation between the pitch and energy sequences [41] extracted": "converted speech maintains the prosody of\nthe source speech.\nFor"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "toolkit and employ its pre-trained mel-based4 vocoder to convert the",
          "correlation between the pitch and energy sequences [41] extracted": "the subjective evaluation, we ask 15 human raters to provide a mean"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "mel-spectrogram into raw audio in the inference stage.",
          "correlation between the pitch and energy sequences [41] extracted": "opinion score (MOS) ranging from 1 to 5 regarding the naturalness"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "To extract prosody information, we use the Pyworld 5\ntoolkit",
          "correlation between the pitch and energy sequences [41] extracted": "of the speech and the prosody similarity between the converted and"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "to extract\nthe pitch (F0)\nfrom the audio. We normalize the pitch",
          "correlation between the pitch and energy sequences [41] extracted": "source speech. These 15 human raters simultaneously participated in"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "and energy values for each utterance to mitigate speaker information",
          "correlation between the pitch and energy sequences [41] extracted": "scoring both naturalness MOS and prosody MOS. When conducting"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "leakage.\nThe pitch and energy values are then tokenized into 256",
          "correlation between the pitch and energy sequences [41] extracted": "a prosody similarity subjective evaluation,\nthe instruction we pro-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "bins, producing pitch and energy tokens. A learnable embedding",
          "correlation between the pitch and energy sequences [41] extracted": "vided is: “Please assess the similarity in speaking prosody between"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "module maps these tokens into embeddings. When using the Emo-",
          "correlation between the pitch and energy sequences [41] extracted": "the provided speech and the reference speech. Give a rating from 1"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "tion2Vec [22] model to extract prosody information, as introduced in",
          "correlation between the pitch and energy sequences [41] extracted": "to 5, with 5 indicating completely consistent prosody and 1 indicat-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "Section 2.3, we first perturb the pitch of the input speech and use the",
          "correlation between the pitch and energy sequences [41] extracted": "ing completely different prosody.” On average, each audio sample"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "final output of the Emotion2Vec model as the prosody embedding.",
          "correlation between the pitch and energy sequences [41] extracted": "receives 3 MOS scores for naturalness evaluation and 4 MOS scores"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "In our experiment, we utilize three pre-trained models to extract",
          "correlation between the pitch and energy sequences [41] extracted": "for prosody similarity evaluation."
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "semantic tokens\nfollowing the fairseq6\nrecipe:\nthe HuBERT base",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "model trained on LibriSpeech-960, the Wav2Vec base model trained",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "4. RESULTS"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "on LibriSpeech-960, and the Wav2Vec-XLSR model trained on 56k",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "hours of multilingual data. For each pre-trained model, we train a k-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "4.1. Zero-shot Voice Conversion Evaluation"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "means tokenizer with 500 clusters separately. Previous research [34,",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "35] has demonstrated that different\nlayers of\nself-supervised pre-",
          "correlation between the pitch and energy sequences [41] extracted": "4.1.1. Comparison between Different Systems"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "trained models contain varying types of information. Based on this",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "In this section, we evaluate our proposed ICL voice conversion strat-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "analysis, we select outputs from the 9th layer of both the HuBERT",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "egy on the LibriTTS test set, with the results summarized in Table 1."
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "base and Wav2Vec base models, and the 14th layer of the Wav2Vec-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "Our findings indicate that all ICL based VC systems utilizing differ-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "XLSR model,\nto train the k-means tokenizer. This layer selection",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "ent semantic codecs demonstrate superior performance in replicat-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "aims to retain the most semantic information while filtering out non-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "ing the target speaker’s timbre and generating a more natural voice."
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "semantic information.",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "As introduced in Section 2.2, unlike other works [12]\nthat use the"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "During the training process of our ICL-VC system, we randomly",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "original\ntokenization strategy in Wav2vec, we employ a simple k-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "select an unmasked region of each input utterance, with a duration",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "means tokenization strategy for Wav2vec. The results reveal that the"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "ranging from 2 to 3 seconds.\nThe left part of\nthis region is then",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "k-means tokenization strategy is effective for\nthe Wav2Vec frame-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "masked with random noise. The system is trained to reconstruct the",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "work, despite Wav2Vec not being trained with labels clustered by"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "masked portion by conditioning on the input semantic embedding",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "the k-means method. This phenomenon suggests that\nthe k-means"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "(or with addition prosody embedding) and the unmasked region.",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "tokenization method could be a general strategy for semantic tok-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "enization."
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "3.3. Baseline System",
          "correlation between the pitch and energy sequences [41] extracted": "Interestingly, the ICL-VC-Wav2Vec-XLSR system exhibits bet-"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "ter\ncosine\nspeaker\nsimilarity compared to ICL-VC-HuBERT and"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "For system comparison, we introduce two baseline systems in our",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "ICL-VC-Wav2Vec. This may be attributed to the Wav2Vec-XLSR"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "experiment. The first one is the YourTTS system [36], where we use",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "model being trained on a larger amount of speech data and exposed"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "the ResNet34-based pre-trained speaker encoder provided by Wes-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "to more\nspeakers,\nthus\nenhancing its\nzero-shot voice\nconversion"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "peaker\ntoolkit\n[37].\nThe other one is RefXVC [38], a voice con-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "capability. However,\nthe ICL-VC-Wav2Vec-XLSR system shows a"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "version method leveraging self-supervised learning features and en-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "higher Character Error Rate (CER) than the other two systems. This"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "hanced reference-based speaker embedding to improve speaker sim-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "discrepancy is likely because the HuBERT and Wav2Vec models are"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "ilarity. All\nthe systems in our experiment are trained on the same",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "directly trained on the LibriSpeech dataset, which has a high content"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "training set for fair comparison.",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "correlation with the LibriTTS test set."
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "3.4. Evaluation Metric",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "Table 1. Voice Conversion Results on LibriTTS dataset. Prosody"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "In our experiments, we conduct both objective and subjective evalu-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "embedding is not used for ICL-VC system’s input in this evaluation."
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "ations. For the objective evaluation, we compute the speaker embed-",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "System\nSECS\nCER\nNatualness MOS"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "ding cosine similarity (SECS) between the converted speech and the",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "YourTTS\n0.824\n3.83\n3.86 ± 0.13"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "reference speech. We use the ECAPA-TDNN [39] pre-trained model",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "RefXVC\n0.797\n2.22\n4.02 ± 0.12"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "from the Wespeaker toolkit [37] to extract\nthe speaker embeddings.",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "ICL-VC-HuBERT\n0.846\n2.66\n4.33 ± 0.11"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "Additionally,\nfollowing [40], we utilize a ASR pre-trained model",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "ICL-VC-Wav2Vec\n0.845\n2.23\n4.37 ± 0.12"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "provided by the NEMO toolkit7\nto evaluate the character error rate",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "ICL-VC-Wav2Vec-XLSR\n0.856\n5.09\n4.32 ± 0.11"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "(CER) of the ground-truth and converted speech, which reflects the",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "",
          "correlation between the pitch and energy sequences [41] extracted": "Ground Truth\n-\n1.69\n4.51 ± 0.09"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "intelligibility of the speech. Furthermore, we calculate the Pearson",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "4https://github.com/gemelo-ai/vocos",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "5https://pypi.org/project/pyworld/",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "6https://github.com/facebookresearch/fairseq/",
          "correlation between the pitch and energy sequences [41] extracted": "4.1.2. Comparison between Different Reference Durations"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "tree/main/examples/hubert/simple_kmeans",
          "correlation between the pitch and energy sequences [41] extracted": ""
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "7https://catalog.ngc.nvidia.com/orgs/nvidia/",
          "correlation between the pitch and energy sequences [41] extracted": "Evaluating the use of shorter reference speech for timbre replication"
        },
        {
          "the Transformer’s\ninput dimension.\nAdditionally,\nanother\nlinear": "teams/nemo/models/stt_en_quartznet15x5",
          "correlation between the pitch and energy sequences [41] extracted": "is essential\nto testing the capabilities of zero-shot voice conversion"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: It is noteworthy that all SECS",
      "data": [
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "are used in our ICL-VC system. The Pitch Corr and Energy Corr correspond to the Pearson correlation introduced in section 3.4."
        },
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "System"
        },
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "YourTTS"
        },
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "RefXVC"
        },
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "ICL-VC"
        },
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "+ Pitch & Energy"
        },
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "+ Emotion Emb"
        },
        {
          "Table 2. Voice Conversion Results on ESD dataset. In this experiment, only the semantic tokens extracted from HuBERT pre-trained model": "Ground Truth"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: It is noteworthy that all SECS",
      "data": [
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "Ground Truth\n-\n3.21\n-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "-\n-\n4.41 ± 0.08"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "systems.\nIn this section, we evaluate the impact of reference dura-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "shortcoming arises because, during inference, our\nICL-VC system"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "tion on speaker embedding cosine similarity, as illustrated in Figure",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "relies on the reference speech to extract both timbre and prosody"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "2.\nFor both YourTTS and our method,\nthe SECS variation trend",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "information, while the source speech provides only semantic infor-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "indicates\nthat\nlonger\nreference speech allows the system to better",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "mation. To enhance our ICL-VC system’s ability to retain prosody,"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "model\nthe speaker’s timbre information. Notably, our ICL-VC sys-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "we employ two strategies introduced in Section 2.3.\nFirst, we in-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "tem achieves high cosine speaker similarity even with a very short",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "tegrate normalized pitch and energy information into the input of"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "(3-second) reference utterance.",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "our ICL-VC system. This approach significantly improves prosody"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "correlation and MOS scores, although it leads to a notable decline in"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "speaker similarity and naturalness MOS scores. While normalized"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "pitch reduces\ntimbre leakage from the source speech to the con-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "verted speech,\nthe ESD dataset’s prosody variations differ greatly"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "from those in the LibriTTS dataset. This discrepancy can cause mis-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "matches during inference,\nimpairing the model’s ability to generate"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "high-quality speech and resulting in lower SECS and naturalness"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "MOS scores.\nInterestingly, our proposed strategy using prosody in-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "formation extracted from the pre-trained Emotion2Vec model helps"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "mitigate\nthis\nissue,\nyielding higher SECS and naturalness MOS"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "scores."
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "5. CONCLUSION"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "In this paper, we introduced the in-context\nlearning (ICL) method"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "to the voice conversion task. Our experimental results demonstrate"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "Fig. 2. The relationship between the speaker embedding cosine sim-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "that the ICL method enables the system to perform voice conversion"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "ilarity (SECS) and reference speech duration.",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "without relying on an external speaker encoder,\nresulting in higher"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "speaker similarity. However, we observed that simply applying the"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "ICL method does not ensure the preservation of the source speech’s"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "prosody. To address this issue, we proposed extracting prosody em-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "4.2. Prosody Preserving Ability Evaluation",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "beddings from a pre-trained emotion recognition model, achieving"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "impressive prosody-preserving performance on the ESD dataset."
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "In addition to mimicking the target speaker’s timbre,\nthe voice con-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "Additionally, we found that\nthe k-means tokenization method is a"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "version task always\nrequires preserving the source speech’s con-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "versatile technique that can be applied to various\nself-supervised"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "tent and prosody [1].\nTo evaluate the system’s ability to maintain",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "pre-trained models, regardless of their different\ntraining objectives."
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "prosody, we directly evaluate the system trained on LibriTTS on the",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "Despite these advancements, our method has been validated only"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "ESD dataset, which features speech with significant prosody varia-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "on small datasets so far.\nThe ICL method, combined with the in-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "tions. Beyond the evaluation metrics listed in Table 1, we incorpo-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "formation decoupling approach based on self-supervised learning"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "rate pitch and energy correlation metrics introduced in section 3.4",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "pre-trained models, has the potential\nto train the system on larger,"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "to assess the prosody similarity between the converted and source",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "noisier,\nstylistically diverse, and even multilingual datasets.\nThis"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "speech. Additionally, we include prosody MOS scores to capture",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "will be the focus of our future work."
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "subjective human judgments of prosody similarity. The correspond-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "ing results are shown in Table 2.\nIt\nis noteworthy that all SECS",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "values in Table 2 are lower than those in Table 1, due to the prosody",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "6. ACKNOWLEDGMENTS"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "mismatch between LibriTTS and ESD speech. Nevertheless,\nthese",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "This work was\nsupported\nin\npart\nby China NSFC projects\nun-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "SECS values remain in the high range, indicating that all systems are",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "der Grants\n62122050\nand\n62071288,\nin\npart\nby Shanghai Mu-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "capable of performing voice conversion on the ESD dataset.",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "nicipal Science and Technology Commission Project under Grant"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "Despite the superior\ntimbre replication capability of\nthe ICL-",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "2021SHZDZX0102.\nThis\nstudy is partially supported by MEXT"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "based VC system,\nthe results in Table 2 indicate that\nit struggles to",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "KAKENHI Grants (24K21324) and CCF-NetEase ThunderFire In-"
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "maintain prosody information. The ICL-VC system exhibits lower",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": "novation Research Funding (CCF-Netease 202302)."
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "prosody MOS scores compared to the two baseline systems,\nand",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        },
        {
          "+ Emotion Emb\n0.789\n4.33\n0.671": "lower pitch and energy correlation than the YourTTS system. This",
          "0.900\n4.19 ± 0.16\n4.14 ± 0.13": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Wang, Jianwu Dang, Korin Richmond, and Junichi Yamagishi,"
        },
        {
          "7. REFERENCES": "[1]\nSongxiang Liu, Yuewen Cao, Shiyin Kang, Na Hu, Xunying",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Zmm-tts: Zero-shot multilingual and multispeaker speech syn-"
        },
        {
          "7. REFERENCES": "Liu, Dan Su, Dong Yu, and Helen Meng, Transferring Source",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "thesis conditioned on self-supervised discrete speech represen-"
        },
        {
          "7. REFERENCES": "Style in Non-Parallel Voice Conversion,” in Proc. Interspeech",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "tations,” arXiv preprint arXiv:2312.14398, 2023."
        },
        {
          "7. REFERENCES": "2020, 2020, pp. 4721–4725.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[13] Chunhui Wang, Chang Zeng, Bowen Zhang, Ziyang Ma, Yefan"
        },
        {
          "7. REFERENCES": "[2] Zhichao Wang, Xinsheng Wang, Lei Xie, Yuanzhe Chen, Qiao",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Zhu, Zifeng Cai, Jian Zhao, Zhonglin Jiang, and Yong Chen,"
        },
        {
          "7. REFERENCES": "Tian, and Yuping Wang,\nDelivering speaking style in low-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Ham-tts: Hierarchical acoustic modeling for token-based zero-"
        },
        {
          "7. REFERENCES": "resource voice conversion with multi-factor constraints,”\nin",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "arXiv\nshot\ntext-to-speech with model\nand\ndata\nscaling,”"
        },
        {
          "7. REFERENCES": "ICASSP 2023-2023 IEEE International Conference on Acous-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "preprint arXiv:2403.05989, 2024."
        },
        {
          "7. REFERENCES": "tics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[14] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and"
        },
        {
          "7. REFERENCES": "1–5.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Mark Hasegawa-Johnson, Autovc: Zero-shot voice style trans-"
        },
        {
          "7. REFERENCES": "[3] Lifa Sun, Kun Li, Hao Wang, Shiyin Kang, and Helen Meng,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "fer with only autoencoder loss,” in International Conference on"
        },
        {
          "7. REFERENCES": "Phonetic\nposteriorgrams\nfor many-to-one\nvoice\nconversion",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Machine Learning. PMLR, 2019, pp. 5210–5219."
        },
        {
          "7. REFERENCES": "without parallel data training,”\nin 2016 IEEE International",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[15] Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, Xin"
        },
        {
          "7. REFERENCES": "Conference on Multimedia and Expo (ICME). IEEE, 2016, pp.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Wang, Nanxin Chen, and Junichi Yamagishi, Zero-shot multi-"
        },
        {
          "7. REFERENCES": "1–6.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "speaker text-to-speech with state-of-the-art neural speaker em-"
        },
        {
          "7. REFERENCES": "[4] Hyeong-Seok Choi, Juheon Lee, Wansoo Kim, Jie Lee, Hoon",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "beddings,”\nin ICASSP 2020-2020 IEEE International Confer-"
        },
        {
          "7. REFERENCES": "Heo, and Kyogu Lee, Neural analysis and synthesis: Recon-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "ence on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "7. REFERENCES": "Ad-\nstructing speech from self-supervised representations,”",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "IEEE, 2020, pp. 6184–6188."
        },
        {
          "7. REFERENCES": "vances in Neural Information Processing Systems, vol. 34, pp.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "16251–16265, 2021.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[16] Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J Weiss,"
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Ye Jia, Zhifeng Chen, and Yonghui Wu,\nLibritts: A corpus"
        },
        {
          "7. REFERENCES": "[5] Yist Y Lin, Chung-Ming Chien, Jheng-Hao Lin, Hung-yi Lee,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "arXiv preprint\nderived from librispeech for\ntext-to-speech,”"
        },
        {
          "7. REFERENCES": "and Lin-shan Lee, Fragmentvc: Any-to-any voice conversion",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "arXiv:1904.02882, 2019."
        },
        {
          "7. REFERENCES": "by end-to-end extracting and fusing fine-grained voice frag-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "ments with attention,”\nin ICASSP 2021-2021 IEEE Interna-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[17] Yuma Koizumi, Heiga Zen, Shigeki Karita, Yifan Ding, Kohei"
        },
        {
          "7. REFERENCES": "tional Conference on Acoustics, Speech and Signal Processing",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Yatabe, Nobuyuki Morioka, Michiel Bacchiani, Yu Zhang, Wei"
        },
        {
          "7. REFERENCES": "(ICASSP). IEEE, 2021, pp. 5939–5943.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Han, and Ankur Bapna,\nLibritts-r: A restored multi-speaker"
        },
        {
          "7. REFERENCES": "[6] Dacheng Yin, Xuanchi Ren, Chong Luo, Yuwang Wang, Zhi-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "arXiv\npreprint\ntext-to-speech\ncorpus,”\narXiv:2305.18802,"
        },
        {
          "7. REFERENCES": "wei Xiong, and Wenjun Zeng,\nRetriever: Learning content-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "2023."
        },
        {
          "7. REFERENCES": "style representation as a token-level bipartite graph,”\nin The",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[18] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long"
        },
        {
          "7. REFERENCES": "Tenth International Conference on Learning Representations,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang,"
        },
        {
          "7. REFERENCES": "ICLR 2022, Virtual Event, April 25-29, 2022. 2022, OpenRe-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Jinyu Li, et al., Neural codec language models are zero-shot"
        },
        {
          "7. REFERENCES": "view.net.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "text to speech synthesizers,” arXiv preprint arXiv:2301.02111,"
        },
        {
          "7. REFERENCES": "[7] Adam Polyak, Yossi Adi,\nJade Copet, Eugene Kharitonov,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "2023."
        },
        {
          "7. REFERENCES": "Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[19] Chenpeng Du, Yiwei Guo, Feiyu Shen, Zhijun Liu, Zheng"
        },
        {
          "7. REFERENCES": "Emmanuel Dupoux, Speech Resynthesis from Discrete Disen-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Liang, Xie Chen, Shuai Wang, Hui Zhang, and Kai Yu, Uni-"
        },
        {
          "7. REFERENCES": "tangled Self-Supervised Representations,” in Proc. Interspeech",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "cats: A unified context-aware text-to-speech framework with"
        },
        {
          "7. REFERENCES": "2021, 2021, pp. 3615–3619.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "the\ncontextual vq-diffusion and vocoding,”\nin Proceedings of"
        },
        {
          "7. REFERENCES": "[8] Ziqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu, Shuo Ren,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "AAAI Conference on Artificial Intelligence, 2024, vol. 38, pp."
        },
        {
          "7. REFERENCES": "Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "17924–17932."
        },
        {
          "7. REFERENCES": "et al., Speechlm: Enhanced speech pre-training with unpaired",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[20] Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda"
        },
        {
          "7. REFERENCES": "IEEE/ACM Transactions on Audio, Speech, and\ntextual data,”",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Sari, Rashel Moritz, Mary Williamson, Vimal Manohar, Yossi"
        },
        {
          "7. REFERENCES": "Language Processing, 2024.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Adi, Jay Mahadeokar, et al., Voicebox: Text-guided multilin-"
        },
        {
          "7. REFERENCES": "[9] Kun Zhou, Shengkui Zhao, Yukun Ma, Chong Zhang, Hao",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "gual universal speech generation at scale,” Advances in neural"
        },
        {
          "7. REFERENCES": "Wang, Dianwen Ng, Chongjia Ni, Nguyen Trung Hieu, Jia Qi",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "information processing systems, vol. 36, 2024."
        },
        {
          "7. REFERENCES": "Yip, and Bin Ma,\nPhonetic enhanced language modeling for",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[21]\nPhilip Anastassiou, Jiawei Chen, Jitong Chen, Yuanzhe Chen,"
        },
        {
          "7. REFERENCES": "text-to-speech synthesis,”\narXiv preprint arXiv:2406.02009,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Zhuo Chen, Ziyi Chen, Jian Cong, Lelai Deng, Chuang Ding,"
        },
        {
          "7. REFERENCES": "2024.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Lu Gao, et al.,\nSeed-tts: A family of high-quality versatile"
        },
        {
          "7. REFERENCES": "[10] Yifan Yang, Feiyu Shen, Chenpeng Du, Ziyang Ma, Kai Yu,",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "speech generation models,” arXiv preprint arXiv:2406.02430,"
        },
        {
          "7. REFERENCES": "Daniel Povey, and Xie Chen,\nTowards universal speech dis-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "2024."
        },
        {
          "7. REFERENCES": "crete tokens: A case study for asr and tts,”\nin ICASSP 2024-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "2024 IEEE International Conference on Acoustics, Speech and",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[22] Ziyang Ma, Zhisheng Zheng, Jiaxin Ye, Jinchao Li, Zhifu Gao,"
        },
        {
          "7. REFERENCES": "Signal Processing (ICASSP). IEEE, 2024, pp. 10401–10405.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Shiliang Zhang, and Xie Chen,\nemotion2vec: Self-supervised"
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "pre-training for speech emotion representation,” arXiv preprint"
        },
        {
          "7. REFERENCES": "[11] Qian Chen, Wen Wang, Qinglin Zhang, Siqi Zheng, Shiliang",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "arXiv:2312.15185, 2023."
        },
        {
          "7. REFERENCES": "Zhang, Chong Deng, Yukun Ma, Hai Yu,\nJiaqing Liu,\nand",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": ""
        },
        {
          "7. REFERENCES": "Chong Zhang,\nLoss masking is not needed in decoder-only",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "[23] Wei-Ning Hsu,\nBenjamin Bolte,\nYao-Hung Hubert\nTsai,"
        },
        {
          "7. REFERENCES": "transformer\nfor discrete-token-based asr,”\nin ICASSP 2024-",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman"
        },
        {
          "7. REFERENCES": "2024 IEEE International Conference on Acoustics, Speech and",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "Mohamed,\nHubert:\nSelf-supervised speech representation"
        },
        {
          "7. REFERENCES": "Signal Processing (ICASSP). IEEE, 2024, pp. 11056–11060.",
          "[12] Cheng Gong, Xin Wang, Erica Cooper, Dan Wells, Longbiao": "IEEE/ACM\nlearning by masked prediction of hidden units,”"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "29, pp. 3451–3460, 2021.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "Chen, Binbin Zhang, Xu Xiang, Yanlei Deng,\nand Yanmin"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "Qian, Wespeaker: A research and production oriented speaker"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[24] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "embedding learning toolkit,” in IEEE International Conference"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Michael Auli, wav2vec 2.0: A framework for self-supervised",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "learning of speech representations,” Advances in neural infor-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "2023, pp. 1–5."
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "mation processing systems, vol. 33, pp. 12449–12460, 2020.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[25] Chenpeng Du, Yiwei Guo, Xie Chen, and Kai Yu, VQTTS:",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "[38] Mingyang Zhang, Yi Zhou, Yi Ren, Chen Zhang, Xiang"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "High-Fidelity Text-to-Speech Synthesis with Self-Supervised",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "Yin,\nand Haizhou Li,\nRefxvc: Cross-lingual voice conver-"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "VQ Acoustic Feature,”\nin Proc.\nInterspeech 2022, 2022, pp.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "arXiv\npreprint\nsion with\nenhanced\nreference\nleveraging,”"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "1596–1600.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "arXiv:2406.16326, 2024."
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[26] Kaizhi Qian, Zeyu Jin, Mark Hasegawa-Johnson,\nand Gau-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "[39] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck,"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "tham J Mysore,\nF0-consistent many-to-many non-parallel",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "ECAPA-TDNN: Emphasized Channel Attention, Propagation"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "voice\nconversion via\nconditional\nautoencoder,”\nin ICASSP",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "and Aggregation in TDNN Based Speaker Verification,”\nin"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "2020-2020\nIEEE\nInternational\nConference\non\nAcoustics,",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "Proc. Interspeech 2020, 2020, pp. 3830–3834."
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Speech\nand\nSignal Processing\n(ICASSP).\nIEEE,\n2020,\npp.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "[40]\nShehzeen Hussain, Paarth Neekhara,\nJocelyn Huang,\nJason"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "6284–6288.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "Li, and Boris Ginsburg,\nAce-vc: Adaptive and controllable"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[27]\nSen Liu, Yiwei Guo, Chenpeng Du, Xie Chen, and Kai Yu,",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "voice conversion using explicitly disentangled self-supervised"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "speech representations,” in ICASSP 2023-2023 IEEE Interna-"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "to-Speech,”\nin Proc.\nINTERSPEECH 2023, 2023, pp. 616–",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "tional Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "620.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "(ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[28] Kentaro Mitsui, Yukiya Hono,\nand Kei Sawada,\nTowards",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "[41] Ziqian Ning, Qicong Xie, Pengcheng Zhu, Zhichao Wang, Li-"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "human-like spoken dialogue generation between ai agents from",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "umeng Xue, Jixun Yao, Lei Xie, and Mengxiao Bi, Expressive-"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "written dialogue,” arXiv preprint arXiv:2310.01088, 2023.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "vc: Highly expressive voice conversion with attention fusion of"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[29] Hitoshi Yamamoto, Kong Aik Lee, Koji Okabe, and Takafumi",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "bottleneck and perturbation features,”\nin ICASSP 2023-2023"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Koshinaka,\nSpeaker augmentation and bandwidth extension",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "IEEE International Conference on Acoustics, Speech and Sig-"
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "for deep speaker embedding,”\nin Interspeech, Gernot Kubin",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": "nal Processing (ICASSP). IEEE, 2023, pp. 1–5."
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "and Zdravko Kacic, Eds., 2019, pp. 406–410.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[30] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Nickel, and Matthew Le, Flow matching for generative mod-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "eling,” in The Eleventh International Conference on Learning",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023,",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "2023.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[31] Kun Zhou, Berrak Sisman, Rui Liu, and Haizhou Li,\nEmo-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Speech\ntional voice conversion: Theory, databases and esd,”",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Communication, vol. 137, pp. 1–18, 2022.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[32] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszko-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Advances in neural\nPolosukhin, Attention is all you need,”",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "information processing systems, vol. 30, 2017.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[33] Hubert Siuzdak, Vocos: Closing the gap between time-domain",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "and fourier-based neural vocoders for high-quality audio syn-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "thesis,” arXiv preprint arXiv:2306.00814, 2023.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[34]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shu-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "jie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yosh-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "ioka, Xiong Xiao, et al., Wavlm: Large-scale self-supervised",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "pre-training for full stack speech processing,” IEEE Journal of",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "1518, 2022.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[35] Zhengyang Chen, Sanyuan Chen, Yu Wu, Yao Qian, Chengyi",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Wang, Shujie Liu, Yanmin Qian, and Michael Zeng,\nLarge-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "scale self-supervised speech representation learning for auto-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "matic speaker verification,” in ICASSP 2022-2022 IEEE Inter-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "national Conference on Acoustics, Speech and Signal Process-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "ing (ICASSP). IEEE, 2022, pp. 6147–6151.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "[36] Edresson Casanova, Julian Weber, Christopher D Shulby, Ar-",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "naldo Candido\nJunior,\nEren G¨olge,\nand Moacir A Ponti,",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "Yourtts:\nTowards\nzero-shot multi-speaker\ntts\nand zero-shot",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "voice conversion for everyone,”\nin International Conference",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        },
        {
          "Transactions on Audio, Speech, and Language Processing, vol.": "on Machine Learning. PMLR, 2022, pp. 2709–2720.",
          "[37] Hongji Wang, Chengdong Liang, Shuai Wang, Zhengyang": ""
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Transferring Source Style in Non-Parallel Voice Conversion",
      "authors": [
        "Songxiang Liu",
        "Yuewen Cao",
        "Shiyin Kang",
        "Na Hu",
        "Xunying Liu",
        "Dan Su",
        "Dong Yu",
        "Helen Meng"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Delivering speaking style in lowresource voice conversion with multi-factor constraints",
      "authors": [
        "Zhichao Wang",
        "Xinsheng Wang",
        "Lei Xie",
        "Yuanzhe Chen",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Phonetic posteriorgrams for many-to-one voice conversion without parallel data training",
      "authors": [
        "Lifa Sun",
        "Kun Li",
        "Hao Wang",
        "Shiyin Kang",
        "Helen Meng"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Multimedia and Expo (ICME"
    },
    {
      "citation_id": "5",
      "title": "Neural analysis and synthesis: Reconstructing speech from self-supervised representations",
      "authors": [
        "Hyeong-Seok Choi",
        "Juheon Lee",
        "Wansoo Kim",
        "Jie Lee",
        "Hoon Heo",
        "Kyogu Lee"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "Fragmentvc: Any-to-any voice conversion by end-to-end extracting and fusing fine-grained voice fragments with attention",
      "authors": [
        "Chung-Ming Yist Y Lin",
        "Jheng-Hao Chien",
        "Hung-Yi Lin",
        "Lin-Shan Lee",
        "Lee"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Retriever: Learning contentstyle representation as a token-level bipartite graph",
      "authors": [
        "Dacheng Yin",
        "Xuanchi Ren",
        "Chong Luo",
        "Yuwang Wang",
        "Zhiwei Xiong",
        "Wenjun Zeng"
      ],
      "year": "2022",
      "venue": "The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event"
    },
    {
      "citation_id": "8",
      "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations",
      "authors": [
        "Adam Polyak",
        "Yossi Adi",
        "Jade Copet",
        "Eugene Kharitonov",
        "Kushal Lakhotia",
        "Wei-Ning Hsu",
        "Abdelrahman Mohamed",
        "Emmanuel Dupoux"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "9",
      "title": "Speechlm: Enhanced speech pre-training with unpaired textual data",
      "authors": [
        "Ziqiang Zhang",
        "Sanyuan Chen",
        "Long Zhou",
        "Yu Wu",
        "Shuo Ren",
        "Shujie Liu",
        "Zhuoyuan Yao",
        "Xun Gong",
        "Lirong Dai",
        "Jinyu Li"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Nguyen Trung Hieu, Jia Qi Yip, and Bin Ma, Phonetic enhanced language modeling for text-to-speech synthesis",
      "authors": [
        "Kun Zhou",
        "Shengkui Zhao",
        "Yukun Ma",
        "Chong Zhang",
        "Hao Wang",
        "Dianwen Ng",
        "Chongjia Ni"
      ],
      "year": "2024",
      "venue": "Nguyen Trung Hieu, Jia Qi Yip, and Bin Ma, Phonetic enhanced language modeling for text-to-speech synthesis",
      "arxiv": "arXiv:2406.02009"
    },
    {
      "citation_id": "11",
      "title": "Towards universal speech discrete tokens: A case study for asr and tts",
      "authors": [
        "Yifan Yang",
        "Feiyu Shen",
        "Chenpeng Du",
        "Ziyang Ma",
        "Kai Yu",
        "Daniel Povey",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Loss masking is not needed in decoder-only transformer for discrete-token-based asr",
      "authors": [
        "Qian Chen",
        "Wen Wang",
        "Qinglin Zhang",
        "Siqi Zheng",
        "Shiliang Zhang",
        "Chong Deng",
        "Yukun Ma",
        "Hai Yu",
        "Jiaqing Liu",
        "Chong Zhang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Zmm-tts: Zero-shot multilingual and multispeaker speech synthesis conditioned on self-supervised discrete speech representations",
      "authors": [
        "Cheng Gong",
        "Xin Wang",
        "Erica Cooper",
        "Dan Wells",
        "Longbiao Wang",
        "Jianwu Dang",
        "Korin Richmond",
        "Junichi Yamagishi"
      ],
      "year": "2023",
      "venue": "Zmm-tts: Zero-shot multilingual and multispeaker speech synthesis conditioned on self-supervised discrete speech representations",
      "arxiv": "arXiv:2312.14398"
    },
    {
      "citation_id": "14",
      "title": "Ham-tts: Hierarchical acoustic modeling for token-based zeroshot text-to-speech with model and data scaling",
      "authors": [
        "Chunhui Wang",
        "Chang Zeng",
        "Bowen Zhang",
        "Ziyang Ma",
        "Yefan Zhu",
        "Zifeng Cai",
        "Jian Zhao",
        "Zhonglin Jiang",
        "Yong Chen"
      ],
      "year": "2024",
      "venue": "Ham-tts: Hierarchical acoustic modeling for token-based zeroshot text-to-speech with model and data scaling",
      "arxiv": "arXiv:2403.05989"
    },
    {
      "citation_id": "15",
      "title": "Autovc: Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "Xuesong Yang",
        "Mark Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Zero-shot multispeaker text-to-speech with state-of-the-art neural speaker embeddings",
      "authors": [
        "Erica Cooper",
        "Cheng-I Lai",
        "Yusuke Yasuda",
        "Fuming Fang",
        "Xin Wang",
        "Nanxin Chen",
        "Junichi Yamagishi"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Libritts: A corpus derived from librispeech for text-to-speech",
      "authors": [
        "Heiga Zen",
        "Viet Dang",
        "Rob Clark",
        "Yu Zhang",
        "Ron Weiss",
        "Ye Jia",
        "Zhifeng Chen",
        "Yonghui Wu"
      ],
      "year": "2019",
      "venue": "Libritts: A corpus derived from librispeech for text-to-speech",
      "arxiv": "arXiv:1904.02882"
    },
    {
      "citation_id": "18",
      "title": "Libritts-r: A restored multi-speaker text-to-speech corpus",
      "authors": [
        "Yuma Koizumi",
        "Heiga Zen",
        "Shigeki Karita",
        "Yifan Ding",
        "Kohei Yatabe",
        "Nobuyuki Morioka",
        "Michiel Bacchiani",
        "Yu Zhang",
        "Wei Han",
        "Ankur Bapna"
      ],
      "year": "2023",
      "venue": "Libritts-r: A restored multi-speaker text-to-speech corpus",
      "arxiv": "arXiv:2305.18802"
    },
    {
      "citation_id": "19",
      "title": "Neural codec language models are zero-shot text to speech synthesizers",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li"
      ],
      "year": "2023",
      "venue": "Neural codec language models are zero-shot text to speech synthesizers",
      "arxiv": "arXiv:2301.02111"
    },
    {
      "citation_id": "20",
      "title": "Unicats: A unified context-aware text-to-speech framework with contextual vq-diffusion and vocoding",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Voicebox: Text-guided multilingual universal speech generation at scale",
      "authors": [
        "Matthew Le",
        "Apoorv Vyas",
        "Bowen Shi",
        "Brian Karrer",
        "Leda Sari",
        "Rashel Moritz",
        "Mary Williamson",
        "Vimal Manohar",
        "Yossi Adi",
        "Jay Mahadeokar"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Seed-tts: A family of high-quality versatile speech generation models",
      "authors": [
        "Philip Anastassiou",
        "Jiawei Chen",
        "Jitong Chen",
        "Yuanzhe Chen",
        "Zhuo Chen",
        "Ziyi Chen",
        "Jian Cong",
        "Lelai Deng",
        "Chuang Ding",
        "Lu Gao"
      ],
      "year": "2024",
      "venue": "Seed-tts: A family of high-quality versatile speech generation models",
      "arxiv": "arXiv:2406.02430"
    },
    {
      "citation_id": "23",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Jiaxin Ye",
        "Jinchao Li",
        "Zhifu Gao",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "year": "2023",
      "venue": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "arxiv": "arXiv:2312.15185"
    },
    {
      "citation_id": "24",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Hubert Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "27",
      "title": "F0-consistent many-to-many non-parallel voice conversion via conditional autoencoder",
      "authors": [
        "Zeyu Kaizhi Qian",
        "Mark Jin",
        "Gautham Hasegawa-Johnson",
        "Mysore"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "DSE-TTS: Dual Speaker Embedding for Cross-Lingual Textto-Speech",
      "authors": [
        "Sen Liu",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "29",
      "title": "Towards human-like spoken dialogue generation between ai agents from written dialogue",
      "authors": [
        "Kentaro Mitsui",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "year": "2023",
      "venue": "Towards human-like spoken dialogue generation between ai agents from written dialogue",
      "arxiv": "arXiv:2310.01088"
    },
    {
      "citation_id": "30",
      "title": "Speaker augmentation and bandwidth extension for deep speaker embedding",
      "authors": [
        "Hitoshi Yamamoto",
        "Aik Kong",
        "Koji Lee",
        "Takafumi Okabe",
        "Koshinaka"
      ],
      "year": "2019",
      "venue": "Speaker augmentation and bandwidth extension for deep speaker embedding"
    },
    {
      "citation_id": "31",
      "title": "Flow matching for generative modeling",
      "authors": [
        "Yaron Lipman",
        "Ricky Chen",
        "Heli Ben-Hamu",
        "Maximilian Nickel",
        "Matthew Le"
      ],
      "year": "2023",
      "venue": "The Eleventh International Conference on Learning Representations, ICLR 2023"
    },
    {
      "citation_id": "32",
      "title": "Emotional voice conversion: Theory, databases and esd",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "33",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "34",
      "title": "Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis",
      "authors": [
        "Hubert Siuzdak",
        "Vocos"
      ],
      "year": "2023",
      "venue": "Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis",
      "arxiv": "arXiv:2306.00814"
    },
    {
      "citation_id": "35",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Largescale self-supervised speech representation learning for automatic speaker verification",
      "authors": [
        "Zhengyang Chen",
        "Sanyuan Chen",
        "Yu Wu",
        "Yao Qian",
        "Chengyi Wang",
        "Shujie Liu",
        "Yanmin Qian",
        "Michael Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "37",
      "title": "Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone",
      "authors": [
        "Edresson Casanova",
        "Julian Weber",
        "Christopher Shulby",
        "Arnaldo Candido Junior",
        "Eren Gölge",
        "Moacir Ponti"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "38",
      "title": "Wespeaker: A research and production oriented speaker embedding learning toolkit",
      "authors": [
        "Hongji Wang",
        "Chengdong Liang",
        "Shuai Wang",
        "Zhengyang Chen",
        "Binbin Zhang",
        "Xu Xiang",
        "Yanlei Deng",
        "Yanmin Qian"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Refxvc: Cross-lingual voice conversion with enhanced reference leveraging",
      "authors": [
        "Mingyang Zhang",
        "Yi Zhou",
        "Yi Ren",
        "Chen Zhang",
        "Xiang Yin",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "Refxvc: Cross-lingual voice conversion with enhanced reference leveraging",
      "arxiv": "arXiv:2406.16326"
    },
    {
      "citation_id": "40",
      "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification",
      "authors": [
        "Brecht Desplanques",
        "Jenthe Thienpondt",
        "Kris Demuynck"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "41",
      "title": "Ace-vc: Adaptive and controllable voice conversion using explicitly disentangled self-supervised speech representations",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Jocelyn Huang",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "Expressivevc: Highly expressive voice conversion with attention fusion of bottleneck and perturbation features",
      "authors": [
        "Ziqian Ning",
        "Qicong Xie",
        "Pengcheng Zhu",
        "Zhichao Wang",
        "Liumeng Xue",
        "Jixun Yao",
        "Lei Xie",
        "Mengxiao Bi"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}