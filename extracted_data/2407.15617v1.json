{
  "paper_id": "2407.15617v1",
  "title": "Norface: Improving Facial Expression Analysis By Identity Normalization",
  "published": "2024-07-22T13:24:32Z",
  "authors": [
    "Hanwei Liu",
    "Rudong An",
    "Zhimeng Zhang",
    "Bowen Ma",
    "Wei Zhang",
    "Yan Song",
    "Yujing Hu",
    "Wei Chen",
    "Yu Ding"
  ],
  "keywords": [
    "Facial emotion recognition",
    "Action Unit detection",
    "AU intensity estimation",
    "Identity normalization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Identity normalization. The figure demonstrates that various original images (i.e. in-the-lab and in-the-wild ones) with AUs and emotions are normalized to a common target identity with a consistent pose, background, etc.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial Expression Analysis (FEA) is a complex task in affective computing that often involves facial Action Units (AU) analysis (AU detection and AU intensity estimation) and Facial Emotions Recognition (FER). Despite the progress made so far  [6, 9, 30, 37, 59, 74] , both FEA tasks still face challenges due to the inherent entanglement of expressions with unexpected factors like identity, head pose, and background  [32, 46] . Specifically, FEA suffers from severe identity bias  [32, 33] , which exacerbates the intricacies of refinement and representation of facial expressions  [27, 78] . FEA model struggles to generalize effectively to unseen identities, this bias also leads the model to overfit seen identities and undermines the overall performance  [25, 26] . In addition, variations in pose and background compound the challenges in FEA tasks  [2, 30] . Hence, a crucial objective of FEA is to mitigate interference arising from irrelevant factors, including identity, pose, and background.\n\nTo address these challenges, some studies attempt to construct identity-based expression pairs  [26, 27, 65, 68, 93]  to obtain identity-invariant representations. Due to significant variations in identities, the issue of identity bias still persists. Alternatively, certain studies aim to incorporate generative techniques, such as generating 'neutral' emotions  [25, 78, 80] , 'average' identities  [4] , or diverse emotions  [1, 22, 76] , to disentangle expression from identity. Nonetheless, these methods either suffer from limited generation quality or heavily rely on controlled lab datasets, thereby restricting their generalizability in complex and diverse realworld scenarios. Moreover, these approaches often overlook non-identity noise factors, such as pose and background variations. Additionally, their frameworks are often tailored to either AU analysis or FER tasks, which cannot be unified for both tasks, despite task-irrelevant noise being common to both.\n\nTo address the above challenges, this paper introduces a novel framework called Norface, which is applicable to AU detection, AU intensity estimation, and FER tasks. The core concept behind Norface is Identity normalization (Idn), which normalizes all images to a common identity with consistent pose and background, as shown in Fig.  1 . By doing so, the resulting normalized data is intended to retain only the facial expression variations that are relevant to the task. Consequently, Idn mitigates the influence of identity bias, pose variations, and background changes, which are commonly encountered in both AU analysis and FER tasks. This process can also be interpreted as the removal of the above task-irrelevant noise, which is carried out by a normalization network in the first stage. Then, in the second stage, normalized images, as complementary to the original images, are used to improve expression analysis (AU detection, AU intensity estimation, or emotion recognition) by a classification network.\n\nSpecifically, in the first stage, the normalization network achieves Idn from all original faces to a target face. Thus, our normalization network is not only suitable for in-the-wild data but also capable of meeting the high-quality requirement for expression consistency. Specifically, we employ a pre-trained Masked AutoEncoder (MAE) to extract facial features, effectively capturing both expression and other attributes. Then, we develop an Expression Merging Module (EMM) to adaptively merge expression features from the original faces into the target face. Moreover, we introduce an expression loss and an eyebrow loss to enforce expression consistency between the normalized faces and the original faces.\n\nIn the second stage, the classification network relies on Mixture of Experts (MoE). MoE aims to learn several expert sub-branches which are automatically trained to dynamically activate task-specific experts. This work proposes Input and Output MoE modules, where the input MoE module can refine latent facial representation, and the output MoE module can facilitate the detection, estimation, or recognition of multiple AU or emotion labels.\n\nAdditionally, there are several multi-task methods  [12, 29, 92, 103]  that combine AU detection and FER tasks, requiring co-annotated AU and emotion labels, which are often unavailable in most datasets. Unlike these multi-task methods, our Norface offers a unified framework for these tasks individually, without the requirement for co-annotated AU and emotion labels, thus avoiding the issue of task imbalance  [66, 98]  being common in those multi-task methods.\n\nOur work is the first to develop a unified framework to address noise issues in three tasks of AU detection, AU intensity estimation, and FER. Our Norface outperforms the existing results on the three tasks, as well as their cross-dataset tasks, showcasing its superior performance. To sum up, ours makes the following contributions:\n\n-This work provides a novel insight and releases a normalized dataset to remove the task-irrelevant noise by identity normalization that normalizes all expression images to a common identity with consistent pose and background. -This work proposes a new unified Facial Expression Analysis framework named Norface, designed for AU detection, AU intensity estimation, or FER tasks. This framework addresses identity normalization using Masked Au-toEncoder and representation refinement based on Mixture of Experts. -Extensive quantitative and qualitative experiments validate the effectiveness of Norface, surpassing existing methods in AU detection, AU intensity estimation, and FER tasks, as well as their cross-dataset tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Identity-Invariant FEA. Identity bias has always been a challenge for FEA tasks, as expressions and identities are inherently entangled. Some methods  [26, 27, 65, 68, 93]  construct pairs of identity-based expressions to obtain identityinvariant representations. DLN  [93]  and GLEE-Net  [94]  construct triples mapping identity to facial expression similarity for performing FER and AU detection tasks, respectively. IdenNet  [65]  leverages face clustering to extract identitydependent features and perform AU detection. Moreover, synthetic images  [1, 4, 22, 25, 76, 78, 80]  have been employed to decouple identity and expression. DeRL  [78]  generates neutral facial images to represent lab facial images. Huang et al.  [22]  synthesize basic emotions for each identity. Differing from these approaches, ours focuses on identity normalization to address identity bias. AU analysis and FER tasks. Current AU analysis and FER methods often rely on auxiliary information to mitigate the interference of task-irrelevant noise, such as facial landmarks  [35, 51, 74, 103] , emotional priors  [31, 64, 102] , AU descriptions  [8, 10, 75, 79] , or patch-based learning methods  [23, 38, 39, 57, 104] . SEV-Net  [79]  utilizes both semantic descriptions and visual features of AU to enhance AU detection. LA-Net  [74]  incorporates facial landmarks to augment expression features. However, they still encounter interference from such noise as they rarely explicitly eliminate task-irrelevant noise. Expression reenactment. Identity normalization aims to normalize the expressions of all single-frame faces onto a fixed identity, pose, and background, which is distinct from popular expression reenactment. Currently, most expression reenactment methods  [20, 45, 61, 71, 72, 101]  cannot achieve identity normalization because they rely on consecutive frames, which cannot be applied to single-frame datasets. Additionally, while a few methods  [44, 54, 81, 82, 91]  can achieve identity normalization, the generated quality is often limited, making it challenging to meet the demand for high-quality expression consistency in identity normalization.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Method",
      "text": "As shown in Fig.  2 , Norface comprises two stages: the first stage employs a normalization network F nor for Identity normalization (Idn), while the second stage utilizes a classification network F cla for expression classification. Let I o be the original face belonging to a given AU or emotion dataset, and I id t be the given fixed target face I t with identity id. Firstly, F nor performs Idn on I o and I id t , resulting in a normalized face I n that maintains consistency in terms of identity, pose, background with I id t , and expression with I o . Then, F cla takes both I n and I o as input and then performs FEA tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Normalization Network",
      "text": "Normalization network F nor performs Idn on the original face I o and the target face I t . As shown in Fig.  2 , a shared face encoder E s first maps I o and I t into patch embeddings e o and e t . Then, an Expression Merging Module (EMM) is designed to adaptively fuse the expression information of I o and other attribute information of I t . Finally, a facial decoder D f outputs the normalized face I n .\n\nShared Face Encoder. We design a shared face encoder following MAE  [19] , to project original and target faces to a common latent representation. The MAE is pre-trained on a large-scale face dataset using a mask training strategy, which has been validated in many domains due to its superior feature extraction performance  [3, 53] . By forcing the model to learn the topological and semantic information of masked image patches, the latent space of the MAE can better capture facial expressions and attributes. That is, based on the pre-trained face encoder E s , we can project a original/target face I o/t into a latent presentation:\n\nwhere e o/t ∈ R N ×L . N and L denote the number of patches and the dimension of each embedding, respectively. Expression Merging Module. We designed an Expression Merging Module (EMM) that adaptively integrates the expression information of the original face with the other attributes of the target face. As shown in Fig.  2 , EMM consists of a multi-head cross-attention block and two transformer blocks  [15] . Given the original patch embeddings e o and the target patch embeddings e t , we first compute Q, K, V for each patch embedding in e o and e t . Then the cross-attention can be formulated as CA\n\n, where CA represents Cross Attention, Q * , K * , V * are predicted by attention heads, and d k is the dimension of K * . Next, the expression information from the original face is aggregated based on the computed CA:\n\nThen, V f u are normalized by a layer normalization (LN) and processed by multilayer perceptrons (MLP). The fused embeddings e f u are further fed into two transformer blocks to obtain the output e n . Finally, we utilize a convolutional decoder D f to generate the normalized face\n\nTraining Loss. The use of multiple loss functions is a pervasive way to constrain the image generation, e.g. those works  [40, 42, 69, 73, [86] [87] [88] [89]  on face manipulation, as these loss functions carry out supervision from perspectives. Inspired by them, we also employ multiple loss functions to ensure high expression consistency and fidelity in identity normalization. Specifically, the adversarial loss, the reconstruction loss, and the perceptual loss supervise the holistic image; an identity loss and a landmark loss are used to dominate the identity information; an expression loss and an eyebrow loss guide the expression transferring. They are formulated below.\n\nwhere λ rec , λ perc , λ id , λ lm , λ exp , λ eye are hyperparameters for each term. During the training phase, the choice of I t is as random as I o , which forces the normalization network to adapt to the variations in facial attributes. Adversarial Loss. The adversarial loss is used to make the normalized images more realistic. We apply the hinge version adversarial loss  [36]  for training:\n\nwhere D res is the discriminator to distinguish between real and fake samples. Reconstruction Loss. Since there is no ground-truth for face normalization results, we force I o = I t with a certain probability during training, and introduce a pixel-level reconstruction loss:\n\nPerceptual Loss. Since high-level feature maps contain semantic information, we employ the feature maps from the last two convolutional layers of pre-trained VGG  [5]  as the facial attribute representation. The loss is formulated as:\n\nIdentity Loss. The identity loss is to constrain I n 's identity information to be consistent with I t :\n\nwhere E id denotes a face recognition model  [13]  and cos denotes the cosine similarity. Landmark Loss. To enhance facial contour consistency of I t and I n , we first use the pre-trained facial landmark detector  [63]  to predict the facial landmarks of I t and I n , and then only apply loss to the facial contour, as follows:\n\nwhere P t and P n are the embeddings of the facial contours of I t and I n , respectively.\n\nExpression Loss. To ensure that I o and I n express similar movements of full face, we use a novel fine-grained expression loss  [93]  to penalize the L 2 distance of expression embeddings of I o and I n : where ∥ * ∥ denotes the euclidean distance.\n\nEyebrow Loss. Since the network tends to be insensitive to subtle changes in facial eyebrows, to further supervise the expression consistency, we use expression blend shapes from 3DMM  [100] , and penalize the L 2 distance in the channel associated with eyebrow movements:\n\nInference. During inference, we set I o from a given AU or emotion dataset and provide a fixed target face I id t . As a result, we obtain normalized face I n that maintains consistent facial expressions as I o and ensures consistency with the identity, pose, and background of I id t . Fig.  3  intuitively showcases Idn results on some in-the-lab and in-the-wild AU and emotion data, and all normalized data * has been released.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Classification Network",
      "text": "The classification network F cla is designed to perform AU detection, AU intensity estimation, or emotion recognition. As shown in Fig.  2 , F cla takes I n as input, as well as I o . Specifically, F cla consists of a facial feature extractor E f , an Input MoE module M i , and an Output MoE module M o . First, E f maps I n and I o to their latent presentations e f , where e f ∈ {e n , e o′ }. Then, e f are fed into M i , which consists of two identity-specific Mixture of Expert (MoE)  [43]  blocks, resulting in the identity-specific facial representation e h . Finally, all of e h are input into M o , which refines an identity-fused representation from fixed and original identities for the final classification. E f , M i , and M o will be further described below, as well as the MoE block.\n\nE f aims to map I n and I o to a latent representation e f . The design of E f is similar to E s and it is also pre-trained on the same large-scale face datasets. That is, based on E f , we can project\n\nM i consists of two identity-specific MoE blocks, with each MoE block receiving the individual latent representations e f of I n and I o , respectively, that is e h = M i (e f ). * Dataset link. https://norface-fea.github.io/.\n\nM o combines the latent representations e h of all identities to induce labelrelated information. First, all e h are concatenated to obtain the embedding e c . Then, e c is input into an MoE block to obtain the identity-fused latent representation. Finally, we use a linear layer to output the AU detection, AU intensity estimation, or FER result. Mixture of Experts. As mentioned above, MoE is used multiple times in M i and M o . The MoE divides a model into a group of experts, with each expert learning individual roles, allowing for collaboration when it contributes to the task. The sparsity of these experts enhances the model's inductive capabilities  [43, 58] , thereby effectively refining the latent representations. Specifically, the MoE in M i focuses on refining the facial representation from identity, while the MoE in M o fuses these representations from identities and focuses on refining representations relevant to different task labels.\n\nIn Formally, for given input presentation x, the noisy  [58]  Top-k routing network G, with parameters W g and W noise , predicts the gating weights for x and selects the Top-k experts from the m available experts to contribute to the final representation, that is G(x) = Topk(Softmax(xW g +N (0, 1)Softplus(xW noise ))).\n\nThe final representation y of the k activated experts are linearly combined according to the gating weights, that is y\n\nTraining Loss. To implement expression analysis tasks, we employ four loss functions to train our F cla :\n\nwhere λ imp and λ g&l are hyperparameters for each term. L cla is the cross entropy loss for AU detection and emotion recognition, or the L2 loss for AU intensity estimation. The importance loss L imp is a classical solution  [18, 58]  in MoE, used to prevent the collapse caused by the Top-k strategy. The global & local loss L global and L local are employed to enhance the optimal utilization of experts  [49] .\n\nImportance Loss. The importance loss L imp , promotes a balanced distribution of the gating weights among the experts. More formally, for a given input x in the batch B, the loss L imp is defined by the squared coefficient of variation:\n\nGlobal & local Loss. To utilize each expert optimally, the global loss L global , maximizes the marginal entropy, aiming for wide expert use from a global perspective. On the other hand, the local loss L local , encourages low entropy in focused routing weights, fostering specialization of each expert in specific tasks from a local perspective. Formally, p(E xp |x) ∈ R E is the probability distribution of the router over E xp experts, L global and L local are defined as:\n\nwhere\n\nis the expert probability distribution averaged over the input x, and H(p) denotes the entropy.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "Experiment",
      "text": "Datasets. Previous works on collecting datasets (e.g., BP4D  [95] , BP4D+  [99] , DISFA  [47] , AffectNet  [48]  and RAF-DB  [34] ) make significant contributions to academic communities. We evaluate the AU analysis task on BP4D, BP4D+, DISFA and the FER task on AffectNet and RAF-DB. BP4D consists of 328 video clips from 41 participants. About 140,000 frames are annotated with the occurrence or absence of 12 AUs for AU detection and the intensity of 5 AUs for AU intensity estimation. BP4D+ contains 1,400 video clips from 140 participants. About 198,000 frames are annotated with the same AUs in BP4D for AU detection. DISFA contains about 131,000 frames from 27 video clips. Each frame of DISFA is annotated with the occurrence or absence of 8 AUs for AU detection and the intensity of 12 AUs for AU intensity estimation. AffectNet, we use 7 basic emotions, selecting approximately 280,000 and 3,500 images in total for training and testing, respectively. RAF-DB, the experiment's training set and test set sizes are 12,271 and 3,068, respectively.\n\nFor fair comparisons, our experiments use the same training and testing datasets, as well as evaluation criteria as other methods. In the AU detection task, our evaluation follows the prior works  [35, 57] , using three-fold crossvalidation, and the report results are the F1 averages from three-fold experiments. For AU intensity estimation task, we evaluate the model performance with intra-class correlation (ICC)  [60] , mean squared error (MSE), and mean absolute error (MAE). We report the accuracy in the FER task.\n\nTo pick up the appropriate target face, we intentionally select 10 individuals from BP4D&BP4D+, ensuring diversity in age, gender, and ethnicity, and used the individual with the best AU detection performance as the fixed target face I id t that can be seen in Fig.  1 . Implementation Details. The normalization network F nor is first pre-trained on CelebA-WebFace  [41]  and Emo135  [7]  datasets. The face images are aligned and cropped to a size of 256×256. Afterward, we fine-tune the network on the target AU and emotion datasets. We adopt Adam  [28]  optimizer with β 1 = 0, β 2 = 0.99, a learning rate of 0.0001, and a batch size of 8. We set λ rec = 10, λ perc = 5, λ id = 10, λ lm = 5000, λ exp = 5000, and λ eye = 10. For the classification network F cla , we performed fine-tuning for 40 epochs, and set λ imp = 0.001, λ g&l = 0.001. The settings for each MoE block are m = 4 and k = 2. The base learning rates are set to 1e -4, 1e -3, and 2e -5 for AU detection, AU intensity estimation, and FER tasks, respectively.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "We verify the effectiveness of our method on AU detection, AU intensity estimation, and facial emotion recognition tasks, as well as their cross-dataset tasks. AU detection task. Tab.1 and 2 report comparison results with those previous methods. Our method outperforms those baselines on all datasets with the average of AUs detection. Specifically, ours exceeds the SOTA of GLEE-Net  [94]  for over 3% on both BP4D and BP4D+ datasets, and exceeds the SOTA of\n\nLGRNet  [17]  for over 5% on the DISFA dataset, which validates our method on the AU detection task. AU intensity estimation. Tab.3 report comparison results with those previous methods. Ours surpasses all existing approaches in terms of MSE and MAE, and exceeds the SOTA of APs  [56]  by 0.19 in terms of ICC on the DISFA dataset, which validates our method on the AU intensity estimation task. Facial emotion recognition task. Tab.4 reports comparison results with previous methods. Ours outperforms these baselines in terms of accuracy on both datasets. Specifically, ours exceeds the SOTA of LA-Net  [74]  for 1.41% and 1.09% on RAF-DB and AffectNet, respectively, validating our method on the FER task.\n\nCross-dataset task. Tab.5 and 6 report comparison results of AU detection and FER with previous methods on cross-dataset tasks, respectively. Ours achieves the best performance, surpassing the SOTA of GLEE-Net  [94]  by 5.1% on AU detection, and outperforming RANDA  [24]  by 6.09% and 11.2% on the 'R→A' and 'A→R' tasks of FER, respectively. Due to the normalization of task-irrelevant noise, domain barriers between datasets are reduced, which highlights the benefits of our method for cross-domain tasks.\n\nThe above experiments validate our framework being unified for those tasks, as identity normalization addresses the interference of task-irrelevant noise, such as identity, pose, and background, etc. More experiments on identity normalization are described below.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Analysis On Identity Normalization",
      "text": "We address the following Idn questions: A1. Is Idn effective? A2. How does the normalization network perform? A3. What is the difference between Idn and data augmentation? A4. Why use normalized images instead of expression features for the classification network? A1. Impact of Idn. Tab.7 reports the results of different FEA tasks w/ and w/o Idn. The results show that the performance w/ Idn consistently outperforms that w/o Idn in all tasks. For example, w/ Idn, there are improvements of 2.1%, 1.9%, and 2.5% in AU detection on BP4D, BP4D+, and DISFA, 3.73% and 1.4% increase in FER on RAF-DB and AffectNet, highlighting the benefits of Idn. In addition, Fig.  4  provides a detailed presentation of the normalization and AU detection results for different target identities. It is clear that using several target identities are validated to consistently yields better results than w/o Idn. Moreover, our study indicates that the selection of target identities contributes differently to performance enhancement, and certain identities are unsuitable as target faces. For example, w/ 'ID3' resulted in a performance decrease of 1.1% compared to w/ 'ID6', which we attribute to the confusion in facial expressions. In Fig.  4 , the non-candidate target faces in the blue box are labeled as having no AUs present, but these faces still exhibit raised eyebrows or lip corner depressors, leading to confusion in the expressions of their normalized faces. In contrast, as shown in the orange box in Fig.  4 , these faces have minimal confusion in facial Method FDRL  [55]  RANDA  [24]  TransFER  [77]      expressions, ensuring high discriminability and performance enhancement. They can be considered as candidate target faces.\n\nOur method is compared to multi-task methods, based on BP4D and Affect-Net for fair comparison, ours exceeds JPML  [103]  for 23.4% and MT-VGG  [29]  for 8.69% under BP4D and AffectNet, respectively. Compared to identity-invariant methods, ours exceeds the SOTA of DLN  [93]  for 4.99% under AffectNet and exceeds the SOTA of IPD-FER  [25]  for 4.16% under RAF-DB. This clearly illustrates the significant benefits of Idn for AU analysis and FER tasks. A2. Normalization performance. Fig.  5  visualizes the results of our F nor and other available methods enabling identity normalization. Our method offers better expression consistency between output and original faces than PIRenderer  [54] , Face2Face ρ  [81] , and StyleHEAT  [82] . Additionally, a subjective evaluation with 100 volunteers rating 50 randomly selected normalized images on a scale of 0 to 5 shows our method (4.6) significantly outperforms PIRenderer (2.   [90] , cutout  [14] , and cutmix  [84] . Ours outperforms these on both AU detection and FER tasks, e.g., outperforming mixup by 2.1%, 1.9%, 1.9%, 3.73%, and 1.4% on the five datasets, respectively. Unlike traditional data augmentation methods that merely boost the diversity of the training samples, our method directly impacts the test samples by additional normalized images.  (2) Moreover, our method is also compared with GLEE-Net  [94] , which is the SOTA method designed for AU detection using features from E exp and additional 3DMM. Tab.9 shows that, on average, ours also performs better than GLEE-Net by 4.6%. (3) For a fair comparison, we replace GLEE-Net's features with E exp &E eye , ours still leads by 4.5%. The above results show the superiority of normalized images for classification.\n\nIt is interesting to discuss the disparities between normalized images and expression features. Compared to highly abstract expression features, images encompass more structured details at pixel level. Furthermore, the used MAE can refine the facial representation from those normalized images due to its training on large-scale datasets. These factors contribute to the superior performance of normalized images over expression features.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "Ablation studies are conducted to validate expression consistency loss in the normalization network and each component in the classification Network. Expression consistency loss. The expression loss L exp and eyebrow loss L eye in F nor are used to ensure expression consistency. Tab.10 validates their effectiveness, presenting the results for w/ both L exp and L eye , as well as only w/ L exp in the AU detection task. The result shows that, in the eyebrow regions, using both L exp and L eye yields an average 0.5% higher than merely using L exp . While L exp guides the full face, L eye enhances the eyebrow regions. Moreover, without L exp , Idn would fail, as L eye solely concentrates on the eyebrow regions. Fig.  6  further visualizes their impacts by a few samples.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "This paper provides a novel insight and a unified framework called Norface for AU detection, AU intensity estimation, and FER tasks, aiming to remove the task-irrelevant noise by identity normalization that normalizes all images to a common identity with consistent pose and background, and improves expression classification. Extensive quantitative and qualitative experiments demonstrate the superior performance of Norface for AU analysis and FER tasks as well as their cross-dataset tasks.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Identity normalization. The figure demonstrates that various original images",
      "page": 1
    },
    {
      "caption": "Figure 2: Structure of the proposed Norface. Norface comprises two key stages: identity",
      "page": 4
    },
    {
      "caption": "Figure 3: Identity normalization of AU and emotion datasets as original faces. The figure",
      "page": 7
    },
    {
      "caption": "Figure 4: Normalization results for different",
      "page": 12
    },
    {
      "caption": "Figure 5: Visualization of different methods",
      "page": 12
    },
    {
      "caption": "Figure 6: Visualization of the effects of the",
      "page": 14
    },
    {
      "caption": "Figure 7: Comparison of different numbers",
      "page": 14
    },
    {
      "caption": "Figure 8: Router decision statistics for the",
      "page": 14
    },
    {
      "caption": "Figure 9: Identity normalization with various target faces. Please zoom in for more de-",
      "page": 23
    },
    {
      "caption": "Figure 10: Identity normalization of BP4D and BPD4+ datasets as original faces. The",
      "page": 24
    },
    {
      "caption": "Figure 11: Identity normalization of AffectNet and RAF-DB datasets as original faces.",
      "page": 25
    },
    {
      "caption": "Figure 12: Identity normalization of DISFA dataset as original faces. The figure presents",
      "page": 26
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison of AU detection results on BP4D and BP4D+ in terms of F1",
      "data": [
        {
          "Dataset": "BP4D",
          "Method": "LP-Net [51]\nJAA-Net [57]\nHMP-PS [62]\nSEV-Net [79]\nMHSA-FFN [23]\nCaFNet [9]\nChang et al. [6]\nGLEE-Net [94]\nNorface (Ours)",
          "AU1\nAU2\nAU4\nAU6\nAU7\nAU10\nAU12\nAU14\nAU15\nAU17\nAU23\nAU24\nAvg.": "43.4\n38.0\n54.2\n77.1\n76.7\n83.8\n87.2\n63.3\n45.3\n60.5\n48.1\n54.2\n61.0\n53.8\n47.8\n58.2\n78.5\n75.8\n82.7\n88.2\n63.7\n43.3\n61.8\n45.6\n49.9\n62.4\n53.1\n46.1\n56.0\n76.5\n76.9\n82.1\n86.4\n64.8\n51.5\n63.0\n[49.9]\n54.5\n63.4\n81.9\n87.8\n58.2\n[50.4]\n58.3\n73.9\n87.5\n61.6\n52.6\n62.2\n44.6\n47.6\n63.9\n51.7\n49.3\n[61.0]\n77.8\n79.5\n82.9\n86.3\n[67.6]\n51.9\n63.0\n43.7\n56.3\n64.2\n55.1\n49.3\n57.7\n78.3\n78.6\n85.1\n86.2\n67.4\n52.0\n64.4\n48.3\n56.2\n64.9\n80.7\n53.3\n47.4\n56.2\n79.4\n85.1\n[89.0]\n67.4\n[55.9]\n61.9\n48.5\n49.0\n64.5\n[60.6]\n44.4\n61.0\n[80.6]\n78.7\n85.4\n88.1\n64.9\n53.7\n[65.1]\n47.7\n[58.5]\n[65.7]\n60.9\n55.3\n67.4\n90.7\n69.8\n58.1\n66.2\n54.9\n61.8\n69.3\n79.7\n[79.9]\n[87.5]",
          "∆ ↑": "0.0\n+1.4\n+2.4\n+2.9\n+3.2\n+3.9\n+3.5\n+4.7\n+8.3"
        },
        {
          "Dataset": "BP4D+",
          "Method": "ML-GCN [11]\nMS-CAM [83]\nSEV-Net [79]\nGLEE-Net [94]\nNorface (Ours)",
          "AU1\nAU2\nAU4\nAU6\nAU7\nAU10\nAU12\nAU14\nAU15\nAU17\nAU23\nAU24\nAvg.": "40.2\n36.9\n32.5\n84.8\n88.9\n89.6\n89.3\n81.2\n[53.3]\n43.1\n55.9\n28.3\n60.3\n90.9\n60.9\n38.3\n37.6\n25.2\n85.0\n[90.9]\n89.0\n81.5\n40.6\n58.2\n28.0\n60.5\n55.6\n47.9\n40.8\n31.2\n[86.9]\n87.5\n89.7\n88.9\n[82.6]\n39.9\n[59.4]\n27.1\n61.5\n54.2\n46.3\n[38.1]\n86.2\n87.6\n90.4\n[89.5]\n81.3\n46.3\n47.4\n57.6\n[39.6]\n[63.7]\n51.7\n88.1\n91.3\n90.1\n83.3\n61.6\n45.4\n66.7\n[52.2]\n[46.0]\n[89.0]\n50.4\n[51.1]",
          "∆ ↑": "0.0\n+0.2\n+1.2\n+3.4\n+6.4"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: Comparison of AU intensity estimation results on BP4D and DISFA.",
      "data": [
        {
          "Metric": "ICC ↑",
          "Method": "ISIR [50]\nHR [52]\nSCC-Heatmap [16]\nAPs [56]\nNorface (Ours)",
          "AU on BP4D\n6\n10\n12\n14\n17\nAvg.": ".79\n.80\n.86\n.71\n.44\n.72\n.82\n.80\n.86\n.69\n.51\n.73\n.74\n.82\n.86\n.68\n.51\n.72\n.74\n.82\n.80\n.86\n.69\n.51\n.74\n.81\n.74\n.90\n.50\n.74",
          "AU on DISFA\n1\n2\n4\n5\n6\n9\n12\n15\n17\n20\n25\n26\nAvg.": "-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n.35\n.19\n.78\n.73\n.52\n.65\n.81\n.49\n.61\n.28\n.92\n.67\n.58\n.73\n.44\n.74\n.06\n.27\n.51\n.71\n.04\n.37\n.04\n.94\n.78\n.47\n.35\n.19\n.78\n.73\n.52\n.65\n.81\n.49\n.61\n.28\n.92\n.67\n.58\n.67\n.72\n.68\n.77\n.68\n.59\n.56\n.87\n.54\n.65\n.34\n.96\n.70"
        },
        {
          "Metric": "MSE ↓",
          "Method": "ISIR [50]\nHR [52]\nAPs [56]\nNorface (Ours)",
          "AU on BP4D\n6\n10\n12\n14\n17\nAvg.": ".83\n.80\n.62\n1.14\n.84\n.85\n.68\n.80\n.79\n.98\n.61\n.78\n.72\n.84\n.60\n1.13\n.57\n.77\n.73\n.71\n.95\n.50\n1.02\n.47",
          "AU on DISFA\n1\n2\n4\n5\n6\n9\n12\n15\n17\n20\n25\n26\nAvg.": "-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n.41\n.37\n.70\n.08\n.44\n.30\n.29\n.14\n.26\n.16\n.24\n.39\n.32\n.68\n.59\n.40\n.03\n.49\n.15\n.26\n.13\n.22\n.20\n.35\n.17\n.30\n.22\n.22\n.19\n.46\n.03\n.41\n.30\n.24\n.09\n.17\n.14\n.15\n.29"
        },
        {
          "Metric": "MAE ↓",
          "Method": "BORMIR [97]\nKBSS [96]\nSCC-Heatmap [56]\nNorface (Ours)",
          "AU on BP4D\n6\n10\n12\n14\n17\nAvg.": ".85\n.90\n.68\n1.05\n.79\n.85\n.65\n.65\n.48\n.98\n.63\n.66\n.61\n.56\n.52\n.73\n.50\n.58\n.53\n.48\n.56\n.39\n.82\n.40",
          "AU on DISFA\n1\n2\n4\n5\n6\n9\n12\n15\n17\n20\n25\n26\nAvg.": ".88\n.78\n1.24\n.59\n.77\n.78\n.76\n.56\n.72\n.63\n.90\n.88\n.79\n.48\n.49\n.57\n.08\n.26\n.22\n.33\n.15\n.44\n.22\n.43\n.36\n.33\n.16\n.16\n.27\n.03\n.25\n.13\n.32\n.15\n.20\n.09\n.30\n.32\n.20\n.17\n.14\n.12\n.30\n.03\n.29\n.17\n.22\n.09\n.14\n.08\n.17\n.26"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Comparison of AU intensity estimation results on BP4D and DISFA.",
      "data": [
        {
          "Method": "AU1\nAU2\nAU4\nAU6\nAU7\nAU10\nAU12\nAU14\nAU15\nAU17\nAU23\nAU24",
          "EAC-Net [35]": "38.0\n37.5\n32.6\n82.0\n83.4\n87.1\n85.1\n62.1\n[44.5]\n43.6\n45.0\n32.8",
          "JAA-Net [57] GLEE-Net [94]": "[39.8]\n[37.9]\n[41.6]\n[83.4]\n[88.2]\n90.2\n86.9\n[76.6]\n48.3\n42.9\n47.7\n29.8",
          "Norface (Ours)": "48.2\n39.1\n47.1\n83.5\n89.7\n[89.4]\n[86.3]\n81.1\n43.2\n[45.4]\n63.2\n[35.1]"
        },
        {
          "Method": "Avg.",
          "EAC-Net [35]": "56.1",
          "JAA-Net [57] GLEE-Net [94]": "[57.5]",
          "Norface (Ours)": "62.6"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial expression recognition by using a disentangled identity-invariant expression representation",
      "authors": [
        "K Ali",
        "C Hughes"
      ],
      "year": "2021",
      "venue": "2020 25th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "2",
      "title": "Learning facial expression-aware global-to-local representation for robust action unit detection",
      "authors": [
        "R An",
        "A Jin",
        "W Chen",
        "W Zhang",
        "H Zeng",
        "Z Deng",
        "Y Ding"
      ],
      "year": "2024",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Masked autoencoders enable efficient knowledge distillers",
      "authors": [
        "Y Bai",
        "Z Wang",
        "J Xiao",
        "C Wei",
        "H Wang",
        "A Yuille",
        "Y Zhou",
        "C Xie"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "4",
      "title": "Identity-free facial expression recognition using conditional generative adversarial network",
      "authors": [
        "J Cai",
        "Z Meng",
        "A Khan",
        "J O'reilly",
        "Z Li",
        "S Han",
        "Y Tong"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "5",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "6",
      "title": "Knowledge-driven self-supervised representation learning for facial action unit recognition",
      "authors": [
        "Y Chang",
        "S Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Semantic-rich facial emotional expression recognition",
      "authors": [
        "K Chen",
        "X Yang",
        "C Fan",
        "W Zhang",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Label distribution learning on auxiliary label space graphs for facial expression recognition",
      "authors": [
        "S Chen",
        "J Wang",
        "Y Chen",
        "Z Shi",
        "X Geng",
        "Y Rui"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Cafgraph: Context-aware facial multi-graph representation for facial action unit recognition",
      "authors": [
        "Y Chen",
        "D Chen",
        "Y Wang",
        "T Wang",
        "Y Liang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Understanding and mitigating annotation bias in facial expression recognition",
      "authors": [
        "Y Chen",
        "J Joo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Multi-label image recognition with graph convolutional networks",
      "authors": [
        "Z Chen",
        "X Wei",
        "P Wang",
        "Y Guo"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "Knowledge augmented deep neural networks for joint facial expression and action unit recognition",
      "authors": [
        "Z Cui",
        "T Song",
        "Y Wang",
        "Q Ji"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "J Deng",
        "J Guo",
        "N Xue",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Improved regularization of convolutional neural networks with cutout",
      "authors": [
        "T Devries",
        "G Taylor"
      ],
      "year": "2017",
      "venue": "Improved regularization of convolutional neural networks with cutout",
      "arxiv": "arXiv:1708.04552"
    },
    {
      "citation_id": "15",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "16",
      "title": "Facial action unit intensity estimation via semantic correspondence learning with dynamic graph convolution",
      "authors": [
        "Y Fan",
        "J Lam",
        "V Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Local global relational network for facial action units recognition",
      "authors": [
        "X Ge",
        "P Wan",
        "H Han",
        "J Jose",
        "Z Ji",
        "Z Wu",
        "X Liu"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "18",
      "title": "Fastmoe: A fast mixtureof-expert training system",
      "authors": [
        "J He",
        "J Qiu",
        "A Zeng",
        "Z Yang",
        "J Zhai",
        "J Tang"
      ],
      "year": "2021",
      "venue": "Fastmoe: A fast mixtureof-expert training system",
      "arxiv": "arXiv:2103.13262"
    },
    {
      "citation_id": "19",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "K He",
        "X Chen",
        "S Xie",
        "Y Li",
        "P Dollár",
        "R Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Depth-aware generative adversarial network for talking head video generation",
      "authors": [
        "F Hong",
        "L Zhang",
        "L Shen",
        "D Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "Ca-fer: Mitigating spurious correlation with counterfactual attention in facial expression recognition",
      "authors": [
        "P Huang",
        "H Xie",
        "H Huang",
        "H Shuai",
        "W Cheng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Identity-aware facial expression recognition via deep metric learning based on synthesized images",
      "authors": [
        "W Huang",
        "S Zhang",
        "P Zhang",
        "Y Zha",
        "Y Fang",
        "Y Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "G Jacob",
        "B Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Region attention enhanced unsupervised cross-domain facial emotion recognition",
      "authors": [
        "Y Ji",
        "Y Hu",
        "Y Yang",
        "H Shen"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "25",
      "title": "Disentangling identity and pose for facial expression recognition",
      "authors": [
        "J Jiang",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Contrastive adversarial learning for person independent facial emotion recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Optimal transport-based identity matching for identityinvariant facial expression recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "29",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "30",
      "title": "Latent-ofer: Detect, mask, and reconstruct with latent vectors for occluded facial expression recognition",
      "authors": [
        "I Lee",
        "E Lee",
        "S Yoo"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "31",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "33",
      "title": "A deeper look at facial expression dataset bias",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "35",
      "title": "Eac-net: Deep nets with enhancing and cropping for facial action unit detection",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu",
        "L Yin"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "36",
      "title": "Geometric gan",
      "authors": [
        "J Lim",
        "J Ye"
      ],
      "year": "2017",
      "venue": "Geometric gan",
      "arxiv": "arXiv:1705.02894"
    },
    {
      "citation_id": "37",
      "title": "Feda: Fine-grained emotion difference analysis for facial expression recognition",
      "authors": [
        "H Liu",
        "H Cai",
        "Q Lin",
        "X Zhang",
        "X Li",
        "H Xiao"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "38",
      "title": "Adaptive multilayer perceptual attention network for facial expression recognition",
      "authors": [
        "H Liu",
        "H Cai",
        "Q Lin",
        "X Li",
        "H Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "39",
      "title": "Learning from more: Combating uncertainty cross-multidomain for facial expression recognition",
      "authors": [
        "H Liu",
        "H Cai",
        "Q Lin",
        "X Li",
        "H Xiao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "40",
      "title": "Towards a simultaneous and granular identity-expression control in personalized face generation",
      "authors": [
        "R Liu",
        "B Ma",
        "W Zhang",
        "Z Hu",
        "C Fan",
        "T Lv",
        "Y Ding",
        "X Cheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "42",
      "title": "Normalized avatar synthesis using stylegan and perceptual refinement",
      "authors": [
        "H Luo",
        "K Nagano",
        "H Kung",
        "Q Xu",
        "Z Wang",
        "L Wei",
        "L Hu",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts",
      "authors": [
        "J Ma",
        "Z Zhao",
        "X Yi",
        "J Chen",
        "L Hong",
        "E Chi"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "44",
      "title": "Gafet: Learning geometry-aware facial expression translation from in-the-wild images",
      "authors": [
        "T Ma",
        "B Li",
        "Q He",
        "J Dong",
        "T Tan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "45",
      "title": "Implicit warping for animation with image sets",
      "authors": [
        "A Mallya",
        "T Wang",
        "M Liu"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "46",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "B Martinez",
        "M Valstar",
        "B Jiang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "47",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "48",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "49",
      "title": "Multimodal contrastive learning with limoe: the language-image mixture of experts",
      "authors": [
        "B Mustafa",
        "C Riquelme",
        "J Puigcerver",
        "R Jenatton",
        "N Houlsby"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "50",
      "title": "",
      "authors": [
        "Liu"
      ],
      "venue": ""
    },
    {
      "citation_id": "51",
      "title": "Facial action unit intensity prediction via hard multi-task metric learning for kernel regression",
      "authors": [
        "J Nicolle",
        "K Bailly",
        "M Chetouani"
      ],
      "year": "2015",
      "venue": "11th ieee international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "52",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "53",
      "title": "A transfer learning approach to heatmap regression for action unit intensity estimation",
      "authors": [
        "I Ntinou",
        "E Sanchez",
        "A Bulat",
        "M Valstar",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Scale-mae: A scale-aware masked autoencoder for multiscale geospatial representation learning",
      "authors": [
        "C Reed",
        "R Gupta",
        "S Li",
        "S Brockman",
        "C Funk",
        "B Clipp",
        "K Keutzer",
        "S Candido",
        "M Uyttendaele",
        "T Darrell"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "55",
      "title": "Pirenderer: Controllable portrait image generation via semantic neural rendering",
      "authors": [
        "Y Ren",
        "G Li",
        "Y Chen",
        "T Li",
        "S Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "56",
      "title": "Feature decomposition and reconstruction learning for effective facial expression recognition",
      "authors": [
        "D Ruan",
        "Y Yan",
        "S Lai",
        "Z Chai",
        "C Shen",
        "H Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "57",
      "title": "Affective processes: stochastic modelling of temporal context for emotion and facial expression recognition",
      "authors": [
        "E Sanchez",
        "M Tellamekala",
        "M Valstar",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "Jaa-net: joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "59",
      "title": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "authors": [
        "N Shazeer",
        "A Mirhoseini",
        "K Maziarz",
        "A Davis",
        "Q Le",
        "G Hinton",
        "J Dean"
      ],
      "year": "2017",
      "venue": "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer",
      "arxiv": "arXiv:1701.06538"
    },
    {
      "citation_id": "60",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "J She",
        "Y Hu",
        "H Shi",
        "J Wang",
        "Q Shen",
        "T Mei"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "61",
      "title": "Intraclass correlations: uses in assessing rater reliability",
      "authors": [
        "P Shrout",
        "J Fleiss"
      ],
      "year": "1979",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "62",
      "title": "First order motion model for image animation",
      "authors": [
        "A Siarohin",
        "S Lathuilière",
        "S Tulyakov",
        "E Ricci",
        "N Sebe"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "63",
      "title": "Hybrid message passing with performancedriven structures for facial action unit detection",
      "authors": [
        "T Song",
        "Z Cui",
        "W Zheng",
        "Q Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "64",
      "title": "High-resolution representations for labeling pixels and regions",
      "authors": [
        "K Sun",
        "Y Zhao",
        "B Jiang",
        "T Cheng",
        "B Xiao",
        "D Liu",
        "Y Mu",
        "X Wang",
        "W Liu",
        "J Wang"
      ],
      "year": "2019",
      "venue": "High-resolution representations for labeling pixels and regions",
      "arxiv": "arXiv:1904.04514"
    },
    {
      "citation_id": "65",
      "title": "Emotion-aware contrastive learning for facial action unit detection",
      "authors": [
        "X Sun",
        "J Zeng",
        "S Shan"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "66",
      "title": "Idennet: Identity-aware facial action unit detection",
      "authors": [
        "C Tu",
        "C Yang",
        "J Hsu"
      ],
      "year": "2019",
      "venue": "14th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "67",
      "title": "Multi-task learning for dense prediction tasks: A survey",
      "authors": [
        "S Vandenhende",
        "S Georgoulis",
        "W Van Gansbeke",
        "M Proesmans",
        "D Dai",
        "L Van Gool"
      ],
      "year": "2021",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "68",
      "title": "Pyramid with super resolution for in-the-wild facial expression recognition",
      "authors": [
        "T Vo",
        "G Lee",
        "H Yang",
        "S Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "69",
      "title": "Identity-and pose-robust facial expression recognition through adversarial feature learning",
      "authors": [
        "C Wang",
        "S Wang",
        "G Liang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 27th ACM international conference on multimedia"
    },
    {
      "citation_id": "70",
      "title": "Lipformer: High-fidelity and generalizable talking face generation with a pre-learned facial codebook",
      "authors": [
        "J Wang",
        "K Zhao",
        "S Zhang",
        "Y Zhang",
        "Y Shen",
        "D Zhao",
        "J Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "71",
      "title": "Suppressing uncertainties for largescale facial expression recognition",
      "authors": [
        "K Wang",
        "X Peng",
        "J Yang",
        "S Lu",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "72",
      "title": "One-shot free-view neural talking-head synthesis for video conferencing",
      "authors": [
        "T Wang",
        "A Mallya",
        "M Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "73",
      "title": "Latent image animator: Learning to animate images via latent space navigation",
      "authors": [
        "Y Wang",
        "D Yang",
        "F Bremond",
        "A Dantcheva"
      ],
      "year": "2022",
      "venue": "Latent image animator: Learning to animate images via latent space navigation",
      "arxiv": "arXiv:2203.09043"
    },
    {
      "citation_id": "74",
      "title": "Ganhead: Towards generative animatable neural head avatars",
      "authors": [
        "S Wu",
        "Y Yan",
        "Y Li",
        "Y Cheng",
        "W Zhu",
        "K Gao",
        "X Li",
        "G Zhai"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "75",
      "title": "La-net: Landmark-aware learning for reliable facial expression recognition under label noise",
      "authors": [
        "Z Wu",
        "J Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "76",
      "title": "Au-assisted graph attention convolutional network for micro-expression recognition",
      "authors": [
        "H Xie",
        "L Lo",
        "H Shuai",
        "W Cheng"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "77",
      "title": "Facial expression recognition with two-branch disentangled generative adversarial network",
      "authors": [
        "S Xie",
        "H Hu",
        "Y Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "78",
      "title": "Transfer: Learning relation-aware facial expression representations with transformers",
      "authors": [
        "F Xue",
        "Q Wang",
        "G Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "79",
      "title": "Facial expression recognition by de-expression residue learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "80",
      "title": "Exploiting semantic embedding and visual feature for facial action unit detection",
      "authors": [
        "H Yang",
        "L Yin",
        "Y Zhou",
        "J Gu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Con-H Liu. et al. ference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "81",
      "title": "Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks",
      "authors": [
        "H Yang",
        "Z Zhang",
        "L Yin"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "82",
      "title": "Face2face ρ: Real-time high-resolution one-shot face reenactment",
      "authors": [
        "K Yang",
        "K Chen",
        "D Guo",
        "S Zhang",
        "Y Guo",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Face2face ρ: Real-time high-resolution one-shot face reenactment"
    },
    {
      "citation_id": "83",
      "title": "Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan",
      "authors": [
        "F Yin",
        "Y Zhang",
        "X Cun",
        "M Cao",
        "Y Fan",
        "X Wang",
        "Q Bai",
        "B Wu",
        "J Wang",
        "Y Yang"
      ],
      "year": "2022",
      "venue": "Styleheat: One-shot high-resolution editable talking face generation via pre-trained stylegan"
    },
    {
      "citation_id": "84",
      "title": "Cross-modality attention with semantic graph embedding for multi-label classification",
      "authors": [
        "R You",
        "Z Guo",
        "L Cui",
        "X Long",
        "Y Bao",
        "S Wen"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "85",
      "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
      "authors": [
        "S Yun",
        "D Han",
        "S Oh",
        "S Chun",
        "J Choe",
        "Y Yoo"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "86",
      "title": "Face2exp: Combating data biases for facial expression recognition",
      "authors": [
        "D Zeng",
        "Z Lin",
        "X Yan",
        "Y Liu",
        "F Wang",
        "B Tang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "87",
      "title": "Paste you into game: Towards expression and identity consistency face swapping",
      "authors": [
        "H Zeng",
        "W Zhang",
        "K Chen",
        "Z Zhang",
        "L Li",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "2022 IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "88",
      "title": "Face identity and expression consistency for game character face swapping",
      "authors": [
        "H Zeng",
        "W Zhang",
        "K Chen",
        "Z Zhang",
        "L Li",
        "Y Ding"
      ],
      "year": "2023",
      "venue": "Computer Vision and Image Understanding"
    },
    {
      "citation_id": "89",
      "title": "Flowface: Semantic flow-guided shape-aware face swapping",
      "authors": [
        "H Zeng",
        "W Zhang",
        "C Fan",
        "T Lv",
        "S Wang",
        "Z Zhang",
        "B Ma",
        "L Li",
        "Y Ding",
        "X Yu"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "90",
      "title": "Metaportrait: Identity-preserving talking head generation with fast personalized adaptation",
      "authors": [
        "B Zhang",
        "C Qi",
        "P Zhang",
        "B Zhang",
        "H Wu",
        "D Chen",
        "Q Chen",
        "Y Wang",
        "F Wen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "91",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "H Zhang",
        "M Cisse",
        "Y Dauphin",
        "D Lopez-Paz"
      ],
      "year": "2017",
      "venue": "mixup: Beyond empirical risk minimization",
      "arxiv": "arXiv:1710.09412"
    },
    {
      "citation_id": "92",
      "title": "Freenet: Multi-identity face reenactment",
      "authors": [
        "J Zhang",
        "X Zeng",
        "M Wang",
        "Y Pan",
        "L Liu",
        "Y Liu",
        "Y Ding",
        "C Fan"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "93",
      "title": "Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition",
      "authors": [
        "W Zhang",
        "Z Guo",
        "K Chen",
        "L Li",
        "Z Zhang",
        "Y Ding"
      ],
      "year": "2021",
      "venue": "Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition",
      "arxiv": "arXiv:2107.03708"
    },
    {
      "citation_id": "94",
      "title": "Learning a facial expression embedding disentangled from identity",
      "authors": [
        "W Zhang",
        "X Ji",
        "K Chen",
        "Y Ding",
        "C Fan"
      ],
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "95",
      "title": "Detecting facial action units from global-local fine-grained expressions",
      "authors": [
        "W Zhang",
        "L Li",
        "Y Ding",
        "W Chen",
        "Z Deng",
        "X Yu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "96",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "97",
      "title": "Weakly-supervised deep convolutional neural network learning for facial action unit intensity estimation",
      "authors": [
        "Y Zhang",
        "W Dong",
        "B Hu",
        "Q Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "98",
      "title": "Bilateral ordinal relevance multiinstance regression for facial action unit intensity estimation",
      "authors": [
        "Y Zhang",
        "R Zhao",
        "W Dong",
        "B Hu",
        "Q Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "99",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "100",
      "title": "Multimodal spontaneous emotion corpus for human behavior analysis",
      "authors": [
        "Z Zhang",
        "J Girard",
        "Y Wu",
        "X Zhang",
        "P Liu",
        "U Ciftci",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "H Yang"
      ],
      "year": "2016",
      "venue": "Multimodal spontaneous emotion corpus for human behavior analysis"
    },
    {
      "citation_id": "101",
      "title": "Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset",
      "authors": [
        "Z Zhang",
        "L Li",
        "Y Ding",
        "C Fan"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "102",
      "title": "Thin-plate spline motion model for image animation",
      "authors": [
        "J Zhao",
        "H Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "103",
      "title": "Learning facial action units from web images with scalable weakly supervised clustering",
      "authors": [
        "K Zhao",
        "W Chu",
        "A Martinez"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "104",
      "title": "Joint patch and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W Chu",
        "F De La Torre",
        "J Cohn",
        "H Zhang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "105",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W Chu",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    }
  ]
}