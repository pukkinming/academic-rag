{
  "paper_id": "2410.09767v3",
  "title": "Libeer: A Comprehensive Benchmark And Algorithm Library For Eeg-Based Emotion Recognition",
  "published": "2024-10-13T07:51:39Z",
  "authors": [
    "Huan Liu",
    "Shusen Yang",
    "Yuzhe Zhang",
    "Mengze Wang",
    "Fanyu Gong",
    "Chengxi Xie",
    "Guanjian Liu",
    "Zejun Liu",
    "Yong-Jin Liu",
    "Bao-Liang Lu",
    "Dalin Zhang"
  ],
  "keywords": [
    "Benchmark",
    "EEG-based emotion recognition",
    "Fair comparison",
    "Open source library"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based emotion recognition (EER) has gained significant attention due to its potential for understanding and analyzing human emotions. While recent advancements in deep learning techniques have substantially improved EER, the field lacks a convincing benchmark and comprehensive open-source libraries. This absence complicates fair comparisons between models and creates reproducibility challenges for practitioners, which collectively hinder progress. To address these issues, we introduce LibEER, a comprehensive benchmark and algorithm library designed to facilitate fair comparisons in EER. LibEER carefully selects popular and powerful baselines, harmonizes key implementation details across methods, and provides a standardized codebase in PyTorch. By offering a consistent evaluation framework with standardized experimental settings, LibEER enables unbiased assessments of seventeen representative deep learning models for EER across the six most widely used datasets. Additionally, we conduct a thorough, reproducible comparison of model performance and efficiency, providing valuable insights to guide researchers in the selection and design of EER models. Moreover, we make observations and in-depth analysis on the experiment results and identify current challenges in this community. We hope that our work will not only lower entry barriers for newcomers to EEG-based emotion recognition but also contribute to the standardization of research in this domain, fostering steady development. The library and source code are publicly available at https://github.com/XJTU-EEG/LibEER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background Of Eer",
      "text": "In this section, we present the general background of EEG, including data collection protocol and formal problem definition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Data Collection Protocol",
      "text": "The general protocol for EER data collection is illustrated in Fig.  1 . Each data collection begins with a start hint shown to the participant. Afterwards, various stimuli, such as images, audio, or video, are presented to evoke specific emotions in the participant, such as happiness, sadness, or fear. After each stimulus, the participant is asked to self-assess their emotional state, providing a subjective emotion score based on their experience during the stimulus. This process is repeated a set number of times with different stimuli to collect enough data.\n\nIn this context, the participant exposed to the stimuli, from whom data is subsequently collected, is commonly referred to as a subject. A continuous segment of EEG data collected from a subject under a specific stimulus condition is called a trial, typically lasting between one and five minutes. Between trials, the subject takes a rest. A series of trials is termed a session. Since EEG data from the same subject, even in the same emotional state, may vary significantly over time, datasets generally include several sessions, with an interval of more than 24 hours between consecutive ones. Given the high cost of EEG data acquisition, a trial is often divided into segments. Two consecutive segments within a trial can be overlapping or non-overlapping. Each segment is called a sample, serving as the fundamental unit for EER. Notably, within a single trial, all samples should be designated for either training or test. Splitting samples from the same trial across training and test sets is impractical for real-world applications and considered invalid.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Problem Formulation",
      "text": "Given an EER dataset D, a sample is denoted as x i,j,k,q ∈ R c×d , where i, j, k, and q refer to the i-th subject, j-th session, k-th trial, and q-th sample within the trial, respectively. Here,\n\nwhere y denotes the emotional state and F is a mapping function. SOTA map functions are usually neural networks trained on a training set D train ⊂ D, and evaluated on a test set D test ⊂ D. Additionally, a validation set D valid ⊂ D is often employed to select the best model. These subsets are required to be mutually exclusive, that is",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Benchmark Building",
      "text": "A comprehensive, well-structured, and transparent benchmark is essential for fair comparisons of research methodologies in the field-a need that remains unmet in the EER domain. In this section, we present our benchmark, covering baseline selection, preprocessing, experimental setups, datasets, and evaluation protocols. For ease of reference, TABLE I provides a summary of the key information in our EER benchmark.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Baseline Selection",
      "text": "We select representative models for the benchmark. Specifically, we set up the following criteria: • We first collected a total of 60 high-quality papers published in the past year in the field of EEG-based Emotion Recognition (EER) from reputable journals and conferences (such as TAFFC, JBHI, and ICASSP). • We then compiled a list of 205 comparative methods cited across these 60 papers. • After that, we ranked these 205 methods based on their frequency as comparison methods in the 60 papers and selected the top 20 for replication. • These 20 methods, following the common practice in previous studies of categorizing deep learning models by their main architecture  [66] ,  [67] , are divided into five categories: CNN, RNN, GNN, DNN, and Transformer. Finally, we selected models with replication results within a 10% error margin compared to the original reported results, ensuring that each category includes at least one representative model.\n\nBased on these steps, we ultimately identified the twelve most representative works  [6] ,  [7] ,  [10] -  [18] ,  [43]  to include in LibEER. In addition, we include two classic transfer learning methods (DAN and DANN  [8] ) that have demonstrated superior performance in EER, as well as two recent stateof-the-art approaches (NSAL-DGAT  [20]  and PR-PL  [9] ). These methods are also categorized into the corresponding architecture-based groups according to their backbone model structures. TABLE II presents detailed information on these seventeen methods. In future work, we will continue to maintain and expand this collection by incorporating more recent and diverse methods. The following subsections will offer detailed descriptions of the five categories, as well as an examination of each selected method.\n\n1) CNN-based Methods: Convolutional Neural Networks (CNNs) are a type of feedforward neural network characterized by their convolutional operations. When applied to EEG signals, CNNs usually treat EEG data as a two-dimensional (2D) representation: one dimension corresponds to the channels, while the other dimension represents either the temporal or spectral features of the EEG data. This structure is illustrated in Fig.  2  (a), which could handle temporal/spectral and spatial information simultaneously. On the other hand, some work relies on 1D CNN to extract temporal and spatial features separately. LibEER includes three representative CNN-based methods: EEGNet  [11] , CDCN  [12] , and TSception  [13] .\n\n2) RNN-based Methods: Recurrent Neural Networks (RNNs) are inherently designed for sequential data, making them well-suited for extracting temporal features from EEG signals. For spatial feature extraction, RNNs treat channels as sequence nodes, as illustrated in Fig.  2 (b ). In practice, Long Short-Term Memory (LSTM) is a widely used variant of RNNs. Our benchmark includes three RNN-based methods: ACRNN  [14] , BiDANN  [43] , and R2G-STNN  [17] .\n\n3) GNN-based Methods: Graph Neural Network (GNN) is a type of neural network designed to handle data structured as graphs. A graph consists of nodes and edges, which can represent complex relationships and structures. When processing EEG data, the EEG channels are typically treated as nodes in a graph, as shown in Fig.  2 (c ). Since EEG data does not inherently contain information about the edges or the adjacency matrix, using graph neural networks to process EEG data often involves leveraging prior knowledge about the relationships between channels or using learnable adjacency matrices as the graph's adjacency matrix. Our benchmark includes five GNN-based methods: DGCNN  [15] , GCBNet  [18] , GCBNet BLS  [18] , RGNN  [16] , and NSAL-DGAT  [20] .\n\n4) DNN-based Methods: Deep Neural Network (DNN), in this paper, specifically refers to the feedforward fully connected neural network, also known as Fully Connected Neural Network (FNN) or Multi-Layer Perceptron (MLP). It consists of an input layer, multiple hidden layers composed of fully connected neurons with nonlinear activation functions, and an output layer. Unlike other deep learning architectures such as CNN, RNN, or GNN, DNN focuses on dense layer transformations without convolutional, recurrent, or graphbased structures. In the field of EER, DNN processes EEG data by performing feature transformation and mapping into a high-dimensional feature space, enabling the extraction and utilization of complex patterns within the data, as illustrated in Fig.  2 (d) . Our benchmark includes five DNN-based methods: DBN  [6] , DAN  [8] , DANN  [8] , MS-MDA  [7] , and PR-PL  [9] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "5)",
      "text": "Transformer-based Methods: Transformer uses a selfattention mechanism to capture dependencies between elements in a sequence, effectively handling long-range dependencies. As shown in Fig.  2  (e), the EEG data are typically segmented into multiple patches on channels, regions, or temporal segments when using transformers to process EEG data. These patches are then fed into the transformer model for further analysis. Our benchmark includes one Transformerbased methods: HSLT  [10] .\n\nB. Data preprocessing and Splitting 1) preprocessing: A significant challenge in reproducing extant studies on EER is the lack of a standardized data preprocessing protocol. Publicly available EER datasets are not immediately usable and require extensive preprocessing. Moreover, this critical information is rarely detailed in the literature, and many open-source packages do not provide the necessary data scripts. To address this issue, our benchmark proposes a comprehensive data preprocessing framework specifically designed for EER research. The procedure includes:  (1)  applying bandpass filtering between 0.3 and 50 Hz; (2) eliminating eye movement artifacts using Principal Component Analysis (PCA); (3) extracting DE features across five frequency bands-[0.5, 4],  [4, 8] ,  [8, 14] ,  [14, 30] , and  [30, 50] -followed by processing with a Linear Dynamic System (LDS); and (4) segmenting data using 1-second nonoverlapping sliding windows to enhance the dataset.\n\n2) Splitting: Data splitting is a critical process that delineates how a dataset is partitioned into training and test sets, significantly influencing model performance. However, prevailing practices in this domain often exhibit a lack of rationality and the absence of a coherent, standardized approach. For example, many current data-splitting methodologies fail to incorporate a validation set for model selection and do not consistently adhere to cross-trial configurations, resulting in unreliable experimental outcomes.\n\nTo mitigate these shortcomings, we propose a systematic and rational data-splitting methodology in our benchmark. Specifically, for tasks that are subject-dependent, we allocate each individual's data into training, validation, and test sets in a 0.6:0.2:0.2 ratio, adjusting as necessary for datasets that do not divide evenly. The SEED-V dataset contains relatively fewer trials and more classes, so the ratio was adjusted to 1:1:1. In the case of cross-subject tasks, we similarly partition subjects into training, validation, and test sets following the same 0.6:0.2:0.2 ratio. Furthermore, our benchmark rigorously adheres to the cross-trial principle, ensuring that samples from the same trial do not appear in both the training and test sets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Experimental Tasks",
      "text": "The field of EER encompasses various experimental tasks, each designed to address distinct challenges with corresponding models. The primary experimental tasks are categorized as follows: (1) Subject-dependent: This task evaluates model performance for individual subjects, requiring both training and testing data from the same subject. (2) Cross-subject: This task aims to generalize model performance across different  In previous studies, the definitions of subject-independent and cross-subject tasks have often overlapped, as subjectindependent is typically implemented using leave-one-subjectout cross-validation (LOSO-CV)  [15] ,  [16] . To avoid ambiguity, we explicitly adopt cross-subject as one of the core evaluation settings in our benchmark. Cross-subject strictly requires training and testing on entirely different subjects, emphasizing the model's generalization ability. This task setup is more aligned with real-world applications where models are expected to perform reliably on new, unseen users in largescale deployment scenarios. In addition, subject-dependent tasks are included to evaluate a model's performance under personalized settings, where training and testing are performed on the same subject. This setup is important for scenarios focusing on optimizing accuracy for specific individuals, such as personalized healthcare or user-specific emotion monitoring. Therefore, our benchmark concentrates on cross-subject and subject-dependent tasks as the most widely adopted and practically meaningful evaluation setups. Additionally, it is important to note that we treat data from different sessions of the same subject as if they originate from distinct subjects. We provide specific definitions for the subject-dependent and cross-subject tasks below.\n\nIn subject-dependent tasks, each task is conducted using data from a single subject and the data is split based on individual trials. We denote the training, validation, and test sets for subject S i as\n\ntest , respectively. These sets must satisfy two essential constraints:\n\n(2) These conditions guarantee that there is no overlap among the trials in the training, validation, and test sets, while ensuring that the union of these sets encompasses all available data from subject S i . Following the cross-trial principle, the\n\ntest can be defined as:\n\nThe EEG samples from\n\ntrain can be represented as\n\ntr ×c×d , where m\n\ntr represents the number of EEG sample of D (i) train , c denotes the number of electrode channels and d is the feature dimension. We denote the corresponding labels of\n\ntr . The subject-dependent task involves learning a mapping function f :\n\ntr with crossentropy loss that accurately predicts the emotion state of S i . The validation set\n\nval is used to tune hyperparameters and select the best model, while the test set\n\ntest is used to evaluate the final model and estimate its performance on unseen data.\n\nThe cross-subject task uses EEG data from all subjects. We denote the training, validation, ans test sets as D train , D val , and D test , respectively. These sets also satisfy two constraints:\n\nThese conditions ensure that there is no overlap subjects between three sets and the union of these sets covers all available data of the dataset. Following the cross-subject principle, the D train , D val , and D test can be defined as:\n\nAnalogously, the EEG samples from D train can be represented as X tr ∈ R mtr×c×d , where m tr represents the number of EEG sample of D train . We denote the corresponding labels of X tr as y tr ∈ R mtr . The cross-subject task learns a mapping function f : X tr → y tr with the cross-entropy loss. The validation set D val is used to tune hyperparameters and select the best model, while the test set D test is used to evaluate the final model and estimate its performance on unseen data.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Datasets",
      "text": "Research in the EER field exhibits variability in dataset selection and often lacks clarity regarding the specific data segments utilized. This study evaluates the performance of popular models using six datasets, which are selected by comprehensively considering factors such as their frequency of use, data scale, and data quality. They will be introduced along with the relevant data segments, as summarized in Table  III .\n\n1) SEED: The SEED  [28]  dataset consists of 15 subjects (7 males, 8 females), each completing three sessions, with 15 emotion-inducing video trials per session (positive, neutral, or negative, 4 minutes each). EEG data were recorded using a 62-channel cap at 200 Hz, following the 10-20 system. The dataset includes raw data and features such as PSD and DE across five frequency bands. Data from all sessions are used for subject-dependent tasks, while the first session is used for cross-subject tasks.\n\n2) SEED-IV: The SEED-IV  [30]  dataset comprises 15 subjects (7 males, 8 females), each participating in three sessions with 24 trials per session. Each 2-minute trial includes videos evoking happiness, sadness, fear, or neutrality. EEG data were recorded at 1000 Hz using a 62-channel cap. The dataset offers raw data and preprocessed features such as PSD and DE. All sessions' data are used for subject-dependent tasks, while the first session is used for cross-subject tasks.\n\n3) DEAP: The DEAP  [27]  dataset consists of 32 subjects (16 males, 16 females) who participated in one session containing 40 trials. Each trial involved a 1-minute music video designed to evoke specific emotions, followed by participant ratings for arousal, valence, dominance, and liking on a 1-9 scale. EEG data were recorded using a 32-channel cap at 512 Hz, and the dataset includes both raw and downsampled (128 Hz) versions. All samples are used for both subject-dependent and cross-subject tasks. In the subsequent experiments, we used the DEAP dataset with the valence labels, the DEAP dataset with the arousal labels, and the DEAP dataset with both labels, denoted as DEAP-V, DEAP-A, and DEAP-VA, respectively.\n\n4) MAHNOB-HCI: The MAHNOB-HCI  [29]  dataset includes 30 subjects (13 males, 17 females), though EEG data are completely missing for 2 subjects and partially missing for 3. Each subject participated in a single session with 20 trials, where videos lasting 34.9 to 117 seconds evoked emotions. After each video, participants rated arousal, valence, dominance, and predictability on a 1-9 scale. EEG data were recorded using a 32-channel system at 512 Hz, and the dataset includes both raw and downsampled (128 Hz) versions. Both complete and partial samples are used for subject-dependent and cross-subject tasks. In the subsequent experiments, we used the MAHNOB-HCI dataset with the valence labels, the MAHNOB-HCI dataset with the arousal labels, and the MAHNOB-HCI dataset with both labels, denoted as HCI-V, HCI-A, and HCI-VA, respectively.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "5) Seed-V:",
      "text": "The SEED-V  [31]  dataset contains 16 subjects (6 males, 10 females) who each participated in three experimental sessions. Each session included 15 emotioninducing movie clips (3 clips per emotion category: happy, sad, neutral, fear, and disgust). EEG signals were recorded using a 62-channel cap following the 10-20 system, accompanied by simultaneous eye movement recordings. The dataset provides differential entropy (DE) features extracted from five frequency bands (delta, theta, alpha, beta, and gamma) for EEG signals. Data from all three sessions are used for subjectdependent and cross-subject tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "6) Mped:",
      "text": "The MPED  [32]  dataset is a comprehensive multimodal resource for discrete emotion recognition, comprising physiological recordings from 23 Chinese participants (10 males, 13 females). During emotion elicitation experiments using 28 standardized video stimuli representing seven discrete emotions (joy, funny, anger, sadness, disgust, fear, neutrality), 62-channel EEG signals were acquired via an ESI NeuroScan System at 1000 Hz sampling rate following the international 10-20 system. All data are used for both subjectdependent and cross-subject tasks.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Evaluation Methods And Metrics",
      "text": "The evaluation method determines how the results reported in the paper are calculated. This part of the experimental setup is also where many studies encounter the most significant issues. Currently, many studies report the best results of the model on the test set for each epoch. This evaluation method is seriously flawed as it greatly exaggerates the performance of the model.\n\nTherefore, in our benchmark, our evaluation method reports the performance on the test set of the model that achieved the highest F1 score on the validation set across all epochs. We report both the mean and standard deviation of two metrics, accuracy and F1 score. The former is a key metric for classification tasks, while the latter provides a more reasonable assessment when sample labels are imbalanced. The methods for calculating accuracy and F1 score are presented in Eq. 6 and Eq. 7, respectively.\n\nwhere TP (True Positives), TN (True Negatives), FP (False Positives), and FN (False Negatives) are the metrics used to calculate accuracy and F1 score. Besides, the random seed is fixed at 2024 to ensure the reproducibility of the results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Libeer Toolkit Framework",
      "text": "In this section, we will present the framework of the algorithm library to facilitate researchers' utilization of these models in the domain of EER. Specifically, we will detail the coding procedures associated with the data loader, data split, and model training and evaluation within the LibEER. 1) Data loader: The data loader module primarily comprises two components: dataset reading and EEG data preprocessing. Currently, the data formats in EEG emotion recognition datasets lack uniformity, necessitating distinct data loading methods for different datasets, which ultimately reduces research efficiency. To address this challenge, LibEER offers the get uniform data() function, which facilitates the reading of datasets through tailored methods for each dataset, subsequently integrating the data into a standardized format of (session, subject, trial). This approach not only streamlines subsequent preprocessing tasks but also enhances compatibility with various experimental tasks and data-splitting methods.\n\nAdditionally, LibEER provides the preprocess() Sample segmentation organizes the data into user-defined sample lengths. Ultimately, the processed data is integrated into the format of (session, subject, trial, sample). An example demonstrating how to use the data loader is provided in our GitHub project.\n\n2) Data split: The data splitting module is primarily responsible for partitioning the dataset into training, testing, and validation sets. LibEER performs this division based on the experimental tasks and splitting methods selected by the user. For the two experimental tasks, the merge to part() function is utilized to integrate the data into a standardized format.\n\nIn the context of subject-dependent tasks, the merge to part() function organizes the data into the format of (subject, trial, sample), wherein the data for each subject is treated as an independent sub-task, allowing for subsequent data splitting on a trial basis for each sub-task. Conversely, for cross-subject and cross-session tasks, the merge to part() function consolidates the data into (subject, sample) and (session, sample) formats, with data splitting conducted on a subject or session basis.\n\nTo accommodate various splitting methods, the get split index() function further divides the data based on the specified formats and labels. The splitting methods available in LibEER include train-test splitting, which partitions the data into training and testing sets according to a predetermined ratio;train-val-test splitting, which divides the data into training, validation, and testing sets while maintaining balanced label distribution;and cross-validation methods, which segment the data into n folds, ensuring that each fold is utilized as the testing set once. An example illustrating how to perform data splitting with LibEER is provided in our GitHub project.\n\n3) Model training and evaluation: Model training and evaluation are essential processes responsible for training the designated model and assessing its performance. LibEER constructs models based on the selected architecture and parameters, subsequently training them according to the specified training configurations. In scenarios where a validation set is not available, LibEER identifies the optimal performance on the testing set during training as the final outcome for that training iteration. Conversely, when a validation set is utilized, LibEER selects the network weights that yield the best performance on the validation set for evaluation against the testing set, thereby determining the final result.\n\nThe ultimate outcomes of the model will be reported according to the performance metrics specified by the user. An example of creating, training, and evaluating a model within LibEER is provided in our GitHub project.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Experimental Results",
      "text": "In this section, we thoroughly validate the importance of LibEER and our benchmark through extensive experimentation. Initially, we utilize LibEER to replicate the selected models as closely as possible following the experimental configurations detailed in the original publications. Subsequently, we employ LibEER to compare the performance of all models based on the established benchmark. Lastly, we investigate the impact of several key experimental settings on model performance through comparative experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Model Reproduction",
      "text": "To ensure the reliability of our benchmark, we conducted a systematic reproduction of the representative EER methods by strictly following the experimental settings reported in their original papers, including model configurations and dataset usage. Table IV presents both our reproduction results and the original reported results, along with the performance gap. Considering that SEED and DEAP are the two most widely used public datasets, and all included methods have been evaluated on at least one of them, we reproduced the results specifically on SEED and DEAP. If the gap was within 10%, we regarded the reproduced implementation as sufficiently\n\n(5.02) (   reliable for subsequent unified benchmark comparisons. In addition, Table  IV  provides the specific original experimental setups used in the reproduction experiments, clarifying the differences in settings across different models compared to our unified benchmark. All verified models have been integrated into our LibEER algorithm library for direct use.\n\nFrom the results in the Table  IV  and the replication process, we can draw the following conclusions:\n\n(1) The reproduced model performances are generally slightly lower than the results reported in the original paper, averaging 3.85% lower, with a maximum difference of less than 10%, indicating a satisfactory overall replication quality. The reproduction results for ACRNN, R2G-STNN, GCBNet BLS, RGNN, DBN, and DAN showed more significant declines compared to the original papers (>5%). The reproduction results for BiDANN, DGCNN, GCBNet, NSAL-DGAT, and PR-PL showed smaller deviations (<5%). Conversely, some scenarios in CDCN, TSception, MS-MDA, and HSLT exhibited improvements in the reproduced results. The primary reason for these discrepancies is the lack of detailed descriptions regarding model specifications and experimental setups in some papers. We determine these parameters and settings through tuning and experience, which could introduce some biases into the results. Consequently, even when attempts are made to replicate the results as closely as possible to the descriptions in the original work, it remains challenging to achieve identical outcomes.\n\n(2) There are substantial inconsistencies in the experimental setups reported in the original papers, including differences in datasets, preprocessing, tasks, and evaluation protocols. Notably, all original settings do not adopt a three-way split (train/validation/test), which increases the risk of overfitting and inflating performance due to the lack of validation-based model selection. This substantial variability in experimental configurations prevents fair comparisons among the models. Therefore, we established a unified benchmark under a consistent and rational experimental framework, as described in Section III, where all methods are evaluated using standardized preprocessing, dataset usage, a three-way split, and consistent evaluation metrics.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Model Comparison",
      "text": "As shown in Table  IV , the implementation details of different methods vary significantly, making it difficult to fairly compare their performance and draw reliable conclusions for the advancement of the EER field. To this end, we conduct a fair comparison seventeen selected methods based on the unified and reasonable benchmark proposed in Section III. The results for the two experimental tasks are reported in Tables V and VI, which clearly present results that differ significantly from those in Table  IV . We select SVM  [36] , one of the most commonly used machine learning algorithms, as the baseline model. Its results are presented first. In the two tables, the top two methods in each scenario are highlighted using bold and underlined formatting. From these results, we make the following key observations. Observation 1. Modeling the spatiotemporal information of EEG signals is crucial for EER tasks. Several GNN-based and RNN-based models achieved superior performance. In addition, integrating transfer learning can effectively alleviate inter-subject variability in EEG data, demonstrating considerable potential.\n\nTo provide a clearer illustration of the performance of each method under the benchmark, we designed a scoring scheme that simultaneously considers accuracy and F1-score across all datasets. Specifically, for ten datasets (SEED, SEED-IV, HCI-V, HCI-A, HCI-VA, DEAP-V, DEAP-A, DEAP-VA, SEED-V, MPED), all n methods are evaluated based on both accuracy and F1 score, yielding a total of 20 rankings (10 datasets × 2 metrics). In each ranking, the top-performing method receives n points, while the lowest receives 1 point. The final score of each method is computed as the sum of its points across all 20 rankings, as shown in Fig.  5   From Fig.  5  (a), it can be observed that in the subjectdependent task, DGCNN achieved the best performance with a total score of 275. Following closely are GCBNet and its variant GCBNet BLS, scoring 259 and 255 points, respectively. CDCN and BiDANN ranked fourth and fifth with scores of 235 and 234, respectively. In contrast, EEGNet, Tsception, ACRNN, DAN, and DANN performed relatively poorly. In the cross-subject task (see Fig.  5 (b )), the overall performance ranking of each model is similar to that in the In terms of model types, the four GNN-based methods demonstrated superior performance on both tasks, providing strong evidence that the graph-structured information between EEG signal channels is crucial for EER tasks. In contrast, the CNN-based methods exhibited varying levels of performance on both tasks. The DNN and Transformer-based methods showed relatively weaker overall performance, though some individual models demonstrated promising results on specific tasks. However, for the Transformer-based methods, as only one model was selected, the results may contain some occasionality. RNN-based models exhibited notable performance divergence, with both BiDANN and R2G-STNN ranking within the top five and even top three across both tasks, while ACRNN consistently ranked last. This may be attributed to the incorporation of transfer learning in BiDANN and R2G-STNN, which effectively models inter-sample variability. However, the two DNN-based transfer learning models, DAN and DANN, performed poorly, indicating that beyond employing transfer learning, it is equally important to capture the intrinsic spatiotemporal characteristics of EEG signals through architectures such as RNNs and GNNs. Based on the current results, we recommend that researchers in the EER field prioritize models capable of capturing the spatiotemporal dynamics of EEG signals, such as those based on RNN or GNN, and consider incorporating transfer learning techniques to enhance model generalizability across subjects. Observation 2. The issue of significant variability in EEG data among different subjects remains unaddressed, with all methods showing large standard deviations in subjectdependent scenarios and relatively low performance in cross-subject scenarios.\n\nTable  V  demonstrates that the standard deviations for each method across various scenarios are significantly high, averaging approximately 15-20%. Table VI indicates that the performance of the methods is relatively low, suggesting considerable room for improvement. The pronounced variability in EEG data among subjects has long been a critical issue in the field of EER, despite some methods asserting the inclusion of specific modules to tackle this challenge. However, under our unified and systematic benchmark, the models failed to effectively address the variability in data from different subjects, as reported in the original studies. Consequently, further in-depth investigation is necessary to adequately confront the challenges posed by the substantial differences in EEG data among various subjects. Observation 3: The scarcity of EEG data limits the effective representation learning capabilities of deep learning methods, leading to some deep learning approaches performing even worse than traditional machine learning methods, such as SVM.\n\nAs demonstrated in Tables V and VI, several carefully designed deep learning algorithms did not outperform the baseline SVM, as reported in their original studies. A significant contributing factor to this discrepancy is the scarcity of EEG data, characterized by both limited quantity and scale of datasets. Deep networks typically necessitate substantial data volumes to fully leverage their potential for representation learning. Additionally, many studies lack validation sets in their experimental designs, which can easily result in overfitting and inflated performance metrics in contexts with limited data. In contrast, our rigorous experimental setup has highlighted these shortcomings. Therefore, a prudent strategy would be to utilize various data augmentation techniques  [49] -  [51]  to expand the dataset and to implement strategies aimed To visually illustrate the performance differences of various models across different datasets, we created radar charts for both experimental tasks with ACC and F1 score, as shown in Fig.  6 . From the results, we summarize that models performed better on the three-class SEED dataset compared to the binary classification tasks in DEAP and HCI. We believe this is due to the higher label quality in SEED, where discrete emotion labels are easier for self-assessment, while continuous Valence-Arousal-Dominance labels tend to be more subjective. Additionally, SEED provides expectations for the type of emotions elicited by stimuli, whereas DEAP and HCI do not. As the number of labels increases, various models achieve relatively similar average accuracy on SEED-IV, HCI-VA, and SEED-V, with only DEAP-VA showing notably lower performance. However, the performance differences among models are more pronounced on SEED-IV and SEED-V compared to the other two datasets. Moreover, due to the more balanced label distribution in the SEED series datasets, their F1 scores are significantly higher than those of the other datasets. The MPED dataset, which contains the largest number of emotion categories, poses significant challenges to most models, indicating that fine-grained recognition of human emotions in the EER field still requires further investigation.",
      "page_start": 10,
      "page_end": 14
    },
    {
      "section_name": "C. Discussion Of Experiment Settings",
      "text": "In this section, we select multiple representative methods from each kind of EER method and conduct comparative experiments on four crucial factors, i.e., sample length, feature type, evaluation methods and random seeds. To be specific, SEED-IV and MAHNOB-HCI with four classification labels are taken as the dataset and SVM, DBN, HSLT, CDCN, ACRNN, and DGCNN will be selected as the representative methods. The ACC and F1 scores for both experimental scenarios are reported.\n\n1) Data Splitting Length: Existing studies have chosen to divide trials into samples of 1 to 10 seconds in length  [11] ,  [14] ,  [15] . To explore the effects and impact of different sample lengths, we examined lengths of 1s, 3s, 5s, 7s, and 9s, with the results presented in Fig.  7 . As illustrated in Fig.  7 , in the subject-dependent scenario, the performance of most methods tends to decline as sample length increases. We attribute this trend to the sparsity of data in the subjectdependent context; shorter sample lengths yield a greater total number of samples, which enhances the learning capacity of deep models.\n\nIn the cross-subject scenario, it can be observed from the figure that the performance of various methods with different sample lengths across the two scenarios does not show a clear pattern. Different methods may require different sample lengths in different scenarios, and choosing the appropriate length has always been a crucial and challenging task in the EER field. This suggests to researchers that the choice of sample length must balance the trade-off between the quantity of sample augmentation and the quality of the samples.\n\n2) Data preprocessing Feature: In the comparative experiments on feature types, we utilized both DE and PSD features, along with their respective variants processed by Locality Differential Sparse (LDS), as inputs. The results are presented in Fig.  8 . As illustrated in Fig.  8 , in the subject-dependent scenario, the DE and PSD features demonstrate comparable performance, while both feature types processed by LDS show significant improvements. In the cross-subject scenario, the DE feature exhibited the lowest performance, whereas the other three features displayed similar efficacy. This finding highlights LDS as a valuable feature processing technique, suggesting that the selection of specific features should be adapted flexibly according to the experimental context and the unique characteristics of the model. Tables VII and VIII show that the training method used in our benchmark achieved the best performance in the majority of cases and obtained the highest average performance. Although \"R2\" also utilized the validation set like \"Ours\", its performance was significantly lower than that of \"Ours\".\n\nOn the other hand, \"R1\", despite using more data and labels in the training process, lacked a validation set, making it prone to overfitting and occasionality. Therefore, although it outperformed \"Ours\" in some cases, its average performance was still lower. These findings highlight the importance of using a reasonable and appropriate training method for the effective training of EER models.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "4) Random Seeds:",
      "text": "The random seed determines the reproducibility of experimental results, yet it is often unspecified in existing studies. To investigate the impact of random seed settings on model performance, we conducted comparisons using seeds set to 2023, 2024, 2025, 2026, and 2027. The results are shown in Figure  9 . Two key conclusions can be drawn from the results. First, the random seed has a significant impact on experimental outcomes, with different models exhibiting considerable performance variation under different seeds. This is because changing the random seed affects both the random data splitting process and the stochastic behaviors during model training, such as parameter initialization, minibatch ordering, and dropout, all of which can lead to variations in the final results. Among all the methods, CDCN is most affected by the random seed, with performance differences reaching approximately 20%. In contrast, HSLT and ACRNN are relatively less affected, though their variations still reach around 10%. Second, in the majority of cases, these models exhibit similar variation patterns when the random seed changes. This suggests that using a unified random seed across all models can help ensure the fairness of comparisons to some extent. This finding underscores the importance of adopting the same random seed for all methods in the EER field and explicitly reporting the seed value in publications to ensure fairness and reproducibility of results.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Vi. Future Direction",
      "text": "While LibEER is able to standardize and accelerate research in EER, we will further improve and enhance the following two aspects in our future work.\n\nModel Reproduction. In replicating representative models in the EER field, we faced several challenges despite closely following the papers. Some methods lacked detailed descriptions of model or training parameters  [6] ,  [10] . In other cases  [14] -  [16] , the available code did not fully align with the reported paper details. We also found that in DGCNN  [15]  minor adjustments during replication improved performance, suggesting the paper's content may not have been updated. When the above issues arose, we used our experience to replicate the models as accurately as possible, adhering to the papers. The replication results (Table  IV ) show discrepancies compared to the original reports. In the future, we aim to collaborate with original authors and the broader EER community to improve and expand this algorithm library.\n\nExperimental Settings. LibEER includes several key experimental setups to facilitate research, including six widely recognized public datasets, common data preprocessing techniques, two main tasks, common data partitioning methods, and reasonable performance evaluation methods. However, LibEER does not cover all experimental configurations used in the EER field. Some studies use other datasets like DREAMER  [54]  and Amigos  [55] . Others develop unique preprocessing methods, including customized filtering, artifact removal  [11] ,  [58] ,  [59] , denoising  [56] ,  [57] , normalization techniques  [11] ,  [57] , and extracting other types of features  [15] ,  [18] ,  [52] . Additionally, some tasks, such as cross-session  [60] -  [62] , subject-independent  [45] ,  [61] , and cross-dataset tasks  [53] ,  [57] , are designed to assess model performance under different conditions, such as data from different sessions or populations. Furthermore, alternative evaluation strategies, including early stopping  [63] -  [65] , are employed in some studies for more robust model training and assessment. Future versions of LibEER will aim to incorporate more diverse experimental setups from the EER field.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this work, we propose LibEER, a comprehensive benchmark and algorithm library specifically designed for EEGbased emotion recognition. By standardizing the experimental settings, including dataset usage, data preprocessing methods, experimental task selection, data splitting strategies, and evaluation methods, LibEER addresses the prevalent issues of inconsistency and reproducibility in the EER field. Through its unified PyTorch codebase and the comprehensive implementation of seventeen representative models, our benchmark ensures fair comparisons and accurate evaluations of models in both subject-dependent and cross-subject scenarios. Additionally, through fair comparison experiments and investigation of key experimental settings, we present findings and discussions that aim to inspire researchers in the field. We believe that LibEER will not only facilitate the development and benchmarking of new EER models but also promote standardization and transparency, thereby advancing research and applications in this domain. In the future, we plan to further enhance LibEER by incorporating additional experimental settings and more advanced models.",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Data collection protocol of EER.",
      "page": 2
    },
    {
      "caption": "Figure 1: Each data collection begins with a start hint shown",
      "page": 2
    },
    {
      "caption": "Figure 2: The distinct utilization of EEG signals by different methods.",
      "page": 3
    },
    {
      "caption": "Figure 2: (a), which could handle temporal/spectral and spatial",
      "page": 4
    },
    {
      "caption": "Figure 2: (b). In practice,",
      "page": 4
    },
    {
      "caption": "Figure 2: (c). Since EEG data",
      "page": 4
    },
    {
      "caption": "Figure 2: (d). Our benchmark includes five DNN-based methods:",
      "page": 4
    },
    {
      "caption": "Figure 2: (e), the EEG data are typically",
      "page": 4
    },
    {
      "caption": "Figure 3: Four different experimental tasks in EER.",
      "page": 5
    },
    {
      "caption": "Figure 3: provides an",
      "page": 5
    },
    {
      "caption": "Figure 4: The framework and data pipeline of LibEER.",
      "page": 7
    },
    {
      "caption": "Figure 4: illustrates the",
      "page": 7
    },
    {
      "caption": "Figure 5: The model performance scoring on subject-dependent and cross-subject tasks based on ACC and F1.",
      "page": 11
    },
    {
      "caption": "Figure 5: (a) and (b).",
      "page": 11
    },
    {
      "caption": "Figure 5: (a), it can be observed that in the subject-",
      "page": 11
    },
    {
      "caption": "Figure 5: (b)), the overall",
      "page": 11
    },
    {
      "caption": "Figure 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject tasks.",
      "page": 12
    },
    {
      "caption": "Figure 7: Experimental results on sample length for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA",
      "page": 13
    },
    {
      "caption": "Figure 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.",
      "page": 13
    },
    {
      "caption": "Figure 6: From the results, we summarize that models performed",
      "page": 14
    },
    {
      "caption": "Figure 7: As illustrated in",
      "page": 14
    },
    {
      "caption": "Figure 7: , in the subject-dependent scenario, the performance of",
      "page": 14
    },
    {
      "caption": "Figure 9: Experimental results on random seed for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA",
      "page": 15
    },
    {
      "caption": "Figure 8: As illustrated in Fig. 8, in the subject-dependent",
      "page": 15
    },
    {
      "caption": "Figure 9: Two key conclusions can be",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "(PSD)"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "advances"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "of"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "the-art performance [2],"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "sign,"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "aim to address are as follows:"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "•"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "•"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": "•"
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        },
        {
          "Yong-Jin Liu, Senior Member IEEE, Bao-Liang Lu, Fellow IEEE, Dalin Zhang, Senior Member IEEE": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "progress from various perspectives. However,\nthese surveys",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "typically\nreport\nfindings without\nhands-on\nexperience\nin",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reproducing results, which may lead to misdirected future",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "research. This\nlack\nof\npractical\ninsight\ncan\nobscure\nreal",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "challenges faced in the field. Therefore, an in-depth analysis",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of SOTA EER research\nbased\non\nreproduction\nresults\nis",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "essential\nfor uncovering the true challenges and advancing",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the field.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To address\nthese issues, we propose LibEER (Library for",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EEG-based Emotion Recognition),\nan open-source platform",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "that enables reproduction and analysis of SOTA EER studies.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To\nestablish\na\nbenchmark\n(addressing\nI1), we\nstandardize",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "datasets, evaluation metrics, and experimental settings. Based",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on a\nreview of\nrecent\nstudies, we\ninclude\nsix widely used",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "datasets, SEED [28], SEED-IV [30], DEAP [27], MAHNOB-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "HCI\n[29], SEED-V [31], and MPED [32], and support both",
          "2": "Fig. 1: Data collection protocol of EER."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subject-dependent\nand\ncross-subject\nconfigurations,\nusing\na",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "rigorous\ntrain/validation/test\nsplit.\nTo\naddress\nI2, we\nim-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "plement\nseventeen\nrepresentative\ndeep\nlearning-based EER",
          "2": "II. BACKGROUND OF EER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "methods within a unified PyTorch training pipeline. LibEER",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "In\nthis\nsection, we\npresent\nthe\ngeneral\nbackground\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "provides\ndiverse EEG preprocessing\ntools,\nevaluation\npro-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "EEG,\nincluding data collection protocol and formal problem"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tocols,\nand\nflexible\nexperimental\nconfigurations\nvia\nuser-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "definition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "friendly\ninterfaces. Finally,\nto\naddress\nI3, we\nconduct\nex-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tensive\nreproduction-based\ncomparisons\nusing LibEER and",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "A. Data Collection Protocol"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "derive\nfour\nkey\nobservations\nthat\nreveal\nthe\nstrengths\nand",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "limitations of\nexisting models. While\nrecent\nefforts\nlike\n[3]",
          "2": "The general protocol\nfor EER data collection is illustrated"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "have introduced EER toolkits, our work differs by providing",
          "2": "in Fig. 1. Each data collection begins with a start hint shown"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "more in-depth benchmarking analysis, explicitly focusing on",
          "2": "to the participant. Afterwards, various stimuli, such as images,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EER-specific models, and systematically reproducing several",
          "2": "audio, or video, are presented to evoke specific emotions\nin"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "influential methods\nthat\nlack\nopen-source\nimplementations.",
          "2": "the participant, such as happiness, sadness, or fear. After each"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These contributions together offer a more comprehensive and",
          "2": "stimulus,\nthe participant\nis asked to self-assess their emotional"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "practical\nfoundation for advancing EER research.",
          "2": "state,\nproviding\na\nsubjective\nemotion\nscore\nbased\non\ntheir"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In summary, our main contributions are as follows:",
          "2": "experience during the stimulus. This process is repeated a set"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "number of times with different stimuli\nto collect enough data."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "• We have pioneered the establishment of the first benchmark",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "In this context,\nthe participant exposed to the stimuli,\nfrom"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "specifically designed to enable\nfair\ncomparisons\nin EEG-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "whom data\nis\nsubsequently collected,\nis\ncommonly referred"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based emotion recognition (EER). This benchmark not only",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "to as a subject. A continuous segment of EEG data collected"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "offers\nan\nobjective\nreflection\nof\nthe\nactual\nprogress\nin",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "from a subject under a specific stimulus condition is called a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the EER field but also serves\nas a practical\nreference\nfor",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "trial,\ntypically lasting between one and five minutes. Between"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "conducting standardized comparative experiments.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "trials,\nthe\nsubject\ntakes\na\nrest. A series of\ntrials\nis\ntermed"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "• We\nhave\nintroduced LibEER as\nan\nopen-source\nproject",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "a\nsession. Since EEG data\nfrom the\nsame\nsubject,\neven in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on GitHub, offering flexible customization of experimental",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "the\nsame\nemotional\nstate, may vary significantly over\ntime,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "schemes through user-friendly interfaces. LibEER supports",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "datasets generally include\nseveral\nsessions, with an interval"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fair\nand\ncomprehensive\nevaluation\nof\nseventeen\npopular",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "of more than 24 hours between consecutive ones. Given the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "models across six datasets. Serving both as a toolbox and a",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "high cost of EEG data\nacquisition,\na\ntrial\nis often divided"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "platform,\nit assists practitioners\nin efficiently utilizing and",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "into segments. Two consecutive\nsegments within a\ntrial\ncan"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reproducing EER methods.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "be overlapping or non-overlapping. Each segment\nis\ncalled"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "• We provide thorough evaluations across various experimen-",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "a sample, serving as the fundamental unit\nfor EER. Notably,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tal\nsettings,\ncarefully\ncontrolling\nimplementation\ndetails.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "within a single trial, all samples should be designated for either"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These experimental\nresults offer valuable insights\ninto the",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "training or\ntest. Splitting samples\nfrom the same trial across"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "advancements made\nand assist\nresearchers\nin selecting or",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "training and test sets is impractical for real-world applications"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "designing EER models.",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "and considered invalid."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "The paper\nis organized as follows. Section II provides the",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "background and formal definition of the EER task. Section III",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "2": "B. Problem Formulation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "presents the design of\nthe benchmark. Section IV details the",
          "2": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "library. Section V covers\nexperimental\nresults\nand in-depth",
          "2": "Given an EER dataset D, a sample is denoted as xi,j,k,q ∈"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "analysis. Section VI\nidentifies\nfuture work, and Section VII",
          "2": "Rc×d, where i, j, k, and q refer to the i-th subject, j-th session,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "concludes the whole work.",
          "2": "k-th trial, and q-th sample within the trial,\nrespectively. Here,"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Summary of our proposed benchmark.": "Key Information",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "Base."
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "freq."
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "Bandpass Filtering / Removing Eye Movement",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "/ Extracting DE features / Segmenting data",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "7"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "2"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "Train : Val\n: Test = 0.6 : 0.2 : 0.2",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "2"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "Subject-dependent\n/ Cross-subject",
          "TABLE II: Detailed information of selected baseline methods.": "3"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "4"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "SEED / SEED-IV / DEAP / MAHNOB-HCI\n/",
          "TABLE II: Detailed information of selected baseline methods.": "8"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "SEED-V / MPED",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "18"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "3"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "Accuracy / F1-score",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "3"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "11"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "-"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "7"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "-"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "each sample. The goal of EER is",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "-"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "6"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "-"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": ""
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "",
          "TABLE II: Detailed information of selected baseline methods.": "2"
        },
        {
          "TABLE I: Summary of our proposed benchmark.": "joy), via the following mapping:",
          "TABLE II: Detailed information of selected baseline methods.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "F : x → y,\n(1)": "where y denotes\nthe\nemotional\nstate\nand F is\na mapping"
        },
        {
          "F : x → y,\n(1)": "function. SOTA map functions\nare usually neural networks"
        },
        {
          "F : x → y,\n(1)": "trained on a training set Dtrain ⊂ D, and evaluated on a test"
        },
        {
          "F : x → y,\n(1)": "set Dtest ⊂ D. Additionally, a validation set Dvalid ⊂ D is often"
        },
        {
          "F : x → y,\n(1)": "employed to select\nthe best model. These subsets are required"
        },
        {
          "F : x → y,\n(1)": "to be mutually exclusive,\nthat\nis Dtrain ∩ Dtest ∩ Dvalid = ∅."
        },
        {
          "F : x → y,\n(1)": ""
        },
        {
          "F : x → y,\n(1)": "III. BENCHMARK BUILDING"
        },
        {
          "F : x → y,\n(1)": "A comprehensive, well-structured,\nand transparent bench-"
        },
        {
          "F : x → y,\n(1)": "mark is\nessential\nfor\nfair\ncomparisons of\nresearch method-"
        },
        {
          "F : x → y,\n(1)": "ologies\nin the field—a need that\nremains unmet\nin the EER"
        },
        {
          "F : x → y,\n(1)": "domain.\nIn\nthis\nsection, we\npresent\nour\nbenchmark,\ncov-"
        },
        {
          "F : x → y,\n(1)": "ering baseline\nselection, preprocessing,\nexperimental\nsetups,"
        },
        {
          "F : x → y,\n(1)": "datasets,\nand\nevaluation\nprotocols.\nFor\nease\nof\nreference,"
        },
        {
          "F : x → y,\n(1)": "TABLE I provides a summary of\nthe key information in our"
        },
        {
          "F : x → y,\n(1)": "EER benchmark."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "superior performance\nin EER,\nas well\nas\ntwo recent\nstate-",
          "4": "5) Transformer-based Methods: Transformer uses a self-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of-the-art\napproaches\n(NSAL-DGAT [20]\nand\nPR-PL [9]).",
          "4": "attention mechanism to\ncapture\ndependencies\nbetween\nele-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These methods\nare\nalso\ncategorized\ninto the\ncorresponding",
          "4": "ments\nin a sequence, effectively handling long-range depen-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "architecture-based groups according to their backbone model",
          "4": "dencies. As\nshown in Fig. 2 (e),\nthe EEG data are typically"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "structures. TABLE II presents detailed information on these",
          "4": "segmented\ninto multiple\npatches\non\nchannels,\nregions,\nor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "seventeen methods.\nIn\nfuture work, we will\ncontinue\nto",
          "4": "temporal\nsegments when using transformers\nto process EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "maintain\nand\nexpand\nthis\ncollection\nby\nincorporating more",
          "4": "data. These patches are then fed into the transformer model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recent\nand diverse methods. The\nfollowing subsections will",
          "4": "for further analysis. Our benchmark includes one Transformer-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "offer detailed descriptions of the five categories, as well as an",
          "4": "based methods: HSLT [10]."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "examination of each selected method.",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1) CNN-based Methods: Convolutional Neural Networks",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "B. Data preprocessing and Splitting"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(CNNs) are a type of feedforward neural network characterized",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "1)\npreprocessing: A significant\nchallenge\nin reproducing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "by their convolutional operations. When applied to EEG sig-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "extant\nstudies\non EER is\nthe\nlack\nof\na\nstandardized\ndata"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nals, CNNs usually treat EEG data as a two-dimensional (2D)",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "preprocessing protocol. Publicly available EER datasets\nare"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "representation:\none\ndimension\ncorresponds\nto\nthe\nchannels,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "not\nimmediately usable and require extensive preprocessing."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "while\nthe other dimension represents\neither\nthe\ntemporal or",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "Moreover,\nthis\ncritical\ninformation is\nrarely detailed in the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "spectral\nfeatures of\nthe EEG data. This structure is illustrated",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "literature,\nand many\nopen-source\npackages\ndo\nnot\nprovide"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in Fig. 2 (a), which could handle temporal/spectral and spatial",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "the necessary data scripts. To address\nthis\nissue, our bench-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "information simultaneously. On the other hand,\nsome work",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "mark proposes a comprehensive data preprocessing framework"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "relies on 1D CNN to extract\ntemporal\nand spatial\nfeatures",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "specifically\ndesigned\nfor EER research. The\nprocedure\nin-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "separately. LibEER includes\nthree representative CNN-based",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "cludes:\n(1)\napplying bandpass filtering between 0.3 and 50"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "methods: EEGNet\n[11], CDCN [12], and TSception [13].",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "Hz;\n(2)\neliminating eye movement\nartifacts using Principal"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2) RNN-based\nMethods:\nRecurrent\nNeural\nNetworks",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "Component Analysis (PCA); (3) extracting DE features across"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(RNNs) are inherently designed for\nsequential data, making",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "five frequency bands—[0.5, 4],\n[4, 8],\n[8, 14],\n[14, 30], and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "them well-suited for extracting temporal\nfeatures\nfrom EEG",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "[30,\n50]—followed\nby\nprocessing with\na Linear Dynamic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals. For\nspatial\nfeature\nextraction, RNNs\ntreat\nchannels",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "System (LDS); and (4) segmenting data using 1-second non-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "as\nsequence nodes,\nas\nillustrated in Fig. 2 (b).\nIn practice,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "overlapping sliding windows to enhance the dataset."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Long Short-Term Memory (LSTM)\nis a widely used variant",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "2)\nSplitting: Data\nsplitting\nis\na\ncritical\nprocess\nthat\nde-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of RNNs. Our benchmark includes three RNN-based methods:",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "lineates\nhow a\ndataset\nis\npartitioned\ninto\ntraining\nand\ntest"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ACRNN [14], BiDANN [43], and R2G-STNN [17].",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "sets,\nsignificantly influencing model performance. However,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "3) GNN-based Methods: Graph Neural Network (GNN) is",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "prevailing practices in this domain often exhibit a lack of\nra-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a type of neural network designed to handle data structured",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "tionality and the absence of a coherent, standardized approach."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "as graphs. A graph consists of nodes and edges, which can",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "For\nexample, many current data-splitting methodologies\nfail"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "represent\ncomplex\nrelationships\nand\nstructures. When\npro-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "to incorporate a validation set\nfor model selection and do not"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cessing EEG data,\nthe EEG channels are typically treated as",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "consistently adhere\nto cross-trial\nconfigurations,\nresulting in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nodes\nin a graph,\nas\nshown in Fig. 2 (c). Since EEG data",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "unreliable experimental outcomes."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "does not\ninherently contain information about\nthe\nedges or",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "To mitigate\nthese\nshortcomings, we propose\na\nsystematic"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the adjacency matrix, using graph neural networks to process",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "and\nrational\ndata-splitting methodology\nin\nour\nbenchmark."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EEG data often involves leveraging prior knowledge about\nthe",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "Specifically,\nfor\ntasks that are subject-dependent, we allocate"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "relationships between channels or using learnable adjacency",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "each individual’s data into training, validation, and test\nsets"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "matrices as the graph’s adjacency matrix. Our benchmark in-",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "in a 0.6:0.2:0.2 ratio, adjusting as necessary for datasets that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cludes five GNN-based methods: DGCNN [15], GCBNet [18],",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "do not divide evenly. The SEED-V dataset contains relatively"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "GCBNet BLS [18], RGNN [16], and NSAL-DGAT [20].",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "fewer\ntrials\nand more\nclasses,\nso the\nratio was\nadjusted to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "4) DNN-based Methods: Deep Neural Network\n(DNN),",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "1:1:1. In the case of cross-subject\ntasks, we similarly partition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "in\nthis\npaper,\nspecifically\nrefers\nto\nthe\nfeedforward\nfully",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "subjects\ninto training, validation, and test\nsets\nfollowing the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "connected\nneural\nnetwork,\nalso\nknown\nas Fully Connected",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "same 0.6:0.2:0.2 ratio. Furthermore, our benchmark rigorously"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Neural Network (FNN) or Multi-Layer Perceptron (MLP).\nIt",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "adheres to the cross-trial principle, ensuring that samples from"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "consists of an input\nlayer, multiple hidden layers composed of",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "the same trial do not appear in both the training and test sets."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "fully connected neurons with nonlinear\nactivation functions,",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and an output\nlayer. Unlike other deep learning architectures",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "4": "C. Experimental Tasks"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "such as CNN, RNN, or GNN, DNN focuses on dense layer",
          "4": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "transformations without\nconvolutional,\nrecurrent,\nor\ngraph-",
          "4": "The field of EER encompasses various experimental\ntasks,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "based structures.\nIn the field of EER, DNN processes EEG",
          "4": "each designed to address distinct challenges with correspond-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "data by performing feature transformation and mapping into",
          "4": "ing models. The primary experimental\ntasks are categorized"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a high-dimensional\nfeature space, enabling the extraction and",
          "4": "as follows:\n(1) Subject-dependent: This task evaluates model"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "utilization of complex patterns within the data, as illustrated in",
          "4": "performance\nfor\nindividual\nsubjects,\nrequiring both training"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fig. 2 (d). Our benchmark includes five DNN-based methods:",
          "4": "and testing data from the same subject. (2) Cross-subject: This"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DBN [6], DAN [8], DANN [8], MS-MDA [7], and PR-PL [9].",
          "4": "task aims\nto generalize model performance\nacross different"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "that\nthe union of\nthese\nsets\nencompasses\nall\navailable data"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "from subject Si. Following the cross-trial principle, the D(i)\ntrain,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "D(i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "test can be defined as:\nval, D(i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "},\n, . . . , T (i)\n2\ntrain = {T (i)\nm1"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "(3)\n},\nval = {T (i)\nm1+1, T (i)\nm1+2, . . . , T (i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "D(i)\ntest = {T (i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "m }.\nm2+1, T (i)\nm2+2, . . . , T (i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "The EEG samples from D(i)\ntr ∈\ntrain can be represented as X (i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Rm(i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "tr ×c×d, where m(i)\nrepresents the number of EEG sample\ntr"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "of D(i)\ntrain, c denotes the number of electrode channels and d"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "is the feature dimension. We denote the corresponding labels"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fig. 3: Four different experimental\ntasks in EER.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "of X (i)\nas y(i)\ntr . The subject-dependent\ntask involves\ntr\ntr ∈ Rm(i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": ": X (i)\nlearning a mapping function f\nwith cross-\ntr → y(i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "entropy\nloss\nthat\naccurately\npredicts\nthe\nemotion\nstate\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subjects,\nrequiring\ntraining\nand\ntesting\ndata\nfrom different",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "is used to tune hyperparameters\nSi. The validation set D(i)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subjects.\n(3) Subject-independent: This\ntask assesses model",
          "5": "val"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "performance without\nconstraining the\nsubject-level\nsplit,\nal-",
          "5": "and select\nthe best model, while\nthe\ntest\nset D(i)\nis used\ntest"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "lowing training and testing data to be randomly divided from",
          "5": "to evaluate the final model and estimate its performance on"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "mixed samples of multiple\nsubjects.\n(4) Cross-session: This",
          "5": "unseen data."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "task\nexamines model\nperformance\nacross\ndifferent\nsessions",
          "5": "The cross-subject task uses EEG data from all subjects. We"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of\nthe same subject,\nrequiring training and testing data from",
          "5": "denote the training, validation, ans test sets as Dtrain, Dval,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "different sessions of the same individual. Figure 3 provides an",
          "5": "and Dtest, respectively. These sets also satisfy two constraints:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "intuitive illustration of\nthe differences among the four\ntasks.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Dtrain ∩ Dval ∩ Dtest = ∅,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In previous\nstudies,\nthe definitions of\nsubject-independent",
          "5": "(4)"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Dtrain ∪ Dval ∪ Dtest = D."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\ncross-subject\ntasks\nhave\noften\noverlapped,\nas\nsubject-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "independent\nis typically implemented using leave-one-subject-",
          "5": "These conditions ensure that\nthere is no overlap subjects be-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "out\ncross-validation (LOSO-CV)\n[15],\n[16]. To avoid ambi-",
          "5": "tween three sets and the union of these sets covers all available"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "guity, we\nexplicitly\nadopt\ncross-subject\nas\none\nof\nthe\ncore",
          "5": "data of\nthe dataset. Following the cross-subject principle,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "evaluation settings\nin\nour\nbenchmark. Cross-subject\nstrictly",
          "5": "Dtrain, Dval, and Dtest can be defined as:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "requires\ntraining\nand\ntesting\non\nentirely\ndifferent\nsubjects,",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Dtrain = {S1, S2, . . . , Sn1},"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emphasizing the model’s generalization ability. This task setup",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "(5)\nDval = {Sn1+1, Sn1+2, . . . , Sn2 },"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "is more aligned with real-world applications where models are",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "expected to perform reliably on new, unseen users\nin large-",
          "5": "Dtest = {Sn2+1, Sn2+2, . . . , Sn}."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "scale\ndeployment\nscenarios.\nIn\naddition,\nsubject-dependent",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "Analogously, the EEG samples from Dtrain can be represented"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tasks are included to evaluate a model’s performance under",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "represents\nthe number of\nas Xtr ∈ Rmtr×c×d, where mtr"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "personalized settings, where training and testing are performed",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "EEG sample of Dtrain. We denote the corresponding labels"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on\nthe\nsame\nsubject. This\nsetup\nis\nimportant\nfor\nscenarios",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "of Xtr as ytr ∈ Rmtr . The cross-subject task learns a mapping"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "focusing on optimizing accuracy for specific individuals, such",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "f\nfunction\nthe\ncross-entropy\nloss. The\n: Xtr → ytr with"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "as personalized healthcare or user-specific emotion monitor-",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "is used to tune hyperparameters and select\nvalidation set Dval"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ing. Therefore, our benchmark concentrates on cross-subject",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "is used to evaluate the\nthe best model, while the test set Dtest"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and subject-dependent\ntasks as\nthe most widely adopted and",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "final model and estimate its performance on unseen data."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "practically meaningful\nevaluation\nsetups. Additionally,\nit\nis",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "important\nto note\nthat we\ntreat data\nfrom different\nsessions",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "D. Datasets"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of the same subject as if they originate from distinct subjects.",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "We provide specific definitions for\nthe subject-dependent and",
          "5": "Research\nin\nthe EER field\nexhibits\nvariability\nin\ndataset"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cross-subject\ntasks below.",
          "5": "selection and often lacks\nclarity regarding the\nspecific data"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "segments\nutilized. This\nstudy\nevaluates\nthe\nperformance\nof"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "In subject-dependent\ntasks, each task is conducted using",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "popular models\nusing\nsix\ndatasets, which\nare\nselected\nby"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "data\nfrom a\nsingle\nsubject\nand\nthe\ndata\nis\nsplit\nbased\non",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "comprehensively considering factors such as their frequency of"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "individual\ntrials. We denote the training, validation, and test",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sets\nfor\nas D(i)\nand D(i)\nrespectively.\nsubject Si\ntest,\ntrain, D(i)",
          "5": "use, data scale, and data quality. They will be introduced along"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "with the relevant data segments, as summarized in Table III."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These sets must satisfy two essential constraints:",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "1)\nSEED: The SEED [28] dataset consists of 15 subjects"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D(i)",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "test = ∅,\ntrain ∩ D(i)\nval ∩ D(i)",
          "5": "(7 males, 8 females), each completing three sessions, with 15"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "(2)",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "emotion-inducing video trials per session (positive, neutral, or"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "D(i)",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "test = D(i).\ntrain ∪ D(i)\nval ∪ D(i)",
          "5": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "5": "negative, 4 minutes each). EEG data were recorded using a"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "These conditions guarantee that\nthere is no overlap among the",
          "5": "62-channel cap at 200 Hz,\nfollowing the 10-20 system. The"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "trials in the training, validation, and test sets, while ensuring",
          "5": "dataset\nincludes\nraw data and features\nsuch as PSD and DE"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III: The statistics of datasets in LibEER.": "#Channel"
        },
        {
          "TABLE III: The statistics of datasets in LibEER.": "62"
        },
        {
          "TABLE III: The statistics of datasets in LibEER.": "62"
        },
        {
          "TABLE III: The statistics of datasets in LibEER.": "32"
        },
        {
          "TABLE III: The statistics of datasets in LibEER.": "32"
        },
        {
          "TABLE III: The statistics of datasets in LibEER.": "62"
        },
        {
          "TABLE III: The statistics of datasets in LibEER.": "62"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "MPED [32]\n1\n23\n28\n62\nmovie clip",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "150s-300s\n1000\nSTFT\njoy/funny/anger/fear/disgust/sad/neutrality"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "across five frequency bands. Data from all\nsessions are used",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "frequency bands\n(delta,\ntheta,\nalpha, beta,\nand gamma)\nfor"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "for subject-dependent\ntasks, while the first session is used for",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "EEG signals. Data from all three sessions are used for subject-"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "cross-subject\ntasks.",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "dependent and cross-subject\ntasks."
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "6) MPED:\nThe MPED [32]\ndataset\nis\na\ncomprehensive"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "2)\nSEED-IV: The SEED-IV [30] dataset comprises 15 sub-",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "multimodal\nresource\nfor discrete\nemotion recognition,\ncom-"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "jects (7 males, 8 females), each participating in three sessions",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "prising physiological recordings from 23 Chinese participants"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "with 24 trials per session. Each 2-minute trial\nincludes videos",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "(10 males,\n13\nfemales). During\nemotion\nelicitation\nexperi-"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "evoking happiness, sadness, fear, or neutrality. EEG data were",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "ments using 28 standardized video stimuli\nrepresenting seven"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "recorded at 1000 Hz using a 62-channel cap. The dataset offers",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "discrete\nemotions\n(joy,\nfunny,\nanger,\nsadness,\ndisgust,\nfear,"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "raw data and preprocessed features such as PSD and DE. All",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "neutrality), 62-channel EEG signals were acquired via an ESI"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "sessions’ data are used for subject-dependent\ntasks, while the",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "NeuroScan System at 1000 Hz\nsampling rate\nfollowing the"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "first session is used for cross-subject\ntasks.",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "international 10-20 system. All data are used for both subject-"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "3) DEAP: The DEAP [27] dataset consists of 32 subjects",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "dependent and cross-subject\ntasks."
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "(16 males, 16 females) who participated in one session con-",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "taining 40 trials. Each trial\ninvolved a 1-minute music video",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "designed to evoke specific emotions,\nfollowed by participant",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "E. Evaluation Methods and Metrics"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "ratings\nfor arousal, valence, dominance, and liking on a 1-9",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "The evaluation method determines how the results reported"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "scale. EEG data were recorded using a 32-channel cap at 512",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "in the paper are calculated. This part of the experimental setup"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "Hz, and the dataset\nincludes both raw and downsampled (128",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "is\nalso where many\nstudies\nencounter\nthe most\nsignificant"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "Hz) versions. All samples are used for both subject-dependent",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "issues. Currently, many studies\nreport\nthe best\nresults of\nthe"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "and\ncross-subject\ntasks.\nIn the\nsubsequent\nexperiments, we",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "model on the test set\nfor each epoch. This evaluation method"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "used the DEAP dataset with the valence\nlabels,\nthe DEAP",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "is\nseriously flawed as\nit greatly exaggerates\nthe performance"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "dataset with the\narousal\nlabels,\nand the DEAP dataset with",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "of\nthe model."
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "both labels, denoted as DEAP-V, DEAP-A,\nand DEAP-VA,",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "Therefore,\nin our benchmark, our evaluation method reports"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "respectively.",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "the performance on the\ntest\nset of\nthe model\nthat\nachieved"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "4) MAHNOB-HCI:\nThe MAHNOB-HCI\n[29]\ndataset\nin-",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "the highest F1 score on the validation set across all epochs."
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "cludes 30 subjects (13 males, 17 females),\nthough EEG data",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "We\nreport\nboth\nthe mean\nand\nstandard\ndeviation\nof\ntwo"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "are\ncompletely missing for 2 subjects\nand partially missing",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "metrics, accuracy and F1 score. The former is a key metric for"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "for\n3. Each\nsubject\nparticipated\nin\na\nsingle\nsession with",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "classification tasks, while the latter provides a more reasonable"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "20 trials, where videos\nlasting 34.9 to 117 seconds\nevoked",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "assessment when sample labels are imbalanced. The methods"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "emotions. After each video, participants rated arousal, valence,",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "for calculating accuracy and F1 score are presented in Eq. 6"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "dominance, and predictability on a 1-9 scale. EEG data were",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "and Eq. 7,\nrespectively."
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "recorded using a 32-channel system at 512 Hz, and the dataset",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "T P + T N"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "(6)\nACC ="
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "includes both raw and downsampled (128 Hz) versions. Both",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "T P + T N + F P + F N"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "complete and partial\nsamples are used for\nsubject-dependent",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "2 × T P"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "and\ncross-subject\ntasks.\nIn the\nsubsequent\nexperiments, we",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "F1 Score =\n(7)"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "2 × T P + F P + F N"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "used\nthe MAHNOB-HCI\ndataset with\nthe\nvalence\nlabels,",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "the MAHNOB-HCI dataset with the\narousal\nlabels,\nand the",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "where TP (True Positives), TN (True Negatives), FP (False"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "MAHNOB-HCI dataset with both labels, denoted as HCI-V,",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "Positives), and FN (False Negatives) are the metrics used to"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "HCI-A, and HCI-VA,\nrespectively.",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "calculate accuracy and F1 score. Besides,\nthe random seed is"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "fixed at 2024 to ensure the reproducibility of\nthe results."
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "5)\nSEED-V:\nThe SEED-V [31] dataset\ncontains 16 sub-",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "jects\n(6 males, 10 females) who each participated in three",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "IV. LIBEER TOOLKIT FRAMEWORK"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "experimental\nsessions. Each\nsession\nincluded\n15\nemotion-",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": ""
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "inducing movie\nclips\n(3 clips per\nemotion category: happy,",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "In\nthis\nsection, we will\npresent\nthe\nframework\nof\nthe"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "sad, neutral,\nfear,\nand disgust). EEG signals were\nrecorded",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "algorithm library to facilitate researchers’ utilization of\nthese"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "using a 62-channel cap following the 10-20 system, accompa-",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "models\nin the domain of EER. Specifically, we will detail"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "nied by simultaneous eye movement\nrecordings. The dataset",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "the\ncoding procedures\nassociated with the data\nloader, data"
        },
        {
          "SEED-V [31]\n3\n16\n15\n62\nmovie clip": "provides differential entropy (DE) features extracted from five",
          "120s-240s\n1000\nraw data\nhappy/sad/disgust/netural/fear": "split, and model\ntraining and evaluation within the LibEER."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "which allows\nfor\nthe preprocessing of data based on user-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "defined settings. The preprocess()\nfunction operates on a trial"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "basis\nand encompasses\nthree\nprimary steps: noise\nremoval,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "feature extraction, and sample segmentation. When utilizing"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "raw data directly from the dataset, only sample segmentation is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "executed. Noise removal aims to eliminate electromyography"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "(EMG) noise, electrooculography (EOG) artifacts, and other"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "forms\nof\ninterference\nfrom the EEG signals. The\nfeature"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "extraction process analyzes\nthe data to derive relevant EEG"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "features, such as DE and PSD features, based on the specified"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "feature\ntypes\nand\nfrequency\nbands\nprovided\nby\nthe\nuser."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Sample\nsegmentation\norganizes\nthe\ndata\ninto\nuser-defined"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "sample\nlengths. Ultimately,\nthe processed data\nis\nintegrated"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "into the format of (session, subject, trial, sample). An example"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "demonstrating how to use the data loader\nis provided in our"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "GitHub project."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "2) Data split:\nThe data\nsplitting module\nis primarily re-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "sponsible for partitioning the dataset\ninto training,\ntesting, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "validation sets. LibEER performs\nthis division based on the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "experimental\ntasks and splitting methods selected by the user."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "to part()\nfunction\nFor\nthe two experimental\ntasks,\nthe merge"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "is utilized to integrate the data into a standardized format."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "In\nthe\ncontext\nof\nsubject-dependent\ntasks,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "merge\nto part()\nfunction\norganizes\nthe\ndata\ninto\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Fig. 4: The framework and data pipeline of LibEER.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "format of\n(subject,\ntrial,\nsample), wherein the data for each"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "subject\nis\ntreated\nas\nan\nindependent\nsub-task,\nallowing\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "subsequent data\nsplitting on a\ntrial basis\nfor\neach sub-task."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "To facilitate the utilization of\nthe LibEER library in the field",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "Conversely,\nfor\ncross-subject\nand\ncross-session\ntasks,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of EER,\nusers\ncan\neasily\ninstall\nvia\npip, which\nstremlines",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "merge\nto part()\nfunction consolidates\nthe data into (subject,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "the installation process and ensures that all necessary depen-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "sample)\nand\n(session,\nsample)\nformats, with\ndata\nsplitting"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "dencies\nare managed\nautomatically. For\nfurther\ndetails\nand",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "conducted on a subject or session basis."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "access\nto the source code, please visit\nthe official\nrepository",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "To\naccommodate\nvarious\nsplitting\nmethods,\nthe"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "at https://github.com/XJTU-EEG/LibEER.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "get\nsplit\nindex()\nfunction\nfurther\ndivides\nthe\ndata\nbased"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "LibEER offers\na user-friendly and equitable platform for",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "on\nthe\nspecified\nformats\nand\nlabels. The\nsplitting methods"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EER implemented using PyTorch.\nIt comprises three primary",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "available\nin\nLibEER\ninclude\ntrain-test\nsplitting,\nwhich"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "modules:\nthe data\nloader, data\nsplitting,\nand model\ntraining",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "partitions\nthe data into training and testing sets according to"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and\nevaluation.\nThe\ndata\nloader module\nstandardizes\nthe",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "a\npredetermined\nratio;train-val-test\nsplitting, which\ndivides"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "data formats across various datasets and provides a range of",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "the\ndata\ninto\ntraining,\nvalidation,\nand\ntesting\nsets while"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "preprocessing techniques. The data splitting module delivers",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "maintaining\nbalanced\nlabel\ndistribution;and\ncross-validation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "a cohesive solution applicable to different experimental\ntasks",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "methods, which segment\nthe data into n folds, ensuring that"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "and partitioning methods. The model\ntraining and evaluation",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "each\nfold\nis\nutilized\nas\nthe\ntesting\nset\nonce. An\nexample"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "module presents a standardized and extensible framework for",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "illustrating\nhow to\nperform data\nsplitting with LibEER is"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "conducting model training and evaluation. Fig. 4 illustrates the",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "provided in our GitHub project."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "framework of LibEER. Below, we provide detailed explana-",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "3) Model\ntraining\nand\nevaluation: Model\ntraining\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tions for each module.",
          "7": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "7": "evaluation are essential processes responsible for\ntraining the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "1) Data loader:\nThe data\nloader module primarily com-",
          "7": "designated model\nand\nassessing\nits\nperformance.\nLibEER"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "prises two components: dataset reading and EEG data prepro-",
          "7": "constructs models based on the selected architecture and pa-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "cessing. Currently,\nthe data formats\nin EEG emotion recog-",
          "7": "rameters, subsequently training them according to the specified"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "nition\ndatasets\nlack\nuniformity,\nnecessitating\ndistinct\ndata",
          "7": "training\nconfigurations.\nIn\nscenarios where\na\nvalidation\nset"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "loading methods\nfor different datasets, which ultimately re-",
          "7": "is not\navailable, LibEER identifies\nthe optimal performance"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "duces research efficiency. To address this challenge, LibEER",
          "7": "on\nthe\ntesting\nset\nduring\ntraining\nas\nthe final\noutcome\nfor"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "offers\nthe get uniform data()\nfunction, which facilitates\nthe",
          "7": "that\ntraining iteration. Conversely, when a validation set\nis"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "reading of datasets through tailored methods for each dataset,",
          "7": "utilized, LibEER selects\nthe network weights\nthat yield the"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subsequently integrating the data into a standardized format",
          "7": "best performance on the validation set\nfor evaluation against"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of (session, subject,\ntrial). This approach not only streamlines",
          "7": "the testing set,\nthereby determining the final\nresult."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "subsequent preprocessing tasks but also enhances compatibil-",
          "7": "The ultimate outcomes of\nthe model will be reported ac-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ity with various experimental tasks and data-splitting methods.",
          "7": "cording to the performance metrics specified by the user. An"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Additionally, LibEER provides\nthe preprocess()\nfunction,",
          "7": "example of creating,\ntraining, and evaluating a model within"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Method",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "EEGNet",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "CDCN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "TSception",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "ACRNN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "BiDANN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "R2G-STNN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DGCNN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "GCBNet",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "GCBNet BLS",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "RGNN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "NSAL-DGAT",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DBN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DAN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "DANN",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "MS-MDA",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "PR-PL",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "HSLT",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "8": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B: Bandpass filtering, R: Removing eye movement, DE: Extracting DE features, Ns: Segmenting data into N seconds.",
          "8": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "each scenario are highlighted using bold and underlined formatting."
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "Method"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "SEED"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "SEED-IV"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "HCI-V"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "HCI-A"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "HCI-VA"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "DEAP-V"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "DEAP-A"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "DEAP-VA"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "SEED-V"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "ACC"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "MPED"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": "F1"
        },
        {
          "TABLE V: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for subject-dependent EER experiment. The top two methods in": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "scenario are highlighted using bold and underlined formatting."
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "Method"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "SEED"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "SEED-IV"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "HCI-V"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "HCI-A"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "HCI-VA"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "DEAP-V"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "DEAP-A"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "DEAP-VA"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "SEED-V"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": "MPED"
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        },
        {
          "TABLE VI: The mean accuracies and F1 scores (and standard deviations) using the proposed benchmark for cross-subject EER experiment. The top two methods in each": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(a) subject-dependent\n(b) cross-subject": "Fig. 5: The model performance scoring on subject-dependent and cross-subject\ntasks based on ACC and F1."
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "B. Model Comparison\nreliable\nfor\nsubsequent\nunified\nbenchmark\ncomparisons.\nIn"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "addition, Table IV provides the specific original experimental"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "As\nshown in Table IV,\nthe implementation details of dif-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "setups\nused\nin\nthe\nreproduction\nexperiments,\nclarifying\nthe"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "ferent methods vary significantly, making it difficult\nto fairly"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "differences in settings across different models compared to our"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "compare their performance and draw reliable conclusions for"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "unified benchmark. All verified models have been integrated"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "the advancement of\nthe EER field. To this end, we conduct a"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "into our LibEER algorithm library for direct use."
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "fair comparison of\nseventeen selected methods based on the"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "unified and reasonable benchmark proposed in Section III. The\nFrom the results in the Table IV and the replication process,"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "results for the two experimental\ntasks are reported in Tables V\nwe can draw the following conclusions:"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "and VI, which clearly present\nresults\nthat differ\nsignificantly"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "(1)\nThe\nreproduced model\nperformances\nare\ngenerally"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "from those in Table IV. We select SVM [36], one of the most"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "slightly\nlower\nthan\nthe\nresults\nreported\nin\nthe\noriginal\npa-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "commonly used machine learning algorithms, as the baseline"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "per,\naveraging 3.85% lower, with a maximum difference of"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "model.\nIts\nresults\nare presented first.\nIn the\ntwo tables,\nthe"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "less\nthan\n10%,\nindicating\na\nsatisfactory\noverall\nreplication"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "top two methods in each scenario are highlighted using bold"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "quality. The\nreproduction\nresults\nfor ACRNN, R2G-STNN,"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "and underlined formatting. From these results, we make the"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "GCBNet BLS, RGNN, DBN, and DAN showed more signif-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "following key observations."
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "icant declines\ncompared to the original papers\n(>5%). The"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "Observation 1. Modeling the spatiotemporal information of"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "reproduction results for BiDANN, DGCNN, GCBNet, NSAL-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "EEG signals is crucial\nfor EER tasks. Several GNN-based"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "DGAT, and PR-PL showed smaller deviations\n(<5%). Con-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "and RNN-based models achieved superior performance. In"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "versely, some scenarios in CDCN, TSception, MS-MDA, and"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "addition,\nintegrating transfer learning can effectively alle-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "HSLT exhibited improvements in the reproduced results. The"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "viate inter-subject variability in EEG data, demonstrating"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "primary reason for\nthese discrepancies is the lack of detailed"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "considerable potential."
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "descriptions\nregarding model\nspecifications and experimental"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "To provide a clearer illustration of the performance of each"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "setups\nin some papers. We determine\nthese parameters\nand"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "method under\nthe benchmark, we designed a scoring scheme"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "settings through tuning and experience, which could introduce"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "that simultaneously considers accuracy and F1-score across all"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "some biases into the results. Consequently, even when attempts"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "datasets. Specifically, for ten datasets (SEED, SEED-IV, HCI-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "are made to replicate the results as closely as possible to the"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "V\n, HCI-A, HCI-VA, DEAP-V, DEAP-A, DEAP-VA, SEED-V,"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "descriptions\nin the original work,\nit\nremains\nchallenging to"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "MPED), all n methods are evaluated based on both accuracy"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "achieve identical outcomes."
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "and F1 score, yielding a total of 20 rankings (10 datasets × 2"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "metrics). In each ranking,\nthe top-performing method receives\n(2) There are substantial\ninconsistencies in the experimental"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "n points, while the lowest receives 1 point. The final score of\nsetups\nreported in the original papers,\nincluding differences"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "each method is computed as\nthe sum of\nits points across all\nin\ndatasets,\npreprocessing,\ntasks,\nand\nevaluation\nprotocols."
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "20 rankings, as shown in Fig. 5 (a) and (b).\nNotably, all original\nsettings do not adopt a three-way split"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "From Fig. 5 (a),\nit\ncan be observed that\nin the\nsubject-\n(train/validation/test), which increases\nthe\nrisk of overfitting"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "dependent\ntask, DGCNN achieved the best performance with\nand inflating performance due to the lack of validation-based"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "a total\nscore of 275. Following closely are GCBNet and its\nmodel\nselection. This\nsubstantial variability in experimental"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "configurations prevents\nfair comparisons among the models.\nvariant GCBNet BLS,\nscoring 259 and 255 points,\nrespec-"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "Therefore, we established a unified benchmark under a con-\ntively. CDCN and BiDANN ranked\nfourth\nand\nfifth with"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "scores\nof\n235\nand\n234,\nrespectively.\nIn\ncontrast, EEGNet,\nsistent and rational experimental\nframework, as described in"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "Section III, where all methods are evaluated using standardized\nTsception, ACRNN, DAN,\nand DANN performed relatively"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "preprocessing, dataset usage, a three-way split, and consistent\npoorly.\nIn the cross-subject\ntask (see Fig. 5 (b)),\nthe overall"
        },
        {
          "(a) subject-dependent\n(b) cross-subject": "evaluation metrics.\nperformance ranking of each model\nis\nsimilar\nto that\nin the"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SVM": ""
        },
        {
          "SVM": "85"
        },
        {
          "SVM": "70"
        },
        {
          "SVM": ""
        },
        {
          "SVM": "55"
        },
        {
          "SVM": ""
        },
        {
          "SVM": "40"
        },
        {
          "SVM": "25"
        },
        {
          "SVM": ""
        },
        {
          "SVM": "10"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GCBNet_BLS\nGCBNet": "(b) subject-dependent with F1"
        },
        {
          "GCBNet_BLS\nGCBNet": "SVM"
        },
        {
          "GCBNet_BLS\nGCBNet": ""
        },
        {
          "GCBNet_BLS\nGCBNet": "70\nHSLT\nEEGNet"
        },
        {
          "GCBNet_BLS\nGCBNet": "PR-PL"
        },
        {
          "GCBNet_BLS\nGCBNet": "50"
        },
        {
          "GCBNet_BLS\nGCBNet": ""
        },
        {
          "GCBNet_BLS\nGCBNet": "MS-MDA"
        },
        {
          "GCBNet_BLS\nGCBNet": ""
        },
        {
          "GCBNet_BLS\nGCBNet": "30"
        },
        {
          "GCBNet_BLS\nGCBNet": ""
        },
        {
          "GCBNet_BLS\nGCBNet": "DANN"
        },
        {
          "GCBNet_BLS\nGCBNet": "10"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "subject-dependent scenario. Several GNN-based methods still\nwhile ACRNN consistently ranked last. This may be attributed"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "achieved relatively superior performance. BiDANN and R2G-\nto\nthe\nincorporation\nof\ntransfer\nlearning\nin BiDANN and"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "STNN,\ntwo models based on RNN and transfer\nlearning that\nR2G-STNN, which effectively models\ninter-sample variabil-"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "already performed well in subject-dependent settings, achieved\nity. However,\nthe\ntwo DNN-based transfer\nlearning models,"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "even better performance in the cross-subject scenario, ranking\nDAN and DANN, performed poorly,\nindicating that beyond"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "second and third,\nrespectively. EEGNet\nand DBN obtained\nemploying transfer learning,\nit\nis equally important\nto capture"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "better\nresults on this\ntask compared to their performance in\nthe\nintrinsic\nspatiotemporal\ncharacteristics\nof EEG signals"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "subject-dependent settings. The MS-MDA model, specifically\nthrough architectures such as RNNs and GNNs. Based on the"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "designed for\ncross-subject\nscenarios,\nresulted in a moderate\ncurrent\nresults, we\nrecommend that\nresearchers\nin the EER"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "ranking in the overall score.\nfield prioritize models capable of capturing the spatiotemporal"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "In\nterms\nof model\ntypes,\nthe\nfour GNN-based methods\ndynamics of EEG signals,\nsuch as\nthose based on RNN or"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "demonstrated superior performance on both tasks, providing\nGNN, and consider incorporating transfer learning techniques"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "strong evidence that\nthe graph-structured information between\nto enhance model generalizability across subjects."
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "EEG signal channels is crucial for EER tasks. In contrast,\nthe\nObservation 2. The issue of significant variability in EEG"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "CNN-based methods exhibited varying levels of performance\ndata among different\nsubjects remains unaddressed, with"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "on\nboth\ntasks. The DNN and Transformer-based methods\nall methods showing large standard deviations in subject-"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "showed relatively weaker overall performance,\nthough some\ndependent\nscenarios\nand\nrelatively\nlow performance\nin"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "individual models demonstrated promising results on specific\ncross-subject scenarios."
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "tasks. However,\nfor\nthe Transformer-based methods, as only\nTable V demonstrates that\nthe standard deviations for each"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "method\nacross\nvarious\nscenarios\nare\nsignificantly\nhigh,\nav-\none model was selected,\nthe results may contain some occa-"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "eraging approximately 15-20%. Table VI\nindicates\nthat\nthe\nsionality. RNN-based models\nexhibited notable performance"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "performance of the methods is relatively low, suggesting con-\ndivergence, with\nboth BiDANN and R2G-STNN ranking"
        },
        {
          "Fig. 6: Radar chart of model performance across different datasets on subject-dependent and cross-subject\ntasks.": "siderable room for\nimprovement. The pronounced variability\nwithin\nthe\ntop\nfive\nand\neven\ntop\nthree\nacross\nboth\ntasks,"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "in EEG data among subjects has long been a critical\nissue in"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "the field of EER, despite some methods asserting the inclusion"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "of\nspecific modules\nto tackle this challenge. However, under"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "our unified and systematic benchmark,\nthe models\nfailed to"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "effectively address\nthe variability in data from different\nsub-"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "jects, as reported in the original studies. Consequently, further"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "in-depth investigation is necessary to adequately confront\nthe"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "challenges posed by the substantial differences\nin EEG data"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "among various subjects."
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "Observation\n3:\nThe\nscarcity\nof\nEEG data\nlimits\nthe"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "effective representation learning capabilities of deep learn-"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "ing methods,\nleading to some deep learning approaches"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "performing even worse than traditional machine learning"
        },
        {
          "Fig. 8: Experimental results on feature type for subject-dependent and cross-subject tasks on the SEED-IV and HCI-VA datasets.": "methods, such as SVM."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VII: Experimental": "subject-dependent",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "HCI-VA",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "Method",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "R2",
          "training methods": "R1",
          "for": "R2",
          "TABLE VIII: Experimental": "Ours",
          "results on training methods": "R2"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "43.96",
          "training methods": "46.58",
          "for": "17.77",
          "TABLE VIII: Experimental": "ACC 36.82",
          "results on training methods": "30.68"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(22.47)",
          "training methods": "(23.59)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "DBN",
          "results\non": "",
          "training methods": "",
          "for": "17.56",
          "TABLE VIII: Experimental": "32.60\nF1",
          "results on training methods": "19.79"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "37.83",
          "training methods": "34.87",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "33.12",
          "TABLE VIII: Experimental": "ACC 30.33",
          "results on training methods": "30.33"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(21.79)",
          "training methods": "(23.67)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "18.46",
          "TABLE VIII: Experimental": "11.64\nF1",
          "results on training methods": "11.64"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "33.15",
          "training methods": "44.02",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "25.77",
          "TABLE VIII: Experimental": "ACC 31.03",
          "results on training methods": "32.94"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(20.25)",
          "training methods": "(20.09)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "HSLT",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "21.72",
          "TABLE VIII: Experimental": "F1\n27.01",
          "results on training methods": "31.99"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "22.03",
          "training methods": "32.46",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(19.80)",
          "training methods": "(18.85)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "24.19",
          "TABLE VIII: Experimental": "ACC 31.97",
          "results on training methods": "30.33"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "52.83",
          "training methods": "40.41",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "16.05",
          "TABLE VIII: Experimental": "F1\n18.82",
          "results on training methods": "11.64"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(22.64)",
          "training methods": "(19.52)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "CDCN",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "26.59",
          "TABLE VIII: Experimental": "ACC 42.54",
          "results on training methods": "42.93"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "47.17",
          "training methods": "24.31",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "20.00",
          "TABLE VIII: Experimental": "43.10\nF1",
          "results on training methods": "42.05"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(23.95)",
          "training methods": "(14.34)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "25.49",
          "TABLE VIII: Experimental": "ACC 34.54",
          "results on training methods": "33.44"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "27.45",
          "training methods": "41.57",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(3.74)",
          "training methods": "(23.37)",
          "for": "18.76",
          "TABLE VIII: Experimental": "26.63\nF1",
          "results on training methods": "23.42"
        },
        {
          "TABLE VII: Experimental": "ACRNN",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "11.74",
          "training methods": "31.78",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(2.23)",
          "training methods": "(20.28)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "to\nthe",
          "TABLE VIII: Experimental": "to\nthe\nother",
          "results on training methods": "datasets. Moreover,"
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "50.01",
          "training methods": "50.16",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "more balanced label distribution in the SEED series datasets,",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(26.99)",
          "training methods": "(25.13)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "of\nthe",
          "TABLE VIII: Experimental": "scores\nare",
          "results on training methods": "higher"
        },
        {
          "TABLE VII: Experimental": "DGCNN",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "other datasets. The MPED dataset, which contains the largest",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "43.49",
          "training methods": "37.94",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "number of emotion categories, poses significant challenges to",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(27.22)",
          "training methods": "(26.70)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "most models, indicating that fine-grained recognition of human",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "41.48",
          "training methods": "44.55",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "investigation.",
          "TABLE VIII: Experimental": "emotions in the EER field still",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(19.22)",
          "training methods": "(22.34)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "Average",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "32.45",
          "training methods": "32.27",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "C. Discussion of Experiment Settings",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "(19.00)",
          "training methods": "(20.77)",
          "for": "",
          "TABLE VIII: Experimental": "",
          "results on training methods": ""
        },
        {
          "TABLE VII: Experimental": "",
          "results\non": "",
          "training methods": "",
          "for": "",
          "TABLE VIII: Experimental": "section, we",
          "results on training methods": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "datasets.",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "length has always been a crucial and challenging task in the",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "On the other hand, “R1”, despite using more data and labels"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "EER field. This\nsuggests\nto\nresearchers\nthat\nthe\nchoice\nof",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "in\nthe\ntraining\nprocess,\nlacked\na\nvalidation\nset, making\nit"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "sample length must balance the trade-off between the quantity",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "prone to overfitting and occasionality. Therefore, although it"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "of sample augmentation and the quality of\nthe samples.",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "outperformed “Ours” in some cases,\nits average performance"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "was\nstill\nlower. These findings highlight\nthe\nimportance of"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "2) Data preprocessing Feature:\nIn the comparative experi-",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "using a\nreasonable\nand appropriate\ntraining method for\nthe"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "ments on feature types, we utilized both DE and PSD features,",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "effective training of EER models."
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "along with\ntheir\nrespective\nvariants\nprocessed\nby Locality",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "Differential Sparse (LDS), as inputs. The results are presented",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "in Fig. 8. As\nillustrated in Fig. 8,\nin the\nsubject-dependent",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "scenario,\nthe DE and PSD features demonstrate comparable",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "4) Random Seeds: The random seed determines the repro-"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "performance, while both feature types processed by LDS show",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "ducibility of experimental\nresults, yet\nit\nis often unspecified"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "significant\nimprovements.\nIn\nthe\ncross-subject\nscenario,\nthe",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "in existing studies. To investigate the impact of\nrandom seed"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "DE feature\nexhibited\nthe\nlowest\nperformance, whereas\nthe",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "settings\non model\nperformance, we\nconducted\ncomparisons"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "other\nthree\nfeatures displayed similar\nefficacy. This finding",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "using seeds\nset\nto 2023, 2024, 2025, 2026,\nand 2027. The"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "highlights LDS as\na valuable\nfeature processing technique,",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "results are shown in Figure 9. Two key conclusions can be"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "suggesting that\nthe\nselection of\nspecific\nfeatures\nshould be",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "drawn from the results. First,\nthe random seed has a signifi-"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "adapted flexibly according to the experimental context and the",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "cant\nimpact on experimental outcomes, with different models"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "unique characteristics of\nthe model.",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": ""
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "exhibiting considerable performance variation under different"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "3) Evaluation Methods:\nIn the comparative experiments on",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "seeds. This is because changing the random seed affects both"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "model\ntraining methods, we explored how different\ntraining",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "the random data splitting process and the stochastic behaviors"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "approaches\nimpact model performance. We\ncompared three",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "during model\ntraining, such as parameter\ninitialization, mini-"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "training methods:\n(1)\nthe method\nused\nin\nour\nbenchmark,",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "batch ordering, and dropout, all of which can lead to variations"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "which\nselects\nthe\nepoch with\nthe\nbest\nperformance\non\nthe",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "in the final\nresults. Among all\nthe methods, CDCN is most"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "validation set (as “Ours” in Tables VII and VIII); (2) selecting",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "affected by the\nrandom seed, with performance differences"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "the model’s result\nfrom the last epoch (as “R1” in Tables VII",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "reaching approximately 20%.\nIn contrast, HSLT and ACRNN"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "and VIII), where,\nfor a fair comparison,\nthe model’s training",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "are relatively less affected,\nthough their variations still\nreach"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "set\nincludes\nthe validation set;\nand (3)\nselecting the\nepoch",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "around 10%. Second,\nin the majority of\ncases,\nthese mod-"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "when the loss on the validation set decreases by less than 1%",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "els\nexhibit\nsimilar variation patterns when the\nrandom seed"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "within the first 10 epochs (as “R2” in Tables VII and VIII).",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "changes. This suggests that using a unified random seed across"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "Tables VII\nand VIII\nshow that\nthe\ntraining method used",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "all models can help ensure the fairness of comparisons to some"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "in our benchmark achieved the best performance in the ma-",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "extent. This finding underscores\nthe importance of adopting"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "jority of cases and obtained the highest average performance.",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "the same random seed for all methods\nin the EER field and"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "Although “R2”\nalso utilized the validation set\nlike\n“Ours”,",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "explicitly reporting the\nseed value\nin publications\nto ensure"
        },
        {
          "Fig. 9: Experimental\nresults on random seed for\nsubject-dependent": "its performance was\nsignificantly lower\nthan that of “Ours”.",
          "and cross-subject\ntasks on the SEED-IV and HCI-VA": "fairness and reproducibility of\nresults."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "REFERENCES"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[1] R. J. Dolan, “Emotion, cognition, and behavior,” Science, vol. 298, no."
        },
        {
          "Centre for Engineering Science and Technology.": "5596, pp. 1191–1194, 2002."
        },
        {
          "Centre for Engineering Science and Technology.": "[2] H. Liu, T. Lou, Y. Zhang, Y. Wu, Y. Xiao, C. S. Jensen, and D. Zhang,"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "“Eeg-based multimodal\nemotion recognition:\na machine\nlearning per-"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "IEEE Transactions\non\nInstrumentation\nspective,”\nand Measurement,"
        },
        {
          "Centre for Engineering Science and Technology.": "2024."
        },
        {
          "Centre for Engineering Science and Technology.": "[3]\nZ. Zhang, S.-h. Zhong,\nand Y. Liu,\n“Torcheegemo: A deep learning"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "toolbox towards eeg-based emotion recognition,” Expert Systems with"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "Applications, vol. 249, p. 123550, 2024."
        },
        {
          "Centre for Engineering Science and Technology.": "[4]\nS. M. Alarcao\nand M.\nJ. Fonseca,\n“Emotions\nrecognition using eeg"
        },
        {
          "Centre for Engineering Science and Technology.": "signals: A survey,” IEEE transactions on affective computing, vol. 10,"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "no. 3, pp. 374–393, 2017."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[5] H. Zhang, D. Chen,\nand C. Wang,\n“Confidence-aware multi-teacher"
        },
        {
          "Centre for Engineering Science and Technology.": "knowledge distillation,” in ICASSP.\nIEEE, 2022, pp. 4498–4502."
        },
        {
          "Centre for Engineering Science and Technology.": "[6] W.-L. Zheng,\nJ.-Y. Zhu, Y. Peng,\nand B.-L. Lu,\n“Eeg-based emotion"
        },
        {
          "Centre for Engineering Science and Technology.": "classification using deep belief networks,” in ICME.\nIEEE, 2014, pp."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "1–6."
        },
        {
          "Centre for Engineering Science and Technology.": "[7] H. Chen, M. Jin, Z. Li, C. Fan, J. Li, and H. He, “Ms-mda: Multisource"
        },
        {
          "Centre for Engineering Science and Technology.": "marginal distribution adaptation for cross-subject and cross-session eeg"
        },
        {
          "Centre for Engineering Science and Technology.": "emotion recognition,” Frontiers\nin Neuroscience, vol. 15, p. 778488,"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "2021."
        },
        {
          "Centre for Engineering Science and Technology.": "[8] H. Li, Y.-M.\nJin, W.-L. Zheng, and B.-L. Lu, “Cross-subject emotion"
        },
        {
          "Centre for Engineering Science and Technology.": "recognition using deep adaptation networks,”\nin ICONIP.\nSpringer,"
        },
        {
          "Centre for Engineering Science and Technology.": "2018, pp. 403–413."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[9] R. Zhou, Z. Zhang, H. Fu, L. Zhang, L. Li, G. Huang, F. Li, X. Yang,"
        },
        {
          "Centre for Engineering Science and Technology.": "Y\n. Dong, Y.-T. Zhang et al., “Pr-pl: A novel prototypical representation"
        },
        {
          "Centre for Engineering Science and Technology.": "based pairwise learning framework for emotion recognition using eeg"
        },
        {
          "Centre for Engineering Science and Technology.": "signals,” IEEE Transactions on Affective Computing, vol. 15, no. 2, pp."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "657–670, 2023."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[10]\nZ. Wang, Y. Wang, C. Hu, Z. Yin, and Y. Song, “Transformers for eeg-"
        },
        {
          "Centre for Engineering Science and Technology.": "based emotion recognition: A hierarchical\nspatial\ninformation learning"
        },
        {
          "Centre for Engineering Science and Technology.": "model,” IEEE Sensors Journal, vol. 22, no. 5, pp. 4359–4368, 2022."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[11] V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "and B. J. Lance, “Eegnet: a compact convolutional neural network for"
        },
        {
          "Centre for Engineering Science and Technology.": "Journal of neural\neeg-based brain–computer\ninterfaces,”\nengineering,"
        },
        {
          "Centre for Engineering Science and Technology.": "vol. 15, no. 5, p. 056013, 2018."
        },
        {
          "Centre for Engineering Science and Technology.": "[12]\nZ. Gao, X. Wang, Y. Yang, Y. Li, K. Ma, and G. Chen, “A channel-fused"
        },
        {
          "Centre for Engineering Science and Technology.": "dense convolutional network for eeg-based emotion recognition,” IEEE"
        },
        {
          "Centre for Engineering Science and Technology.": "Transactions on Cognitive and Developmental Systems, vol. 13, no. 4,"
        },
        {
          "Centre for Engineering Science and Technology.": "pp. 945–954, 2020."
        },
        {
          "Centre for Engineering Science and Technology.": "[13] Y. Ding, N. Robinson, S. Zhang, Q. Zeng, and C. Guan, “Tsception:"
        },
        {
          "Centre for Engineering Science and Technology.": "Capturing temporal dynamics and spatial asymmetry from eeg for emo-"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "tion recognition,” IEEE Transactions on Affective Computing, vol. 14,"
        },
        {
          "Centre for Engineering Science and Technology.": "no. 3, pp. 2238–2250, 2022."
        },
        {
          "Centre for Engineering Science and Technology.": "[14] W. Tao, C. Li, R. Song, J. Cheng, Y. Liu, F. Wan, and X. Chen, “Eeg-"
        },
        {
          "Centre for Engineering Science and Technology.": "based emotion recognition via channel-wise attention and self attention,”"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "IEEE Transactions on Affective Computing, vol. 14, no. 1, pp. 382–393,"
        },
        {
          "Centre for Engineering Science and Technology.": "2020."
        },
        {
          "Centre for Engineering Science and Technology.": "[15]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using"
        },
        {
          "Centre for Engineering Science and Technology.": "dynamical graph convolutional neural networks,” IEEE Transactions on"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[16]\nP. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition using"
        },
        {
          "Centre for Engineering Science and Technology.": "IEEE Transactions\non Affective\nregularized\ngraph\nneural\nnetworks,”"
        },
        {
          "Centre for Engineering Science and Technology.": "Computing, vol. 13, no. 3, pp. 1290–1301, 2020."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[17] Y. Li, W. Zheng, L. Wang, Y. Zong, and Z. Cui, “From regional to global"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "brain: A novel hierarchical\nspatial-temporal neural network model\nfor"
        },
        {
          "Centre for Engineering Science and Technology.": "eeg emotion recognition,” IEEE Transactions on Affective Computing,"
        },
        {
          "Centre for Engineering Science and Technology.": "vol. 13, no. 2, pp. 568–578, 2022."
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "[18]\nT. Zhang, X. Wang, X. Xu, and C. P. Chen, “Gcb-net: Graph convolu-"
        },
        {
          "Centre for Engineering Science and Technology.": ""
        },
        {
          "Centre for Engineering Science and Technology.": "tional broad network and its application in emotion recognition,” IEEE"
        },
        {
          "Centre for Engineering Science and Technology.": "Transactions on Affective Computing, vol. 13, no. 1, pp. 379–388, 2019."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[19] M.\nJin, C. Du, H. He, T. Cai,\nand\nJ. Li,\n“Pgcn: Pyramidal\ngraph",
          "17": "[42]\nJ. Cheng, M. Chen, C. Li, Y. Liu, R. Song, A. Liu,\nand X. Chen,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "convolutional network for eeg emotion recognition,” IEEE Transactions",
          "17": "IEEE\n“Emotion recognition from multi-channel\neeg via deep forest,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on Multimedia, 2024.",
          "17": "Journal of Biomedical and Health Informatics, vol. 25, no. 2, pp. 453–"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[20] Y. Yang, Z. Wang, Y. Song, Z.\nJia, B. Wang, T.-P.\nJung, and F. Wan,",
          "17": "464, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Exploiting the\nintrinsic neighborhood semantic\nstructure\nfor domain",
          "17": "[43] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang,\nand X. Zhou,\n“A bi-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions\non\nadaptation\nin\neeg-based\nemotion\nrecognition,”",
          "17": "hemisphere domain adversarial neural network model\nfor eeg emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Affective Computing, 2025.",
          "17": "recognition,” IEEE Transactions on Affective Computing, vol. 12, no. 2,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[21] R.-N. Duan,\nJ.-Y. Zhu, and B.-L. Lu, “Differential entropy feature for",
          "17": "pp. 494–504, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "eeg-based emotion classification,” in NER.\nIEEE, 2013, pp. 81–84.",
          "17": "[44] D. Li, L. Xie, Z. Wang,\nand H. Yang,\n“Brain\nemotion\nperception"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[22] Y. Zhang, H. Liu, D. Zhang, X. Chen, T. Qin, and Q. Zheng, “Eeg-based",
          "17": "inspired\neeg\nemotion\nrecognition with\ndeep\nreinforcement\nlearning,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion\nrecognition with\nemotion\nlocalization\nvia\nhierarchical\nself-",
          "17": "IEEE Transactions on Neural Networks and Learning Systems, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "attention,” IEEE Transactions on Affective Computing, vol. 14, no. 3,",
          "17": "[45] C. Li, P. Li, Y. Zhang, N. Li, Y. Si, F. Li, Z. Cao, H. Chen, B. Chen,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 2458–2469, 2022.",
          "17": "D. Yao et al., “Effective emotion recognition by learning discriminative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[23] Y. Wu, H. Liu, D. Zhang, Y. Zhang, T. Lou, and Q. Zheng, “Autoeer: au-",
          "17": "graph topologies in eeg brain networks,” IEEE Transactions on Neural"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tomatic eeg-based emotion recognition with neural architecture search,”",
          "17": "Networks and Learning Systems, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Journal of Neural Engineering, vol. 20, no. 4, p. 046029, 2023.",
          "17": "[46] R. Khosrowabadi, C. Quek, K. K. Ang, and A. Wahab, “Ernn: A bio-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[24] Y. Zhang, H. Liu, D. Wang, D. Zhang, T. Lou, Q. Zheng, and C. Quek,",
          "17": "logically inspired feedforward neural network to discriminate emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Cross-modal credibility modelling for eeg-based multimodal emotion",
          "17": "from eeg signal,” IEEE transactions on neural networks and learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” Journal of Neural Engineering, vol. 21, no. 2, p. 026040,",
          "17": "systems, vol. 25, no. 3, pp. 609–620, 2013."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2024.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[47] M. Alsolamy and A. Fattouh,\n“Emotion estimation from eeg signals"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[25] A. Paszke,\nS. Gross, F. Massa, A. Lerer,\nJ. Bradbury, G. Chanan,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "during listening to quran using psd features,” in CSIT.\nIEEE, 2016, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "et\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga\nal.,\n“Pytorch: An",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "1–5."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "imperative style, high-performance deep learning library,” Advances in",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[48] Y. Zhang, H. Liu, Y. Xiao, M. Amoon, D. Zhang, D. Wang, S. Yang,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural\ninformation processing systems, vol. 32, 2019.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "and C. Quek, “Llm-enhanced multi-teacher knowledge distillation for"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[26] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "IEEE\nmodality-incomplete\nemotion\nrecognition\nin\ndaily\nhealthcare,”"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "S. Ghemawat, G.\nIrving, M.\nIsard et al., “{TensorFlow}: a system for",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Journal of Biomedical and Health Informatics, pp. 1–11, 2024."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "{Large-Scale} machine learning,” in OSDI, 2016, pp. 265–283.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[49] Y. Luo and B.-L. Lu, “Eeg data augmentation for emotion recognition"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[27]\nS. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "using a conditional wasserstein gan,” in EMBC, 2018, pp. 2535–2538."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "T. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion analysis;",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[50]\nZ. Zhang, S. Zhong, and Y. Liu, “Beyond mimicking under-represented"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using physiological signals,” IEEE transactions on affective computing,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "emotions: Deep data augmentation with emotional subspace constraints"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 3, no. 1, pp. 18–31, 2011.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "for eeg-based emotion recognition,” Proceedings of the AAAI Conference"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[28] W.-L. Zheng and B.-L. Lu, “Investigating critical\nfrequency bands and",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "on\nArtificial\nIntelligence,\nvol.\n38,\nno.\n9,\npp.\n10 252–10 260, Mar."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "channels for eeg-based emotion recognition with deep neural networks,”",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "2024.\n[Online]. Available:\nhttps://ojs.aaai.org/index.php/AAAI/article/"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on autonomous mental development, vol. 7, no. 3,",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "view/28891"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "pp. 162–175, 2015.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[51] Y. Luo, L.-Z. Zhu, Z.-Y. Wan,\nand B.-L. Lu,\n“Data\naugmentation"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[29] M. Soleymani,\nJ. Lichtenauer, T. Pun, and M. Pantic, “A multimodal",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "for\nenhancing\neeg-based\nemotion\nrecognition with\ndeep\ngenerative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "database for affect recognition and implicit\ntagging,” IEEE transactions",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "models,” Journal of Neural Engineering, vol. 17, no. 5, p. 056021, oct"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "on affective computing, vol. 3, no. 1, pp. 42–55, 2011.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "2020. [Online]. Available: https://dx.doi.org/10.1088/1741-2552/abb580"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[30] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “Emotionmeter:",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[52] Y. Li, L. Wang, W. Zheng, Y. Zong, L. Qi, Z. Cui, T. Zhang,\nand"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE\nA multimodal\nframework\nfor\nrecognizing\nhuman\nemotions,”",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "T. Song, “A novel bi-hemispheric discrepancy model\nfor eeg emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "transactions on cybernetics, vol. 49, no. 3, pp. 1110–1122, 2018.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "recognition,” IEEE Transactions on Cognitive and Developmental Sys-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[31] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, “Comparing recognition",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "tems, vol. 13, no. 2, pp. 354–367, 2020."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "performance\nand robustness of multimodal deep learning models\nfor",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[53]\nT. Song, S. Liu, W. Zheng, Y. Zong, Z. Cui, Y. Li,\nand X. Zhou,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal emotion recognition,” IEEE Transactions on Cognitive and",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "“Variational\ninstance-adaptive graph for eeg emotion recognition,” IEEE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Developmental Systems, vol. 14, no. 2, pp. 715–729, 2021.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Transactions on Affective Computing, vol. 14, no. 1, pp. 343–356, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[32]\nT. Song, W. Zheng, C. Lu, Y. Zong, X. Zhang,\nand Z. Cui,\n“Mped:",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[54]\nS. Katsigiannis\nand N. Ramzan,\n“Dreamer: A database\nfor\nemotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "A multi-modal\nphysiological\nemotion\ndatabase\nfor\ndiscrete\nemotion",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "recognition through eeg and ecg signals\nfrom wireless\nlow-cost off-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "recognition,” IEEE Access, vol. 7, pp. 12 177–12 191, 2019.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "the-shelf devices,” IEEE journal of biomedical and health informatics,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[33] W. Jiang, L. Zhao, and B.-l. Lu, “Large brain model for learning generic",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "vol. 22, no. 1, pp. 98–107, 2017."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "representations with tremendous eeg data in bci,” in ICLR, 2024.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[55]\nJ. A. Miranda-Correa, M. K. Abadi, N. Sebe, and I. Patras, “Amigos:"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[34] C. Brunner, A. Delorme,\nand\nS. Makeig,\n“Eeglab–an\nopen\nsource",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "A dataset\nfor affect, personality and mood research on individuals and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Biomedical\nEn-\nmatlab\ntoolbox\nfor\nelectrophysiological\nresearch,”",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "groups,” IEEE transactions on affective computing, vol. 12, no. 2, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "gineering/Biomedizinische\nTechnik,\nvol.\n58,\nno.\nSI-1-Track-G,\np.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "479–493, 2018."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "000010151520134182, 2013.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "[56] R. Li, Y. Wang, and B.-L. Lu, “A multi-domain adaptive graph convolu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[35] Y. Liu, Z.\nJia,\nand H. Wang,\n“Emotionkd:\na\ncross-modal knowledge",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "tional network for eeg-based emotion recognition,” in ACM MM, 2021,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "distillation framework for emotion recognition based on physiological",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "pp. 5565–5573."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "signals,” in ACM MM, 2023, pp. 6122–6131.",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[36]\nJ. A. Suykens and J. Vandewalle, “Least squares support vector machine",
          "17": "[57]\nZ.\nHe,\nY\n.\nZhong,\nand\nJ.\nPan,\n“An\nadversarial\ndiscriminative"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "classifiers,” Neural Processing Letters, vol. 9, no. 3, pp. 293–300, 1999.",
          "17": "temporal\nconvolutional\nnetwork\nfor\neeg-based\ncross-domain\nemotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "",
          "17": "Computers\nin\nBiology\nrecognition,”\nand Medicine,\nvol.\n141,\np."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[37] Y. Ding, N. Robinson, C. Tong, Q. Zeng,\nand C. Guan,\n“Lggnet:",
          "17": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Learning from local-global-graph representations for brain–computer in-",
          "17": "105048,\n2022.\n[Online].\nAvailable:\nhttps://www.sciencedirect.com/"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "terface,” IEEE Transactions on Neural Networks and Learning Systems,",
          "17": "science/article/pii/S0010482521008428"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2023.",
          "17": "[58] X. Shen, X. Liu, X. Hu, D. Zhang, and S. Song, “Contrastive learning"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[38]\nS. K. Khare and V. Bajaj, “Time–frequency representation and convolu-",
          "17": "of\nsubject-invariant\neeg\nrepresentations\nfor\ncross-subject\nemotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "tional neural network-based emotion recognition,” IEEE transactions on",
          "17": "recognition,” IEEE Transactions on Affective Computing, vol. 14, pp."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "neural networks and learning systems, vol. 32, no. 7, pp. 2901–2909,",
          "17": "2496–2511,\n2021.\n[Online]. Available:\nhttps://api.semanticscholar.org/"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2020.",
          "17": "CorpusID:237572066"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[39] M. A. Hasnul, N. A. A. Aziz, S. Alelyani, M. Mohana,\nand A. A.",
          "17": "[59] W. Wang, F. Qi, D. P. Wipf, C. Cai, T. Yu, Y. Li, Y. Zhang, Z. Yu, and"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Aziz, “Electrocardiogram-based emotion recognition systems and their",
          "17": "W. Wu, “Sparse bayesian learning for end-to-end eeg decoding,” IEEE"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "applications in healthcare—a review,” Sensors, vol. 21, no. 15, p. 5015,",
          "17": "Transactions\non Pattern Analysis\nand Machine\nIntelligence,\nvol.\n45,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2021.",
          "17": "no. 12, pp. 15 632–15 649, 2023."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[40] H. Gauba, P. Kumar, P. P. Roy, P. Singh, D. P. Dogra, and B. Raman,",
          "17": "[60]\nZ. Li, E. Zhu, M. Jin, C. Fan, H. He, T. Cai, and J. Li, “Dynamic domain"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“Prediction\nof\nadvertisement\npreference\nby\nfusing\neeg\nresponse\nand",
          "17": "adaptation for class-aware cross-subject and cross-session eeg emotion"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "sentiment analysis,” Neural Networks, vol. 92, pp. 77–88, 2017.",
          "17": "IEEE\nJournal\nof\nBiomedical\nand Health\nrecognition,”\nInformatics,"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[41] A. Nandi,\nF. Xhafa, L.\nSubirats,\nand\nS.\nFort,\n“Real-time\nemotion",
          "17": "vol. 26, no. 12, pp. 5964–5973, 2022."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "classification using eeg data\nstream in e-learning contexts,” Sensors,",
          "17": "[61]\nL. Feng, C. Cheng, M. Zhao, H. Deng,\nand Y. Zhang,\n“Eeg-based"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "vol. 21, no. 5, p. 1589, 2021.",
          "17": "emotion\nrecognition\nusing\nspatial-temporal\ngraph\nconvolutional\nlstm"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Journal of Biomedical and Health\nwith attention mechanism,”",
          "18": "Fanyu Gong received her B.S. degree from Xi’an"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Informatics, vol. 26, no. 11, pp. 5406–5417, 2022.",
          "18": "Jiaotong University in 2023. She is currently pursu-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[62] R. Zhou, Z. Zhang, H. Fu, L. Zhang, L. Li, G. Huang, F. Li, X. Yang,",
          "18": "ing the M.S. degree with the MOEKLINNS Labo-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Y\n. Dong, Y.-T. Zhang, and Z. Liang, “Pr-pl: A novel prototypical\nrep-",
          "18": "ratory, Department of Computer Science and Tech-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "resentation based pairwise learning framework for emotion recognition",
          "18": "nology, Xi’an Jiaotong University. Her\nresearch in-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "using eeg signals,” IEEE Transactions on Affective Computing, vol. 15,",
          "18": "terests include EEG-based emotion recognition."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "no. 2, pp. 657–670, 2024.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[63]\nJ. Ma, H. Tang, W.-L. Zheng, and B.-L. Lu, “Emotion recognition using",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "multimodal\nresidual\nlstm network,” in ACM MM, 2019, pp. 176–183.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[64] M. Soleymani, S. Asghari-Esfeden, Y. Fu, and M. Pantic, “Analysis of",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "eeg signals\nand facial\nexpressions\nfor\ncontinuous\nemotion detection,”",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE Transactions on Affective Computing, vol. 7, no. 1, pp. 17–28,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "2016.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[65] M. Soleymani, S. Asghari-Esfeden, M. Pantic, and Y. Fu, “Continuous",
          "18": "Chengxi Xie is currently pursuing the B.S. degree in"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "emotion detection using eeg signals and facial expressions,” in ICME.",
          "18": "Joint School of Design and Innovation, Xi’an Jiao-"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "IEEE, 2014, pp. 1–6.",
          "18": "tong University, Xi’an 710049, China. His research"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[66] Y. Yang, X. Xia, D. Lo, and J. Grundy, “A survey on deep learning for",
          "18": "interests\ninclude\nemotion recognition and machine"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "software engineering,” ACM Computing Surveys\n(CSUR), vol. 54, no.",
          "18": "learning."
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "10s, pp. 1–73, 2022.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "[67] M. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, P. Sidike, M. S.",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Nasrin, M. Hasan, B. C. Van Essen, A. A. Awwal,\nand V. K. Asari,",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "“A state-of-the-art\nsurvey on deep learning theory and architectures,”",
          "18": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "electronics, vol. 8, no. 3, p. 292, 2019.",
          "18": ""
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": ""
        },
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": "Huan Liu received his B.S. degree and Ph.D. degree"
        },
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": "from Xi’an Jiaotong University in 2013 and 2020,"
        },
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": "respectively. He is currently an Associate Professor"
        },
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": "at\nthe Department of Computer Science and Tech-"
        },
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": "nology, Xi’an\nJiaotong University, Xi’an\n710049,"
        },
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": "China. His research interests include affective com-"
        },
        {
          "electronics, vol. 8, no. 3, p. 292, 2019.": "puting, machine learning, and deep learning."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "B.S.\ndegree"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "Professor at"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "His\nresearch"
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": ""
        },
        {
          "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021": "of Things."
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion, cognition, and behavior",
      "authors": [
        "R Dolan"
      ],
      "year": "2002",
      "venue": "Science"
    },
    {
      "citation_id": "2",
      "title": "Eeg-based multimodal emotion recognition: a machine learning perspective",
      "authors": [
        "H Liu",
        "T Lou",
        "Y Zhang",
        "Y Wu",
        "Y Xiao",
        "C Jensen",
        "D Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "3",
      "title": "Torcheegemo: A deep learning toolbox towards eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S.-H Zhong",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "4",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "Confidence-aware multi-teacher knowledge distillation",
      "authors": [
        "H Zhang",
        "D Chen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Eeg-based emotion classification using deep belief networks",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "Y Peng",
        "B.-L Lu"
      ],
      "year": "2014",
      "venue": "ICME"
    },
    {
      "citation_id": "7",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "8",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "ICONIP"
    },
    {
      "citation_id": "9",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Transformers for eegbased emotion recognition: A hierarchical spatial information learning model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "11",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "12",
      "title": "A channel-fused dense convolutional network for eeg-based emotion recognition",
      "authors": [
        "Z Gao",
        "X Wang",
        "Y Yang",
        "Y Li",
        "K Ma",
        "G Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "13",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "14",
      "title": "Eegbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Gcb-net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "T Zhang",
        "X Wang",
        "X Xu",
        "C Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Pgcn: Pyramidal graph convolutional network for eeg emotion recognition",
      "authors": [
        "M Jin",
        "C Du",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "20",
      "title": "Exploiting the intrinsic neighborhood semantic structure for domain adaptation in eeg-based emotion recognition",
      "authors": [
        "Y Yang",
        "Z Wang",
        "Y Song",
        "Z Jia",
        "B Wang",
        "T.-P Jung",
        "F Wan"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "NER"
    },
    {
      "citation_id": "22",
      "title": "Eeg-based emotion recognition with emotion localization via hierarchical selfattention",
      "authors": [
        "Y Zhang",
        "H Liu",
        "D Zhang",
        "X Chen",
        "T Qin",
        "Q Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Autoeer: automatic eeg-based emotion recognition with neural architecture search",
      "authors": [
        "Y Wu",
        "H Liu",
        "D Zhang",
        "Y Zhang",
        "T Lou",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "24",
      "title": "Cross-modal credibility modelling for eeg-based multimodal emotion recognition",
      "authors": [
        "Y Zhang",
        "H Liu",
        "D Wang",
        "D Zhang",
        "T Lou",
        "Q Zheng",
        "C Quek"
      ],
      "year": "2024",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "25",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "{TensorFlow}: a system for {Large-Scale} machine learning",
      "authors": [
        "M Abadi",
        "P Barham",
        "J Chen",
        "Z Chen",
        "A Davis",
        "J Dean",
        "M Devin",
        "S Ghemawat",
        "G Irving",
        "M Isard"
      ],
      "year": "2016",
      "venue": "OSDI"
    },
    {
      "citation_id": "27",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "28",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "29",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "30",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "31",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "32",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "33",
      "title": "Large brain model for learning generic representations with tremendous eeg data in bci",
      "authors": [
        "W Jiang",
        "L Zhao",
        "B.-L Lu"
      ],
      "year": "2024",
      "venue": "Large brain model for learning generic representations with tremendous eeg data in bci"
    },
    {
      "citation_id": "34",
      "title": "Eeglab-an open source matlab toolbox for electrophysiological research",
      "authors": [
        "C Brunner",
        "A Delorme",
        "S Makeig"
      ],
      "year": "2013",
      "venue": "Biomedical Engineering/Biomedizinische Technik"
    },
    {
      "citation_id": "35",
      "title": "Emotionkd: a cross-modal knowledge distillation framework for emotion recognition based on physiological signals",
      "authors": [
        "Y Liu",
        "Z Jia",
        "H Wang"
      ],
      "venue": "ACM MM, 2023"
    },
    {
      "citation_id": "36",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "37",
      "title": "Lggnet: Learning from local-global-graph representations for brain-computer interface",
      "authors": [
        "Y Ding",
        "N Robinson",
        "C Tong",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "38",
      "title": "Time-frequency representation and convolutional neural network-based emotion recognition",
      "authors": [
        "S Khare",
        "V Bajaj"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "39",
      "title": "Electrocardiogram-based emotion recognition systems and their applications in healthcare-a review",
      "authors": [
        "M Hasnul",
        "N Aziz",
        "S Alelyani",
        "M Mohana",
        "A Aziz"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "40",
      "title": "Prediction of advertisement preference by fusing eeg response and sentiment analysis",
      "authors": [
        "H Gauba",
        "P Kumar",
        "P Roy",
        "P Singh",
        "D Dogra",
        "B Raman"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "41",
      "title": "Real-time emotion classification using eeg data stream in e-learning contexts",
      "authors": [
        "A Nandi",
        "F Xhafa",
        "L Subirats",
        "S Fort"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "42",
      "title": "Emotion recognition from multi-channel eeg via deep forest",
      "authors": [
        "J Cheng",
        "M Chen",
        "C Li",
        "Y Liu",
        "R Song",
        "A Liu",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "43",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Brain emotion perception inspired eeg emotion recognition with deep reinforcement learning",
      "authors": [
        "D Li",
        "L Xie",
        "Z Wang",
        "H Yang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "45",
      "title": "Effective emotion recognition by learning discriminative graph topologies in eeg brain networks",
      "authors": [
        "C Li",
        "P Li",
        "Y Zhang",
        "N Li",
        "Y Si",
        "F Li",
        "Z Cao",
        "H Chen",
        "B Chen",
        "D Yao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "46",
      "title": "Ernn: A biologically inspired feedforward neural network to discriminate emotion from eeg signal",
      "authors": [
        "R Khosrowabadi",
        "C Quek",
        "K Ang",
        "A Wahab"
      ],
      "year": "2013",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "47",
      "title": "Emotion estimation from eeg signals during listening to quran using psd features",
      "authors": [
        "M Alsolamy",
        "A Fattouh"
      ],
      "year": "2016",
      "venue": "CSIT"
    },
    {
      "citation_id": "48",
      "title": "Llm-enhanced multi-teacher knowledge distillation for modality-incomplete emotion recognition in daily healthcare",
      "authors": [
        "Y Zhang",
        "H Liu",
        "Y Xiao",
        "M Amoon",
        "D Zhang",
        "D Wang",
        "S Yang",
        "C Quek"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "49",
      "title": "Eeg data augmentation for emotion recognition using a conditional wasserstein gan",
      "authors": [
        "Y Luo",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "EMBC"
    },
    {
      "citation_id": "50",
      "title": "Beyond mimicking under-represented emotions: Deep data augmentation with emotional subspace constraints for eeg-based emotion recognition",
      "authors": [
        "Z Zhang",
        "S Zhong",
        "Y Liu"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Data augmentation for enhancing eeg-based emotion recognition with deep generative models",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "Z.-Y Wan",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering",
      "doi": "10.1088/1741-2552/abb580"
    },
    {
      "citation_id": "52",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "53",
      "title": "Variational instance-adaptive graph for eeg emotion recognition",
      "authors": [
        "T Song",
        "S Liu",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "Y Li",
        "X Zhou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "55",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Miranda-Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "56",
      "title": "A multi-domain adaptive graph convolutional network for eeg-based emotion recognition",
      "authors": [
        "R Li",
        "Y Wang",
        "B.-L Lu"
      ],
      "venue": "ACM MM, 2021"
    },
    {
      "citation_id": "57",
      "title": "An adversarial discriminative temporal convolutional network for eeg-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "58",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "59",
      "title": "Sparse bayesian learning for end-to-end eeg decoding",
      "authors": [
        "W Wang",
        "F Qi",
        "D Wipf",
        "C Cai",
        "T Yu",
        "Y Li",
        "Y Zhang",
        "Z Yu",
        "W Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "60",
      "title": "Dynamic domain adaptation for class-aware cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "Z Li",
        "E Zhu",
        "M Jin",
        "C Fan",
        "H He",
        "T Cai",
        "J Li"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "61",
      "title": "Eeg-based emotion recognition using spatial-temporal graph convolutional lstm with attention mechanism",
      "authors": [
        "L Feng",
        "C Cheng",
        "M Zhao",
        "H Deng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "62",
      "title": "Pr-pl: A novel prototypical representation based pairwise learning framework for emotion recognition using eeg signals",
      "authors": [
        "R Zhou",
        "Z Zhang",
        "H Fu",
        "L Zhang",
        "L Li",
        "G Huang",
        "F Li",
        "X Yang",
        "Y Dong",
        "Y.-T Zhang",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Emotion recognition using multimodal residual lstm network",
      "authors": [
        "J Ma",
        "H Tang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "ACM MM"
    },
    {
      "citation_id": "64",
      "title": "Analysis of eeg signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "65",
      "title": "Continuous emotion detection using eeg signals and facial expressions",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "M Pantic",
        "Y Fu"
      ],
      "year": "2014",
      "venue": "ICME"
    },
    {
      "citation_id": "66",
      "title": "A survey on deep learning for software engineering",
      "authors": [
        "Y Yang",
        "X Xia",
        "D Lo",
        "J Grundy"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "67",
      "title": "A state-of-the-art survey on deep learning theory and architectures",
      "authors": [
        "M Alom",
        "T Taha",
        "C Yakopcic",
        "S Westberg",
        "P Sidike",
        "M Nasrin",
        "M Hasan",
        "B Van Essen",
        "A Awwal",
        "V Asari"
      ],
      "year": "2019",
      "venue": "A state-of-the-art survey on deep learning theory and architectures"
    },
    {
      "citation_id": "68",
      "title": "His research interest includes affective computing and brain-computer interface. Mengze Wang received her B.E. degree from Xi'an Jiaotong University in 2024. She is currently pursuing the Master's degree with the MOEKLINNS Lab, Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an 710049, China",
      "year": "1989",
      "venue": "His research interest includes brain-computer interface. Zejun Liu is currently pursuing the B.S. degree in Joint School of Design and Innovation"
    }
  ]
}