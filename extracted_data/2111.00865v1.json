{
  "paper_id": "2111.00865v1",
  "title": "Memobert: Pre-Training Model With Prompt-Based Learning For Multimodal Emotion Recognition",
  "published": "2021-10-27T09:57:00Z",
  "authors": [
    "Jinming Zhao",
    "Ruichen Li",
    "Qin Jin",
    "Xinchao Wang",
    "Haizhou Li"
  ],
  "keywords": [
    "Emotion Recognition",
    "Multimodal",
    "Pretraining",
    "Prompt"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition study is hindered by the lack of labelled corpora in terms of scale and diversity, due to the high annotation cost and label ambiguity. In this paper, we propose a pre-training model MEmoBERT for multimodal emotion recognition, which learns multimodal joint representations through self-supervised learning from large-scale unlabeled video data that come in sheer volume. Furthermore, unlike the conventional \"pre-train, finetune\" paradigm, we propose a prompt-based method that reformulates the downstream emotion classification task as a masked text prediction one, bringing the downstream task closer to the pre-training. Extensive experiments on two benchmark datasets, IEMOCAP and MSP-IMPROV, show that our proposed MEmoBERT significantly enhances emotion recognition performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic Multimodal Emotion Recognition aims to interpret human emotions through multiple modalities such as text, audio and visual, that is an enabling technology for many applications including human-computer interactions  [1] . Many previous works have explored multimodal fusion strategies. For example, MFN  [2]  and MARN  [3]  are attention-and memory-based approaches applicable to sequences aligned at the word-level. MulT  [4]  is a transformerbased framework, which handles both non-aligned multimodal sequences and long-range dependencies.\n\nIn recent years, various pre-trained models via selfsupervised learning on large-scale unlabeled data have achieved promising results. The pre-trained language representation models, such as BERT  [5] , ELMo  [6]  and GPT  [7] , have attracted much attention and are widely adopted. Inspired by the success of self-supervised pre-training in textual modality, many visual+language cross-modality pre-trained models are proposed  [8, 9, 10]  and have achieved new state-ofthe-art performances on various tasks, such as Image-Text *Corresponding author.\n\nRetrieval, Visual Question Answering etc. Such models, including UNITER  [10]  and LXMERT  [11] , typically follow a single-or dual-stream multi-layer Transformer architecture and optimize through several pre-training tasks to learn joint multimodal representations, such as Masked Language Modeling (MLM), Masked Region Modeling (MRM), Image-Text Matching (ITM). VideoBERT  [12] , ActBERT  [13]  and HERO  [14]  extend to the video+language representation learning for the video-related tasks, such as Video-Text Retrieval, Video Action Classification. Furthermore, VATT  [15]  considers three modalities (text, visual and audio) with a modality-agnostic Transformer and uses multimodal contrastive learning to learn multimodal representations.\n\nPrompt-based learning  [16] , on the other hand, has achieved great success and has become a new learning paradigm in NLP. Compared to the \"pre-train, finetune\" paradigm, which adapts pre-trained models to downstream tasks via objective engineering, the \"pre-train, prompt and predict\" paradigm reformulates the downstream tasks to resemble the masked language modeling task optimized in the original pre-training with the help of a textual prompt. This paradigm brings the downstream tasks closer to the pre-training tasks, which can retain more learned knowledge during pre-training. It outperforms the \"pre-train, finetune\" paradigm, especially under low-resource conditions, and has achieved promising results in many NLP tasks, such as Question Answering  [17, 18] , Text Classification  [19, 20] .\n\nMotivated by the above studies, we propose a multimodal transformer-based pre-training model, MEmoBERT, to learn joint multimodal representations for emotion recognition. It is trained through self-supervised learning based on a large-scale unlabeled video dataset comprising more than 300 movies. We design four efficient self-supervised pre-training tasks to learn joint multimodal emotional representations, including Whole Word Masked Language Modeling (WWMLM), Span Masked Visual Frame Modeling (SpanMVFM), Span Masked Visual Frame Classification with KL-divergence (SpanMVFC-KL), Span Masked Acoustic Frame Modeling (SpanMAFM). Furthermore, we explore a prompt-based learning method to efficiently adapt the pretrained model to multimodel emotion recognition. To the best of our knowledge, the proposed MEmoBERT is the first mul-  timodal pre-training model in the area of multimodal emotion recognition, and it is also the first attempt to adopt promptbased learning for this area. We carry out experiments on two benchmark datasets, IEMOCAP and MSP-IMPROV, to evaluate our pre-trained MEmoBERT. We specifically compare the proposed prompt-based learning method with the traditional \"pre-train, finetune\" method on full-and part-training data. The experiments results demonstrate that our MEmo-BERT yields significant improvement and the prompt-based learning method further improves the performance.\n\nThe main contributions of this work include, 1) We proposed a multimodal transformer-based pre-trained model, MEmoBERT, under self-supervised learning on large-scale unlabeled movies dataset for multimodel emotion recognition. 2) We propose a prompt-based learning method that better adapts the pre-trained MEmoBERT to downstream multimodal emotion recognition tasks. 3) Our proposed model achieves a new state-of-the-art performance on both IEMOCAP and MSP multimodal emotion recognition benchmark datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "Fig.  1  illustrates the overall model framework of our proposed MEmoBERT and its learning process during pretraining. MEmoBERT consists of three independent modality Encoders to generate modality-specific token-or frame-level raw features for the textual, visual, and acoustic modalities, and three Embedders to generate embeddings based on corresponding raw features respectively. Specifically, the embedding layer in BERT  [5]  is adopted as the Text Encoder. The Visual Encoder is a pre-trained facial expression model that generates the visual expression features based on the speaker faces. The Acoustic Encoder is a pre-trained speech model that generates the acoustic features based on the audio waveform. The final embedding for each modality is obtained via its modality Embedder which sums up the raw features, position embedding and type embedding, and then gets normalized via Layer Norm. Please note that the parameters of Acoustic Encoder and Visual Encoder are fixed during pretraining. A cross-modality transformer in MEmoBERT then learns cross-modality contextualized representation based on the embeddings from different modalities.\n\nWe design four efficient pre-training tasks to optimize MEmoBERT in the pre-training phase to learn joint multimodal emotional representations. Once the model is well pre-trained, we adopt the prompt-based learning method to adapt it to downstream tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cross Modality Transformer",
      "text": "The cross-modality transformer adopts the most established Transformer architecture  [5]  and extends it to three modalities (text, visual and audio) for multimodal pretraining. We follow the modality-agnostic strategy  [10, 15] , that is, a single backbone Transformer is applied to any of the modalities. During pre-training, the modality-specific embeddings are fed into the multi-layer Transformer to learn highlevel cross-modality contextualized representations across different modalities.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pre-Training Tasks",
      "text": "We design four pre-training tasks including text, visual and audio modality-related tasks to enhance the crossmodality interaction and to learn joint multimodal emotional representations. In all following pre-training tasks, we adopt the conditional masking strategy instead of applying joint random masking to all modalities. It only masks one modality and keeps other modalities intact in corresponding tasks, which can learn the better latent alignment across three modalities and enables the model to learn better joint multimodal representations effectively  [10] . For example, as shown in Fig.  1 , in the case where the word \"cool\" is masked (WWMLM), our model should be able to infer the masked word based on the surrounding text, facial expressions and acoustic signal. While in the case where the Whole Word Masked Language Modeling (WWMLM) learns to predict the masked whole word conditioned on the visual and the acoustic modalities. The whole word masking strategy that masks whole word rather than sub-tokens can better capture the accurate semantics  [21] . For example, masking partial WordPiece tokens of a word may lead to completely opposite emotional semantics of the whole word, especially for words with the prefixes (e.g. \"un-\", \"im-\", \"op-\") and the suffixes (e.g \"-less\" ).\n\nSpan Masked Acoustic Frame Regression (SpanMAFR) learns to reconstruct the masked acoustic frame features conditioned on the textual and visual modalities  [10] . We apply L2 regression as the objective function to minimize the reconstruction error between the predicted and ground-truth frames. Furthermore, inspired by the span masking strategy  [22, 23]  that aims to avoid the model exploiting the local smoothness of acoustic frames, we apply the span masking strategy that masks consecutive frames to zero. It can ensure the model to capture global emotional expression rather than local information. Span Masked Visual Frame Regression (SpanMVFR) learns to reconstruct the masked visual facial expression features conditioned on the textual and acoustic modalities  [10] . Due to the similarity of consecutive visual frames, similar to that in the acoustic modality, we also apply the span masking strategy for the visual modality. Span Masked Visual Frame Classification with KLdivergence (SpanMVFC-KL) learns to predict the facial expression class (such as happy, sad, anger) for each masked visual frame conditioned on the textual and acoustic modalities. We first feed the Transformer output of the masked frame into a fully connected layer to predict the emotion distribution of K facial expression classes. Finally, we use the KL-divergence objective function to optimize the predicted emotion distributions with respect to the ground-truth emotion distributions which is produced by a pre-trained facial expression recognition model (Sec. 3.3.1). 3 Experiments",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Prompt-Based Emotion Classification",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Training Dataset",
      "text": "Learning a pre-trained model for multimodal emotion recognition requires large-scale multimodal emotional data. We collect 351 movies and TV series that should belong to these categories, such as family, romance, soap opera, which have rich and natural emotional expressions. We extract the speakers' faces, audio, and corresponding subtitles of each utterance and filter out the empty utterances. In the end, we build a pre-training dataset containing about 180k utterances with three modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Benchmark Datasets",
      "text": "We evaluate our proposed MEmoBERT model on two benchmark multimodal emotion recognition datasets, including IEMOCAP  [24]  and MSP-IMPROV  [25] . We follow the emotional label processing in  [26]  to form the four-class emotion recognition setup. The statistics of the datasets are shown in Table .  Table  1 . A summary of the benchmark datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Implementation Details",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modality Raw Feature Extraction Acoustic:",
      "text": "We extract the frame-level acoustic features from a pre-trained Wav2Vec2.0 model  [27] . We sub-sample the frame-level features by average pooling every 3 frames.\n\nVisual: We first design an active speaker detection strategy based on the consistency of voice activation and mouth movement to get the speaker's faces. We then extract the face-level features and emotional probability distributions of the speaker's faces from a pre-trained DenseNet model  [28]  trained on a facial expression corpus, FER+  [29] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiment Setups",
      "text": "During MEmoBERT pre-training, we first initialize its weights from a text pre-trained BERT checkpoint 1  . Specifically, MEmoBERT uses the same backbone architecture as BERT. For text modality, we follow the masking strategy used in BERT  [5] . For the visual and acoustic masking strategy, we follow Mockingjay  [22]  and set the consecutive masking number as 3. We use AdamW optimizer with initial learning rate of 5e-5 over maximum 40K steps. The batch size is 640.\n\nFor experiments of the downstream tasks, we use the 10-fold and 12-fold speaker-independent cross-validation to evaluate the models on IEMOCAP and MSP-IMPROV respectively. In each fold, we use one speaker for testing and the remaining speakers for training. We use the weighted accuracy (WA) and unweighted average recall (UAR) as the evaluation metrics. We run three times for each experiment and report the average performance. We set the initial learning rate as 5e-5 and 3e-5 for experiments on full-and part-training data respectively over maximum 15 epochs. The batch size is 32.\n\nIn order to verify the effectiveness of our model framework and the prompt-based learning method, we specifically define four experiment settings: 1) \"Direct\" denotes that we directly train the MEmoBERT followed by a new emotion classifier for downstream tasks from scratch. 2) \"BERT+Direct\" denotes that we directly finetune the MEmo-BERT followed by a new emotion classifier for downstream tasks, in which the MEmoBERT is initialized by a pre-trained text BERT. 3) \"Pretrain+Finetune\" denotes that we finetune the pre-trained MEmoBERT followed by a new emotion classifier for downstream tasks. 4) \"Pretrain+Prompt\" denotes that we adopt the prompt-based learning method based on the pre-trained MEmoBERT without introducing any additional parameters for downstream tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments Results",
      "text": "Table  2  presents the multimodal emotion recognition results on the two benchmark datasets, IEMOCAP and MSP-IMPROV. Compared to other state-of-the-art models without pre-training in the first block, \"BERT+Direct\" achieves superior performance on both datasets, which demonstrates that the pre-trained BERT language modal can benefit the multimodal emotion recognition. \"Pretrain+Finetune\" based on our pre-trained MEmoBERT achieves significant improvement compared to \"BERT+Direct\". Furthermore, \"Pre-train+Prompt\" with prompt-based learning over our pretrained MEmoBERT can bring additional improvement.  Table  3 . Ablation study of the pre-training tasks. \"spanwhole word\" refers to the span masking strategy and whole work masking strategy. \"visual pre-train tasks\" refers to \"SpanMVFR\" and \"SpanMVFC-KL\". \"acoustic pre-train task\" refers to \"SpanMAFR\".\n\nAblation of different amounts of training data. In order to validate the generalization ability of the pre-trained MEm-oBERT and prompt-based method under low-resource conditions, we conduct experiments using different amounts of training data in the downstream tasks. As shown in Fig.  3 , applying \"Finetune\" and \"Prompt\" on the pre-trained MEmo-BERT both significantly outperforms the \"BERT+Direct\" setting under all low-resource conditions, and the less training data, the more obvious the improvement brought by the pretrained MEmoBERT. The prompt-based method outperforms the finetune-based method on IEMOCAP and MSP under almost all conditions. It indicates that the prompt-based method can better adapt the pre-trained MEmoBERT to downstream tasks, especially under low-resource conditions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel multimodal transformerbased pre-trained model, MEmoBERT, under self-supervised learning on a large-scale unlabeled movie dataset for multimodal emotion recognition. We further investigate a promptbased learning method that can better adapt the pre-trained MEmoBERT to downstream tasks. Extensive experiments on two public datasets demonstrate the effectiveness of our proposed methods.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed MEmoBERT model consisting of three modality-speciﬁc Encoders, three modality-speciﬁc",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the overall model framework of our",
      "page": 2
    },
    {
      "caption": "Figure 1: , in the case where the word “cool”",
      "page": 2
    },
    {
      "caption": "Figure 2: Prompt-Prediction based on pre-trained MEmoBERT",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the “prompt, predict” paradigm. Given a",
      "page": 3
    },
    {
      "caption": "Figure 3: Performance (UAR) comparison with different",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 3: Ablation study of the pre-training tasks. “span-",
      "data": [
        {
          "IEMOCAP\nWA\nUAR": "80.01%\n81.09%\n79.46%\n80.70%\n79.73%\n80.89%\n79.48%\n80.82%",
          "MSP-IMPROV\nWA\nUAR": "72.36%\n72.22%\n70.73%\n71.01%\n71.04%\n70.88%\n71.78%\n71.52%"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Ablation study of the pre-training tasks. “span-",
      "data": [
        {
          "IEMOCAP\nWA\nUAR": "73.94%\n–\n75.60%\n74.50%\n–\n78.12%",
          "MSP-IMPROV\nWA\nUAR": "–\n–\n–\n–\n–\n68.55%"
        },
        {
          "IEMOCAP\nWA\nUAR": "74.64%\n75.76%\n77.98%\n78.98%\n79.63%\n80.61% (+1.6)\n80.01%\n81.09% (+2.1)",
          "MSP-IMPROV\nWA\nUAR": "67.17%\n65.57%\n70.08%\n69.67%\n71.77%\n71.35% (+1.7)\n72.36%\n72.22% (+2.5)"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Nickolaos Fragopanagos",
        "John Taylor"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "2",
      "title": "Memory fusion network for multi-view sequential learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "3",
      "title": "Multiattention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria"
      ],
      "year": "2018",
      "venue": "AAAI"
    },
    {
      "citation_id": "4",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang"
      ],
      "year": "2019",
      "venue": "ACL"
    },
    {
      "citation_id": "5",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee"
      ],
      "year": "2019",
      "venue": "NAACL-HLT (1)"
    },
    {
      "citation_id": "6",
      "title": "Deep contextualized word representations",
      "authors": [
        "Mark Matthew E Peters",
        "Mohit Neumann",
        "Iyyer"
      ],
      "year": "2018",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "7",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child"
      ],
      "venue": "Language models are unsupervised multitask learners"
    },
    {
      "citation_id": "8",
      "title": "Multimodal pretraining unmasked: A meta-analysis and a unified framework of vision-and-language berts",
      "authors": [
        "Emanuele Bugliarello",
        "Ryan Cotterell",
        "Naoaki Okazaki"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "9",
      "title": "Vl-bert: Pretraining of generic visual-linguistic representations",
      "authors": [
        "Weijie Su",
        "Xizhou Zhu",
        "Yue Cao"
      ],
      "year": "2019",
      "venue": "Vl-bert: Pretraining of generic visual-linguistic representations",
      "arxiv": "arXiv:1908.08530"
    },
    {
      "citation_id": "10",
      "title": "Uniter: Universal image-text representation learning",
      "authors": [
        "Yen-Chun Chen",
        "Linjie Li",
        "Licheng Yu"
      ],
      "year": "2020",
      "venue": "Uniter: Universal image-text representation learning"
    },
    {
      "citation_id": "11",
      "title": "Lxmert: Learning cross-modality encoder representations from transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "2019",
      "venue": "Lxmert: Learning cross-modality encoder representations from transformers",
      "arxiv": "arXiv:1908.07490"
    },
    {
      "citation_id": "12",
      "title": "Videobert: A joint model for video and language representation learning",
      "authors": [
        "Chen Sun",
        "Austin Myers",
        "Carl Vondrick"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "13",
      "title": "Actbert: Learning global-local video-text representations",
      "authors": [
        "Linchao Zhu",
        "Yi Yang"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "14",
      "title": "Hero: Hierarchical encoder for video+ language omni-representation pretraining",
      "authors": [
        "Linjie Li",
        "Yen-Chun Chen",
        "Yu Cheng"
      ],
      "year": "2020",
      "venue": "EMNLP"
    },
    {
      "citation_id": "15",
      "title": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text",
      "authors": [
        "Hassan Akbari",
        "Linagzhe Yuan",
        "Rui Qian"
      ],
      "year": "2021",
      "venue": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text",
      "arxiv": "arXiv:2104.11178"
    },
    {
      "citation_id": "16",
      "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "authors": [
        "Pengfei Liu",
        "Weizhe Yuan",
        "Jinlan Fu"
      ],
      "year": "2021",
      "venue": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
      "arxiv": "arXiv:2107.13586"
    },
    {
      "citation_id": "17",
      "title": "Language models as knowledge bases?",
      "authors": [
        "Fabio Petroni",
        "Tim Rocktäschel",
        "Patrick Lewis"
      ],
      "year": "2019",
      "venue": "Language models as knowledge bases?",
      "arxiv": "arXiv:1909.01066"
    },
    {
      "citation_id": "18",
      "title": "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections",
      "authors": [
        "Ruiqi Zhong",
        "Kristy Lee",
        "Zheng Zhang"
      ],
      "year": "2021",
      "venue": "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections"
    },
    {
      "citation_id": "19",
      "title": "Few-shot text generation with pattern-exploiting training",
      "authors": [
        "Timo Schick",
        "Hinrich Schütze"
      ],
      "year": "2020",
      "venue": "Few-shot text generation with pattern-exploiting training",
      "arxiv": "arXiv:2012.11926"
    },
    {
      "citation_id": "20",
      "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
      "authors": [
        "Timo Schick",
        "Hinrich Schütze"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "21",
      "title": "Pre-training with whole word masking for chinese bert",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Ting Liu",
        "Bing Qin",
        "Ziqing Yang",
        "Shijin Wang",
        "Guoping Hu"
      ],
      "year": "2019",
      "venue": "Pre-training with whole word masking for chinese bert",
      "arxiv": "arXiv:1906.08101"
    },
    {
      "citation_id": "22",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "Andy Liu",
        "Shu-Wen Yang",
        "Po-Han Chi"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "23",
      "title": "A further study of unsupervised pretraining for transformer based speech recognition",
      "authors": [
        "Dongwei Jiang",
        "Wubo Li",
        "Ruixiong Zhang"
      ],
      "year": "2021",
      "venue": "ICASSP"
    },
    {
      "citation_id": "24",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "25",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "ACL"
    },
    {
      "citation_id": "27",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "arxiv": "arXiv:2006.11477"
    },
    {
      "citation_id": "28",
      "title": "Densely connected convolutional networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "29",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer"
      ],
      "year": "2016",
      "venue": "ICMI"
    },
    {
      "citation_id": "30",
      "title": "Multi-modal attention for speech emotion recognition",
      "authors": [
        "Zexu Pan",
        "Zhaojie Luo",
        "Jichen Yang"
      ],
      "year": "2020",
      "venue": "Multi-modal attention for speech emotion recognition"
    },
    {
      "citation_id": "31",
      "title": "Semi-supervised multi-modal emotion recognition with cross-modal distribution matching",
      "authors": [
        "Jingjun Liang",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2020",
      "venue": "ACM Multimedia"
    }
  ]
}