{
  "paper_id": "2408.10328v1",
  "title": "Decoding Human Emotions: Analyzing Multi-Channel Eeg Data Using Lstm Networks",
  "published": "2024-08-19T18:10:47Z",
  "authors": [
    "Shyam K Sateesh",
    "Sparsh BK",
    "Uma D"
  ],
  "keywords": [
    "EEG",
    "Emotion Recognition",
    "LSTM",
    "Neural Networks",
    "Deep Learning",
    "Valence",
    "Arousal",
    "Dominance",
    "Likeness",
    "HCI"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition from electroencephalogram (EEG) signals is a thriving field, particularly in neuroscience and Human-Computer Interaction (HCI). This study aims to understand and improve the predictive accuracy of emotional state classification through metrics such as valence, arousal, dominance, and likeness by applying a Long Short-Term Memory (LSTM) network to analyze EEG signals. Using a popular dataset of multi-channel EEG recordings known as DEAP, we look towards leveraging LSTM networks' properties to handle temporal dependencies within EEG signal data. This allows for a more comprehensive understanding and classification of emotional parameter states. We obtain accuracies of 89.89%, 90.33%, 90.70%, and 90.54% for arousal, valence, dominance, and likeness, respectively, demonstrating significant improvements in emotion recognition model capabilities. This paper elucidates the methodology and architectural specifics of our LSTM model and provides a benchmark analysis with existing papers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "EEG is defined as the electrical activity of an alternating type recorded from the scalp surface after being picked up by metal electrodes and conductive media  [19] . The unique ability of EEG signals to provide a very descriptive temporal view of brain activity makes it an indispensable tool for understanding complex human emotional states. This capability is especially critical in contexts where the traditional means of emotion assessment are impractical or unfeasible.\n\nIn recent years, there has been a necessity for understanding and quantifying emotional responses, which has led to advancements in academic research. This has opened new doors for consumer research, mental health, and assistive technologies. The prospect of its ability to assist individuals who would otherwise not be able to express emotions through traditional ways, such as facial expressions, body language, and speech, makes this one of the exciting fields for EEG-based recognition of emotions. These individuals would include, but not be limited to, people with communication disabilities, for example, aphasia; other conditions encompass Autism Spectrum Disorder (ASD)  [7] , among others; and those who have severe physical disabilities from traumatic brain injuries or other progressive diseases, such as Amyotrophic Lateral Sclerosis (ALS)  [3] .\n\nStatistically, it has been estimated that in the United States alone, approximately 6.6 million people have been diagnosed with some communication disorder  [14] . From such figures in the global context, it can be estimated that up to 1% of the world population has some form of autism spectrum disorder. These people's emotional expression and interpretation remain very conventional, usually exhibiting a failing nature. Such failures, thus, stimulate the need to develop standalone technologies that will independently interpret emotional states from physiological data. EEG-based technologies offer a non-invasive, more direct window into the neural underpinnings of emotion. Since it measures electrical activity, the EEG provides a dynamic mapping of activity in the brain, potentially associated with states of emotion without the need for verbal reports or precise physical gestures. This approach is particularly suitable for those whose neurological conditions impair their effective communication.\n\nThe rise of Long Short-Term Memory (LSTM), a variation of Recurrent Neural Networks (RNNs), has revolutionized this field with its ability to analyze and classify EEG data at unprecedented success rates. LSTMs, in particular, are very strong at modelling the time-dependent features that underlie EEG data. They can capture such underlying patterns temporally, which indicate different emotional states. This advanced machine learning method elevates the prediction performance for emotion classification systems from EEG. It opens an avenue to building real-time responsive systems that can adapt to the emotional feedback of users in different applications. Furthermore, emotion recognition by EEG is possible in healthcare and societal applications. In healthcare, technology could offer better patient care, for instance, since it can interpret pain, discomfort, or emotional distress that patients might be unable to express. One case for this was gauging emotional states in palliative care cancer patients  [17] . In special education, EEG in non-verbal students could help teachers and caregivers explore the thoughts and emotional states of the students. This could enable tailor-made educational approaches that are more in sync with the mindset of the students  [18] .\n\nThis study aims to develop more accurate and specific tools for cognitive emotion recognition, particularly for detecting and interpreting emotional states in persons unable to express themselves by traditional means.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "The DEAP dataset, detailed by Koelstra et al.  [10] , has been foundational in the field, providing a rich data source for subsequent research. Also, significant correlations were found between the participant ratings and EEG frequencies. The single-trial classification was performed for arousal, valence, and liking scales using features extracted from the EEG, peripheral, and MCA modalities. The results were shown to be significantly better than random classification.\n\nAlhagry et al.  [2]  proposed LSTM networks for emotion recognition from raw EEG signals. Their study demonstrates that the LSTM model achieves high average accuracies across three emotional dimensions and outperforms traditional emotion recognition techniques, marking a significant advancement in the field.\n\nNie et al.  [16]  explore the relationship between EEG signals and emotional responses while watching movies, focusing on classifying emotions into positive and negative categories. Their application of a Support Vector Machine (SVM) on processed EEG features resulted in an impressive average testing accuracy of 87.53%, underscoring the potential of EEG-based methods in practical multimedia applications.\n\nLi et al.  [11]  provide a comprehensive overview of EEG-based emotion recognition, exploring the integration of psychological theories with physiological measurements. They review various machine learning techniques, from conventional models to advanced computational methods, highlighting key advancements and challenges in the field.\n\nZheng et al.  [21]  develop an innovative approach by integrating deep belief networks with hidden Markov models for EEG-based emotion classification. Their findings indicate that this combined DBN-HMM model achieves higher accuracy than traditional classifiers, highlighting its effectiveness in leveraging spatial and temporal EEG data dimensions. Bhagwat et al.  [6]  proposed a novel approach for classifying four primary emotions: happy, angry, crying, and sad, which can be visualized as four quadrants. They used Wavelet Transforms (WT) to extract features from raw EEG signals and employed a Hidden Markov Model (HMM) to classify emotions. Lin et al.  [13]  utilize EEG data and machine learning to enhance emotional state predictions during music listening. Using an SVM, their approach achieves an average classification accuracy of 82.29% for emotions such as joy, anger, sadness, and pleasure.\n\nNaser and Saha  [15]  applied advanced signal processing techniques to improve feature extraction for emotion classification from EEG signals. Their study utilizes dual-tree complex wavelet packet transform (DT-CWPT) and statistical methods like QR factorization and singular value decomposition (SVD) to select discriminative features effectively. The enhanced feature set is then classified using an SVM, demonstrating notable improvements in classification accuracy.\n\nLi et al.  [12]  found that while it is feasible to work with single-channel EEG data, it is much more effective to combine multiple channels of EEG features into a single feature vector. They also found that the beta and gamma frequency bands are more related to emotional processing than the other bands.\n\nThe exploration of adaptive emotion detection using EEG and the Valence-Arousal-Dominance model by Gannouni et al.  [8]  advances the field by adapting computational models to individual brain activity variations. Their method employs an adaptive selection of electrodes, significantly enhancing emotion detection accuracy. Utilizing machine learning algorithms, the study demonstrates a 5% and 2% increase in accuracy for valence, arousal, and dominance dimensions, respectively, compared to fixed-electrode approaches.\n\nAlvarez-Jiménez et al.  [22]  enhance EEG-based emotion recognition by integrating diverse feature sets from multiple domains. Their use of various classifiers, including Artificial Neural Networks, achieves a high accuracy of 96%, demonstrating the effectiveness of hybrid features in improving model robustness.\n\nAtkinson-Abutridy et al.  [5]  proposed a feature-based emotion recognition model combining statistical-based feature selection methods with SVM classifiers, focusing on Valence/Arousal dimensions for classification. This combined approach outperformed other recognition methods.\n\nYoon and Chung  [20]  detailed a probabilistic classifier based on Bayes' theorem and a supervised learning approach using a perceptron convergence algorithm, offering a methodologically distinct perspective on emotion classification from EEG signals.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "The Database for Emotion Analysis using Physiological Signals (DEAP)  [10]  is at the core of our study, and it presents a rich source of EEG and peripheral physiological signals for analyzing emotions. The dataset was built to boost and proliferate the development of systems that would be capable of recognizing human emotions from physiological responses, with particular emphasis on the paradigms of human-computer interaction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Description",
      "text": "The DEAP dataset consists of EEG data recordings from 32 participants between 19 and 37 years old, with a mean age of 26.9 years. Each participant was presented with 40 one-minute music video clips to elicit emotional responses. Participants had to rate their experience after each stimulus on a 1 to 9 integer scale for arousal, valence, dominance, and liking. We will use these subjective ratings as labels to train our models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data Acquisition",
      "text": "EEG and peripheral physiological signals were acquired simultaneously, viewing each music video clip by all the participants. In the course of the experiment, the recording of the EEG data was carried out at 512 Hz through the 32-channel systems, which was eventually reduced to 128 Hz during analysis. Concurrently with EEG, other physiological signals such as galvanic skin response and heart rate were also recorded to deliver complete states regarding the participant's physiological states during each trial.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Structure",
      "text": "For each participant, it is composed of two main arrays: the EEG signals and an array of labels for each trial. The EEG data array has a dimension of 40x40x8064 for 40 trials, 40 channels, and 8064 data points per channel per trial. Corresponding to four emotional dimensions assessed per video clip, the array structure of labels is 40x4. (Shown in Table  1 )  for managing these complexities due to its ability to discern high-level, abstract features from vast amounts of data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Deep Learning Vs. Traditional Statistical Methods",
      "text": "Deep learning models that handle unstructured data, like images, speech, or biological signals, perform this function due to their use of neural networks. Traditional statistical approaches to data analysis require manual choice of features, and at most, they can only model the linear effects. This is essential in EEG data, where emotional states are not explicitly encoded but latent constructs reflected in slight signal variations. Deep learning models enable learning such patterns directly from raw data, optimizing feature extraction, selection, and classification tasks in a joint form. This shows that robust and accurate analysis is developed in high dimensionality and noise levels that are usually related to EEG recordings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Overview Of Lstms",
      "text": "LSTMs are one of the unique variants of Recurrent Neural Networks (RNN). It was first introduced by Hochreiter et al.  [9]  to eliminate the problem of long-term dependencies seen in conventional RNNs. Traditional RNNs are known to also suffer from gradient-related issues. This problem, in turn, makes it very hard for them to be trained on sequential data where long-term contextual information is essential. LSTMs solve this problem due to the exceptional structure of their gates, which allows them to regulate the flow of information in a way that enables them to remember or forget information for long periods.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Bidirectional Lstms",
      "text": "The capabilities of standard LSTMs are further advanced through the usage of bidirectional LSTMs, allowing more context to be available from the subsequent points in the data sequence. So, bidirectional LSTMs can capture the context information from past and future states by processing data in the forward and reverse directions. This is very useful in emotion recognition from the EEG signals when the emotional state reflected in a data segment may depend not only on the earlier but also on the latter events.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Lstm For Our Work",
      "text": "In this project, we choose LSTM networks due to their prowess in sequence prediction problems, thus capable of adequately modelling the temporal dynamics characteristic of EEG data. Applying LSTMs will help reach the deepest emotional timelines that fall within the EEG signals, making them more helpful in predicting emotional states with better accuracy. The bidirectional approach of the capability enforces the complete context from all the data points, which increases the recognition accuracy for complex emotional states. This makes LSTMs very apt for the development of a robust system for emotion recognition from EEG-based data.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Proposed Method",
      "text": "Our study uses an LSTM model to classify emotional states from EEG data and focuses on feature extraction, data preparation, and architectural considerations to achieve high accuracy percentages.  The feature extraction process was tailored to capture significant information from the EEG signals. We utilized specific EEG channels and frequency bands relevant to emotional processing. The chosen channels included a subset correlating with emotional states, such as frontal and temporal regions. Frequency bands were segmented into five distinct ranges: theta (4-8 Hz), alpha (8-12 Hz), low beta (12-16 Hz), high beta (16-30 Hz), and gamma (30-45 Hz), which are traditionally associated with different aspects of cognitive processing and emotional regulation (Refer to Table  2  and Fig.  3 [4] ). Each of these bands aids in extracting vital information from input EEG data, which has been proven to support sentiment analysis  [4] . The Fast Fourier Transform (FFT) process was applied to a select 14 channels of the recorded 32 channels, chosen to fit Emotiv Epoc, with a window size of 256 points, corresponding to 2 seconds of data, with an overlap of 0.125 seconds to ensure comprehensive temporal analysis.  The dataset was first split into training and test splits using an 80-20 ratio. That is, 80% of the data was used to train the LSTM model, and the remaining 20% was utilized to test the model's performance. This split ratio enabled the practical training of the model as well as a reliable evaluation to establish how it generalizes to unseen data.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Deap Dataset",
      "text": "Data Normalization was necessary to normalize the input features to reduce discrepancies in signal amplitudes caused by data variations across individuals. All feature vectors were normalized to zero mean and unit variance, a standard approach in processing EEG signals to overcome inter-subject differences.\n\nThe next step was converting each valence, arousal, dominance, and likeness label (initially scaled from 1-9) into one-hot encodings to create nine classes before sending the label data into the LSTM Networks. This was implemented using the keras.utils.to categorical() function.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Lstm Architecture",
      "text": "The LSTM network architecture employed in our study is designed to handle EEG data sequentially and temporally effectively. We will use one LSTM model for each emotional parameter in observation. The model initiates with a Bidirectional LSTM layer consisting of 128 units, enhancing the model's ability to capture dependencies in both forward and backward directions of the input sequence. This layer is followed by a dropout of 0.6 to reduce overfitting by randomly ignoring a fraction of the neurons during training.\n\nSubsequent layers include multiple LSTM layers with varying numbers of neurons to extract and refine features from the data incrementally. Specifically, the model includes an LSTM layer with 256 units and another two LSTM layers, each with 64 units, all incorporating a dropout of 0.6 after each LSTM layer to prevent overfitting further. The final LSTM layer consists of 32 units, followed by a dropout of 0.4, aiming to consolidate the features extracted by previous layers into a more manageable form.\n\nThe output from the LSTM layers is then passed through two dense layers. The first dense layer has 16 units with a ReLU activation function intended to introduce non-linearity into the model, facilitating the network's ability to learn complex patterns. The final output layer consists of some units equal to the classes of emotions being classified, with a softmax activation function to output the probability distribution over the classes. This architecture is compiled with the Adam optimizer and categorical crossentropy as the loss function, suitable for multi-class classification problems. The detailed structure and parameterization of the model are crucial for its ability to discern nuanced emotional states from EEG data, as visualized in the accompanying architectural diagram in our study. A representation of the model is shown in Fig.  4 .",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Results",
      "text": "Our LSTM-based model demonstrated outstanding performance in emotion recognition from EEG data, achieving individual class accuracies of 90.33% for valence, 89.89% for arousal, 90.70% for dominance, and 90.54% for likeness, with an overall accuracy of 90.36%. These results underline the model's efficacy in capturing complex emotional states through advanced feature extraction and a robust LSTM architecture. This performance showcases the model's capabilities and sets a foundation for future advancements in EEG-based emotion recognition. A comparison of accuracies is attached in Table  3 , showing our method of using an LSTM network to be highly accurate and effective in classifying emotional parameters correctly compared to related papers.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "This study successfully showcases the efficacy of LSTM networks in accurately classifying emotional states from EEG data, achieving high performance across various emotional dimensions. The customized LSTM architecture, incorporating bidirectional layers and strategic dropout stages, adeptly handles the complexities of EEG signals. Our LSTM architecture paired with uniform frequency band ranges taken for EEG feature extraction has proven to provide improved results from previous LSTM-based EEG studies. Such capabilities pave the way for advancements in cognitive neuroscience and human-computer interaction, promising enhancements in responsive systems that adapt to user emotions in real time. Future work can further build upon this model with more robust neural networks, including time-frequency and location domain features, along with the possible usage of more than 14 EEG channels for better efficiency of emotion recognition. Upcoming research will benefit from exploring hybrid models that integrate additional physiological signals, further refining the precision and application of EEG-based emotion recognition in creating empathetic user interfaces.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Valence-Arousal-Dominance Model Depiction as A 3-D Graph [8]",
      "page": 6
    },
    {
      "caption": "Figure 4: Fig. 2: Flowchart of Proposed Scheme",
      "page": 8
    },
    {
      "caption": "Figure 3: [4]). Each of these bands aids in",
      "page": 8
    },
    {
      "caption": "Figure 3: EEG signal energy and relative sub-band energy [4]",
      "page": 9
    },
    {
      "caption": "Figure 4: Input layer",
      "page": 10
    },
    {
      "caption": "Figure 4: Our LSTM Architecture",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 2: and Fig. 3 [4]). Each of these bands aids in",
      "data": [
        {
          "Theta": "Alpha",
          "4 - 8": "8 - 12",
          "Intuitive,\ncreative,\nrecall,\nfantasy,\nimaginary, dream": "Relaxed but not drowsy, calm, con-\nscious"
        },
        {
          "Theta": "Low-Beta",
          "4 - 8": "12 - 16",
          "Intuitive,\ncreative,\nrecall,\nfantasy,\nimaginary, dream": "Relaxed yet focused,\nintegrated"
        },
        {
          "Theta": "High-Beta",
          "4 - 8": "16 - 30",
          "Intuitive,\ncreative,\nrecall,\nfantasy,\nimaginary, dream": "Alertness, agitation"
        },
        {
          "Theta": "Gamma",
          "4 - 8": "30 - 45",
          "Intuitive,\ncreative,\nrecall,\nfantasy,\nimaginary, dream": "Cognition,\ninformation processing"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: , showing our method",
      "data": [
        {
          "Koelstra et al.\n[10]\nAtkinson and Campos [5]\nYoon and Chung [20]\nNaser and Saha [15]\nAlhagry et al.\n[2]\nLi et al.\n[11]\nAcharya D et al.\n[1]": "Proposed Method",
          "62.00\n73.06\n70.10\n66.20\n85.65\n83.78\n–": "89.89",
          "56.70\n73.41\n70.90\n64.30\n85.45\n80.72\n–": "90.33",
          "55.40\n–\n–\n70.20\n87.99\n–\n88.60": "90.54",
          "Frequency Based\nStatistical Based\nFrequency Based\nTime-Frequency Based\nFrequency Based\nFrequency Based\nFrequency Based": "Frequency Based",
          "(4-7), (8-13), (14-29), (30-47)\n-\n(4-8), (8-13), (13-30), (36-44)\n-\n(4-8), (8-10), (8-12), (12-30), (30+)\n(4-8), (8-13), (13-30), (30-45)\n(4-8), (8-12), (12-16), (16-25), (25-45)": "(4-8), (8-12), (12-16), (16-30), (30-45)"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Sanika Prashant Deshmukh, and Arpit Bhardwaj. Multi-class emotion classification using eeg signals",
      "authors": [
        "Divya Acharya",
        "Riddhi Jain",
        "Siba Smarak Panigrahi",
        "Rahul Sahni",
        "Siddhi Jain"
      ],
      "year": "2021",
      "venue": "Advanced Computing"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition based on eeg using lstm recurrent neural network",
      "authors": [
        "Salma Alhagry",
        "Aly Aly Fahmy",
        "Reda El-Khoribi"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications (IJACSA)"
    },
    {
      "citation_id": "3",
      "title": "Eegbased brain-computer interface methods with the aim of rehabilitating advanced stage als patients",
      "authors": [
        "Manouchehr Shamseini",
        "Ghiyasvand Alireza",
        "Majid Pouladian"
      ],
      "year": "2024",
      "venue": "Disability and Rehabilitation: Assistive Technology"
    },
    {
      "citation_id": "4",
      "title": "Classification of eeg signals based on pattern recognition approach",
      "year": "2017",
      "venue": "Frontiers in Computational Neuroscience"
    },
    {
      "citation_id": "5",
      "title": "Improving bci-based emotion recognition by combining eeg feature selection and kernel classifiers",
      "authors": [
        "John Atkinson",
        "Daniel Campos"
      ],
      "year": "2016",
      "venue": "Improving bci-based emotion recognition by combining eeg feature selection and kernel classifiers"
    },
    {
      "citation_id": "6",
      "title": "Human disposition detection using eeg signals",
      "authors": [
        "R Anuja",
        "A Bhagwat",
        "Paithane"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Computing, Analytics and Security Trends (CAST)"
    },
    {
      "citation_id": "7",
      "title": "Eeg changes associated with autistic spectrum disorders",
      "authors": [
        "N Nash",
        "Renee Boutros",
        "Andrew Lajiness-O'neill",
        "Anette Zillgitt",
        "Susan Richard",
        "Bowyer"
      ],
      "year": "2015",
      "venue": "Neuropsychiatric Electrophysiology"
    },
    {
      "citation_id": "8",
      "title": "Adaptive emotion detection using the valence-arousal-dominance model and eeg brain rhythmic activity changes in relevant brain lobes",
      "authors": [
        "Sofien Gannouni",
        "Arwa Aledaily",
        "Kais Belwafi",
        "Hatim Aboalsamh"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "9",
      "title": "Long Short-Term Memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "10",
      "title": "Deap: A database for emotion analysis ;using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Eeg based emotion recognition: A tutorial and review",
      "authors": [
        "Xiang Li",
        "Yazhou Zhang",
        "Prayag Tiwari",
        "Dawei Song",
        "Bin Hu",
        "Meihong Yang",
        "Zhigang Zhao",
        "Neeraj Kumar",
        "Pekka Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "12",
      "title": "Channel division based multiple classifiers fusion for emotion recognition using eeg signals",
      "authors": [
        "Xian Li",
        "Jian-Zhuo Yan",
        "Jian- Hui"
      ],
      "year": "2017",
      "venue": "ITM Web Conf"
    },
    {
      "citation_id": "13",
      "title": "Eeg-based emotion recognition in music listening: A comparison of schemes for multiclass support vector machine",
      "authors": [
        "Yuan-Pin Lin",
        "Chi-Hong Wang",
        "Tien-Lin Wu",
        "Shyh-Kang Jeng",
        "Jyh-Horng Chen"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Prevalence and etiologies of adult communication disabilities in the united states: Results from the 2012 national health interview survey",
      "authors": [
        "Megan Morris",
        "Sarah Meier",
        "Joan Griffin",
        "Megan Branda",
        "Sean Phelan"
      ],
      "year": "2016",
      "venue": "Disability and Health Journal"
    },
    {
      "citation_id": "15",
      "title": "Recognition of emotions induced by music videos using dt-cwpt",
      "authors": [
        "Daimi Syed",
        "Goutam Saha"
      ],
      "year": "2013",
      "venue": "2013 Indian Conference on Medical Informatics and Telemedicine (ICMIT)"
    },
    {
      "citation_id": "16",
      "title": "Eeg-based emotion recognition during watching movies",
      "authors": [
        "Dan Nie",
        "Xiao-Wei Wang",
        "Li-Chen Shi",
        "Bao-Liang Lu"
      ],
      "year": "2011",
      "venue": "2011 5th International IEEE/EMBS Conference on Neural Engineering"
    },
    {
      "citation_id": "17",
      "title": "Eeg-based analysis of the emotional effect of music therapy on palliative care cancer patients",
      "authors": [
        "Rafael Ramirez",
        "Josep Planas",
        "Nuria Escude",
        "Jordi Mercade",
        "Cristina Farriols"
      ],
      "year": "2018",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "18",
      "title": "Eeg-based emotion recognition: A state-of-the-art review of current trends and opportunities",
      "authors": [
        "James Nazmi Sofian Suhaimi",
        "Jason Mountstephens",
        "Teo"
      ],
      "year": "2020",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "19",
      "title": "Fundamental of eeg measurement",
      "authors": [
        "Michal Teplan"
      ],
      "year": "2002",
      "venue": "MEASUREMENT SCIENCE REVIEW"
    },
    {
      "citation_id": "20",
      "title": "Eeg-based emotion estimation using bayesian weighted-log-posterior function and perceptron convergence algorithm",
      "authors": [
        "Hyun Joong",
        "Seong Youb"
      ],
      "year": "2013",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "21",
      "title": "Eeg-based emotion classification using deep belief networks",
      "authors": [
        "Wei-Long Zheng",
        "Jia-Yi Zhu",
        "Yong Peng",
        "Bao-Liang Lu"
      ],
      "venue": "Eeg-based emotion classification using deep belief networks"
    },
    {
      "citation_id": "22",
      "title": "A comprehensive evaluation of features and simple machine learning algorithms for electroencephalographic-based emotion recognition",
      "authors": [
        "Mayra Álvarez",
        "Tania Calle-Jimenez",
        "Myriam Alvarez"
      ],
      "venue": "Applied Sciences"
    }
  ]
}