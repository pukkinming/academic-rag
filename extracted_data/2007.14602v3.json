{
  "paper_id": "2007.14602v3",
  "title": "Transformer Based Unsupervised Pre-Training For Acoustic Representation Learning",
  "published": "2020-07-29T05:11:09Z",
  "authors": [
    "Ruixiong Zhang",
    "Haiwei Wu",
    "Wubo Li",
    "Dongwei Jiang",
    "Wei Zou",
    "Xiangang Li"
  ],
  "keywords": [
    "unsupervised pre-training",
    "Transformer",
    "acoustic representation learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, a variety of acoustic tasks and related applications arised. For many acoustic tasks, the labeled data size may be limited. To handle this problem, we propose an unsupervised pre-training method using Transformer based encoder to learn a general and robust high-level representation for all acoustic tasks. Experiments have been conducted on three kinds of acoustic tasks: speech emotion recognition, sound event detection and speech translation. All the experiments have shown that pre-training using its own training data can significantly improve the performance. With a larger pre-training data combining MuST-C, Librispeech and ESC-US datasets, for speech emotion recognition, the UAR can further improve absolutely 4.3% on IEMOCAP dataset. For sound event detection, the F1 score can further improve absolutely 1.5% on DCASE2018 task5 development set and 2.1% on evaluation set. For speech translation, the BLEU score can further improve relatively 12.2% on En-De dataset and 8.4% on En-Fr dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The goal of acoustic representation learning is to transform raw or surface features into the high-level feature which are more accessible to acoustic tasks  [1] . It is critical to make acoustic representations more general and robust to improve the performance of acoustic tasks. However, the labeled data size of the specific acoustic task may be limited so that the learned representations can be less robust and the performance can be vulnerable to unseen data. On the other hand, there exists varieties of acoustic tasks which range from speaker verification, speech recognition to event and scene detection. For supervised learning, the learned representation useful for one task may be less suited for another task. It is worthwhile to explore how to utilize all kinds of datasets to learn a general and robust representation for all kinds of acoustic tasks.\n\nUnsupervised pre-training can provide an appealing method to learn more general and robust high-level features that are less specialized towards solving a single supervised task. The training objective of unsupervised pre-training is only related with acoustic features themselves and is not dependent on any other downstream target. Because of this advantage, much more unlabeled data can be utilized so that a larger and more general model can be learned. At the same time, the learned representations can be directly utilized or fine-tuned for specific downstream tasks.\n\nContrastive Predictive Coding(CPC)  [2]  has provided a universal unsupervised learning approach to extract useful representations from high-dimensional data. The autoregressive mechanism is used for predicting future information. However, it can only be applied in uni-directional models. Masked Predictive Coding(MPC)  [3]  has been proposed to utilize speech data in an unsupervised manner for speech recognition. It uses the bidirectional transformer based architecture and uses Masked-LM  [4]  like structure to perform predictive coding. The pre-trained representations can be further fine-tuned to improve specific speech recognition tasks. However, the speech or acoustic representation pre-trained from this method has not yet been applied to other kinds of acoustic tasks and also the performance of this unsupervised pre-training method on non-speech audio tasks remains unknown.\n\nIn this paper, we get intuition from MPC and utilize a Transformer  [5]  based unsupervised pre-training method for acoustic representation learning. Transformer based encoder can be pre-trained by a large amount of unlabeled audio from various kinds of datasets. After pre-training, all we should do is to add a decoder layer targeted for downstream tasks and fine-tune the whole model. we have demonstrated that our method can learn a more general and robust acoustic representation which can significantly improve the performance of various kinds of acoustic tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Contrastive Predictive Coding(CPC) provided a universal unsupervised learning approach and the learned representation is able to achieve strong performance on four domains: speech, images, text and reinforcement learning in 3D environments. This model is mainly composed of two parts: a non-linear encoder g enc and an autoregressive model g ar . Given an input sequence (x 1 , x 2 , ..., x T ), g enc encodes obser-arXiv:2007.14602v3 [eess.AS] 8 Feb 2021 vations x t to a latent embedding space z t = g enc (x t ) and g ar accepts z t to produce a context representation c t = g ar (z ≤t ). Targeting at predicting future observations x t+k ,a density ratio f (x t+k , c t ) is modelled to maximally preserve the mutual information between x t+k and c t . To optimize g enc and g ar , the contrastive loss is minimized:\n\nwhere N represents number of samples in X = x 1 , x 2 , ..., x N , with one positive sample from distribution p(x t+k |c t ) and the rest being negative samples from distribution p(x t+k ).\n\nAutoregressive Predictive Coding(APC)  [6]  also proposed an autoregressive model for unsupervised speech representation learning. It used a deep LSTM network and make the model to predict further steps ahead of the current frame during training. APCs have demonstrated a strong capability of extracting useful phone and speaker information.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "To learn a general high-level acoustic representation, we use Transformer based encoder in an unsupervised manner. The architecture of Transformer based encoder is illustrated in Figure  1(a) .\n\nFor unsupervised pre-training, Figure  1 (b) shows our pretraining procedure. 15% of frames of the acoustic feature sequence will be masked by zeros and the object of unsupervised pre-training is similar as that of  [3]  which is to restore the masked frames given the left and right context features. However, we have two aspects that are different from that of  [3] . On one hand, we have different masking mechanisms. Generally speaking, the CNN modules of Transformer based encoder provide a downsampling mechanism, by which the frames would be N-fold downsampled. Therefore, to reserve the masked information after downsampling operations, we split frames into chunks each of which contains N frames and 15% of all chunks will be selected randomly and all frames of the selected chunks will be masked by zeros. On the other hand, Transformer encoder is followed by a feed-forward layer to transform each chunk-level prediction into framelevel predictions. With these changes, we also use L1 loss to minimize the gap between the predicted frames and the corresponding real frames.\n\nFor fine-tuning, Transformer encoder needs to be pretrained only once and can be adapted to varieties of acoustic tasks no matter whether the downstream task deal with the speech or non-speech acoustic sequences, and no matter whether the output of the task is a sequence or a tag. All we should do is to add a decoder layer after the pre-trained encoder to fine-tune the whole model for specific tasks. The choice of decoder layers is based on the tasks as shown in Figure  1 (c). We can use Transformer decoder for seq-to-seq tasks and specific pooling layers for tagging tasks.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "To prove the effectiveness of our unsupervised pre-training method on various kinds of acoustic tasks, we selected three representative kinds of tasks: speech emotion recognition, acoustic event detection and speech translation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data",
      "text": "To pre-train the model using a larger dataset which can be adapted to various kinds of downstream tasks, we merge MuST-C En-De  [7] (408 hours), Librispeech  [8] (960 hours) and ESC-US  [9] ( 347 hours) datasets into one dataset(almost 1715 hours) and we call it OpenAudio. Among them, ESC is an open dataset for environmental sound classification while ESC-US is a compilation of 250k unlabeled clips which were extracted from public field recordings. MuST-C is a multilingual corpus for speech translation from English into 8 languages. For each target language, MuST-C comprises at least 385 hours of audio recordings from English TED Talks. LibriSpeech is a corpus of reading English speech with sampling rate of 16 kHz. The data has been carefully segmented and aligned.\n\nFor pre-training, we did not use speed perturbation but for fine-tuning in every downstream task, we used speed perturbation with factor of 0.9 and 1.1 for data augmentation. We use 40-dimensional Mel filter-banks extracted from the audio signals using window size of 25 ms and step size of 10 ms for pre-training and fine-tuning in all downstream tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setups",
      "text": "For Transformer based model, we use the structure discussed before with hidden dimension size of 256, feed-forward size of 2048, attention heads of 4, dropout rate of 0.1 and encoder layers of 12 for all tasks.\n\nWe pre-trained our model using OpenAudio only once and fine-tuned it in each downstream task. It was trained on 4 GPUs with a total batch size of 256 for 50 epochs. We used the Adam optimizer  [10]  with warmup schedule  [5]  according to the formula:\n\nwhere n is the step number. k = 0.5 and warmup n = 8000 were chosen for all experiments. For comparison, we also pre-trained our model on each task using its own training data with the same setups as discussed before.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "The IEMOCAP database  [11]  was commonly used in previous speech emotion studies  [12] . We also use it for our experiments. We used the recordings where majority of annotators agreed on the emotion labels and it contains 4 kinds of  emotions: angry, happy, sad and neutral state. Happy and excited emotions were combined as happy in order to balance the number of samples in each emotion class. The dataset contains 5,531 utterances (1,103 angry, 1,636 happy, 1,708 neutral, 1,084 sad) grouped into 5 sessions. We conducted 5-fold cross validation on IEMOCAP, taking samples from 8 speakers for training and the others for evaluation. For finetuning, we add an average pooling layer followed by one feedforward layer. It was trained on 4 GPUs with a total batch size of 64 for 25 epochs. We also use the optimizer which is the same as that of pre-training. For evaluating the performance, we restore the checkpoint averaged from best 5 checkpoints during training. We used UAR which is defined as the unweighted average of the class-specific recalls achieved by the system as our metrics.\n\nIn our experiments as shown in Table  1 , we achieve a mean UAR of 64.9% which is significantly better than the state-of-the-art result on this setup. According to  [13]  and the best of our knowledge,  [14]  and  [15]  presented the best results in the condition that almost match our setups. Specifically, they all use 4 emotion classes and merge happy and excited as one class, except that they used leave-one-speaker-out cross validation and we use leave-one-session-out cross validation. Compared with  [13]  which has provided another unsupervised pre-training method, our Transformer based model with pre-training can achieve better performance. We used DCASE2018 task5 dataset  [16]  for sound event detection. It contains a continuous recording of one person living in a vacation home over a period of one week. The continuous recordings were split into audio segments of 10s and each segment represents one activity. The dataset presents 10 kinds of activities like cooking, eating and so on. The DCASE2018 task5 has provided development and evaluation datasets for evaluation and test. We use the macro-averaged F1-score as the metrics of this task. It was trained on 4 GPUs with a total batch size of 128 for 50 epochs. We also use the optimizer which is the same as that of pre-training except that k = 0.3. For evaluating the performance, we restore the checkpoint averaged from best 5 checkpoints during training. Similar to speech emotion recognition, we used an average pooling layer as the decoder layer for finetuning. We compared our work with top three teams' technical reports  [17, 18, 19]  listed on the DCASE community website. Table  2  shows that with pre-training using OpenAudio, Transformer based model can achieve better performance than all of them on the development set and one of them on the evaluation set. Consider that they used well-designed hand-crafted features with various kinds of data augmentation and ensemble tricks, our method presents a simple but effective training scheme. The aim of speech translation is to translate one language directly from the speech into another language. We used MuST-C English-to-German(En-De) and English-to-French(En-Fr) datasets  [7]  which were commonly used in previous speech translation studies  [20, 21] . For fine-tuning, we used a 6-layer Transformer decoder as the decoder layer. To avoid overfitting, we also used label smoothing with the rate of 0.1. Similar to  [21] , we used 8k vocabularies based on byte pair encoding (BPE)  [22] . It was trained on 4 GPUs with a total batch size of 512 for 50 epochs. We also use the optimizer which is the same as that of pre-training except that k = 2.5 and warmup n = 25000. For evaluating the performance, we restore the checkpoint averaged from best 5 checkpoints during training. We used beam search with beam size of 10 and performance was evaluated using case-sensitive 4-gram BLEU  [23]  on the tst-COMMON set.\n\nAccording to  [21]  and  [20] , for end-to-end speech translation, Transformer based model has provided state-of-the-art results on MuST-C datasets. However, its performance depends on ASR pre-training which needs English transcripts. In our experiments as shown in Table  3 , we do not need English transcripts and the performance of Transformer pretrained by its own training audio can be comparable with that of Transformer pre-trained by ASR. Furthermore, because we can easily extend our pre-training data without any specific label, we evaluated the results of Transformer pre-trained by OpenAudio. The results have shown that the BLEU scores have exceeded that of  [21]  pre-trained by ASR on both datasets.\n\nWe can see that different from current end-to-end speech translation methods, our methods provide not only better performance but an easier training scheme without transcripts of speech in same language which is more practical for industrial application. It is also promising that combining our unsupervised pre-training method with the current supervised pretraining mechanism will further improve the performance. Compared with current state-of-the-art acoustic systems, our method is able to provide a more general and robust acoustic representation for all acoustic tasks and it is easy to be transferred, easy to be built without many hand-crafted designs and is more practical for industrial applications. It suggests that our method can provide a promising alternative for acoustic representation learning.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (b) shows our pre-",
      "page": 2
    },
    {
      "caption": "Figure 1: (c). We can use Transformer decoder for seq-to-seq",
      "page": 2
    },
    {
      "caption": "Figure 1: training structure and procedure: (a) The structure of Transformer based encoder. (b) Pre-training: it is trained to predict",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "DiDi Chuxing, Beijing, China": "ABSTRACT"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "Recently, a variety of acoustic tasks and related applications"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "arised. For many acoustic tasks, the labeled data size may be"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "limited. To handle this problem, we propose an unsupervised"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "pre-training method using Transformer based encoder to learn"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "a general and robust high-level\nrepresentation for all acous-"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "tic tasks.\nExperiments have been conducted on three kinds"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "of acoustic tasks:\nspeech emotion recognition,\nsound event"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "detection and speech translation. All\nthe experiments have"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "shown that pre-training using its own training data can signif-"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "icantly improve the performance. With a larger pre-training"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "data combining MuST-C, Librispeech and ESC-US datasets,"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "for speech emotion recognition, the UAR can further improve"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "absolutely 4.3% on IEMOCAP dataset. For sound event de-"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "tection,\nthe F1 score can further improve absolutely 1.5% on"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "DCASE2018 task5 development set and 2.1% on evaluation"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "set. For speech translation,\nthe BLEU score can further im-"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "prove relatively 12.2% on En-De dataset and 8.4% on En-Fr"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "dataset."
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "Index Terms— unsupervised pre-training, Transformer,"
        },
        {
          "DiDi Chuxing, Beijing, China": "acoustic representation learning"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "1.\nINTRODUCTION"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "The goal of acoustic representation learning is to transform"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "raw or surface features into the high-level feature which are"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "more accessible to acoustic tasks[1].\nIt\nis critical\nto make"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "acoustic representations more general and robust\nto improve"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "the performance of acoustic tasks. However, the labeled data"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "size of\nthe speciﬁc acoustic task may be limited so that\nthe"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "learned representations\ncan be\nless\nrobust\nand the perfor-"
        },
        {
          "DiDi Chuxing, Beijing, China": ""
        },
        {
          "DiDi Chuxing, Beijing, China": "mance can be vulnerable to unseen data. On the other hand,"
        },
        {
          "DiDi Chuxing, Beijing, China": "there\nexists\nvarieties\nof\nacoustic\ntasks which\nrange\nfrom"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "accepts zt to produce a context representation ct = gar(z≤t).",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "Targeting at predicting future observations xt+k ,a density ra-",
          "4. EXPERIMENTS": "To prove the effectiveness of our unsupervised pre-training"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "tio f (xt+k, ct) is modelled to maximally preserve the mutual",
          "4. EXPERIMENTS": "method on various kinds of acoustic tasks, we selected three"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "information between xt+k and ct. To optimize genc and gar ,",
          "4. EXPERIMENTS": "representative kinds of\ntasks:\nspeech emotion recognition,"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "the contrastive loss is minimized:",
          "4. EXPERIMENTS": "acoustic event detection and speech translation."
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "f (xt+k, ct)",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "],\n[log\n(1)\nŁN = − E",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "(cid:80)",
          "4. EXPERIMENTS": "4.1. Data"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "X\nxj ∈X fk(xj, ct)",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "To pre-train the model using a larger dataset which can be"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "where N represents number of samples in X = x1, x2, ..., xN ,",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "adapted to various kinds of downstream tasks, we merge"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "with one positive sample from distribution p(xt+k|ct) and the",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "MuST-C En-De[7](408\nhours),\nLibrispeech[8](960\nhours)"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "rest being negative samples from distribution p(xt+k).",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "and ESC-US[9]( 347 hours) datasets into one dataset(almost"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "Autoregressive Predictive Coding(APC)[6] also proposed",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "1715 hours) and we call it OpenAudio. Among them, ESC is"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "an autoregressive model for unsupervised speech representa-",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "an open dataset for environmental sound classiﬁcation while"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "tion learning.\nIt used a deep LSTM network and make the",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "ESC-US is a compilation of 250k unlabeled clips which were"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "model to predict further steps ahead of the current frame dur-",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "extracted from public ﬁeld recordings. MuST-C is a mul-"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "ing training. APCs have demonstrated a strong capability of",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "tilingual corpus\nfor\nspeech translation from English into 8"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "extracting useful phone and speaker information.",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "languages.\nFor each target\nlanguage, MuST-C comprises at"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "least 385 hours of audio recordings from English TED Talks."
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "3. METHODOLOGY",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "LibriSpeech is a corpus of reading English speech with sam-"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "pling rate of 16 kHz. The data has been carefully segmented"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "To learn a general high-level acoustic representation, we use",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "and aligned."
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "Transformer based encoder in an unsupervised manner. The",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "For pre-training, we did not use speed perturbation but for"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "architecture of Transformer based encoder\nis\nillustrated in",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "ﬁne-tuning in every downstream task, we used speed pertur-"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "Figure 1(a).",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "bation with factor of 0.9 and 1.1 for data augmentation. We"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "For unsupervised pre-training, Figure 1(b) shows our pre-",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "use 40-dimensional Mel ﬁlter-banks extracted from the audio"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "training procedure.\n15% of\nframes of\nthe acoustic feature",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "signals using window size of 25 ms and step size of 10 ms for"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "sequence will be masked by zeros and the object of unsuper-",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "pre-training and ﬁne-tuning in all downstream tasks."
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "vised pre-training is similar as that of [3] which is to restore",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "the masked frames given the left and right context\nfeatures.",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "4.2. Experimental setups"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "However, we have two aspects that are different from that of",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "[3]. On one hand, we have different masking mechanisms.",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "For Transformer based model, we use the structure discussed"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "Generally speaking,\nthe CNN modules of Transformer based",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "before with hidden dimension size of 256, feed-forward size"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "encoder provide a downsampling mechanism, by which the",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "of 2048, attention heads of 4, dropout rate of 0.1 and encoder"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "frames would be N-fold downsampled. Therefore,\nto reserve",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "layers of 12 for all tasks."
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "the masked information after downsampling operations, we",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "We pre-trained our model using OpenAudio only once"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "split frames into chunks each of which contains N frames and",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "and ﬁne-tuned it\nin each downstream task.\nIt was trained on"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "15% of all chunks will be selected randomly and all frames",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "4 GPUs with a total batch size of 256 for 50 epochs. We used"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "of the selected chunks will be masked by zeros. On the other",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "the Adam optimizer[10] with warmup schedule[5] according"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "hand, Transformer\nencoder\nis\nfollowed by a\nfeed-forward",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "to the formula:"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "layer\nto transform each chunk-level prediction into frame-",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "level predictions. With these changes, we also use L1 loss",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "lrate = k ∗ d0.5\n(2)\nmodel ∗ min(n−0.5, n ∗ warmup n−1.5)"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "to minimize the gap between the predicted frames and the",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "corresponding real frames.",
          "4. EXPERIMENTS": "where n is the step number.\nk = 0.5 and warmup n = 8000"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "For ﬁne-tuning, Transformer\nencoder needs\nto be pre-",
          "4. EXPERIMENTS": "were chosen for all experiments.\nFor comparison, we also"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "trained only once and can be adapted to varieties of acoustic",
          "4. EXPERIMENTS": "pre-trained our model on each task using its own training data"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "tasks no matter whether\nthe downstream task deal with the",
          "4. EXPERIMENTS": "with the same setups as discussed before."
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "speech\nor\nnon-speech\nacoustic\nsequences,\nand\nno matter",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "whether\nthe output of\nthe task is a sequence or a tag.\nAll",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "",
          "4. EXPERIMENTS": "4.3.\nSpeech emotion recognition"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "we should do is to add a decoder\nlayer after\nthe pre-trained",
          "4. EXPERIMENTS": ""
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "encoder to ﬁne-tune the whole model for speciﬁc tasks. The",
          "4. EXPERIMENTS": "The IEMOCAP database[11] was commonly used in previ-"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "choice of decoder\nlayers is based on the tasks as shown in",
          "4. EXPERIMENTS": "ous speech emotion studies[12]. We also use it\nfor our ex-"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "Figure 1(c). We can use Transformer decoder for seq-to-seq",
          "4. EXPERIMENTS": "periments. We used the recordings where majority of anno-"
        },
        {
          "vations xt to a latent embedding space zt = genc(xt) and gar": "tasks and speciﬁc pooling layers for tagging tasks.",
          "4. EXPERIMENTS": "tators agreed on the emotion labels and it contains 4 kinds of"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Results of speech emotion recognition (Note:",
      "data": [
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "Loss",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "Target \tPredict ion",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "Feed-forward",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "Layer",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "Decoder\tLayer",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "",
          "......": ""
        },
        {
          "Cross\tEncropy": "Transformer",
          "......": ""
        },
        {
          "Cross\tEncropy": "encoder",
          "......": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Results of speech emotion recognition (Note:",
      "data": [
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "decoder layer to adapt to the speciﬁc task.",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "emotions: angry, happy, sad and neutral state. Happy and ex-",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "cited emotions were combined as happy in order to balance",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "the number of samples in each emotion class.",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "Table\n1.\nResults"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "contains 5,531 utterances (1,103 angry, 1,636 happy, 1,708",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "Method\nand Data"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "neutral, 1,084 sad) grouped into 5 sessions. We conducted",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "training data respectively)"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "5-fold cross validation on IEMOCAP, taking samples from 8",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "speakers for training and the others for evaluation. For ﬁne-",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "Rozgic et al.[14]"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "tuning, we add an average pooling layer followed by one feed-",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "Xia et al.[15]"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "forward layer. It was trained on 4 GPUs with a total batch size",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "of 64 for 25 epochs. We also use the optimizer which is the",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "Transformer"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "same as that of pre-training. For evaluating the performance,",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "Transformer"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "we restore the checkpoint averaged from best 5 checkpoints",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": "Transformer"
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "during training. We used UAR which is deﬁned as the un-",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "weighted average of the class-speciﬁc recalls achieved by the",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        },
        {
          "the masked acoustic feature using L1 loss. (c) Fine-tuning:": "system as our metrics.",
          "the pre-trained transformer encoder is ﬁne-tuned with an additional": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Results of speech emotion recognition (Note:",
      "data": [
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "system as our metrics."
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": ""
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "In our experiments as\nshown in Table 1, we achieve a"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "mean UAR of 64.9% which is\nsigniﬁcantly better\nthan the"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "state-of-the-art result on this setup. According to [13] and the"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "best of our knowledge, [14] and [15] presented the best results"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "in the condition that almost match our setups.\nSpeciﬁcally,"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "they all use 4 emotion classes and merge happy and excited as"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "one class, except\nthat\nthey used leave-one-speaker-out cross"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "validation and we use leave-one-session-out cross validation."
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "Compared with [13] which has provided another unsuper-"
        },
        {
          "weighted average of the class-speciﬁc recalls achieved by the": "vised pre-training method, our Transformer based model with"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Results of speech translation (Note: Method and",
      "data": [
        {
          "with a total batch size of 128 for 50 epochs. We also use": "the optimizer which is the same as that of pre-training except",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "can easily extend our pre-training data without"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "that k = 0.3. For evaluating the performance, we restore the",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "any speciﬁc label, we evaluated the results of Transformer"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "checkpoint averaged from best 5 checkpoints during training.",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "pre-trained by OpenAudio. The results have shown that"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "to speech emotion recognition, we used an average",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "BLEU scores have exceeded that of [21] pre-trained by ASR"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "pooling layer as the decoder layer for ﬁnetuning.",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "We compared our work with top three teams’",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "We can see that different from current end-to-end speech"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "reports[17, 18, 19] listed on the DCASE community website.",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "translation methods, our methods provide not only better per-"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "Table 2 shows that with pre-training using OpenAudio, Trans-",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "formance but an easier training scheme without transcripts of"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "former based model can achieve better performance than all",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "speech in same language which is more practical for industrial"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "of them on the development set and one of them on the evalu-",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "is also promising that combining our unsu-"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "ation set. Consider that they used well-designed hand-crafted",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "pervised pre-training method with the current supervised pre-"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "features with various kinds of data augmentation and ensem-",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "training mechanism will further improve the performance."
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "ble tricks, our method presents a simple but effective training",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "Results of speech translation (Note: Method and"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "Data represent pre-training method and pre-training data re-"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "Method"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "-"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "Method",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "-"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "-",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "ASR"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "-",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "-"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "-",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "ASR"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "-",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "Ours"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "Ours",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "",
          "that of Transformer pre-trained by ASR. Furthermore, be-": "Ours"
        },
        {
          "with a total batch size of 128 for 50 epochs. We also use": "Ours",
          "that of Transformer pre-trained by ASR. Furthermore, be-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "recognition with unsupervised representation learning"
        },
        {
          "6. REFERENCES": "[1] G. Tzanetakis and P. Cook, “Marsyas: A framework for",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "on unlabeled speech,” in International Conference on"
        },
        {
          "6. REFERENCES": "audio analysis,” Organised sound, vol. 4, no. 3, pp. 169–",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "Acoustics, Speech, and Signal Processing, 2019, 2019."
        },
        {
          "6. REFERENCES": "175, 2000.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[14] V. Rozgic, S. Ananthakrishnan, S. Saleem et al., “En-"
        },
        {
          "6. REFERENCES": "[2] A. v. d. Oord, Y. Li,\nand O. Vinyals,\n“Representa-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "semble of\nsvm trees\nfor multimodal\nemotion recog-"
        },
        {
          "6. REFERENCES": "tion learning with contrastive predictive coding,” arXiv",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "Information Processing Association\nnition,”\nin Signal"
        },
        {
          "6. REFERENCES": "preprint arXiv:1807.03748, 2018.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "Summit And Conference, 2012."
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[15] R. Xia\nand Y. Liu,\n“Leveraging valence\nand activa-"
        },
        {
          "6. REFERENCES": "[3] D. Jiang, X. Lei, W. Li et al., “Improving transformer-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "tion information via multi-task learning for\ncategori-"
        },
        {
          "6. REFERENCES": "based\nspeech\nrecognition\nusing\nunsupervised\npre-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "cal emotion recognition,” in 2015 IEEE International"
        },
        {
          "6. REFERENCES": "training,” arXiv preprint arXiv:1910.09932, 2019.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "6. REFERENCES": "[4]\nJ. Devlin, M.-W. Chang, K. Lee\net al.,\n“Bert:\nPre-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "(ICASSP).\nIEEE, 2015, pp. 5301–5305."
        },
        {
          "6. REFERENCES": "training of deep bidirectional transformers for language",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[16] G. Dekkers, S. Lauwereins, B. Thoen et al., “The SINS"
        },
        {
          "6. REFERENCES": "arXiv\npreprint\nunderstanding,”\narXiv:1810.04805,",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "database for detection of daily activities in a home en-"
        },
        {
          "6. REFERENCES": "2018.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "vironment using an acoustic sensor network,” in Pro-"
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "ceedings of\nthe Detection and Classiﬁcation of Acous-"
        },
        {
          "6. REFERENCES": "[5] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "tic Scenes and Events 2017 Workshop (DCASE2017),"
        },
        {
          "6. REFERENCES": "information pro-\nall you need,” in Advances in neural",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "November 2017, pp. 32–36."
        },
        {
          "6. REFERENCES": "cessing systems, 2017, pp. 5998–6008.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[17] T. Inoue, P. Vinayavekhin, S. Wang et al., “Domestic ac-"
        },
        {
          "6. REFERENCES": "[6] Y.-A. Chung, W.-N. Hsu, H. Tang et al.,\n“An unsu-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "tivities classiﬁcation based on CNN using shufﬂing and"
        },
        {
          "6. REFERENCES": "pervised autoregressive model for speech representation",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "mixing\ndata\naugmentation,” DCASE2018 Challenge,"
        },
        {
          "6. REFERENCES": "learning,” arXiv preprint arXiv:1904.03240, 2019.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "Tech. Rep., September 2018."
        },
        {
          "6. REFERENCES": "[7] M. A. Di Gangi, R. Cattoni, L. Bentivogli et al., “Must-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[18] H. Liu, F. Wang, X. Liu et al., “An ensemble system for"
        },
        {
          "6. REFERENCES": "c:\na multilingual\nspeech translation corpus,”\nin 2019",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "domestic activity recognition,” DCASE2018 Challenge,"
        },
        {
          "6. REFERENCES": "Conference of\nthe North American Chapter of\nthe As-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "Tech. Rep., September 2018."
        },
        {
          "6. REFERENCES": "sociation for Computational Linguistics: Human Lan-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "guage Technologies.\nAssociation for Computational",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[19] H.-W. Liao, J.-Y. Huang, S.-S. Lan et al., “DCASE 2018"
        },
        {
          "6. REFERENCES": "Linguistics, 2019, pp. 2012–2017.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "task 5 challenge technical report: Sound event classiﬁ-"
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "cation by a deep neural network with attention and min-"
        },
        {
          "6. REFERENCES": "[8] V. Panayotov, G. Chen, D. Povey et al., “Librispeech:",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "imum variance distortionless\nresponse\nenhancement,”"
        },
        {
          "6. REFERENCES": "an asr corpus based on public domain audio books,”",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "DCASE2018 Challenge, Tech. Rep., September 2018."
        },
        {
          "6. REFERENCES": "in 2015 IEEE International Conference on Acoustics,",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[20] M. A. D. Gangi, M. Negri, and M. Turchi, “Adapting"
        },
        {
          "6. REFERENCES": "Speech and Signal Processing (ICASSP).\nIEEE, 2015,",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "transformer to end-to-end spoken language translation,”"
        },
        {
          "6. REFERENCES": "pp. 5206–5210.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "in Interspeech 2019, 2019."
        },
        {
          "6. REFERENCES": "[9] K.\nJ. Piczak,\n“Esc: Dataset\nfor environmental\nsound",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "et\n[21] H.\nInaguma,\nS. Kiyono, K. Duh\nal.,\n“Espnet-st:"
        },
        {
          "6. REFERENCES": "classiﬁcation,” in Proceedings of the 23rd ACM interna-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "arXiv preprint\nAll-in-one\nspeech translation toolkit,”"
        },
        {
          "6. REFERENCES": "tional conference on Multimedia, 2015, pp. 1015–1018.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "arXiv:2004.10234, 2020."
        },
        {
          "6. REFERENCES": "[10] D. P. Kingma and J. Ba, “Adam: A method for stochastic",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[22] R. Sennrich, B. Haddow, and A. Birch, “Neural machine"
        },
        {
          "6. REFERENCES": "optimization,” arXiv preprint arXiv:1412.6980, 2014.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "translation of\nrare words with subword units,” in Pro-"
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "ceedings of the 54th Annual Meeting of the Association"
        },
        {
          "6. REFERENCES": "[11] C. Busso, M. Bulut, C.-C. Lee et al., “Iemocap:\nInter-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "for Computational Linguistics (Volume 1: Long Papers)."
        },
        {
          "6. REFERENCES": "active emotional dyadic motion capture database,” Lan-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "Berlin, Germany: Association for Computational Lin-"
        },
        {
          "6. REFERENCES": "guage resources and evaluation, vol. 42, no. 4, p. 335,",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "guistics, Aug. 2016, pp. 1715–1725."
        },
        {
          "6. REFERENCES": "2008.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "[23] K. Papineni, S. Roukos, T. Ward et al., “Bleu: a method"
        },
        {
          "6. REFERENCES": "[12] R. Pappagari, T. Wang, J. Villalba et al., “x-vectors meet",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "for automatic evaluation of machine translation,” in Pro-"
        },
        {
          "6. REFERENCES": "emotions: A study on dependencies between emotion",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "ceedings of\nthe 40th annual meeting on association for"
        },
        {
          "6. REFERENCES": "and speaker\nrecognition,” in ICASSP 2020-2020 IEEE",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "computational\nlinguistics.\nAssociation for Computa-"
        },
        {
          "6. REFERENCES": "International Conference on Acoustics, Speech and Sig-",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        },
        {
          "6. REFERENCES": "",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": "tional Linguistics, 2002, pp. 311–318."
        },
        {
          "6. REFERENCES": "nal Processing (ICASSP).\nIEEE, 2020, pp. 7169–7173.",
          "[13] M. Neumann and T. Vu,\n“Improving speech emotion": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Marsyas: A framework for audio analysis",
      "authors": [
        "G Tzanetakis",
        "P Cook"
      ],
      "year": "2000",
      "venue": "Organised sound"
    },
    {
      "citation_id": "3",
      "title": "Representation learning with contrastive predictive coding",
      "authors": [
        "A Oord",
        "Y Li",
        "O Vinyals"
      ],
      "year": "2018",
      "venue": "Representation learning with contrastive predictive coding",
      "arxiv": "arXiv:1807.03748"
    },
    {
      "citation_id": "4",
      "title": "Improving transformerbased speech recognition using unsupervised pretraining",
      "authors": [
        "D Jiang",
        "X Lei",
        "W Li"
      ],
      "year": "2019",
      "venue": "Improving transformerbased speech recognition using unsupervised pretraining",
      "arxiv": "arXiv:1910.09932"
    },
    {
      "citation_id": "5",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "6",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "7",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Y.-A Chung",
        "W.-N Hsu",
        "H Tang"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning",
      "arxiv": "arXiv:1904.03240"
    },
    {
      "citation_id": "8",
      "title": "Mustc: a multilingual speech translation corpus",
      "authors": [
        "M Di Gangi",
        "R Cattoni",
        "L Bentivogli"
      ],
      "year": "2019",
      "venue": "2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "9",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Esc: Dataset for environmental sound classification",
      "authors": [
        "K Piczak"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM international conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "12",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "13",
      "title": "x-vectors meet emotions: A study on dependencies between emotion and speaker recognition",
      "authors": [
        "R Pappagari",
        "T Wang",
        "J Villalba"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Improving speech emotion recognition with unsupervised representation learning on unlabeled speech",
      "authors": [
        "M Neumann",
        "T Vu"
      ],
      "year": "2019",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Ensemble of svm trees for multimodal emotion recognition",
      "authors": [
        "V Rozgic",
        "S Ananthakrishnan",
        "S Saleem"
      ],
      "year": "2012",
      "venue": "Signal Information Processing Association Summit And Conference"
    },
    {
      "citation_id": "16",
      "title": "Leveraging valence and activation information via multi-task learning for categorical emotion recognition",
      "authors": [
        "R Xia",
        "Y Liu"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "The SINS database for detection of daily activities in a home environment using an acoustic sensor network",
      "authors": [
        "G Dekkers",
        "S Lauwereins",
        "B Thoen"
      ],
      "year": "2017",
      "venue": "Proceedings of the Detection and Classification of Acoustic Scenes and Events"
    },
    {
      "citation_id": "18",
      "title": "Domestic activities classification based on CNN using shuffling and mixing data augmentation",
      "authors": [
        "T Inoue",
        "P Vinayavekhin",
        "S Wang"
      ],
      "year": "2018",
      "venue": "DCASE2018 Challenge, Tech. Rep"
    },
    {
      "citation_id": "19",
      "title": "An ensemble system for domestic activity recognition",
      "authors": [
        "H Liu",
        "F Wang",
        "X Liu"
      ],
      "year": "2018",
      "venue": "DCASE2018 Challenge, Tech. Rep"
    },
    {
      "citation_id": "20",
      "title": "Sound event classification by a deep neural network with attention and minimum variance distortionless response enhancement",
      "authors": [
        "H.-W Liao",
        "J.-Y Huang",
        "S.-S Lan"
      ],
      "year": "2018",
      "venue": "DCASE 2018 task 5 challenge technical report"
    },
    {
      "citation_id": "21",
      "title": "Adapting transformer to end-to-end spoken language translation",
      "authors": [
        "M Gangi",
        "M Negri",
        "M Turchi"
      ],
      "year": "2019",
      "venue": "Adapting transformer to end-to-end spoken language translation"
    },
    {
      "citation_id": "22",
      "title": "Espnet-st: All-in-one speech translation toolkit",
      "authors": [
        "H Inaguma",
        "S Kiyono",
        "K Duh"
      ],
      "year": "2020",
      "venue": "Espnet-st: All-in-one speech translation toolkit",
      "arxiv": "arXiv:2004.10234"
    },
    {
      "citation_id": "23",
      "title": "Neural machine translation of rare words with subword units",
      "authors": [
        "R Sennrich",
        "B Haddow",
        "A Birch"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "K Papineni",
        "S Roukos",
        "T Ward"
      ],
      "year": "2002",
      "venue": "Proceedings of the 40th annual meeting on association for computational linguistics"
    }
  ]
}